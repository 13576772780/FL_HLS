nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.224, Test loss: 2.103, Test accuracy: 24.16 

Round   0, Global train loss: 2.224, Global test loss: 2.093, Global test accuracy: 25.68 

Round   1, Train loss: 2.017, Test loss: 1.975, Test accuracy: 27.81 

Round   1, Global train loss: 2.017, Global test loss: 1.920, Global test accuracy: 30.44 

Round   2, Train loss: 1.928, Test loss: 1.918, Test accuracy: 30.32 

Round   2, Global train loss: 1.928, Global test loss: 1.807, Global test accuracy: 36.31 

Round   3, Train loss: 1.783, Test loss: 1.881, Test accuracy: 30.95 

Round   3, Global train loss: 1.783, Global test loss: 1.683, Global test accuracy: 38.93 

Round   4, Train loss: 1.808, Test loss: 1.864, Test accuracy: 31.68 

Round   4, Global train loss: 1.808, Global test loss: 1.732, Global test accuracy: 37.51 

Round   5, Train loss: 1.809, Test loss: 1.836, Test accuracy: 32.88 

Round   5, Global train loss: 1.809, Global test loss: 1.750, Global test accuracy: 38.01 

Round   6, Train loss: 1.735, Test loss: 1.816, Test accuracy: 33.88 

Round   6, Global train loss: 1.735, Global test loss: 1.702, Global test accuracy: 39.19 

Round   7, Train loss: 1.667, Test loss: 1.795, Test accuracy: 34.82 

Round   7, Global train loss: 1.667, Global test loss: 1.645, Global test accuracy: 40.35 

Round   8, Train loss: 1.630, Test loss: 1.782, Test accuracy: 35.12 

Round   8, Global train loss: 1.630, Global test loss: 1.632, Global test accuracy: 40.79 

Round   9, Train loss: 1.514, Test loss: 1.774, Test accuracy: 35.58 

Round   9, Global train loss: 1.514, Global test loss: 1.578, Global test accuracy: 42.55 

Round  10, Train loss: 1.484, Test loss: 1.776, Test accuracy: 35.87 

Round  10, Global train loss: 1.484, Global test loss: 1.548, Global test accuracy: 43.73 

Round  11, Train loss: 1.566, Test loss: 1.764, Test accuracy: 36.39 

Round  11, Global train loss: 1.566, Global test loss: 1.639, Global test accuracy: 41.05 

Round  12, Train loss: 1.542, Test loss: 1.764, Test accuracy: 36.84 

Round  12, Global train loss: 1.542, Global test loss: 1.625, Global test accuracy: 41.17 

Round  13, Train loss: 1.425, Test loss: 1.778, Test accuracy: 36.63 

Round  13, Global train loss: 1.425, Global test loss: 1.534, Global test accuracy: 43.74 

Round  14, Train loss: 1.449, Test loss: 1.789, Test accuracy: 36.99 

Round  14, Global train loss: 1.449, Global test loss: 1.530, Global test accuracy: 44.18 

Round  15, Train loss: 1.282, Test loss: 1.776, Test accuracy: 37.47 

Round  15, Global train loss: 1.282, Global test loss: 1.495, Global test accuracy: 45.43 

Round  16, Train loss: 1.324, Test loss: 1.782, Test accuracy: 37.73 

Round  16, Global train loss: 1.324, Global test loss: 1.518, Global test accuracy: 44.41 

Round  17, Train loss: 1.314, Test loss: 1.788, Test accuracy: 37.97 

Round  17, Global train loss: 1.314, Global test loss: 1.519, Global test accuracy: 44.69 

Round  18, Train loss: 1.267, Test loss: 1.803, Test accuracy: 38.19 

Round  18, Global train loss: 1.267, Global test loss: 1.502, Global test accuracy: 45.41 

Round  19, Train loss: 1.292, Test loss: 1.817, Test accuracy: 38.41 

Round  19, Global train loss: 1.292, Global test loss: 1.551, Global test accuracy: 43.53 

Round  20, Train loss: 1.295, Test loss: 1.825, Test accuracy: 38.62 

Round  20, Global train loss: 1.295, Global test loss: 1.516, Global test accuracy: 44.77 

Round  21, Train loss: 1.371, Test loss: 1.805, Test accuracy: 39.40 

Round  21, Global train loss: 1.371, Global test loss: 1.562, Global test accuracy: 43.77 

Round  22, Train loss: 1.175, Test loss: 1.811, Test accuracy: 39.62 

Round  22, Global train loss: 1.175, Global test loss: 1.465, Global test accuracy: 47.10 

Round  23, Train loss: 1.163, Test loss: 1.833, Test accuracy: 39.75 

Round  23, Global train loss: 1.163, Global test loss: 1.545, Global test accuracy: 43.80 

Round  24, Train loss: 1.048, Test loss: 1.863, Test accuracy: 39.87 

Round  24, Global train loss: 1.048, Global test loss: 1.512, Global test accuracy: 45.60 

Round  25, Train loss: 1.092, Test loss: 1.877, Test accuracy: 40.30 

Round  25, Global train loss: 1.092, Global test loss: 1.493, Global test accuracy: 45.72 

Round  26, Train loss: 1.059, Test loss: 1.901, Test accuracy: 40.26 

Round  26, Global train loss: 1.059, Global test loss: 1.523, Global test accuracy: 44.63 

Round  27, Train loss: 0.951, Test loss: 1.933, Test accuracy: 40.22 

Round  27, Global train loss: 0.951, Global test loss: 1.518, Global test accuracy: 45.93 

Round  28, Train loss: 1.046, Test loss: 1.976, Test accuracy: 39.88 

Round  28, Global train loss: 1.046, Global test loss: 1.494, Global test accuracy: 46.50 

Round  29, Train loss: 0.862, Test loss: 2.017, Test accuracy: 39.68 

Round  29, Global train loss: 0.862, Global test loss: 1.517, Global test accuracy: 45.97 

Round  30, Train loss: 0.778, Test loss: 2.032, Test accuracy: 39.82 

Round  30, Global train loss: 0.778, Global test loss: 1.572, Global test accuracy: 46.35 

Round  31, Train loss: 0.786, Test loss: 2.036, Test accuracy: 40.34 

Round  31, Global train loss: 0.786, Global test loss: 1.496, Global test accuracy: 47.32 

Round  32, Train loss: 0.906, Test loss: 2.077, Test accuracy: 40.25 

Round  32, Global train loss: 0.906, Global test loss: 1.459, Global test accuracy: 47.35 

Round  33, Train loss: 0.620, Test loss: 2.112, Test accuracy: 40.15 

Round  33, Global train loss: 0.620, Global test loss: 1.540, Global test accuracy: 47.55 

Round  34, Train loss: 0.779, Test loss: 2.139, Test accuracy: 40.21 

Round  34, Global train loss: 0.779, Global test loss: 1.565, Global test accuracy: 46.46 

Round  35, Train loss: 0.983, Test loss: 2.168, Test accuracy: 40.23 

Round  35, Global train loss: 0.983, Global test loss: 1.472, Global test accuracy: 46.69 

Round  36, Train loss: 0.769, Test loss: 2.181, Test accuracy: 40.31 

Round  36, Global train loss: 0.769, Global test loss: 1.511, Global test accuracy: 45.36 

Round  37, Train loss: 0.789, Test loss: 2.194, Test accuracy: 40.48 

Round  37, Global train loss: 0.789, Global test loss: 1.516, Global test accuracy: 45.21 

Round  38, Train loss: 0.730, Test loss: 2.232, Test accuracy: 40.60 

Round  38, Global train loss: 0.730, Global test loss: 1.519, Global test accuracy: 47.59 

Round  39, Train loss: 0.622, Test loss: 2.239, Test accuracy: 40.59 

Round  39, Global train loss: 0.622, Global test loss: 1.514, Global test accuracy: 47.34 

Round  40, Train loss: 0.591, Test loss: 2.312, Test accuracy: 40.40 

Round  40, Global train loss: 0.591, Global test loss: 1.521, Global test accuracy: 48.41 

Round  41, Train loss: 0.590, Test loss: 2.379, Test accuracy: 40.05 

Round  41, Global train loss: 0.590, Global test loss: 1.491, Global test accuracy: 48.70 

Round  42, Train loss: 0.765, Test loss: 2.418, Test accuracy: 40.13 

Round  42, Global train loss: 0.765, Global test loss: 1.528, Global test accuracy: 44.77 

Round  43, Train loss: 0.646, Test loss: 2.432, Test accuracy: 40.14 

Round  43, Global train loss: 0.646, Global test loss: 1.509, Global test accuracy: 47.84 

Round  44, Train loss: 0.492, Test loss: 2.465, Test accuracy: 39.90 

Round  44, Global train loss: 0.492, Global test loss: 1.561, Global test accuracy: 46.58 

Round  45, Train loss: 0.516, Test loss: 2.496, Test accuracy: 40.44 

Round  45, Global train loss: 0.516, Global test loss: 1.532, Global test accuracy: 48.16 

Round  46, Train loss: 0.450, Test loss: 2.492, Test accuracy: 40.21 

Round  46, Global train loss: 0.450, Global test loss: 1.557, Global test accuracy: 48.22 

Round  47, Train loss: 0.467, Test loss: 2.531, Test accuracy: 40.41 

Round  47, Global train loss: 0.467, Global test loss: 1.597, Global test accuracy: 47.49 

Round  48, Train loss: 0.507, Test loss: 2.552, Test accuracy: 40.58 

Round  48, Global train loss: 0.507, Global test loss: 1.561, Global test accuracy: 46.81 

Round  49, Train loss: 0.501, Test loss: 2.576, Test accuracy: 40.39 

Round  49, Global train loss: 0.501, Global test loss: 1.543, Global test accuracy: 47.26 

Round  50, Train loss: 0.364, Test loss: 2.656, Test accuracy: 40.41 

Round  50, Global train loss: 0.364, Global test loss: 1.602, Global test accuracy: 47.65 

Round  51, Train loss: 0.483, Test loss: 2.664, Test accuracy: 40.85 

Round  51, Global train loss: 0.483, Global test loss: 1.564, Global test accuracy: 48.58 

Round  52, Train loss: 0.426, Test loss: 2.752, Test accuracy: 40.91 

Round  52, Global train loss: 0.426, Global test loss: 1.588, Global test accuracy: 47.12 

Round  53, Train loss: 0.420, Test loss: 2.747, Test accuracy: 40.89 

Round  53, Global train loss: 0.420, Global test loss: 1.639, Global test accuracy: 48.27 

Round  54, Train loss: 0.403, Test loss: 2.777, Test accuracy: 40.64 

Round  54, Global train loss: 0.403, Global test loss: 1.614, Global test accuracy: 48.18 

Round  55, Train loss: 0.371, Test loss: 2.828, Test accuracy: 40.55 

Round  55, Global train loss: 0.371, Global test loss: 1.621, Global test accuracy: 46.59 

Round  56, Train loss: 0.395, Test loss: 2.852, Test accuracy: 40.58 

Round  56, Global train loss: 0.395, Global test loss: 1.509, Global test accuracy: 48.02 

Round  57, Train loss: 0.454, Test loss: 2.910, Test accuracy: 40.60 

Round  57, Global train loss: 0.454, Global test loss: 1.516, Global test accuracy: 47.90 

Round  58, Train loss: 0.411, Test loss: 2.931, Test accuracy: 40.42 

Round  58, Global train loss: 0.411, Global test loss: 1.561, Global test accuracy: 46.71 

Round  59, Train loss: 0.414, Test loss: 2.975, Test accuracy: 40.54 

Round  59, Global train loss: 0.414, Global test loss: 1.639, Global test accuracy: 46.62 

Round  60, Train loss: 0.505, Test loss: 3.002, Test accuracy: 41.06 

Round  60, Global train loss: 0.505, Global test loss: 1.559, Global test accuracy: 45.31 

Round  61, Train loss: 0.428, Test loss: 3.071, Test accuracy: 40.65 

Round  61, Global train loss: 0.428, Global test loss: 1.688, Global test accuracy: 45.45 

Round  62, Train loss: 0.477, Test loss: 3.092, Test accuracy: 40.39 

Round  62, Global train loss: 0.477, Global test loss: 1.584, Global test accuracy: 46.95 

Round  63, Train loss: 0.301, Test loss: 3.121, Test accuracy: 40.57 

Round  63, Global train loss: 0.301, Global test loss: 1.625, Global test accuracy: 47.06 

Round  64, Train loss: 0.254, Test loss: 3.188, Test accuracy: 40.40 

Round  64, Global train loss: 0.254, Global test loss: 1.739, Global test accuracy: 47.91 

Round  65, Train loss: 0.353, Test loss: 3.218, Test accuracy: 40.30 

Round  65, Global train loss: 0.353, Global test loss: 1.578, Global test accuracy: 46.92 

Round  66, Train loss: 0.298, Test loss: 3.214, Test accuracy: 40.69 

Round  66, Global train loss: 0.298, Global test loss: 1.649, Global test accuracy: 46.76 

Round  67, Train loss: 0.277, Test loss: 3.263, Test accuracy: 40.41 

Round  67, Global train loss: 0.277, Global test loss: 1.581, Global test accuracy: 48.02 

Round  68, Train loss: 0.296, Test loss: 3.313, Test accuracy: 40.22 

Round  68, Global train loss: 0.296, Global test loss: 1.586, Global test accuracy: 47.94 

Round  69, Train loss: 0.297, Test loss: 3.293, Test accuracy: 40.39 

Round  69, Global train loss: 0.297, Global test loss: 1.660, Global test accuracy: 46.76 

Round  70, Train loss: 0.336, Test loss: 3.359, Test accuracy: 40.41 

Round  70, Global train loss: 0.336, Global test loss: 1.633, Global test accuracy: 45.84 

Round  71, Train loss: 0.292, Test loss: 3.396, Test accuracy: 40.45 

Round  71, Global train loss: 0.292, Global test loss: 1.601, Global test accuracy: 47.61 

Round  72, Train loss: 0.299, Test loss: 3.390, Test accuracy: 40.88 

Round  72, Global train loss: 0.299, Global test loss: 1.610, Global test accuracy: 47.31 

Round  73, Train loss: 0.343, Test loss: 3.400, Test accuracy: 40.66 

Round  73, Global train loss: 0.343, Global test loss: 1.566, Global test accuracy: 45.85 

Round  74, Train loss: 0.305, Test loss: 3.429, Test accuracy: 40.63 

Round  74, Global train loss: 0.305, Global test loss: 1.601, Global test accuracy: 45.77 

Round  75, Train loss: 0.261, Test loss: 3.488, Test accuracy: 40.38 

Round  75, Global train loss: 0.261, Global test loss: 1.615, Global test accuracy: 46.36 

Round  76, Train loss: 0.264, Test loss: 3.505, Test accuracy: 40.50 

Round  76, Global train loss: 0.264, Global test loss: 1.659, Global test accuracy: 45.91 

Round  77, Train loss: 0.253, Test loss: 3.478, Test accuracy: 40.92 

Round  77, Global train loss: 0.253, Global test loss: 1.580, Global test accuracy: 49.31 

Round  78, Train loss: 0.259, Test loss: 3.523, Test accuracy: 40.99 

Round  78, Global train loss: 0.259, Global test loss: 1.567, Global test accuracy: 48.65 

Round  79, Train loss: 0.268, Test loss: 3.527, Test accuracy: 40.88 

Round  79, Global train loss: 0.268, Global test loss: 1.721, Global test accuracy: 47.56 

Round  80, Train loss: 0.234, Test loss: 3.537, Test accuracy: 40.68 

Round  80, Global train loss: 0.234, Global test loss: 1.615, Global test accuracy: 47.69 

Round  81, Train loss: 0.264, Test loss: 3.589, Test accuracy: 40.83 

Round  81, Global train loss: 0.264, Global test loss: 1.599, Global test accuracy: 44.78 

Round  82, Train loss: 0.219, Test loss: 3.599, Test accuracy: 40.98 

Round  82, Global train loss: 0.219, Global test loss: 1.638, Global test accuracy: 47.16 

Round  83, Train loss: 0.275, Test loss: 3.650, Test accuracy: 40.56 

Round  83, Global train loss: 0.275, Global test loss: 1.551, Global test accuracy: 45.84 

Round  84, Train loss: 0.169, Test loss: 3.663, Test accuracy: 40.55 

Round  84, Global train loss: 0.169, Global test loss: 1.633, Global test accuracy: 46.24 

Round  85, Train loss: 0.167, Test loss: 3.715, Test accuracy: 40.51 

Round  85, Global train loss: 0.167, Global test loss: 1.766, Global test accuracy: 47.55 

Round  86, Train loss: 0.229, Test loss: 3.736, Test accuracy: 40.52 

Round  86, Global train loss: 0.229, Global test loss: 1.668, Global test accuracy: 46.59 

Round  87, Train loss: 0.205, Test loss: 3.768, Test accuracy: 40.46 

Round  87, Global train loss: 0.205, Global test loss: 1.644, Global test accuracy: 47.25 

Round  88, Train loss: 0.192, Test loss: 3.800, Test accuracy: 40.59 

Round  88, Global train loss: 0.192, Global test loss: 1.622, Global test accuracy: 47.86 

Round  89, Train loss: 0.220, Test loss: 3.764, Test accuracy: 40.85 

Round  89, Global train loss: 0.220, Global test loss: 1.595, Global test accuracy: 47.61 

Round  90, Train loss: 0.157, Test loss: 3.828, Test accuracy: 40.61 

Round  90, Global train loss: 0.157, Global test loss: 1.760, Global test accuracy: 46.81 

Round  91, Train loss: 0.170, Test loss: 3.867, Test accuracy: 40.76 

Round  91, Global train loss: 0.170, Global test loss: 1.645, Global test accuracy: 47.34 

Round  92, Train loss: 0.189, Test loss: 3.886, Test accuracy: 40.64 

Round  92, Global train loss: 0.189, Global test loss: 1.727, Global test accuracy: 45.95 

Round  93, Train loss: 0.138, Test loss: 3.931, Test accuracy: 40.97 

Round  93, Global train loss: 0.138, Global test loss: 1.807, Global test accuracy: 47.39 

Round  94, Train loss: 0.157, Test loss: 4.057, Test accuracy: 40.54 

Round  94, Global train loss: 0.157, Global test loss: 1.749, Global test accuracy: 46.50 

Round  95, Train loss: 0.172, Test loss: 4.061, Test accuracy: 40.39 

Round  95, Global train loss: 0.172, Global test loss: 1.656, Global test accuracy: 47.23 

Round  96, Train loss: 0.181, Test loss: 4.014, Test accuracy: 40.50 

Round  96, Global train loss: 0.181, Global test loss: 1.622, Global test accuracy: 47.41 

Round  97, Train loss: 0.156, Test loss: 3.970, Test accuracy: 40.91 

Round  97, Global train loss: 0.156, Global test loss: 1.674, Global test accuracy: 46.94 

Round  98, Train loss: 0.176, Test loss: 3.941, Test accuracy: 41.12 

Round  98, Global train loss: 0.176, Global test loss: 1.599, Global test accuracy: 46.06 

Round  99, Train loss: 0.150, Test loss: 4.021, Test accuracy: 41.03 

Round  99, Global train loss: 0.150, Global test loss: 1.643, Global test accuracy: 46.37 

Final Round, Train loss: 0.159, Test loss: 4.157, Test accuracy: 41.26 

Final Round, Global train loss: 0.159, Global test loss: 1.643, Global test accuracy: 46.37 

Average accuracy final 10 rounds: 40.747 

Average global accuracy final 10 rounds: 46.7985 

2945.1408081054688
[1.4819788932800293, 2.5733373165130615, 3.679443836212158, 4.795589447021484, 5.896623611450195, 6.992550849914551, 8.236435174942017, 9.478853940963745, 10.717509508132935, 11.819994449615479, 12.915416479110718, 14.18061089515686, 15.434199571609497, 16.68618679046631, 17.93885040283203, 19.184128284454346, 20.41937518119812, 21.68595004081726, 22.842997074127197, 23.955073356628418, 25.07674813270569, 26.287684202194214, 27.547672748565674, 28.822453022003174, 30.00201725959778, 31.256408214569092, 32.500298261642456, 33.75621581077576, 35.00240468978882, 36.241923809051514, 37.499062299728394, 38.774115324020386, 40.024359464645386, 41.27697157859802, 42.55954933166504, 43.80467677116394, 45.05329775810242, 46.29110622406006, 47.518908977508545, 48.747525691986084, 49.843286752700806, 50.91208457946777, 51.96383047103882, 53.02062249183655, 54.07359266281128, 55.154351234436035, 56.223745822906494, 57.31195092201233, 58.531723976135254, 59.744168519973755, 60.96131134033203, 62.169737815856934, 63.38912892341614, 64.5946888923645, 65.79396390914917, 66.87397789955139, 67.95358228683472, 69.03375720977783, 70.11507201194763, 71.19535875320435, 72.28167176246643, 73.36353492736816, 74.44350028038025, 75.52886486053467, 76.61265277862549, 77.691237449646, 78.7657539844513, 79.99285578727722, 81.20931005477905, 82.29477882385254, 83.3847644329071, 84.46275854110718, 85.52357935905457, 86.58478355407715, 87.64400005340576, 88.69951939582825, 89.75601291656494, 90.82615852355957, 92.06724429130554, 93.27872586250305, 94.48111891746521, 95.69256615638733, 96.9246621131897, 98.16360926628113, 99.38412284851074, 100.58022785186768, 101.7714171409607, 102.97474217414856, 104.17779445648193, 105.37642168998718, 106.58592748641968, 107.80157208442688, 108.85841608047485, 109.96183681488037, 111.03199315071106, 112.09890580177307, 113.15374064445496, 114.20523595809937, 115.27054238319397, 116.36596751213074, 118.51174092292786]
[24.1625, 27.8125, 30.3175, 30.945, 31.685, 32.8775, 33.88, 34.8225, 35.1225, 35.58, 35.8725, 36.3875, 36.835, 36.63, 36.9875, 37.4725, 37.725, 37.97, 38.185, 38.4125, 38.615, 39.4025, 39.62, 39.7525, 39.8675, 40.295, 40.255, 40.2225, 39.88, 39.68, 39.82, 40.3375, 40.2475, 40.15, 40.2125, 40.23, 40.31, 40.475, 40.605, 40.585, 40.4, 40.0525, 40.135, 40.14, 39.9, 40.44, 40.2075, 40.41, 40.575, 40.3875, 40.405, 40.85, 40.9125, 40.8925, 40.64, 40.555, 40.5825, 40.605, 40.4175, 40.5425, 41.0625, 40.645, 40.3925, 40.57, 40.395, 40.295, 40.6875, 40.405, 40.22, 40.3925, 40.405, 40.4475, 40.8825, 40.66, 40.6275, 40.385, 40.4975, 40.92, 40.99, 40.885, 40.68, 40.8275, 40.975, 40.56, 40.555, 40.51, 40.515, 40.4625, 40.595, 40.85, 40.6125, 40.76, 40.6375, 40.9725, 40.5425, 40.3925, 40.5025, 40.905, 41.115, 41.03, 41.26]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.218, Test loss: 2.082, Test accuracy: 24.77 

Round   0, Global train loss: 2.218, Global test loss: 2.071, Global test accuracy: 25.65 

Round   1, Train loss: 2.005, Test loss: 1.964, Test accuracy: 28.64 

Round   1, Global train loss: 2.005, Global test loss: 1.882, Global test accuracy: 32.38 

Round   2, Train loss: 1.862, Test loss: 1.856, Test accuracy: 32.40 

Round   2, Global train loss: 1.862, Global test loss: 1.730, Global test accuracy: 37.92 

Round   3, Train loss: 1.759, Test loss: 1.784, Test accuracy: 34.66 

Round   3, Global train loss: 1.759, Global test loss: 1.632, Global test accuracy: 40.70 

Round   4, Train loss: 1.691, Test loss: 1.777, Test accuracy: 35.29 

Round   4, Global train loss: 1.691, Global test loss: 1.559, Global test accuracy: 43.28 

Round   5, Train loss: 1.649, Test loss: 1.705, Test accuracy: 37.74 

Round   5, Global train loss: 1.649, Global test loss: 1.499, Global test accuracy: 45.37 

Round   6, Train loss: 1.587, Test loss: 1.639, Test accuracy: 40.37 

Round   6, Global train loss: 1.587, Global test loss: 1.464, Global test accuracy: 46.62 

Round   7, Train loss: 1.524, Test loss: 1.633, Test accuracy: 40.66 

Round   7, Global train loss: 1.524, Global test loss: 1.426, Global test accuracy: 48.34 

Round   8, Train loss: 1.489, Test loss: 1.611, Test accuracy: 41.59 

Round   8, Global train loss: 1.489, Global test loss: 1.391, Global test accuracy: 49.89 

Round   9, Train loss: 1.461, Test loss: 1.595, Test accuracy: 42.24 

Round   9, Global train loss: 1.461, Global test loss: 1.345, Global test accuracy: 51.52 

Round  10, Train loss: 1.444, Test loss: 1.578, Test accuracy: 43.15 

Round  10, Global train loss: 1.444, Global test loss: 1.323, Global test accuracy: 52.18 

Round  11, Train loss: 1.359, Test loss: 1.532, Test accuracy: 44.89 

Round  11, Global train loss: 1.359, Global test loss: 1.307, Global test accuracy: 53.23 

Round  12, Train loss: 1.392, Test loss: 1.502, Test accuracy: 46.11 

Round  12, Global train loss: 1.392, Global test loss: 1.273, Global test accuracy: 54.20 

Round  13, Train loss: 1.336, Test loss: 1.471, Test accuracy: 47.59 

Round  13, Global train loss: 1.336, Global test loss: 1.248, Global test accuracy: 55.49 

Round  14, Train loss: 1.293, Test loss: 1.467, Test accuracy: 48.07 

Round  14, Global train loss: 1.293, Global test loss: 1.221, Global test accuracy: 56.35 

Round  15, Train loss: 1.255, Test loss: 1.457, Test accuracy: 48.34 

Round  15, Global train loss: 1.255, Global test loss: 1.221, Global test accuracy: 56.25 

Round  16, Train loss: 1.234, Test loss: 1.432, Test accuracy: 49.48 

Round  16, Global train loss: 1.234, Global test loss: 1.195, Global test accuracy: 57.05 

Round  17, Train loss: 1.212, Test loss: 1.415, Test accuracy: 49.96 

Round  17, Global train loss: 1.212, Global test loss: 1.175, Global test accuracy: 58.00 

Round  18, Train loss: 1.191, Test loss: 1.405, Test accuracy: 50.48 

Round  18, Global train loss: 1.191, Global test loss: 1.168, Global test accuracy: 58.47 

Round  19, Train loss: 1.161, Test loss: 1.397, Test accuracy: 50.97 

Round  19, Global train loss: 1.161, Global test loss: 1.145, Global test accuracy: 59.32 

Round  20, Train loss: 1.134, Test loss: 1.391, Test accuracy: 51.53 

Round  20, Global train loss: 1.134, Global test loss: 1.138, Global test accuracy: 59.65 

Round  21, Train loss: 1.118, Test loss: 1.367, Test accuracy: 52.61 

Round  21, Global train loss: 1.118, Global test loss: 1.121, Global test accuracy: 60.81 

Round  22, Train loss: 1.083, Test loss: 1.377, Test accuracy: 52.65 

Round  22, Global train loss: 1.083, Global test loss: 1.126, Global test accuracy: 60.92 

Round  23, Train loss: 1.107, Test loss: 1.365, Test accuracy: 53.34 

Round  23, Global train loss: 1.107, Global test loss: 1.106, Global test accuracy: 61.18 

Round  24, Train loss: 1.055, Test loss: 1.350, Test accuracy: 53.80 

Round  24, Global train loss: 1.055, Global test loss: 1.086, Global test accuracy: 61.77 

Round  25, Train loss: 1.050, Test loss: 1.324, Test accuracy: 54.53 

Round  25, Global train loss: 1.050, Global test loss: 1.087, Global test accuracy: 61.82 

Round  26, Train loss: 0.995, Test loss: 1.331, Test accuracy: 54.62 

Round  26, Global train loss: 0.995, Global test loss: 1.084, Global test accuracy: 62.27 

Round  27, Train loss: 0.982, Test loss: 1.342, Test accuracy: 54.82 

Round  27, Global train loss: 0.982, Global test loss: 1.093, Global test accuracy: 61.65 

Round  28, Train loss: 0.984, Test loss: 1.353, Test accuracy: 54.70 

Round  28, Global train loss: 0.984, Global test loss: 1.058, Global test accuracy: 63.17 

Round  29, Train loss: 0.952, Test loss: 1.337, Test accuracy: 55.18 

Round  29, Global train loss: 0.952, Global test loss: 1.070, Global test accuracy: 62.74 

Round  30, Train loss: 0.970, Test loss: 1.329, Test accuracy: 55.52 

Round  30, Global train loss: 0.970, Global test loss: 1.054, Global test accuracy: 63.05 

Round  31, Train loss: 0.893, Test loss: 1.347, Test accuracy: 55.39 

Round  31, Global train loss: 0.893, Global test loss: 1.065, Global test accuracy: 62.78 

Round  32, Train loss: 0.937, Test loss: 1.343, Test accuracy: 55.96 

Round  32, Global train loss: 0.937, Global test loss: 1.056, Global test accuracy: 63.22 

Round  33, Train loss: 0.904, Test loss: 1.338, Test accuracy: 56.15 

Round  33, Global train loss: 0.904, Global test loss: 1.057, Global test accuracy: 63.86 

Round  34, Train loss: 0.866, Test loss: 1.337, Test accuracy: 56.29 

Round  34, Global train loss: 0.866, Global test loss: 1.050, Global test accuracy: 63.58 

Round  35, Train loss: 0.827, Test loss: 1.339, Test accuracy: 56.58 

Round  35, Global train loss: 0.827, Global test loss: 1.058, Global test accuracy: 63.67 

Round  36, Train loss: 0.858, Test loss: 1.339, Test accuracy: 56.84 

Round  36, Global train loss: 0.858, Global test loss: 1.043, Global test accuracy: 63.91 

Round  37, Train loss: 0.859, Test loss: 1.362, Test accuracy: 56.67 

Round  37, Global train loss: 0.859, Global test loss: 1.052, Global test accuracy: 64.05 

Round  38, Train loss: 0.813, Test loss: 1.366, Test accuracy: 56.80 

Round  38, Global train loss: 0.813, Global test loss: 1.029, Global test accuracy: 65.08 

Round  39, Train loss: 0.783, Test loss: 1.372, Test accuracy: 56.84 

Round  39, Global train loss: 0.783, Global test loss: 1.052, Global test accuracy: 64.92 

Round  40, Train loss: 0.833, Test loss: 1.353, Test accuracy: 57.37 

Round  40, Global train loss: 0.833, Global test loss: 1.035, Global test accuracy: 64.65 

Round  41, Train loss: 0.818, Test loss: 1.337, Test accuracy: 57.81 

Round  41, Global train loss: 0.818, Global test loss: 1.028, Global test accuracy: 65.19 

Round  42, Train loss: 0.830, Test loss: 1.327, Test accuracy: 58.02 

Round  42, Global train loss: 0.830, Global test loss: 1.019, Global test accuracy: 65.40 

Round  43, Train loss: 0.774, Test loss: 1.329, Test accuracy: 58.40 

Round  43, Global train loss: 0.774, Global test loss: 1.020, Global test accuracy: 65.71 

Round  44, Train loss: 0.714, Test loss: 1.330, Test accuracy: 58.61 

Round  44, Global train loss: 0.714, Global test loss: 1.032, Global test accuracy: 65.65 

Round  45, Train loss: 0.820, Test loss: 1.331, Test accuracy: 58.76 

Round  45, Global train loss: 0.820, Global test loss: 1.005, Global test accuracy: 66.06 

Round  46, Train loss: 0.762, Test loss: 1.364, Test accuracy: 58.08 

Round  46, Global train loss: 0.762, Global test loss: 1.020, Global test accuracy: 66.01 

Round  47, Train loss: 0.715, Test loss: 1.341, Test accuracy: 58.61 

Round  47, Global train loss: 0.715, Global test loss: 1.034, Global test accuracy: 65.92 

Round  48, Train loss: 0.690, Test loss: 1.356, Test accuracy: 58.69 

Round  48, Global train loss: 0.690, Global test loss: 1.054, Global test accuracy: 66.26 

Round  49, Train loss: 0.758, Test loss: 1.350, Test accuracy: 59.13 

Round  49, Global train loss: 0.758, Global test loss: 1.015, Global test accuracy: 66.38 

Round  50, Train loss: 0.703, Test loss: 1.352, Test accuracy: 59.00 

Round  50, Global train loss: 0.703, Global test loss: 1.037, Global test accuracy: 66.01 

Round  51, Train loss: 0.704, Test loss: 1.347, Test accuracy: 59.31 

Round  51, Global train loss: 0.704, Global test loss: 1.021, Global test accuracy: 66.63 

Round  52, Train loss: 0.709, Test loss: 1.354, Test accuracy: 59.47 

Round  52, Global train loss: 0.709, Global test loss: 1.033, Global test accuracy: 66.03 

Round  53, Train loss: 0.720, Test loss: 1.362, Test accuracy: 59.51 

Round  53, Global train loss: 0.720, Global test loss: 0.997, Global test accuracy: 67.12 

Round  54, Train loss: 0.672, Test loss: 1.375, Test accuracy: 59.19 

Round  54, Global train loss: 0.672, Global test loss: 1.024, Global test accuracy: 66.18 

Round  55, Train loss: 0.654, Test loss: 1.367, Test accuracy: 59.39 

Round  55, Global train loss: 0.654, Global test loss: 1.016, Global test accuracy: 66.83 

Round  56, Train loss: 0.635, Test loss: 1.374, Test accuracy: 59.59 

Round  56, Global train loss: 0.635, Global test loss: 1.056, Global test accuracy: 66.56 

Round  57, Train loss: 0.637, Test loss: 1.387, Test accuracy: 59.55 

Round  57, Global train loss: 0.637, Global test loss: 1.033, Global test accuracy: 66.78 

Round  58, Train loss: 0.633, Test loss: 1.382, Test accuracy: 59.69 

Round  58, Global train loss: 0.633, Global test loss: 1.047, Global test accuracy: 66.23 

Round  59, Train loss: 0.663, Test loss: 1.386, Test accuracy: 59.91 

Round  59, Global train loss: 0.663, Global test loss: 1.035, Global test accuracy: 66.50 

Round  60, Train loss: 0.615, Test loss: 1.399, Test accuracy: 59.91 

Round  60, Global train loss: 0.615, Global test loss: 1.050, Global test accuracy: 66.68 

Round  61, Train loss: 0.632, Test loss: 1.385, Test accuracy: 60.19 

Round  61, Global train loss: 0.632, Global test loss: 1.040, Global test accuracy: 67.00 

Round  62, Train loss: 0.642, Test loss: 1.385, Test accuracy: 60.39 

Round  62, Global train loss: 0.642, Global test loss: 1.037, Global test accuracy: 66.84 

Round  63, Train loss: 0.635, Test loss: 1.375, Test accuracy: 60.44 

Round  63, Global train loss: 0.635, Global test loss: 1.024, Global test accuracy: 67.30 

Round  64, Train loss: 0.620, Test loss: 1.395, Test accuracy: 60.34 

Round  64, Global train loss: 0.620, Global test loss: 1.042, Global test accuracy: 66.95 

Round  65, Train loss: 0.626, Test loss: 1.395, Test accuracy: 60.28 

Round  65, Global train loss: 0.626, Global test loss: 1.034, Global test accuracy: 67.38 

Round  66, Train loss: 0.569, Test loss: 1.382, Test accuracy: 60.67 

Round  66, Global train loss: 0.569, Global test loss: 1.050, Global test accuracy: 67.33 

Round  67, Train loss: 0.586, Test loss: 1.387, Test accuracy: 60.60 

Round  67, Global train loss: 0.586, Global test loss: 1.035, Global test accuracy: 67.78 

Round  68, Train loss: 0.572, Test loss: 1.385, Test accuracy: 60.66 

Round  68, Global train loss: 0.572, Global test loss: 1.041, Global test accuracy: 67.44 

Round  69, Train loss: 0.533, Test loss: 1.386, Test accuracy: 60.68 

Round  69, Global train loss: 0.533, Global test loss: 1.043, Global test accuracy: 67.76 

Round  70, Train loss: 0.585, Test loss: 1.406, Test accuracy: 60.65 

Round  70, Global train loss: 0.585, Global test loss: 1.065, Global test accuracy: 67.11 

Round  71, Train loss: 0.610, Test loss: 1.415, Test accuracy: 60.74 

Round  71, Global train loss: 0.610, Global test loss: 1.036, Global test accuracy: 67.29 

Round  72, Train loss: 0.531, Test loss: 1.428, Test accuracy: 60.62 

Round  72, Global train loss: 0.531, Global test loss: 1.057, Global test accuracy: 67.88 

Round  73, Train loss: 0.541, Test loss: 1.417, Test accuracy: 60.80 

Round  73, Global train loss: 0.541, Global test loss: 1.054, Global test accuracy: 67.96 

Round  74, Train loss: 0.565, Test loss: 1.439, Test accuracy: 60.65 

Round  74, Global train loss: 0.565, Global test loss: 1.058, Global test accuracy: 67.92 

Round  75, Train loss: 0.593, Test loss: 1.426, Test accuracy: 61.19 

Round  75, Global train loss: 0.593, Global test loss: 1.047, Global test accuracy: 68.15 

Round  76, Train loss: 0.516, Test loss: 1.423, Test accuracy: 61.22 

Round  76, Global train loss: 0.516, Global test loss: 1.052, Global test accuracy: 67.76 

Round  77, Train loss: 0.487, Test loss: 1.433, Test accuracy: 61.01 

Round  77, Global train loss: 0.487, Global test loss: 1.079, Global test accuracy: 67.94 

Round  78, Train loss: 0.496, Test loss: 1.437, Test accuracy: 61.14 

Round  78, Global train loss: 0.496, Global test loss: 1.078, Global test accuracy: 67.58 

Round  79, Train loss: 0.551, Test loss: 1.462, Test accuracy: 61.33 

Round  79, Global train loss: 0.551, Global test loss: 1.081, Global test accuracy: 67.63 

Round  80, Train loss: 0.498, Test loss: 1.463, Test accuracy: 61.51 

Round  80, Global train loss: 0.498, Global test loss: 1.087, Global test accuracy: 67.60 

Round  81, Train loss: 0.522, Test loss: 1.444, Test accuracy: 61.64 

Round  81, Global train loss: 0.522, Global test loss: 1.063, Global test accuracy: 67.88 

Round  82, Train loss: 0.471, Test loss: 1.449, Test accuracy: 61.56 

Round  82, Global train loss: 0.471, Global test loss: 1.094, Global test accuracy: 68.07 

Round  83, Train loss: 0.496, Test loss: 1.463, Test accuracy: 61.64 

Round  83, Global train loss: 0.496, Global test loss: 1.083, Global test accuracy: 67.91 

Round  84, Train loss: 0.538, Test loss: 1.471, Test accuracy: 61.53 

Round  84, Global train loss: 0.538, Global test loss: 1.075, Global test accuracy: 67.83 

Round  85, Train loss: 0.472, Test loss: 1.455, Test accuracy: 61.77 

Round  85, Global train loss: 0.472, Global test loss: 1.092, Global test accuracy: 68.04 

Round  86, Train loss: 0.508, Test loss: 1.453, Test accuracy: 61.88 

Round  86, Global train loss: 0.508, Global test loss: 1.084, Global test accuracy: 67.77 

Round  87, Train loss: 0.453, Test loss: 1.434, Test accuracy: 62.26 

Round  87, Global train loss: 0.453, Global test loss: 1.110, Global test accuracy: 68.07 

Round  88, Train loss: 0.473, Test loss: 1.473, Test accuracy: 62.00 

Round  88, Global train loss: 0.473, Global test loss: 1.127, Global test accuracy: 67.64 

Round  89, Train loss: 0.515, Test loss: 1.466, Test accuracy: 61.96 

Round  89, Global train loss: 0.515, Global test loss: 1.066, Global test accuracy: 68.38 

Round  90, Train loss: 0.470, Test loss: 1.454, Test accuracy: 62.25 

Round  90, Global train loss: 0.470, Global test loss: 1.100, Global test accuracy: 68.08 

Round  91, Train loss: 0.504, Test loss: 1.438, Test accuracy: 62.44 

Round  91, Global train loss: 0.504, Global test loss: 1.070, Global test accuracy: 68.57 

Round  92, Train loss: 0.447, Test loss: 1.446, Test accuracy: 62.44 

Round  92, Global train loss: 0.447, Global test loss: 1.110, Global test accuracy: 68.59 

Round  93, Train loss: 0.476, Test loss: 1.449, Test accuracy: 62.43 

Round  93, Global train loss: 0.476, Global test loss: 1.103, Global test accuracy: 67.86 

Round  94, Train loss: 0.489, Test loss: 1.440, Test accuracy: 62.55 

Round  94, Global train loss: 0.489, Global test loss: 1.072, Global test accuracy: 68.70 

Round  95, Train loss: 0.448, Test loss: 1.472, Test accuracy: 62.26 

Round  95, Global train loss: 0.448, Global test loss: 1.130, Global test accuracy: 68.92 

Round  96, Train loss: 0.478, Test loss: 1.470, Test accuracy: 62.45 

Round  96, Global train loss: 0.478, Global test loss: 1.097, Global test accuracy: 68.24 

Round  97, Train loss: 0.458, Test loss: 1.473, Test accuracy: 62.51 

Round  97, Global train loss: 0.458, Global test loss: 1.090, Global test accuracy: 68.46 

Round  98, Train loss: 0.416, Test loss: 1.467, Test accuracy: 62.52 

Round  98, Global train loss: 0.416, Global test loss: 1.121, Global test accuracy: 68.01 

Round  99, Train loss: 0.486, Test loss: 1.469, Test accuracy: 62.61 

Round  99, Global train loss: 0.486, Global test loss: 1.102, Global test accuracy: 68.52 

Final Round, Train loss: 0.354, Test loss: 1.673, Test accuracy: 61.87 

Final Round, Global train loss: 0.354, Global test loss: 1.102, Global test accuracy: 68.52 

Average accuracy final 10 rounds: 62.44725 

Average global accuracy final 10 rounds: 68.39524999999999 

3165.046957731247
[1.4576804637908936, 2.6816303730010986, 3.9060730934143066, 5.03795862197876, 6.230193376541138, 7.259289264678955, 8.282835721969604, 9.587402820587158, 10.802609920501709, 12.11606216430664, 13.690427780151367, 14.950023651123047, 16.19369602203369, 17.467503547668457, 18.756086826324463, 20.006558656692505, 21.258273601531982, 22.521442890167236, 23.70201325416565, 24.837844848632812, 26.069597959518433, 27.37642526626587, 28.663793087005615, 29.863595247268677, 30.994845151901245, 32.122798681259155, 33.276158571243286, 34.42185831069946, 35.55831241607666, 36.68056058883667, 37.83325433731079, 38.97289776802063, 40.12212109565735, 41.26232719421387, 42.3895800113678, 43.550010204315186, 44.71546649932861, 45.82239627838135, 46.96412706375122, 48.11476445198059, 49.258495807647705, 50.34173536300659, 51.42175555229187, 52.49887251853943, 53.598100900650024, 54.66589117050171, 55.75328040122986, 56.912471771240234, 58.04134654998779, 59.196250438690186, 60.54617404937744, 61.77834963798523, 62.82338809967041, 64.03329014778137, 65.25160026550293, 66.46311330795288, 67.8976583480835, 69.1749951839447, 70.44233226776123, 71.61921238899231, 72.79720878601074, 73.97602272033691, 75.17717146873474, 76.36036419868469, 77.51940894126892, 78.71874594688416, 79.89009284973145, 81.05931615829468, 82.2283616065979, 83.43517065048218, 84.6063506603241, 85.8967878818512, 87.15562629699707, 88.36772441864014, 89.59724807739258, 90.84586477279663, 91.962411403656, 93.2540602684021, 94.42917251586914, 95.58963179588318, 97.00609970092773, 98.25602841377258, 99.5133285522461, 100.7824776172638, 102.01609539985657, 103.2452142238617, 104.50859665870667, 105.78063583374023, 107.02513885498047, 108.30062222480774, 109.49874234199524, 110.71022057533264, 111.99259829521179, 113.18124914169312, 114.4286253452301, 115.64286875724792, 116.84338331222534, 117.98870611190796, 119.12116265296936, 120.25121569633484, 122.52293395996094]
[24.775, 28.6425, 32.4025, 34.665, 35.2925, 37.7375, 40.3725, 40.6625, 41.59, 42.2425, 43.1475, 44.8925, 46.1125, 47.5875, 48.0725, 48.34, 49.4775, 49.9625, 50.4825, 50.9675, 51.535, 52.6075, 52.645, 53.345, 53.8, 54.5275, 54.625, 54.8175, 54.7025, 55.1775, 55.515, 55.39, 55.9625, 56.1475, 56.29, 56.5825, 56.8425, 56.67, 56.795, 56.845, 57.3725, 57.815, 58.0225, 58.4, 58.6125, 58.76, 58.08, 58.6125, 58.685, 59.13, 59.0, 59.31, 59.465, 59.5075, 59.19, 59.3925, 59.59, 59.5525, 59.6925, 59.9125, 59.91, 60.185, 60.3875, 60.4425, 60.3375, 60.2825, 60.675, 60.6, 60.6575, 60.6825, 60.65, 60.74, 60.62, 60.8, 60.65, 61.19, 61.22, 61.0125, 61.14, 61.3275, 61.5125, 61.6375, 61.565, 61.64, 61.535, 61.775, 61.885, 62.26, 62.0, 61.96, 62.2475, 62.4425, 62.4425, 62.4325, 62.5475, 62.255, 62.455, 62.5125, 62.525, 62.6125, 61.865]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.286, Test loss: 2.228, Test accuracy: 17.98 

Round   1, Train loss: 2.168, Test loss: 2.022, Test accuracy: 26.68 

Round   2, Train loss: 1.996, Test loss: 1.932, Test accuracy: 30.07 

Round   3, Train loss: 1.915, Test loss: 1.828, Test accuracy: 33.17 

Round   4, Train loss: 1.823, Test loss: 1.758, Test accuracy: 35.32 

Round   5, Train loss: 1.752, Test loss: 1.700, Test accuracy: 36.97 

Round   6, Train loss: 1.732, Test loss: 1.654, Test accuracy: 39.15 

Round   7, Train loss: 1.679, Test loss: 1.629, Test accuracy: 40.60 

Round   8, Train loss: 1.665, Test loss: 1.587, Test accuracy: 41.47 

Round   9, Train loss: 1.591, Test loss: 1.552, Test accuracy: 43.29 

Round  10, Train loss: 1.574, Test loss: 1.545, Test accuracy: 42.82 

Round  11, Train loss: 1.534, Test loss: 1.527, Test accuracy: 43.75 

Round  12, Train loss: 1.525, Test loss: 1.523, Test accuracy: 44.45 

Round  13, Train loss: 1.504, Test loss: 1.489, Test accuracy: 45.79 

Round  14, Train loss: 1.474, Test loss: 1.468, Test accuracy: 46.69 

Round  15, Train loss: 1.458, Test loss: 1.438, Test accuracy: 47.77 

Round  16, Train loss: 1.424, Test loss: 1.430, Test accuracy: 48.08 

Round  17, Train loss: 1.414, Test loss: 1.415, Test accuracy: 48.84 

Round  18, Train loss: 1.407, Test loss: 1.392, Test accuracy: 49.49 

Round  19, Train loss: 1.368, Test loss: 1.395, Test accuracy: 49.67 

Round  20, Train loss: 1.371, Test loss: 1.375, Test accuracy: 50.63 

Round  21, Train loss: 1.351, Test loss: 1.359, Test accuracy: 51.29 

Round  22, Train loss: 1.336, Test loss: 1.358, Test accuracy: 51.28 

Round  23, Train loss: 1.293, Test loss: 1.360, Test accuracy: 50.71 

Round  24, Train loss: 1.320, Test loss: 1.330, Test accuracy: 51.86 

Round  25, Train loss: 1.276, Test loss: 1.307, Test accuracy: 52.85 

Round  26, Train loss: 1.253, Test loss: 1.283, Test accuracy: 53.71 

Round  27, Train loss: 1.237, Test loss: 1.287, Test accuracy: 53.30 

Round  28, Train loss: 1.235, Test loss: 1.273, Test accuracy: 54.41 

Round  29, Train loss: 1.173, Test loss: 1.263, Test accuracy: 54.20 

Round  30, Train loss: 1.215, Test loss: 1.246, Test accuracy: 55.10 

Round  31, Train loss: 1.152, Test loss: 1.243, Test accuracy: 55.59 

Round  32, Train loss: 1.160, Test loss: 1.244, Test accuracy: 55.82 

Round  33, Train loss: 1.182, Test loss: 1.240, Test accuracy: 55.86 

Round  34, Train loss: 1.142, Test loss: 1.222, Test accuracy: 56.31 

Round  35, Train loss: 1.136, Test loss: 1.222, Test accuracy: 56.25 

Round  36, Train loss: 1.076, Test loss: 1.222, Test accuracy: 56.40 

Round  37, Train loss: 1.103, Test loss: 1.214, Test accuracy: 56.91 

Round  38, Train loss: 1.074, Test loss: 1.187, Test accuracy: 57.71 

Round  39, Train loss: 1.054, Test loss: 1.198, Test accuracy: 57.56 

Round  40, Train loss: 1.044, Test loss: 1.207, Test accuracy: 57.49 

Round  41, Train loss: 1.050, Test loss: 1.180, Test accuracy: 58.26 

Round  42, Train loss: 1.010, Test loss: 1.204, Test accuracy: 57.69 

Round  43, Train loss: 1.053, Test loss: 1.173, Test accuracy: 58.49 

Round  44, Train loss: 1.020, Test loss: 1.183, Test accuracy: 58.15 

Round  45, Train loss: 1.028, Test loss: 1.180, Test accuracy: 58.58 

Round  46, Train loss: 1.006, Test loss: 1.156, Test accuracy: 59.34 

Round  47, Train loss: 0.968, Test loss: 1.176, Test accuracy: 58.79 

Round  48, Train loss: 0.975, Test loss: 1.161, Test accuracy: 59.53 

Round  49, Train loss: 0.967, Test loss: 1.176, Test accuracy: 59.15 

Round  50, Train loss: 0.920, Test loss: 1.168, Test accuracy: 59.09 

Round  51, Train loss: 0.945, Test loss: 1.168, Test accuracy: 59.02 

Round  52, Train loss: 0.928, Test loss: 1.165, Test accuracy: 59.03 

Round  53, Train loss: 0.918, Test loss: 1.183, Test accuracy: 59.40 

Round  54, Train loss: 0.921, Test loss: 1.168, Test accuracy: 59.73 

Round  55, Train loss: 0.912, Test loss: 1.162, Test accuracy: 59.68 

Round  56, Train loss: 0.886, Test loss: 1.171, Test accuracy: 59.26 

Round  57, Train loss: 0.927, Test loss: 1.168, Test accuracy: 60.02 

Round  58, Train loss: 0.875, Test loss: 1.172, Test accuracy: 60.02 

Round  59, Train loss: 0.854, Test loss: 1.160, Test accuracy: 60.51 

Round  60, Train loss: 0.841, Test loss: 1.170, Test accuracy: 59.94 

Round  61, Train loss: 0.825, Test loss: 1.194, Test accuracy: 59.93 

Round  62, Train loss: 0.889, Test loss: 1.173, Test accuracy: 60.03 

Round  63, Train loss: 0.846, Test loss: 1.171, Test accuracy: 60.63 

Round  64, Train loss: 0.825, Test loss: 1.173, Test accuracy: 60.21 

Round  65, Train loss: 0.800, Test loss: 1.172, Test accuracy: 60.48 

Round  66, Train loss: 0.825, Test loss: 1.190, Test accuracy: 60.19 

Round  67, Train loss: 0.814, Test loss: 1.181, Test accuracy: 60.46 

Round  68, Train loss: 0.825, Test loss: 1.167, Test accuracy: 60.73 

Round  69, Train loss: 0.785, Test loss: 1.181, Test accuracy: 60.89 

Round  70, Train loss: 0.768, Test loss: 1.168, Test accuracy: 60.99 

Round  71, Train loss: 0.784, Test loss: 1.163, Test accuracy: 61.38 

Round  72, Train loss: 0.725, Test loss: 1.184, Test accuracy: 61.12 

Round  73, Train loss: 0.744, Test loss: 1.169, Test accuracy: 61.92 

Round  74, Train loss: 0.735, Test loss: 1.188, Test accuracy: 61.40 

Round  75, Train loss: 0.773, Test loss: 1.174, Test accuracy: 61.79 

Round  76, Train loss: 0.733, Test loss: 1.187, Test accuracy: 62.15 

Round  77, Train loss: 0.721, Test loss: 1.190, Test accuracy: 61.69 

Round  78, Train loss: 0.740, Test loss: 1.188, Test accuracy: 61.70 

Round  79, Train loss: 0.690, Test loss: 1.202, Test accuracy: 61.70 

Round  80, Train loss: 0.720, Test loss: 1.203, Test accuracy: 62.01 

Round  81, Train loss: 0.724, Test loss: 1.187, Test accuracy: 62.52 

Round  82, Train loss: 0.710, Test loss: 1.184, Test accuracy: 62.05 

Round  83, Train loss: 0.671, Test loss: 1.210, Test accuracy: 62.08 

Round  84, Train loss: 0.672, Test loss: 1.210, Test accuracy: 62.34 

Round  85, Train loss: 0.691, Test loss: 1.226, Test accuracy: 61.81 

Round  86, Train loss: 0.692, Test loss: 1.222, Test accuracy: 61.97 

Round  87, Train loss: 0.675, Test loss: 1.215, Test accuracy: 62.10 

Round  88, Train loss: 0.664, Test loss: 1.207, Test accuracy: 62.05 

Round  89, Train loss: 0.644, Test loss: 1.240, Test accuracy: 61.82 

Round  90, Train loss: 0.655, Test loss: 1.221, Test accuracy: 62.47 

Round  91, Train loss: 0.653, Test loss: 1.215, Test accuracy: 61.84 

Round  92, Train loss: 0.635, Test loss: 1.247, Test accuracy: 62.38 

Round  93, Train loss: 0.610, Test loss: 1.256, Test accuracy: 62.19 

Round  94, Train loss: 0.620, Test loss: 1.221, Test accuracy: 62.45 

Round  95, Train loss: 0.627, Test loss: 1.249, Test accuracy: 62.15 

Round  96, Train loss: 0.612, Test loss: 1.235, Test accuracy: 62.26 

Round  97, Train loss: 0.610, Test loss: 1.228, Test accuracy: 62.25 

Round  98, Train loss: 0.589, Test loss: 1.266, Test accuracy: 62.47 

Round  99, Train loss: 0.634, Test loss: 1.234, Test accuracy: 62.48 

Final Round, Train loss: 0.533, Test loss: 1.248, Test accuracy: 63.08 

Average accuracy final 10 rounds: 62.293 

1905.239984035492
[1.458648681640625, 2.6435983180999756, 3.8140006065368652, 4.934945106506348, 6.066142320632935, 7.200292110443115, 8.335392236709595, 9.484697818756104, 10.652511358261108, 11.784353256225586, 12.915084600448608, 14.057092189788818, 15.14673376083374, 16.23609185218811, 17.305948495864868, 18.370641708374023, 19.438287019729614, 20.514307498931885, 21.597391605377197, 22.666630268096924, 23.719217538833618, 24.76726269721985, 25.85747218132019, 26.9333336353302, 28.010340452194214, 29.097792625427246, 30.163501501083374, 31.22903800010681, 32.31184196472168, 33.377458333969116, 34.42802906036377, 35.51337790489197, 36.599581241607666, 37.68365430831909, 38.75491499900818, 39.835808753967285, 40.94022512435913, 42.00663113594055, 43.07689189910889, 44.14273428916931, 45.21657586097717, 46.282124757766724, 47.36048150062561, 48.43806219100952, 49.512532472610474, 50.60057330131531, 51.684343099594116, 52.76364779472351, 53.83357524871826, 54.90551733970642, 55.98596739768982, 57.04096746444702, 58.146419286727905, 59.227362871170044, 60.303313970565796, 61.385907888412476, 62.47075438499451, 63.55487251281738, 64.64073133468628, 65.71880650520325, 66.81294989585876, 67.90717458724976, 68.9871916770935, 70.05988931655884, 71.13386654853821, 72.19434094429016, 73.27621746063232, 74.3521409034729, 75.4159083366394, 76.49067568778992, 77.56339502334595, 78.6683497428894, 79.75248956680298, 80.83353090286255, 81.90427088737488, 82.97258734703064, 84.04863953590393, 85.1104428768158, 86.19375991821289, 87.25790357589722, 88.34944343566895, 89.42964315414429, 90.51016497612, 91.59215116500854, 92.67350816726685, 93.7597587108612, 94.82722783088684, 95.9198591709137, 96.99832773208618, 98.05316710472107, 99.14993381500244, 100.22954940795898, 101.32039451599121, 102.38938355445862, 103.45665979385376, 104.51618909835815, 105.58316969871521, 106.6852674484253, 107.78799080848694, 108.88698768615723, 110.88362336158752]
[17.9775, 26.6775, 30.0725, 33.17, 35.32, 36.97, 39.15, 40.605, 41.47, 43.29, 42.82, 43.7525, 44.4525, 45.7925, 46.685, 47.7725, 48.08, 48.835, 49.49, 49.675, 50.635, 51.2925, 51.28, 50.7075, 51.86, 52.85, 53.71, 53.305, 54.41, 54.205, 55.1, 55.5925, 55.8225, 55.86, 56.3125, 56.25, 56.4, 56.9125, 57.7075, 57.56, 57.49, 58.2625, 57.685, 58.4875, 58.145, 58.575, 59.3425, 58.79, 59.5325, 59.15, 59.085, 59.0175, 59.035, 59.4025, 59.735, 59.6825, 59.2625, 60.015, 60.02, 60.51, 59.935, 59.93, 60.035, 60.6275, 60.2125, 60.4825, 60.185, 60.46, 60.73, 60.8875, 60.9875, 61.3825, 61.115, 61.925, 61.3975, 61.79, 62.1525, 61.69, 61.705, 61.7025, 62.005, 62.525, 62.0525, 62.075, 62.34, 61.815, 61.965, 62.1, 62.0475, 61.8225, 62.47, 61.845, 62.375, 62.1875, 62.4475, 62.1475, 62.2575, 62.2525, 62.4675, 62.48, 63.0825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.167, Test loss: 2.003, Test accuracy: 27.43 

Round   1, Train loss: 1.962, Test loss: 1.839, Test accuracy: 32.17 

Round   2, Train loss: 1.853, Test loss: 1.728, Test accuracy: 36.27 

Round   3, Train loss: 1.751, Test loss: 1.646, Test accuracy: 39.43 

Round   4, Train loss: 1.657, Test loss: 1.581, Test accuracy: 41.43 

Round   5, Train loss: 1.616, Test loss: 1.546, Test accuracy: 42.85 

Round   6, Train loss: 1.563, Test loss: 1.497, Test accuracy: 45.28 

Round   7, Train loss: 1.539, Test loss: 1.468, Test accuracy: 46.32 

Round   8, Train loss: 1.509, Test loss: 1.440, Test accuracy: 47.58 

Round   9, Train loss: 1.460, Test loss: 1.414, Test accuracy: 48.38 

Round  10, Train loss: 1.422, Test loss: 1.392, Test accuracy: 49.41 

Round  11, Train loss: 1.382, Test loss: 1.362, Test accuracy: 50.45 

Round  12, Train loss: 1.361, Test loss: 1.358, Test accuracy: 50.90 

Round  13, Train loss: 1.313, Test loss: 1.323, Test accuracy: 52.16 

Round  14, Train loss: 1.296, Test loss: 1.321, Test accuracy: 52.65 

Round  15, Train loss: 1.278, Test loss: 1.302, Test accuracy: 54.01 

Round  16, Train loss: 1.235, Test loss: 1.276, Test accuracy: 54.85 

Round  17, Train loss: 1.214, Test loss: 1.256, Test accuracy: 55.45 

Round  18, Train loss: 1.166, Test loss: 1.252, Test accuracy: 56.02 

Round  19, Train loss: 1.154, Test loss: 1.231, Test accuracy: 56.33 

Round  20, Train loss: 1.108, Test loss: 1.239, Test accuracy: 56.05 

Round  21, Train loss: 1.089, Test loss: 1.227, Test accuracy: 57.01 

Round  22, Train loss: 1.104, Test loss: 1.215, Test accuracy: 57.29 

Round  23, Train loss: 1.064, Test loss: 1.202, Test accuracy: 57.94 

Round  24, Train loss: 1.026, Test loss: 1.203, Test accuracy: 58.62 

Round  25, Train loss: 1.032, Test loss: 1.182, Test accuracy: 58.98 

Round  26, Train loss: 0.989, Test loss: 1.178, Test accuracy: 59.27 

Round  27, Train loss: 0.957, Test loss: 1.189, Test accuracy: 59.22 

Round  28, Train loss: 0.941, Test loss: 1.170, Test accuracy: 59.85 

Round  29, Train loss: 0.929, Test loss: 1.174, Test accuracy: 59.91 

Round  30, Train loss: 0.948, Test loss: 1.166, Test accuracy: 59.99 

Round  31, Train loss: 0.917, Test loss: 1.175, Test accuracy: 60.22 

Round  32, Train loss: 0.851, Test loss: 1.162, Test accuracy: 60.49 

Round  33, Train loss: 0.831, Test loss: 1.167, Test accuracy: 60.88 

Round  34, Train loss: 0.867, Test loss: 1.181, Test accuracy: 60.60 

Round  35, Train loss: 0.789, Test loss: 1.193, Test accuracy: 60.69 

Round  36, Train loss: 0.843, Test loss: 1.156, Test accuracy: 61.24 

Round  37, Train loss: 0.832, Test loss: 1.169, Test accuracy: 60.90 

Round  38, Train loss: 0.833, Test loss: 1.167, Test accuracy: 61.03 

Round  39, Train loss: 0.779, Test loss: 1.203, Test accuracy: 60.45 

Round  40, Train loss: 0.739, Test loss: 1.217, Test accuracy: 60.72 

Round  41, Train loss: 0.758, Test loss: 1.196, Test accuracy: 61.01 

Round  42, Train loss: 0.764, Test loss: 1.203, Test accuracy: 61.36 

Round  43, Train loss: 0.779, Test loss: 1.199, Test accuracy: 61.05 

Round  44, Train loss: 0.710, Test loss: 1.218, Test accuracy: 61.13 

Round  45, Train loss: 0.746, Test loss: 1.212, Test accuracy: 61.26 

Round  46, Train loss: 0.715, Test loss: 1.191, Test accuracy: 61.98 

Round  47, Train loss: 0.670, Test loss: 1.226, Test accuracy: 61.43 

Round  48, Train loss: 0.713, Test loss: 1.243, Test accuracy: 61.41 

Round  49, Train loss: 0.696, Test loss: 1.222, Test accuracy: 61.66 

Round  50, Train loss: 0.643, Test loss: 1.226, Test accuracy: 61.69 

Round  51, Train loss: 0.626, Test loss: 1.240, Test accuracy: 61.45 

Round  52, Train loss: 0.641, Test loss: 1.261, Test accuracy: 61.53 

Round  53, Train loss: 0.625, Test loss: 1.242, Test accuracy: 61.79 

Round  54, Train loss: 0.599, Test loss: 1.268, Test accuracy: 61.90 

Round  55, Train loss: 0.612, Test loss: 1.262, Test accuracy: 62.00 

Round  56, Train loss: 0.573, Test loss: 1.284, Test accuracy: 62.13 

Round  57, Train loss: 0.587, Test loss: 1.319, Test accuracy: 61.55 

Round  58, Train loss: 0.547, Test loss: 1.303, Test accuracy: 61.66 

Round  59, Train loss: 0.609, Test loss: 1.321, Test accuracy: 61.85 

Round  60, Train loss: 0.613, Test loss: 1.321, Test accuracy: 61.68 

Round  61, Train loss: 0.572, Test loss: 1.320, Test accuracy: 61.52 

Round  62, Train loss: 0.558, Test loss: 1.336, Test accuracy: 61.79 

Round  63, Train loss: 0.552, Test loss: 1.350, Test accuracy: 61.68 

Round  64, Train loss: 0.518, Test loss: 1.359, Test accuracy: 62.03 

Round  65, Train loss: 0.541, Test loss: 1.352, Test accuracy: 61.54 

Round  66, Train loss: 0.478, Test loss: 1.365, Test accuracy: 61.65 

Round  67, Train loss: 0.501, Test loss: 1.396, Test accuracy: 61.45 

Round  68, Train loss: 0.559, Test loss: 1.394, Test accuracy: 61.56 

Round  69, Train loss: 0.544, Test loss: 1.360, Test accuracy: 62.06 

Round  70, Train loss: 0.543, Test loss: 1.379, Test accuracy: 61.45 

Round  71, Train loss: 0.517, Test loss: 1.393, Test accuracy: 61.80 

Round  72, Train loss: 0.476, Test loss: 1.427, Test accuracy: 61.59 

Round  73, Train loss: 0.485, Test loss: 1.408, Test accuracy: 61.40 

Round  74, Train loss: 0.476, Test loss: 1.445, Test accuracy: 61.60 

Round  75, Train loss: 0.477, Test loss: 1.421, Test accuracy: 61.82 

Round  76, Train loss: 0.438, Test loss: 1.409, Test accuracy: 61.88 

Round  77, Train loss: 0.475, Test loss: 1.426, Test accuracy: 62.17 

Round  78, Train loss: 0.460, Test loss: 1.474, Test accuracy: 61.67 

Round  79, Train loss: 0.450, Test loss: 1.444, Test accuracy: 61.78 

Round  80, Train loss: 0.431, Test loss: 1.503, Test accuracy: 61.40 

Round  81, Train loss: 0.414, Test loss: 1.469, Test accuracy: 61.71 

Round  82, Train loss: 0.428, Test loss: 1.502, Test accuracy: 61.33 

Round  83, Train loss: 0.468, Test loss: 1.478, Test accuracy: 61.45 

Round  84, Train loss: 0.473, Test loss: 1.488, Test accuracy: 61.58 

Round  85, Train loss: 0.455, Test loss: 1.478, Test accuracy: 62.02 

Round  86, Train loss: 0.441, Test loss: 1.539, Test accuracy: 61.55 

Round  87, Train loss: 0.411, Test loss: 1.545, Test accuracy: 61.87 

Round  88, Train loss: 0.439, Test loss: 1.527, Test accuracy: 61.72 

Round  89, Train loss: 0.401, Test loss: 1.588, Test accuracy: 61.17 

Round  90, Train loss: 0.374, Test loss: 1.551, Test accuracy: 61.65 

Round  91, Train loss: 0.406, Test loss: 1.566, Test accuracy: 61.27 

Round  92, Train loss: 0.413, Test loss: 1.561, Test accuracy: 61.35 

Round  93, Train loss: 0.414, Test loss: 1.517, Test accuracy: 61.56 

Round  94, Train loss: 0.425, Test loss: 1.593, Test accuracy: 61.66 

Round  95, Train loss: 0.398, Test loss: 1.586, Test accuracy: 61.84 

Round  96, Train loss: 0.406, Test loss: 1.601, Test accuracy: 62.02 

Round  97, Train loss: 0.402, Test loss: 1.622, Test accuracy: 61.76 

Round  98, Train loss: 0.406, Test loss: 1.567, Test accuracy: 61.67 

Round  99, Train loss: 0.386, Test loss: 1.609, Test accuracy: 61.77 

Final Round, Train loss: 0.303, Test loss: 1.634, Test accuracy: 61.77 

Average accuracy final 10 rounds: 61.65425 

1943.1735429763794
[1.4343955516815186, 2.683506965637207, 3.902240037918091, 5.114027738571167, 6.3434412479400635, 7.592385292053223, 8.807329654693604, 10.007685661315918, 11.2174391746521, 12.437276840209961, 13.695075750350952, 14.913077116012573, 16.126545190811157, 17.330047130584717, 18.538381814956665, 19.736437797546387, 20.98592472076416, 22.184187173843384, 23.403254747390747, 24.60490918159485, 25.826806783676147, 27.02425241470337, 28.278934717178345, 29.489075899124146, 30.70166301727295, 31.917924165725708, 33.13157606124878, 34.35710692405701, 35.57544445991516, 36.765275716781616, 37.88831853866577, 39.01302981376648, 40.14076590538025, 41.27300977706909, 42.37976145744324, 43.518619775772095, 44.628933906555176, 45.74268436431885, 46.913395404815674, 48.0409619808197, 49.17304992675781, 50.28986620903015, 51.40069246292114, 52.52177143096924, 53.64537835121155, 54.757407903671265, 55.871182918548584, 57.02870988845825, 58.12325859069824, 59.24048471450806, 60.356051206588745, 61.507418632507324, 62.64747714996338, 63.76402044296265, 64.90547466278076, 65.99976205825806, 67.10288906097412, 68.21846723556519, 69.32108521461487, 70.45264005661011, 71.56763744354248, 72.69094681739807, 73.82020711898804, 74.9350163936615, 76.03422164916992, 77.1543641090393, 78.25775265693665, 79.36606287956238, 80.47198367118835, 81.58047413825989, 82.69615769386292, 83.84435987472534, 84.93524217605591, 86.0567135810852, 87.1830883026123, 88.29841327667236, 89.41869521141052, 90.5487380027771, 91.70559072494507, 92.82733845710754, 93.95555996894836, 95.07177472114563, 96.17838335037231, 97.28183007240295, 98.38949632644653, 99.50687170028687, 100.61694407463074, 101.72094821929932, 102.81316184997559, 103.91691541671753, 105.04845476150513, 106.1571593284607, 107.27397179603577, 108.38255047798157, 109.50057077407837, 110.6323709487915, 111.75793743133545, 112.9099063873291, 114.03102660179138, 115.15681052207947, 117.19096446037292]
[27.43, 32.17, 36.27, 39.43, 41.4325, 42.85, 45.2825, 46.3175, 47.575, 48.38, 49.415, 50.455, 50.8975, 52.1575, 52.645, 54.005, 54.8475, 55.4525, 56.025, 56.3275, 56.045, 57.0075, 57.29, 57.9375, 58.62, 58.9825, 59.2675, 59.2175, 59.855, 59.91, 59.9925, 60.22, 60.4875, 60.88, 60.5975, 60.69, 61.2375, 60.9, 61.03, 60.445, 60.7175, 61.005, 61.3625, 61.0475, 61.1325, 61.255, 61.9825, 61.4275, 61.41, 61.66, 61.6875, 61.4525, 61.5325, 61.7875, 61.9025, 62.0, 62.1325, 61.5475, 61.655, 61.8475, 61.68, 61.525, 61.7925, 61.68, 62.0325, 61.54, 61.645, 61.45, 61.5575, 62.0625, 61.445, 61.795, 61.5875, 61.4025, 61.6025, 61.8225, 61.875, 62.1725, 61.6675, 61.7775, 61.4, 61.7125, 61.325, 61.4475, 61.575, 62.025, 61.55, 61.87, 61.72, 61.175, 61.65, 61.265, 61.355, 61.5575, 61.6625, 61.835, 62.015, 61.755, 61.675, 61.7725, 61.775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.210, Test loss: 2.064, Test accuracy: 24.32 

Round   1, Train loss: 1.985, Test loss: 1.918, Test accuracy: 28.95 

Round   2, Train loss: 1.890, Test loss: 1.827, Test accuracy: 32.72 

Round   3, Train loss: 1.804, Test loss: 1.801, Test accuracy: 34.01 

Round   4, Train loss: 1.721, Test loss: 1.773, Test accuracy: 34.96 

Round   5, Train loss: 1.683, Test loss: 1.743, Test accuracy: 36.13 

Round   6, Train loss: 1.576, Test loss: 1.738, Test accuracy: 36.76 

Round   7, Train loss: 1.586, Test loss: 1.727, Test accuracy: 37.76 

Round   8, Train loss: 1.579, Test loss: 1.707, Test accuracy: 38.15 

Round   9, Train loss: 1.514, Test loss: 1.696, Test accuracy: 38.76 

Round  10, Train loss: 1.577, Test loss: 1.660, Test accuracy: 40.16 

Round  11, Train loss: 1.487, Test loss: 1.654, Test accuracy: 40.41 

Round  12, Train loss: 1.440, Test loss: 1.665, Test accuracy: 40.78 

Round  13, Train loss: 1.445, Test loss: 1.685, Test accuracy: 40.74 

Round  14, Train loss: 1.279, Test loss: 1.685, Test accuracy: 41.29 

Round  15, Train loss: 1.329, Test loss: 1.672, Test accuracy: 41.76 

Round  16, Train loss: 1.158, Test loss: 1.701, Test accuracy: 41.52 

Round  17, Train loss: 1.125, Test loss: 1.727, Test accuracy: 41.69 

Round  18, Train loss: 1.338, Test loss: 1.747, Test accuracy: 41.40 

Round  19, Train loss: 1.135, Test loss: 1.776, Test accuracy: 41.62 

Round  20, Train loss: 1.188, Test loss: 1.802, Test accuracy: 41.63 

Round  21, Train loss: 1.142, Test loss: 1.790, Test accuracy: 41.73 

Round  22, Train loss: 1.090, Test loss: 1.814, Test accuracy: 41.95 

Round  23, Train loss: 1.059, Test loss: 1.834, Test accuracy: 42.00 

Round  24, Train loss: 1.002, Test loss: 1.886, Test accuracy: 41.94 

Round  25, Train loss: 0.968, Test loss: 1.913, Test accuracy: 42.27 

Round  26, Train loss: 0.936, Test loss: 1.902, Test accuracy: 42.34 

Round  27, Train loss: 0.932, Test loss: 1.941, Test accuracy: 42.13 

Round  28, Train loss: 0.887, Test loss: 1.977, Test accuracy: 42.47 

Round  29, Train loss: 0.907, Test loss: 2.000, Test accuracy: 42.56 

Round  30, Train loss: 0.892, Test loss: 2.025, Test accuracy: 42.33 

Round  31, Train loss: 0.758, Test loss: 2.072, Test accuracy: 42.44 

Round  32, Train loss: 0.764, Test loss: 2.107, Test accuracy: 42.44 

Round  33, Train loss: 0.736, Test loss: 2.183, Test accuracy: 41.79 

Round  34, Train loss: 0.638, Test loss: 2.201, Test accuracy: 42.19 

Round  35, Train loss: 0.638, Test loss: 2.235, Test accuracy: 42.27 

Round  36, Train loss: 0.639, Test loss: 2.249, Test accuracy: 42.39 

Round  37, Train loss: 0.553, Test loss: 2.298, Test accuracy: 42.08 

Round  38, Train loss: 0.539, Test loss: 2.416, Test accuracy: 41.97 

Round  39, Train loss: 0.728, Test loss: 2.377, Test accuracy: 42.38 

Round  40, Train loss: 0.584, Test loss: 2.387, Test accuracy: 42.02 

Round  41, Train loss: 0.562, Test loss: 2.427, Test accuracy: 42.11 

Round  42, Train loss: 0.463, Test loss: 2.470, Test accuracy: 42.22 

Round  43, Train loss: 0.538, Test loss: 2.517, Test accuracy: 42.01 

Round  44, Train loss: 0.528, Test loss: 2.559, Test accuracy: 42.12 

Round  45, Train loss: 0.580, Test loss: 2.568, Test accuracy: 42.13 

Round  46, Train loss: 0.502, Test loss: 2.617, Test accuracy: 41.84 

Round  47, Train loss: 0.472, Test loss: 2.649, Test accuracy: 42.13 

Round  48, Train loss: 0.421, Test loss: 2.674, Test accuracy: 42.25 

Round  49, Train loss: 0.465, Test loss: 2.707, Test accuracy: 42.48 

Round  50, Train loss: 0.340, Test loss: 2.754, Test accuracy: 42.37 

Round  51, Train loss: 0.383, Test loss: 2.767, Test accuracy: 42.41 

Round  52, Train loss: 0.348, Test loss: 2.827, Test accuracy: 42.36 

Round  53, Train loss: 0.397, Test loss: 2.866, Test accuracy: 42.04 

Round  54, Train loss: 0.414, Test loss: 2.846, Test accuracy: 42.04 

Round  55, Train loss: 0.384, Test loss: 2.924, Test accuracy: 42.09 

Round  56, Train loss: 0.295, Test loss: 2.939, Test accuracy: 42.09 

Round  57, Train loss: 0.335, Test loss: 2.968, Test accuracy: 42.00 

Round  58, Train loss: 0.369, Test loss: 3.077, Test accuracy: 42.16 

Round  59, Train loss: 0.320, Test loss: 3.036, Test accuracy: 42.26 

Round  60, Train loss: 0.290, Test loss: 3.147, Test accuracy: 42.12 

Round  61, Train loss: 0.314, Test loss: 3.202, Test accuracy: 41.92 

Round  62, Train loss: 0.332, Test loss: 3.191, Test accuracy: 42.22 

Round  63, Train loss: 0.249, Test loss: 3.198, Test accuracy: 42.23 

Round  64, Train loss: 0.283, Test loss: 3.261, Test accuracy: 42.23 

Round  65, Train loss: 0.269, Test loss: 3.284, Test accuracy: 41.88 

Round  66, Train loss: 0.291, Test loss: 3.246, Test accuracy: 42.36 

Round  67, Train loss: 0.285, Test loss: 3.275, Test accuracy: 42.38 

Round  68, Train loss: 0.212, Test loss: 3.346, Test accuracy: 42.25 

Round  69, Train loss: 0.310, Test loss: 3.366, Test accuracy: 42.32 

Round  70, Train loss: 0.247, Test loss: 3.368, Test accuracy: 42.06 

Round  71, Train loss: 0.231, Test loss: 3.367, Test accuracy: 42.08 

Round  72, Train loss: 0.219, Test loss: 3.341, Test accuracy: 42.37 

Round  73, Train loss: 0.214, Test loss: 3.359, Test accuracy: 42.21 

Round  74, Train loss: 0.262, Test loss: 3.361, Test accuracy: 42.15 

Round  75, Train loss: 0.231, Test loss: 3.409, Test accuracy: 42.27 

Round  76, Train loss: 0.192, Test loss: 3.499, Test accuracy: 41.94 

Round  77, Train loss: 0.247, Test loss: 3.491, Test accuracy: 42.33 

Round  78, Train loss: 0.250, Test loss: 3.505, Test accuracy: 42.30 

Round  79, Train loss: 0.198, Test loss: 3.513, Test accuracy: 42.64 

Round  80, Train loss: 0.185, Test loss: 3.624, Test accuracy: 42.42 

Round  81, Train loss: 0.171, Test loss: 3.668, Test accuracy: 42.59 

Round  82, Train loss: 0.183, Test loss: 3.693, Test accuracy: 42.28 

Round  83, Train loss: 0.187, Test loss: 3.674, Test accuracy: 42.45 

Round  84, Train loss: 0.178, Test loss: 3.642, Test accuracy: 42.59 

Round  85, Train loss: 0.177, Test loss: 3.649, Test accuracy: 42.23 

Round  86, Train loss: 0.173, Test loss: 3.700, Test accuracy: 42.51 

Round  87, Train loss: 0.197, Test loss: 3.723, Test accuracy: 42.51 

Round  88, Train loss: 0.160, Test loss: 3.778, Test accuracy: 42.58 

Round  89, Train loss: 0.169, Test loss: 3.740, Test accuracy: 42.52 

Round  90, Train loss: 0.150, Test loss: 3.772, Test accuracy: 42.41 

Round  91, Train loss: 0.191, Test loss: 3.806, Test accuracy: 42.41 

Round  92, Train loss: 0.153, Test loss: 3.788, Test accuracy: 42.27 

Round  93, Train loss: 0.131, Test loss: 3.865, Test accuracy: 42.53 

Round  94, Train loss: 0.171, Test loss: 3.865, Test accuracy: 42.33 

Round  95, Train loss: 0.160, Test loss: 3.956, Test accuracy: 42.04 

Round  96, Train loss: 0.174, Test loss: 3.956, Test accuracy: 42.23 

Round  97, Train loss: 0.172, Test loss: 3.900, Test accuracy: 42.33 

Round  98, Train loss: 0.140, Test loss: 3.883, Test accuracy: 42.52 

Round  99, Train loss: 0.118, Test loss: 3.950, Test accuracy: 42.43 

Final Round, Train loss: 0.101, Test loss: 4.272, Test accuracy: 42.46 

Average accuracy final 10 rounds: 42.349000000000004 

1944.7026257514954
[1.471256971359253, 2.703814744949341, 3.9481379985809326, 5.198821544647217, 6.449038982391357, 7.671117305755615, 8.886036157608032, 10.112164974212646, 11.323356628417969, 12.551512956619263, 13.809937953948975, 15.070666790008545, 16.304606199264526, 17.52499485015869, 18.773349046707153, 20.01979374885559, 21.284457683563232, 22.517051935195923, 23.746067762374878, 24.980838537216187, 26.226881742477417, 27.46270990371704, 28.68305540084839, 29.913272857666016, 31.131758451461792, 32.338858127593994, 33.56454825401306, 34.78807353973389, 36.05752730369568, 37.27520298957825, 38.495476961135864, 39.72148132324219, 40.946192026138306, 42.160508155822754, 43.40287137031555, 44.63945531845093, 45.88064169883728, 47.004159927368164, 48.13654351234436, 49.269004344940186, 50.3972110748291, 51.52252125740051, 52.65007925033569, 53.780038595199585, 54.91192626953125, 56.04110932350159, 57.18587827682495, 58.29908752441406, 59.41614365577698, 60.57200288772583, 61.70848226547241, 62.84025573730469, 63.960304975509644, 65.08407688140869, 66.23271346092224, 67.36996221542358, 68.55504369735718, 69.68377256393433, 70.8391637802124, 71.9811532497406, 73.12643957138062, 74.2643563747406, 75.40356731414795, 76.54989099502563, 77.6950454711914, 78.82900595664978, 79.96993494033813, 81.09987282752991, 82.24355387687683, 83.37523102760315, 84.49746966362, 85.64893841743469, 86.77363228797913, 87.90593028068542, 89.04173755645752, 90.16136884689331, 91.31025075912476, 92.45025062561035, 93.56249833106995, 94.66087293624878, 95.75552797317505, 96.87522768974304, 97.964040517807, 99.07938385009766, 100.1702344417572, 101.27688646316528, 102.38362765312195, 103.4758780002594, 104.57848596572876, 105.68905806541443, 106.79792785644531, 107.8970901966095, 108.98546719551086, 110.09178400039673, 111.17665505409241, 112.29250836372375, 113.39044785499573, 114.47147870063782, 115.5410795211792, 116.60304880142212, 118.68343353271484]
[24.325, 28.945, 32.715, 34.01, 34.9575, 36.1275, 36.755, 37.76, 38.145, 38.755, 40.165, 40.4075, 40.78, 40.745, 41.2875, 41.755, 41.5225, 41.6875, 41.3975, 41.6175, 41.63, 41.73, 41.9525, 42.0, 41.9375, 42.275, 42.345, 42.135, 42.4675, 42.5625, 42.325, 42.4375, 42.4425, 41.79, 42.1875, 42.27, 42.3925, 42.075, 41.9675, 42.385, 42.0175, 42.1125, 42.2225, 42.0075, 42.115, 42.13, 41.8425, 42.135, 42.25, 42.475, 42.365, 42.41, 42.3575, 42.04, 42.0425, 42.085, 42.085, 42.0, 42.1625, 42.26, 42.125, 41.9175, 42.215, 42.23, 42.2275, 41.88, 42.3575, 42.38, 42.25, 42.3225, 42.0575, 42.0775, 42.3675, 42.21, 42.1475, 42.27, 41.94, 42.33, 42.295, 42.64, 42.4175, 42.585, 42.28, 42.445, 42.585, 42.2325, 42.5075, 42.5075, 42.575, 42.5225, 42.41, 42.415, 42.2675, 42.5275, 42.325, 42.0425, 42.2275, 42.33, 42.515, 42.43, 42.46]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.465, Test loss: 1.897, Test accuracy: 32.00
Round   1, Train loss: 1.264, Test loss: 1.871, Test accuracy: 32.04
Round   2, Train loss: 1.140, Test loss: 1.849, Test accuracy: 33.28
Round   3, Train loss: 1.122, Test loss: 1.841, Test accuracy: 34.20
Round   4, Train loss: 1.050, Test loss: 2.003, Test accuracy: 27.62
Round   5, Train loss: 1.016, Test loss: 2.045, Test accuracy: 25.70
Round   6, Train loss: 0.945, Test loss: 2.080, Test accuracy: 24.37
Round   7, Train loss: 0.891, Test loss: 2.064, Test accuracy: 25.30
Round   8, Train loss: 0.826, Test loss: 2.056, Test accuracy: 25.48
Round   9, Train loss: 0.838, Test loss: 2.107, Test accuracy: 23.35
Round  10, Train loss: 0.786, Test loss: 2.102, Test accuracy: 22.82
Round  11, Train loss: 0.764, Test loss: 2.091, Test accuracy: 24.38
Round  12, Train loss: 0.714, Test loss: 2.082, Test accuracy: 24.85
Round  13, Train loss: 0.698, Test loss: 2.080, Test accuracy: 25.23
Round  14, Train loss: 0.697, Test loss: 2.078, Test accuracy: 24.76
Round  15, Train loss: 0.644, Test loss: 2.067, Test accuracy: 25.41
Round  16, Train loss: 0.632, Test loss: 2.061, Test accuracy: 25.78
Round  17, Train loss: 0.563, Test loss: 2.056, Test accuracy: 26.04
Round  18, Train loss: 0.619, Test loss: 2.049, Test accuracy: 26.21
Round  19, Train loss: 0.565, Test loss: 2.044, Test accuracy: 27.26
Round  20, Train loss: 0.493, Test loss: 2.031, Test accuracy: 27.93
Round  21, Train loss: 0.557, Test loss: 2.029, Test accuracy: 26.93
Round  22, Train loss: 0.497, Test loss: 2.017, Test accuracy: 28.21
Round  23, Train loss: 0.435, Test loss: 2.006, Test accuracy: 29.61
Round  24, Train loss: 0.457, Test loss: 1.998, Test accuracy: 29.02
Round  25, Train loss: 0.432, Test loss: 1.994, Test accuracy: 30.05
Round  26, Train loss: 0.431, Test loss: 1.987, Test accuracy: 30.61
Round  27, Train loss: 0.423, Test loss: 1.982, Test accuracy: 30.66
Round  28, Train loss: 0.393, Test loss: 1.970, Test accuracy: 31.12
Round  29, Train loss: 0.473, Test loss: 1.966, Test accuracy: 31.38
Round  30, Train loss: 0.365, Test loss: 1.954, Test accuracy: 32.50
Round  31, Train loss: 0.411, Test loss: 1.949, Test accuracy: 32.42
Round  32, Train loss: 0.393, Test loss: 1.942, Test accuracy: 32.68
Round  33, Train loss: 0.339, Test loss: 1.936, Test accuracy: 33.67
Round  34, Train loss: 0.345, Test loss: 1.933, Test accuracy: 33.67
Round  35, Train loss: 0.317, Test loss: 1.929, Test accuracy: 34.00
Round  36, Train loss: 0.337, Test loss: 1.923, Test accuracy: 34.29
Round  37, Train loss: 0.328, Test loss: 1.915, Test accuracy: 34.99
Round  38, Train loss: 0.327, Test loss: 1.900, Test accuracy: 35.86
Round  39, Train loss: 0.331, Test loss: 1.898, Test accuracy: 36.34
Round  40, Train loss: 0.269, Test loss: 1.884, Test accuracy: 37.00
Round  41, Train loss: 0.283, Test loss: 1.869, Test accuracy: 37.66
Round  42, Train loss: 0.318, Test loss: 1.879, Test accuracy: 36.73
Round  43, Train loss: 0.270, Test loss: 1.871, Test accuracy: 37.46
Round  44, Train loss: 0.266, Test loss: 1.868, Test accuracy: 37.16
Round  45, Train loss: 0.278, Test loss: 1.864, Test accuracy: 36.66
Round  46, Train loss: 0.257, Test loss: 1.867, Test accuracy: 36.37
Round  47, Train loss: 0.239, Test loss: 1.851, Test accuracy: 37.69
Round  48, Train loss: 0.250, Test loss: 1.850, Test accuracy: 37.38
Round  49, Train loss: 0.294, Test loss: 1.852, Test accuracy: 37.49
Round  50, Train loss: 0.253, Test loss: 1.839, Test accuracy: 38.04
Round  51, Train loss: 0.238, Test loss: 1.831, Test accuracy: 38.38
Round  52, Train loss: 0.245, Test loss: 1.822, Test accuracy: 38.25
Round  53, Train loss: 0.236, Test loss: 1.812, Test accuracy: 38.88
Round  54, Train loss: 0.229, Test loss: 1.809, Test accuracy: 39.17
Round  55, Train loss: 0.227, Test loss: 1.805, Test accuracy: 39.54
Round  56, Train loss: 0.223, Test loss: 1.797, Test accuracy: 39.75
Round  57, Train loss: 0.226, Test loss: 1.796, Test accuracy: 40.10
Round  58, Train loss: 0.203, Test loss: 1.784, Test accuracy: 40.55
Round  59, Train loss: 0.190, Test loss: 1.780, Test accuracy: 40.73
Round  60, Train loss: 0.186, Test loss: 1.781, Test accuracy: 40.62
Round  61, Train loss: 0.207, Test loss: 1.785, Test accuracy: 40.23
Round  62, Train loss: 0.200, Test loss: 1.780, Test accuracy: 40.50
Round  63, Train loss: 0.203, Test loss: 1.776, Test accuracy: 40.53
Round  64, Train loss: 0.187, Test loss: 1.770, Test accuracy: 40.58
Round  65, Train loss: 0.180, Test loss: 1.770, Test accuracy: 40.17
Round  66, Train loss: 0.193, Test loss: 1.764, Test accuracy: 40.96
Round  67, Train loss: 0.179, Test loss: 1.766, Test accuracy: 40.52
Round  68, Train loss: 0.179, Test loss: 1.770, Test accuracy: 40.37
Round  69, Train loss: 0.181, Test loss: 1.747, Test accuracy: 41.87
Round  70, Train loss: 0.180, Test loss: 1.757, Test accuracy: 40.76
Round  71, Train loss: 0.178, Test loss: 1.756, Test accuracy: 40.98
Round  72, Train loss: 0.170, Test loss: 1.750, Test accuracy: 41.42
Round  73, Train loss: 0.170, Test loss: 1.738, Test accuracy: 42.14
Round  74, Train loss: 0.180, Test loss: 1.738, Test accuracy: 41.83
Round  75, Train loss: 0.171, Test loss: 1.731, Test accuracy: 42.41
Round  76, Train loss: 0.162, Test loss: 1.729, Test accuracy: 42.18
Round  77, Train loss: 0.170, Test loss: 1.732, Test accuracy: 41.72
Round  78, Train loss: 0.154, Test loss: 1.727, Test accuracy: 41.90
Round  79, Train loss: 0.158, Test loss: 1.726, Test accuracy: 41.75
Round  80, Train loss: 0.147, Test loss: 1.719, Test accuracy: 42.06
Round  81, Train loss: 0.158, Test loss: 1.725, Test accuracy: 41.66
Round  82, Train loss: 0.145, Test loss: 1.719, Test accuracy: 41.98
Round  83, Train loss: 0.165, Test loss: 1.719, Test accuracy: 41.85
Round  84, Train loss: 0.148, Test loss: 1.716, Test accuracy: 41.95
Round  85, Train loss: 0.147, Test loss: 1.715, Test accuracy: 42.05
Round  86, Train loss: 0.168, Test loss: 1.715, Test accuracy: 42.43
Round  87, Train loss: 0.154, Test loss: 1.710, Test accuracy: 42.83
Round  88, Train loss: 0.155, Test loss: 1.706, Test accuracy: 42.45
Round  89, Train loss: 0.151, Test loss: 1.701, Test accuracy: 42.80
Round  90, Train loss: 0.144, Test loss: 1.695, Test accuracy: 42.88
Round  91, Train loss: 0.148, Test loss: 1.702, Test accuracy: 42.67
Round  92, Train loss: 0.141, Test loss: 1.699, Test accuracy: 42.39
Round  93, Train loss: 0.142, Test loss: 1.698, Test accuracy: 42.57
Round  94, Train loss: 0.140, Test loss: 1.697, Test accuracy: 42.58
Round  95, Train loss: 0.137, Test loss: 1.694, Test accuracy: 42.67
Round  96, Train loss: 0.134, Test loss: 1.685, Test accuracy: 43.21
Round  97, Train loss: 0.137, Test loss: 1.689, Test accuracy: 42.92
Round  98, Train loss: 0.136, Test loss: 1.693, Test accuracy: 42.82
Round  99, Train loss: 0.137, Test loss: 1.688, Test accuracy: 43.12
Final Round, Train loss: 0.138, Test loss: 1.674, Test accuracy: 43.36
Average accuracy final 10 rounds: 42.78325
5843.911920309067
[]
[32.0, 32.0375, 33.28, 34.2, 27.6175, 25.7025, 24.37, 25.295, 25.475, 23.3475, 22.82, 24.3825, 24.8475, 25.2275, 24.7625, 25.4125, 25.7775, 26.0375, 26.21, 27.2575, 27.93, 26.9275, 28.2125, 29.6075, 29.015, 30.0525, 30.61, 30.66, 31.1175, 31.3825, 32.5025, 32.425, 32.6825, 33.6675, 33.6725, 33.9975, 34.2875, 34.995, 35.8625, 36.3425, 37.0025, 37.665, 36.7325, 37.4625, 37.165, 36.6575, 36.365, 37.6925, 37.385, 37.4875, 38.0425, 38.38, 38.2475, 38.875, 39.1725, 39.5375, 39.7475, 40.1, 40.545, 40.735, 40.615, 40.225, 40.4975, 40.5325, 40.575, 40.175, 40.96, 40.52, 40.365, 41.8675, 40.7575, 40.975, 41.4175, 42.1425, 41.83, 42.4125, 42.1775, 41.715, 41.9025, 41.7475, 42.06, 41.6575, 41.9825, 41.8525, 41.955, 42.05, 42.4325, 42.8325, 42.455, 42.8, 42.885, 42.6725, 42.3925, 42.5675, 42.5775, 42.6675, 43.21, 42.925, 42.82, 43.115, 43.36]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.025, Test loss: 2.146, Test accuracy: 17.05
Round   0: Global train loss: 2.025, Global test loss: 2.297, Global test accuracy: 10.00
Round   1, Train loss: 1.788, Test loss: 2.049, Test accuracy: 21.32
Round   1: Global train loss: 1.788, Global test loss: 2.292, Global test accuracy: 10.00
Round   2, Train loss: 1.538, Test loss: 1.906, Test accuracy: 28.50
Round   2: Global train loss: 1.538, Global test loss: 2.274, Global test accuracy: 12.25
Round   3, Train loss: 1.602, Test loss: 1.888, Test accuracy: 30.77
Round   3: Global train loss: 1.602, Global test loss: 2.260, Global test accuracy: 18.17
Round   4, Train loss: 1.425, Test loss: 1.805, Test accuracy: 34.66
Round   4: Global train loss: 1.425, Global test loss: 2.242, Global test accuracy: 24.12
Round   5, Train loss: 0.951, Test loss: 1.770, Test accuracy: 36.82
Round   5: Global train loss: 0.951, Global test loss: 2.234, Global test accuracy: 28.50
Round   6, Train loss: 1.140, Test loss: 1.746, Test accuracy: 37.33
Round   6: Global train loss: 1.140, Global test loss: 2.225, Global test accuracy: 29.69
Round   7, Train loss: 0.766, Test loss: 1.669, Test accuracy: 40.21
Round   7: Global train loss: 0.766, Global test loss: 2.203, Global test accuracy: 33.16
Round   8, Train loss: 0.686, Test loss: 1.638, Test accuracy: 41.80
Round   8: Global train loss: 0.686, Global test loss: 2.191, Global test accuracy: 33.31
Round   9, Train loss: 0.496, Test loss: 1.624, Test accuracy: 42.43
Round   9: Global train loss: 0.496, Global test loss: 2.175, Global test accuracy: 34.44
Round  10, Train loss: 0.654, Test loss: 1.613, Test accuracy: 42.72
Round  10: Global train loss: 0.654, Global test loss: 2.169, Global test accuracy: 34.45
Round  11, Train loss: 0.316, Test loss: 1.590, Test accuracy: 43.47
Round  11: Global train loss: 0.316, Global test loss: 2.164, Global test accuracy: 33.20
Round  12, Train loss: 0.123, Test loss: 1.569, Test accuracy: 43.55
Round  12: Global train loss: 0.123, Global test loss: 2.153, Global test accuracy: 31.35
Round  13, Train loss: -0.312, Test loss: 1.574, Test accuracy: 43.62
Round  13: Global train loss: -0.312, Global test loss: 2.134, Global test accuracy: 30.95
Round  14, Train loss: -0.163, Test loss: 1.560, Test accuracy: 44.40
Round  14: Global train loss: -0.163, Global test loss: 2.127, Global test accuracy: 29.83
Round  15, Train loss: -0.076, Test loss: 1.563, Test accuracy: 44.20
Round  15: Global train loss: -0.076, Global test loss: 2.115, Global test accuracy: 28.97
Round  16, Train loss: -1.178, Test loss: 1.525, Test accuracy: 45.53
Round  16: Global train loss: -1.178, Global test loss: 2.088, Global test accuracy: 30.73
Round  17, Train loss: -1.026, Test loss: 1.505, Test accuracy: 46.29
Round  17: Global train loss: -1.026, Global test loss: 2.069, Global test accuracy: 32.27
Round  18, Train loss: -0.867, Test loss: 1.495, Test accuracy: 46.40
Round  18: Global train loss: -0.867, Global test loss: 2.051, Global test accuracy: 32.62
Round  19, Train loss: -1.072, Test loss: 1.494, Test accuracy: 46.80
Round  19: Global train loss: -1.072, Global test loss: 2.042, Global test accuracy: 32.71
Round  20, Train loss: -2.075, Test loss: 1.485, Test accuracy: 47.30
Round  20: Global train loss: -2.075, Global test loss: 2.020, Global test accuracy: 34.36
Round  21, Train loss: -1.813, Test loss: 1.476, Test accuracy: 47.68
Round  21: Global train loss: -1.813, Global test loss: 2.011, Global test accuracy: 34.34
Round  22, Train loss: -1.350, Test loss: 1.463, Test accuracy: 48.44
Round  22: Global train loss: -1.350, Global test loss: 1.996, Global test accuracy: 35.38
Round  23, Train loss: -2.076, Test loss: 1.467, Test accuracy: 48.46
Round  23: Global train loss: -2.076, Global test loss: 1.970, Global test accuracy: 36.92
Round  24, Train loss: -1.881, Test loss: 1.470, Test accuracy: 48.77
Round  24: Global train loss: -1.881, Global test loss: 1.957, Global test accuracy: 36.89
Round  25, Train loss: -2.888, Test loss: 1.450, Test accuracy: 49.55
Round  25: Global train loss: -2.888, Global test loss: 1.926, Global test accuracy: 38.30
Round  26, Train loss: -1.666, Test loss: 1.476, Test accuracy: 48.83
Round  26: Global train loss: -1.666, Global test loss: 1.924, Global test accuracy: 37.71
Round  27, Train loss: -1.208, Test loss: 1.469, Test accuracy: 49.37
Round  27: Global train loss: -1.208, Global test loss: 1.914, Global test accuracy: 38.81
Round  28, Train loss: -1.770, Test loss: 1.477, Test accuracy: 48.81
Round  28: Global train loss: -1.770, Global test loss: 1.901, Global test accuracy: 39.42
Round  29, Train loss: -2.551, Test loss: 1.473, Test accuracy: 49.16
Round  29: Global train loss: -2.551, Global test loss: 1.880, Global test accuracy: 40.33
Round  30, Train loss: -2.831, Test loss: 1.470, Test accuracy: 49.84
Round  30: Global train loss: -2.831, Global test loss: 1.857, Global test accuracy: 41.83
Round  31, Train loss: -3.572, Test loss: 1.483, Test accuracy: 49.26
Round  31: Global train loss: -3.572, Global test loss: 1.854, Global test accuracy: 41.45
Round  32, Train loss: -2.462, Test loss: 1.459, Test accuracy: 50.06
Round  32: Global train loss: -2.462, Global test loss: 1.838, Global test accuracy: 42.06
Round  33, Train loss: -3.091, Test loss: 1.449, Test accuracy: 50.30
Round  33: Global train loss: -3.091, Global test loss: 1.831, Global test accuracy: 41.95
Round  34, Train loss: -3.511, Test loss: 1.441, Test accuracy: 50.99
Round  34: Global train loss: -3.511, Global test loss: 1.813, Global test accuracy: 42.74
Round  35, Train loss: -2.957, Test loss: 1.443, Test accuracy: 51.01
Round  35: Global train loss: -2.957, Global test loss: 1.800, Global test accuracy: 43.37
Round  36, Train loss: -3.474, Test loss: 1.468, Test accuracy: 51.50
Round  36: Global train loss: -3.474, Global test loss: 1.769, Global test accuracy: 44.69
Round  37, Train loss: -3.083, Test loss: 1.467, Test accuracy: 51.44
Round  37: Global train loss: -3.083, Global test loss: 1.754, Global test accuracy: 45.41
Round  38, Train loss: -3.189, Test loss: 1.441, Test accuracy: 51.27
Round  38: Global train loss: -3.189, Global test loss: 1.738, Global test accuracy: 46.18
Round  39, Train loss: -2.539, Test loss: 1.440, Test accuracy: 51.73
Round  39: Global train loss: -2.539, Global test loss: 1.717, Global test accuracy: 47.14
Round  40, Train loss: -3.882, Test loss: 1.427, Test accuracy: 51.99
Round  40: Global train loss: -3.882, Global test loss: 1.699, Global test accuracy: 47.66
Round  41, Train loss: -3.491, Test loss: 1.418, Test accuracy: 52.47
Round  41: Global train loss: -3.491, Global test loss: 1.676, Global test accuracy: 48.44
Round  42, Train loss: -4.028, Test loss: 1.428, Test accuracy: 52.24
Round  42: Global train loss: -4.028, Global test loss: 1.661, Global test accuracy: 48.46
Round  43, Train loss: -3.828, Test loss: 1.437, Test accuracy: 51.62
Round  43: Global train loss: -3.828, Global test loss: 1.644, Global test accuracy: 48.83
Round  44, Train loss: -3.831, Test loss: 1.439, Test accuracy: 51.89
Round  44: Global train loss: -3.831, Global test loss: 1.626, Global test accuracy: 49.36
Round  45, Train loss: -3.387, Test loss: 1.437, Test accuracy: 52.20
Round  45: Global train loss: -3.387, Global test loss: 1.603, Global test accuracy: 49.95
Round  46, Train loss: -4.491, Test loss: 1.441, Test accuracy: 52.75
Round  46: Global train loss: -4.491, Global test loss: 1.583, Global test accuracy: 51.20
Round  47, Train loss: -4.246, Test loss: 1.445, Test accuracy: 53.03
Round  47: Global train loss: -4.246, Global test loss: 1.559, Global test accuracy: 51.84
Round  48, Train loss: -4.069, Test loss: 1.436, Test accuracy: 53.35
Round  48: Global train loss: -4.069, Global test loss: 1.541, Global test accuracy: 52.58
Round  49, Train loss: -4.406, Test loss: 1.447, Test accuracy: 53.30
Round  49: Global train loss: -4.406, Global test loss: 1.524, Global test accuracy: 52.89
Round  50, Train loss: -3.944, Test loss: 1.425, Test accuracy: 53.03
Round  50: Global train loss: -3.944, Global test loss: 1.519, Global test accuracy: 52.95
Round  51, Train loss: -4.450, Test loss: 1.429, Test accuracy: 53.39
Round  51: Global train loss: -4.450, Global test loss: 1.500, Global test accuracy: 53.61
Round  52, Train loss: -4.068, Test loss: 1.420, Test accuracy: 53.45
Round  52: Global train loss: -4.068, Global test loss: 1.492, Global test accuracy: 53.82
Round  53, Train loss: -4.621, Test loss: 1.421, Test accuracy: 53.38
Round  53: Global train loss: -4.621, Global test loss: 1.480, Global test accuracy: 54.41
Round  54, Train loss: -4.443, Test loss: 1.416, Test accuracy: 53.70
Round  54: Global train loss: -4.443, Global test loss: 1.466, Global test accuracy: 54.74
Round  55, Train loss: -5.098, Test loss: 1.439, Test accuracy: 53.38
Round  55: Global train loss: -5.098, Global test loss: 1.446, Global test accuracy: 55.09
Round  56, Train loss: -4.305, Test loss: 1.441, Test accuracy: 53.64
Round  56: Global train loss: -4.305, Global test loss: 1.431, Global test accuracy: 55.55
Round  57, Train loss: -4.839, Test loss: 1.438, Test accuracy: 53.77
Round  57: Global train loss: -4.839, Global test loss: 1.418, Global test accuracy: 56.21
Round  58, Train loss: -4.564, Test loss: 1.427, Test accuracy: 54.02
Round  58: Global train loss: -4.564, Global test loss: 1.404, Global test accuracy: 56.25
Round  59, Train loss: -5.260, Test loss: 1.428, Test accuracy: 54.09
Round  59: Global train loss: -5.260, Global test loss: 1.391, Global test accuracy: 56.59
Round  60, Train loss: -4.835, Test loss: 1.462, Test accuracy: 54.07
Round  60: Global train loss: -4.835, Global test loss: 1.370, Global test accuracy: 57.30
Round  61, Train loss: -4.731, Test loss: 1.445, Test accuracy: 54.17
Round  61: Global train loss: -4.731, Global test loss: 1.353, Global test accuracy: 57.68
Round  62, Train loss: -4.549, Test loss: 1.414, Test accuracy: 54.45
Round  62: Global train loss: -4.549, Global test loss: 1.349, Global test accuracy: 57.83
Round  63, Train loss: -5.018, Test loss: 1.417, Test accuracy: 54.24
Round  63: Global train loss: -5.018, Global test loss: 1.338, Global test accuracy: 57.83
Round  64, Train loss: -4.238, Test loss: 1.416, Test accuracy: 54.24
Round  64: Global train loss: -4.238, Global test loss: 1.325, Global test accuracy: 58.41
Round  65, Train loss: -5.445, Test loss: 1.439, Test accuracy: 54.19
Round  65: Global train loss: -5.445, Global test loss: 1.311, Global test accuracy: 58.62
Round  66, Train loss: -4.947, Test loss: 1.418, Test accuracy: 54.43
Round  66: Global train loss: -4.947, Global test loss: 1.298, Global test accuracy: 59.03
Round  67, Train loss: -4.606, Test loss: 1.422, Test accuracy: 54.63
Round  67: Global train loss: -4.606, Global test loss: 1.284, Global test accuracy: 59.33
Round  68, Train loss: -4.559, Test loss: 1.418, Test accuracy: 54.55
Round  68: Global train loss: -4.559, Global test loss: 1.281, Global test accuracy: 59.20
Round  69, Train loss: -4.951, Test loss: 1.417, Test accuracy: 54.84
Round  69: Global train loss: -4.951, Global test loss: 1.269, Global test accuracy: 59.75
Round  70, Train loss: -5.072, Test loss: 1.401, Test accuracy: 55.06
Round  70: Global train loss: -5.072, Global test loss: 1.260, Global test accuracy: 60.02
Round  71, Train loss: -5.115, Test loss: 1.424, Test accuracy: 54.72
Round  71: Global train loss: -5.115, Global test loss: 1.250, Global test accuracy: 60.30
Round  72, Train loss: -4.757, Test loss: 1.422, Test accuracy: 55.16
Round  72: Global train loss: -4.757, Global test loss: 1.236, Global test accuracy: 60.63
Round  73, Train loss: -4.607, Test loss: 1.413, Test accuracy: 55.02
Round  73: Global train loss: -4.607, Global test loss: 1.229, Global test accuracy: 60.72
Round  74, Train loss: -4.746, Test loss: 1.392, Test accuracy: 55.69
Round  74: Global train loss: -4.746, Global test loss: 1.220, Global test accuracy: 60.92
Round  75, Train loss: -5.401, Test loss: 1.419, Test accuracy: 55.55
Round  75: Global train loss: -5.401, Global test loss: 1.208, Global test accuracy: 61.32
Round  76, Train loss: -4.760, Test loss: 1.443, Test accuracy: 55.38
Round  76: Global train loss: -4.760, Global test loss: 1.198, Global test accuracy: 61.51
Round  77, Train loss: -4.725, Test loss: 1.442, Test accuracy: 55.34
Round  77: Global train loss: -4.725, Global test loss: 1.189, Global test accuracy: 61.69
Round  78, Train loss: -4.997, Test loss: 1.435, Test accuracy: 55.48
Round  78: Global train loss: -4.997, Global test loss: 1.176, Global test accuracy: 62.03
Round  79, Train loss: -4.528, Test loss: 1.423, Test accuracy: 55.50
Round  79: Global train loss: -4.528, Global test loss: 1.168, Global test accuracy: 62.34
Round  80, Train loss: -4.575, Test loss: 1.427, Test accuracy: 55.34
Round  80: Global train loss: -4.575, Global test loss: 1.165, Global test accuracy: 62.43
Round  81, Train loss: -4.812, Test loss: 1.403, Test accuracy: 55.88
Round  81: Global train loss: -4.812, Global test loss: 1.157, Global test accuracy: 62.46
Round  82, Train loss: -4.591, Test loss: 1.394, Test accuracy: 56.10
Round  82: Global train loss: -4.591, Global test loss: 1.151, Global test accuracy: 62.70
Round  83, Train loss: -4.776, Test loss: 1.401, Test accuracy: 56.20
Round  83: Global train loss: -4.776, Global test loss: 1.143, Global test accuracy: 63.07
Round  84, Train loss: -4.848, Test loss: 1.413, Test accuracy: 56.19
Round  84: Global train loss: -4.848, Global test loss: 1.137, Global test accuracy: 63.07
Round  85, Train loss: -4.993, Test loss: 1.410, Test accuracy: 56.17
Round  85: Global train loss: -4.993, Global test loss: 1.133, Global test accuracy: 63.23
Round  86, Train loss: -4.686, Test loss: 1.416, Test accuracy: 56.33
Round  86: Global train loss: -4.686, Global test loss: 1.124, Global test accuracy: 63.40
Round  87, Train loss: -5.018, Test loss: 1.420, Test accuracy: 56.44
Round  87: Global train loss: -5.018, Global test loss: 1.115, Global test accuracy: 63.56
Round  88, Train loss: -4.887, Test loss: 1.413, Test accuracy: 56.48
Round  88: Global train loss: -4.887, Global test loss: 1.109, Global test accuracy: 63.77
Round  89, Train loss: -4.312, Test loss: 1.398, Test accuracy: 56.23
Round  89: Global train loss: -4.312, Global test loss: 1.103, Global test accuracy: 63.83
Round  90, Train loss: -4.794, Test loss: 1.411, Test accuracy: 56.44
Round  90: Global train loss: -4.794, Global test loss: 1.095, Global test accuracy: 64.23
Round  91, Train loss: -4.884, Test loss: 1.408, Test accuracy: 56.51
Round  91: Global train loss: -4.884, Global test loss: 1.088, Global test accuracy: 64.54
Round  92, Train loss: -4.944, Test loss: 1.411, Test accuracy: 56.27
Round  92: Global train loss: -4.944, Global test loss: 1.082, Global test accuracy: 64.66
Round  93, Train loss: -4.389, Test loss: 1.403, Test accuracy: 56.56
Round  93: Global train loss: -4.389, Global test loss: 1.077, Global test accuracy: 64.86
Round  94, Train loss: -4.762, Test loss: 1.400, Test accuracy: 56.65
Round  94: Global train loss: -4.762, Global test loss: 1.073, Global test accuracy: 65.00
Round  95, Train loss: -4.613, Test loss: 1.404, Test accuracy: 56.78
Round  95: Global train loss: -4.613, Global test loss: 1.066, Global test accuracy: 65.07
Round  96, Train loss: -5.034, Test loss: 1.433, Test accuracy: 56.60
Round  96: Global train loss: -5.034, Global test loss: 1.060, Global test accuracy: 65.30
Round  97, Train loss: -4.701, Test loss: 1.418, Test accuracy: 56.81
Round  97: Global train loss: -4.701, Global test loss: 1.055, Global test accuracy: 65.56
Round  98, Train loss: -4.748, Test loss: 1.415, Test accuracy: 56.79
Round  98: Global train loss: -4.748, Global test loss: 1.050, Global test accuracy: 65.77
Round  99, Train loss: -4.363, Test loss: 1.406, Test accuracy: 56.62
Round  99: Global train loss: -4.363, Global test loss: 1.048, Global test accuracy: 65.74
Final Round: Train loss: 0.995, Test loss: 1.168, Test accuracy: 60.18
Final Round: Global train loss: 0.995, Global test loss: 1.027, Global test accuracy: 66.22
Average accuracy final 10 rounds: 56.602999999999994
Average global accuracy final 10 rounds: 65.07150000000001
5411.448037862778
[]
[17.05, 21.3225, 28.4975, 30.7725, 34.655, 36.8175, 37.3325, 40.2125, 41.7975, 42.43, 42.7225, 43.47, 43.5475, 43.6225, 44.4, 44.1975, 45.53, 46.2875, 46.4025, 46.7975, 47.305, 47.6775, 48.4375, 48.4575, 48.7675, 49.55, 48.8325, 49.3675, 48.81, 49.155, 49.84, 49.255, 50.0625, 50.3, 50.995, 51.0125, 51.4975, 51.435, 51.2675, 51.73, 51.995, 52.465, 52.2425, 51.6175, 51.8875, 52.1975, 52.7525, 53.03, 53.35, 53.3025, 53.0325, 53.3925, 53.45, 53.375, 53.6975, 53.375, 53.6375, 53.7725, 54.025, 54.09, 54.0675, 54.175, 54.4475, 54.245, 54.2425, 54.1875, 54.43, 54.635, 54.5475, 54.8425, 55.06, 54.7225, 55.165, 55.0225, 55.69, 55.55, 55.3825, 55.3375, 55.4825, 55.5, 55.34, 55.875, 56.105, 56.205, 56.1875, 56.1675, 56.3325, 56.44, 56.475, 56.235, 56.435, 56.505, 56.2725, 56.56, 56.6475, 56.7825, 56.6, 56.815, 56.7875, 56.625, 60.18]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.088, Test loss: 2.216, Test accuracy: 23.38
Round   1, Train loss: 0.884, Test loss: 2.408, Test accuracy: 25.89
Round   2, Train loss: 0.869, Test loss: 2.185, Test accuracy: 26.57
Round   3, Train loss: 0.789, Test loss: 2.363, Test accuracy: 33.92
Round   4, Train loss: 0.835, Test loss: 1.872, Test accuracy: 31.65
Round   5, Train loss: 0.684, Test loss: 1.890, Test accuracy: 41.06
Round   6, Train loss: 0.758, Test loss: 2.008, Test accuracy: 33.48
Round   7, Train loss: 0.648, Test loss: 1.761, Test accuracy: 39.68
Round   8, Train loss: 0.706, Test loss: 1.687, Test accuracy: 38.82
Round   9, Train loss: 0.634, Test loss: 1.630, Test accuracy: 45.73
Round  10, Train loss: 0.604, Test loss: 1.676, Test accuracy: 40.52
Round  11, Train loss: 0.590, Test loss: 1.965, Test accuracy: 39.03
Round  12, Train loss: 0.544, Test loss: 1.862, Test accuracy: 44.88
Round  13, Train loss: 0.525, Test loss: 1.828, Test accuracy: 41.26
Round  14, Train loss: 0.588, Test loss: 1.552, Test accuracy: 44.88
Round  15, Train loss: 0.507, Test loss: 1.605, Test accuracy: 45.37
Round  16, Train loss: 0.537, Test loss: 1.724, Test accuracy: 41.93
Round  17, Train loss: 0.502, Test loss: 1.498, Test accuracy: 49.54
Round  18, Train loss: 0.566, Test loss: 1.543, Test accuracy: 49.46
Round  19, Train loss: 0.464, Test loss: 1.394, Test accuracy: 51.74
Round  20, Train loss: 0.475, Test loss: 1.444, Test accuracy: 51.56
Round  21, Train loss: 0.440, Test loss: 1.843, Test accuracy: 43.55
Round  22, Train loss: 0.457, Test loss: 1.681, Test accuracy: 48.25
Round  23, Train loss: 0.436, Test loss: 1.591, Test accuracy: 50.19
Round  24, Train loss: 0.456, Test loss: 1.653, Test accuracy: 45.35
Round  25, Train loss: 0.413, Test loss: 1.455, Test accuracy: 52.73
Round  26, Train loss: 0.479, Test loss: 1.339, Test accuracy: 54.67
Round  27, Train loss: 0.406, Test loss: 1.392, Test accuracy: 51.12
Round  28, Train loss: 0.418, Test loss: 1.470, Test accuracy: 50.32
Round  29, Train loss: 0.477, Test loss: 1.383, Test accuracy: 51.42
Round  30, Train loss: 0.480, Test loss: 1.627, Test accuracy: 44.85
Round  31, Train loss: 0.436, Test loss: 1.343, Test accuracy: 52.92
Round  32, Train loss: 0.381, Test loss: 1.654, Test accuracy: 47.09
Round  33, Train loss: 0.368, Test loss: 1.445, Test accuracy: 52.52
Round  34, Train loss: 0.358, Test loss: 1.462, Test accuracy: 52.52
Round  35, Train loss: 0.414, Test loss: 1.379, Test accuracy: 53.48
Round  36, Train loss: 0.351, Test loss: 1.296, Test accuracy: 57.20
Round  37, Train loss: 0.325, Test loss: 1.247, Test accuracy: 58.96
Round  38, Train loss: 0.326, Test loss: 1.732, Test accuracy: 48.99
Round  39, Train loss: 0.327, Test loss: 1.303, Test accuracy: 55.08
Round  40, Train loss: 0.415, Test loss: 1.299, Test accuracy: 54.72
Round  41, Train loss: 0.293, Test loss: 1.319, Test accuracy: 57.10
Round  42, Train loss: 0.316, Test loss: 1.723, Test accuracy: 47.12
Round  43, Train loss: 0.285, Test loss: 2.024, Test accuracy: 45.38
Round  44, Train loss: 0.404, Test loss: 1.405, Test accuracy: 54.32
Round  45, Train loss: 0.323, Test loss: 1.339, Test accuracy: 54.91
Round  46, Train loss: 0.313, Test loss: 1.369, Test accuracy: 55.21
Round  47, Train loss: 0.305, Test loss: 1.566, Test accuracy: 53.03
Round  48, Train loss: 0.321, Test loss: 1.613, Test accuracy: 50.23
Round  49, Train loss: 0.346, Test loss: 1.207, Test accuracy: 59.83
Round  50, Train loss: 0.303, Test loss: 1.264, Test accuracy: 59.35
Round  51, Train loss: 0.271, Test loss: 1.326, Test accuracy: 57.81
Round  52, Train loss: 0.254, Test loss: 1.610, Test accuracy: 52.31
Round  53, Train loss: 0.292, Test loss: 1.192, Test accuracy: 59.81
Round  54, Train loss: 0.280, Test loss: 1.259, Test accuracy: 58.64
Round  55, Train loss: 0.337, Test loss: 1.229, Test accuracy: 58.70
Round  56, Train loss: 0.347, Test loss: 1.304, Test accuracy: 57.70
Round  57, Train loss: 0.232, Test loss: 1.433, Test accuracy: 53.66
Round  58, Train loss: 0.230, Test loss: 1.331, Test accuracy: 58.91
Round  59, Train loss: 0.234, Test loss: 1.404, Test accuracy: 55.38
Round  60, Train loss: 0.276, Test loss: 1.435, Test accuracy: 56.42
Round  61, Train loss: 0.330, Test loss: 1.189, Test accuracy: 60.83
Round  62, Train loss: 0.350, Test loss: 1.260, Test accuracy: 59.35
Round  63, Train loss: 0.268, Test loss: 1.163, Test accuracy: 62.21
Round  64, Train loss: 0.254, Test loss: 1.653, Test accuracy: 50.55
Round  65, Train loss: 0.244, Test loss: 1.225, Test accuracy: 60.94
Round  66, Train loss: 0.193, Test loss: 1.316, Test accuracy: 58.40
Round  67, Train loss: 0.237, Test loss: 1.310, Test accuracy: 59.20
Round  68, Train loss: 0.255, Test loss: 1.446, Test accuracy: 55.52
Round  69, Train loss: 0.185, Test loss: 1.244, Test accuracy: 62.62
Round  70, Train loss: 0.211, Test loss: 1.306, Test accuracy: 60.19
Round  71, Train loss: 0.226, Test loss: 1.487, Test accuracy: 56.31
Round  72, Train loss: 0.243, Test loss: 1.107, Test accuracy: 64.16
Round  73, Train loss: 0.169, Test loss: 1.449, Test accuracy: 55.90
Round  74, Train loss: 0.257, Test loss: 1.729, Test accuracy: 48.39
Round  75, Train loss: 0.166, Test loss: 1.307, Test accuracy: 60.85
Round  76, Train loss: 0.214, Test loss: 1.180, Test accuracy: 62.92
Round  77, Train loss: 0.234, Test loss: 1.206, Test accuracy: 61.41
Round  78, Train loss: 0.230, Test loss: 1.242, Test accuracy: 59.79
Round  79, Train loss: 0.234, Test loss: 1.121, Test accuracy: 62.02
Round  80, Train loss: 0.199, Test loss: 1.540, Test accuracy: 57.56
Round  81, Train loss: 0.188, Test loss: 1.225, Test accuracy: 63.00
Round  82, Train loss: 0.227, Test loss: 1.445, Test accuracy: 55.80
Round  83, Train loss: 0.222, Test loss: 1.164, Test accuracy: 63.58
Round  84, Train loss: 0.265, Test loss: 1.293, Test accuracy: 59.50
Round  85, Train loss: 0.232, Test loss: 1.474, Test accuracy: 58.03
Round  86, Train loss: 0.196, Test loss: 2.143, Test accuracy: 49.40
Round  87, Train loss: 0.164, Test loss: 1.556, Test accuracy: 57.86
Round  88, Train loss: 0.172, Test loss: 1.740, Test accuracy: 52.31
Round  89, Train loss: 0.145, Test loss: 1.461, Test accuracy: 57.43
Round  90, Train loss: 0.133, Test loss: 1.645, Test accuracy: 55.10
Round  91, Train loss: 0.173, Test loss: 1.475, Test accuracy: 56.92
Round  92, Train loss: 0.235, Test loss: 1.189, Test accuracy: 62.70
Round  93, Train loss: 0.170, Test loss: 1.433, Test accuracy: 57.52
Round  94, Train loss: 0.197, Test loss: 1.306, Test accuracy: 61.18
Round  95, Train loss: 0.167, Test loss: 1.732, Test accuracy: 53.31
Round  96, Train loss: 0.181, Test loss: 1.297, Test accuracy: 59.78
Round  97, Train loss: 0.186, Test loss: 1.486, Test accuracy: 58.51
Round  98, Train loss: 0.166, Test loss: 1.503, Test accuracy: 58.33
Round  99, Train loss: 0.120, Test loss: 1.267, Test accuracy: 62.30
Final Round, Train loss: 0.174, Test loss: 1.096, Test accuracy: 65.40
Average accuracy final 10 rounds: 58.56583333333333
1970.5994024276733
[3.237898111343384, 6.322779178619385, 9.394627094268799, 12.463778734207153, 15.529375553131104, 18.591131687164307, 21.66206645965576, 24.721341609954834, 27.78069543838501, 30.869059085845947, 33.94297218322754, 37.011239767074585, 40.067187547683716, 43.13286566734314, 46.19393968582153, 49.25152897834778, 52.31842541694641, 55.38799738883972, 58.02090549468994, 60.64394187927246, 63.2801718711853, 65.92124342918396, 68.54544377326965, 71.18310952186584, 73.82010817527771, 76.46545672416687, 79.10984778404236, 81.74920392036438, 84.39291143417358, 87.02691864967346, 89.66084718704224, 92.29667496681213, 94.92887091636658, 97.57165193557739, 100.21277713775635, 102.84804081916809, 105.48151111602783, 108.11516165733337, 110.73845505714417, 113.36517453193665, 116.00393176078796, 118.6355414390564, 121.28108096122742, 123.91079759597778, 126.53450894355774, 129.16313791275024, 131.78895330429077, 134.44495844841003, 137.09722518920898, 139.7570400238037, 142.41302847862244, 145.06943225860596, 147.70894479751587, 150.35935831069946, 153.02450275421143, 155.68814826011658, 158.36225962638855, 161.01214241981506, 163.65178298950195, 166.29489374160767, 168.94061017036438, 171.59770393371582, 174.26114010810852, 176.90961837768555, 179.56319379806519, 182.221533536911, 184.88359713554382, 187.5280704498291, 190.17599892616272, 192.83107042312622, 195.4891619682312, 198.1585123538971, 200.83024072647095, 203.4781527519226, 206.12536120414734, 208.76777291297913, 211.41915273666382, 214.06986594200134, 216.72737979888916, 219.37416195869446, 222.0343177318573, 224.69012713432312, 227.33973097801208, 229.98330950737, 232.62518215179443, 235.28256058692932, 237.9442434310913, 240.60519409179688, 243.24671983718872, 245.90172600746155, 248.54567193984985, 251.19942831993103, 253.84967923164368, 256.4841260910034, 259.1226649284363, 261.7557489871979, 264.4123423099518, 267.05998492240906, 269.7079908847809, 272.3449475765228, 274.982919216156]
[23.383333333333333, 25.891666666666666, 26.566666666666666, 33.925, 31.65, 41.05833333333333, 33.475, 39.68333333333333, 38.81666666666667, 45.725, 40.516666666666666, 39.03333333333333, 44.875, 41.25833333333333, 44.875, 45.36666666666667, 41.93333333333333, 49.541666666666664, 49.458333333333336, 51.74166666666667, 51.55833333333333, 43.55, 48.25, 50.19166666666667, 45.35, 52.725, 54.666666666666664, 51.125, 50.31666666666667, 51.416666666666664, 44.85, 52.925, 47.09166666666667, 52.525, 52.525, 53.475, 57.2, 58.958333333333336, 48.99166666666667, 55.083333333333336, 54.71666666666667, 57.1, 47.11666666666667, 45.38333333333333, 54.31666666666667, 54.90833333333333, 55.208333333333336, 53.03333333333333, 50.225, 59.825, 59.35, 57.80833333333333, 52.30833333333333, 59.80833333333333, 58.641666666666666, 58.7, 57.7, 53.65833333333333, 58.90833333333333, 55.38333333333333, 56.416666666666664, 60.825, 59.35, 62.208333333333336, 50.55, 60.94166666666667, 58.4, 59.2, 55.516666666666666, 62.61666666666667, 60.19166666666667, 56.30833333333333, 64.15833333333333, 55.9, 48.391666666666666, 60.85, 62.925, 61.40833333333333, 59.791666666666664, 62.025, 57.55833333333333, 63.0, 55.8, 63.583333333333336, 59.5, 58.03333333333333, 49.4, 57.858333333333334, 52.30833333333333, 57.43333333333333, 55.1, 56.925, 62.7, 57.525, 61.18333333333333, 53.30833333333333, 59.78333333333333, 58.50833333333333, 58.325, 62.3, 65.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.294, Test loss: 2.324, Test accuracy: 10.23 

Round   0, Global train loss: 2.294, Global test loss: 2.327, Global test accuracy: 9.04 

Round   1, Train loss: 2.306, Test loss: 2.323, Test accuracy: 10.96 

Round   1, Global train loss: 2.306, Global test loss: 2.326, Global test accuracy: 9.14 

Round   2, Train loss: 2.305, Test loss: 2.321, Test accuracy: 10.79 

Round   2, Global train loss: 2.305, Global test loss: 2.323, Global test accuracy: 8.61 

Round   3, Train loss: 2.320, Test loss: 2.323, Test accuracy: 9.07 

Round   3, Global train loss: 2.320, Global test loss: 2.322, Global test accuracy: 7.64 

Round   4, Train loss: 2.354, Test loss: 2.328, Test accuracy: 8.32 

Round   4, Global train loss: 2.354, Global test loss: 2.323, Global test accuracy: 6.87 

Round   5, Train loss: nan, Test loss: nan, Test accuracy: 7.67 

Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round   6, Train loss: nan, Test loss: nan, Test accuracy: 5.80 

Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round   7, Train loss: nan, Test loss: nan, Test accuracy: 3.54 

Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round   8, Train loss: nan, Test loss: nan, Test accuracy: 3.49 

Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round   9, Train loss: nan, Test loss: nan, Test accuracy: 3.49 

Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  10, Train loss: nan, Test loss: nan, Test accuracy: 3.49 

Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  11, Train loss: nan, Test loss: nan, Test accuracy: 3.49 

Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  12, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  13, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  14, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  15, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  16, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  17, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  18, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  19, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  20, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  21, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  22, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  23, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  24, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  25, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  26, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  27, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  28, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  29, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  30, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  31, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  32, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  33, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  34, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  35, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  36, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  37, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  38, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  39, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  40, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  41, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  42, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  43, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  44, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  45, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  46, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  47, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  48, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  49, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  50, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  51, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  52, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  53, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  54, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  55, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  56, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  57, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  58, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  59, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  60, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  61, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  62, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  63, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  64, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  65, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  66, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  67, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  68, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  69, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  70, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  71, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  72, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  73, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  74, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  75, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  76, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  77, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  78, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  79, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  80, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  81, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  82, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  83, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  84, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  85, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  86, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  87, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  88, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  89, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  90, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  91, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  92, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  93, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  94, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  95, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  96, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  97, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  98, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round  99, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 100, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 101, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 102, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 103, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 104, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 105, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 106, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 107, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 108, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 109, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 110, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 111, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 112, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 113, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 114, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 115, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 116, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 117, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 118, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 119, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 120, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 121, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 122, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 123, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 124, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 125, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 126, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 127, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 128, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 129, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 130, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 131, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 132, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 133, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 134, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 135, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 136, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 137, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 138, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 139, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 140, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 141, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 142, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 143, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 144, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 145, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 146, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 147, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 148, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 149, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 150, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 151, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 152, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 153, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 154, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 155, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 156, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 157, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 158, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 159, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 160, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 161, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 162, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 163, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 164, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 165, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 166, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 167, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 168, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 169, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 170, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 171, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 172, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 173, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 174, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 175, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 176, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 177, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 178, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 179, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 180, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 181, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 182, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 183, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 184, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 185, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 186, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 187, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 188, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 189, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 190, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 191, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 192, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 193, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 194, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 195, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 196, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 197, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 198, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 199, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 200, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 201, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 202, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 203, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 204, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 205, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 206, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 207, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 208, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 209, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 210, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 211, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 212, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 213, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 214, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 215, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 216, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 217, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 218, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 219, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 220, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 221, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 222, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 223, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 224, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 225, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 226, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 227, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 228, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 229, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 230, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 231, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 232, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 233, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 234, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 235, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 236, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 237, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 238, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 239, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 240, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 241, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 242, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 243, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 244, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 245, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 246, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 247, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 248, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 249, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 250, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 251, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 252, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 253, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 254, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 255, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 256, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 257, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 258, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 259, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 260, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 261, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 262, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 263, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 264, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 265, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 266, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 267, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 268, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 269, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 270, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 271, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 272, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 273, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 274, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 275, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 276, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 277, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 278, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 279, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 280, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 281, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 282, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 283, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 284, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 285, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 286, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 287, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 288, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 289, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 290, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 291, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 292, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 293, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 294, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 295, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 296, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 297, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 298, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Round 299, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 3.33 

Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 3.33 

Average accuracy final 10 rounds: 3.3333333333333344 

Average global accuracy final 10 rounds: 3.3333333333333344 

3838.8152356147766
[1.6168363094329834, 2.9492688179016113, 4.072025775909424, 5.203515291213989, 6.327677965164185, 7.457795858383179, 8.580948114395142, 9.705827951431274, 10.832269191741943, 11.954602241516113, 13.080999851226807, 14.207819700241089, 15.336125373840332, 16.465739250183105, 17.59164524078369, 18.71533966064453, 19.843163013458252, 20.9693341255188, 22.096580266952515, 23.224745988845825, 24.349565505981445, 25.47494864463806, 26.599740028381348, 27.72328209877014, 28.849428415298462, 29.975255012512207, 31.099658489227295, 32.2171733379364, 33.33925533294678, 34.464547634124756, 35.587002754211426, 36.71432185173035, 37.838764905929565, 38.95777463912964, 40.08316373825073, 41.20945620536804, 42.33407187461853, 43.452670097351074, 44.576205253601074, 45.69886136054993, 46.82471561431885, 47.94148635864258, 49.064228773117065, 50.18653082847595, 51.30723810195923, 52.42964029312134, 53.55107760429382, 54.67532181739807, 55.79955554008484, 56.922340869903564, 58.046491622924805, 59.171812295913696, 60.29901170730591, 61.420833349227905, 62.542391538619995, 63.665557622909546, 64.79063081741333, 65.91511917114258, 67.03623175621033, 68.15640330314636, 69.27726721763611, 70.397709608078, 71.52072882652283, 72.64232778549194, 73.76618814468384, 74.88384127616882, 76.00266695022583, 77.12068581581116, 78.24018955230713, 79.36104202270508, 80.4877667427063, 81.61672139167786, 82.745441198349, 83.86989426612854, 84.99311995506287, 86.12037658691406, 87.24256014823914, 88.371009349823, 89.49244952201843, 90.62148332595825, 91.74538969993591, 92.87241005897522, 93.99342012405396, 95.11268854141235, 96.23379731178284, 97.36846113204956, 98.49109649658203, 99.61873698234558, 100.74009084701538, 101.86732530593872, 102.99137568473816, 104.11606335639954, 105.23576307296753, 106.3664288520813, 107.48886775970459, 108.61449003219604, 109.74980330467224, 110.87840533256531, 112.00286483764648, 113.12869024276733, 114.25208258628845, 115.3808262348175, 116.50666046142578, 117.63704419136047, 118.7602789402008, 119.88487482070923, 121.00858116149902, 122.1355631351471, 123.2593457698822, 124.37959790229797, 125.49931454658508, 126.62024784088135, 127.74481344223022, 128.87081623077393, 129.99427819252014, 131.11666870117188, 132.23661518096924, 133.35819697380066, 134.47661876678467, 135.5950105190277, 136.72030687332153, 137.8396179676056, 138.96651577949524, 140.08848333358765, 141.2132284641266, 142.3340446949005, 143.45693516731262, 144.57787108421326, 145.70482993125916, 146.8287582397461, 147.96833205223083, 149.0935606956482, 150.21395611763, 151.33275413513184, 152.4515564441681, 153.57195162773132, 154.69696950912476, 155.82071566581726, 156.95646691322327, 158.08070015907288, 159.20581579208374, 160.33601427078247, 161.46545219421387, 162.59441494941711, 163.72319340705872, 164.8529806137085, 165.97879457473755, 167.10831999778748, 168.2345325946808, 169.36311388015747, 170.48828864097595, 171.61650776863098, 172.7405595779419, 173.86505341529846, 174.9883909225464, 176.11358618736267, 177.2370216846466, 178.36831951141357, 179.49554085731506, 180.6244180202484, 181.75429034233093, 182.88005590438843, 184.0060329437256, 185.13160967826843, 186.2590126991272, 187.53771424293518, 188.8249707221985, 190.10799956321716, 191.39795780181885, 192.58599066734314, 193.71595215797424, 194.83990454673767, 195.97035026550293, 197.0989284515381, 198.22127056121826, 199.3474190235138, 200.47048258781433, 201.5944013595581, 202.7220287322998, 203.85147619247437, 204.98424077033997, 206.1082890033722, 207.23232078552246, 208.35790419578552, 209.48031044006348, 210.60621404647827, 211.730801820755, 212.857754945755, 213.98527264595032, 215.1086916923523, 216.23572278022766, 217.36440753936768, 218.49184107780457, 219.62029790878296, 220.74477910995483, 221.87180924415588, 223.00158190727234, 224.13342666625977, 225.25460624694824, 226.37699007987976, 227.49956822395325, 228.62314319610596, 229.75406765937805, 230.88671803474426, 232.0184166431427, 233.14427065849304, 234.2700924873352, 235.39440202713013, 236.5307424068451, 237.65815424919128, 238.79164838790894, 239.91853880882263, 241.04162693023682, 242.1695592403412, 243.29516696929932, 244.42573237419128, 245.5575978755951, 246.68413496017456, 247.80796265602112, 248.93433666229248, 250.05762696266174, 251.1972050666809, 252.3228030204773, 253.44755673408508, 254.57091999053955, 255.69416427612305, 256.8222029209137, 257.9547049999237, 259.07928824424744, 260.2045633792877, 261.320307970047, 262.4427709579468, 263.5627484321594, 264.686509847641, 265.802579164505, 266.9255471229553, 268.0450096130371, 269.16810393333435, 270.2943344116211, 271.4227271080017, 272.5503029823303, 273.6738750934601, 274.8054895401001, 275.93604731559753, 277.06033658981323, 278.18774580955505, 279.3159964084625, 280.43967294692993, 281.5591633319855, 282.68061661720276, 283.80716919898987, 284.929580450058, 286.05667090415955, 287.18785524368286, 288.321186542511, 289.45362734794617, 290.5837199687958, 291.7115480899811, 292.8444604873657, 293.97027945518494, 295.0928602218628, 296.2152020931244, 297.34890389442444, 298.4795923233032, 299.60927867889404, 300.7354636192322, 301.8599557876587, 302.98891615867615, 304.11994457244873, 305.25064063072205, 306.37543749809265, 307.5007629394531, 308.63308811187744, 309.7586352825165, 310.88454604148865, 312.0149157047272, 313.14097690582275, 314.26413679122925, 315.3916070461273, 316.51750898361206, 317.6455616950989, 318.7723743915558, 319.9035577774048, 321.029949426651, 322.1526880264282, 323.2793107032776, 324.4038007259369, 325.5283076763153, 326.65181398391724, 327.77393674850464, 328.8970220088959, 330.0218732357025, 331.15007734298706, 332.27883887290955, 333.4030463695526, 334.5296001434326, 335.6503121852875, 336.7800831794739, 337.9091258049011, 339.0332009792328, 341.27761340141296]
[10.233333333333333, 10.958333333333334, 10.791666666666666, 9.075, 8.325, 7.666666666666667, 5.8, 3.5416666666666665, 3.4916666666666667, 3.4916666666666667, 3.4916666666666667, 3.4916666666666667, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.532, Test loss: 2.067, Test accuracy: 33.26
Round   1, Train loss: 0.944, Test loss: 1.709, Test accuracy: 43.24
Round   2, Train loss: 0.844, Test loss: 1.351, Test accuracy: 48.85
Round   3, Train loss: 0.747, Test loss: 1.450, Test accuracy: 50.71
Round   4, Train loss: 0.722, Test loss: 1.005, Test accuracy: 58.75
Round   5, Train loss: 0.674, Test loss: 1.112, Test accuracy: 60.89
Round   6, Train loss: 0.693, Test loss: 0.902, Test accuracy: 66.19
Round   7, Train loss: 0.704, Test loss: 0.838, Test accuracy: 65.84
Round   8, Train loss: 0.610, Test loss: 0.732, Test accuracy: 69.93
Round   9, Train loss: 0.567, Test loss: 0.604, Test accuracy: 73.90
Round  10, Train loss: 0.616, Test loss: 0.602, Test accuracy: 74.61
Round  11, Train loss: 0.631, Test loss: 0.593, Test accuracy: 74.62
Round  12, Train loss: 0.642, Test loss: 0.588, Test accuracy: 74.53
Round  13, Train loss: 0.537, Test loss: 0.572, Test accuracy: 76.00
Round  14, Train loss: 0.520, Test loss: 0.559, Test accuracy: 76.40
Round  15, Train loss: 0.534, Test loss: 0.550, Test accuracy: 76.97
Round  16, Train loss: 0.550, Test loss: 0.539, Test accuracy: 77.38
Round  17, Train loss: 0.471, Test loss: 0.549, Test accuracy: 77.14
Round  18, Train loss: 0.509, Test loss: 0.542, Test accuracy: 77.83
Round  19, Train loss: 0.546, Test loss: 0.524, Test accuracy: 78.76
Round  20, Train loss: 0.421, Test loss: 0.513, Test accuracy: 79.33
Round  21, Train loss: 0.483, Test loss: 0.501, Test accuracy: 79.55
Round  22, Train loss: 0.458, Test loss: 0.489, Test accuracy: 79.67
Round  23, Train loss: 0.544, Test loss: 0.472, Test accuracy: 80.36
Round  24, Train loss: 0.495, Test loss: 0.475, Test accuracy: 80.47
Round  25, Train loss: 0.452, Test loss: 0.469, Test accuracy: 80.68
Round  26, Train loss: 0.455, Test loss: 0.465, Test accuracy: 80.92
Round  27, Train loss: 0.453, Test loss: 0.460, Test accuracy: 80.99
Round  28, Train loss: 0.404, Test loss: 0.450, Test accuracy: 81.02
Round  29, Train loss: 0.437, Test loss: 0.454, Test accuracy: 80.79
Round  30, Train loss: 0.439, Test loss: 0.452, Test accuracy: 81.26
Round  31, Train loss: 0.405, Test loss: 0.443, Test accuracy: 81.46
Round  32, Train loss: 0.421, Test loss: 0.449, Test accuracy: 81.25
Round  33, Train loss: 0.393, Test loss: 0.448, Test accuracy: 81.25
Round  34, Train loss: 0.518, Test loss: 0.443, Test accuracy: 81.65
Round  35, Train loss: 0.468, Test loss: 0.427, Test accuracy: 82.52
Round  36, Train loss: 0.430, Test loss: 0.427, Test accuracy: 82.79
Round  37, Train loss: 0.406, Test loss: 0.421, Test accuracy: 82.44
Round  38, Train loss: 0.374, Test loss: 0.423, Test accuracy: 82.45
Round  39, Train loss: 0.436, Test loss: 0.420, Test accuracy: 82.38
Round  40, Train loss: 0.468, Test loss: 0.417, Test accuracy: 82.88
Round  41, Train loss: 0.362, Test loss: 0.412, Test accuracy: 83.03
Round  42, Train loss: 0.415, Test loss: 0.418, Test accuracy: 83.37
Round  43, Train loss: 0.431, Test loss: 0.418, Test accuracy: 83.19
Round  44, Train loss: 0.402, Test loss: 0.405, Test accuracy: 83.46
Round  45, Train loss: 0.397, Test loss: 0.406, Test accuracy: 83.26
Round  46, Train loss: 0.391, Test loss: 0.400, Test accuracy: 83.69
Round  47, Train loss: 0.407, Test loss: 0.392, Test accuracy: 84.12
Round  48, Train loss: 0.374, Test loss: 0.388, Test accuracy: 84.05
Round  49, Train loss: 0.426, Test loss: 0.402, Test accuracy: 83.62
Round  50, Train loss: 0.316, Test loss: 0.385, Test accuracy: 84.32
Round  51, Train loss: 0.319, Test loss: 0.386, Test accuracy: 84.44
Round  52, Train loss: 0.367, Test loss: 0.379, Test accuracy: 84.66
Round  53, Train loss: 0.345, Test loss: 0.378, Test accuracy: 84.70
Round  54, Train loss: 0.323, Test loss: 0.380, Test accuracy: 84.91
Round  55, Train loss: 0.383, Test loss: 0.379, Test accuracy: 84.63
Round  56, Train loss: 0.385, Test loss: 0.386, Test accuracy: 84.31
Round  57, Train loss: 0.353, Test loss: 0.385, Test accuracy: 84.64
Round  58, Train loss: 0.379, Test loss: 0.377, Test accuracy: 84.58
Round  59, Train loss: 0.304, Test loss: 0.374, Test accuracy: 84.84
Round  60, Train loss: 0.351, Test loss: 0.366, Test accuracy: 85.38
Round  61, Train loss: 0.322, Test loss: 0.368, Test accuracy: 85.05
Round  62, Train loss: 0.377, Test loss: 0.365, Test accuracy: 85.14
Round  63, Train loss: 0.307, Test loss: 0.373, Test accuracy: 84.97
Round  64, Train loss: 0.340, Test loss: 0.371, Test accuracy: 84.98
Round  65, Train loss: 0.301, Test loss: 0.362, Test accuracy: 85.43
Round  66, Train loss: 0.315, Test loss: 0.361, Test accuracy: 85.19
Round  67, Train loss: 0.304, Test loss: 0.363, Test accuracy: 85.29
Round  68, Train loss: 0.265, Test loss: 0.368, Test accuracy: 85.04
Round  69, Train loss: 0.333, Test loss: 0.367, Test accuracy: 85.06
Round  70, Train loss: 0.362, Test loss: 0.369, Test accuracy: 84.99
Round  71, Train loss: 0.316, Test loss: 0.360, Test accuracy: 85.35
Round  72, Train loss: 0.287, Test loss: 0.357, Test accuracy: 85.53
Round  73, Train loss: 0.274, Test loss: 0.355, Test accuracy: 85.53
Round  74, Train loss: 0.339, Test loss: 0.357, Test accuracy: 85.38
Round  75, Train loss: 0.296, Test loss: 0.358, Test accuracy: 85.57
Round  76, Train loss: 0.255, Test loss: 0.361, Test accuracy: 85.54
Round  77, Train loss: 0.273, Test loss: 0.356, Test accuracy: 85.62
Round  78, Train loss: 0.286, Test loss: 0.354, Test accuracy: 85.77
Round  79, Train loss: 0.285, Test loss: 0.351, Test accuracy: 85.82
Round  80, Train loss: 0.284, Test loss: 0.351, Test accuracy: 85.87
Round  81, Train loss: 0.285, Test loss: 0.351, Test accuracy: 85.82
Round  82, Train loss: 0.262, Test loss: 0.349, Test accuracy: 86.10
Round  83, Train loss: 0.235, Test loss: 0.350, Test accuracy: 85.98
Round  84, Train loss: 0.288, Test loss: 0.351, Test accuracy: 85.64
Round  85, Train loss: 0.262, Test loss: 0.350, Test accuracy: 85.99
Round  86, Train loss: 0.249, Test loss: 0.353, Test accuracy: 85.90
Round  87, Train loss: 0.252, Test loss: 0.357, Test accuracy: 85.85
Round  88, Train loss: 0.286, Test loss: 0.343, Test accuracy: 86.42
Round  89, Train loss: 0.268, Test loss: 0.343, Test accuracy: 86.46
Round  90, Train loss: 0.261, Test loss: 0.353, Test accuracy: 86.22
Round  91, Train loss: 0.279, Test loss: 0.353, Test accuracy: 85.92
Round  92, Train loss: 0.245, Test loss: 0.347, Test accuracy: 86.40
Round  93, Train loss: 0.206, Test loss: 0.345, Test accuracy: 86.42
Round  94, Train loss: 0.258, Test loss: 0.351, Test accuracy: 86.28
Round  95, Train loss: 0.253, Test loss: 0.349, Test accuracy: 86.47
Round  96, Train loss: 0.244, Test loss: 0.342, Test accuracy: 86.53
Round  97, Train loss: 0.218, Test loss: 0.347, Test accuracy: 86.31
Round  98, Train loss: 0.234, Test loss: 0.347, Test accuracy: 86.21
Round  99, Train loss: 0.235, Test loss: 0.345, Test accuracy: 86.63
Final Round, Train loss: 0.201, Test loss: 0.343, Test accuracy: 86.62
Average accuracy final 10 rounds: 86.33916666666667
1106.9961035251617
[1.7198193073272705, 3.12575626373291, 4.5422539710998535, 5.952512979507446, 7.366869688034058, 8.796674251556396, 10.201322317123413, 11.57959532737732, 12.882514476776123, 14.184435367584229, 15.600645542144775, 17.008535385131836, 18.42441749572754, 19.82294797897339, 21.137337923049927, 22.438223361968994, 23.749141454696655, 25.05479407310486, 26.357739448547363, 27.65862250328064, 28.96673011779785, 30.295989513397217, 31.60599637031555, 32.91802787780762, 34.23213291168213, 35.54646706581116, 36.85242676734924, 38.16383504867554, 39.46921110153198, 40.76849961280823, 42.072869062423706, 43.38647150993347, 44.69757962226868, 46.00562882423401, 47.30581307411194, 48.610321044921875, 49.906742334365845, 51.2157723903656, 52.51462125778198, 53.81424260139465, 55.12135863304138, 56.433308839797974, 57.74591255187988, 59.04473257064819, 60.360066413879395, 61.65768003463745, 62.97554397583008, 64.2966661453247, 65.6120216846466, 66.92159700393677, 68.23910164833069, 69.54779148101807, 70.86205744743347, 72.17975354194641, 73.49812507629395, 74.81203866004944, 76.12120342254639, 77.43461322784424, 78.75861597061157, 80.0647361278534, 81.3782570362091, 82.69712901115417, 84.00587797164917, 85.32106614112854, 86.62277603149414, 87.94294834136963, 89.24928331375122, 90.56410646438599, 91.89079093933105, 93.21029162406921, 94.50945734977722, 95.81274008750916, 97.11203622817993, 98.41765475273132, 99.7357885837555, 101.05553889274597, 102.36411833763123, 103.68864178657532, 105.00695586204529, 106.32290458679199, 107.63054299354553, 108.9544095993042, 110.27349090576172, 111.58700656890869, 112.88819980621338, 114.2102735042572, 115.52352857589722, 116.84196877479553, 118.14604091644287, 119.31788396835327, 120.6240701675415, 121.91981434822083, 123.23006868362427, 124.54036235809326, 125.84341597557068, 127.15754437446594, 128.4589488506317, 129.7897002696991, 131.06745171546936, 132.23567175865173, 134.14263129234314]
[33.25833333333333, 43.24166666666667, 48.85, 50.708333333333336, 58.75, 60.891666666666666, 66.19166666666666, 65.84166666666667, 69.93333333333334, 73.9, 74.60833333333333, 74.625, 74.525, 76.0, 76.4, 76.975, 77.38333333333334, 77.14166666666667, 77.825, 78.75833333333334, 79.33333333333333, 79.55, 79.675, 80.35833333333333, 80.475, 80.68333333333334, 80.91666666666667, 80.99166666666666, 81.01666666666667, 80.79166666666667, 81.25833333333334, 81.45833333333333, 81.25, 81.25, 81.65, 82.51666666666667, 82.79166666666667, 82.44166666666666, 82.45, 82.375, 82.875, 83.025, 83.36666666666666, 83.19166666666666, 83.45833333333333, 83.25833333333334, 83.69166666666666, 84.125, 84.05, 83.61666666666666, 84.31666666666666, 84.44166666666666, 84.65833333333333, 84.7, 84.90833333333333, 84.63333333333334, 84.30833333333334, 84.64166666666667, 84.575, 84.84166666666667, 85.38333333333334, 85.05, 85.14166666666667, 84.96666666666667, 84.98333333333333, 85.43333333333334, 85.19166666666666, 85.29166666666667, 85.04166666666667, 85.05833333333334, 84.99166666666666, 85.35, 85.525, 85.53333333333333, 85.375, 85.56666666666666, 85.54166666666667, 85.61666666666666, 85.76666666666667, 85.81666666666666, 85.86666666666666, 85.81666666666666, 86.1, 85.98333333333333, 85.64166666666667, 85.99166666666666, 85.9, 85.85, 86.425, 86.45833333333333, 86.225, 85.925, 86.4, 86.41666666666667, 86.275, 86.46666666666667, 86.53333333333333, 86.30833333333334, 86.20833333333333, 86.63333333333334, 86.61666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.291, Test loss: 2.192, Test accuracy: 19.71
Round   1, Train loss: 2.149, Test loss: 2.015, Test accuracy: 28.02
Round   2, Train loss: 2.019, Test loss: 1.921, Test accuracy: 32.20
Round   3, Train loss: 1.943, Test loss: 1.821, Test accuracy: 35.48
Round   4, Train loss: 1.847, Test loss: 1.752, Test accuracy: 37.55
Round   5, Train loss: 1.784, Test loss: 1.700, Test accuracy: 39.46
Round   6, Train loss: 1.726, Test loss: 1.638, Test accuracy: 40.48
Round   7, Train loss: 1.685, Test loss: 1.607, Test accuracy: 41.55
Round   8, Train loss: 1.650, Test loss: 1.579, Test accuracy: 42.84
Round   9, Train loss: 1.604, Test loss: 1.551, Test accuracy: 43.62
Round  10, Train loss: 1.602, Test loss: 1.518, Test accuracy: 45.17
Round  11, Train loss: 1.574, Test loss: 1.484, Test accuracy: 46.29
Round  12, Train loss: 1.559, Test loss: 1.461, Test accuracy: 47.32
Round  13, Train loss: 1.508, Test loss: 1.430, Test accuracy: 48.15
Round  14, Train loss: 1.486, Test loss: 1.417, Test accuracy: 49.27
Round  15, Train loss: 1.443, Test loss: 1.399, Test accuracy: 49.69
Round  16, Train loss: 1.439, Test loss: 1.383, Test accuracy: 50.58
Round  17, Train loss: 1.407, Test loss: 1.391, Test accuracy: 50.59
Round  18, Train loss: 1.412, Test loss: 1.373, Test accuracy: 51.26
Round  19, Train loss: 1.393, Test loss: 1.334, Test accuracy: 52.50
Round  20, Train loss: 1.346, Test loss: 1.336, Test accuracy: 53.04
Round  21, Train loss: 1.362, Test loss: 1.309, Test accuracy: 53.68
Round  22, Train loss: 1.330, Test loss: 1.274, Test accuracy: 54.79
Round  23, Train loss: 1.295, Test loss: 1.258, Test accuracy: 55.71
Round  24, Train loss: 1.269, Test loss: 1.263, Test accuracy: 55.46
Round  25, Train loss: 1.271, Test loss: 1.250, Test accuracy: 55.69
Round  26, Train loss: 1.248, Test loss: 1.228, Test accuracy: 56.66
Round  27, Train loss: 1.233, Test loss: 1.201, Test accuracy: 57.72
Round  28, Train loss: 1.252, Test loss: 1.197, Test accuracy: 57.64
Round  29, Train loss: 1.192, Test loss: 1.193, Test accuracy: 57.95
Round  30, Train loss: 1.201, Test loss: 1.166, Test accuracy: 59.00
Round  31, Train loss: 1.170, Test loss: 1.159, Test accuracy: 59.49
Round  32, Train loss: 1.144, Test loss: 1.165, Test accuracy: 59.44
Round  33, Train loss: 1.179, Test loss: 1.149, Test accuracy: 59.97
Round  34, Train loss: 1.145, Test loss: 1.130, Test accuracy: 60.77
Round  35, Train loss: 1.145, Test loss: 1.135, Test accuracy: 60.72
Round  36, Train loss: 1.122, Test loss: 1.110, Test accuracy: 60.90
Round  37, Train loss: 1.092, Test loss: 1.114, Test accuracy: 60.74
Round  38, Train loss: 1.101, Test loss: 1.105, Test accuracy: 61.45
Round  39, Train loss: 1.111, Test loss: 1.090, Test accuracy: 62.20
Round  40, Train loss: 1.039, Test loss: 1.093, Test accuracy: 62.13
Round  41, Train loss: 1.030, Test loss: 1.085, Test accuracy: 62.90
Round  42, Train loss: 1.041, Test loss: 1.077, Test accuracy: 62.86
Round  43, Train loss: 1.009, Test loss: 1.074, Test accuracy: 62.88
Round  44, Train loss: 1.042, Test loss: 1.066, Test accuracy: 63.01
Round  45, Train loss: 0.992, Test loss: 1.069, Test accuracy: 62.83
Round  46, Train loss: 1.028, Test loss: 1.056, Test accuracy: 63.10
Round  47, Train loss: 0.984, Test loss: 1.053, Test accuracy: 63.32
Round  48, Train loss: 1.017, Test loss: 1.050, Test accuracy: 63.74
Round  49, Train loss: 0.958, Test loss: 1.046, Test accuracy: 64.03
Round  50, Train loss: 0.933, Test loss: 1.044, Test accuracy: 63.87
Round  51, Train loss: 0.964, Test loss: 1.038, Test accuracy: 63.92
Round  52, Train loss: 0.960, Test loss: 1.031, Test accuracy: 64.16
Round  53, Train loss: 0.915, Test loss: 1.033, Test accuracy: 64.41
Round  54, Train loss: 0.930, Test loss: 1.023, Test accuracy: 64.83
Round  55, Train loss: 0.909, Test loss: 1.034, Test accuracy: 64.24
Round  56, Train loss: 0.881, Test loss: 1.030, Test accuracy: 64.47
Round  57, Train loss: 0.908, Test loss: 1.018, Test accuracy: 64.97
Round  58, Train loss: 0.879, Test loss: 1.003, Test accuracy: 65.46
Round  59, Train loss: 0.929, Test loss: 0.991, Test accuracy: 66.05
Round  60, Train loss: 0.857, Test loss: 0.995, Test accuracy: 65.99
Round  61, Train loss: 0.876, Test loss: 0.994, Test accuracy: 65.56
Round  62, Train loss: 0.870, Test loss: 0.985, Test accuracy: 66.28
Round  63, Train loss: 0.848, Test loss: 0.982, Test accuracy: 66.06
Round  64, Train loss: 0.830, Test loss: 0.982, Test accuracy: 66.48
Round  65, Train loss: 0.805, Test loss: 0.995, Test accuracy: 65.92
Round  66, Train loss: 0.858, Test loss: 0.993, Test accuracy: 66.10
Round  67, Train loss: 0.852, Test loss: 0.980, Test accuracy: 66.36
Round  68, Train loss: 0.851, Test loss: 0.980, Test accuracy: 66.30
Round  69, Train loss: 0.767, Test loss: 0.979, Test accuracy: 66.32
Round  70, Train loss: 0.793, Test loss: 0.977, Test accuracy: 66.75
Round  71, Train loss: 0.810, Test loss: 0.969, Test accuracy: 66.77
Round  72, Train loss: 0.765, Test loss: 0.975, Test accuracy: 66.58
Round  73, Train loss: 0.772, Test loss: 0.970, Test accuracy: 66.86
Round  74, Train loss: 0.725, Test loss: 0.986, Test accuracy: 66.31
Round  75, Train loss: 0.750, Test loss: 0.968, Test accuracy: 67.20
Round  76, Train loss: 0.765, Test loss: 0.979, Test accuracy: 66.62
Round  77, Train loss: 0.776, Test loss: 0.963, Test accuracy: 67.68
Round  78, Train loss: 0.740, Test loss: 0.953, Test accuracy: 67.86
Round  79, Train loss: 0.716, Test loss: 0.956, Test accuracy: 67.80
Round  80, Train loss: 0.783, Test loss: 0.974, Test accuracy: 67.42
Round  81, Train loss: 0.708, Test loss: 0.963, Test accuracy: 67.50
Round  82, Train loss: 0.719, Test loss: 0.956, Test accuracy: 67.68
Round  83, Train loss: 0.729, Test loss: 0.955, Test accuracy: 67.90
Round  84, Train loss: 0.704, Test loss: 0.954, Test accuracy: 67.86
Round  85, Train loss: 0.707, Test loss: 0.958, Test accuracy: 67.82
Round  86, Train loss: 0.706, Test loss: 0.951, Test accuracy: 67.98
Round  87, Train loss: 0.633, Test loss: 0.962, Test accuracy: 67.61
Round  88, Train loss: 0.771, Test loss: 0.948, Test accuracy: 68.05
Round  89, Train loss: 0.660, Test loss: 0.951, Test accuracy: 68.11
Round  90, Train loss: 0.702, Test loss: 0.964, Test accuracy: 67.97
Round  91, Train loss: 0.715, Test loss: 0.973, Test accuracy: 67.56
Round  92, Train loss: 0.629, Test loss: 0.957, Test accuracy: 68.05
Round  93, Train loss: 0.683, Test loss: 0.951, Test accuracy: 67.99
Round  94, Train loss: 0.662, Test loss: 0.957, Test accuracy: 68.14
Round  95, Train loss: 0.667, Test loss: 0.952, Test accuracy: 68.47
Round  96, Train loss: 0.641, Test loss: 0.954, Test accuracy: 68.39
Round  97, Train loss: 0.642, Test loss: 0.948, Test accuracy: 68.60
Round  98, Train loss: 0.702, Test loss: 0.953, Test accuracy: 68.36
Round  99, Train loss: 0.612, Test loss: 0.953, Test accuracy: 68.14
Final Round, Train loss: 0.557, Test loss: 0.954, Test accuracy: 68.59
Average accuracy final 10 rounds: 68.1675
1757.309448480606
[1.7964239120483398, 3.0357260704040527, 4.247637987136841, 5.4574501514434814, 6.669042587280273, 7.87323522567749, 9.07203984260559, 10.278324604034424, 11.485066652297974, 12.692005395889282, 13.900508403778076, 15.10772180557251, 16.317118883132935, 17.528964281082153, 18.735305070877075, 19.94510245323181, 21.15935230255127, 22.36423659324646, 23.572062015533447, 24.78250288963318, 25.990942239761353, 27.19994616508484, 28.414612770080566, 29.62207269668579, 30.828150987625122, 32.02906107902527, 33.235735177993774, 34.59309911727905, 35.95578145980835, 37.32082200050354, 38.67793536186218, 40.041314125061035, 41.3991060256958, 42.761415004730225, 44.130409955978394, 45.49161696434021, 46.86769080162048, 48.2308464050293, 49.58961033821106, 50.94220209121704, 52.272725105285645, 53.46341609954834, 54.65698575973511, 55.857786417007446, 57.06354284286499, 58.26843023300171, 59.46191763877869, 60.66021394729614, 61.87019944190979, 63.06841063499451, 64.26640462875366, 65.45185899734497, 66.63731908798218, 67.82230162620544, 69.01143145561218, 70.22143769264221, 71.42032814025879, 72.62490034103394, 73.82644724845886, 75.02286124229431, 76.22985506057739, 77.43011355400085, 78.63445448875427, 79.83622646331787, 81.03318047523499, 82.23229193687439, 83.44488549232483, 84.65697264671326, 85.85826587677002, 87.06327795982361, 88.25905847549438, 89.44997429847717, 90.64463353157043, 91.83706164360046, 93.02411079406738, 94.21494650840759, 95.40691351890564, 96.60654187202454, 97.80885314941406, 99.00171709060669, 100.1928403377533, 101.38905501365662, 102.58112168312073, 103.77807903289795, 104.97876310348511, 106.17620515823364, 107.371901512146, 108.57319498062134, 109.76616168022156, 110.96316075325012, 112.16558694839478, 113.35842108726501, 114.54760098457336, 115.74162197113037, 116.93744707107544, 118.13234353065491, 119.32952404022217, 120.5349633693695, 121.73641443252563, 122.94139456748962, 124.90000820159912]
[19.7125, 28.0225, 32.1975, 35.475, 37.555, 39.46, 40.4825, 41.5525, 42.84, 43.615, 45.17, 46.2925, 47.3225, 48.145, 49.275, 49.685, 50.575, 50.5925, 51.2625, 52.5, 53.0425, 53.6825, 54.7875, 55.7075, 55.4575, 55.6875, 56.66, 57.7175, 57.6425, 57.95, 59.0025, 59.4875, 59.4425, 59.9725, 60.775, 60.7225, 60.8975, 60.74, 61.4475, 62.2, 62.1325, 62.9, 62.86, 62.875, 63.0075, 62.83, 63.105, 63.3175, 63.74, 64.025, 63.87, 63.9225, 64.155, 64.405, 64.8325, 64.2375, 64.465, 64.975, 65.46, 66.05, 65.99, 65.555, 66.2825, 66.055, 66.4775, 65.9225, 66.1025, 66.3575, 66.3025, 66.32, 66.745, 66.77, 66.5775, 66.855, 66.315, 67.2025, 66.6175, 67.6775, 67.865, 67.8, 67.425, 67.495, 67.68, 67.9, 67.86, 67.8175, 67.985, 67.61, 68.0525, 68.1125, 67.9675, 67.565, 68.05, 67.9925, 68.1425, 68.4725, 68.39, 68.6, 68.36, 68.135, 68.595]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.137, Test loss: 1.902, Test accuracy: 27.58 

Round   0, Global train loss: 1.137, Global test loss: 2.288, Global test accuracy: 15.07 

Round   1, Train loss: 0.939, Test loss: 1.508, Test accuracy: 43.23 

Round   1, Global train loss: 0.939, Global test loss: 2.140, Global test accuracy: 24.27 

Round   2, Train loss: 0.808, Test loss: 1.408, Test accuracy: 45.08 

Round   2, Global train loss: 0.808, Global test loss: 2.233, Global test accuracy: 21.38 

Round   3, Train loss: 0.766, Test loss: 1.128, Test accuracy: 54.25 

Round   3, Global train loss: 0.766, Global test loss: 2.167, Global test accuracy: 20.35 

Round   4, Train loss: 0.768, Test loss: 0.951, Test accuracy: 59.87 

Round   4, Global train loss: 0.768, Global test loss: 2.017, Global test accuracy: 25.10 

Round   5, Train loss: 0.729, Test loss: 0.852, Test accuracy: 64.38 

Round   5, Global train loss: 0.729, Global test loss: 2.278, Global test accuracy: 18.93 

Round   6, Train loss: 0.647, Test loss: 0.721, Test accuracy: 68.51 

Round   6, Global train loss: 0.647, Global test loss: 2.224, Global test accuracy: 27.32 

Round   7, Train loss: 0.708, Test loss: 0.706, Test accuracy: 69.46 

Round   7, Global train loss: 0.708, Global test loss: 2.152, Global test accuracy: 22.77 

Round   8, Train loss: 0.656, Test loss: 0.707, Test accuracy: 69.37 

Round   8, Global train loss: 0.656, Global test loss: 2.202, Global test accuracy: 21.03 

Round   9, Train loss: 0.540, Test loss: 0.698, Test accuracy: 70.01 

Round   9, Global train loss: 0.540, Global test loss: 2.047, Global test accuracy: 25.89 

Round  10, Train loss: 0.688, Test loss: 0.680, Test accuracy: 70.72 

Round  10, Global train loss: 0.688, Global test loss: 2.274, Global test accuracy: 11.18 

Round  11, Train loss: 0.604, Test loss: 0.666, Test accuracy: 71.57 

Round  11, Global train loss: 0.604, Global test loss: 2.288, Global test accuracy: 21.32 

Round  12, Train loss: 0.504, Test loss: 0.664, Test accuracy: 71.71 

Round  12, Global train loss: 0.504, Global test loss: 2.053, Global test accuracy: 26.79 

Round  13, Train loss: 0.524, Test loss: 0.647, Test accuracy: 73.07 

Round  13, Global train loss: 0.524, Global test loss: 2.083, Global test accuracy: 27.24 

Round  14, Train loss: 0.442, Test loss: 0.655, Test accuracy: 72.78 

Round  14, Global train loss: 0.442, Global test loss: 2.070, Global test accuracy: 23.98 

Round  15, Train loss: 0.549, Test loss: 0.650, Test accuracy: 72.56 

Round  15, Global train loss: 0.549, Global test loss: 2.023, Global test accuracy: 27.61 

Round  16, Train loss: 0.560, Test loss: 0.643, Test accuracy: 73.08 

Round  16, Global train loss: 0.560, Global test loss: 2.336, Global test accuracy: 19.08 

Round  17, Train loss: 0.550, Test loss: 0.634, Test accuracy: 73.40 

Round  17, Global train loss: 0.550, Global test loss: 2.003, Global test accuracy: 31.74 

Round  18, Train loss: 0.513, Test loss: 0.636, Test accuracy: 73.64 

Round  18, Global train loss: 0.513, Global test loss: 2.105, Global test accuracy: 22.33 

Round  19, Train loss: 0.463, Test loss: 0.629, Test accuracy: 74.22 

Round  19, Global train loss: 0.463, Global test loss: 2.310, Global test accuracy: 19.38 

Round  20, Train loss: 0.521, Test loss: 0.626, Test accuracy: 74.30 

Round  20, Global train loss: 0.521, Global test loss: 2.127, Global test accuracy: 20.44 

Round  21, Train loss: 0.553, Test loss: 0.627, Test accuracy: 74.78 

Round  21, Global train loss: 0.553, Global test loss: 2.023, Global test accuracy: 27.22 

Round  22, Train loss: 0.430, Test loss: 0.631, Test accuracy: 74.61 

Round  22, Global train loss: 0.430, Global test loss: 2.034, Global test accuracy: 29.16 

Round  23, Train loss: 0.383, Test loss: 0.618, Test accuracy: 75.20 

Round  23, Global train loss: 0.383, Global test loss: 1.980, Global test accuracy: 31.33 

Round  24, Train loss: 0.340, Test loss: 0.635, Test accuracy: 75.07 

Round  24, Global train loss: 0.340, Global test loss: 1.966, Global test accuracy: 27.14 

Round  25, Train loss: 0.471, Test loss: 0.626, Test accuracy: 75.59 

Round  25, Global train loss: 0.471, Global test loss: 2.049, Global test accuracy: 20.85 

Round  26, Train loss: 0.395, Test loss: 0.624, Test accuracy: 75.75 

Round  26, Global train loss: 0.395, Global test loss: 2.052, Global test accuracy: 23.41 

Round  27, Train loss: 0.343, Test loss: 0.629, Test accuracy: 75.67 

Round  27, Global train loss: 0.343, Global test loss: 2.084, Global test accuracy: 21.07 

Round  28, Train loss: 0.413, Test loss: 0.626, Test accuracy: 75.90 

Round  28, Global train loss: 0.413, Global test loss: 2.100, Global test accuracy: 18.28 

Round  29, Train loss: 0.295, Test loss: 0.647, Test accuracy: 75.52 

Round  29, Global train loss: 0.295, Global test loss: 2.049, Global test accuracy: 19.74 

Round  30, Train loss: 0.327, Test loss: 0.656, Test accuracy: 75.20 

Round  30, Global train loss: 0.327, Global test loss: 2.089, Global test accuracy: 24.34 

Round  31, Train loss: 0.344, Test loss: 0.654, Test accuracy: 75.62 

Round  31, Global train loss: 0.344, Global test loss: 2.035, Global test accuracy: 28.01 

Round  32, Train loss: 0.375, Test loss: 0.680, Test accuracy: 75.22 

Round  32, Global train loss: 0.375, Global test loss: 2.036, Global test accuracy: 22.77 

Round  33, Train loss: 0.395, Test loss: 0.680, Test accuracy: 75.50 

Round  33, Global train loss: 0.395, Global test loss: 2.072, Global test accuracy: 28.07 

Round  34, Train loss: 0.312, Test loss: 0.678, Test accuracy: 75.85 

Round  34, Global train loss: 0.312, Global test loss: 2.090, Global test accuracy: 21.15 

Round  35, Train loss: 0.273, Test loss: 0.652, Test accuracy: 76.17 

Round  35, Global train loss: 0.273, Global test loss: 2.040, Global test accuracy: 26.17 

Round  36, Train loss: 0.350, Test loss: 0.665, Test accuracy: 76.32 

Round  36, Global train loss: 0.350, Global test loss: 2.224, Global test accuracy: 18.35 

Round  37, Train loss: 0.274, Test loss: 0.684, Test accuracy: 76.07 

Round  37, Global train loss: 0.274, Global test loss: 2.012, Global test accuracy: 28.19 

Round  38, Train loss: 0.291, Test loss: 0.708, Test accuracy: 75.59 

Round  38, Global train loss: 0.291, Global test loss: 2.132, Global test accuracy: 22.70 

Round  39, Train loss: 0.359, Test loss: 0.705, Test accuracy: 75.99 

Round  39, Global train loss: 0.359, Global test loss: 2.138, Global test accuracy: 19.62 

Round  40, Train loss: 0.216, Test loss: 0.713, Test accuracy: 75.93 

Round  40, Global train loss: 0.216, Global test loss: 2.069, Global test accuracy: 27.36 

Round  41, Train loss: 0.321, Test loss: 0.734, Test accuracy: 75.92 

Round  41, Global train loss: 0.321, Global test loss: 2.116, Global test accuracy: 27.10 

Round  42, Train loss: 0.243, Test loss: 0.754, Test accuracy: 75.75 

Round  42, Global train loss: 0.243, Global test loss: 2.207, Global test accuracy: 20.75 

Round  43, Train loss: 0.296, Test loss: 0.774, Test accuracy: 75.72 

Round  43, Global train loss: 0.296, Global test loss: 2.075, Global test accuracy: 27.81 

Round  44, Train loss: 0.262, Test loss: 0.765, Test accuracy: 76.42 

Round  44, Global train loss: 0.262, Global test loss: 1.943, Global test accuracy: 32.07 

Round  45, Train loss: 0.271, Test loss: 0.752, Test accuracy: 76.70 

Round  45, Global train loss: 0.271, Global test loss: 2.351, Global test accuracy: 18.01 

Round  46, Train loss: 0.261, Test loss: 0.786, Test accuracy: 76.21 

Round  46, Global train loss: 0.261, Global test loss: 2.156, Global test accuracy: 21.68 

Round  47, Train loss: 0.188, Test loss: 0.810, Test accuracy: 75.94 

Round  47, Global train loss: 0.188, Global test loss: 2.087, Global test accuracy: 19.26 

Round  48, Train loss: 0.158, Test loss: 0.810, Test accuracy: 76.24 

Round  48, Global train loss: 0.158, Global test loss: 1.884, Global test accuracy: 36.88 

Round  49, Train loss: 0.162, Test loss: 0.812, Test accuracy: 76.29 

Round  49, Global train loss: 0.162, Global test loss: 2.374, Global test accuracy: 20.59 

Round  50, Train loss: 0.180, Test loss: 0.799, Test accuracy: 76.16 

Round  50, Global train loss: 0.180, Global test loss: 2.046, Global test accuracy: 28.31 

Round  51, Train loss: 0.197, Test loss: 0.796, Test accuracy: 76.30 

Round  51, Global train loss: 0.197, Global test loss: 2.090, Global test accuracy: 27.18 

Round  52, Train loss: 0.183, Test loss: 0.796, Test accuracy: 76.43 

Round  52, Global train loss: 0.183, Global test loss: 2.110, Global test accuracy: 27.77 

Round  53, Train loss: 0.213, Test loss: 0.789, Test accuracy: 76.89 

Round  53, Global train loss: 0.213, Global test loss: 2.149, Global test accuracy: 22.13 

Round  54, Train loss: 0.195, Test loss: 0.812, Test accuracy: 76.82 

Round  54, Global train loss: 0.195, Global test loss: 2.120, Global test accuracy: 18.41 

Round  55, Train loss: 0.207, Test loss: 0.818, Test accuracy: 76.53 

Round  55, Global train loss: 0.207, Global test loss: 2.170, Global test accuracy: 27.67 

Round  56, Train loss: 0.169, Test loss: 0.824, Test accuracy: 76.53 

Round  56, Global train loss: 0.169, Global test loss: 2.190, Global test accuracy: 24.36 

Round  57, Train loss: 0.169, Test loss: 0.817, Test accuracy: 76.88 

Round  57, Global train loss: 0.169, Global test loss: 2.117, Global test accuracy: 23.49 

Round  58, Train loss: 0.149, Test loss: 0.861, Test accuracy: 76.89 

Round  58, Global train loss: 0.149, Global test loss: 2.256, Global test accuracy: 19.61 

Round  59, Train loss: 0.129, Test loss: 0.865, Test accuracy: 77.10 

Round  59, Global train loss: 0.129, Global test loss: 2.049, Global test accuracy: 24.52 

Round  60, Train loss: 0.175, Test loss: 0.869, Test accuracy: 77.15 

Round  60, Global train loss: 0.175, Global test loss: 2.244, Global test accuracy: 19.71 

Round  61, Train loss: 0.134, Test loss: 0.903, Test accuracy: 76.78 

Round  61, Global train loss: 0.134, Global test loss: 1.986, Global test accuracy: 30.38 

Round  62, Train loss: 0.137, Test loss: 0.895, Test accuracy: 77.12 

Round  62, Global train loss: 0.137, Global test loss: 2.064, Global test accuracy: 25.06 

Round  63, Train loss: 0.157, Test loss: 0.909, Test accuracy: 77.11 

Round  63, Global train loss: 0.157, Global test loss: 1.926, Global test accuracy: 32.24 

Round  64, Train loss: 0.164, Test loss: 0.931, Test accuracy: 76.65 

Round  64, Global train loss: 0.164, Global test loss: 2.085, Global test accuracy: 28.76 

Round  65, Train loss: 0.136, Test loss: 0.921, Test accuracy: 76.58 

Round  65, Global train loss: 0.136, Global test loss: 2.050, Global test accuracy: 28.69 

Round  66, Train loss: 0.098, Test loss: 0.928, Test accuracy: 76.62 

Round  66, Global train loss: 0.098, Global test loss: 2.100, Global test accuracy: 27.34 

Round  67, Train loss: 0.152, Test loss: 0.922, Test accuracy: 77.15 

Round  67, Global train loss: 0.152, Global test loss: 2.103, Global test accuracy: 26.69 

Round  68, Train loss: 0.108, Test loss: 0.933, Test accuracy: 76.88 

Round  68, Global train loss: 0.108, Global test loss: 2.101, Global test accuracy: 28.98 

Round  69, Train loss: 0.125, Test loss: 0.916, Test accuracy: 77.53 

Round  69, Global train loss: 0.125, Global test loss: 2.117, Global test accuracy: 27.82 

Round  70, Train loss: 0.130, Test loss: 0.952, Test accuracy: 77.43 

Round  70, Global train loss: 0.130, Global test loss: 2.095, Global test accuracy: 17.43 

Round  71, Train loss: 0.103, Test loss: 0.941, Test accuracy: 77.42 

Round  71, Global train loss: 0.103, Global test loss: 2.047, Global test accuracy: 26.33 

Round  72, Train loss: 0.092, Test loss: 0.971, Test accuracy: 76.92 

Round  72, Global train loss: 0.092, Global test loss: 2.269, Global test accuracy: 19.23 

Round  73, Train loss: 0.076, Test loss: 0.973, Test accuracy: 77.22 

Round  73, Global train loss: 0.076, Global test loss: 2.335, Global test accuracy: 22.47 

Round  74, Train loss: 0.118, Test loss: 1.000, Test accuracy: 77.40 

Round  74, Global train loss: 0.118, Global test loss: 2.112, Global test accuracy: 24.21 

Round  75, Train loss: 0.098, Test loss: 1.020, Test accuracy: 77.28 

Round  75, Global train loss: 0.098, Global test loss: 2.180, Global test accuracy: 22.54 

Round  76, Train loss: 0.139, Test loss: 1.015, Test accuracy: 77.32 

Round  76, Global train loss: 0.139, Global test loss: 2.197, Global test accuracy: 18.94 

Round  77, Train loss: 0.092, Test loss: 1.020, Test accuracy: 77.17 

Round  77, Global train loss: 0.092, Global test loss: 2.121, Global test accuracy: 21.59 

Round  78, Train loss: 0.102, Test loss: 1.022, Test accuracy: 77.27 

Round  78, Global train loss: 0.102, Global test loss: 2.056, Global test accuracy: 23.23 

Round  79, Train loss: 0.089, Test loss: 1.007, Test accuracy: 77.38 

Round  79, Global train loss: 0.089, Global test loss: 2.062, Global test accuracy: 24.38 

Round  80, Train loss: 0.122, Test loss: 1.012, Test accuracy: 77.43 

Round  80, Global train loss: 0.122, Global test loss: 2.165, Global test accuracy: 21.02 

Round  81, Train loss: 0.074, Test loss: 1.024, Test accuracy: 77.47 

Round  81, Global train loss: 0.074, Global test loss: 2.024, Global test accuracy: 26.46 

Round  82, Train loss: 0.121, Test loss: 1.037, Test accuracy: 77.23 

Round  82, Global train loss: 0.121, Global test loss: 2.113, Global test accuracy: 24.48 

Round  83, Train loss: 0.085, Test loss: 1.041, Test accuracy: 76.80 

Round  83, Global train loss: 0.085, Global test loss: 2.081, Global test accuracy: 27.45 

Round  84, Train loss: 0.091, Test loss: 1.040, Test accuracy: 76.89 

Round  84, Global train loss: 0.091, Global test loss: 2.075, Global test accuracy: 20.07 

Round  85, Train loss: 0.093, Test loss: 1.032, Test accuracy: 76.89 

Round  85, Global train loss: 0.093, Global test loss: 2.153, Global test accuracy: 24.68 

Round  86, Train loss: 0.108, Test loss: 1.047, Test accuracy: 76.83 

Round  86, Global train loss: 0.108, Global test loss: 2.092, Global test accuracy: 28.02 

Round  87, Train loss: 0.073, Test loss: 1.075, Test accuracy: 76.79 

Round  87, Global train loss: 0.073, Global test loss: 2.106, Global test accuracy: 23.39 

Round  88, Train loss: 0.085, Test loss: 1.047, Test accuracy: 77.04 

Round  88, Global train loss: 0.085, Global test loss: 2.036, Global test accuracy: 30.32 

Round  89, Train loss: 0.090, Test loss: 1.095, Test accuracy: 77.03 

Round  89, Global train loss: 0.090, Global test loss: 2.251, Global test accuracy: 12.90 

Round  90, Train loss: 0.096, Test loss: 1.088, Test accuracy: 76.97 

Round  90, Global train loss: 0.096, Global test loss: 2.172, Global test accuracy: 22.82 

Round  91, Train loss: 0.056, Test loss: 1.121, Test accuracy: 76.99 

Round  91, Global train loss: 0.056, Global test loss: 1.929, Global test accuracy: 29.31 

Round  92, Train loss: 0.065, Test loss: 1.142, Test accuracy: 77.28 

Round  92, Global train loss: 0.065, Global test loss: 2.102, Global test accuracy: 27.62 

Round  93, Train loss: 0.081, Test loss: 1.110, Test accuracy: 77.62 

Round  93, Global train loss: 0.081, Global test loss: 2.245, Global test accuracy: 20.11 

Round  94, Train loss: 0.061, Test loss: 1.119, Test accuracy: 77.92 

Round  94, Global train loss: 0.061, Global test loss: 2.166, Global test accuracy: 19.17 

Round  95, Train loss: 0.059, Test loss: 1.103, Test accuracy: 78.09 

Round  95, Global train loss: 0.059, Global test loss: 2.039, Global test accuracy: 29.60 

Round  96, Train loss: 0.071, Test loss: 1.124, Test accuracy: 77.98 

Round  96, Global train loss: 0.071, Global test loss: 2.007, Global test accuracy: 29.02 

Round  97, Train loss: 0.087, Test loss: 1.109, Test accuracy: 78.20 

Round  97, Global train loss: 0.087, Global test loss: 2.189, Global test accuracy: 20.52 

Round  98, Train loss: 0.062, Test loss: 1.100, Test accuracy: 77.92 

Round  98, Global train loss: 0.062, Global test loss: 1.948, Global test accuracy: 28.29 

Round  99, Train loss: 0.054, Test loss: 1.132, Test accuracy: 77.83 

Round  99, Global train loss: 0.054, Global test loss: 2.430, Global test accuracy: 18.22 

Final Round, Train loss: 0.062, Test loss: 1.187, Test accuracy: 77.41 

Final Round, Global train loss: 0.062, Global test loss: 2.430, Global test accuracy: 18.22 

Average accuracy final 10 rounds: 77.68166666666667 

Average global accuracy final 10 rounds: 24.46833333333333 

1250.4327292442322
[1.388566017150879, 2.545135021209717, 3.7106359004974365, 4.868635177612305, 6.036388158798218, 7.19413161277771, 8.347983837127686, 9.511375904083252, 10.670795679092407, 11.83998727798462, 12.996265172958374, 14.15707802772522, 15.315677165985107, 16.477209091186523, 17.636417627334595, 18.79273295402527, 19.947034120559692, 21.11055040359497, 22.26078510284424, 23.258371114730835, 24.25088143348694, 25.245973110198975, 26.241724967956543, 27.236613988876343, 28.23486065864563, 29.232122659683228, 30.228540182113647, 31.222651958465576, 32.21807670593262, 33.214399337768555, 34.211076498031616, 35.20512104034424, 36.20237731933594, 37.19786596298218, 38.19457697868347, 39.189635038375854, 40.18668079376221, 41.18023371696472, 42.178149700164795, 43.175358295440674, 44.16906428337097, 45.169886112213135, 46.16548705101013, 47.16733407974243, 48.16143178939819, 49.16757345199585, 50.16195750236511, 51.16784334182739, 52.16262483596802, 53.166499376297, 54.16365957260132, 55.16429567337036, 56.163127183914185, 57.15978980064392, 58.162551403045654, 59.15887689590454, 60.22161340713501, 61.357605934143066, 62.50197124481201, 63.64288592338562, 64.7765703201294, 65.79021692276001, 66.78897428512573, 67.7877094745636, 68.78290295600891, 69.77940940856934, 70.78338742256165, 71.78565239906311, 72.78444385528564, 73.78754806518555, 74.78809642791748, 75.78653597831726, 76.78824996948242, 77.78846788406372, 78.78760004043579, 79.78503918647766, 80.78634595870972, 81.78802442550659, 82.78540229797363, 83.78565001487732, 84.78430962562561, 85.78152203559875, 86.78140592575073, 87.78249406814575, 88.78183555603027, 89.7834460735321, 90.78840970993042, 91.79189205169678, 92.80397534370422, 93.97268676757812, 95.13924241065979, 96.30532765388489, 97.465336561203, 98.6251585483551, 99.78250908851624, 100.93678426742554, 102.09457349777222, 103.24747085571289, 104.40310645103455, 105.55336213111877, 107.8631067276001]
[27.583333333333332, 43.233333333333334, 45.075, 54.25, 59.86666666666667, 64.375, 68.50833333333334, 69.45833333333333, 69.36666666666666, 70.00833333333334, 70.725, 71.56666666666666, 71.70833333333333, 73.06666666666666, 72.775, 72.55833333333334, 73.075, 73.4, 73.64166666666667, 74.21666666666667, 74.3, 74.78333333333333, 74.60833333333333, 75.2, 75.06666666666666, 75.59166666666667, 75.75, 75.675, 75.9, 75.51666666666667, 75.2, 75.625, 75.21666666666667, 75.5, 75.85, 76.16666666666667, 76.31666666666666, 76.06666666666666, 75.59166666666667, 75.99166666666666, 75.93333333333334, 75.925, 75.75, 75.71666666666667, 76.41666666666667, 76.7, 76.20833333333333, 75.94166666666666, 76.24166666666666, 76.29166666666667, 76.15833333333333, 76.3, 76.43333333333334, 76.89166666666667, 76.81666666666666, 76.525, 76.525, 76.875, 76.89166666666667, 77.1, 77.15, 76.775, 77.11666666666666, 77.10833333333333, 76.65, 76.575, 76.61666666666666, 77.15, 76.875, 77.53333333333333, 77.43333333333334, 77.425, 76.91666666666667, 77.21666666666667, 77.4, 77.275, 77.31666666666666, 77.16666666666667, 77.26666666666667, 77.38333333333334, 77.43333333333334, 77.475, 77.23333333333333, 76.8, 76.89166666666667, 76.89166666666667, 76.83333333333333, 76.79166666666667, 77.04166666666667, 77.03333333333333, 76.975, 76.99166666666666, 77.28333333333333, 77.61666666666666, 77.925, 78.09166666666667, 77.98333333333333, 78.2, 77.91666666666667, 77.83333333333333, 77.40833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.113, Test loss: 1.927, Test accuracy: 25.32 

Round   0, Global train loss: 1.113, Global test loss: 2.276, Global test accuracy: 16.96 

Round   1, Train loss: 0.945, Test loss: 1.641, Test accuracy: 38.23 

Round   1, Global train loss: 0.945, Global test loss: 2.258, Global test accuracy: 22.77 

Round   2, Train loss: 0.867, Test loss: 1.531, Test accuracy: 44.01 

Round   2, Global train loss: 0.867, Global test loss: 2.357, Global test accuracy: 24.95 

Round   3, Train loss: 0.844, Test loss: 0.960, Test accuracy: 58.40 

Round   3, Global train loss: 0.844, Global test loss: 1.830, Global test accuracy: 33.09 

Round   4, Train loss: 0.761, Test loss: 0.866, Test accuracy: 63.38 

Round   4, Global train loss: 0.761, Global test loss: 1.984, Global test accuracy: 27.07 

Round   5, Train loss: 0.636, Test loss: 0.867, Test accuracy: 63.83 

Round   5, Global train loss: 0.636, Global test loss: 1.846, Global test accuracy: 35.61 

Round   6, Train loss: 0.789, Test loss: 0.736, Test accuracy: 69.17 

Round   6, Global train loss: 0.789, Global test loss: 2.011, Global test accuracy: 33.00 

Round   7, Train loss: 0.634, Test loss: 0.680, Test accuracy: 70.78 

Round   7, Global train loss: 0.634, Global test loss: 2.050, Global test accuracy: 31.27 

Round   8, Train loss: 0.779, Test loss: 0.664, Test accuracy: 71.48 

Round   8, Global train loss: 0.779, Global test loss: 1.788, Global test accuracy: 36.04 

Round   9, Train loss: 0.632, Test loss: 0.658, Test accuracy: 72.10 

Round   9, Global train loss: 0.632, Global test loss: 1.852, Global test accuracy: 36.32 

Round  10, Train loss: 0.618, Test loss: 0.667, Test accuracy: 71.51 

Round  10, Global train loss: 0.618, Global test loss: 1.688, Global test accuracy: 40.16 

Round  11, Train loss: 0.644, Test loss: 0.653, Test accuracy: 72.12 

Round  11, Global train loss: 0.644, Global test loss: 2.133, Global test accuracy: 28.66 

Round  12, Train loss: 0.586, Test loss: 0.636, Test accuracy: 73.04 

Round  12, Global train loss: 0.586, Global test loss: 2.281, Global test accuracy: 30.26 

Round  13, Train loss: 0.621, Test loss: 0.623, Test accuracy: 73.30 

Round  13, Global train loss: 0.621, Global test loss: 1.716, Global test accuracy: 41.44 

Round  14, Train loss: 0.651, Test loss: 0.605, Test accuracy: 74.42 

Round  14, Global train loss: 0.651, Global test loss: 1.763, Global test accuracy: 38.12 

Round  15, Train loss: 0.530, Test loss: 0.606, Test accuracy: 74.81 

Round  15, Global train loss: 0.530, Global test loss: 1.472, Global test accuracy: 46.92 

Round  16, Train loss: 0.649, Test loss: 0.595, Test accuracy: 75.44 

Round  16, Global train loss: 0.649, Global test loss: 1.934, Global test accuracy: 32.22 

Round  17, Train loss: 0.536, Test loss: 0.577, Test accuracy: 76.23 

Round  17, Global train loss: 0.536, Global test loss: 1.562, Global test accuracy: 45.16 

Round  18, Train loss: 0.621, Test loss: 0.568, Test accuracy: 76.47 

Round  18, Global train loss: 0.621, Global test loss: 1.596, Global test accuracy: 42.66 

Round  19, Train loss: 0.481, Test loss: 0.564, Test accuracy: 76.92 

Round  19, Global train loss: 0.481, Global test loss: 1.424, Global test accuracy: 48.42 

Round  20, Train loss: 0.611, Test loss: 0.562, Test accuracy: 77.08 

Round  20, Global train loss: 0.611, Global test loss: 1.683, Global test accuracy: 43.98 

Round  21, Train loss: 0.562, Test loss: 0.573, Test accuracy: 76.78 

Round  21, Global train loss: 0.562, Global test loss: 1.470, Global test accuracy: 48.94 

Round  22, Train loss: 0.540, Test loss: 0.571, Test accuracy: 76.90 

Round  22, Global train loss: 0.540, Global test loss: 1.559, Global test accuracy: 42.24 

Round  23, Train loss: 0.475, Test loss: 0.568, Test accuracy: 76.72 

Round  23, Global train loss: 0.475, Global test loss: 1.716, Global test accuracy: 41.37 

Round  24, Train loss: 0.470, Test loss: 0.540, Test accuracy: 77.54 

Round  24, Global train loss: 0.470, Global test loss: 1.545, Global test accuracy: 46.76 

Round  25, Train loss: 0.535, Test loss: 0.543, Test accuracy: 77.55 

Round  25, Global train loss: 0.535, Global test loss: 1.528, Global test accuracy: 46.76 

Round  26, Train loss: 0.482, Test loss: 0.541, Test accuracy: 78.00 

Round  26, Global train loss: 0.482, Global test loss: 1.564, Global test accuracy: 45.57 

Round  27, Train loss: 0.436, Test loss: 0.537, Test accuracy: 78.29 

Round  27, Global train loss: 0.436, Global test loss: 1.701, Global test accuracy: 45.00 

Round  28, Train loss: 0.485, Test loss: 0.538, Test accuracy: 78.41 

Round  28, Global train loss: 0.485, Global test loss: 1.843, Global test accuracy: 42.53 

Round  29, Train loss: 0.545, Test loss: 0.538, Test accuracy: 78.44 

Round  29, Global train loss: 0.545, Global test loss: 1.481, Global test accuracy: 49.60 

Round  30, Train loss: 0.471, Test loss: 0.537, Test accuracy: 78.66 

Round  30, Global train loss: 0.471, Global test loss: 1.429, Global test accuracy: 49.57 

Round  31, Train loss: 0.470, Test loss: 0.535, Test accuracy: 78.60 

Round  31, Global train loss: 0.470, Global test loss: 1.547, Global test accuracy: 45.27 

Round  32, Train loss: 0.499, Test loss: 0.536, Test accuracy: 78.67 

Round  32, Global train loss: 0.499, Global test loss: 1.531, Global test accuracy: 51.44 

Round  33, Train loss: 0.456, Test loss: 0.535, Test accuracy: 78.83 

Round  33, Global train loss: 0.456, Global test loss: 1.513, Global test accuracy: 49.94 

Round  34, Train loss: 0.409, Test loss: 0.544, Test accuracy: 78.96 

Round  34, Global train loss: 0.409, Global test loss: 1.766, Global test accuracy: 46.13 

Round  35, Train loss: 0.387, Test loss: 0.548, Test accuracy: 78.91 

Round  35, Global train loss: 0.387, Global test loss: 1.747, Global test accuracy: 45.82 

Round  36, Train loss: 0.387, Test loss: 0.541, Test accuracy: 79.08 

Round  36, Global train loss: 0.387, Global test loss: 1.872, Global test accuracy: 44.13 

Round  37, Train loss: 0.390, Test loss: 0.527, Test accuracy: 79.67 

Round  37, Global train loss: 0.390, Global test loss: 1.538, Global test accuracy: 49.17 

Round  38, Train loss: 0.398, Test loss: 0.538, Test accuracy: 79.36 

Round  38, Global train loss: 0.398, Global test loss: 1.630, Global test accuracy: 48.02 

Round  39, Train loss: 0.398, Test loss: 0.531, Test accuracy: 79.72 

Round  39, Global train loss: 0.398, Global test loss: 1.290, Global test accuracy: 53.47 

Round  40, Train loss: 0.394, Test loss: 0.536, Test accuracy: 79.87 

Round  40, Global train loss: 0.394, Global test loss: 1.426, Global test accuracy: 50.71 

Round  41, Train loss: 0.370, Test loss: 0.536, Test accuracy: 79.86 

Round  41, Global train loss: 0.370, Global test loss: 1.346, Global test accuracy: 52.17 

Round  42, Train loss: 0.428, Test loss: 0.526, Test accuracy: 80.18 

Round  42, Global train loss: 0.428, Global test loss: 1.345, Global test accuracy: 53.23 

Round  43, Train loss: 0.429, Test loss: 0.535, Test accuracy: 79.88 

Round  43, Global train loss: 0.429, Global test loss: 1.439, Global test accuracy: 50.20 

Round  44, Train loss: 0.399, Test loss: 0.542, Test accuracy: 79.80 

Round  44, Global train loss: 0.399, Global test loss: 1.611, Global test accuracy: 48.13 

Round  45, Train loss: 0.423, Test loss: 0.533, Test accuracy: 80.03 

Round  45, Global train loss: 0.423, Global test loss: 1.398, Global test accuracy: 50.08 

Round  46, Train loss: 0.284, Test loss: 0.532, Test accuracy: 80.15 

Round  46, Global train loss: 0.284, Global test loss: 1.265, Global test accuracy: 55.74 

Round  47, Train loss: 0.322, Test loss: 0.521, Test accuracy: 80.46 

Round  47, Global train loss: 0.322, Global test loss: 1.379, Global test accuracy: 53.26 

Round  48, Train loss: 0.371, Test loss: 0.523, Test accuracy: 80.61 

Round  48, Global train loss: 0.371, Global test loss: 1.534, Global test accuracy: 51.32 

Round  49, Train loss: 0.308, Test loss: 0.517, Test accuracy: 80.88 

Round  49, Global train loss: 0.308, Global test loss: 1.260, Global test accuracy: 56.01 

Round  50, Train loss: 0.409, Test loss: 0.528, Test accuracy: 80.58 

Round  50, Global train loss: 0.409, Global test loss: 1.432, Global test accuracy: 53.02 

Round  51, Train loss: 0.394, Test loss: 0.517, Test accuracy: 81.02 

Round  51, Global train loss: 0.394, Global test loss: 1.547, Global test accuracy: 49.33 

Round  52, Train loss: 0.394, Test loss: 0.512, Test accuracy: 81.19 

Round  52, Global train loss: 0.394, Global test loss: 1.490, Global test accuracy: 49.03 

Round  53, Train loss: 0.333, Test loss: 0.522, Test accuracy: 80.86 

Round  53, Global train loss: 0.333, Global test loss: 1.656, Global test accuracy: 46.42 

Round  54, Train loss: 0.343, Test loss: 0.533, Test accuracy: 80.67 

Round  54, Global train loss: 0.343, Global test loss: 1.581, Global test accuracy: 51.00 

Round  55, Train loss: 0.411, Test loss: 0.536, Test accuracy: 80.40 

Round  55, Global train loss: 0.411, Global test loss: 1.418, Global test accuracy: 51.34 

Round  56, Train loss: 0.305, Test loss: 0.537, Test accuracy: 80.37 

Round  56, Global train loss: 0.305, Global test loss: 1.282, Global test accuracy: 55.39 

Round  57, Train loss: 0.367, Test loss: 0.551, Test accuracy: 80.09 

Round  57, Global train loss: 0.367, Global test loss: 1.586, Global test accuracy: 50.71 

Round  58, Train loss: 0.256, Test loss: 0.553, Test accuracy: 80.07 

Round  58, Global train loss: 0.256, Global test loss: 1.312, Global test accuracy: 55.44 

Round  59, Train loss: 0.305, Test loss: 0.559, Test accuracy: 80.03 

Round  59, Global train loss: 0.305, Global test loss: 1.447, Global test accuracy: 53.89 

Round  60, Train loss: 0.342, Test loss: 0.542, Test accuracy: 80.58 

Round  60, Global train loss: 0.342, Global test loss: 1.247, Global test accuracy: 57.31 

Round  61, Train loss: 0.308, Test loss: 0.535, Test accuracy: 81.12 

Round  61, Global train loss: 0.308, Global test loss: 1.712, Global test accuracy: 46.91 

Round  62, Train loss: 0.311, Test loss: 0.520, Test accuracy: 81.87 

Round  62, Global train loss: 0.311, Global test loss: 1.360, Global test accuracy: 54.27 

Round  63, Train loss: 0.325, Test loss: 0.523, Test accuracy: 81.78 

Round  63, Global train loss: 0.325, Global test loss: 1.457, Global test accuracy: 54.48 

Round  64, Train loss: 0.327, Test loss: 0.522, Test accuracy: 81.92 

Round  64, Global train loss: 0.327, Global test loss: 1.621, Global test accuracy: 52.36 

Round  65, Train loss: 0.282, Test loss: 0.527, Test accuracy: 81.78 

Round  65, Global train loss: 0.282, Global test loss: 1.464, Global test accuracy: 53.73 

Round  66, Train loss: 0.330, Test loss: 0.532, Test accuracy: 81.77 

Round  66, Global train loss: 0.330, Global test loss: 1.397, Global test accuracy: 52.21 

Round  67, Train loss: 0.303, Test loss: 0.551, Test accuracy: 81.30 

Round  67, Global train loss: 0.303, Global test loss: 1.276, Global test accuracy: 57.48 

Round  68, Train loss: 0.302, Test loss: 0.552, Test accuracy: 81.31 

Round  68, Global train loss: 0.302, Global test loss: 1.433, Global test accuracy: 55.56 

Round  69, Train loss: 0.237, Test loss: 0.550, Test accuracy: 81.38 

Round  69, Global train loss: 0.237, Global test loss: 1.322, Global test accuracy: 55.38 

Round  70, Train loss: 0.315, Test loss: 0.541, Test accuracy: 81.83 

Round  70, Global train loss: 0.315, Global test loss: 1.401, Global test accuracy: 54.96 

Round  71, Train loss: 0.269, Test loss: 0.565, Test accuracy: 81.39 

Round  71, Global train loss: 0.269, Global test loss: 1.322, Global test accuracy: 56.77 

Round  72, Train loss: 0.287, Test loss: 0.558, Test accuracy: 81.22 

Round  72, Global train loss: 0.287, Global test loss: 1.569, Global test accuracy: 53.73 

Round  73, Train loss: 0.234, Test loss: 0.566, Test accuracy: 81.08 

Round  73, Global train loss: 0.234, Global test loss: 1.591, Global test accuracy: 53.62 

Round  74, Train loss: 0.309, Test loss: 0.571, Test accuracy: 81.13 

Round  74, Global train loss: 0.309, Global test loss: 1.427, Global test accuracy: 54.45 

Round  75, Train loss: 0.246, Test loss: 0.573, Test accuracy: 80.97 

Round  75, Global train loss: 0.246, Global test loss: 1.519, Global test accuracy: 54.89 

Round  76, Train loss: 0.235, Test loss: 0.569, Test accuracy: 81.23 

Round  76, Global train loss: 0.235, Global test loss: 1.782, Global test accuracy: 51.48 

Round  77, Train loss: 0.264, Test loss: 0.556, Test accuracy: 81.16 

Round  77, Global train loss: 0.264, Global test loss: 1.439, Global test accuracy: 56.88 

Round  78, Train loss: 0.269, Test loss: 0.576, Test accuracy: 80.67 

Round  78, Global train loss: 0.269, Global test loss: 1.515, Global test accuracy: 56.20 

Round  79, Train loss: 0.238, Test loss: 0.573, Test accuracy: 81.02 

Round  79, Global train loss: 0.238, Global test loss: 1.338, Global test accuracy: 58.73 

Round  80, Train loss: 0.301, Test loss: 0.554, Test accuracy: 81.72 

Round  80, Global train loss: 0.301, Global test loss: 1.352, Global test accuracy: 56.26 

Round  81, Train loss: 0.260, Test loss: 0.564, Test accuracy: 81.61 

Round  81, Global train loss: 0.260, Global test loss: 1.499, Global test accuracy: 52.25 

Round  82, Train loss: 0.255, Test loss: 0.558, Test accuracy: 81.81 

Round  82, Global train loss: 0.255, Global test loss: 1.286, Global test accuracy: 58.04 

Round  83, Train loss: 0.289, Test loss: 0.569, Test accuracy: 81.54 

Round  83, Global train loss: 0.289, Global test loss: 1.540, Global test accuracy: 54.62 

Round  84, Train loss: 0.275, Test loss: 0.573, Test accuracy: 81.48 

Round  84, Global train loss: 0.275, Global test loss: 1.352, Global test accuracy: 57.50 

Round  85, Train loss: 0.193, Test loss: 0.567, Test accuracy: 81.78 

Round  85, Global train loss: 0.193, Global test loss: 1.700, Global test accuracy: 52.05 

Round  86, Train loss: 0.232, Test loss: 0.581, Test accuracy: 81.75 

Round  86, Global train loss: 0.232, Global test loss: 1.587, Global test accuracy: 54.07 

Round  87, Train loss: 0.281, Test loss: 0.565, Test accuracy: 82.20 

Round  87, Global train loss: 0.281, Global test loss: 1.507, Global test accuracy: 53.46 

Round  88, Train loss: 0.287, Test loss: 0.566, Test accuracy: 82.05 

Round  88, Global train loss: 0.287, Global test loss: 1.599, Global test accuracy: 50.38 

Round  89, Train loss: 0.268, Test loss: 0.556, Test accuracy: 81.93 

Round  89, Global train loss: 0.268, Global test loss: 1.638, Global test accuracy: 51.25 

Round  90, Train loss: 0.194, Test loss: 0.561, Test accuracy: 81.93 

Round  90, Global train loss: 0.194, Global test loss: 1.314, Global test accuracy: 59.03 

Round  91, Train loss: 0.193, Test loss: 0.570, Test accuracy: 81.88 

Round  91, Global train loss: 0.193, Global test loss: 1.762, Global test accuracy: 51.19 

Round  92, Train loss: 0.266, Test loss: 0.566, Test accuracy: 82.08 

Round  92, Global train loss: 0.266, Global test loss: 1.332, Global test accuracy: 57.31 

Round  93, Train loss: 0.226, Test loss: 0.583, Test accuracy: 81.77 

Round  93, Global train loss: 0.226, Global test loss: 1.342, Global test accuracy: 58.51 

Round  94, Train loss: 0.201, Test loss: 0.576, Test accuracy: 81.97 

Round  94, Global train loss: 0.201, Global test loss: 1.458, Global test accuracy: 57.01 

Round  95, Train loss: 0.231, Test loss: 0.599, Test accuracy: 81.72 

Round  95, Global train loss: 0.231, Global test loss: 1.641, Global test accuracy: 55.48 

Round  96, Train loss: 0.214, Test loss: 0.605, Test accuracy: 81.52 

Round  96, Global train loss: 0.214, Global test loss: 1.342, Global test accuracy: 59.04 

Round  97, Train loss: 0.189, Test loss: 0.598, Test accuracy: 81.70 

Round  97, Global train loss: 0.189, Global test loss: 1.749, Global test accuracy: 52.86 

Round  98, Train loss: 0.218, Test loss: 0.591, Test accuracy: 81.64 

Round  98, Global train loss: 0.218, Global test loss: 1.623, Global test accuracy: 53.32 

Round  99, Train loss: 0.233, Test loss: 0.597, Test accuracy: 81.92 

Round  99, Global train loss: 0.233, Global test loss: 1.607, Global test accuracy: 53.33 

Final Round, Train loss: 0.175, Test loss: 0.622, Test accuracy: 82.24 

Final Round, Global train loss: 0.175, Global test loss: 1.607, Global test accuracy: 53.33 

Average accuracy final 10 rounds: 81.81333333333333 

Average global accuracy final 10 rounds: 55.7075 

1271.2592923641205
[1.3363854885101318, 2.4501969814300537, 3.561933994293213, 4.675517559051514, 5.787182807922363, 6.964020490646362, 8.14149785041809, 9.318411350250244, 10.494601726531982, 11.665106058120728, 12.83710527420044, 14.006042957305908, 15.173463582992554, 16.34107518196106, 17.51190733909607, 18.679263830184937, 19.85640835762024, 21.040831089019775, 22.224801301956177, 23.407077074050903, 24.589065074920654, 25.76747465133667, 26.943684577941895, 28.10784387588501, 29.283881902694702, 30.454994201660156, 31.625606536865234, 32.79702663421631, 33.94810080528259, 35.112608432769775, 36.206013679504395, 37.30398988723755, 38.397252321243286, 39.49258852005005, 40.602659702301025, 41.74266839027405, 42.895429849624634, 44.05642533302307, 45.22149682044983, 46.387277126312256, 47.56107449531555, 48.73007941246033, 49.90174055099487, 51.06815481185913, 52.234716176986694, 53.407984256744385, 54.58722972869873, 55.75386166572571, 56.92677330970764, 58.09988617897034, 59.27506113052368, 60.450082540512085, 61.62095904350281, 62.79788637161255, 63.96527886390686, 65.13401341438293, 66.39610362052917, 67.57499599456787, 68.74182629585266, 69.75086665153503, 70.76034665107727, 71.75808620452881, 72.75733351707458, 73.75967597961426, 74.75811886787415, 75.75502228736877, 76.75236129760742, 77.7502510547638, 78.74576044082642, 79.74287867546082, 80.74497318267822, 81.74145865440369, 82.73887228965759, 83.73671269416809, 84.73519897460938, 85.73437285423279, 86.73557305335999, 87.7360360622406, 88.73644685745239, 89.74462151527405, 90.75002241134644, 91.75264930725098, 92.75794553756714, 93.77715516090393, 94.78268384933472, 95.78936719894409, 96.79631090164185, 97.80124163627625, 98.80738735198975, 99.8139419555664, 100.8182954788208, 101.82622790336609, 102.83416509628296, 103.8407974243164, 104.84795784950256, 105.8540506362915, 106.86486768722534, 107.87606906890869, 108.88533926010132, 109.8837559223175, 111.88374733924866]
[25.325, 38.233333333333334, 44.00833333333333, 58.4, 63.375, 63.833333333333336, 69.16666666666667, 70.775, 71.48333333333333, 72.1, 71.50833333333334, 72.125, 73.04166666666667, 73.3, 74.425, 74.80833333333334, 75.44166666666666, 76.23333333333333, 76.475, 76.91666666666667, 77.075, 76.775, 76.9, 76.725, 77.54166666666667, 77.55, 78.0, 78.29166666666667, 78.40833333333333, 78.44166666666666, 78.65833333333333, 78.6, 78.675, 78.83333333333333, 78.95833333333333, 78.90833333333333, 79.08333333333333, 79.675, 79.35833333333333, 79.71666666666667, 79.86666666666666, 79.85833333333333, 80.18333333333334, 79.875, 79.8, 80.025, 80.15, 80.45833333333333, 80.60833333333333, 80.875, 80.58333333333333, 81.01666666666667, 81.19166666666666, 80.85833333333333, 80.66666666666667, 80.4, 80.36666666666666, 80.09166666666667, 80.06666666666666, 80.03333333333333, 80.58333333333333, 81.125, 81.86666666666666, 81.78333333333333, 81.91666666666667, 81.78333333333333, 81.76666666666667, 81.3, 81.30833333333334, 81.38333333333334, 81.825, 81.39166666666667, 81.21666666666667, 81.08333333333333, 81.13333333333334, 80.975, 81.23333333333333, 81.15833333333333, 80.66666666666667, 81.01666666666667, 81.71666666666667, 81.60833333333333, 81.80833333333334, 81.54166666666667, 81.48333333333333, 81.78333333333333, 81.75, 82.2, 82.05, 81.93333333333334, 81.93333333333334, 81.88333333333334, 82.08333333333333, 81.76666666666667, 81.96666666666667, 81.725, 81.51666666666667, 81.7, 81.64166666666667, 81.91666666666667, 82.24166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.507, Test loss: 2.296, Test accuracy: 19.28 

Round   1, Train loss: 1.073, Test loss: 2.284, Test accuracy: 28.17 

Round   2, Train loss: 0.996, Test loss: 1.548, Test accuracy: 40.59 

Round   3, Train loss: 0.909, Test loss: 1.258, Test accuracy: 49.76 

Round   4, Train loss: 0.802, Test loss: 1.132, Test accuracy: 53.01 

Round   5, Train loss: 0.842, Test loss: 1.171, Test accuracy: 53.94 

Round   6, Train loss: 0.824, Test loss: 0.864, Test accuracy: 61.45 

Round   7, Train loss: 0.730, Test loss: 0.753, Test accuracy: 66.11 

Round   8, Train loss: 0.649, Test loss: 0.731, Test accuracy: 68.01 

Round   9, Train loss: 0.649, Test loss: 0.711, Test accuracy: 69.05 

Round  10, Train loss: 0.680, Test loss: 0.708, Test accuracy: 69.40 

Round  11, Train loss: 0.650, Test loss: 0.675, Test accuracy: 70.70 

Round  12, Train loss: 0.605, Test loss: 0.676, Test accuracy: 70.92 

Round  13, Train loss: 0.701, Test loss: 0.640, Test accuracy: 72.12 

Round  14, Train loss: 0.639, Test loss: 0.644, Test accuracy: 72.50 

Round  15, Train loss: 0.574, Test loss: 0.618, Test accuracy: 74.19 

Round  16, Train loss: 0.613, Test loss: 0.615, Test accuracy: 74.48 

Round  17, Train loss: 0.559, Test loss: 0.598, Test accuracy: 74.09 

Round  18, Train loss: 0.594, Test loss: 0.594, Test accuracy: 74.49 

Round  19, Train loss: 0.659, Test loss: 0.594, Test accuracy: 74.56 

Round  20, Train loss: 0.492, Test loss: 0.582, Test accuracy: 74.89 

Round  21, Train loss: 0.488, Test loss: 0.573, Test accuracy: 75.62 

Round  22, Train loss: 0.485, Test loss: 0.572, Test accuracy: 75.52 

Round  23, Train loss: 0.583, Test loss: 0.553, Test accuracy: 76.46 

Round  24, Train loss: 0.582, Test loss: 0.558, Test accuracy: 76.19 

Round  25, Train loss: 0.625, Test loss: 0.561, Test accuracy: 76.37 

Round  26, Train loss: 0.539, Test loss: 0.546, Test accuracy: 77.28 

Round  27, Train loss: 0.502, Test loss: 0.535, Test accuracy: 77.53 

Round  28, Train loss: 0.489, Test loss: 0.535, Test accuracy: 77.75 

Round  29, Train loss: 0.498, Test loss: 0.534, Test accuracy: 77.60 

Round  30, Train loss: 0.439, Test loss: 0.507, Test accuracy: 78.79 

Round  31, Train loss: 0.497, Test loss: 0.505, Test accuracy: 79.06 

Round  32, Train loss: 0.449, Test loss: 0.504, Test accuracy: 78.63 

Round  33, Train loss: 0.463, Test loss: 0.499, Test accuracy: 79.28 

Round  34, Train loss: 0.459, Test loss: 0.505, Test accuracy: 78.83 

Round  35, Train loss: 0.502, Test loss: 0.527, Test accuracy: 78.13 

Round  36, Train loss: 0.428, Test loss: 0.496, Test accuracy: 79.47 

Round  37, Train loss: 0.508, Test loss: 0.504, Test accuracy: 79.32 

Round  38, Train loss: 0.395, Test loss: 0.491, Test accuracy: 79.83 

Round  39, Train loss: 0.467, Test loss: 0.482, Test accuracy: 80.31 

Round  40, Train loss: 0.412, Test loss: 0.513, Test accuracy: 79.08 

Round  41, Train loss: 0.489, Test loss: 0.495, Test accuracy: 79.87 

Round  42, Train loss: 0.443, Test loss: 0.479, Test accuracy: 80.48 

Round  43, Train loss: 0.365, Test loss: 0.475, Test accuracy: 80.65 

Round  44, Train loss: 0.370, Test loss: 0.480, Test accuracy: 80.39 

Round  45, Train loss: 0.457, Test loss: 0.476, Test accuracy: 80.51 

Round  46, Train loss: 0.486, Test loss: 0.473, Test accuracy: 80.42 

Round  47, Train loss: 0.424, Test loss: 0.482, Test accuracy: 80.28 

Round  48, Train loss: 0.415, Test loss: 0.469, Test accuracy: 81.08 

Round  49, Train loss: 0.410, Test loss: 0.462, Test accuracy: 81.10 

Round  50, Train loss: 0.357, Test loss: 0.456, Test accuracy: 81.53 

Round  51, Train loss: 0.448, Test loss: 0.459, Test accuracy: 81.53 

Round  52, Train loss: 0.447, Test loss: 0.451, Test accuracy: 82.13 

Round  53, Train loss: 0.409, Test loss: 0.461, Test accuracy: 81.58 

Round  54, Train loss: 0.483, Test loss: 0.467, Test accuracy: 81.36 

Round  55, Train loss: 0.430, Test loss: 0.458, Test accuracy: 81.69 

Round  56, Train loss: 0.412, Test loss: 0.451, Test accuracy: 82.20 

Round  57, Train loss: 0.352, Test loss: 0.447, Test accuracy: 82.12 

Round  58, Train loss: 0.332, Test loss: 0.451, Test accuracy: 81.74 

Round  59, Train loss: 0.407, Test loss: 0.441, Test accuracy: 82.15 

Round  60, Train loss: 0.396, Test loss: 0.443, Test accuracy: 82.28 

Round  61, Train loss: 0.446, Test loss: 0.438, Test accuracy: 82.52 

Round  62, Train loss: 0.398, Test loss: 0.445, Test accuracy: 82.45 

Round  63, Train loss: 0.368, Test loss: 0.454, Test accuracy: 82.23 

Round  64, Train loss: 0.357, Test loss: 0.447, Test accuracy: 82.07 

Round  65, Train loss: 0.454, Test loss: 0.451, Test accuracy: 82.18 

Round  66, Train loss: 0.408, Test loss: 0.455, Test accuracy: 82.25 

Round  67, Train loss: 0.361, Test loss: 0.455, Test accuracy: 82.01 

Round  68, Train loss: 0.399, Test loss: 0.452, Test accuracy: 82.01 

Round  69, Train loss: 0.266, Test loss: 0.454, Test accuracy: 81.92 

Round  70, Train loss: 0.367, Test loss: 0.456, Test accuracy: 82.24 

Round  71, Train loss: 0.320, Test loss: 0.444, Test accuracy: 82.68 

Round  72, Train loss: 0.351, Test loss: 0.437, Test accuracy: 82.97 

Round  73, Train loss: 0.354, Test loss: 0.447, Test accuracy: 82.69 

Round  74, Train loss: 0.330, Test loss: 0.441, Test accuracy: 82.95 

Round  75, Train loss: 0.358, Test loss: 0.441, Test accuracy: 82.54 

Round  76, Train loss: 0.311, Test loss: 0.443, Test accuracy: 83.10 

Round  77, Train loss: 0.248, Test loss: 0.452, Test accuracy: 82.54 

Round  78, Train loss: 0.243, Test loss: 0.449, Test accuracy: 82.53 

Round  79, Train loss: 0.383, Test loss: 0.430, Test accuracy: 83.45 

Round  80, Train loss: 0.317, Test loss: 0.454, Test accuracy: 82.81 

Round  81, Train loss: 0.313, Test loss: 0.441, Test accuracy: 83.23 

Round  82, Train loss: 0.353, Test loss: 0.438, Test accuracy: 83.50 

Round  83, Train loss: 0.300, Test loss: 0.434, Test accuracy: 83.33 

Round  84, Train loss: 0.281, Test loss: 0.431, Test accuracy: 83.66 

Round  85, Train loss: 0.265, Test loss: 0.438, Test accuracy: 83.55 

Round  86, Train loss: 0.336, Test loss: 0.438, Test accuracy: 83.69 

Round  87, Train loss: 0.328, Test loss: 0.447, Test accuracy: 83.28 

Round  88, Train loss: 0.299, Test loss: 0.437, Test accuracy: 83.54 

Round  89, Train loss: 0.286, Test loss: 0.449, Test accuracy: 83.28 

Round  90, Train loss: 0.275, Test loss: 0.449, Test accuracy: 83.41 

Round  91, Train loss: 0.282, Test loss: 0.447, Test accuracy: 83.52 

Round  92, Train loss: 0.300, Test loss: 0.441, Test accuracy: 83.74 

Round  93, Train loss: 0.246, Test loss: 0.441, Test accuracy: 83.83 

Round  94, Train loss: 0.278, Test loss: 0.442, Test accuracy: 83.72 

Round  95, Train loss: 0.216, Test loss: 0.449, Test accuracy: 83.70 

Round  96, Train loss: 0.300, Test loss: 0.454, Test accuracy: 83.33 

Round  97, Train loss: 0.260, Test loss: 0.433, Test accuracy: 83.99 

Round  98, Train loss: 0.238, Test loss: 0.443, Test accuracy: 83.92 

Round  99, Train loss: 0.225, Test loss: 0.439, Test accuracy: 83.64 

Final Round, Train loss: 0.239, Test loss: 0.433, Test accuracy: 84.12 

Average accuracy final 10 rounds: 83.68083333333331 

955.8133418560028
[1.3107283115386963, 2.3717801570892334, 3.4302754402160645, 4.490542411804199, 5.5499255657196045, 6.610563516616821, 7.669645309448242, 8.72551965713501, 9.787948369979858, 10.846946477890015, 11.908489227294922, 12.957208395004272, 14.00752305984497, 15.059211730957031, 16.10722017288208, 17.131120443344116, 18.152445316314697, 19.170156002044678, 20.19730019569397, 21.218935012817383, 22.245378971099854, 23.296257972717285, 24.34358549118042, 25.38939356803894, 26.43935990333557, 27.494926929473877, 28.542250156402588, 29.594032764434814, 30.64613938331604, 31.69638752937317, 32.746586561203, 33.797321796417236, 34.84921193122864, 35.89685940742493, 36.94719648361206, 37.99927854537964, 39.05325531959534, 40.109262466430664, 41.15984511375427, 42.2163462638855, 43.26935601234436, 44.32364892959595, 45.372722864151, 46.424935817718506, 47.476080656051636, 48.53357434272766, 49.58604311943054, 50.638983964920044, 51.69771885871887, 52.751869916915894, 53.810922145843506, 54.87010645866394, 55.92181658744812, 56.97336196899414, 58.03291463851929, 59.08903527259827, 60.14601731300354, 61.20472025871277, 62.26285648345947, 63.31835341453552, 64.37756729125977, 65.43212819099426, 66.49673962593079, 67.55366683006287, 68.61499857902527, 69.67263197898865, 70.72924089431763, 71.7802484035492, 72.83377957344055, 73.88965034484863, 74.94178295135498, 76.00072002410889, 77.05619764328003, 78.11716747283936, 79.16944885253906, 80.22396516799927, 81.27512979507446, 82.32645988464355, 83.37942910194397, 84.43032336235046, 85.47915053367615, 86.52955889701843, 87.57753896713257, 88.63360953330994, 89.68452024459839, 90.73665857315063, 91.79158759117126, 92.83008027076721, 93.85373592376709, 94.87692594528198, 95.90066528320312, 96.92554569244385, 97.94744777679443, 98.97641658782959, 100.00404453277588, 101.02826642990112, 102.05032563209534, 103.07381343841553, 104.09866213798523, 105.12179017066956, 107.01052355766296]
[19.283333333333335, 28.166666666666668, 40.59166666666667, 49.75833333333333, 53.00833333333333, 53.94166666666667, 61.45, 66.10833333333333, 68.00833333333334, 69.05, 69.4, 70.7, 70.925, 72.11666666666666, 72.5, 74.19166666666666, 74.48333333333333, 74.09166666666667, 74.49166666666666, 74.55833333333334, 74.89166666666667, 75.625, 75.51666666666667, 76.45833333333333, 76.19166666666666, 76.36666666666666, 77.275, 77.525, 77.75, 77.6, 78.79166666666667, 79.05833333333334, 78.63333333333334, 79.28333333333333, 78.83333333333333, 78.13333333333334, 79.46666666666667, 79.31666666666666, 79.83333333333333, 80.30833333333334, 79.075, 79.86666666666666, 80.48333333333333, 80.65, 80.39166666666667, 80.50833333333334, 80.41666666666667, 80.28333333333333, 81.08333333333333, 81.1, 81.53333333333333, 81.525, 82.13333333333334, 81.58333333333333, 81.35833333333333, 81.69166666666666, 82.2, 82.11666666666666, 81.74166666666666, 82.15, 82.275, 82.51666666666667, 82.45, 82.23333333333333, 82.06666666666666, 82.18333333333334, 82.25, 82.00833333333334, 82.00833333333334, 81.925, 82.24166666666666, 82.68333333333334, 82.975, 82.69166666666666, 82.95, 82.54166666666667, 83.1, 82.54166666666667, 82.525, 83.45, 82.80833333333334, 83.23333333333333, 83.5, 83.33333333333333, 83.65833333333333, 83.55, 83.69166666666666, 83.275, 83.54166666666667, 83.28333333333333, 83.40833333333333, 83.51666666666667, 83.74166666666666, 83.825, 83.725, 83.7, 83.33333333333333, 83.99166666666666, 83.925, 83.64166666666667, 84.11666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.128, Test loss: 2.157, Test accuracy: 26.90 

Round   1, Train loss: 0.937, Test loss: 1.996, Test accuracy: 33.17 

Round   2, Train loss: 0.868, Test loss: 1.474, Test accuracy: 42.32 

Round   3, Train loss: 0.768, Test loss: 1.497, Test accuracy: 47.79 

Round   4, Train loss: 0.761, Test loss: 1.110, Test accuracy: 59.53 

Round   5, Train loss: 0.776, Test loss: 1.070, Test accuracy: 62.24 

Round   6, Train loss: 0.766, Test loss: 0.920, Test accuracy: 62.67 

Round   7, Train loss: 0.689, Test loss: 0.959, Test accuracy: 65.43 

Round   8, Train loss: 0.636, Test loss: 0.838, Test accuracy: 66.96 

Round   9, Train loss: 0.549, Test loss: 0.820, Test accuracy: 67.90 

Round  10, Train loss: 0.636, Test loss: 0.662, Test accuracy: 73.03 

Round  11, Train loss: 0.596, Test loss: 0.665, Test accuracy: 72.42 

Round  12, Train loss: 0.631, Test loss: 0.629, Test accuracy: 74.22 

Round  13, Train loss: 0.588, Test loss: 0.596, Test accuracy: 75.52 

Round  14, Train loss: 0.546, Test loss: 0.593, Test accuracy: 75.32 

Round  15, Train loss: 0.537, Test loss: 0.583, Test accuracy: 75.86 

Round  16, Train loss: 0.602, Test loss: 0.570, Test accuracy: 76.30 

Round  17, Train loss: 0.549, Test loss: 0.583, Test accuracy: 75.71 

Round  18, Train loss: 0.590, Test loss: 0.568, Test accuracy: 76.58 

Round  19, Train loss: 0.513, Test loss: 0.542, Test accuracy: 77.63 

Round  20, Train loss: 0.493, Test loss: 0.531, Test accuracy: 78.07 

Round  21, Train loss: 0.548, Test loss: 0.558, Test accuracy: 76.78 

Round  22, Train loss: 0.464, Test loss: 0.531, Test accuracy: 78.04 

Round  23, Train loss: 0.533, Test loss: 0.521, Test accuracy: 79.41 

Round  24, Train loss: 0.469, Test loss: 0.498, Test accuracy: 79.90 

Round  25, Train loss: 0.462, Test loss: 0.498, Test accuracy: 79.77 

Round  26, Train loss: 0.502, Test loss: 0.491, Test accuracy: 80.12 

Round  27, Train loss: 0.442, Test loss: 0.500, Test accuracy: 79.21 

Round  28, Train loss: 0.468, Test loss: 0.495, Test accuracy: 79.81 

Round  29, Train loss: 0.364, Test loss: 0.492, Test accuracy: 79.86 

Round  30, Train loss: 0.351, Test loss: 0.497, Test accuracy: 79.76 

Round  31, Train loss: 0.433, Test loss: 0.475, Test accuracy: 80.64 

Round  32, Train loss: 0.416, Test loss: 0.483, Test accuracy: 80.45 

Round  33, Train loss: 0.392, Test loss: 0.465, Test accuracy: 81.49 

Round  34, Train loss: 0.375, Test loss: 0.467, Test accuracy: 81.62 

Round  35, Train loss: 0.390, Test loss: 0.473, Test accuracy: 81.17 

Round  36, Train loss: 0.406, Test loss: 0.458, Test accuracy: 81.77 

Round  37, Train loss: 0.410, Test loss: 0.484, Test accuracy: 80.88 

Round  38, Train loss: 0.323, Test loss: 0.471, Test accuracy: 81.58 

Round  39, Train loss: 0.429, Test loss: 0.470, Test accuracy: 81.55 

Round  40, Train loss: 0.375, Test loss: 0.463, Test accuracy: 82.11 

Round  41, Train loss: 0.408, Test loss: 0.455, Test accuracy: 82.10 

Round  42, Train loss: 0.304, Test loss: 0.450, Test accuracy: 82.38 

Round  43, Train loss: 0.377, Test loss: 0.454, Test accuracy: 82.36 

Round  44, Train loss: 0.381, Test loss: 0.460, Test accuracy: 82.28 

Round  45, Train loss: 0.369, Test loss: 0.461, Test accuracy: 81.93 

Round  46, Train loss: 0.328, Test loss: 0.462, Test accuracy: 82.28 

Round  47, Train loss: 0.333, Test loss: 0.458, Test accuracy: 82.21 

Round  48, Train loss: 0.307, Test loss: 0.456, Test accuracy: 82.59 

Round  49, Train loss: 0.274, Test loss: 0.465, Test accuracy: 82.48 

Round  50, Train loss: 0.265, Test loss: 0.462, Test accuracy: 82.42 

Round  51, Train loss: 0.309, Test loss: 0.461, Test accuracy: 82.15 

Round  52, Train loss: 0.274, Test loss: 0.472, Test accuracy: 82.12 

Round  53, Train loss: 0.305, Test loss: 0.457, Test accuracy: 82.68 

Round  54, Train loss: 0.323, Test loss: 0.470, Test accuracy: 82.17 

Round  55, Train loss: 0.280, Test loss: 0.460, Test accuracy: 82.81 

Round  56, Train loss: 0.264, Test loss: 0.462, Test accuracy: 82.62 

Round  57, Train loss: 0.238, Test loss: 0.472, Test accuracy: 82.78 

Round  58, Train loss: 0.234, Test loss: 0.463, Test accuracy: 83.13 

Round  59, Train loss: 0.300, Test loss: 0.458, Test accuracy: 83.26 

Round  60, Train loss: 0.233, Test loss: 0.469, Test accuracy: 83.07 

Round  61, Train loss: 0.230, Test loss: 0.479, Test accuracy: 82.82 

Round  62, Train loss: 0.221, Test loss: 0.468, Test accuracy: 83.63 

Round  63, Train loss: 0.275, Test loss: 0.466, Test accuracy: 83.17 

Round  64, Train loss: 0.244, Test loss: 0.482, Test accuracy: 82.70 

Round  65, Train loss: 0.224, Test loss: 0.466, Test accuracy: 83.15 

Round  66, Train loss: 0.187, Test loss: 0.483, Test accuracy: 83.17 

Round  67, Train loss: 0.258, Test loss: 0.474, Test accuracy: 82.79 

Round  68, Train loss: 0.238, Test loss: 0.468, Test accuracy: 83.02 

Round  69, Train loss: 0.208, Test loss: 0.474, Test accuracy: 83.33 

Round  70, Train loss: 0.363, Test loss: 0.457, Test accuracy: 83.16 

Round  71, Train loss: 0.299, Test loss: 0.463, Test accuracy: 83.39 

Round  72, Train loss: 0.200, Test loss: 0.474, Test accuracy: 83.48 

Round  73, Train loss: 0.251, Test loss: 0.475, Test accuracy: 83.74 

Round  74, Train loss: 0.232, Test loss: 0.473, Test accuracy: 83.72 

Round  75, Train loss: 0.261, Test loss: 0.479, Test accuracy: 83.17 

Round  76, Train loss: 0.224, Test loss: 0.479, Test accuracy: 83.30 

Round  77, Train loss: 0.221, Test loss: 0.474, Test accuracy: 83.64 

Round  78, Train loss: 0.228, Test loss: 0.500, Test accuracy: 82.79 

Round  79, Train loss: 0.234, Test loss: 0.481, Test accuracy: 83.41 

Round  80, Train loss: 0.204, Test loss: 0.483, Test accuracy: 83.28 

Round  81, Train loss: 0.223, Test loss: 0.484, Test accuracy: 83.88 

Round  82, Train loss: 0.188, Test loss: 0.492, Test accuracy: 83.39 

Round  83, Train loss: 0.153, Test loss: 0.510, Test accuracy: 83.13 

Round  84, Train loss: 0.220, Test loss: 0.490, Test accuracy: 83.44 

Round  85, Train loss: 0.158, Test loss: 0.488, Test accuracy: 83.78 

Round  86, Train loss: 0.211, Test loss: 0.487, Test accuracy: 84.09 

Round  87, Train loss: 0.154, Test loss: 0.500, Test accuracy: 83.43 

Round  88, Train loss: 0.168, Test loss: 0.507, Test accuracy: 83.51 

Round  89, Train loss: 0.222, Test loss: 0.495, Test accuracy: 83.66 

Round  90, Train loss: 0.187, Test loss: 0.490, Test accuracy: 83.69 

Round  91, Train loss: 0.157, Test loss: 0.494, Test accuracy: 83.83 

Round  92, Train loss: 0.149, Test loss: 0.486, Test accuracy: 83.92 

Round  93, Train loss: 0.216, Test loss: 0.496, Test accuracy: 83.79 

Round  94, Train loss: 0.185, Test loss: 0.497, Test accuracy: 83.83 

Round  95, Train loss: 0.192, Test loss: 0.498, Test accuracy: 83.57 

Round  96, Train loss: 0.193, Test loss: 0.490, Test accuracy: 84.15 

Round  97, Train loss: 0.193, Test loss: 0.505, Test accuracy: 83.86 

Round  98, Train loss: 0.154, Test loss: 0.515, Test accuracy: 83.91 

Round  99, Train loss: 0.159, Test loss: 0.515, Test accuracy: 83.42 

Final Round, Train loss: 0.156, Test loss: 0.489, Test accuracy: 84.34 

Average accuracy final 10 rounds: 83.79666666666665 

937.7315142154694
[1.3858318328857422, 2.5465238094329834, 3.727100372314453, 4.900129079818726, 6.076468229293823, 7.252402305603027, 8.421099424362183, 9.582177639007568, 10.749046087265015, 11.91442346572876, 13.078984498977661, 14.243758916854858, 15.409449577331543, 16.573761224746704, 17.740715265274048, 18.90503692626953, 20.067091703414917, 21.235525131225586, 22.399275541305542, 23.39461398124695, 24.389955520629883, 25.382859706878662, 26.377744674682617, 27.38245964050293, 28.381551504135132, 29.374016284942627, 30.369259357452393, 31.3637216091156, 32.35707092285156, 33.35238480567932, 34.34643030166626, 35.34182572364807, 36.3362295627594, 37.33240723609924, 38.32583785057068, 39.322487354278564, 40.31970739364624, 41.314831018447876, 42.31113624572754, 43.30763292312622, 44.301281213760376, 45.29590821266174, 46.296295166015625, 47.292073011398315, 48.287996768951416, 49.27849745750427, 50.269450664520264, 51.26046299934387, 52.25581383705139, 53.24938678741455, 54.241809129714966, 55.228293895721436, 56.22360324859619, 57.215837717056274, 58.214165925979614, 59.212897539138794, 60.20837926864624, 61.201804637908936, 62.19749164581299, 63.191784620285034, 64.18523359298706, 65.1852011680603, 66.18627047538757, 67.18769884109497, 68.18955111503601, 69.18711400032043, 70.18844747543335, 71.17945885658264, 72.17644357681274, 73.17545771598816, 74.17614698410034, 75.17527770996094, 76.17526912689209, 77.17789793014526, 78.17797923088074, 79.17837834358215, 80.17776465415955, 81.17894339561462, 82.17211532592773, 83.1716513633728, 84.16874766349792, 85.16578960418701, 86.15927863121033, 87.15746569633484, 88.15368580818176, 89.15376234054565, 90.14753270149231, 91.14695191383362, 92.14399909973145, 93.14244747161865, 94.14254069328308, 95.14229035377502, 96.13809490203857, 97.13586926460266, 98.13438105583191, 99.1355345249176, 100.1335060596466, 101.12998557090759, 102.1251220703125, 103.12211680412292, 104.90378475189209]
[26.9, 33.175, 42.31666666666667, 47.791666666666664, 59.53333333333333, 62.24166666666667, 62.675, 65.43333333333334, 66.95833333333333, 67.9, 73.025, 72.425, 74.225, 75.51666666666667, 75.31666666666666, 75.85833333333333, 76.3, 75.70833333333333, 76.58333333333333, 77.63333333333334, 78.06666666666666, 76.78333333333333, 78.04166666666667, 79.40833333333333, 79.9, 79.76666666666667, 80.125, 79.20833333333333, 79.80833333333334, 79.85833333333333, 79.75833333333334, 80.64166666666667, 80.45, 81.49166666666666, 81.61666666666666, 81.175, 81.76666666666667, 80.88333333333334, 81.58333333333333, 81.55, 82.10833333333333, 82.1, 82.375, 82.35833333333333, 82.28333333333333, 81.93333333333334, 82.28333333333333, 82.20833333333333, 82.59166666666667, 82.48333333333333, 82.425, 82.15, 82.125, 82.68333333333334, 82.16666666666667, 82.80833333333334, 82.625, 82.775, 83.13333333333334, 83.25833333333334, 83.06666666666666, 82.81666666666666, 83.63333333333334, 83.16666666666667, 82.7, 83.15, 83.16666666666667, 82.79166666666667, 83.01666666666667, 83.33333333333333, 83.15833333333333, 83.39166666666667, 83.48333333333333, 83.74166666666666, 83.725, 83.16666666666667, 83.3, 83.64166666666667, 82.79166666666667, 83.40833333333333, 83.275, 83.875, 83.39166666666667, 83.13333333333334, 83.44166666666666, 83.78333333333333, 84.09166666666667, 83.43333333333334, 83.50833333333334, 83.65833333333333, 83.69166666666666, 83.825, 83.925, 83.79166666666667, 83.825, 83.56666666666666, 84.15, 83.85833333333333, 83.90833333333333, 83.425, 84.34166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.169, Test loss: 2.167, Test accuracy: 21.48 

Round   1, Train loss: 1.016, Test loss: 1.847, Test accuracy: 33.12 

Round   2, Train loss: 0.871, Test loss: 1.621, Test accuracy: 41.49 

Round   3, Train loss: 0.785, Test loss: 1.797, Test accuracy: 36.90 

Round   4, Train loss: 0.723, Test loss: 1.360, Test accuracy: 52.49 

Round   5, Train loss: 0.888, Test loss: 1.188, Test accuracy: 56.05 

Round   6, Train loss: 0.689, Test loss: 1.260, Test accuracy: 52.58 

Round   7, Train loss: 0.601, Test loss: 1.147, Test accuracy: 57.88 

Round   8, Train loss: 0.757, Test loss: 0.995, Test accuracy: 63.56 

Round   9, Train loss: 0.712, Test loss: 0.917, Test accuracy: 67.22 

Round  10, Train loss: 0.691, Test loss: 0.904, Test accuracy: 67.87 

Round  11, Train loss: 0.666, Test loss: 0.931, Test accuracy: 65.62 

Round  12, Train loss: 0.563, Test loss: 0.940, Test accuracy: 65.92 

Round  13, Train loss: 0.581, Test loss: 0.930, Test accuracy: 66.72 

Round  14, Train loss: 0.570, Test loss: 0.965, Test accuracy: 65.33 

Round  15, Train loss: 0.538, Test loss: 0.932, Test accuracy: 66.83 

Round  16, Train loss: 0.593, Test loss: 0.893, Test accuracy: 67.19 

Round  17, Train loss: 0.516, Test loss: 0.852, Test accuracy: 68.62 

Round  18, Train loss: 0.494, Test loss: 0.850, Test accuracy: 68.62 

Round  19, Train loss: 0.525, Test loss: 0.830, Test accuracy: 69.12 

Round  20, Train loss: 0.547, Test loss: 0.794, Test accuracy: 70.97 

Round  21, Train loss: 0.525, Test loss: 0.758, Test accuracy: 72.57 

Round  22, Train loss: 0.489, Test loss: 0.696, Test accuracy: 74.59 

Round  23, Train loss: 0.455, Test loss: 0.680, Test accuracy: 75.27 

Round  24, Train loss: 0.517, Test loss: 0.677, Test accuracy: 75.50 

Round  25, Train loss: 0.400, Test loss: 0.664, Test accuracy: 75.96 

Round  26, Train loss: 0.529, Test loss: 0.666, Test accuracy: 75.93 

Round  27, Train loss: 0.383, Test loss: 0.672, Test accuracy: 76.22 

Round  28, Train loss: 0.403, Test loss: 0.667, Test accuracy: 76.19 

Round  29, Train loss: 0.382, Test loss: 0.676, Test accuracy: 76.03 

Round  30, Train loss: 0.315, Test loss: 0.695, Test accuracy: 76.05 

Round  31, Train loss: 0.335, Test loss: 0.690, Test accuracy: 76.23 

Round  32, Train loss: 0.324, Test loss: 0.686, Test accuracy: 76.62 

Round  33, Train loss: 0.289, Test loss: 0.690, Test accuracy: 76.47 

Round  34, Train loss: 0.325, Test loss: 0.692, Test accuracy: 76.79 

Round  35, Train loss: 0.346, Test loss: 0.708, Test accuracy: 76.85 

Round  36, Train loss: 0.407, Test loss: 0.712, Test accuracy: 76.89 

Round  37, Train loss: 0.335, Test loss: 0.707, Test accuracy: 77.03 

Round  38, Train loss: 0.278, Test loss: 0.719, Test accuracy: 76.42 

Round  39, Train loss: 0.261, Test loss: 0.753, Test accuracy: 75.96 

Round  40, Train loss: 0.289, Test loss: 0.762, Test accuracy: 75.97 

Round  41, Train loss: 0.260, Test loss: 0.750, Test accuracy: 76.60 

Round  42, Train loss: 0.209, Test loss: 0.738, Test accuracy: 76.81 

Round  43, Train loss: 0.279, Test loss: 0.742, Test accuracy: 77.12 

Round  44, Train loss: 0.250, Test loss: 0.763, Test accuracy: 76.92 

Round  45, Train loss: 0.257, Test loss: 0.772, Test accuracy: 76.85 

Round  46, Train loss: 0.185, Test loss: 0.767, Test accuracy: 77.04 

Round  47, Train loss: 0.233, Test loss: 0.767, Test accuracy: 76.76 

Round  48, Train loss: 0.238, Test loss: 0.769, Test accuracy: 76.90 

Round  49, Train loss: 0.220, Test loss: 0.762, Test accuracy: 77.14 

Round  50, Train loss: 0.242, Test loss: 0.772, Test accuracy: 77.16 

Round  51, Train loss: 0.202, Test loss: 0.799, Test accuracy: 77.41 

Round  52, Train loss: 0.173, Test loss: 0.795, Test accuracy: 77.69 

Round  53, Train loss: 0.222, Test loss: 0.817, Test accuracy: 77.24 

Round  54, Train loss: 0.203, Test loss: 0.822, Test accuracy: 77.29 

Round  55, Train loss: 0.174, Test loss: 0.826, Test accuracy: 77.28 

Round  56, Train loss: 0.244, Test loss: 0.813, Test accuracy: 77.57 

Round  57, Train loss: 0.142, Test loss: 0.845, Test accuracy: 77.03 

Round  58, Train loss: 0.203, Test loss: 0.863, Test accuracy: 77.04 

Round  59, Train loss: 0.140, Test loss: 0.870, Test accuracy: 77.03 

Round  60, Train loss: 0.120, Test loss: 0.894, Test accuracy: 76.95 

Round  61, Train loss: 0.163, Test loss: 0.922, Test accuracy: 76.99 

Round  62, Train loss: 0.167, Test loss: 0.934, Test accuracy: 77.10 

Round  63, Train loss: 0.152, Test loss: 0.924, Test accuracy: 77.27 

Round  64, Train loss: 0.162, Test loss: 0.939, Test accuracy: 77.49 

Round  65, Train loss: 0.166, Test loss: 0.937, Test accuracy: 77.21 

Round  66, Train loss: 0.115, Test loss: 0.931, Test accuracy: 77.53 

Round  67, Train loss: 0.155, Test loss: 0.932, Test accuracy: 77.56 

Round  68, Train loss: 0.125, Test loss: 0.948, Test accuracy: 77.58 

Round  69, Train loss: 0.116, Test loss: 0.936, Test accuracy: 77.71 

Round  70, Train loss: 0.126, Test loss: 0.939, Test accuracy: 77.73 

Round  71, Train loss: 0.170, Test loss: 0.918, Test accuracy: 78.30 

Round  72, Train loss: 0.139, Test loss: 0.936, Test accuracy: 77.69 

Round  73, Train loss: 0.099, Test loss: 0.946, Test accuracy: 77.55 

Round  74, Train loss: 0.096, Test loss: 0.998, Test accuracy: 77.69 

Round  75, Train loss: 0.121, Test loss: 0.996, Test accuracy: 77.82 

Round  76, Train loss: 0.088, Test loss: 0.985, Test accuracy: 78.04 

Round  77, Train loss: 0.125, Test loss: 1.006, Test accuracy: 77.79 

Round  78, Train loss: 0.100, Test loss: 1.017, Test accuracy: 77.58 

Round  79, Train loss: 0.104, Test loss: 1.028, Test accuracy: 77.57 

Round  80, Train loss: 0.107, Test loss: 0.989, Test accuracy: 78.02 

Round  81, Train loss: 0.088, Test loss: 0.977, Test accuracy: 77.71 

Round  82, Train loss: 0.070, Test loss: 1.055, Test accuracy: 77.17 

Round  83, Train loss: 0.070, Test loss: 1.026, Test accuracy: 78.17 

Round  84, Train loss: 0.077, Test loss: 1.051, Test accuracy: 77.99 

Round  85, Train loss: 0.111, Test loss: 1.053, Test accuracy: 78.12 

Round  86, Train loss: 0.070, Test loss: 1.065, Test accuracy: 78.14 

Round  87, Train loss: 0.103, Test loss: 1.062, Test accuracy: 78.08 

Round  88, Train loss: 0.074, Test loss: 1.064, Test accuracy: 78.08 

Round  89, Train loss: 0.107, Test loss: 1.072, Test accuracy: 78.11 

Round  90, Train loss: 0.093, Test loss: 1.075, Test accuracy: 78.18 

Round  91, Train loss: 0.070, Test loss: 1.092, Test accuracy: 77.88 

Round  92, Train loss: 0.084, Test loss: 1.091, Test accuracy: 77.94 

Round  93, Train loss: 0.056, Test loss: 1.140, Test accuracy: 77.58 

Round  94, Train loss: 0.076, Test loss: 1.132, Test accuracy: 77.80 

Round  95, Train loss: 0.071, Test loss: 1.128, Test accuracy: 77.73 

Round  96, Train loss: 0.061, Test loss: 1.154, Test accuracy: 77.92 

Round  97, Train loss: 0.063, Test loss: 1.148, Test accuracy: 78.12 

Round  98, Train loss: 0.078, Test loss: 1.136, Test accuracy: 78.33 

Round  99, Train loss: 0.056, Test loss: 1.152, Test accuracy: 78.12 

Final Round, Train loss: 0.051, Test loss: 1.188, Test accuracy: 78.68 

Average accuracy final 10 rounds: 77.96 

973.0154812335968
[1.4059131145477295, 2.5770833492279053, 3.7560811042785645, 4.9244372844696045, 6.0992302894592285, 7.269985198974609, 8.442168951034546, 9.615004539489746, 10.799129962921143, 11.980337381362915, 13.161655902862549, 14.345330715179443, 15.528183698654175, 16.711002826690674, 17.89460253715515, 19.084475994110107, 20.27222490310669, 21.459274291992188, 22.646440267562866, 23.829872846603394, 25.01377820968628, 26.197824716567993, 27.384110689163208, 28.575788259506226, 29.757290363311768, 30.939455270767212, 32.12104558944702, 33.29860043525696, 34.47749352455139, 35.66088151931763, 36.845956325531006, 37.84844970703125, 38.85183238983154, 39.853816747665405, 40.85983371734619, 41.86721467971802, 42.87134385108948, 43.87867760658264, 44.883540868759155, 45.88648223876953, 46.89698672294617, 47.904170989990234, 48.90592050552368, 49.91514778137207, 50.92110085487366, 51.925753116607666, 52.93460440635681, 53.93918299674988, 54.9422287940979, 55.95011806488037, 56.95616149902344, 57.9617702960968, 58.96316981315613, 59.96929979324341, 60.973398208618164, 61.98283553123474, 62.98972010612488, 63.99434947967529, 65.00185489654541, 66.00917315483093, 67.01466107368469, 68.02224922180176, 69.03251004219055, 70.03806900978088, 71.04908967018127, 72.05946350097656, 73.06444144248962, 74.07128620147705, 75.07916975021362, 76.0887942314148, 77.09873604774475, 78.10646271705627, 79.11078906059265, 80.11302328109741, 81.12118625640869, 82.12826538085938, 83.13899683952332, 84.14343309402466, 85.14885663986206, 86.15656352043152, 87.15891861915588, 88.16225218772888, 89.16816973686218, 90.17455220222473, 91.18099236488342, 92.18844866752625, 93.19558095932007, 94.20132160186768, 95.20947670936584, 96.22116947174072, 97.22731566429138, 98.23134112358093, 99.23948907852173, 100.24686169624329, 101.2483561038971, 102.25431632995605, 103.2622811794281, 104.26585030555725, 105.27499127388, 106.27959084510803, 108.55419111251831]
[21.475, 33.11666666666667, 41.49166666666667, 36.9, 52.49166666666667, 56.05, 52.583333333333336, 57.88333333333333, 63.55833333333333, 67.225, 67.86666666666666, 65.625, 65.925, 66.725, 65.33333333333333, 66.825, 67.19166666666666, 68.61666666666666, 68.61666666666666, 69.125, 70.975, 72.56666666666666, 74.59166666666667, 75.26666666666667, 75.5, 75.95833333333333, 75.93333333333334, 76.225, 76.19166666666666, 76.03333333333333, 76.05, 76.23333333333333, 76.625, 76.46666666666667, 76.79166666666667, 76.85, 76.89166666666667, 77.03333333333333, 76.41666666666667, 75.95833333333333, 75.96666666666667, 76.6, 76.80833333333334, 77.125, 76.91666666666667, 76.85, 77.04166666666667, 76.75833333333334, 76.9, 77.14166666666667, 77.15833333333333, 77.40833333333333, 77.69166666666666, 77.24166666666666, 77.29166666666667, 77.28333333333333, 77.56666666666666, 77.03333333333333, 77.04166666666667, 77.025, 76.95, 76.99166666666666, 77.1, 77.26666666666667, 77.49166666666666, 77.20833333333333, 77.53333333333333, 77.55833333333334, 77.58333333333333, 77.70833333333333, 77.73333333333333, 78.3, 77.69166666666666, 77.55, 77.69166666666666, 77.81666666666666, 78.04166666666667, 77.79166666666667, 77.575, 77.56666666666666, 78.01666666666667, 77.70833333333333, 77.175, 78.16666666666667, 77.99166666666666, 78.125, 78.14166666666667, 78.08333333333333, 78.08333333333333, 78.10833333333333, 78.18333333333334, 77.875, 77.94166666666666, 77.58333333333333, 77.8, 77.73333333333333, 77.91666666666667, 78.125, 78.325, 78.11666666666666, 78.68333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 0.761, Test loss: 2.242, Test accuracy: 20.32
Round   1, Train loss: 0.702, Test loss: 2.602, Test accuracy: 29.43
Round   2, Train loss: 0.604, Test loss: 2.125, Test accuracy: 35.89
Round   3, Train loss: 0.562, Test loss: 2.191, Test accuracy: 38.76
Round   4, Train loss: 0.573, Test loss: 2.094, Test accuracy: 38.94
Round   5, Train loss: 0.552, Test loss: 1.939, Test accuracy: 44.19
Round   6, Train loss: 0.539, Test loss: 1.919, Test accuracy: 45.48
Round   7, Train loss: 0.434, Test loss: 1.912, Test accuracy: 45.54
Round   8, Train loss: 0.464, Test loss: 1.836, Test accuracy: 47.80
Round   9, Train loss: 0.386, Test loss: 1.791, Test accuracy: 50.93
Round  10, Train loss: 0.410, Test loss: 1.774, Test accuracy: 50.23
Round  11, Train loss: 0.422, Test loss: 1.746, Test accuracy: 48.32
Round  12, Train loss: 0.415, Test loss: 1.714, Test accuracy: 51.12
Round  13, Train loss: 0.353, Test loss: 1.668, Test accuracy: 52.62
Round  14, Train loss: 0.330, Test loss: 1.652, Test accuracy: 51.79
Round  15, Train loss: 0.373, Test loss: 1.627, Test accuracy: 51.38
Round  16, Train loss: 0.337, Test loss: 1.597, Test accuracy: 51.94
Round  17, Train loss: 0.333, Test loss: 1.590, Test accuracy: 51.45
Round  18, Train loss: 0.348, Test loss: 1.555, Test accuracy: 51.99
Round  19, Train loss: 0.352, Test loss: 1.529, Test accuracy: 53.39
Round  20, Train loss: 0.333, Test loss: 1.501, Test accuracy: 53.04
Round  21, Train loss: 0.263, Test loss: 1.471, Test accuracy: 54.76
Round  22, Train loss: 0.350, Test loss: 1.464, Test accuracy: 55.12
Round  23, Train loss: 0.354, Test loss: 1.425, Test accuracy: 56.44
Round  24, Train loss: 0.292, Test loss: 1.415, Test accuracy: 56.60
Round  25, Train loss: 0.289, Test loss: 1.378, Test accuracy: 59.00
Round  26, Train loss: 0.325, Test loss: 1.363, Test accuracy: 58.26
Round  27, Train loss: 0.272, Test loss: 1.340, Test accuracy: 58.32
Round  28, Train loss: 0.248, Test loss: 1.325, Test accuracy: 58.24
Round  29, Train loss: 0.244, Test loss: 1.291, Test accuracy: 59.59
Round  30, Train loss: 0.257, Test loss: 1.279, Test accuracy: 59.22
Round  31, Train loss: 0.240, Test loss: 1.245, Test accuracy: 61.45
Round  32, Train loss: 0.258, Test loss: 1.220, Test accuracy: 62.61
Round  33, Train loss: 0.220, Test loss: 1.211, Test accuracy: 61.92
Round  34, Train loss: 0.178, Test loss: 1.184, Test accuracy: 62.32
Round  35, Train loss: 0.193, Test loss: 1.189, Test accuracy: 61.42
Round  36, Train loss: 0.179, Test loss: 1.176, Test accuracy: 62.42
Round  37, Train loss: 0.173, Test loss: 1.148, Test accuracy: 62.58
Round  38, Train loss: 0.192, Test loss: 1.132, Test accuracy: 62.70
Round  39, Train loss: 0.179, Test loss: 1.119, Test accuracy: 62.77
Round  40, Train loss: 0.179, Test loss: 1.104, Test accuracy: 62.94
Round  41, Train loss: 0.169, Test loss: 1.082, Test accuracy: 64.09
Round  42, Train loss: 0.150, Test loss: 1.057, Test accuracy: 64.58
Round  43, Train loss: 0.214, Test loss: 1.058, Test accuracy: 63.81
Round  44, Train loss: 0.141, Test loss: 1.057, Test accuracy: 63.51
Round  45, Train loss: 0.154, Test loss: 1.048, Test accuracy: 63.53
Round  46, Train loss: 0.146, Test loss: 1.036, Test accuracy: 64.96
Round  47, Train loss: 0.168, Test loss: 1.018, Test accuracy: 65.19
Round  48, Train loss: 0.127, Test loss: 0.994, Test accuracy: 66.45
Round  49, Train loss: 0.134, Test loss: 0.996, Test accuracy: 66.07
Round  50, Train loss: 0.166, Test loss: 0.974, Test accuracy: 66.83
Round  51, Train loss: 0.155, Test loss: 0.961, Test accuracy: 67.06
Round  52, Train loss: 0.124, Test loss: 0.950, Test accuracy: 67.06
Round  53, Train loss: 0.116, Test loss: 0.946, Test accuracy: 67.28
Round  54, Train loss: 0.130, Test loss: 0.941, Test accuracy: 65.89
Round  55, Train loss: 0.138, Test loss: 0.938, Test accuracy: 65.89
Round  56, Train loss: 0.106, Test loss: 0.930, Test accuracy: 66.72
Round  57, Train loss: 0.105, Test loss: 0.914, Test accuracy: 66.90
Round  58, Train loss: 0.118, Test loss: 0.915, Test accuracy: 66.71
Round  59, Train loss: 0.121, Test loss: 0.906, Test accuracy: 66.59
Round  60, Train loss: 0.105, Test loss: 0.891, Test accuracy: 66.97
Round  61, Train loss: 0.098, Test loss: 0.891, Test accuracy: 66.31
Round  62, Train loss: 0.110, Test loss: 0.876, Test accuracy: 66.46
Round  63, Train loss: 0.080, Test loss: 0.875, Test accuracy: 66.54
Round  64, Train loss: 0.102, Test loss: 0.873, Test accuracy: 66.47
Round  65, Train loss: 0.075, Test loss: 0.858, Test accuracy: 66.85
Round  66, Train loss: 0.105, Test loss: 0.849, Test accuracy: 66.67
Round  67, Train loss: 0.116, Test loss: 0.849, Test accuracy: 66.71
Round  68, Train loss: 0.079, Test loss: 0.838, Test accuracy: 67.62
Round  69, Train loss: 0.068, Test loss: 0.826, Test accuracy: 68.37
Round  70, Train loss: 0.072, Test loss: 0.835, Test accuracy: 67.17
Round  71, Train loss: 0.112, Test loss: 0.830, Test accuracy: 67.41
Round  72, Train loss: 0.116, Test loss: 0.828, Test accuracy: 67.22
Round  73, Train loss: 0.090, Test loss: 0.818, Test accuracy: 67.56
Round  74, Train loss: 0.068, Test loss: 0.799, Test accuracy: 68.92
Round  75, Train loss: 0.057, Test loss: 0.796, Test accuracy: 68.53
Round  76, Train loss: 0.099, Test loss: 0.798, Test accuracy: 68.08
Round  77, Train loss: 0.073, Test loss: 0.788, Test accuracy: 68.73
Round  78, Train loss: 0.073, Test loss: 0.791, Test accuracy: 68.14
Round  79, Train loss: 0.094, Test loss: 0.789, Test accuracy: 68.53
Round  80, Train loss: 0.075, Test loss: 0.784, Test accuracy: 68.94
Round  81, Train loss: 0.058, Test loss: 0.778, Test accuracy: 69.47
Round  82, Train loss: 0.090, Test loss: 0.775, Test accuracy: 69.12
Round  83, Train loss: 0.090, Test loss: 0.781, Test accuracy: 68.41
Round  84, Train loss: 0.074, Test loss: 0.781, Test accuracy: 67.90
Round  85, Train loss: 0.073, Test loss: 0.787, Test accuracy: 67.03
Round  86, Train loss: 0.072, Test loss: 0.779, Test accuracy: 68.19
Round  87, Train loss: 0.065, Test loss: 0.767, Test accuracy: 68.19
Round  88, Train loss: 0.080, Test loss: 0.779, Test accuracy: 67.53
Round  89, Train loss: 0.077, Test loss: 0.784, Test accuracy: 67.04
Round  90, Train loss: 0.061, Test loss: 0.773, Test accuracy: 67.53
Round  91, Train loss: 0.064, Test loss: 0.767, Test accuracy: 68.50
Round  92, Train loss: 0.063, Test loss: 0.758, Test accuracy: 68.92
Round  93, Train loss: 0.064, Test loss: 0.760, Test accuracy: 68.68
Round  94, Train loss: 0.063, Test loss: 0.760, Test accuracy: 68.91
Round  95, Train loss: 0.081, Test loss: 0.780, Test accuracy: 67.04
Round  96, Train loss: 0.059, Test loss: 0.757, Test accuracy: 68.12
Round  97, Train loss: 0.068, Test loss: 0.771, Test accuracy: 67.28
Round  98, Train loss: 0.064, Test loss: 0.765, Test accuracy: 67.74
Round  99, Train loss: 0.058, Test loss: 0.770, Test accuracy: 67.65
Final Round, Train loss: 0.060, Test loss: 0.750, Test accuracy: 67.88
Average accuracy final 10 rounds: 68.03666666666666
1844.355268239975
[]
[20.316666666666666, 29.425, 35.891666666666666, 38.75833333333333, 38.94166666666667, 44.19166666666667, 45.475, 45.541666666666664, 47.8, 50.93333333333333, 50.233333333333334, 48.31666666666667, 51.11666666666667, 52.61666666666667, 51.791666666666664, 51.375, 51.94166666666667, 51.45, 51.99166666666667, 53.391666666666666, 53.041666666666664, 54.75833333333333, 55.125, 56.44166666666667, 56.6, 59.0, 58.25833333333333, 58.31666666666667, 58.24166666666667, 59.59166666666667, 59.21666666666667, 61.45, 62.608333333333334, 61.925, 62.31666666666667, 61.416666666666664, 62.416666666666664, 62.583333333333336, 62.7, 62.766666666666666, 62.94166666666667, 64.09166666666667, 64.58333333333333, 63.80833333333333, 63.50833333333333, 63.53333333333333, 64.95833333333333, 65.19166666666666, 66.45, 66.06666666666666, 66.83333333333333, 67.05833333333334, 67.05833333333334, 67.275, 65.89166666666667, 65.89166666666667, 66.71666666666667, 66.9, 66.70833333333333, 66.59166666666667, 66.975, 66.30833333333334, 66.45833333333333, 66.54166666666667, 66.46666666666667, 66.85, 66.675, 66.70833333333333, 67.625, 68.36666666666666, 67.16666666666667, 67.40833333333333, 67.225, 67.55833333333334, 68.91666666666667, 68.53333333333333, 68.08333333333333, 68.73333333333333, 68.14166666666667, 68.525, 68.94166666666666, 69.475, 69.11666666666666, 68.40833333333333, 67.9, 67.025, 68.19166666666666, 68.19166666666666, 67.53333333333333, 67.04166666666667, 67.525, 68.5, 68.91666666666667, 68.68333333333334, 68.90833333333333, 67.04166666666667, 68.125, 67.275, 67.74166666666666, 67.65, 67.875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.018, Test loss: 1.859, Test accuracy: 24.07
Round   0: Global train loss: 1.018, Global test loss: 2.304, Global test accuracy: 10.00
Round   1, Train loss: 1.002, Test loss: 1.513, Test accuracy: 35.27
Round   1: Global train loss: 1.002, Global test loss: 2.298, Global test accuracy: 10.00
Round   2, Train loss: 0.773, Test loss: 1.286, Test accuracy: 45.57
Round   2: Global train loss: 0.773, Global test loss: 2.290, Global test accuracy: 9.96
Round   3, Train loss: 0.613, Test loss: 1.159, Test accuracy: 49.77
Round   3: Global train loss: 0.613, Global test loss: 2.281, Global test accuracy: 10.80
Round   4, Train loss: 0.500, Test loss: 1.075, Test accuracy: 53.92
Round   4: Global train loss: 0.500, Global test loss: 2.269, Global test accuracy: 14.70
Round   5, Train loss: 0.784, Test loss: 1.116, Test accuracy: 52.02
Round   5: Global train loss: 0.784, Global test loss: 2.262, Global test accuracy: 15.72
Round   6, Train loss: 0.180, Test loss: 1.012, Test accuracy: 56.87
Round   6: Global train loss: 0.180, Global test loss: 2.249, Global test accuracy: 18.71
Round   7, Train loss: 0.444, Test loss: 0.910, Test accuracy: 60.47
Round   7: Global train loss: 0.444, Global test loss: 2.235, Global test accuracy: 18.95
Round   8, Train loss: -0.329, Test loss: 0.907, Test accuracy: 60.58
Round   8: Global train loss: -0.329, Global test loss: 2.226, Global test accuracy: 19.37
Round   9, Train loss: 0.626, Test loss: 0.902, Test accuracy: 61.13
Round   9: Global train loss: 0.626, Global test loss: 2.217, Global test accuracy: 20.09
Round  10, Train loss: -0.010, Test loss: 0.834, Test accuracy: 62.95
Round  10: Global train loss: -0.010, Global test loss: 2.209, Global test accuracy: 19.69
Round  11, Train loss: -0.292, Test loss: 0.829, Test accuracy: 62.96
Round  11: Global train loss: -0.292, Global test loss: 2.203, Global test accuracy: 19.38
Round  12, Train loss: 0.225, Test loss: 0.805, Test accuracy: 64.16
Round  12: Global train loss: 0.225, Global test loss: 2.196, Global test accuracy: 19.37
Round  13, Train loss: -0.920, Test loss: 0.811, Test accuracy: 64.18
Round  13: Global train loss: -0.920, Global test loss: 2.188, Global test accuracy: 18.56
Round  14, Train loss: -0.487, Test loss: 0.795, Test accuracy: 64.95
Round  14: Global train loss: -0.487, Global test loss: 2.187, Global test accuracy: 18.36
Round  15, Train loss: -0.156, Test loss: 0.793, Test accuracy: 65.15
Round  15: Global train loss: -0.156, Global test loss: 2.186, Global test accuracy: 18.21
Round  16, Train loss: -1.292, Test loss: 0.773, Test accuracy: 66.15
Round  16: Global train loss: -1.292, Global test loss: 2.179, Global test accuracy: 18.17
Round  17, Train loss: -1.115, Test loss: 0.743, Test accuracy: 67.34
Round  17: Global train loss: -1.115, Global test loss: 2.172, Global test accuracy: 17.89
Round  18, Train loss: -1.297, Test loss: 0.754, Test accuracy: 67.31
Round  18: Global train loss: -1.297, Global test loss: 2.171, Global test accuracy: 17.88
Round  19, Train loss: -1.254, Test loss: 0.761, Test accuracy: 67.41
Round  19: Global train loss: -1.254, Global test loss: 2.163, Global test accuracy: 17.80
Round  20, Train loss: -1.250, Test loss: 0.759, Test accuracy: 67.87
Round  20: Global train loss: -1.250, Global test loss: 2.161, Global test accuracy: 17.71
Round  21, Train loss: -1.513, Test loss: 0.737, Test accuracy: 68.28
Round  21: Global train loss: -1.513, Global test loss: 2.160, Global test accuracy: 17.77
Round  22, Train loss: -1.431, Test loss: 0.715, Test accuracy: 69.26
Round  22: Global train loss: -1.431, Global test loss: 2.150, Global test accuracy: 17.72
Round  23, Train loss: -1.200, Test loss: 0.696, Test accuracy: 69.90
Round  23: Global train loss: -1.200, Global test loss: 2.145, Global test accuracy: 17.77
Round  24, Train loss: -2.304, Test loss: 0.695, Test accuracy: 70.05
Round  24: Global train loss: -2.304, Global test loss: 2.135, Global test accuracy: 18.63
Round  25, Train loss: -1.620, Test loss: 0.694, Test accuracy: 70.54
Round  25: Global train loss: -1.620, Global test loss: 2.136, Global test accuracy: 18.73
Round  26, Train loss: -1.909, Test loss: 0.686, Test accuracy: 71.03
Round  26: Global train loss: -1.909, Global test loss: 2.129, Global test accuracy: 18.70
Round  27, Train loss: -1.922, Test loss: 0.704, Test accuracy: 70.12
Round  27: Global train loss: -1.922, Global test loss: 2.127, Global test accuracy: 18.91
Round  28, Train loss: -1.661, Test loss: 0.677, Test accuracy: 71.77
Round  28: Global train loss: -1.661, Global test loss: 2.122, Global test accuracy: 19.41
Round  29, Train loss: -2.358, Test loss: 0.671, Test accuracy: 71.78
Round  29: Global train loss: -2.358, Global test loss: 2.114, Global test accuracy: 19.42
Round  30, Train loss: -1.818, Test loss: 0.681, Test accuracy: 71.27
Round  30: Global train loss: -1.818, Global test loss: 2.108, Global test accuracy: 19.34
Round  31, Train loss: -2.360, Test loss: 0.699, Test accuracy: 70.50
Round  31: Global train loss: -2.360, Global test loss: 2.102, Global test accuracy: 19.52
Round  32, Train loss: -2.703, Test loss: 0.695, Test accuracy: 70.62
Round  32: Global train loss: -2.703, Global test loss: 2.097, Global test accuracy: 19.59
Round  33, Train loss: -2.482, Test loss: 0.698, Test accuracy: 70.55
Round  33: Global train loss: -2.482, Global test loss: 2.090, Global test accuracy: 19.50
Round  34, Train loss: -2.127, Test loss: 0.698, Test accuracy: 70.60
Round  34: Global train loss: -2.127, Global test loss: 2.086, Global test accuracy: 19.57
Round  35, Train loss: -2.219, Test loss: 0.685, Test accuracy: 71.35
Round  35: Global train loss: -2.219, Global test loss: 2.076, Global test accuracy: 20.12
Round  36, Train loss: -2.743, Test loss: 0.687, Test accuracy: 71.42
Round  36: Global train loss: -2.743, Global test loss: 2.066, Global test accuracy: 20.23
Round  37, Train loss: -2.849, Test loss: 0.695, Test accuracy: 71.02
Round  37: Global train loss: -2.849, Global test loss: 2.061, Global test accuracy: 20.95
Round  38, Train loss: -2.538, Test loss: 0.674, Test accuracy: 72.10
Round  38: Global train loss: -2.538, Global test loss: 2.050, Global test accuracy: 22.14
Round  39, Train loss: -2.620, Test loss: 0.659, Test accuracy: 72.25
Round  39: Global train loss: -2.620, Global test loss: 2.043, Global test accuracy: 24.40
Round  40, Train loss: -3.129, Test loss: 0.667, Test accuracy: 72.24
Round  40: Global train loss: -3.129, Global test loss: 2.031, Global test accuracy: 25.86
Round  41, Train loss: -3.973, Test loss: 0.677, Test accuracy: 72.25
Round  41: Global train loss: -3.973, Global test loss: 2.024, Global test accuracy: 27.19
Round  42, Train loss: -3.409, Test loss: 0.685, Test accuracy: 72.26
Round  42: Global train loss: -3.409, Global test loss: 2.016, Global test accuracy: 27.82
Round  43, Train loss: -2.696, Test loss: 0.685, Test accuracy: 71.73
Round  43: Global train loss: -2.696, Global test loss: 2.012, Global test accuracy: 27.52
Round  44, Train loss: -2.990, Test loss: 0.669, Test accuracy: 71.70
Round  44: Global train loss: -2.990, Global test loss: 2.009, Global test accuracy: 27.06
Round  45, Train loss: -3.166, Test loss: 0.652, Test accuracy: 73.07
Round  45: Global train loss: -3.166, Global test loss: 1.995, Global test accuracy: 28.88
Round  46, Train loss: -3.380, Test loss: 0.643, Test accuracy: 73.42
Round  46: Global train loss: -3.380, Global test loss: 1.983, Global test accuracy: 30.30
Round  47, Train loss: -3.182, Test loss: 0.656, Test accuracy: 72.88
Round  47: Global train loss: -3.182, Global test loss: 1.978, Global test accuracy: 31.20
Round  48, Train loss: -3.409, Test loss: 0.670, Test accuracy: 72.42
Round  48: Global train loss: -3.409, Global test loss: 1.974, Global test accuracy: 30.83
Round  49, Train loss: -2.979, Test loss: 0.659, Test accuracy: 72.88
Round  49: Global train loss: -2.979, Global test loss: 1.970, Global test accuracy: 31.48
Round  50, Train loss: -3.073, Test loss: 0.660, Test accuracy: 73.33
Round  50: Global train loss: -3.073, Global test loss: 1.963, Global test accuracy: 32.69
Round  51, Train loss: -3.787, Test loss: 0.639, Test accuracy: 74.27
Round  51: Global train loss: -3.787, Global test loss: 1.951, Global test accuracy: 33.33
Round  52, Train loss: -3.700, Test loss: 0.643, Test accuracy: 74.33
Round  52: Global train loss: -3.700, Global test loss: 1.948, Global test accuracy: 32.63
Round  53, Train loss: -3.168, Test loss: 0.646, Test accuracy: 74.00
Round  53: Global train loss: -3.168, Global test loss: 1.941, Global test accuracy: 34.09
Round  54, Train loss: -4.757, Test loss: 0.649, Test accuracy: 74.13
Round  54: Global train loss: -4.757, Global test loss: 1.930, Global test accuracy: 34.84
Round  55, Train loss: -3.031, Test loss: 0.646, Test accuracy: 74.09
Round  55: Global train loss: -3.031, Global test loss: 1.922, Global test accuracy: 35.48
Round  56, Train loss: -3.657, Test loss: 0.666, Test accuracy: 73.77
Round  56: Global train loss: -3.657, Global test loss: 1.916, Global test accuracy: 35.74
Round  57, Train loss: -3.876, Test loss: 0.654, Test accuracy: 74.28
Round  57: Global train loss: -3.876, Global test loss: 1.916, Global test accuracy: 35.47
Round  58, Train loss: -3.903, Test loss: 0.641, Test accuracy: 74.24
Round  58: Global train loss: -3.903, Global test loss: 1.908, Global test accuracy: 36.23
Round  59, Train loss: -4.234, Test loss: 0.626, Test accuracy: 75.02
Round  59: Global train loss: -4.234, Global test loss: 1.900, Global test accuracy: 36.50
Round  60, Train loss: -4.289, Test loss: 0.613, Test accuracy: 75.60
Round  60: Global train loss: -4.289, Global test loss: 1.892, Global test accuracy: 36.38
Round  61, Train loss: -4.446, Test loss: 0.632, Test accuracy: 75.07
Round  61: Global train loss: -4.446, Global test loss: 1.886, Global test accuracy: 36.39
Round  62, Train loss: -4.109, Test loss: 0.638, Test accuracy: 74.48
Round  62: Global train loss: -4.109, Global test loss: 1.883, Global test accuracy: 36.00
Round  63, Train loss: -4.796, Test loss: 0.648, Test accuracy: 74.58
Round  63: Global train loss: -4.796, Global test loss: 1.873, Global test accuracy: 37.00
Round  64, Train loss: -4.263, Test loss: 0.639, Test accuracy: 74.99
Round  64: Global train loss: -4.263, Global test loss: 1.870, Global test accuracy: 35.82
Round  65, Train loss: -4.665, Test loss: 0.641, Test accuracy: 74.86
Round  65: Global train loss: -4.665, Global test loss: 1.860, Global test accuracy: 36.48
Round  66, Train loss: -3.700, Test loss: 0.628, Test accuracy: 75.35
Round  66: Global train loss: -3.700, Global test loss: 1.855, Global test accuracy: 37.04
Round  67, Train loss: -4.526, Test loss: 0.641, Test accuracy: 75.17
Round  67: Global train loss: -4.526, Global test loss: 1.853, Global test accuracy: 36.77
Round  68, Train loss: -4.411, Test loss: 0.639, Test accuracy: 75.20
Round  68: Global train loss: -4.411, Global test loss: 1.847, Global test accuracy: 36.74
Round  69, Train loss: -4.041, Test loss: 0.631, Test accuracy: 75.42
Round  69: Global train loss: -4.041, Global test loss: 1.843, Global test accuracy: 37.45
Round  70, Train loss: -4.792, Test loss: 0.617, Test accuracy: 75.53
Round  70: Global train loss: -4.792, Global test loss: 1.828, Global test accuracy: 38.07
Round  71, Train loss: -4.748, Test loss: 0.643, Test accuracy: 74.43
Round  71: Global train loss: -4.748, Global test loss: 1.821, Global test accuracy: 38.35
Round  72, Train loss: -5.608, Test loss: 0.639, Test accuracy: 74.93
Round  72: Global train loss: -5.608, Global test loss: 1.811, Global test accuracy: 38.20
Round  73, Train loss: -4.453, Test loss: 0.635, Test accuracy: 75.46
Round  73: Global train loss: -4.453, Global test loss: 1.803, Global test accuracy: 38.93
Round  74, Train loss: -4.181, Test loss: 0.625, Test accuracy: 75.24
Round  74: Global train loss: -4.181, Global test loss: 1.796, Global test accuracy: 39.35
Round  75, Train loss: -4.290, Test loss: 0.630, Test accuracy: 75.35
Round  75: Global train loss: -4.290, Global test loss: 1.792, Global test accuracy: 39.45
Round  76, Train loss: -4.483, Test loss: 0.618, Test accuracy: 75.92
Round  76: Global train loss: -4.483, Global test loss: 1.785, Global test accuracy: 39.67
Round  77, Train loss: -5.115, Test loss: 0.617, Test accuracy: 75.90
Round  77: Global train loss: -5.115, Global test loss: 1.781, Global test accuracy: 39.11
Round  78, Train loss: -5.346, Test loss: 0.638, Test accuracy: 75.23
Round  78: Global train loss: -5.346, Global test loss: 1.779, Global test accuracy: 38.94
Round  79, Train loss: -4.348, Test loss: 0.638, Test accuracy: 75.13
Round  79: Global train loss: -4.348, Global test loss: 1.768, Global test accuracy: 39.42
Round  80, Train loss: -5.114, Test loss: 0.633, Test accuracy: 75.88
Round  80: Global train loss: -5.114, Global test loss: 1.760, Global test accuracy: 39.51
Round  81, Train loss: -4.982, Test loss: 0.641, Test accuracy: 75.60
Round  81: Global train loss: -4.982, Global test loss: 1.757, Global test accuracy: 39.93
Round  82, Train loss: -4.979, Test loss: 0.646, Test accuracy: 75.45
Round  82: Global train loss: -4.979, Global test loss: 1.749, Global test accuracy: 40.07
Round  83, Train loss: -4.402, Test loss: 0.647, Test accuracy: 75.28
Round  83: Global train loss: -4.402, Global test loss: 1.742, Global test accuracy: 40.64
Round  84, Train loss: -4.854, Test loss: 0.647, Test accuracy: 75.23
Round  84: Global train loss: -4.854, Global test loss: 1.736, Global test accuracy: 40.73
Round  85, Train loss: -4.839, Test loss: 0.647, Test accuracy: 75.42
Round  85: Global train loss: -4.839, Global test loss: 1.728, Global test accuracy: 40.67
Round  86, Train loss: -4.420, Test loss: 0.663, Test accuracy: 74.69
Round  86: Global train loss: -4.420, Global test loss: 1.724, Global test accuracy: 40.18
Round  87, Train loss: -4.314, Test loss: 0.657, Test accuracy: 74.68
Round  87: Global train loss: -4.314, Global test loss: 1.719, Global test accuracy: 40.31
Round  88, Train loss: -5.319, Test loss: 0.663, Test accuracy: 75.00
Round  88: Global train loss: -5.319, Global test loss: 1.715, Global test accuracy: 40.51
Round  89, Train loss: -5.251, Test loss: 0.631, Test accuracy: 75.62
Round  89: Global train loss: -5.251, Global test loss: 1.713, Global test accuracy: 41.02
Round  90, Train loss: -4.837, Test loss: 0.651, Test accuracy: 75.16
Round  90: Global train loss: -4.837, Global test loss: 1.711, Global test accuracy: 40.72
Round  91, Train loss: -4.830, Test loss: 0.642, Test accuracy: 75.46
Round  91: Global train loss: -4.830, Global test loss: 1.709, Global test accuracy: 40.85
Round  92, Train loss: -4.665, Test loss: 0.634, Test accuracy: 75.63
Round  92: Global train loss: -4.665, Global test loss: 1.703, Global test accuracy: 40.23
Round  93, Train loss: -4.753, Test loss: 0.636, Test accuracy: 75.36
Round  93: Global train loss: -4.753, Global test loss: 1.697, Global test accuracy: 40.73
Round  94, Train loss: -4.898, Test loss: 0.636, Test accuracy: 75.47
Round  94: Global train loss: -4.898, Global test loss: 1.691, Global test accuracy: 40.83
Round  95, Train loss: -5.134, Test loss: 0.643, Test accuracy: 75.54
Round  95: Global train loss: -5.134, Global test loss: 1.688, Global test accuracy: 40.62
Round  96, Train loss: -4.568, Test loss: 0.632, Test accuracy: 75.98
Round  96: Global train loss: -4.568, Global test loss: 1.688, Global test accuracy: 40.83
Round  97, Train loss: -4.665, Test loss: 0.638, Test accuracy: 75.88
Round  97: Global train loss: -4.665, Global test loss: 1.685, Global test accuracy: 40.98
Round  98, Train loss: -3.919, Test loss: 0.637, Test accuracy: 75.54
Round  98: Global train loss: -3.919, Global test loss: 1.685, Global test accuracy: 41.45
Round  99, Train loss: -4.199, Test loss: 0.632, Test accuracy: 75.89
Round  99: Global train loss: -4.199, Global test loss: 1.681, Global test accuracy: 41.92
Final Round: Train loss: 0.621, Test loss: 0.573, Test accuracy: 76.53
Final Round: Global train loss: 0.621, Global test loss: 1.668, Global test accuracy: 41.84
Average accuracy final 10 rounds: 75.59166666666667
Average global accuracy final 10 rounds: 40.91333333333334
1685.766669511795
[]
[24.075, 35.275, 45.56666666666667, 49.775, 53.925, 52.016666666666666, 56.86666666666667, 60.46666666666667, 60.575, 61.13333333333333, 62.95, 62.958333333333336, 64.15833333333333, 64.18333333333334, 64.95, 65.15, 66.15, 67.34166666666667, 67.30833333333334, 67.40833333333333, 67.86666666666666, 68.28333333333333, 69.25833333333334, 69.9, 70.05, 70.54166666666667, 71.025, 70.125, 71.76666666666667, 71.775, 71.26666666666667, 70.5, 70.61666666666666, 70.55, 70.6, 71.35, 71.425, 71.01666666666667, 72.1, 72.25, 72.24166666666666, 72.25, 72.25833333333334, 71.73333333333333, 71.7, 73.06666666666666, 73.425, 72.875, 72.41666666666667, 72.88333333333334, 73.325, 74.26666666666667, 74.33333333333333, 74.0, 74.13333333333334, 74.09166666666667, 73.76666666666667, 74.28333333333333, 74.24166666666666, 75.01666666666667, 75.6, 75.06666666666666, 74.48333333333333, 74.58333333333333, 74.99166666666666, 74.85833333333333, 75.35, 75.175, 75.2, 75.41666666666667, 75.525, 74.43333333333334, 74.93333333333334, 75.45833333333333, 75.24166666666666, 75.35, 75.925, 75.9, 75.23333333333333, 75.13333333333334, 75.88333333333334, 75.6, 75.45, 75.28333333333333, 75.23333333333333, 75.425, 74.69166666666666, 74.68333333333334, 75.0, 75.625, 75.15833333333333, 75.45833333333333, 75.63333333333334, 75.35833333333333, 75.46666666666667, 75.54166666666667, 75.98333333333333, 75.88333333333334, 75.54166666666667, 75.89166666666667, 76.53333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.61 

Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.60 

Round   1, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.57 

Round   1, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 10.39 

Round   2, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.79 

Round   2, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.73 

Round   3, Train loss: 2.303, Test loss: 2.301, Test accuracy: 10.74 

Round   3, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 10.62 

Round   4, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.76 

Round   4, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.65 

Round   5, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.04 

Round   5, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.07 

Round   6, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.90 

Round   6, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.66 

Round   7, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.82 

Round   7, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.54 

Round   8, Train loss: 2.301, Test loss: 2.300, Test accuracy: 10.83 

Round   8, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.60 

Round   9, Train loss: 2.298, Test loss: 2.299, Test accuracy: 10.65 

Round   9, Global train loss: 2.298, Global test loss: 2.299, Global test accuracy: 10.54 

Round  10, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.61 

Round  10, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.73 

Round  11, Train loss: 2.299, Test loss: 2.299, Test accuracy: 10.59 

Round  11, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 10.56 

Round  12, Train loss: 2.298, Test loss: 2.299, Test accuracy: 10.62 

Round  12, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 10.52 

Round  13, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.75 

Round  13, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.73 

Round  14, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.75 

Round  14, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.76 

Round  15, Train loss: 2.299, Test loss: 2.297, Test accuracy: 10.90 

Round  15, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.97 

Round  16, Train loss: 2.298, Test loss: 2.297, Test accuracy: 11.01 

Round  16, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 11.51 

Round  17, Train loss: 2.299, Test loss: 2.297, Test accuracy: 11.06 

Round  17, Global train loss: 2.299, Global test loss: 2.296, Global test accuracy: 11.11 

Round  18, Train loss: 2.298, Test loss: 2.296, Test accuracy: 11.29 

Round  18, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 11.38 

Round  19, Train loss: 2.297, Test loss: 2.296, Test accuracy: 11.24 

Round  19, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 11.36 

Round  20, Train loss: 2.297, Test loss: 2.296, Test accuracy: 11.21 

Round  20, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 11.28 

Round  21, Train loss: 2.296, Test loss: 2.295, Test accuracy: 11.24 

Round  21, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 11.35 

Round  22, Train loss: 2.296, Test loss: 2.295, Test accuracy: 11.19 

Round  22, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 11.19 

Round  23, Train loss: 2.296, Test loss: 2.295, Test accuracy: 11.26 

Round  23, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 11.35 

Round  24, Train loss: 2.296, Test loss: 2.294, Test accuracy: 11.26 

Round  24, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 11.35 

Round  25, Train loss: 2.295, Test loss: 2.293, Test accuracy: 11.28 

Round  25, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 11.39 

Round  26, Train loss: 2.292, Test loss: 2.293, Test accuracy: 11.29 

Round  26, Global train loss: 2.292, Global test loss: 2.292, Global test accuracy: 11.43 

Round  27, Train loss: 2.293, Test loss: 2.293, Test accuracy: 11.40 

Round  27, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 11.28 

Round  28, Train loss: 2.293, Test loss: 2.292, Test accuracy: 11.33 

Round  28, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 11.41 

Round  29, Train loss: 2.291, Test loss: 2.292, Test accuracy: 11.18 

Round  29, Global train loss: 2.291, Global test loss: 2.290, Global test accuracy: 11.15 

Round  30, Train loss: 2.294, Test loss: 2.291, Test accuracy: 11.15 

Round  30, Global train loss: 2.294, Global test loss: 2.290, Global test accuracy: 10.94 

Round  31, Train loss: 2.291, Test loss: 2.290, Test accuracy: 11.27 

Round  31, Global train loss: 2.291, Global test loss: 2.289, Global test accuracy: 11.25 

Round  32, Train loss: 2.292, Test loss: 2.290, Test accuracy: 11.46 

Round  32, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 11.24 

Round  33, Train loss: 2.290, Test loss: 2.290, Test accuracy: 11.52 

Round  33, Global train loss: 2.290, Global test loss: 2.288, Global test accuracy: 11.63 

Round  34, Train loss: 2.291, Test loss: 2.289, Test accuracy: 11.60 

Round  34, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 11.48 

Round  35, Train loss: 2.289, Test loss: 2.289, Test accuracy: 11.58 

Round  35, Global train loss: 2.289, Global test loss: 2.287, Global test accuracy: 11.34 

Round  36, Train loss: 2.292, Test loss: 2.288, Test accuracy: 11.61 

Round  36, Global train loss: 2.292, Global test loss: 2.287, Global test accuracy: 11.34 

Round  37, Train loss: 2.289, Test loss: 2.287, Test accuracy: 11.64 

Round  37, Global train loss: 2.289, Global test loss: 2.286, Global test accuracy: 11.71 

Round  38, Train loss: 2.290, Test loss: 2.287, Test accuracy: 11.88 

Round  38, Global train loss: 2.290, Global test loss: 2.286, Global test accuracy: 12.15 

Round  39, Train loss: 2.291, Test loss: 2.286, Test accuracy: 11.94 

Round  39, Global train loss: 2.291, Global test loss: 2.285, Global test accuracy: 12.19 

Round  40, Train loss: 2.287, Test loss: 2.286, Test accuracy: 12.08 

Round  40, Global train loss: 2.287, Global test loss: 2.284, Global test accuracy: 12.41 

Round  41, Train loss: 2.288, Test loss: 2.285, Test accuracy: 12.13 

Round  41, Global train loss: 2.288, Global test loss: 2.283, Global test accuracy: 12.04 

Round  42, Train loss: 2.287, Test loss: 2.285, Test accuracy: 11.84 

Round  42, Global train loss: 2.287, Global test loss: 2.283, Global test accuracy: 12.38 

Round  43, Train loss: 2.288, Test loss: 2.284, Test accuracy: 11.86 

Round  43, Global train loss: 2.288, Global test loss: 2.282, Global test accuracy: 12.24 

Round  44, Train loss: 2.290, Test loss: 2.283, Test accuracy: 11.88 

Round  44, Global train loss: 2.290, Global test loss: 2.282, Global test accuracy: 12.29 

Round  45, Train loss: 2.287, Test loss: 2.283, Test accuracy: 12.37 

Round  45, Global train loss: 2.287, Global test loss: 2.281, Global test accuracy: 12.72 

Round  46, Train loss: 2.287, Test loss: 2.282, Test accuracy: 12.45 

Round  46, Global train loss: 2.287, Global test loss: 2.280, Global test accuracy: 13.30 

Round  47, Train loss: 2.287, Test loss: 2.281, Test accuracy: 12.44 

Round  47, Global train loss: 2.287, Global test loss: 2.279, Global test accuracy: 12.87 

Round  48, Train loss: 2.286, Test loss: 2.281, Test accuracy: 12.65 

Round  48, Global train loss: 2.286, Global test loss: 2.279, Global test accuracy: 13.56 

Round  49, Train loss: 2.284, Test loss: 2.280, Test accuracy: 13.18 

Round  49, Global train loss: 2.284, Global test loss: 2.279, Global test accuracy: 14.11 

Round  50, Train loss: 2.281, Test loss: 2.279, Test accuracy: 13.79 

Round  50, Global train loss: 2.281, Global test loss: 2.278, Global test accuracy: 14.15 

Round  51, Train loss: 2.280, Test loss: 2.278, Test accuracy: 13.46 

Round  51, Global train loss: 2.280, Global test loss: 2.277, Global test accuracy: 13.59 

Round  52, Train loss: 2.284, Test loss: 2.277, Test accuracy: 13.16 

Round  52, Global train loss: 2.284, Global test loss: 2.276, Global test accuracy: 13.54 

Round  53, Train loss: 2.280, Test loss: 2.277, Test accuracy: 13.24 

Round  53, Global train loss: 2.280, Global test loss: 2.275, Global test accuracy: 13.59 

Round  54, Train loss: 2.282, Test loss: 2.276, Test accuracy: 13.09 

Round  54, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 13.20 

Round  55, Train loss: 2.280, Test loss: 2.276, Test accuracy: 13.10 

Round  55, Global train loss: 2.280, Global test loss: 2.274, Global test accuracy: 13.13 

Round  56, Train loss: 2.280, Test loss: 2.275, Test accuracy: 13.31 

Round  56, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 13.19 

Round  57, Train loss: 2.279, Test loss: 2.274, Test accuracy: 13.35 

Round  57, Global train loss: 2.279, Global test loss: 2.272, Global test accuracy: 13.65 

Round  58, Train loss: 2.282, Test loss: 2.273, Test accuracy: 13.20 

Round  58, Global train loss: 2.282, Global test loss: 2.271, Global test accuracy: 13.27 

Round  59, Train loss: 2.280, Test loss: 2.272, Test accuracy: 13.50 

Round  59, Global train loss: 2.280, Global test loss: 2.271, Global test accuracy: 14.56 

Round  60, Train loss: 2.276, Test loss: 2.271, Test accuracy: 13.83 

Round  60, Global train loss: 2.276, Global test loss: 2.270, Global test accuracy: 14.92 

Round  61, Train loss: 2.279, Test loss: 2.271, Test accuracy: 14.16 

Round  61, Global train loss: 2.279, Global test loss: 2.269, Global test accuracy: 15.12 

Round  62, Train loss: 2.276, Test loss: 2.270, Test accuracy: 14.66 

Round  62, Global train loss: 2.276, Global test loss: 2.268, Global test accuracy: 15.43 

Round  63, Train loss: 2.278, Test loss: 2.269, Test accuracy: 15.34 

Round  63, Global train loss: 2.278, Global test loss: 2.268, Global test accuracy: 16.66 

Round  64, Train loss: 2.275, Test loss: 2.269, Test accuracy: 15.85 

Round  64, Global train loss: 2.275, Global test loss: 2.267, Global test accuracy: 17.05 

Round  65, Train loss: 2.271, Test loss: 2.268, Test accuracy: 15.94 

Round  65, Global train loss: 2.271, Global test loss: 2.266, Global test accuracy: 16.93 

Round  66, Train loss: 2.274, Test loss: 2.267, Test accuracy: 16.05 

Round  66, Global train loss: 2.274, Global test loss: 2.265, Global test accuracy: 17.12 

Round  67, Train loss: 2.273, Test loss: 2.266, Test accuracy: 16.05 

Round  67, Global train loss: 2.273, Global test loss: 2.263, Global test accuracy: 17.20 

Round  68, Train loss: 2.272, Test loss: 2.265, Test accuracy: 16.55 

Round  68, Global train loss: 2.272, Global test loss: 2.262, Global test accuracy: 17.37 

Round  69, Train loss: 2.268, Test loss: 2.263, Test accuracy: 16.73 

Round  69, Global train loss: 2.268, Global test loss: 2.261, Global test accuracy: 17.06 

Round  70, Train loss: 2.267, Test loss: 2.262, Test accuracy: 16.64 

Round  70, Global train loss: 2.267, Global test loss: 2.260, Global test accuracy: 17.45 

Round  71, Train loss: 2.268, Test loss: 2.262, Test accuracy: 16.73 

Round  71, Global train loss: 2.268, Global test loss: 2.259, Global test accuracy: 17.48 

Round  72, Train loss: 2.264, Test loss: 2.261, Test accuracy: 16.93 

Round  72, Global train loss: 2.264, Global test loss: 2.258, Global test accuracy: 17.39 

Round  73, Train loss: 2.267, Test loss: 2.260, Test accuracy: 16.83 

Round  73, Global train loss: 2.267, Global test loss: 2.257, Global test accuracy: 17.48 

Round  74, Train loss: 2.264, Test loss: 2.259, Test accuracy: 16.73 

Round  74, Global train loss: 2.264, Global test loss: 2.256, Global test accuracy: 17.16 

Round  75, Train loss: 2.268, Test loss: 2.259, Test accuracy: 16.98 

Round  75, Global train loss: 2.268, Global test loss: 2.255, Global test accuracy: 18.24 

Round  76, Train loss: 2.263, Test loss: 2.257, Test accuracy: 17.11 

Round  76, Global train loss: 2.263, Global test loss: 2.254, Global test accuracy: 17.92 

Round  77, Train loss: 2.265, Test loss: 2.255, Test accuracy: 17.20 

Round  77, Global train loss: 2.265, Global test loss: 2.252, Global test accuracy: 17.85 

Round  78, Train loss: 2.264, Test loss: 2.254, Test accuracy: 17.11 

Round  78, Global train loss: 2.264, Global test loss: 2.250, Global test accuracy: 17.39 

Round  79, Train loss: 2.266, Test loss: 2.253, Test accuracy: 16.74 

Round  79, Global train loss: 2.266, Global test loss: 2.250, Global test accuracy: 16.76 

Round  80, Train loss: 2.259, Test loss: 2.251, Test accuracy: 16.98 

Round  80, Global train loss: 2.259, Global test loss: 2.249, Global test accuracy: 17.56 

Round  81, Train loss: 2.260, Test loss: 2.250, Test accuracy: 16.78 

Round  81, Global train loss: 2.260, Global test loss: 2.247, Global test accuracy: 17.70 

Round  82, Train loss: 2.257, Test loss: 2.250, Test accuracy: 17.05 

Round  82, Global train loss: 2.257, Global test loss: 2.246, Global test accuracy: 18.00 

Round  83, Train loss: 2.259, Test loss: 2.248, Test accuracy: 17.22 

Round  83, Global train loss: 2.259, Global test loss: 2.244, Global test accuracy: 17.57 

Round  84, Train loss: 2.253, Test loss: 2.247, Test accuracy: 17.52 

Round  84, Global train loss: 2.253, Global test loss: 2.242, Global test accuracy: 17.46 

Round  85, Train loss: 2.253, Test loss: 2.245, Test accuracy: 17.52 

Round  85, Global train loss: 2.253, Global test loss: 2.241, Global test accuracy: 18.04 

Round  86, Train loss: 2.258, Test loss: 2.242, Test accuracy: 17.78 

Round  86, Global train loss: 2.258, Global test loss: 2.239, Global test accuracy: 17.69 

Round  87, Train loss: 2.254, Test loss: 2.241, Test accuracy: 17.69 

Round  87, Global train loss: 2.254, Global test loss: 2.237, Global test accuracy: 17.37 

Round  88, Train loss: 2.253, Test loss: 2.239, Test accuracy: 17.80 

Round  88, Global train loss: 2.253, Global test loss: 2.236, Global test accuracy: 18.60 

Round  89, Train loss: 2.256, Test loss: 2.237, Test accuracy: 18.05 

Round  89, Global train loss: 2.256, Global test loss: 2.235, Global test accuracy: 19.31 

Round  90, Train loss: 2.250, Test loss: 2.236, Test accuracy: 18.13 

Round  90, Global train loss: 2.250, Global test loss: 2.233, Global test accuracy: 18.56 

Round  91, Train loss: 2.252, Test loss: 2.234, Test accuracy: 18.25 

Round  91, Global train loss: 2.252, Global test loss: 2.231, Global test accuracy: 18.84 

Round  92, Train loss: 2.255, Test loss: 2.233, Test accuracy: 18.15 

Round  92, Global train loss: 2.255, Global test loss: 2.231, Global test accuracy: 18.95 

Round  93, Train loss: 2.253, Test loss: 2.232, Test accuracy: 18.44 

Round  93, Global train loss: 2.253, Global test loss: 2.230, Global test accuracy: 19.17 

Round  94, Train loss: 2.251, Test loss: 2.232, Test accuracy: 18.41 

Round  94, Global train loss: 2.251, Global test loss: 2.228, Global test accuracy: 18.77 

Round  95, Train loss: 2.247, Test loss: 2.230, Test accuracy: 18.74 

Round  95, Global train loss: 2.247, Global test loss: 2.228, Global test accuracy: 18.82 

Round  96, Train loss: 2.247, Test loss: 2.230, Test accuracy: 18.73 

Round  96, Global train loss: 2.247, Global test loss: 2.226, Global test accuracy: 19.07 

Round  97, Train loss: 2.252, Test loss: 2.229, Test accuracy: 18.83 

Round  97, Global train loss: 2.252, Global test loss: 2.226, Global test accuracy: 19.67 

Round  98, Train loss: 2.247, Test loss: 2.228, Test accuracy: 19.40 

Round  98, Global train loss: 2.247, Global test loss: 2.225, Global test accuracy: 20.13 

Round  99, Train loss: 2.244, Test loss: 2.227, Test accuracy: 19.79 

Round  99, Global train loss: 2.244, Global test loss: 2.225, Global test accuracy: 20.48 

Round 100, Train loss: 2.249, Test loss: 2.226, Test accuracy: 20.05 

Round 100, Global train loss: 2.249, Global test loss: 2.224, Global test accuracy: 20.85 

Round 101, Train loss: 2.245, Test loss: 2.225, Test accuracy: 20.13 

Round 101, Global train loss: 2.245, Global test loss: 2.221, Global test accuracy: 20.82 

Round 102, Train loss: 2.244, Test loss: 2.224, Test accuracy: 20.41 

Round 102, Global train loss: 2.244, Global test loss: 2.222, Global test accuracy: 21.00 

Round 103, Train loss: 2.246, Test loss: 2.223, Test accuracy: 20.66 

Round 103, Global train loss: 2.246, Global test loss: 2.221, Global test accuracy: 21.43 

Round 104, Train loss: 2.240, Test loss: 2.222, Test accuracy: 20.75 

Round 104, Global train loss: 2.240, Global test loss: 2.219, Global test accuracy: 21.39 

Round 105, Train loss: 2.244, Test loss: 2.221, Test accuracy: 21.16 

Round 105, Global train loss: 2.244, Global test loss: 2.218, Global test accuracy: 21.76 

Round 106, Train loss: 2.240, Test loss: 2.219, Test accuracy: 21.36 

Round 106, Global train loss: 2.240, Global test loss: 2.216, Global test accuracy: 21.85 

Round 107, Train loss: 2.234, Test loss: 2.218, Test accuracy: 21.52 

Round 107, Global train loss: 2.234, Global test loss: 2.215, Global test accuracy: 22.11 

Round 108, Train loss: 2.245, Test loss: 2.217, Test accuracy: 21.60 

Round 108, Global train loss: 2.245, Global test loss: 2.216, Global test accuracy: 22.32 

Round 109, Train loss: 2.241, Test loss: 2.216, Test accuracy: 21.86 

Round 109, Global train loss: 2.241, Global test loss: 2.213, Global test accuracy: 21.82 

Round 110, Train loss: 2.236, Test loss: 2.215, Test accuracy: 21.82 

Round 110, Global train loss: 2.236, Global test loss: 2.213, Global test accuracy: 21.97 

Round 111, Train loss: 2.239, Test loss: 2.214, Test accuracy: 21.93 

Round 111, Global train loss: 2.239, Global test loss: 2.212, Global test accuracy: 21.98 

Round 112, Train loss: 2.236, Test loss: 2.214, Test accuracy: 22.05 

Round 112, Global train loss: 2.236, Global test loss: 2.215, Global test accuracy: 22.04 

Round 113, Train loss: 2.230, Test loss: 2.214, Test accuracy: 22.10 

Round 113, Global train loss: 2.230, Global test loss: 2.212, Global test accuracy: 22.32 

Round 114, Train loss: 2.231, Test loss: 2.213, Test accuracy: 22.11 

Round 114, Global train loss: 2.231, Global test loss: 2.210, Global test accuracy: 22.30 

Round 115, Train loss: 2.240, Test loss: 2.211, Test accuracy: 22.16 

Round 115, Global train loss: 2.240, Global test loss: 2.209, Global test accuracy: 22.25 

Round 116, Train loss: 2.235, Test loss: 2.210, Test accuracy: 22.20 

Round 116, Global train loss: 2.235, Global test loss: 2.206, Global test accuracy: 22.23 

Round 117, Train loss: 2.232, Test loss: 2.209, Test accuracy: 22.15 

Round 117, Global train loss: 2.232, Global test loss: 2.207, Global test accuracy: 22.48 

Round 118, Train loss: 2.239, Test loss: 2.210, Test accuracy: 22.34 

Round 118, Global train loss: 2.239, Global test loss: 2.209, Global test accuracy: 22.74 

Round 119, Train loss: 2.229, Test loss: 2.210, Test accuracy: 22.27 

Round 119, Global train loss: 2.229, Global test loss: 2.206, Global test accuracy: 22.68 

Round 120, Train loss: 2.226, Test loss: 2.209, Test accuracy: 22.29 

Round 120, Global train loss: 2.226, Global test loss: 2.205, Global test accuracy: 22.48 

Round 121, Train loss: 2.237, Test loss: 2.208, Test accuracy: 22.25 

Round 121, Global train loss: 2.237, Global test loss: 2.203, Global test accuracy: 22.39 

Round 122, Train loss: 2.226, Test loss: 2.205, Test accuracy: 22.15 

Round 122, Global train loss: 2.226, Global test loss: 2.197, Global test accuracy: 22.19 

Round 123, Train loss: 2.231, Test loss: 2.202, Test accuracy: 22.03 

Round 123, Global train loss: 2.231, Global test loss: 2.195, Global test accuracy: 22.63 

Round 124, Train loss: 2.229, Test loss: 2.200, Test accuracy: 21.96 

Round 124, Global train loss: 2.229, Global test loss: 2.191, Global test accuracy: 22.28 

Round 125, Train loss: 2.232, Test loss: 2.198, Test accuracy: 22.34 

Round 125, Global train loss: 2.232, Global test loss: 2.191, Global test accuracy: 22.92 

Round 126, Train loss: 2.234, Test loss: 2.195, Test accuracy: 22.48 

Round 126, Global train loss: 2.234, Global test loss: 2.191, Global test accuracy: 23.08 

Round 127, Train loss: 2.222, Test loss: 2.194, Test accuracy: 22.57 

Round 127, Global train loss: 2.222, Global test loss: 2.191, Global test accuracy: 23.20 

Round 128, Train loss: 2.221, Test loss: 2.192, Test accuracy: 22.41 

Round 128, Global train loss: 2.221, Global test loss: 2.188, Global test accuracy: 22.82 

Round 129, Train loss: 2.223, Test loss: 2.189, Test accuracy: 22.45 

Round 129, Global train loss: 2.223, Global test loss: 2.185, Global test accuracy: 22.95 

Round 130, Train loss: 2.224, Test loss: 2.188, Test accuracy: 22.51 

Round 130, Global train loss: 2.224, Global test loss: 2.186, Global test accuracy: 23.38 

Round 131, Train loss: 2.226, Test loss: 2.189, Test accuracy: 22.86 

Round 131, Global train loss: 2.226, Global test loss: 2.189, Global test accuracy: 23.86 

Round 132, Train loss: 2.219, Test loss: 2.190, Test accuracy: 22.97 

Round 132, Global train loss: 2.219, Global test loss: 2.190, Global test accuracy: 23.89 

Round 133, Train loss: 2.212, Test loss: 2.189, Test accuracy: 23.12 

Round 133, Global train loss: 2.212, Global test loss: 2.188, Global test accuracy: 23.99 

Round 134, Train loss: 2.225, Test loss: 2.189, Test accuracy: 23.23 

Round 134, Global train loss: 2.225, Global test loss: 2.187, Global test accuracy: 24.15 

Round 135, Train loss: 2.222, Test loss: 2.188, Test accuracy: 23.36 

Round 135, Global train loss: 2.222, Global test loss: 2.184, Global test accuracy: 23.79 

Round 136, Train loss: 2.218, Test loss: 2.187, Test accuracy: 23.57 

Round 136, Global train loss: 2.218, Global test loss: 2.188, Global test accuracy: 23.99 

Round 137, Train loss: 2.221, Test loss: 2.187, Test accuracy: 23.55 

Round 137, Global train loss: 2.221, Global test loss: 2.184, Global test accuracy: 24.04 

Round 138, Train loss: 2.220, Test loss: 2.184, Test accuracy: 23.73 

Round 138, Global train loss: 2.220, Global test loss: 2.180, Global test accuracy: 23.94 

Round 139, Train loss: 2.225, Test loss: 2.183, Test accuracy: 23.54 

Round 139, Global train loss: 2.225, Global test loss: 2.179, Global test accuracy: 23.69 

Round 140, Train loss: 2.212, Test loss: 2.181, Test accuracy: 23.32 

Round 140, Global train loss: 2.212, Global test loss: 2.177, Global test accuracy: 23.70 

Round 141, Train loss: 2.217, Test loss: 2.180, Test accuracy: 23.30 

Round 141, Global train loss: 2.217, Global test loss: 2.175, Global test accuracy: 23.48 

Round 142, Train loss: 2.219, Test loss: 2.178, Test accuracy: 23.47 

Round 142, Global train loss: 2.219, Global test loss: 2.173, Global test accuracy: 23.76 

Round 143, Train loss: 2.211, Test loss: 2.177, Test accuracy: 23.46 

Round 143, Global train loss: 2.211, Global test loss: 2.173, Global test accuracy: 24.16 

Round 144, Train loss: 2.213, Test loss: 2.174, Test accuracy: 23.54 

Round 144, Global train loss: 2.213, Global test loss: 2.170, Global test accuracy: 24.21 

Round 145, Train loss: 2.229, Test loss: 2.174, Test accuracy: 23.87 

Round 145, Global train loss: 2.229, Global test loss: 2.173, Global test accuracy: 24.44 

Round 146, Train loss: 2.217, Test loss: 2.173, Test accuracy: 23.80 

Round 146, Global train loss: 2.217, Global test loss: 2.171, Global test accuracy: 24.18 

Round 147, Train loss: 2.215, Test loss: 2.172, Test accuracy: 23.97 

Round 147, Global train loss: 2.215, Global test loss: 2.169, Global test accuracy: 24.20 

Round 148, Train loss: 2.223, Test loss: 2.172, Test accuracy: 24.04 

Round 148, Global train loss: 2.223, Global test loss: 2.167, Global test accuracy: 23.98 

Round 149, Train loss: 2.209, Test loss: 2.170, Test accuracy: 23.84 

Round 149, Global train loss: 2.209, Global test loss: 2.165, Global test accuracy: 23.68 

Round 150, Train loss: 2.202, Test loss: 2.168, Test accuracy: 23.84 

Round 150, Global train loss: 2.202, Global test loss: 2.162, Global test accuracy: 23.67 

Round 151, Train loss: 2.222, Test loss: 2.166, Test accuracy: 23.54 

Round 151, Global train loss: 2.222, Global test loss: 2.164, Global test accuracy: 23.62 

Round 152, Train loss: 2.204, Test loss: 2.167, Test accuracy: 23.61 

Round 152, Global train loss: 2.204, Global test loss: 2.164, Global test accuracy: 23.71 

Round 153, Train loss: 2.217, Test loss: 2.166, Test accuracy: 23.62 

Round 153, Global train loss: 2.217, Global test loss: 2.165, Global test accuracy: 23.82 

Round 154, Train loss: 2.200, Test loss: 2.166, Test accuracy: 23.69 

Round 154, Global train loss: 2.200, Global test loss: 2.166, Global test accuracy: 24.47 

Round 155, Train loss: 2.203, Test loss: 2.165, Test accuracy: 23.81 

Round 155, Global train loss: 2.203, Global test loss: 2.165, Global test accuracy: 24.36 

Round 156, Train loss: 2.210, Test loss: 2.164, Test accuracy: 23.90 

Round 156, Global train loss: 2.210, Global test loss: 2.163, Global test accuracy: 24.55 

Round 157, Train loss: 2.201, Test loss: 2.162, Test accuracy: 24.09 

Round 157, Global train loss: 2.201, Global test loss: 2.158, Global test accuracy: 24.55 

Round 158, Train loss: 2.202, Test loss: 2.162, Test accuracy: 24.23 

Round 158, Global train loss: 2.202, Global test loss: 2.159, Global test accuracy: 24.54 

Round 159, Train loss: 2.206, Test loss: 2.162, Test accuracy: 24.27 

Round 159, Global train loss: 2.206, Global test loss: 2.160, Global test accuracy: 24.67 

Round 160, Train loss: 2.204, Test loss: 2.161, Test accuracy: 24.25 

Round 160, Global train loss: 2.204, Global test loss: 2.160, Global test accuracy: 24.95 

Round 161, Train loss: 2.201, Test loss: 2.160, Test accuracy: 24.41 

Round 161, Global train loss: 2.201, Global test loss: 2.158, Global test accuracy: 24.72 

Round 162, Train loss: 2.197, Test loss: 2.158, Test accuracy: 24.34 

Round 162, Global train loss: 2.197, Global test loss: 2.153, Global test accuracy: 24.88 

Round 163, Train loss: 2.207, Test loss: 2.156, Test accuracy: 24.48 

Round 163, Global train loss: 2.207, Global test loss: 2.149, Global test accuracy: 24.65 

Round 164, Train loss: 2.205, Test loss: 2.155, Test accuracy: 24.41 

Round 164, Global train loss: 2.205, Global test loss: 2.151, Global test accuracy: 24.51 

Round 165, Train loss: 2.204, Test loss: 2.151, Test accuracy: 24.39 

Round 165, Global train loss: 2.204, Global test loss: 2.147, Global test accuracy: 24.47 

Round 166, Train loss: 2.207, Test loss: 2.151, Test accuracy: 24.43 

Round 166, Global train loss: 2.207, Global test loss: 2.148, Global test accuracy: 24.57 

Round 167, Train loss: 2.201, Test loss: 2.150, Test accuracy: 24.41 

Round 167, Global train loss: 2.201, Global test loss: 2.147, Global test accuracy: 24.62 

Round 168, Train loss: 2.199, Test loss: 2.150, Test accuracy: 24.43 

Round 168, Global train loss: 2.199, Global test loss: 2.147, Global test accuracy: 24.71 

Round 169, Train loss: 2.203, Test loss: 2.149, Test accuracy: 24.51 

Round 169, Global train loss: 2.203, Global test loss: 2.145, Global test accuracy: 24.73 

Round 170, Train loss: 2.201, Test loss: 2.148, Test accuracy: 24.45 

Round 170, Global train loss: 2.201, Global test loss: 2.145, Global test accuracy: 24.54 

Round 171, Train loss: 2.202, Test loss: 2.147, Test accuracy: 24.56 

Round 171, Global train loss: 2.202, Global test loss: 2.144, Global test accuracy: 24.83 

Round 172, Train loss: 2.199, Test loss: 2.148, Test accuracy: 24.59 

Round 172, Global train loss: 2.199, Global test loss: 2.151, Global test accuracy: 24.81 

Round 173, Train loss: 2.190, Test loss: 2.148, Test accuracy: 24.70 

Round 173, Global train loss: 2.190, Global test loss: 2.150, Global test accuracy: 25.16 

Round 174, Train loss: 2.196, Test loss: 2.149, Test accuracy: 24.80 

Round 174, Global train loss: 2.196, Global test loss: 2.151, Global test accuracy: 25.43 

Round 175, Train loss: 2.195, Test loss: 2.149, Test accuracy: 25.20 

Round 175, Global train loss: 2.195, Global test loss: 2.148, Global test accuracy: 25.72 

Round 176, Train loss: 2.197, Test loss: 2.147, Test accuracy: 25.11 

Round 176, Global train loss: 2.197, Global test loss: 2.142, Global test accuracy: 25.28 

Round 177, Train loss: 2.192, Test loss: 2.145, Test accuracy: 25.10 

Round 177, Global train loss: 2.192, Global test loss: 2.141, Global test accuracy: 24.94 

Round 178, Train loss: 2.185, Test loss: 2.145, Test accuracy: 25.04 

Round 178, Global train loss: 2.185, Global test loss: 2.143, Global test accuracy: 25.30 

Round 179, Train loss: 2.183, Test loss: 2.145, Test accuracy: 25.05 

Round 179, Global train loss: 2.183, Global test loss: 2.143, Global test accuracy: 25.43 

Round 180, Train loss: 2.202, Test loss: 2.144, Test accuracy: 24.98 

Round 180, Global train loss: 2.202, Global test loss: 2.142, Global test accuracy: 25.25 

Round 181, Train loss: 2.191, Test loss: 2.143, Test accuracy: 25.02 

Round 181, Global train loss: 2.191, Global test loss: 2.143, Global test accuracy: 25.32 

Round 182, Train loss: 2.187, Test loss: 2.141, Test accuracy: 24.91 

Round 182, Global train loss: 2.187, Global test loss: 2.140, Global test accuracy: 25.07 

Round 183, Train loss: 2.194, Test loss: 2.143, Test accuracy: 24.80 

Round 183, Global train loss: 2.194, Global test loss: 2.144, Global test accuracy: 24.85 

Round 184, Train loss: 2.185, Test loss: 2.142, Test accuracy: 24.56 

Round 184, Global train loss: 2.185, Global test loss: 2.135, Global test accuracy: 24.30 

Round 185, Train loss: 2.180, Test loss: 2.141, Test accuracy: 24.46 

Round 185, Global train loss: 2.180, Global test loss: 2.135, Global test accuracy: 24.55 

Round 186, Train loss: 2.190, Test loss: 2.139, Test accuracy: 24.55 

Round 186, Global train loss: 2.190, Global test loss: 2.132, Global test accuracy: 24.48 

Round 187, Train loss: 2.194, Test loss: 2.136, Test accuracy: 24.55 

Round 187, Global train loss: 2.194, Global test loss: 2.130, Global test accuracy: 24.42 

Round 188, Train loss: 2.177, Test loss: 2.134, Test accuracy: 24.45 

Round 188, Global train loss: 2.177, Global test loss: 2.128, Global test accuracy: 24.59 

Round 189, Train loss: 2.188, Test loss: 2.133, Test accuracy: 24.41 

Round 189, Global train loss: 2.188, Global test loss: 2.129, Global test accuracy: 24.54 

Round 190, Train loss: 2.187, Test loss: 2.131, Test accuracy: 24.46 

Round 190, Global train loss: 2.187, Global test loss: 2.128, Global test accuracy: 24.51 

Round 191, Train loss: 2.194, Test loss: 2.130, Test accuracy: 24.32 

Round 191, Global train loss: 2.194, Global test loss: 2.123, Global test accuracy: 24.25 

Round 192, Train loss: 2.194, Test loss: 2.127, Test accuracy: 24.17 

Round 192, Global train loss: 2.194, Global test loss: 2.119, Global test accuracy: 24.51 

Round 193, Train loss: 2.201, Test loss: 2.126, Test accuracy: 24.35 

Round 193, Global train loss: 2.201, Global test loss: 2.118, Global test accuracy: 24.71 

Round 194, Train loss: 2.196, Test loss: 2.123, Test accuracy: 24.16 

Round 194, Global train loss: 2.196, Global test loss: 2.116, Global test accuracy: 24.50 

Round 195, Train loss: 2.232, Test loss: 2.125, Test accuracy: 23.94 

Round 195, Global train loss: 2.232, Global test loss: 2.113, Global test accuracy: 24.77 

Round 196, Train loss: 2.209, Test loss: 2.120, Test accuracy: 24.36 

Round 196, Global train loss: 2.209, Global test loss: 2.115, Global test accuracy: 25.13 

Round 197, Train loss: nan, Test loss: nan, Test accuracy: 23.68 

Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 198, Train loss: nan, Test loss: nan, Test accuracy: 20.21 

Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 199, Train loss: nan, Test loss: nan, Test accuracy: 15.92 

Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 200, Train loss: nan, Test loss: nan, Test accuracy: 13.65 

Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 201, Train loss: nan, Test loss: nan, Test accuracy: 12.84 

Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 202, Train loss: nan, Test loss: nan, Test accuracy: 12.84 

Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 203, Train loss: nan, Test loss: nan, Test accuracy: 11.35 

Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 204, Train loss: nan, Test loss: nan, Test accuracy: 11.35 

Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.63 

Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00 

Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00 

Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

8060.714681386948
[1.5175058841705322, 2.7953271865844727, 4.082510709762573, 5.36842679977417, 6.577006578445435, 7.838306665420532, 9.094188690185547, 10.357059240341187, 11.6176016330719, 12.87500810623169, 14.134371042251587, 15.393816709518433, 16.651302099227905, 17.907631397247314, 19.16593337059021, 20.423633098602295, 21.67931866645813, 22.937897205352783, 24.19663715362549, 25.456000566482544, 26.714250802993774, 27.97426414489746, 29.235326051712036, 30.496102333068848, 31.758092164993286, 33.023555278778076, 34.28162956237793, 35.54345417022705, 36.80236458778381, 38.06638836860657, 39.327009439468384, 40.58988928794861, 41.8544135093689, 43.1162588596344, 44.38246250152588, 45.64863610267639, 46.91004157066345, 48.17005777359009, 49.42465853691101, 50.68131709098816, 51.960723638534546, 53.21900415420532, 54.478031635284424, 55.733174562454224, 56.99959182739258, 58.28172421455383, 59.53930640220642, 60.79778337478638, 62.06081938743591, 63.32114267349243, 64.57895731925964, 65.83879780769348, 67.09688234329224, 68.36234045028687, 69.58121061325073, 70.69772553443909, 71.81353545188904, 72.92850923538208, 74.04483914375305, 75.16172623634338, 76.27370285987854, 77.38475966453552, 78.49576830863953, 79.60750770568848, 80.72163820266724, 81.84082102775574, 82.96141362190247, 84.0807671546936, 85.1983323097229, 86.31813549995422, 87.43673586845398, 88.54810452461243, 89.65961742401123, 90.7759199142456, 91.8910083770752, 93.00489044189453, 94.12029218673706, 95.23564195632935, 96.52889347076416, 97.79427790641785, 99.0615119934082, 100.33393979072571, 101.60519099235535, 102.87873148918152, 104.1466863155365, 105.40739798545837, 106.66759657859802, 107.92729878425598, 109.18819689750671, 110.45007944107056, 111.72646474838257, 112.99175024032593, 114.25281548500061, 115.54502582550049, 116.67075705528259, 117.79150581359863, 119.07689452171326, 120.35130953788757, 121.46570539474487, 122.59089493751526, 123.71233773231506, 124.83462691307068, 125.95726370811462, 127.24389123916626, 128.52423310279846, 129.80681824684143, 131.09195375442505, 132.38876056671143, 133.6815812587738, 134.97023105621338, 136.27009081840515, 137.55664920806885, 138.8354847431183, 140.1219482421875, 141.40596437454224, 142.68250441551208, 143.96105647087097, 145.23439764976501, 146.34912037849426, 147.46724724769592, 148.58497023582458, 149.70574831962585, 150.82448840141296, 151.94189405441284, 153.06199431419373, 154.17991971969604, 155.29547715187073, 156.41040754318237, 157.52636241912842, 158.6457874774933, 159.76208925247192, 160.878808259964, 161.9935073852539, 163.10981488227844, 164.22403049468994, 165.34224891662598, 166.45913100242615, 167.57802867889404, 168.69424986839294, 169.82636189460754, 170.95359420776367, 172.07076621055603, 173.1922390460968, 174.30743503570557, 175.4198021888733, 176.53331089019775, 177.6506950855255, 178.76686716079712, 179.8820939064026, 180.99685788154602, 182.26709961891174, 183.5429584980011, 184.82580471038818, 186.10343408584595, 187.3816282749176, 188.63761186599731, 189.9039454460144, 191.17238974571228, 192.45282411575317, 193.73380708694458, 195.01404118537903, 196.28819370269775, 197.57416605949402, 198.85914754867554, 200.1434781551361, 201.426504611969, 202.70819067955017, 204.00100207328796, 205.287663936615, 206.57735466957092, 207.86647057533264, 209.15257215499878, 210.44705891609192, 211.7340750694275, 213.0239498615265, 214.316157579422, 215.6120183467865, 216.89629125595093, 218.18236231803894, 219.4677984714508, 220.7462933063507, 221.86803555488586, 222.9932165145874, 224.11475825309753, 225.23682022094727, 226.35258173942566, 227.46717739105225, 228.5863902568817, 229.7007133960724, 230.83379292488098, 231.9517481327057, 233.0744731426239, 234.2020890712738, 235.3275887966156, 236.45292830467224, 237.5763018131256, 238.7044198513031, 239.8278329372406, 241.11124634742737, 242.3911738395691, 243.67211318016052, 244.95739436149597, 246.23468565940857, 247.51110553741455, 248.77772116661072, 250.0472445487976, 251.3253297805786, 252.4583704471588, 253.58882093429565, 254.71741795539856, 255.85330533981323, 257.16221261024475, 258.46679878234863, 259.7726254463196, 261.06975078582764, 262.37444496154785, 263.67703914642334, 264.9781506061554, 266.27660751342773, 267.57434344291687, 268.84725975990295, 270.13092947006226, 271.40975046157837, 272.7144613265991, 274.0199246406555, 275.3178515434265, 276.6139132976532, 277.915807723999, 279.22419810295105, 280.5338544845581, 281.84559512138367, 283.157018661499, 284.46833539009094, 285.77795696258545, 287.09371399879456, 288.40474367141724, 289.711701631546, 291.01512908935547, 292.3257670402527, 293.6354887485504, 294.9407353401184, 296.2515194416046, 297.55749249458313, 298.86854100227356, 300.17553091049194, 301.4745240211487, 302.77866983413696, 303.94260239601135, 305.08535385131836, 306.2119462490082, 307.335928440094, 308.4620432853699, 309.58814001083374, 310.7274432182312, 311.86791038513184, 313.0130977630615, 314.1509380340576, 315.29087495803833, 316.4337794780731, 317.5650565624237, 318.6883101463318, 319.81151700019836, 320.9429533481598, 322.0722851753235, 323.19550108909607, 324.3331665992737, 325.46184277534485, 326.60418152809143, 327.73682141304016, 328.8637080192566, 329.98972153663635, 331.12230825424194, 332.26012992858887, 333.38448119163513, 334.50411081314087, 335.6418945789337, 336.78123593330383, 337.9144072532654, 339.048828125, 340.17087864875793, 341.3004095554352, 342.4393491744995, 343.57732939720154, 344.7134120464325, 345.8521156311035, 346.9858877658844, 348.11427068710327, 349.2407908439636, 350.3847641944885, 351.5097715854645, 352.6257975101471, 353.74535036087036, 354.8715806007385, 355.9941928386688, 357.1147599220276, 358.2371871471405, 359.35998940467834, 360.4809949398041, 361.6035418510437, 362.7267644405365, 364.9665496349335]
[10.6075, 10.5675, 10.7925, 10.745, 10.755, 11.0375, 10.905, 10.8225, 10.8275, 10.6525, 10.6125, 10.59, 10.6175, 10.7525, 10.7525, 10.905, 11.01, 11.06, 11.2875, 11.2375, 11.2075, 11.24, 11.19, 11.2625, 11.255, 11.275, 11.29, 11.4025, 11.33, 11.175, 11.1525, 11.2725, 11.46, 11.52, 11.5975, 11.5775, 11.605, 11.645, 11.885, 11.9425, 12.0775, 12.1275, 11.8425, 11.855, 11.88, 12.3725, 12.45, 12.44, 12.6525, 13.18, 13.7925, 13.4575, 13.1625, 13.2375, 13.09, 13.1, 13.3075, 13.3525, 13.195, 13.5025, 13.8325, 14.1625, 14.6625, 15.34, 15.845, 15.9375, 16.0475, 16.0525, 16.55, 16.73, 16.64, 16.725, 16.925, 16.8275, 16.73, 16.985, 17.105, 17.205, 17.1125, 16.74, 16.9775, 16.7775, 17.055, 17.22, 17.5175, 17.515, 17.7825, 17.6875, 17.7975, 18.055, 18.13, 18.2525, 18.1525, 18.4425, 18.415, 18.74, 18.725, 18.8325, 19.4025, 19.785, 20.0525, 20.13, 20.4075, 20.6575, 20.7475, 21.165, 21.355, 21.52, 21.6, 21.855, 21.815, 21.925, 22.0525, 22.1025, 22.11, 22.16, 22.1975, 22.1475, 22.3425, 22.275, 22.29, 22.25, 22.1475, 22.0325, 21.9575, 22.3425, 22.475, 22.575, 22.405, 22.45, 22.5125, 22.855, 22.97, 23.1175, 23.23, 23.355, 23.5725, 23.555, 23.725, 23.5425, 23.3225, 23.3025, 23.4725, 23.4575, 23.5375, 23.8725, 23.795, 23.9725, 24.0375, 23.84, 23.845, 23.5375, 23.615, 23.6225, 23.69, 23.81, 23.9025, 24.085, 24.235, 24.265, 24.2525, 24.4075, 24.335, 24.485, 24.405, 24.39, 24.435, 24.405, 24.4275, 24.5075, 24.4475, 24.56, 24.595, 24.705, 24.8025, 25.2025, 25.11, 25.1, 25.035, 25.05, 24.9825, 25.015, 24.91, 24.805, 24.5575, 24.465, 24.5475, 24.5525, 24.4525, 24.41, 24.4625, 24.3175, 24.1675, 24.35, 24.165, 23.94, 24.3575, 23.6775, 20.215, 15.9225, 13.65, 12.835, 12.835, 11.3475, 11.3475, 10.6275, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.263, Test loss: 2.405, Test accuracy: 6.73
Round   1, Train loss: 1.057, Test loss: 2.213, Test accuracy: 24.10
Round   2, Train loss: 0.958, Test loss: 2.155, Test accuracy: 27.35
Round   3, Train loss: 0.873, Test loss: 2.162, Test accuracy: 27.35
Round   4, Train loss: 0.819, Test loss: 2.015, Test accuracy: 32.15
Round   5, Train loss: 0.803, Test loss: 2.216, Test accuracy: 30.27
Round   6, Train loss: 0.776, Test loss: 2.044, Test accuracy: 35.32
Round   7, Train loss: 0.743, Test loss: 1.836, Test accuracy: 36.77
Round   8, Train loss: 0.672, Test loss: 1.770, Test accuracy: 39.08
Round   9, Train loss: 0.667, Test loss: 2.129, Test accuracy: 34.57
Round  10, Train loss: 0.649, Test loss: 2.354, Test accuracy: 32.98
Round  11, Train loss: 0.608, Test loss: 1.749, Test accuracy: 39.92
Round  12, Train loss: 0.649, Test loss: 1.637, Test accuracy: 42.38
Round  13, Train loss: 0.544, Test loss: 1.610, Test accuracy: 44.43
Round  14, Train loss: 0.661, Test loss: 1.637, Test accuracy: 44.53
Round  15, Train loss: 0.551, Test loss: 2.149, Test accuracy: 35.75
Round  16, Train loss: 0.602, Test loss: 1.705, Test accuracy: 43.73
Round  17, Train loss: 0.609, Test loss: 1.801, Test accuracy: 42.85
Round  18, Train loss: 0.557, Test loss: 1.748, Test accuracy: 44.10
Round  19, Train loss: 0.493, Test loss: 2.002, Test accuracy: 42.08
Round  20, Train loss: 0.511, Test loss: 1.689, Test accuracy: 42.43
Round  21, Train loss: 0.540, Test loss: 1.492, Test accuracy: 48.18
Round  22, Train loss: 0.534, Test loss: 1.872, Test accuracy: 42.23
Round  23, Train loss: 0.461, Test loss: 1.593, Test accuracy: 47.03
Round  24, Train loss: 0.466, Test loss: 1.395, Test accuracy: 51.93
Round  25, Train loss: 0.530, Test loss: 1.361, Test accuracy: 51.77
Round  26, Train loss: 0.394, Test loss: 1.821, Test accuracy: 44.55
Round  27, Train loss: 0.464, Test loss: 1.658, Test accuracy: 43.27
Round  28, Train loss: 0.385, Test loss: 1.420, Test accuracy: 51.70
Round  29, Train loss: 0.377, Test loss: 1.506, Test accuracy: 50.70
Round  30, Train loss: 0.376, Test loss: 1.545, Test accuracy: 47.60
Round  31, Train loss: 0.405, Test loss: 1.543, Test accuracy: 49.55
Round  32, Train loss: 0.375, Test loss: 1.309, Test accuracy: 53.50
Round  33, Train loss: 0.410, Test loss: 1.348, Test accuracy: 53.75
Round  34, Train loss: 0.357, Test loss: 1.300, Test accuracy: 53.90
Round  35, Train loss: 0.359, Test loss: 2.021, Test accuracy: 45.02
Round  36, Train loss: 0.340, Test loss: 1.438, Test accuracy: 51.85
Round  37, Train loss: 0.355, Test loss: 1.650, Test accuracy: 46.32
Round  38, Train loss: 0.350, Test loss: 1.791, Test accuracy: 48.88
Round  39, Train loss: 0.313, Test loss: 1.562, Test accuracy: 52.75
Round  40, Train loss: 0.298, Test loss: 1.824, Test accuracy: 48.48
Round  41, Train loss: 0.417, Test loss: 1.303, Test accuracy: 53.88
Round  42, Train loss: 0.358, Test loss: 1.243, Test accuracy: 56.35
Round  43, Train loss: 0.364, Test loss: 1.189, Test accuracy: 58.05
Round  44, Train loss: 0.354, Test loss: 1.341, Test accuracy: 53.30
Round  45, Train loss: 0.331, Test loss: 1.434, Test accuracy: 53.78
Round  46, Train loss: 0.318, Test loss: 1.405, Test accuracy: 53.25
Round  47, Train loss: 0.384, Test loss: 1.373, Test accuracy: 52.03
Round  48, Train loss: 0.385, Test loss: 1.510, Test accuracy: 50.55
Round  49, Train loss: 0.333, Test loss: 1.562, Test accuracy: 51.97
Round  50, Train loss: 0.358, Test loss: 1.698, Test accuracy: 51.17
Round  51, Train loss: 0.274, Test loss: 1.333, Test accuracy: 59.00
Round  52, Train loss: 0.288, Test loss: 2.107, Test accuracy: 47.40
Round  53, Train loss: 0.267, Test loss: 1.266, Test accuracy: 57.65
Round  54, Train loss: 0.245, Test loss: 1.400, Test accuracy: 56.17
Round  55, Train loss: 0.258, Test loss: 1.885, Test accuracy: 49.48
Round  56, Train loss: 0.240, Test loss: 1.339, Test accuracy: 57.77
Round  57, Train loss: 0.230, Test loss: 1.229, Test accuracy: 60.35
Round  58, Train loss: 0.244, Test loss: 1.644, Test accuracy: 55.67
Round  59, Train loss: 0.259, Test loss: 1.383, Test accuracy: 55.97
Round  60, Train loss: 0.210, Test loss: 1.317, Test accuracy: 60.30
Round  61, Train loss: 0.247, Test loss: 1.306, Test accuracy: 58.00
Round  62, Train loss: 0.223, Test loss: 1.269, Test accuracy: 58.63
Round  63, Train loss: 0.245, Test loss: 1.542, Test accuracy: 53.98
Round  64, Train loss: 0.235, Test loss: 1.232, Test accuracy: 60.32
Round  65, Train loss: 0.203, Test loss: 1.497, Test accuracy: 56.35
Round  66, Train loss: 0.225, Test loss: 1.800, Test accuracy: 53.48
Round  67, Train loss: 0.259, Test loss: 1.383, Test accuracy: 57.27
Round  68, Train loss: 0.200, Test loss: 1.217, Test accuracy: 61.23
Round  69, Train loss: 0.239, Test loss: 1.209, Test accuracy: 60.37
Round  70, Train loss: 0.298, Test loss: 1.341, Test accuracy: 56.47
Round  71, Train loss: 0.169, Test loss: 1.521, Test accuracy: 54.72
Round  72, Train loss: 0.200, Test loss: 1.381, Test accuracy: 57.28
Round  73, Train loss: 0.260, Test loss: 1.200, Test accuracy: 60.93
Round  74, Train loss: 0.235, Test loss: 1.494, Test accuracy: 55.07
Round  75, Train loss: 0.193, Test loss: 1.445, Test accuracy: 54.58
Round  76, Train loss: 0.149, Test loss: 1.432, Test accuracy: 58.22
Round  77, Train loss: 0.166, Test loss: 1.397, Test accuracy: 58.22
Round  78, Train loss: 0.212, Test loss: 1.707, Test accuracy: 50.52
Round  79, Train loss: 0.290, Test loss: 1.320, Test accuracy: 58.47
Round  80, Train loss: 0.152, Test loss: 1.343, Test accuracy: 57.72
Round  81, Train loss: 0.192, Test loss: 1.955, Test accuracy: 52.17
Round  82, Train loss: 0.191, Test loss: 1.556, Test accuracy: 56.87
Round  83, Train loss: 0.157, Test loss: 1.848, Test accuracy: 54.58
Round  84, Train loss: 0.148, Test loss: 1.424, Test accuracy: 57.80
Round  85, Train loss: 0.232, Test loss: 1.513, Test accuracy: 55.95
Round  86, Train loss: 0.224, Test loss: 1.488, Test accuracy: 55.77
Round  87, Train loss: 0.141, Test loss: 1.411, Test accuracy: 58.55
Round  88, Train loss: 0.191, Test loss: 1.368, Test accuracy: 59.15
Round  89, Train loss: 0.186, Test loss: 1.428, Test accuracy: 56.92
Round  90, Train loss: 0.145, Test loss: 1.821, Test accuracy: 49.23
Round  91, Train loss: 0.196, Test loss: 1.365, Test accuracy: 58.00
Round  92, Train loss: 0.157, Test loss: 1.452, Test accuracy: 56.05
Round  93, Train loss: 0.195, Test loss: 1.233, Test accuracy: 60.27
Round  94, Train loss: 0.129, Test loss: 1.217, Test accuracy: 61.08
Round  95, Train loss: 0.197, Test loss: 1.398, Test accuracy: 57.08
Round  96, Train loss: 0.136, Test loss: 1.540, Test accuracy: 57.57
Round  97, Train loss: 0.170, Test loss: 1.317, Test accuracy: 59.82
Round  98, Train loss: 0.121, Test loss: 1.270, Test accuracy: 61.47
Round  99, Train loss: 0.124, Test loss: 1.369, Test accuracy: 60.63
Final Round, Train loss: 0.150, Test loss: 1.190, Test accuracy: 63.08
Average accuracy final 10 rounds: 58.120000000000005
1238.3606481552124
[1.9489376544952393, 3.720839023590088, 5.516555309295654, 7.308404922485352, 9.108054637908936, 10.900348663330078, 12.694031953811646, 14.49066162109375, 16.28792691230774, 18.07745385169983, 19.87042760848999, 21.659749746322632, 23.44675374031067, 25.236303329467773, 27.02783989906311, 28.819941997528076, 30.606689453125, 32.399662733078, 34.19719433784485, 35.991090297698975, 37.77915143966675, 39.56648063659668, 41.35357403755188, 43.14318060874939, 44.934656381607056, 46.72577214241028, 48.51192116737366, 50.30150318145752, 52.09269142150879, 53.88857674598694, 55.68168091773987, 57.4686803817749, 59.258793115615845, 61.04878807067871, 62.83780336380005, 64.62094974517822, 66.41688227653503, 68.18087482452393, 69.94639205932617, 71.7239499092102, 73.48094463348389, 75.24323153495789, 77.00136971473694, 78.76396012306213, 80.52389526367188, 82.28292393684387, 84.051096200943, 85.81334066390991, 87.57888722419739, 89.34178733825684, 91.10431694984436, 92.88356518745422, 94.66843271255493, 96.45273756980896, 98.23955154418945, 100.01987028121948, 101.81113624572754, 103.59301781654358, 105.38175010681152, 107.16220593452454, 108.9495644569397, 110.74500560760498, 112.5241346359253, 114.31654977798462, 116.10254621505737, 117.89642858505249, 119.67395687103271, 121.46990370750427, 123.26453828811646, 125.04739356040955, 126.84511756896973, 128.63193464279175, 130.41211795806885, 132.18705439567566, 133.97050404548645, 135.74439930915833, 137.50654220581055, 139.29074573516846, 141.08764100074768, 142.87387466430664, 144.66639947891235, 146.46178770065308, 148.24548172950745, 150.03635597229004, 151.8190634250641, 153.608891248703, 155.38848972320557, 157.17733645439148, 158.96461987495422, 160.75479340553284, 162.54476261138916, 164.33463716506958, 166.1232476234436, 167.7436683177948, 169.36631274223328, 170.99038314819336, 172.61186170578003, 174.2286238670349, 175.8452820777893, 177.4618525505066, 179.08187437057495]
[6.733333333333333, 24.1, 27.35, 27.35, 32.15, 30.266666666666666, 35.31666666666667, 36.766666666666666, 39.083333333333336, 34.56666666666667, 32.983333333333334, 39.916666666666664, 42.38333333333333, 44.43333333333333, 44.53333333333333, 35.75, 43.733333333333334, 42.85, 44.1, 42.083333333333336, 42.43333333333333, 48.18333333333333, 42.233333333333334, 47.03333333333333, 51.93333333333333, 51.766666666666666, 44.55, 43.266666666666666, 51.7, 50.7, 47.6, 49.55, 53.5, 53.75, 53.9, 45.016666666666666, 51.85, 46.31666666666667, 48.88333333333333, 52.75, 48.483333333333334, 53.88333333333333, 56.35, 58.05, 53.3, 53.78333333333333, 53.25, 52.03333333333333, 50.55, 51.96666666666667, 51.166666666666664, 59.0, 47.4, 57.65, 56.166666666666664, 49.483333333333334, 57.766666666666666, 60.35, 55.666666666666664, 55.96666666666667, 60.3, 58.0, 58.63333333333333, 53.983333333333334, 60.31666666666667, 56.35, 53.483333333333334, 57.266666666666666, 61.233333333333334, 60.36666666666667, 56.46666666666667, 54.71666666666667, 57.28333333333333, 60.93333333333333, 55.06666666666667, 54.583333333333336, 58.21666666666667, 58.21666666666667, 50.516666666666666, 58.46666666666667, 57.71666666666667, 52.166666666666664, 56.86666666666667, 54.583333333333336, 57.8, 55.95, 55.766666666666666, 58.55, 59.15, 56.916666666666664, 49.233333333333334, 58.0, 56.05, 60.266666666666666, 61.083333333333336, 57.083333333333336, 57.56666666666667, 59.81666666666667, 61.46666666666667, 60.63333333333333, 63.083333333333336]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.302, Test loss: 2.212, Test accuracy: 22.66
Round   1, Train loss: 2.153, Test loss: 2.037, Test accuracy: 26.73
Round   2, Train loss: 2.017, Test loss: 1.916, Test accuracy: 31.30
Round   3, Train loss: 1.914, Test loss: 1.836, Test accuracy: 33.88
Round   4, Train loss: 1.847, Test loss: 1.768, Test accuracy: 35.88
Round   5, Train loss: 1.789, Test loss: 1.707, Test accuracy: 37.92
Round   6, Train loss: 1.744, Test loss: 1.657, Test accuracy: 39.28
Round   7, Train loss: 1.679, Test loss: 1.614, Test accuracy: 41.01
Round   8, Train loss: 1.657, Test loss: 1.588, Test accuracy: 41.47
Round   9, Train loss: 1.613, Test loss: 1.554, Test accuracy: 42.87
Round  10, Train loss: 1.579, Test loss: 1.536, Test accuracy: 43.60
Round  11, Train loss: 1.564, Test loss: 1.510, Test accuracy: 44.98
Round  12, Train loss: 1.538, Test loss: 1.478, Test accuracy: 46.55
Round  13, Train loss: 1.533, Test loss: 1.455, Test accuracy: 47.06
Round  14, Train loss: 1.509, Test loss: 1.449, Test accuracy: 47.55
Round  15, Train loss: 1.469, Test loss: 1.427, Test accuracy: 48.79
Round  16, Train loss: 1.431, Test loss: 1.430, Test accuracy: 48.98
Round  17, Train loss: 1.430, Test loss: 1.395, Test accuracy: 50.29
Round  18, Train loss: 1.401, Test loss: 1.386, Test accuracy: 51.08
Round  19, Train loss: 1.399, Test loss: 1.372, Test accuracy: 50.92
Round  20, Train loss: 1.375, Test loss: 1.363, Test accuracy: 51.79
Round  21, Train loss: 1.360, Test loss: 1.333, Test accuracy: 52.66
Round  22, Train loss: 1.347, Test loss: 1.312, Test accuracy: 53.34
Round  23, Train loss: 1.312, Test loss: 1.309, Test accuracy: 53.67
Round  24, Train loss: 1.289, Test loss: 1.303, Test accuracy: 53.62
Round  25, Train loss: 1.312, Test loss: 1.267, Test accuracy: 55.05
Round  26, Train loss: 1.301, Test loss: 1.261, Test accuracy: 55.22
Round  27, Train loss: 1.253, Test loss: 1.259, Test accuracy: 55.34
Round  28, Train loss: 1.231, Test loss: 1.247, Test accuracy: 55.47
Round  29, Train loss: 1.216, Test loss: 1.233, Test accuracy: 56.37
Round  30, Train loss: 1.195, Test loss: 1.227, Test accuracy: 56.88
Round  31, Train loss: 1.180, Test loss: 1.205, Test accuracy: 57.67
Round  32, Train loss: 1.192, Test loss: 1.207, Test accuracy: 57.45
Round  33, Train loss: 1.136, Test loss: 1.211, Test accuracy: 57.47
Round  34, Train loss: 1.146, Test loss: 1.201, Test accuracy: 57.80
Round  35, Train loss: 1.136, Test loss: 1.189, Test accuracy: 58.20
Round  36, Train loss: 1.123, Test loss: 1.169, Test accuracy: 58.95
Round  37, Train loss: 1.160, Test loss: 1.151, Test accuracy: 59.53
Round  38, Train loss: 1.075, Test loss: 1.149, Test accuracy: 59.87
Round  39, Train loss: 1.052, Test loss: 1.152, Test accuracy: 59.49
Round  40, Train loss: 1.104, Test loss: 1.130, Test accuracy: 60.09
Round  41, Train loss: 1.051, Test loss: 1.127, Test accuracy: 60.52
Round  42, Train loss: 1.029, Test loss: 1.126, Test accuracy: 60.61
Round  43, Train loss: 1.034, Test loss: 1.120, Test accuracy: 61.02
Round  44, Train loss: 1.041, Test loss: 1.112, Test accuracy: 61.21
Round  45, Train loss: 1.024, Test loss: 1.123, Test accuracy: 60.76
Round  46, Train loss: 1.011, Test loss: 1.112, Test accuracy: 61.05
Round  47, Train loss: 0.985, Test loss: 1.102, Test accuracy: 61.26
Round  48, Train loss: 0.996, Test loss: 1.104, Test accuracy: 61.41
Round  49, Train loss: 0.966, Test loss: 1.081, Test accuracy: 62.32
Round  50, Train loss: 0.959, Test loss: 1.085, Test accuracy: 62.28
Round  51, Train loss: 0.939, Test loss: 1.084, Test accuracy: 62.24
Round  52, Train loss: 0.962, Test loss: 1.076, Test accuracy: 62.59
Round  53, Train loss: 0.896, Test loss: 1.070, Test accuracy: 62.96
Round  54, Train loss: 0.933, Test loss: 1.074, Test accuracy: 62.66
Round  55, Train loss: 0.922, Test loss: 1.066, Test accuracy: 62.58
Round  56, Train loss: 0.905, Test loss: 1.063, Test accuracy: 62.90
Round  57, Train loss: 0.885, Test loss: 1.047, Test accuracy: 63.55
Round  58, Train loss: 0.851, Test loss: 1.050, Test accuracy: 63.65
Round  59, Train loss: 0.846, Test loss: 1.059, Test accuracy: 63.62
Round  60, Train loss: 0.841, Test loss: 1.059, Test accuracy: 63.60
Round  61, Train loss: 0.893, Test loss: 1.059, Test accuracy: 63.38
Round  62, Train loss: 0.832, Test loss: 1.064, Test accuracy: 63.37
Round  63, Train loss: 0.842, Test loss: 1.068, Test accuracy: 62.93
Round  64, Train loss: 0.835, Test loss: 1.055, Test accuracy: 63.63
Round  65, Train loss: 0.823, Test loss: 1.063, Test accuracy: 63.60
Round  66, Train loss: 0.803, Test loss: 1.042, Test accuracy: 64.17
Round  67, Train loss: 0.811, Test loss: 1.033, Test accuracy: 64.47
Round  68, Train loss: 0.798, Test loss: 1.038, Test accuracy: 64.56
Round  69, Train loss: 0.809, Test loss: 1.056, Test accuracy: 64.12
Round  70, Train loss: 0.798, Test loss: 1.023, Test accuracy: 64.90
Round  71, Train loss: 0.814, Test loss: 1.023, Test accuracy: 64.88
Round  72, Train loss: 0.788, Test loss: 1.028, Test accuracy: 65.26
Round  73, Train loss: 0.764, Test loss: 1.012, Test accuracy: 65.27
Round  74, Train loss: 0.747, Test loss: 1.008, Test accuracy: 65.28
Round  75, Train loss: 0.761, Test loss: 1.019, Test accuracy: 65.14
Round  76, Train loss: 0.744, Test loss: 1.027, Test accuracy: 65.04
Round  77, Train loss: 0.751, Test loss: 1.026, Test accuracy: 65.01
Round  78, Train loss: 0.751, Test loss: 1.023, Test accuracy: 65.02
Round  79, Train loss: 0.759, Test loss: 1.013, Test accuracy: 65.50
Round  80, Train loss: 0.731, Test loss: 1.017, Test accuracy: 65.54
Round  81, Train loss: 0.720, Test loss: 1.019, Test accuracy: 65.50
Round  82, Train loss: 0.734, Test loss: 1.021, Test accuracy: 65.51
Round  83, Train loss: 0.691, Test loss: 1.019, Test accuracy: 65.61
Round  84, Train loss: 0.715, Test loss: 1.018, Test accuracy: 65.96
Round  85, Train loss: 0.723, Test loss: 1.018, Test accuracy: 66.43
Round  86, Train loss: 0.731, Test loss: 1.024, Test accuracy: 65.45
Round  87, Train loss: 0.709, Test loss: 1.024, Test accuracy: 65.64
Round  88, Train loss: 0.693, Test loss: 1.019, Test accuracy: 65.71
Round  89, Train loss: 0.655, Test loss: 1.022, Test accuracy: 65.50
Round  90, Train loss: 0.669, Test loss: 1.017, Test accuracy: 65.97
Round  91, Train loss: 0.637, Test loss: 1.027, Test accuracy: 65.55
Round  92, Train loss: 0.649, Test loss: 1.021, Test accuracy: 65.89
Round  93, Train loss: 0.654, Test loss: 1.004, Test accuracy: 66.36
Round  94, Train loss: 0.676, Test loss: 1.020, Test accuracy: 65.99
Round  95, Train loss: 0.646, Test loss: 0.996, Test accuracy: 66.51
Round  96, Train loss: 0.652, Test loss: 1.004, Test accuracy: 66.47
Round  97, Train loss: 0.624, Test loss: 1.033, Test accuracy: 65.92
Round  98, Train loss: 0.643, Test loss: 1.031, Test accuracy: 65.79
Round  99, Train loss: 0.652, Test loss: 1.011, Test accuracy: 66.42
Final Round, Train loss: 0.558, Test loss: 1.001, Test accuracy: 66.96
Average accuracy final 10 rounds: 66.08475000000001
1812.59769821167
[1.6548147201538086, 2.9890472888946533, 4.35671854019165, 5.725766181945801, 7.072317838668823, 8.430705308914185, 9.764854907989502, 11.08930516242981, 12.418918132781982, 13.760572910308838, 15.101111650466919, 16.440269231796265, 17.797749996185303, 19.150556087493896, 20.485941886901855, 21.825692892074585, 23.15344262123108, 24.495517015457153, 25.838473081588745, 27.1705424785614, 28.503080368041992, 29.855730533599854, 31.1963050365448, 32.533958196640015, 33.863035678863525, 35.19996500015259, 36.53915572166443, 37.87656760215759, 39.21222925186157, 40.55272459983826, 41.89807891845703, 43.230043172836304, 44.573097229003906, 45.91455912590027, 47.25843858718872, 48.598989486694336, 49.94988417625427, 51.27608942985535, 52.625048875808716, 53.962684631347656, 55.30240440368652, 56.6356520652771, 57.8450870513916, 59.03776741027832, 60.250553131103516, 61.45364999771118, 62.65691876411438, 63.86938691139221, 65.0703010559082, 66.2649827003479, 67.4769389629364, 68.67855453491211, 69.8787636756897, 71.08522486686707, 72.28324937820435, 73.49955034255981, 74.69919490814209, 75.9081072807312, 77.11471605300903, 78.31614112854004, 79.5199785232544, 80.73480033874512, 81.9435384273529, 83.14706873893738, 84.3599362373352, 85.58564734458923, 86.79015493392944, 87.99966168403625, 89.203866481781, 90.41180801391602, 91.61522674560547, 92.9615786075592, 94.29318046569824, 95.62343835830688, 96.96520352363586, 98.3247606754303, 99.64591407775879, 100.95727276802063, 102.30830550193787, 103.64428210258484, 104.94687676429749, 106.26628565788269, 107.59383296966553, 108.9294764995575, 110.26562333106995, 111.61229038238525, 112.95149064064026, 114.3015878200531, 115.65852642059326, 116.99322819709778, 118.33219337463379, 119.6764829158783, 121.00859761238098, 122.33551716804504, 123.66512656211853, 125.0125195980072, 126.36572575569153, 127.69761300086975, 129.0418679714203, 130.38107013702393, 132.42631721496582]
[22.655, 26.735, 31.3, 33.8775, 35.875, 37.92, 39.285, 41.0125, 41.4725, 42.865, 43.5975, 44.9775, 46.5475, 47.065, 47.545, 48.79, 48.985, 50.2875, 51.0825, 50.925, 51.7875, 52.665, 53.34, 53.6725, 53.6225, 55.045, 55.2225, 55.3425, 55.465, 56.3675, 56.885, 57.67, 57.4475, 57.4725, 57.8025, 58.2025, 58.955, 59.5325, 59.8725, 59.495, 60.09, 60.5225, 60.61, 61.0225, 61.2125, 60.76, 61.045, 61.26, 61.4125, 62.32, 62.2825, 62.2375, 62.5875, 62.9625, 62.6625, 62.575, 62.9025, 63.5475, 63.6475, 63.6225, 63.5975, 63.38, 63.3675, 62.9275, 63.635, 63.6025, 64.17, 64.465, 64.565, 64.1175, 64.9025, 64.875, 65.2625, 65.265, 65.2775, 65.135, 65.0425, 65.01, 65.015, 65.5, 65.5375, 65.495, 65.51, 65.6125, 65.96, 66.4325, 65.4475, 65.645, 65.7125, 65.495, 65.965, 65.545, 65.8875, 66.365, 65.9925, 66.51, 66.465, 65.915, 65.7875, 66.415, 66.9625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.573, Test loss: 2.205, Test accuracy: 17.13
Round   1, Train loss: 1.038, Test loss: 1.914, Test accuracy: 32.96
Round   2, Train loss: 0.962, Test loss: 1.630, Test accuracy: 42.28
Round   3, Train loss: 0.834, Test loss: 1.218, Test accuracy: 55.20
Round   4, Train loss: 0.848, Test loss: 1.013, Test accuracy: 60.21
Round   5, Train loss: 0.782, Test loss: 0.870, Test accuracy: 65.76
Round   6, Train loss: 0.706, Test loss: 0.821, Test accuracy: 67.56
Round   7, Train loss: 0.814, Test loss: 0.772, Test accuracy: 69.75
Round   8, Train loss: 0.707, Test loss: 0.667, Test accuracy: 72.64
Round   9, Train loss: 0.616, Test loss: 0.663, Test accuracy: 73.21
Round  10, Train loss: 0.690, Test loss: 0.654, Test accuracy: 73.68
Round  11, Train loss: 0.713, Test loss: 0.628, Test accuracy: 75.01
Round  12, Train loss: 0.643, Test loss: 0.627, Test accuracy: 75.59
Round  13, Train loss: 0.598, Test loss: 0.604, Test accuracy: 76.68
Round  14, Train loss: 0.600, Test loss: 0.601, Test accuracy: 76.82
Round  15, Train loss: 0.625, Test loss: 0.608, Test accuracy: 77.06
Round  16, Train loss: 0.638, Test loss: 0.587, Test accuracy: 77.17
Round  17, Train loss: 0.514, Test loss: 0.585, Test accuracy: 77.35
Round  18, Train loss: 0.561, Test loss: 0.570, Test accuracy: 77.56
Round  19, Train loss: 0.523, Test loss: 0.552, Test accuracy: 77.51
Round  20, Train loss: 0.536, Test loss: 0.556, Test accuracy: 77.82
Round  21, Train loss: 0.520, Test loss: 0.536, Test accuracy: 78.16
Round  22, Train loss: 0.582, Test loss: 0.533, Test accuracy: 78.88
Round  23, Train loss: 0.581, Test loss: 0.548, Test accuracy: 78.91
Round  24, Train loss: 0.538, Test loss: 0.512, Test accuracy: 79.64
Round  25, Train loss: 0.536, Test loss: 0.512, Test accuracy: 79.86
Round  26, Train loss: 0.471, Test loss: 0.501, Test accuracy: 79.99
Round  27, Train loss: 0.519, Test loss: 0.511, Test accuracy: 79.37
Round  28, Train loss: 0.417, Test loss: 0.495, Test accuracy: 80.18
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2226, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.960, Test loss: 1.021, Test accuracy: 43.38 

Round   0, Global train loss: 0.960, Global test loss: 1.096, Global test accuracy: 36.72 

Round   1, Train loss: 0.866, Test loss: 0.960, Test accuracy: 49.67 

Round   1, Global train loss: 0.866, Global test loss: 1.115, Global test accuracy: 37.67 

Round   2, Train loss: 0.768, Test loss: 0.920, Test accuracy: 54.96 

Round   2, Global train loss: 0.768, Global test loss: 1.178, Global test accuracy: 37.56 

Round   3, Train loss: 0.657, Test loss: 0.828, Test accuracy: 59.01 

Round   3, Global train loss: 0.657, Global test loss: 1.103, Global test accuracy: 38.17 

Round   4, Train loss: 0.681, Test loss: 0.798, Test accuracy: 60.18 

Round   4, Global train loss: 0.681, Global test loss: 1.110, Global test accuracy: 37.00 

Round   5, Train loss: 0.633, Test loss: 0.760, Test accuracy: 63.33 

Round   5, Global train loss: 0.633, Global test loss: 1.141, Global test accuracy: 36.01 

Round   6, Train loss: 0.718, Test loss: 0.747, Test accuracy: 63.09 

Round   6, Global train loss: 0.718, Global test loss: 1.144, Global test accuracy: 32.39 

Round   7, Train loss: 0.622, Test loss: 0.765, Test accuracy: 65.07 

Round   7, Global train loss: 0.622, Global test loss: 1.237, Global test accuracy: 39.49 

Round   8, Train loss: 0.657, Test loss: 0.690, Test accuracy: 68.58 

Round   8, Global train loss: 0.657, Global test loss: 1.158, Global test accuracy: 40.01 

Round   9, Train loss: 0.563, Test loss: 0.661, Test accuracy: 69.87 

Round   9, Global train loss: 0.563, Global test loss: 1.131, Global test accuracy: 38.94 

Round  10, Train loss: 0.530, Test loss: 0.653, Test accuracy: 70.57 

Round  10, Global train loss: 0.530, Global test loss: 1.166, Global test accuracy: 38.87 

Round  11, Train loss: 0.549, Test loss: 0.657, Test accuracy: 70.38 

Round  11, Global train loss: 0.549, Global test loss: 1.119, Global test accuracy: 38.73 

Round  12, Train loss: 0.496, Test loss: 0.643, Test accuracy: 71.11 

Round  12, Global train loss: 0.496, Global test loss: 1.151, Global test accuracy: 37.49 

Round  13, Train loss: 0.506, Test loss: 0.641, Test accuracy: 71.49 

Round  13, Global train loss: 0.506, Global test loss: 1.122, Global test accuracy: 38.91 

Round  14, Train loss: 0.507, Test loss: 0.639, Test accuracy: 71.72 

Round  14, Global train loss: 0.507, Global test loss: 1.198, Global test accuracy: 38.00 

Round  15, Train loss: 0.518, Test loss: 0.645, Test accuracy: 71.42 

Round  15, Global train loss: 0.518, Global test loss: 1.151, Global test accuracy: 37.94 

Round  16, Train loss: 0.436, Test loss: 0.648, Test accuracy: 71.32 

Round  16, Global train loss: 0.436, Global test loss: 1.139, Global test accuracy: 33.76 

Round  17, Train loss: 0.435, Test loss: 0.645, Test accuracy: 72.49 

Round  17, Global train loss: 0.435, Global test loss: 1.264, Global test accuracy: 35.34 

Round  18, Train loss: 0.550, Test loss: 0.620, Test accuracy: 74.09 

Round  18, Global train loss: 0.550, Global test loss: 1.149, Global test accuracy: 41.55 

Round  19, Train loss: 0.514, Test loss: 0.615, Test accuracy: 75.00 

Round  19, Global train loss: 0.514, Global test loss: 1.098, Global test accuracy: 39.37 

Round  20, Train loss: 0.417, Test loss: 0.620, Test accuracy: 75.12 

Round  20, Global train loss: 0.417, Global test loss: 1.297, Global test accuracy: 40.52 

Round  21, Train loss: 0.433, Test loss: 0.643, Test accuracy: 74.56 

Round  21, Global train loss: 0.433, Global test loss: 1.153, Global test accuracy: 40.25 

Round  22, Train loss: 0.461, Test loss: 0.637, Test accuracy: 74.97 

Round  22, Global train loss: 0.461, Global test loss: 1.202, Global test accuracy: 34.85 

Round  23, Train loss: 0.449, Test loss: 0.624, Test accuracy: 75.50 

Round  23, Global train loss: 0.449, Global test loss: 1.297, Global test accuracy: 35.37 

Round  24, Train loss: 0.384, Test loss: 0.612, Test accuracy: 75.72 

Round  24, Global train loss: 0.384, Global test loss: 1.190, Global test accuracy: 34.53 

Round  25, Train loss: 0.375, Test loss: 0.626, Test accuracy: 75.42 

Round  25, Global train loss: 0.375, Global test loss: 1.280, Global test accuracy: 40.58 

Round  26, Train loss: 0.344, Test loss: 0.639, Test accuracy: 75.61 

Round  26, Global train loss: 0.344, Global test loss: 1.294, Global test accuracy: 38.47 

Round  27, Train loss: 0.339, Test loss: 0.647, Test accuracy: 75.38 

Round  27, Global train loss: 0.339, Global test loss: 1.170, Global test accuracy: 36.72 

Round  28, Train loss: 0.310, Test loss: 0.645, Test accuracy: 75.67 

Round  28, Global train loss: 0.310, Global test loss: 1.335, Global test accuracy: 39.52 

Round  29, Train loss: 0.364, Test loss: 0.645, Test accuracy: 75.56 

Round  29, Global train loss: 0.364, Global test loss: 1.160, Global test accuracy: 37.98 

Round  30, Train loss: 0.305, Test loss: 0.691, Test accuracy: 75.31 

Round  30, Global train loss: 0.305, Global test loss: 1.279, Global test accuracy: 37.40 

Round  31, Train loss: 0.357, Test loss: 0.688, Test accuracy: 75.49 

Round  31, Global train loss: 0.357, Global test loss: 1.168, Global test accuracy: 39.19 

Round  32, Train loss: 0.284, Test loss: 0.695, Test accuracy: 75.42 

Round  32, Global train loss: 0.284, Global test loss: 1.205, Global test accuracy: 33.25 

Round  33, Train loss: 0.314, Test loss: 0.703, Test accuracy: 75.62 

Round  33, Global train loss: 0.314, Global test loss: 1.208, Global test accuracy: 39.90 

Round  34, Train loss: 0.311, Test loss: 0.717, Test accuracy: 75.53 

Round  34, Global train loss: 0.311, Global test loss: 1.334, Global test accuracy: 38.79 

Round  35, Train loss: 0.223, Test loss: 0.720, Test accuracy: 75.32 

Round  35, Global train loss: 0.223, Global test loss: 1.344, Global test accuracy: 39.31 

Round  36, Train loss: 0.237, Test loss: 0.717, Test accuracy: 76.12 

Round  36, Global train loss: 0.237, Global test loss: 1.207, Global test accuracy: 38.47 

Round  37, Train loss: 0.340, Test loss: 0.717, Test accuracy: 76.44 

Round  37, Global train loss: 0.340, Global test loss: 1.173, Global test accuracy: 37.40 

Round  38, Train loss: 0.287, Test loss: 0.713, Test accuracy: 76.62 

Round  38, Global train loss: 0.287, Global test loss: 1.237, Global test accuracy: 36.41 

Round  39, Train loss: 0.207, Test loss: 0.716, Test accuracy: 76.61 

Round  39, Global train loss: 0.207, Global test loss: 1.423, Global test accuracy: 33.23 

Round  40, Train loss: 0.226, Test loss: 0.731, Test accuracy: 76.50 

Round  40, Global train loss: 0.226, Global test loss: 1.390, Global test accuracy: 39.13 

Round  41, Train loss: 0.272, Test loss: 0.736, Test accuracy: 76.19 

Round  41, Global train loss: 0.272, Global test loss: 1.181, Global test accuracy: 38.95 

Round  42, Train loss: 0.297, Test loss: 0.782, Test accuracy: 75.85 

Round  42, Global train loss: 0.297, Global test loss: 1.283, Global test accuracy: 38.43 

Round  43, Train loss: 0.256, Test loss: 0.791, Test accuracy: 76.02 

Round  43, Global train loss: 0.256, Global test loss: 1.293, Global test accuracy: 38.49 

Round  44, Train loss: 0.299, Test loss: 0.783, Test accuracy: 76.28 

Round  44, Global train loss: 0.299, Global test loss: 1.169, Global test accuracy: 36.16 

Round  45, Train loss: 0.221, Test loss: 0.773, Test accuracy: 76.87 

Round  45, Global train loss: 0.221, Global test loss: 1.444, Global test accuracy: 32.58 

Round  46, Train loss: 0.161, Test loss: 0.778, Test accuracy: 76.92 

Round  46, Global train loss: 0.161, Global test loss: 1.260, Global test accuracy: 39.11 

Round  47, Train loss: 0.172, Test loss: 0.784, Test accuracy: 76.97 

Round  47, Global train loss: 0.172, Global test loss: 1.311, Global test accuracy: 38.32 

Round  48, Train loss: 0.204, Test loss: 0.801, Test accuracy: 76.59 

Round  48, Global train loss: 0.204, Global test loss: 1.320, Global test accuracy: 37.02 

Round  49, Train loss: 0.213, Test loss: 0.807, Test accuracy: 76.72 

Round  49, Global train loss: 0.213, Global test loss: 1.334, Global test accuracy: 36.14 

Round  50, Train loss: 0.197, Test loss: 0.831, Test accuracy: 76.39 

Round  50, Global train loss: 0.197, Global test loss: 1.342, Global test accuracy: 39.57 

Round  51, Train loss: 0.218, Test loss: 0.834, Test accuracy: 76.46 

Round  51, Global train loss: 0.218, Global test loss: 1.210, Global test accuracy: 34.69 

Round  52, Train loss: 0.192, Test loss: 0.839, Test accuracy: 76.51 

Round  52, Global train loss: 0.192, Global test loss: 1.261, Global test accuracy: 36.51 

Round  53, Train loss: 0.181, Test loss: 0.848, Test accuracy: 76.78 

Round  53, Global train loss: 0.181, Global test loss: 1.389, Global test accuracy: 37.77 

Round  54, Train loss: 0.150, Test loss: 0.859, Test accuracy: 76.90 

Round  54, Global train loss: 0.150, Global test loss: 1.388, Global test accuracy: 38.98 

Round  55, Train loss: 0.145, Test loss: 0.866, Test accuracy: 77.38 

Round  55, Global train loss: 0.145, Global test loss: 1.229, Global test accuracy: 35.52 

Round  56, Train loss: 0.173, Test loss: 0.846, Test accuracy: 77.77 

Round  56, Global train loss: 0.173, Global test loss: 1.860, Global test accuracy: 40.35 

Round  57, Train loss: 0.145, Test loss: 0.854, Test accuracy: 77.70 

Round  57, Global train loss: 0.145, Global test loss: 1.320, Global test accuracy: 35.63 

Round  58, Train loss: 0.131, Test loss: 0.878, Test accuracy: 77.59 

Round  58, Global train loss: 0.131, Global test loss: 1.354, Global test accuracy: 37.26 

Round  59, Train loss: 0.132, Test loss: 0.901, Test accuracy: 77.25 

Round  59, Global train loss: 0.132, Global test loss: 1.324, Global test accuracy: 37.96 

Round  60, Train loss: 0.190, Test loss: 0.928, Test accuracy: 76.92 

Round  60, Global train loss: 0.190, Global test loss: 1.167, Global test accuracy: 38.83 

Round  61, Train loss: 0.165, Test loss: 0.962, Test accuracy: 76.55 

Round  61, Global train loss: 0.165, Global test loss: 1.410, Global test accuracy: 37.48 

Round  62, Train loss: 0.140, Test loss: 0.949, Test accuracy: 76.83 

Round  62, Global train loss: 0.140, Global test loss: 1.212, Global test accuracy: 35.17 

Round  63, Train loss: 0.124, Test loss: 0.961, Test accuracy: 76.75 

Round  63, Global train loss: 0.124, Global test loss: 1.321, Global test accuracy: 38.92 

Round  64, Train loss: 0.145, Test loss: 0.967, Test accuracy: 76.51 

Round  64, Global train loss: 0.145, Global test loss: 1.463, Global test accuracy: 39.38 

Round  65, Train loss: 0.102, Test loss: 0.961, Test accuracy: 76.83 

Round  65, Global train loss: 0.102, Global test loss: 1.308, Global test accuracy: 39.47 

Round  66, Train loss: 0.120, Test loss: 0.970, Test accuracy: 76.87 

Round  66, Global train loss: 0.120, Global test loss: 1.304, Global test accuracy: 39.17 

Round  67, Train loss: 0.128, Test loss: 0.982, Test accuracy: 76.66 

Round  67, Global train loss: 0.128, Global test loss: 1.210, Global test accuracy: 36.02 

Round  68, Train loss: 0.138, Test loss: 1.010, Test accuracy: 76.73 

Round  68, Global train loss: 0.138, Global test loss: 1.407, Global test accuracy: 37.73 

Round  69, Train loss: 0.087, Test loss: 1.016, Test accuracy: 76.73 

Round  69, Global train loss: 0.087, Global test loss: 2.291, Global test accuracy: 37.79 

Round  70, Train loss: 0.116, Test loss: 0.997, Test accuracy: 76.83 

Round  70, Global train loss: 0.116, Global test loss: 1.256, Global test accuracy: 39.38 

Round  71, Train loss: 0.092, Test loss: 1.001, Test accuracy: 76.42 

Round  71, Global train loss: 0.092, Global test loss: 1.323, Global test accuracy: 33.58 

Round  72, Train loss: 0.107, Test loss: 1.019, Test accuracy: 76.43 

Round  72, Global train loss: 0.107, Global test loss: 1.298, Global test accuracy: 33.99 

Round  73, Train loss: 0.100, Test loss: 1.056, Test accuracy: 76.19 

Round  73, Global train loss: 0.100, Global test loss: 1.503, Global test accuracy: 38.04 

Round  74, Train loss: 0.107, Test loss: 1.046, Test accuracy: 76.84 

Round  74, Global train loss: 0.107, Global test loss: 1.354, Global test accuracy: 36.73 

Round  75, Train loss: 0.097, Test loss: 1.026, Test accuracy: 76.78 

Round  75, Global train loss: 0.097, Global test loss: 1.260, Global test accuracy: 40.08 

Round  76, Train loss: 0.104, Test loss: 1.017, Test accuracy: 76.92 

Round  76, Global train loss: 0.104, Global test loss: 1.401, Global test accuracy: 37.90 

Round  77, Train loss: 0.077, Test loss: 1.048, Test accuracy: 77.08 

Round  77, Global train loss: 0.077, Global test loss: 1.662, Global test accuracy: 40.86 

Round  78, Train loss: 0.116, Test loss: 1.046, Test accuracy: 76.95 

Round  78, Global train loss: 0.116, Global test loss: 1.283, Global test accuracy: 37.52 

Round  79, Train loss: 0.094, Test loss: 1.051, Test accuracy: 77.01 

Round  79, Global train loss: 0.094, Global test loss: 1.474, Global test accuracy: 39.63 

Round  80, Train loss: 0.076, Test loss: 1.027, Test accuracy: 77.37 

Round  80, Global train loss: 0.076, Global test loss: 1.400, Global test accuracy: 38.65 

Round  81, Train loss: 0.084, Test loss: 1.036, Test accuracy: 77.34 

Round  81, Global train loss: 0.084, Global test loss: 1.510, Global test accuracy: 37.98 

Round  82, Train loss: 0.057, Test loss: 1.039, Test accuracy: 77.47 

Round  82, Global train loss: 0.057, Global test loss: 1.765, Global test accuracy: 40.24 

Round  83, Train loss: 0.101, Test loss: 1.104, Test accuracy: 76.93 

Round  83, Global train loss: 0.101, Global test loss: 1.327, Global test accuracy: 34.65 

Round  84, Train loss: 0.068, Test loss: 1.142, Test accuracy: 76.81 

Round  84, Global train loss: 0.068, Global test loss: 1.313, Global test accuracy: 35.52 

Round  85, Train loss: 0.115, Test loss: 1.149, Test accuracy: 76.83 

Round  85, Global train loss: 0.115, Global test loss: 1.420, Global test accuracy: 40.38 

Round  86, Train loss: 0.072, Test loss: 1.172, Test accuracy: 76.40 

Round  86, Global train loss: 0.072, Global test loss: 1.231, Global test accuracy: 38.76 

Round  87, Train loss: 0.084, Test loss: 1.152, Test accuracy: 76.13 

Round  87, Global train loss: 0.084, Global test loss: 1.656, Global test accuracy: 39.24 

Round  88, Train loss: 0.067, Test loss: 1.133, Test accuracy: 76.43 

Round  88, Global train loss: 0.067, Global test loss: 1.382, Global test accuracy: 39.15 

Round  89, Train loss: 0.108, Test loss: 1.144, Test accuracy: 76.76 

Round  89, Global train loss: 0.108, Global test loss: 1.250, Global test accuracy: 38.24 

Round  90, Train loss: 0.061, Test loss: 1.154, Test accuracy: 76.89 

Round  90, Global train loss: 0.061, Global test loss: 1.321, Global test accuracy: 36.68 

Round  91, Train loss: 0.057, Test loss: 1.153, Test accuracy: 77.15 

Round  91, Global train loss: 0.057, Global test loss: 1.597, Global test accuracy: 38.26 

Round  92, Train loss: 0.082, Test loss: 1.157, Test accuracy: 77.51 

Round  92, Global train loss: 0.082, Global test loss: 1.480, Global test accuracy: 37.12 

Round  93, Train loss: 0.077, Test loss: 1.150, Test accuracy: 77.42 

Round  93, Global train loss: 0.077, Global test loss: 1.718, Global test accuracy: 39.38 

Round  94, Train loss: 0.068, Test loss: 1.135, Test accuracy: 77.47 

Round  94, Global train loss: 0.068, Global test loss: 1.335, Global test accuracy: 37.60 

Round  95, Train loss: 0.054, Test loss: 1.154, Test accuracy: 77.28 

Round  95, Global train loss: 0.054, Global test loss: 1.728, Global test accuracy: 39.12 

Round  96, Train loss: 0.047, Test loss: 1.177, Test accuracy: 77.28 

Round  96, Global train loss: 0.047, Global test loss: 2.417, Global test accuracy: 37.93 

Round  97, Train loss: 0.079, Test loss: 1.153, Test accuracy: 77.50 

Round  97, Global train loss: 0.079, Global test loss: 1.329, Global test accuracy: 39.76 

Round  98, Train loss: 0.068, Test loss: 1.142, Test accuracy: 77.45 

Round  98, Global train loss: 0.068, Global test loss: 1.316, Global test accuracy: 36.42 

Round  99, Train loss: 0.048, Test loss: 1.147, Test accuracy: 77.47 

Round  99, Global train loss: 0.048, Global test loss: 1.194, Global test accuracy: 39.27 

Final Round, Train loss: 0.058, Test loss: 1.220, Test accuracy: 77.68 

Final Round, Global train loss: 0.058, Global test loss: 1.194, Global test accuracy: 39.27 

Average accuracy final 10 rounds: 77.34083333333334 

Average global accuracy final 10 rounds: 38.15500000000001 

1397.2452845573425
[1.5311241149902344, 2.7149198055267334, 3.907458543777466, 5.180981636047363, 6.440991401672363, 7.697595834732056, 8.947704553604126, 10.198980808258057, 11.45531177520752, 12.711126327514648, 13.977927923202515, 15.240078926086426, 16.510281801223755, 17.769288301467896, 18.989487409591675, 20.25865864753723, 21.47208070755005, 22.513323307037354, 23.717877626419067, 24.982242584228516, 26.248560667037964, 27.512789726257324, 28.777600526809692, 30.04141068458557, 31.307982683181763, 32.572142362594604, 33.833351612091064, 35.07922172546387, 36.33171725273132, 37.57434582710266, 38.824459075927734, 40.06038737297058, 41.309200286865234, 42.55403995513916, 43.79649758338928, 45.04779028892517, 46.29762363433838, 47.55050492286682, 48.8103814125061, 50.066574573516846, 51.32621097564697, 52.58130621910095, 53.83322596549988, 55.0913028717041, 56.34345579147339, 57.60529327392578, 58.8687949180603, 60.131744146347046, 61.3931884765625, 62.64925813674927, 63.9063024520874, 65.1769232749939, 66.42923045158386, 67.69654989242554, 68.95801067352295, 70.20744037628174, 71.46197986602783, 72.71772074699402, 73.97040700912476, 75.2258608341217, 76.33551836013794, 77.55861854553223, 78.77964758872986, 79.94111943244934, 81.11154413223267, 82.27597618103027, 83.3963577747345, 84.51260137557983, 85.63119983673096, 86.82585096359253, 88.01382040977478, 89.19570207595825, 90.36911082267761, 91.5511212348938, 92.7137336730957, 93.83149886131287, 94.9626476764679, 96.07855653762817, 97.24505019187927, 98.36819744110107, 99.48506689071655, 100.59780073165894, 101.78304958343506, 102.9244282245636, 104.08090877532959, 105.24927997589111, 106.40599846839905, 107.59991717338562, 108.8331389427185, 110.06610345840454, 111.29742550849915, 112.51003432273865, 113.6931700706482, 114.81871366500854, 115.95044088363647, 117.07989406585693, 118.2201201915741, 119.36229038238525, 120.48986530303955, 121.61299347877502, 123.86485815048218]
[43.375, 49.675, 54.958333333333336, 59.00833333333333, 60.18333333333333, 63.333333333333336, 63.09166666666667, 65.06666666666666, 68.58333333333333, 69.86666666666666, 70.56666666666666, 70.375, 71.10833333333333, 71.49166666666666, 71.71666666666667, 71.425, 71.31666666666666, 72.49166666666666, 74.09166666666667, 75.0, 75.125, 74.55833333333334, 74.975, 75.5, 75.71666666666667, 75.425, 75.60833333333333, 75.375, 75.675, 75.55833333333334, 75.30833333333334, 75.49166666666666, 75.425, 75.61666666666666, 75.525, 75.31666666666666, 76.125, 76.44166666666666, 76.625, 76.60833333333333, 76.5, 76.19166666666666, 75.85, 76.01666666666667, 76.28333333333333, 76.86666666666666, 76.91666666666667, 76.96666666666667, 76.59166666666667, 76.725, 76.39166666666667, 76.45833333333333, 76.50833333333334, 76.78333333333333, 76.9, 77.375, 77.76666666666667, 77.7, 77.59166666666667, 77.25, 76.91666666666667, 76.55, 76.825, 76.75, 76.50833333333334, 76.825, 76.86666666666666, 76.65833333333333, 76.73333333333333, 76.73333333333333, 76.83333333333333, 76.425, 76.43333333333334, 76.19166666666666, 76.84166666666667, 76.775, 76.91666666666667, 77.08333333333333, 76.95, 77.00833333333334, 77.36666666666666, 77.34166666666667, 77.475, 76.93333333333334, 76.80833333333334, 76.83333333333333, 76.4, 76.13333333333334, 76.43333333333334, 76.75833333333334, 76.89166666666667, 77.15, 77.50833333333334, 77.41666666666667, 77.46666666666667, 77.28333333333333, 77.275, 77.5, 77.45, 77.46666666666667, 77.68333333333334]
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307387 (local), 7939 (global); Percentage 2.58 (7939/307387 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307384
307387
# Params: 307387 (local), 307192 (global); Percentage 99.94 (307192/307387)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=3, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 18.84 

Round   0, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 19.20 

Round   1, Train loss: 2.298, Test loss: 2.297, Test accuracy: 24.27 

Round   1, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 27.24 

Round   2, Train loss: 2.293, Test loss: 2.293, Test accuracy: 25.69 

Round   2, Global train loss: 2.293, Global test loss: 2.289, Global test accuracy: 29.22 

Round   3, Train loss: 2.288, Test loss: 2.287, Test accuracy: 25.87 

Round   3, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 31.78 

Round   4, Train loss: 2.247, Test loss: 2.257, Test accuracy: 25.35 

Round   4, Global train loss: 2.247, Global test loss: 2.223, Global test accuracy: 33.36 

Round   5, Train loss: 2.211, Test loss: 2.224, Test accuracy: 30.66 

Round   5, Global train loss: 2.211, Global test loss: 2.181, Global test accuracy: 47.42 

Round   6, Train loss: 2.250, Test loss: 2.210, Test accuracy: 30.63 

Round   6, Global train loss: 2.250, Global test loss: 2.253, Global test accuracy: 44.44 

Round   7, Train loss: 2.116, Test loss: 2.164, Test accuracy: 35.62 

Round   7, Global train loss: 2.116, Global test loss: 2.086, Global test accuracy: 56.08 

Round   8, Train loss: 2.148, Test loss: 2.148, Test accuracy: 37.55 

Round   8, Global train loss: 2.148, Global test loss: 2.169, Global test accuracy: 49.05 

Round   9, Train loss: 2.056, Test loss: 2.109, Test accuracy: 40.50 

Round   9, Global train loss: 2.056, Global test loss: 2.053, Global test accuracy: 57.09 

Round  10, Train loss: 2.047, Test loss: 2.087, Test accuracy: 41.99 

Round  10, Global train loss: 2.047, Global test loss: 2.072, Global test accuracy: 60.06 

Round  11, Train loss: 1.831, Test loss: 2.051, Test accuracy: 44.79 

Round  11, Global train loss: 1.831, Global test loss: 1.838, Global test accuracy: 67.83 

Round  12, Train loss: 1.962, Test loss: 2.032, Test accuracy: 46.37 

Round  12, Global train loss: 1.962, Global test loss: 1.966, Global test accuracy: 60.12 

Round  13, Train loss: 1.845, Test loss: 1.997, Test accuracy: 49.89 

Round  13, Global train loss: 1.845, Global test loss: 1.832, Global test accuracy: 72.83 

Round  14, Train loss: 1.923, Test loss: 1.971, Test accuracy: 51.44 

Round  14, Global train loss: 1.923, Global test loss: 1.912, Global test accuracy: 59.96 

Round  15, Train loss: 1.965, Test loss: 1.951, Test accuracy: 53.47 

Round  15, Global train loss: 1.965, Global test loss: 1.994, Global test accuracy: 47.71 

Round  16, Train loss: 1.777, Test loss: 1.921, Test accuracy: 56.55 

Round  16, Global train loss: 1.777, Global test loss: 1.811, Global test accuracy: 67.17 

Round  17, Train loss: 1.877, Test loss: 1.899, Test accuracy: 59.57 

Round  17, Global train loss: 1.877, Global test loss: 1.887, Global test accuracy: 67.54 

Round  18, Train loss: 1.717, Test loss: 1.884, Test accuracy: 60.67 

Round  18, Global train loss: 1.717, Global test loss: 1.765, Global test accuracy: 74.46 

Round  19, Train loss: 1.693, Test loss: 1.867, Test accuracy: 62.62 

Round  19, Global train loss: 1.693, Global test loss: 1.732, Global test accuracy: 77.03 

Round  20, Train loss: 1.811, Test loss: 1.849, Test accuracy: 65.17 

Round  20, Global train loss: 1.811, Global test loss: 1.832, Global test accuracy: 67.28 

Round  21, Train loss: 1.710, Test loss: 1.825, Test accuracy: 67.43 

Round  21, Global train loss: 1.710, Global test loss: 1.756, Global test accuracy: 74.96 

Round  22, Train loss: 1.643, Test loss: 1.816, Test accuracy: 68.03 

Round  22, Global train loss: 1.643, Global test loss: 1.707, Global test accuracy: 77.30 

Round  23, Train loss: 1.684, Test loss: 1.806, Test accuracy: 69.09 

Round  23, Global train loss: 1.684, Global test loss: 1.747, Global test accuracy: 73.88 

Round  24, Train loss: 1.721, Test loss: 1.785, Test accuracy: 70.59 

Round  24, Global train loss: 1.721, Global test loss: 1.778, Global test accuracy: 70.75 

Round  25, Train loss: 1.671, Test loss: 1.775, Test accuracy: 71.26 

Round  25, Global train loss: 1.671, Global test loss: 1.803, Global test accuracy: 65.95 

Round  26, Train loss: 1.666, Test loss: 1.763, Test accuracy: 72.11 

Round  26, Global train loss: 1.666, Global test loss: 1.761, Global test accuracy: 72.47 

Round  27, Train loss: 1.646, Test loss: 1.755, Test accuracy: 72.79 

Round  27, Global train loss: 1.646, Global test loss: 1.755, Global test accuracy: 71.86 

Round  28, Train loss: 1.614, Test loss: 1.750, Test accuracy: 73.24 

Round  28, Global train loss: 1.614, Global test loss: 1.759, Global test accuracy: 70.94 

Round  29, Train loss: 1.694, Test loss: 1.740, Test accuracy: 74.09 

Round  29, Global train loss: 1.694, Global test loss: 1.784, Global test accuracy: 69.16 

Round  30, Train loss: 1.604, Test loss: 1.738, Test accuracy: 74.17 

Round  30, Global train loss: 1.604, Global test loss: 1.751, Global test accuracy: 71.36 

Round  31, Train loss: 1.657, Test loss: 1.726, Test accuracy: 75.30 

Round  31, Global train loss: 1.657, Global test loss: 1.789, Global test accuracy: 67.53 

Round  32, Train loss: 1.599, Test loss: 1.724, Test accuracy: 75.50 

Round  32, Global train loss: 1.599, Global test loss: 1.711, Global test accuracy: 77.74 

Round  33, Train loss: 1.563, Test loss: 1.715, Test accuracy: 76.17 

Round  33, Global train loss: 1.563, Global test loss: 1.661, Global test accuracy: 82.44 

Round  34, Train loss: 1.571, Test loss: 1.712, Test accuracy: 76.44 

Round  34, Global train loss: 1.571, Global test loss: 1.705, Global test accuracy: 76.52 

Round  35, Train loss: 1.597, Test loss: 1.711, Test accuracy: 76.51 

Round  35, Global train loss: 1.597, Global test loss: 1.728, Global test accuracy: 74.22 

Round  36, Train loss: 1.584, Test loss: 1.710, Test accuracy: 76.52 

Round  36, Global train loss: 1.584, Global test loss: 1.708, Global test accuracy: 76.99 

Round  37, Train loss: 1.557, Test loss: 1.709, Test accuracy: 76.69 

Round  37, Global train loss: 1.557, Global test loss: 1.701, Global test accuracy: 77.74 

Round  38, Train loss: 1.572, Test loss: 1.707, Test accuracy: 76.74 

Round  38, Global train loss: 1.572, Global test loss: 1.698, Global test accuracy: 78.14 

Round  39, Train loss: 1.571, Test loss: 1.705, Test accuracy: 76.87 

Round  39, Global train loss: 1.571, Global test loss: 1.719, Global test accuracy: 75.44 

Round  40, Train loss: 1.576, Test loss: 1.703, Test accuracy: 77.17 

Round  40, Global train loss: 1.576, Global test loss: 1.733, Global test accuracy: 73.55 

Round  41, Train loss: 1.608, Test loss: 1.699, Test accuracy: 77.45 

Round  41, Global train loss: 1.608, Global test loss: 1.704, Global test accuracy: 77.07 

Round  42, Train loss: 1.588, Test loss: 1.695, Test accuracy: 77.87 

Round  42, Global train loss: 1.588, Global test loss: 1.706, Global test accuracy: 77.67 

Round  43, Train loss: 1.570, Test loss: 1.693, Test accuracy: 77.89 

Round  43, Global train loss: 1.570, Global test loss: 1.741, Global test accuracy: 72.32 

Round  44, Train loss: 1.562, Test loss: 1.692, Test accuracy: 78.00 

Round  44, Global train loss: 1.562, Global test loss: 1.701, Global test accuracy: 76.69 

Round  45, Train loss: 1.550, Test loss: 1.692, Test accuracy: 78.00 

Round  45, Global train loss: 1.550, Global test loss: 1.713, Global test accuracy: 75.67 

Round  46, Train loss: 1.575, Test loss: 1.691, Test accuracy: 78.00 

Round  46, Global train loss: 1.575, Global test loss: 1.710, Global test accuracy: 76.29 

Round  47, Train loss: 1.561, Test loss: 1.690, Test accuracy: 78.06 

Round  47, Global train loss: 1.561, Global test loss: 1.670, Global test accuracy: 80.19 

Round  48, Train loss: 1.569, Test loss: 1.690, Test accuracy: 78.05 

Round  48, Global train loss: 1.569, Global test loss: 1.699, Global test accuracy: 77.76 

Round  49, Train loss: 1.558, Test loss: 1.689, Test accuracy: 78.07 

Round  49, Global train loss: 1.558, Global test loss: 1.696, Global test accuracy: 77.41 

Round  50, Train loss: 1.554, Test loss: 1.689, Test accuracy: 78.09 

Round  50, Global train loss: 1.554, Global test loss: 1.689, Global test accuracy: 78.01 

Round  51, Train loss: 1.547, Test loss: 1.689, Test accuracy: 78.07 

Round  51, Global train loss: 1.547, Global test loss: 1.668, Global test accuracy: 79.92 

Round  52, Train loss: 1.572, Test loss: 1.688, Test accuracy: 78.14 

Round  52, Global train loss: 1.572, Global test loss: 1.688, Global test accuracy: 78.50 

Round  53, Train loss: 1.536, Test loss: 1.686, Test accuracy: 78.50 

Round  53, Global train loss: 1.536, Global test loss: 1.675, Global test accuracy: 79.22 

Round  54, Train loss: 1.554, Test loss: 1.685, Test accuracy: 78.48 

Round  54, Global train loss: 1.554, Global test loss: 1.700, Global test accuracy: 77.19 

Round  55, Train loss: 1.570, Test loss: 1.685, Test accuracy: 78.48 

Round  55, Global train loss: 1.570, Global test loss: 1.701, Global test accuracy: 77.48 

Round  56, Train loss: 1.512, Test loss: 1.684, Test accuracy: 78.49 

Round  56, Global train loss: 1.512, Global test loss: 1.654, Global test accuracy: 81.03 

Round  57, Train loss: 1.555, Test loss: 1.684, Test accuracy: 78.55 

Round  57, Global train loss: 1.555, Global test loss: 1.726, Global test accuracy: 73.61 

Round  58, Train loss: 1.553, Test loss: 1.684, Test accuracy: 78.53 

Round  58, Global train loss: 1.553, Global test loss: 1.696, Global test accuracy: 76.91 

Round  59, Train loss: 1.532, Test loss: 1.683, Test accuracy: 78.52 

Round  59, Global train loss: 1.532, Global test loss: 1.689, Global test accuracy: 77.72 

Round  60, Train loss: 1.563, Test loss: 1.683, Test accuracy: 78.50 

Round  60, Global train loss: 1.563, Global test loss: 1.699, Global test accuracy: 77.73 

Round  61, Train loss: 1.555, Test loss: 1.683, Test accuracy: 78.53 

Round  61, Global train loss: 1.555, Global test loss: 1.687, Global test accuracy: 78.31 

Round  62, Train loss: 1.536, Test loss: 1.683, Test accuracy: 78.48 

Round  62, Global train loss: 1.536, Global test loss: 1.704, Global test accuracy: 76.06 

Round  63, Train loss: 1.522, Test loss: 1.683, Test accuracy: 78.52 

Round  63, Global train loss: 1.522, Global test loss: 1.678, Global test accuracy: 78.47 

Round  64, Train loss: 1.564, Test loss: 1.683, Test accuracy: 78.54 

Round  64, Global train loss: 1.564, Global test loss: 1.693, Global test accuracy: 77.69 

Round  65, Train loss: 1.526, Test loss: 1.682, Test accuracy: 78.55 

Round  65, Global train loss: 1.526, Global test loss: 1.667, Global test accuracy: 80.09 

Round  66, Train loss: 1.547, Test loss: 1.682, Test accuracy: 78.57 

Round  66, Global train loss: 1.547, Global test loss: 1.693, Global test accuracy: 77.25 

Round  67, Train loss: 1.511, Test loss: 1.682, Test accuracy: 78.59 

Round  67, Global train loss: 1.511, Global test loss: 1.642, Global test accuracy: 82.72 

Round  68, Train loss: 1.530, Test loss: 1.682, Test accuracy: 78.58 

Round  68, Global train loss: 1.530, Global test loss: 1.688, Global test accuracy: 77.83 

Round  69, Train loss: 1.581, Test loss: 1.682, Test accuracy: 78.57 

Round  69, Global train loss: 1.581, Global test loss: 1.702, Global test accuracy: 76.96 

Round  70, Train loss: 1.542, Test loss: 1.682, Test accuracy: 78.57 

Round  70, Global train loss: 1.542, Global test loss: 1.665, Global test accuracy: 80.23 

Round  71, Train loss: 1.527, Test loss: 1.682, Test accuracy: 78.58 

Round  71, Global train loss: 1.527, Global test loss: 1.667, Global test accuracy: 80.01 

Round  72, Train loss: 1.545, Test loss: 1.682, Test accuracy: 78.59 

Round  72, Global train loss: 1.545, Global test loss: 1.691, Global test accuracy: 77.80 

Round  73, Train loss: 1.594, Test loss: 1.681, Test accuracy: 78.60 

Round  73, Global train loss: 1.594, Global test loss: 1.699, Global test accuracy: 77.22 

Round  74, Train loss: 1.549, Test loss: 1.681, Test accuracy: 78.63 

Round  74, Global train loss: 1.549, Global test loss: 1.684, Global test accuracy: 78.34 

Round  75, Train loss: 1.539, Test loss: 1.681, Test accuracy: 78.61 

Round  75, Global train loss: 1.539, Global test loss: 1.693, Global test accuracy: 77.06 

Round  76, Train loss: 1.564, Test loss: 1.681, Test accuracy: 78.66 

Round  76, Global train loss: 1.564, Global test loss: 1.703, Global test accuracy: 76.83 

Round  77, Train loss: 1.537, Test loss: 1.681, Test accuracy: 78.66 

Round  77, Global train loss: 1.537, Global test loss: 1.702, Global test accuracy: 76.14 

Round  78, Train loss: 1.533, Test loss: 1.681, Test accuracy: 78.69 

Round  78, Global train loss: 1.533, Global test loss: 1.689, Global test accuracy: 77.86 

Round  79, Train loss: 1.550, Test loss: 1.681, Test accuracy: 78.69 

Round  79, Global train loss: 1.550, Global test loss: 1.704, Global test accuracy: 76.10 

Round  80, Train loss: 1.553, Test loss: 1.681, Test accuracy: 78.67 

Round  80, Global train loss: 1.553, Global test loss: 1.664, Global test accuracy: 79.98 

Round  81, Train loss: 1.535, Test loss: 1.681, Test accuracy: 78.66 

Round  81, Global train loss: 1.535, Global test loss: 1.685, Global test accuracy: 78.30 

Round  82, Train loss: 1.545, Test loss: 1.681, Test accuracy: 78.64 

Round  82, Global train loss: 1.545, Global test loss: 1.703, Global test accuracy: 76.39 

Round  83, Train loss: 1.575, Test loss: 1.681, Test accuracy: 78.64 

Round  83, Global train loss: 1.575, Global test loss: 1.694, Global test accuracy: 77.75 

Round  84, Train loss: 1.547, Test loss: 1.680, Test accuracy: 78.65 

Round  84, Global train loss: 1.547, Global test loss: 1.694, Global test accuracy: 77.23 

Round  85, Train loss: 1.551, Test loss: 1.680, Test accuracy: 78.64 

Round  85, Global train loss: 1.551, Global test loss: 1.674, Global test accuracy: 79.27 

Round  86, Train loss: 1.570, Test loss: 1.680, Test accuracy: 78.64 

Round  86, Global train loss: 1.570, Global test loss: 1.688, Global test accuracy: 78.53 

Round  87, Train loss: 1.548, Test loss: 1.680, Test accuracy: 78.64 

Round  87, Global train loss: 1.548, Global test loss: 1.699, Global test accuracy: 76.67 

Round  88, Train loss: 1.552, Test loss: 1.680, Test accuracy: 78.64 

Round  88, Global train loss: 1.552, Global test loss: 1.691, Global test accuracy: 77.65 

Round  89, Train loss: 1.572, Test loss: 1.680, Test accuracy: 78.64 

Round  89, Global train loss: 1.572, Global test loss: 1.720, Global test accuracy: 75.19 

Round  90, Train loss: 1.533, Test loss: 1.680, Test accuracy: 78.64 

Round  90, Global train loss: 1.533, Global test loss: 1.692, Global test accuracy: 77.40 

Round  91, Train loss: 1.562, Test loss: 1.680, Test accuracy: 78.66 

Round  91, Global train loss: 1.562, Global test loss: 1.726, Global test accuracy: 73.58 

Round  92, Train loss: 1.555, Test loss: 1.679, Test accuracy: 78.79 

Round  92, Global train loss: 1.555, Global test loss: 1.706, Global test accuracy: 75.95 

Round  93, Train loss: 1.537, Test loss: 1.679, Test accuracy: 78.78 

Round  93, Global train loss: 1.537, Global test loss: 1.686, Global test accuracy: 78.03 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.553, Test loss: 1.679, Test accuracy: 78.78 

Round  94, Global train loss: 1.553, Global test loss: 1.685, Global test accuracy: 78.28 

Round  95, Train loss: 1.580, Test loss: 1.679, Test accuracy: 78.77 

Round  95, Global train loss: 1.580, Global test loss: 1.690, Global test accuracy: 77.68 

Round  96, Train loss: 1.545, Test loss: 1.678, Test accuracy: 78.79 

Round  96, Global train loss: 1.545, Global test loss: 1.710, Global test accuracy: 75.25 

Round  97, Train loss: 1.534, Test loss: 1.678, Test accuracy: 78.80 

Round  97, Global train loss: 1.534, Global test loss: 1.669, Global test accuracy: 79.83 

Round  98, Train loss: 1.551, Test loss: 1.678, Test accuracy: 78.80 

Round  98, Global train loss: 1.551, Global test loss: 1.665, Global test accuracy: 80.22 

Round  99, Train loss: 1.559, Test loss: 1.678, Test accuracy: 78.78 

Round  99, Global train loss: 1.559, Global test loss: 1.714, Global test accuracy: 74.92 

Final Round, Train loss: 1.545, Test loss: 1.674, Test accuracy: 79.31 

Final Round, Global train loss: 1.545, Global test loss: 1.714, Global test accuracy: 74.92 

Average accuracy final 10 rounds: 78.75999999999999 

Average global accuracy final 10 rounds: 77.11425 

2280.0960063934326
[0.929955244064331, 1.7377464771270752, 2.539172649383545, 3.3682923316955566, 4.226583480834961, 5.040011405944824, 5.85919189453125, 6.605528116226196, 7.377950191497803, 8.156224250793457, 8.987598419189453, 9.796594619750977, 10.559863328933716, 11.332129716873169, 12.105762004852295, 12.865167379379272, 13.627336025238037, 14.386209726333618, 15.200099468231201, 15.957475423812866, 16.738359928131104, 17.409616708755493, 18.097153425216675, 18.781073331832886, 19.567636728286743, 20.351383209228516, 21.131943225860596, 21.918145895004272, 22.721526861190796, 23.5036518573761, 24.285280227661133, 25.059208869934082, 25.85409951210022, 26.647738456726074, 27.338744640350342, 28.066198110580444, 28.805086374282837, 29.476454257965088, 30.16767406463623, 30.850066661834717, 31.655985832214355, 32.455461263656616, 33.24665331840515, 34.0383038520813, 34.82888603210449, 35.61500096321106, 36.41878890991211, 37.233487129211426, 38.04149603843689, 38.839956760406494, 39.62843298912048, 40.42378354072571, 41.19161939620972, 41.98878049850464, 42.69137930870056, 43.374552726745605, 44.05582904815674, 44.74378252029419, 45.4330735206604, 46.13168239593506, 46.81588101387024, 47.49917769432068, 48.1802339553833, 48.86691236495972, 49.559463024139404, 50.25215983390808, 50.95255708694458, 51.76563382148743, 52.56116032600403, 53.35191535949707, 54.18875002861023, 54.88703417778015, 55.6334810256958, 56.36051607131958, 57.05624032020569, 57.73460149765015, 58.42173409461975, 59.11798143386841, 59.814847469329834, 60.49644613265991, 61.170167446136475, 61.851723432540894, 62.5304901599884, 63.22657585144043, 63.929163694381714, 64.62350964546204, 65.31669902801514, 66.00011205673218, 66.68340396881104, 67.36245727539062, 68.05837655067444, 68.84654307365417, 69.54122805595398, 70.22663760185242, 70.90816617012024, 71.59293103218079, 72.28795981407166, 72.99233150482178, 73.68152666091919, 74.36688160896301, 75.75271725654602]
[18.845, 24.275, 25.6875, 25.87, 25.35, 30.66, 30.6275, 35.6175, 37.5475, 40.5025, 41.9875, 44.7875, 46.37, 49.8875, 51.435, 53.4675, 56.55, 59.5675, 60.67, 62.62, 65.175, 67.4325, 68.03, 69.095, 70.5875, 71.26, 72.115, 72.79, 73.24, 74.0875, 74.17, 75.2975, 75.5025, 76.1725, 76.4425, 76.5075, 76.5225, 76.69, 76.74, 76.8675, 77.17, 77.45, 77.8675, 77.895, 77.995, 78.0, 78.0025, 78.06, 78.05, 78.0675, 78.09, 78.0675, 78.1375, 78.5, 78.4775, 78.48, 78.49, 78.5475, 78.535, 78.5175, 78.4975, 78.525, 78.485, 78.52, 78.5425, 78.5525, 78.5675, 78.59, 78.585, 78.57, 78.5675, 78.58, 78.5925, 78.6, 78.6275, 78.61, 78.6625, 78.66, 78.6875, 78.6875, 78.675, 78.66, 78.635, 78.645, 78.6525, 78.6425, 78.6425, 78.645, 78.64, 78.64, 78.645, 78.6575, 78.7875, 78.785, 78.78, 78.7725, 78.79, 78.7975, 78.8, 78.785, 79.3125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 17.75 

Round   0, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 17.80 

Round   1, Train loss: 2.296, Test loss: 2.294, Test accuracy: 25.09 

Round   1, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 26.34 

Round   2, Train loss: 2.286, Test loss: 2.284, Test accuracy: 38.50 

Round   2, Global train loss: 2.286, Global test loss: 2.278, Global test accuracy: 47.03 

Round   3, Train loss: 2.245, Test loss: 2.235, Test accuracy: 37.95 

Round   3, Global train loss: 2.245, Global test loss: 2.203, Global test accuracy: 39.88 

Round   4, Train loss: 2.093, Test loss: 2.123, Test accuracy: 45.52 

Round   4, Global train loss: 2.093, Global test loss: 2.004, Global test accuracy: 56.97 

Round   5, Train loss: 1.871, Test loss: 1.996, Test accuracy: 55.99 

Round   5, Global train loss: 1.871, Global test loss: 1.818, Global test accuracy: 71.25 

Round   6, Train loss: 1.751, Test loss: 1.897, Test accuracy: 63.80 

Round   6, Global train loss: 1.751, Global test loss: 1.726, Global test accuracy: 78.17 

Round   7, Train loss: 1.691, Test loss: 1.840, Test accuracy: 67.87 

Round   7, Global train loss: 1.691, Global test loss: 1.686, Global test accuracy: 80.50 

Round   8, Train loss: 1.658, Test loss: 1.826, Test accuracy: 68.81 

Round   8, Global train loss: 1.658, Global test loss: 1.665, Global test accuracy: 81.76 

Round   9, Train loss: 1.649, Test loss: 1.754, Test accuracy: 74.33 

Round   9, Global train loss: 1.649, Global test loss: 1.654, Global test accuracy: 82.25 

Round  10, Train loss: 1.632, Test loss: 1.746, Test accuracy: 74.89 

Round  10, Global train loss: 1.632, Global test loss: 1.647, Global test accuracy: 82.57 

Round  11, Train loss: 1.621, Test loss: 1.707, Test accuracy: 78.33 

Round  11, Global train loss: 1.621, Global test loss: 1.643, Global test accuracy: 82.62 

Round  12, Train loss: 1.609, Test loss: 1.700, Test accuracy: 78.78 

Round  12, Global train loss: 1.609, Global test loss: 1.640, Global test accuracy: 82.70 

Round  13, Train loss: 1.614, Test loss: 1.664, Test accuracy: 81.26 

Round  13, Global train loss: 1.614, Global test loss: 1.638, Global test accuracy: 82.98 

Round  14, Train loss: 1.604, Test loss: 1.653, Test accuracy: 81.98 

Round  14, Global train loss: 1.604, Global test loss: 1.635, Global test accuracy: 83.15 

Round  15, Train loss: 1.611, Test loss: 1.650, Test accuracy: 82.12 

Round  15, Global train loss: 1.611, Global test loss: 1.632, Global test accuracy: 83.47 

Round  16, Train loss: 1.614, Test loss: 1.644, Test accuracy: 82.55 

Round  16, Global train loss: 1.614, Global test loss: 1.632, Global test accuracy: 83.47 

Round  17, Train loss: 1.609, Test loss: 1.642, Test accuracy: 82.66 

Round  17, Global train loss: 1.609, Global test loss: 1.629, Global test accuracy: 83.55 

Round  18, Train loss: 1.602, Test loss: 1.640, Test accuracy: 82.77 

Round  18, Global train loss: 1.602, Global test loss: 1.628, Global test accuracy: 83.65 

Round  19, Train loss: 1.590, Test loss: 1.639, Test accuracy: 82.80 

Round  19, Global train loss: 1.590, Global test loss: 1.627, Global test accuracy: 83.61 

Round  20, Train loss: 1.593, Test loss: 1.636, Test accuracy: 82.93 

Round  20, Global train loss: 1.593, Global test loss: 1.625, Global test accuracy: 84.03 

Round  21, Train loss: 1.591, Test loss: 1.636, Test accuracy: 82.97 

Round  21, Global train loss: 1.591, Global test loss: 1.625, Global test accuracy: 83.92 

Round  22, Train loss: 1.587, Test loss: 1.635, Test accuracy: 83.10 

Round  22, Global train loss: 1.587, Global test loss: 1.624, Global test accuracy: 84.09 

Round  23, Train loss: 1.596, Test loss: 1.634, Test accuracy: 83.22 

Round  23, Global train loss: 1.596, Global test loss: 1.624, Global test accuracy: 84.03 

Round  24, Train loss: 1.592, Test loss: 1.630, Test accuracy: 83.49 

Round  24, Global train loss: 1.592, Global test loss: 1.621, Global test accuracy: 84.06 

Round  25, Train loss: 1.560, Test loss: 1.621, Test accuracy: 84.60 

Round  25, Global train loss: 1.560, Global test loss: 1.586, Global test accuracy: 88.45 

Round  26, Train loss: 1.534, Test loss: 1.612, Test accuracy: 85.62 

Round  26, Global train loss: 1.534, Global test loss: 1.574, Global test accuracy: 89.58 

Round  27, Train loss: 1.533, Test loss: 1.599, Test accuracy: 86.89 

Round  27, Global train loss: 1.533, Global test loss: 1.569, Global test accuracy: 90.04 

Round  28, Train loss: 1.519, Test loss: 1.592, Test accuracy: 87.61 

Round  28, Global train loss: 1.519, Global test loss: 1.566, Global test accuracy: 90.18 

Round  29, Train loss: 1.518, Test loss: 1.587, Test accuracy: 88.12 

Round  29, Global train loss: 1.518, Global test loss: 1.564, Global test accuracy: 90.34 

Round  30, Train loss: 1.528, Test loss: 1.581, Test accuracy: 88.75 

Round  30, Global train loss: 1.528, Global test loss: 1.562, Global test accuracy: 90.45 

Round  31, Train loss: 1.514, Test loss: 1.573, Test accuracy: 89.48 

Round  31, Global train loss: 1.514, Global test loss: 1.559, Global test accuracy: 90.79 

Round  32, Train loss: 1.509, Test loss: 1.572, Test accuracy: 89.65 

Round  32, Global train loss: 1.509, Global test loss: 1.558, Global test accuracy: 90.68 

Round  33, Train loss: 1.511, Test loss: 1.569, Test accuracy: 89.85 

Round  33, Global train loss: 1.511, Global test loss: 1.556, Global test accuracy: 90.96 

Round  34, Train loss: 1.510, Test loss: 1.568, Test accuracy: 89.95 

Round  34, Global train loss: 1.510, Global test loss: 1.556, Global test accuracy: 91.16 

Round  35, Train loss: 1.509, Test loss: 1.568, Test accuracy: 89.97 

Round  35, Global train loss: 1.509, Global test loss: 1.555, Global test accuracy: 91.11 

Round  36, Train loss: 1.505, Test loss: 1.566, Test accuracy: 90.06 

Round  36, Global train loss: 1.505, Global test loss: 1.554, Global test accuracy: 91.04 

Round  37, Train loss: 1.507, Test loss: 1.563, Test accuracy: 90.35 

Round  37, Global train loss: 1.507, Global test loss: 1.553, Global test accuracy: 91.25 

Round  38, Train loss: 1.507, Test loss: 1.562, Test accuracy: 90.45 

Round  38, Global train loss: 1.507, Global test loss: 1.552, Global test accuracy: 91.33 

Round  39, Train loss: 1.504, Test loss: 1.561, Test accuracy: 90.58 

Round  39, Global train loss: 1.504, Global test loss: 1.552, Global test accuracy: 91.22 

Round  40, Train loss: 1.500, Test loss: 1.560, Test accuracy: 90.55 

Round  40, Global train loss: 1.500, Global test loss: 1.552, Global test accuracy: 91.33 

Round  41, Train loss: 1.500, Test loss: 1.560, Test accuracy: 90.64 

Round  41, Global train loss: 1.500, Global test loss: 1.550, Global test accuracy: 91.33 

Round  42, Train loss: 1.507, Test loss: 1.557, Test accuracy: 90.90 

Round  42, Global train loss: 1.507, Global test loss: 1.550, Global test accuracy: 91.47 

Round  43, Train loss: 1.499, Test loss: 1.556, Test accuracy: 90.94 

Round  43, Global train loss: 1.499, Global test loss: 1.549, Global test accuracy: 91.53 

Round  44, Train loss: 1.502, Test loss: 1.556, Test accuracy: 91.00 

Round  44, Global train loss: 1.502, Global test loss: 1.549, Global test accuracy: 91.54 

Round  45, Train loss: 1.499, Test loss: 1.555, Test accuracy: 91.05 

Round  45, Global train loss: 1.499, Global test loss: 1.548, Global test accuracy: 91.61 

Round  46, Train loss: 1.501, Test loss: 1.554, Test accuracy: 91.10 

Round  46, Global train loss: 1.501, Global test loss: 1.548, Global test accuracy: 91.61 

Round  47, Train loss: 1.500, Test loss: 1.554, Test accuracy: 91.14 

Round  47, Global train loss: 1.500, Global test loss: 1.546, Global test accuracy: 91.70 

Round  48, Train loss: 1.498, Test loss: 1.553, Test accuracy: 91.16 

Round  48, Global train loss: 1.498, Global test loss: 1.546, Global test accuracy: 91.71 

Round  49, Train loss: 1.495, Test loss: 1.552, Test accuracy: 91.29 

Round  49, Global train loss: 1.495, Global test loss: 1.545, Global test accuracy: 91.84 

Round  50, Train loss: 1.495, Test loss: 1.551, Test accuracy: 91.32 

Round  50, Global train loss: 1.495, Global test loss: 1.545, Global test accuracy: 91.83 

Round  51, Train loss: 1.495, Test loss: 1.550, Test accuracy: 91.41 

Round  51, Global train loss: 1.495, Global test loss: 1.544, Global test accuracy: 91.90 

Round  52, Train loss: 1.494, Test loss: 1.550, Test accuracy: 91.48 

Round  52, Global train loss: 1.494, Global test loss: 1.545, Global test accuracy: 91.77 

Round  53, Train loss: 1.498, Test loss: 1.548, Test accuracy: 91.58 

Round  53, Global train loss: 1.498, Global test loss: 1.544, Global test accuracy: 91.93 

Round  54, Train loss: 1.493, Test loss: 1.548, Test accuracy: 91.57 

Round  54, Global train loss: 1.493, Global test loss: 1.544, Global test accuracy: 91.88 

Round  55, Train loss: 1.493, Test loss: 1.548, Test accuracy: 91.65 

Round  55, Global train loss: 1.493, Global test loss: 1.543, Global test accuracy: 92.02 

Round  56, Train loss: 1.491, Test loss: 1.547, Test accuracy: 91.72 

Round  56, Global train loss: 1.491, Global test loss: 1.543, Global test accuracy: 92.00 

Round  57, Train loss: 1.494, Test loss: 1.547, Test accuracy: 91.69 

Round  57, Global train loss: 1.494, Global test loss: 1.544, Global test accuracy: 91.83 

Round  58, Train loss: 1.493, Test loss: 1.548, Test accuracy: 91.60 

Round  58, Global train loss: 1.493, Global test loss: 1.542, Global test accuracy: 92.00 

Round  59, Train loss: 1.488, Test loss: 1.547, Test accuracy: 91.62 

Round  59, Global train loss: 1.488, Global test loss: 1.542, Global test accuracy: 92.11 

Round  60, Train loss: 1.491, Test loss: 1.546, Test accuracy: 91.77 

Round  60, Global train loss: 1.491, Global test loss: 1.542, Global test accuracy: 92.12 

Round  61, Train loss: 1.491, Test loss: 1.545, Test accuracy: 91.81 

Round  61, Global train loss: 1.491, Global test loss: 1.541, Global test accuracy: 92.10 

Round  62, Train loss: 1.490, Test loss: 1.545, Test accuracy: 91.88 

Round  62, Global train loss: 1.490, Global test loss: 1.541, Global test accuracy: 92.12 

Round  63, Train loss: 1.490, Test loss: 1.545, Test accuracy: 91.88 

Round  63, Global train loss: 1.490, Global test loss: 1.541, Global test accuracy: 92.10 

Round  64, Train loss: 1.493, Test loss: 1.545, Test accuracy: 91.86 

Round  64, Global train loss: 1.493, Global test loss: 1.541, Global test accuracy: 92.25 

Round  65, Train loss: 1.491, Test loss: 1.544, Test accuracy: 91.92 

Round  65, Global train loss: 1.491, Global test loss: 1.540, Global test accuracy: 92.27 

Round  66, Train loss: 1.491, Test loss: 1.544, Test accuracy: 91.93 

Round  66, Global train loss: 1.491, Global test loss: 1.538, Global test accuracy: 92.35 

Round  67, Train loss: 1.492, Test loss: 1.543, Test accuracy: 92.02 

Round  67, Global train loss: 1.492, Global test loss: 1.539, Global test accuracy: 92.43 

Round  68, Train loss: 1.485, Test loss: 1.543, Test accuracy: 92.08 

Round  68, Global train loss: 1.485, Global test loss: 1.540, Global test accuracy: 92.25 

Round  69, Train loss: 1.490, Test loss: 1.543, Test accuracy: 92.08 

Round  69, Global train loss: 1.490, Global test loss: 1.539, Global test accuracy: 92.24 

Round  70, Train loss: 1.485, Test loss: 1.542, Test accuracy: 92.10 

Round  70, Global train loss: 1.485, Global test loss: 1.538, Global test accuracy: 92.35 

Round  71, Train loss: 1.483, Test loss: 1.542, Test accuracy: 92.08 

Round  71, Global train loss: 1.483, Global test loss: 1.539, Global test accuracy: 92.30 

Round  72, Train loss: 1.489, Test loss: 1.542, Test accuracy: 92.10 

Round  72, Global train loss: 1.489, Global test loss: 1.537, Global test accuracy: 92.49 

Round  73, Train loss: 1.486, Test loss: 1.542, Test accuracy: 92.13 

Round  73, Global train loss: 1.486, Global test loss: 1.537, Global test accuracy: 92.62 

Round  74, Train loss: 1.486, Test loss: 1.542, Test accuracy: 92.19 

Round  74, Global train loss: 1.486, Global test loss: 1.537, Global test accuracy: 92.38 

Round  75, Train loss: 1.490, Test loss: 1.542, Test accuracy: 92.17 

Round  75, Global train loss: 1.490, Global test loss: 1.537, Global test accuracy: 92.59 

Round  76, Train loss: 1.483, Test loss: 1.541, Test accuracy: 92.21 

Round  76, Global train loss: 1.483, Global test loss: 1.537, Global test accuracy: 92.61 

Round  77, Train loss: 1.488, Test loss: 1.541, Test accuracy: 92.22 

Round  77, Global train loss: 1.488, Global test loss: 1.537, Global test accuracy: 92.50 

Round  78, Train loss: 1.487, Test loss: 1.541, Test accuracy: 92.22 

Round  78, Global train loss: 1.487, Global test loss: 1.537, Global test accuracy: 92.50 

Round  79, Train loss: 1.486, Test loss: 1.541, Test accuracy: 92.18 

Round  79, Global train loss: 1.486, Global test loss: 1.536, Global test accuracy: 92.59 

Round  80, Train loss: 1.484, Test loss: 1.540, Test accuracy: 92.19 

Round  80, Global train loss: 1.484, Global test loss: 1.536, Global test accuracy: 92.58 

Round  81, Train loss: 1.491, Test loss: 1.540, Test accuracy: 92.25 

Round  81, Global train loss: 1.491, Global test loss: 1.536, Global test accuracy: 92.62 

Round  82, Train loss: 1.486, Test loss: 1.540, Test accuracy: 92.30 

Round  82, Global train loss: 1.486, Global test loss: 1.536, Global test accuracy: 92.67 

Round  83, Train loss: 1.488, Test loss: 1.539, Test accuracy: 92.33 

Round  83, Global train loss: 1.488, Global test loss: 1.536, Global test accuracy: 92.64 

Round  84, Train loss: 1.483, Test loss: 1.539, Test accuracy: 92.35 

Round  84, Global train loss: 1.483, Global test loss: 1.535, Global test accuracy: 92.73 

Round  85, Train loss: 1.489, Test loss: 1.539, Test accuracy: 92.34 

Round  85, Global train loss: 1.489, Global test loss: 1.535, Global test accuracy: 92.75 

Round  86, Train loss: 1.485, Test loss: 1.539, Test accuracy: 92.36 

Round  86, Global train loss: 1.485, Global test loss: 1.535, Global test accuracy: 92.75 

Round  87, Train loss: 1.484, Test loss: 1.539, Test accuracy: 92.41 

Round  87, Global train loss: 1.484, Global test loss: 1.535, Global test accuracy: 92.83 

Round  88, Train loss: 1.483, Test loss: 1.539, Test accuracy: 92.42 

Round  88, Global train loss: 1.483, Global test loss: 1.534, Global test accuracy: 92.92 

Round  89, Train loss: 1.486, Test loss: 1.539, Test accuracy: 92.40 

Round  89, Global train loss: 1.486, Global test loss: 1.535, Global test accuracy: 92.83 

Round  90, Train loss: 1.491, Test loss: 1.538, Test accuracy: 92.43 

Round  90, Global train loss: 1.491, Global test loss: 1.534, Global test accuracy: 92.74 

Round  91, Train loss: 1.488, Test loss: 1.538, Test accuracy: 92.41 

Round  91, Global train loss: 1.488, Global test loss: 1.534, Global test accuracy: 92.84 

Round  92, Train loss: 1.487, Test loss: 1.538, Test accuracy: 92.40 

Round  92, Global train loss: 1.487, Global test loss: 1.534, Global test accuracy: 92.93 

Round  93, Train loss: 1.482, Test loss: 1.538, Test accuracy: 92.42 

Round  93, Global train loss: 1.482, Global test loss: 1.533, Global test accuracy: 92.92 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.486, Test loss: 1.537, Test accuracy: 92.52 

Round  94, Global train loss: 1.486, Global test loss: 1.533, Global test accuracy: 92.95 

Round  95, Train loss: 1.484, Test loss: 1.537, Test accuracy: 92.55 

Round  95, Global train loss: 1.484, Global test loss: 1.534, Global test accuracy: 92.88 

Round  96, Train loss: 1.482, Test loss: 1.537, Test accuracy: 92.51 

Round  96, Global train loss: 1.482, Global test loss: 1.533, Global test accuracy: 92.96 

Round  97, Train loss: 1.484, Test loss: 1.537, Test accuracy: 92.56 

Round  97, Global train loss: 1.484, Global test loss: 1.533, Global test accuracy: 92.99 

Round  98, Train loss: 1.486, Test loss: 1.536, Test accuracy: 92.58 

Round  98, Global train loss: 1.486, Global test loss: 1.534, Global test accuracy: 92.93 

Round  99, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.59 

Round  99, Global train loss: 1.487, Global test loss: 1.533, Global test accuracy: 93.04 

Final Round, Train loss: 1.482, Test loss: 1.536, Test accuracy: 92.67 

Final Round, Global train loss: 1.482, Global test loss: 1.533, Global test accuracy: 93.04 

Average accuracy final 10 rounds: 92.49749999999999 

Average global accuracy final 10 rounds: 92.91916666666667 

1552.9877099990845
[0.8953104019165039, 1.689445972442627, 2.475475549697876, 3.2201168537139893, 3.948620080947876, 4.691183805465698, 5.412537336349487, 6.149012327194214, 6.879951238632202, 7.613078594207764, 8.336673259735107, 9.065937042236328, 9.799019575119019, 10.520833015441895, 11.24757170677185, 11.978991985321045, 12.713422536849976, 13.438501596450806, 14.226635217666626, 15.01457142829895, 15.80648922920227, 16.56087589263916, 17.335042476654053, 18.083659410476685, 18.855056762695312, 19.583167791366577, 20.39324975013733, 21.132752418518066, 21.869314432144165, 22.59630537033081, 23.375142812728882, 24.16739320755005, 24.94628381729126, 25.72016716003418, 26.4520423412323, 27.240153551101685, 28.01417326927185, 28.739413022994995, 29.46589708328247, 30.186758756637573, 30.913086652755737, 31.639954090118408, 32.34796643257141, 33.05781364440918, 33.767122983932495, 34.47498154640198, 35.185537576675415, 35.89658188819885, 36.6138801574707, 37.32261395454407, 38.0313241481781, 38.7367639541626, 39.4560604095459, 40.163344860076904, 40.912710666656494, 41.633296966552734, 42.33846855163574, 43.05579423904419, 43.77517366409302, 44.49971580505371, 45.23536443710327, 45.90523815155029, 46.67162728309631, 47.46012306213379, 48.22383213043213, 48.985989809036255, 49.75343942642212, 50.53188371658325, 51.30871224403381, 52.081833600997925, 52.84612441062927, 53.610403299331665, 54.370705366134644, 55.12875747680664, 55.89618444442749, 56.66911792755127, 57.4407422542572, 58.199383020401, 58.96493649482727, 59.72967457771301, 60.49650025367737, 61.27235794067383, 62.06479787826538, 62.84554982185364, 63.624672412872314, 64.39223194122314, 65.1536271572113, 65.93152904510498, 66.69606065750122, 67.46660161018372, 68.31185722351074, 69.09862327575684, 69.88811159133911, 70.66723299026489, 71.43551230430603, 72.19937205314636, 72.9682195186615, 73.78821611404419, 74.55309724807739, 75.31335949897766, 76.83424782752991]
[17.745833333333334, 25.0875, 38.5, 37.954166666666666, 45.525, 55.99166666666667, 63.8, 67.87083333333334, 68.8125, 74.33333333333333, 74.8875, 78.32916666666667, 78.775, 81.25833333333334, 81.98333333333333, 82.12083333333334, 82.55416666666666, 82.6625, 82.76666666666667, 82.80416666666666, 82.92916666666666, 82.97083333333333, 83.10416666666667, 83.21666666666667, 83.4875, 84.59583333333333, 85.62083333333334, 86.8875, 87.6125, 88.11666666666666, 88.74583333333334, 89.47916666666667, 89.64583333333333, 89.85, 89.95, 89.97083333333333, 90.05833333333334, 90.34583333333333, 90.45416666666667, 90.57916666666667, 90.55, 90.64166666666667, 90.89583333333333, 90.9375, 91.00416666666666, 91.05, 91.09583333333333, 91.1375, 91.1625, 91.29166666666667, 91.32083333333334, 91.40833333333333, 91.48333333333333, 91.575, 91.57083333333334, 91.65416666666667, 91.72083333333333, 91.6875, 91.59583333333333, 91.62083333333334, 91.76666666666667, 91.80833333333334, 91.875, 91.87916666666666, 91.8625, 91.92083333333333, 91.92916666666666, 92.01666666666667, 92.07916666666667, 92.075, 92.09583333333333, 92.08333333333333, 92.09583333333333, 92.13333333333334, 92.1875, 92.175, 92.2125, 92.21666666666667, 92.21666666666667, 92.17916666666666, 92.1875, 92.25, 92.29583333333333, 92.33333333333333, 92.35416666666667, 92.34166666666667, 92.3625, 92.4125, 92.41666666666667, 92.39583333333333, 92.43333333333334, 92.4125, 92.39583333333333, 92.42083333333333, 92.51666666666667, 92.54583333333333, 92.5125, 92.5625, 92.58333333333333, 92.59166666666667, 92.66666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 15.45 

Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 18.57 

Round   2, Train loss: 2.298, Test loss: 2.297, Test accuracy: 23.68 

Round   3, Train loss: 2.295, Test loss: 2.293, Test accuracy: 30.73 

Round   4, Train loss: 2.290, Test loss: 2.287, Test accuracy: 34.70 

Round   5, Train loss: 2.279, Test loss: 2.269, Test accuracy: 30.37 

Round   6, Train loss: 2.247, Test loss: 2.232, Test accuracy: 30.70 

Round   7, Train loss: 2.197, Test loss: 2.186, Test accuracy: 35.43 

Round   8, Train loss: 2.129, Test loss: 2.110, Test accuracy: 43.72 

Round   9, Train loss: 2.048, Test loss: 2.029, Test accuracy: 48.94 

Round  10, Train loss: 1.943, Test loss: 1.946, Test accuracy: 60.12 

Round  11, Train loss: 1.852, Test loss: 1.874, Test accuracy: 65.54 

Round  12, Train loss: 1.812, Test loss: 1.821, Test accuracy: 68.78 

Round  13, Train loss: 1.756, Test loss: 1.789, Test accuracy: 71.09 

Round  14, Train loss: 1.717, Test loss: 1.757, Test accuracy: 74.16 

Round  15, Train loss: 1.671, Test loss: 1.730, Test accuracy: 76.41 

Round  16, Train loss: 1.649, Test loss: 1.700, Test accuracy: 79.19 

Round  17, Train loss: 1.643, Test loss: 1.673, Test accuracy: 81.85 

Round  18, Train loss: 1.615, Test loss: 1.660, Test accuracy: 82.70 

Round  19, Train loss: 1.606, Test loss: 1.649, Test accuracy: 83.42 

Round  20, Train loss: 1.596, Test loss: 1.635, Test accuracy: 84.74 

Round  21, Train loss: 1.592, Test loss: 1.624, Test accuracy: 85.72 

Round  22, Train loss: 1.576, Test loss: 1.620, Test accuracy: 85.97 

Round  23, Train loss: 1.578, Test loss: 1.613, Test accuracy: 86.60 

Round  24, Train loss: 1.570, Test loss: 1.609, Test accuracy: 86.80 

Round  25, Train loss: 1.560, Test loss: 1.602, Test accuracy: 87.45 

Round  26, Train loss: 1.558, Test loss: 1.600, Test accuracy: 87.63 

Round  27, Train loss: 1.562, Test loss: 1.596, Test accuracy: 87.94 

Round  28, Train loss: 1.561, Test loss: 1.596, Test accuracy: 87.80 

Round  29, Train loss: 1.556, Test loss: 1.594, Test accuracy: 87.97 

Round  30, Train loss: 1.554, Test loss: 1.592, Test accuracy: 88.17 

Round  31, Train loss: 1.546, Test loss: 1.590, Test accuracy: 88.17 

Round  32, Train loss: 1.551, Test loss: 1.588, Test accuracy: 88.36 

Round  33, Train loss: 1.536, Test loss: 1.588, Test accuracy: 88.32 

Round  34, Train loss: 1.528, Test loss: 1.586, Test accuracy: 88.39 

Round  35, Train loss: 1.542, Test loss: 1.585, Test accuracy: 88.51 

Round  36, Train loss: 1.534, Test loss: 1.584, Test accuracy: 88.63 

Round  37, Train loss: 1.533, Test loss: 1.584, Test accuracy: 88.67 

Round  38, Train loss: 1.527, Test loss: 1.583, Test accuracy: 88.75 

Round  39, Train loss: 1.531, Test loss: 1.583, Test accuracy: 88.80 

Round  40, Train loss: 1.535, Test loss: 1.582, Test accuracy: 88.70 

Round  41, Train loss: 1.533, Test loss: 1.584, Test accuracy: 88.41 

Round  42, Train loss: 1.534, Test loss: 1.583, Test accuracy: 88.56 

Round  43, Train loss: 1.529, Test loss: 1.581, Test accuracy: 88.70 

Round  44, Train loss: 1.530, Test loss: 1.581, Test accuracy: 88.70 

Round  45, Train loss: 1.522, Test loss: 1.581, Test accuracy: 88.67 

Round  46, Train loss: 1.525, Test loss: 1.581, Test accuracy: 88.65 

Round  47, Train loss: 1.521, Test loss: 1.580, Test accuracy: 88.80 

Round  48, Train loss: 1.528, Test loss: 1.579, Test accuracy: 88.89 

Round  49, Train loss: 1.522, Test loss: 1.579, Test accuracy: 88.86 

Round  50, Train loss: 1.513, Test loss: 1.578, Test accuracy: 88.98 

Round  51, Train loss: 1.518, Test loss: 1.577, Test accuracy: 88.99 

Round  52, Train loss: 1.510, Test loss: 1.577, Test accuracy: 88.97 

Round  53, Train loss: 1.514, Test loss: 1.577, Test accuracy: 88.93 

Round  54, Train loss: 1.514, Test loss: 1.576, Test accuracy: 88.93 

Round  55, Train loss: 1.517, Test loss: 1.576, Test accuracy: 88.99 

Round  56, Train loss: 1.516, Test loss: 1.575, Test accuracy: 89.03 

Round  57, Train loss: 1.516, Test loss: 1.575, Test accuracy: 89.10 

Round  58, Train loss: 1.514, Test loss: 1.574, Test accuracy: 89.06 

Round  59, Train loss: 1.515, Test loss: 1.573, Test accuracy: 89.27 

Round  60, Train loss: 1.516, Test loss: 1.572, Test accuracy: 89.25 

Round  61, Train loss: 1.513, Test loss: 1.573, Test accuracy: 89.12 

Round  62, Train loss: 1.510, Test loss: 1.572, Test accuracy: 89.31 

Round  63, Train loss: 1.507, Test loss: 1.571, Test accuracy: 89.42 

Round  64, Train loss: 1.508, Test loss: 1.571, Test accuracy: 89.45 

Round  65, Train loss: 1.510, Test loss: 1.571, Test accuracy: 89.35 

Round  66, Train loss: 1.505, Test loss: 1.572, Test accuracy: 89.35 

Round  67, Train loss: 1.511, Test loss: 1.571, Test accuracy: 89.40 

Round  68, Train loss: 1.507, Test loss: 1.571, Test accuracy: 89.34 

Round  69, Train loss: 1.509, Test loss: 1.572, Test accuracy: 89.29 

Round  70, Train loss: 1.507, Test loss: 1.571, Test accuracy: 89.38 

Round  71, Train loss: 1.505, Test loss: 1.571, Test accuracy: 89.37 

Round  72, Train loss: 1.504, Test loss: 1.570, Test accuracy: 89.38 

Round  73, Train loss: 1.504, Test loss: 1.571, Test accuracy: 89.34 

Round  74, Train loss: 1.501, Test loss: 1.570, Test accuracy: 89.39 

Round  75, Train loss: 1.502, Test loss: 1.569, Test accuracy: 89.49 

Round  76, Train loss: 1.505, Test loss: 1.569, Test accuracy: 89.55 

Round  77, Train loss: 1.504, Test loss: 1.569, Test accuracy: 89.58 

Round  78, Train loss: 1.501, Test loss: 1.569, Test accuracy: 89.59 

Round  79, Train loss: 1.502, Test loss: 1.568, Test accuracy: 89.62 

Round  80, Train loss: 1.508, Test loss: 1.568, Test accuracy: 89.58 

Round  81, Train loss: 1.502, Test loss: 1.568, Test accuracy: 89.55 

Round  82, Train loss: 1.505, Test loss: 1.568, Test accuracy: 89.64 

Round  83, Train loss: 1.497, Test loss: 1.568, Test accuracy: 89.60 

Round  84, Train loss: 1.504, Test loss: 1.568, Test accuracy: 89.52 

Round  85, Train loss: 1.500, Test loss: 1.567, Test accuracy: 89.61 

Round  86, Train loss: 1.500, Test loss: 1.568, Test accuracy: 89.63 

Round  87, Train loss: 1.500, Test loss: 1.568, Test accuracy: 89.52 

Round  88, Train loss: 1.503, Test loss: 1.568, Test accuracy: 89.52 

Round  89, Train loss: 1.499, Test loss: 1.568, Test accuracy: 89.56 

Round  90, Train loss: 1.498, Test loss: 1.568, Test accuracy: 89.57 

Round  91, Train loss: 1.496, Test loss: 1.568, Test accuracy: 89.53 

Round  92, Train loss: 1.496, Test loss: 1.568, Test accuracy: 89.51 

Round  93, Train loss: 1.502, Test loss: 1.568, Test accuracy: 89.58 

Round  94, Train loss: 1.500, Test loss: 1.568, Test accuracy: 89.51 

Round  95, Train loss: 1.496, Test loss: 1.568, Test accuracy: 89.50 

Round  96, Train loss: 1.502, Test loss: 1.568, Test accuracy: 89.51 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.499, Test loss: 1.568, Test accuracy: 89.58 

Round  98, Train loss: 1.501, Test loss: 1.568, Test accuracy: 89.61 

Round  99, Train loss: 1.495, Test loss: 1.567, Test accuracy: 89.56 

Final Round, Train loss: 1.498, Test loss: 1.567, Test accuracy: 89.58 

Average accuracy final 10 rounds: 89.545 

967.5328953266144
[0.8064167499542236, 1.5009090900421143, 2.1970160007476807, 2.910649061203003, 3.6054656505584717, 4.307293176651001, 5.016210079193115, 5.7261834144592285, 6.411076784133911, 7.102302551269531, 7.797654390335083, 8.499608993530273, 9.195321321487427, 9.919674634933472, 10.632213830947876, 11.333986520767212, 12.040103673934937, 12.753026247024536, 13.456572771072388, 14.167261123657227, 14.88765001296997, 15.587766647338867, 16.29499125480652, 17.011703968048096, 17.714811086654663, 18.420700311660767, 19.135445594787598, 19.840261220932007, 20.554267168045044, 21.268276929855347, 21.972400426864624, 22.692345142364502, 23.409812927246094, 24.115484476089478, 24.825416088104248, 25.530511379241943, 26.244978427886963, 26.95508122444153, 27.674256324768066, 28.389865159988403, 29.10219693183899, 29.803269863128662, 30.515561819076538, 31.22619342803955, 31.939321041107178, 32.641449213027954, 33.34954762458801, 34.068272829055786, 34.77640390396118, 35.477092027664185, 36.18278193473816, 36.885032415390015, 37.592506885528564, 38.29179286956787, 38.9962100982666, 39.663695335388184, 40.332066774368286, 41.049808740615845, 41.744221687316895, 42.45349192619324, 43.15511965751648, 43.86083102226257, 44.567180156707764, 45.261605739593506, 45.964516162872314, 46.66029715538025, 47.375688552856445, 48.07715463638306, 48.790666341781616, 49.49388337135315, 50.19762659072876, 50.9234516620636, 51.627663373947144, 52.33125352859497, 53.064600229263306, 53.77564477920532, 54.473530530929565, 55.19940900802612, 55.91742706298828, 56.62067914009094, 57.35260558128357, 58.072895526885986, 58.78845477104187, 59.49057388305664, 60.200732707977295, 60.91790533065796, 61.55337643623352, 62.313576221466064, 63.10873746871948, 63.80247402191162, 64.53016662597656, 65.24845218658447, 65.93952178955078, 66.64676809310913, 67.36087036132812, 68.07394051551819, 68.70888566970825, 69.34825372695923, 69.98439598083496, 70.62887215614319, 72.07287120819092]
[15.45, 18.570833333333333, 23.683333333333334, 30.729166666666668, 34.7, 30.366666666666667, 30.704166666666666, 35.42916666666667, 43.71666666666667, 48.9375, 60.11666666666667, 65.54166666666667, 68.77916666666667, 71.09166666666667, 74.15833333333333, 76.4125, 79.19166666666666, 81.85, 82.7, 83.41666666666667, 84.74166666666666, 85.725, 85.97083333333333, 86.59583333333333, 86.8, 87.45416666666667, 87.63333333333334, 87.9375, 87.80416666666666, 87.97083333333333, 88.16666666666667, 88.17083333333333, 88.3625, 88.31666666666666, 88.3875, 88.5125, 88.62916666666666, 88.66666666666667, 88.74583333333334, 88.80416666666666, 88.69583333333334, 88.4125, 88.55833333333334, 88.7, 88.7, 88.67083333333333, 88.65416666666667, 88.8, 88.89166666666667, 88.85833333333333, 88.97916666666667, 88.99166666666666, 88.975, 88.93333333333334, 88.93333333333334, 88.99166666666666, 89.03333333333333, 89.1, 89.05833333333334, 89.27083333333333, 89.25416666666666, 89.12083333333334, 89.3125, 89.41666666666667, 89.44583333333334, 89.35416666666667, 89.35416666666667, 89.4, 89.3375, 89.2875, 89.38333333333334, 89.37083333333334, 89.38333333333334, 89.3375, 89.3875, 89.4875, 89.54583333333333, 89.57916666666667, 89.59166666666667, 89.625, 89.58333333333333, 89.55416666666666, 89.6375, 89.6, 89.51666666666667, 89.60833333333333, 89.63333333333334, 89.52083333333333, 89.52083333333333, 89.55833333333334, 89.57083333333334, 89.525, 89.5125, 89.57916666666667, 89.50833333333334, 89.49583333333334, 89.50833333333334, 89.58333333333333, 89.60833333333333, 89.55833333333334, 89.575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 13.61 

Round   1, Train loss: 2.298, Test loss: 2.296, Test accuracy: 26.71 

Round   2, Train loss: 2.293, Test loss: 2.288, Test accuracy: 36.80 

Round   3, Train loss: 2.267, Test loss: 2.244, Test accuracy: 28.50 

Round   4, Train loss: 2.166, Test loss: 2.147, Test accuracy: 36.38 

Round   5, Train loss: 2.053, Test loss: 2.028, Test accuracy: 51.45 

Round   6, Train loss: 1.931, Test loss: 1.938, Test accuracy: 56.26 

Round   7, Train loss: 1.872, Test loss: 1.888, Test accuracy: 58.77 

Round   8, Train loss: 1.815, Test loss: 1.850, Test accuracy: 62.48 

Round   9, Train loss: 1.771, Test loss: 1.797, Test accuracy: 68.25 

Round  10, Train loss: 1.711, Test loss: 1.759, Test accuracy: 71.97 

Round  11, Train loss: 1.664, Test loss: 1.717, Test accuracy: 76.31 

Round  12, Train loss: 1.636, Test loss: 1.672, Test accuracy: 81.01 

Round  13, Train loss: 1.578, Test loss: 1.650, Test accuracy: 82.87 

Round  14, Train loss: 1.584, Test loss: 1.629, Test accuracy: 84.80 

Round  15, Train loss: 1.584, Test loss: 1.607, Test accuracy: 87.28 

Round  16, Train loss: 1.560, Test loss: 1.600, Test accuracy: 87.58 

Round  17, Train loss: 1.547, Test loss: 1.595, Test accuracy: 87.93 

Round  18, Train loss: 1.553, Test loss: 1.585, Test accuracy: 88.87 

Round  19, Train loss: 1.541, Test loss: 1.582, Test accuracy: 89.11 

Round  20, Train loss: 1.549, Test loss: 1.574, Test accuracy: 89.78 

Round  21, Train loss: 1.533, Test loss: 1.571, Test accuracy: 89.91 

Round  22, Train loss: 1.528, Test loss: 1.569, Test accuracy: 90.00 

Round  23, Train loss: 1.522, Test loss: 1.568, Test accuracy: 90.10 

Round  24, Train loss: 1.533, Test loss: 1.565, Test accuracy: 90.41 

Round  25, Train loss: 1.533, Test loss: 1.565, Test accuracy: 90.38 

Round  26, Train loss: 1.522, Test loss: 1.563, Test accuracy: 90.58 

Round  27, Train loss: 1.524, Test loss: 1.562, Test accuracy: 90.63 

Round  28, Train loss: 1.522, Test loss: 1.561, Test accuracy: 90.64 

Round  29, Train loss: 1.521, Test loss: 1.560, Test accuracy: 90.68 

Round  30, Train loss: 1.516, Test loss: 1.559, Test accuracy: 90.79 

Round  31, Train loss: 1.511, Test loss: 1.559, Test accuracy: 90.77 

Round  32, Train loss: 1.516, Test loss: 1.558, Test accuracy: 90.88 

Round  33, Train loss: 1.511, Test loss: 1.557, Test accuracy: 90.94 

Round  34, Train loss: 1.505, Test loss: 1.557, Test accuracy: 90.96 

Round  35, Train loss: 1.514, Test loss: 1.556, Test accuracy: 91.08 

Round  36, Train loss: 1.506, Test loss: 1.554, Test accuracy: 91.16 

Round  37, Train loss: 1.511, Test loss: 1.554, Test accuracy: 91.20 

Round  38, Train loss: 1.499, Test loss: 1.554, Test accuracy: 91.13 

Round  39, Train loss: 1.501, Test loss: 1.553, Test accuracy: 91.19 

Round  40, Train loss: 1.506, Test loss: 1.553, Test accuracy: 91.22 

Round  41, Train loss: 1.499, Test loss: 1.552, Test accuracy: 91.37 

Round  42, Train loss: 1.495, Test loss: 1.552, Test accuracy: 91.32 

Round  43, Train loss: 1.499, Test loss: 1.552, Test accuracy: 91.32 

Round  44, Train loss: 1.494, Test loss: 1.551, Test accuracy: 91.32 

Round  45, Train loss: 1.496, Test loss: 1.550, Test accuracy: 91.45 

Round  46, Train loss: 1.498, Test loss: 1.550, Test accuracy: 91.43 

Round  47, Train loss: 1.497, Test loss: 1.550, Test accuracy: 91.51 

Round  48, Train loss: 1.493, Test loss: 1.550, Test accuracy: 91.46 

Round  49, Train loss: 1.499, Test loss: 1.550, Test accuracy: 91.49 

Round  50, Train loss: 1.495, Test loss: 1.549, Test accuracy: 91.61 

Round  51, Train loss: 1.499, Test loss: 1.549, Test accuracy: 91.51 

Round  52, Train loss: 1.496, Test loss: 1.549, Test accuracy: 91.43 

Round  53, Train loss: 1.488, Test loss: 1.549, Test accuracy: 91.57 

Round  54, Train loss: 1.496, Test loss: 1.548, Test accuracy: 91.53 

Round  55, Train loss: 1.496, Test loss: 1.548, Test accuracy: 91.58 

Round  56, Train loss: 1.495, Test loss: 1.548, Test accuracy: 91.55 

Round  57, Train loss: 1.491, Test loss: 1.548, Test accuracy: 91.59 

Round  58, Train loss: 1.491, Test loss: 1.548, Test accuracy: 91.60 

Round  59, Train loss: 1.497, Test loss: 1.548, Test accuracy: 91.59 

Round  60, Train loss: 1.491, Test loss: 1.548, Test accuracy: 91.57 

Round  61, Train loss: 1.497, Test loss: 1.547, Test accuracy: 91.63 

Round  62, Train loss: 1.496, Test loss: 1.547, Test accuracy: 91.61 

Round  63, Train loss: 1.494, Test loss: 1.547, Test accuracy: 91.68 

Round  64, Train loss: 1.491, Test loss: 1.547, Test accuracy: 91.71 

Round  65, Train loss: 1.486, Test loss: 1.546, Test accuracy: 91.69 

Round  66, Train loss: 1.491, Test loss: 1.546, Test accuracy: 91.78 

Round  67, Train loss: 1.492, Test loss: 1.546, Test accuracy: 91.77 

Round  68, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.77 

Round  69, Train loss: 1.489, Test loss: 1.546, Test accuracy: 91.76 

Round  70, Train loss: 1.490, Test loss: 1.546, Test accuracy: 91.77 

Round  71, Train loss: 1.489, Test loss: 1.546, Test accuracy: 91.78 

Round  72, Train loss: 1.487, Test loss: 1.546, Test accuracy: 91.80 

Round  73, Train loss: 1.489, Test loss: 1.545, Test accuracy: 91.83 

Round  74, Train loss: 1.493, Test loss: 1.545, Test accuracy: 91.86 

Round  75, Train loss: 1.489, Test loss: 1.545, Test accuracy: 91.87 

Round  76, Train loss: 1.490, Test loss: 1.545, Test accuracy: 91.90 

Round  77, Train loss: 1.488, Test loss: 1.545, Test accuracy: 91.93 

Round  78, Train loss: 1.490, Test loss: 1.545, Test accuracy: 91.87 

Round  79, Train loss: 1.487, Test loss: 1.545, Test accuracy: 91.89 

Round  80, Train loss: 1.487, Test loss: 1.544, Test accuracy: 91.93 

Round  81, Train loss: 1.488, Test loss: 1.545, Test accuracy: 91.89 

Round  82, Train loss: 1.488, Test loss: 1.544, Test accuracy: 91.89 

Round  83, Train loss: 1.490, Test loss: 1.544, Test accuracy: 91.89 

Round  84, Train loss: 1.491, Test loss: 1.544, Test accuracy: 91.93 

Round  85, Train loss: 1.489, Test loss: 1.544, Test accuracy: 91.92 

Round  86, Train loss: 1.488, Test loss: 1.544, Test accuracy: 91.94 

Round  87, Train loss: 1.489, Test loss: 1.544, Test accuracy: 91.98 

Round  88, Train loss: 1.490, Test loss: 1.544, Test accuracy: 91.94 

Round  89, Train loss: 1.492, Test loss: 1.544, Test accuracy: 91.95 

Round  90, Train loss: 1.491, Test loss: 1.544, Test accuracy: 91.94 

Round  91, Train loss: 1.488, Test loss: 1.544, Test accuracy: 91.97 

Round  92, Train loss: 1.484, Test loss: 1.544, Test accuracy: 91.86 

Round  93, Train loss: 1.486, Test loss: 1.544, Test accuracy: 91.92 

Round  94, Train loss: 1.485, Test loss: 1.543, Test accuracy: 91.96 

Round  95, Train loss: 1.489, Test loss: 1.544, Test accuracy: 91.97 

Round  96, Train loss: 1.482, Test loss: 1.543, Test accuracy: 92.00 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 1.492, Test loss: 1.543, Test accuracy: 91.94 

Round  98, Train loss: 1.486, Test loss: 1.543, Test accuracy: 91.97 

Round  99, Train loss: 1.487, Test loss: 1.543, Test accuracy: 91.95 

Final Round, Train loss: 1.487, Test loss: 1.544, Test accuracy: 91.88 

Average accuracy final 10 rounds: 91.94725 

1312.6537284851074
[0.8671011924743652, 1.6518175601959229, 2.428720235824585, 3.211690902709961, 3.9793343544006348, 4.745970249176025, 5.512547492980957, 6.190376043319702, 6.94463324546814, 7.7078282833099365, 8.457167148590088, 9.213133811950684, 9.970770120620728, 10.727103233337402, 11.486729621887207, 12.255068063735962, 13.019532203674316, 13.790574312210083, 14.523873567581177, 15.274861574172974, 16.03079915046692, 16.78571915626526, 17.63083553314209, 18.290853261947632, 18.972169399261475, 19.6308753490448, 20.312336683273315, 20.960796356201172, 21.64022421836853, 22.287991762161255, 23.11345911026001, 23.788095235824585, 24.542363166809082, 25.21592354774475, 25.868545532226562, 26.552165031433105, 27.197285890579224, 27.876340866088867, 28.519158363342285, 29.20153284072876, 29.8669216632843, 30.563772678375244, 31.226439714431763, 31.92083740234375, 32.6034460067749, 33.29109811782837, 33.97287845611572, 34.647239208221436, 35.328819274902344, 35.99285650253296, 36.67279314994812, 37.33540940284729, 38.0209846496582, 38.69564652442932, 39.37561106681824, 40.04163455963135, 40.71954131126404, 41.40036106109619, 42.06536293029785, 42.74460577964783, 43.411521434783936, 44.09211087226868, 44.76215124130249, 45.45008087158203, 46.12987518310547, 46.80963325500488, 47.48821425437927, 48.16803526878357, 48.84701108932495, 49.51552700996399, 50.20466494560242, 50.87234568595886, 51.560004234313965, 52.225003719329834, 52.911805629730225, 53.57896637916565, 54.25658321380615, 54.9221887588501, 55.6006805896759, 56.267508029937744, 56.93574571609497, 57.62168025970459, 58.29542636871338, 58.98489189147949, 59.771257162094116, 60.46826195716858, 61.14691925048828, 61.83099102973938, 62.4945592880249, 63.171790599823, 63.839247703552246, 64.61203479766846, 65.38858413696289, 66.17673373222351, 66.84245991706848, 67.51963138580322, 68.20014071464539, 68.89414310455322, 69.57680034637451, 70.26500654220581, 71.50745224952698]
[13.6125, 26.715, 36.8025, 28.4975, 36.3775, 51.455, 56.2575, 58.77, 62.485, 68.255, 71.97, 76.315, 81.0125, 82.87, 84.7975, 87.275, 87.5775, 87.9325, 88.8725, 89.11, 89.78, 89.905, 90.0025, 90.0975, 90.4125, 90.3825, 90.5825, 90.6325, 90.635, 90.6775, 90.7925, 90.77, 90.875, 90.94, 90.9575, 91.075, 91.155, 91.2025, 91.1275, 91.1925, 91.2175, 91.3675, 91.3225, 91.3175, 91.32, 91.4475, 91.4275, 91.5125, 91.4575, 91.4875, 91.6075, 91.51, 91.4325, 91.5675, 91.53, 91.58, 91.5525, 91.5925, 91.5975, 91.5875, 91.5725, 91.6275, 91.6125, 91.6825, 91.7075, 91.6875, 91.78, 91.765, 91.7675, 91.7625, 91.77, 91.7825, 91.795, 91.8275, 91.865, 91.8675, 91.8975, 91.9275, 91.8725, 91.8875, 91.9275, 91.885, 91.89, 91.89, 91.9325, 91.925, 91.945, 91.98, 91.94, 91.9475, 91.94, 91.97, 91.855, 91.925, 91.9575, 91.965, 91.9975, 91.94, 91.975, 91.9475, 91.8825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.299, Test accuracy: 28.91 

Round   1, Train loss: 2.295, Test loss: 2.293, Test accuracy: 30.15 

Round   2, Train loss: 2.286, Test loss: 2.281, Test accuracy: 26.33 

Round   3, Train loss: 2.250, Test loss: 2.235, Test accuracy: 26.26 

Round   4, Train loss: 2.180, Test loss: 2.167, Test accuracy: 36.26 

Round   5, Train loss: 2.036, Test loss: 2.058, Test accuracy: 49.37 

Round   6, Train loss: 1.922, Test loss: 1.968, Test accuracy: 58.47 

Round   7, Train loss: 1.809, Test loss: 1.888, Test accuracy: 66.02 

Round   8, Train loss: 1.733, Test loss: 1.830, Test accuracy: 69.85 

Round   9, Train loss: 1.674, Test loss: 1.807, Test accuracy: 71.37 

Round  10, Train loss: 1.673, Test loss: 1.779, Test accuracy: 72.93 

Round  11, Train loss: 1.655, Test loss: 1.756, Test accuracy: 74.35 

Round  12, Train loss: 1.637, Test loss: 1.732, Test accuracy: 75.31 

Round  13, Train loss: 1.633, Test loss: 1.724, Test accuracy: 75.83 

Round  14, Train loss: 1.601, Test loss: 1.719, Test accuracy: 76.02 

Round  15, Train loss: 1.586, Test loss: 1.717, Test accuracy: 76.08 

Round  16, Train loss: 1.592, Test loss: 1.714, Test accuracy: 76.17 

Round  17, Train loss: 1.594, Test loss: 1.712, Test accuracy: 76.28 

Round  18, Train loss: 1.616, Test loss: 1.695, Test accuracy: 77.67 

Round  19, Train loss: 1.582, Test loss: 1.693, Test accuracy: 77.73 

Round  20, Train loss: 1.584, Test loss: 1.692, Test accuracy: 77.70 

Round  21, Train loss: 1.576, Test loss: 1.691, Test accuracy: 77.76 

Round  22, Train loss: 1.587, Test loss: 1.691, Test accuracy: 77.72 

Round  23, Train loss: 1.574, Test loss: 1.690, Test accuracy: 77.77 

Round  24, Train loss: 1.576, Test loss: 1.685, Test accuracy: 78.08 

Round  25, Train loss: 1.545, Test loss: 1.664, Test accuracy: 81.44 

Round  26, Train loss: 1.530, Test loss: 1.654, Test accuracy: 82.28 

Round  27, Train loss: 1.519, Test loss: 1.647, Test accuracy: 82.82 

Round  28, Train loss: 1.514, Test loss: 1.643, Test accuracy: 83.25 

Round  29, Train loss: 1.502, Test loss: 1.639, Test accuracy: 83.41 

Round  30, Train loss: 1.509, Test loss: 1.634, Test accuracy: 83.97 

Round  31, Train loss: 1.507, Test loss: 1.630, Test accuracy: 84.39 

Round  32, Train loss: 1.496, Test loss: 1.627, Test accuracy: 84.48 

Round  33, Train loss: 1.494, Test loss: 1.626, Test accuracy: 84.54 

Round  34, Train loss: 1.492, Test loss: 1.625, Test accuracy: 84.66 

Round  35, Train loss: 1.495, Test loss: 1.623, Test accuracy: 84.82 

Round  36, Train loss: 1.492, Test loss: 1.622, Test accuracy: 84.80 

Round  37, Train loss: 1.489, Test loss: 1.622, Test accuracy: 84.78 

Round  38, Train loss: 1.486, Test loss: 1.621, Test accuracy: 84.82 

Round  39, Train loss: 1.488, Test loss: 1.621, Test accuracy: 84.77 

Round  40, Train loss: 1.486, Test loss: 1.621, Test accuracy: 84.81 

Round  41, Train loss: 1.487, Test loss: 1.620, Test accuracy: 84.80 

Round  42, Train loss: 1.484, Test loss: 1.620, Test accuracy: 84.83 

Round  43, Train loss: 1.485, Test loss: 1.620, Test accuracy: 84.81 

Round  44, Train loss: 1.487, Test loss: 1.620, Test accuracy: 84.81 

Round  45, Train loss: 1.482, Test loss: 1.619, Test accuracy: 84.84 

Round  46, Train loss: 1.488, Test loss: 1.619, Test accuracy: 84.86 

Round  47, Train loss: 1.485, Test loss: 1.619, Test accuracy: 84.86 

Round  48, Train loss: 1.487, Test loss: 1.619, Test accuracy: 84.86 

Round  49, Train loss: 1.483, Test loss: 1.619, Test accuracy: 84.86 

Round  50, Train loss: 1.484, Test loss: 1.619, Test accuracy: 84.81 

Round  51, Train loss: 1.485, Test loss: 1.618, Test accuracy: 84.86 

Round  52, Train loss: 1.486, Test loss: 1.618, Test accuracy: 84.89 

Round  53, Train loss: 1.488, Test loss: 1.618, Test accuracy: 84.90 

Round  54, Train loss: 1.482, Test loss: 1.618, Test accuracy: 84.91 

Round  55, Train loss: 1.484, Test loss: 1.618, Test accuracy: 84.90 

Round  56, Train loss: 1.485, Test loss: 1.618, Test accuracy: 84.88 

Round  57, Train loss: 1.482, Test loss: 1.618, Test accuracy: 84.87 

Round  58, Train loss: 1.483, Test loss: 1.618, Test accuracy: 84.88 

Round  59, Train loss: 1.486, Test loss: 1.617, Test accuracy: 84.92 

Round  60, Train loss: 1.481, Test loss: 1.617, Test accuracy: 84.91 

Round  61, Train loss: 1.483, Test loss: 1.617, Test accuracy: 84.90 

Round  62, Train loss: 1.485, Test loss: 1.617, Test accuracy: 84.91 

Round  63, Train loss: 1.485, Test loss: 1.617, Test accuracy: 84.92 

Round  64, Train loss: 1.483, Test loss: 1.617, Test accuracy: 84.93 

Round  65, Train loss: 1.485, Test loss: 1.617, Test accuracy: 84.94 

Round  66, Train loss: 1.484, Test loss: 1.617, Test accuracy: 84.93 

Round  67, Train loss: 1.482, Test loss: 1.617, Test accuracy: 84.93 

Round  68, Train loss: 1.483, Test loss: 1.617, Test accuracy: 84.94 

Round  69, Train loss: 1.485, Test loss: 1.617, Test accuracy: 84.95 

Round  70, Train loss: 1.485, Test loss: 1.617, Test accuracy: 84.95 

Round  71, Train loss: 1.478, Test loss: 1.617, Test accuracy: 84.96 

Round  72, Train loss: 1.480, Test loss: 1.616, Test accuracy: 84.95 

Round  73, Train loss: 1.485, Test loss: 1.616, Test accuracy: 84.95 

Round  74, Train loss: 1.482, Test loss: 1.616, Test accuracy: 84.97 

Round  75, Train loss: 1.481, Test loss: 1.616, Test accuracy: 84.96 

Round  76, Train loss: 1.483, Test loss: 1.616, Test accuracy: 84.97 

Round  77, Train loss: 1.481, Test loss: 1.616, Test accuracy: 84.95 

Round  78, Train loss: 1.486, Test loss: 1.616, Test accuracy: 84.97 

Round  79, Train loss: 1.483, Test loss: 1.616, Test accuracy: 84.95 

Round  80, Train loss: 1.479, Test loss: 1.616, Test accuracy: 84.97 

Round  81, Train loss: 1.483, Test loss: 1.616, Test accuracy: 84.92 

Round  82, Train loss: 1.484, Test loss: 1.616, Test accuracy: 84.92 

Round  83, Train loss: 1.484, Test loss: 1.616, Test accuracy: 84.95 

Round  84, Train loss: 1.482, Test loss: 1.616, Test accuracy: 84.96 

Round  85, Train loss: 1.480, Test loss: 1.616, Test accuracy: 84.96 

Round  86, Train loss: 1.480, Test loss: 1.616, Test accuracy: 84.97 

Round  87, Train loss: 1.482, Test loss: 1.616, Test accuracy: 84.98 

Round  88, Train loss: 1.478, Test loss: 1.616, Test accuracy: 84.98 

Round  89, Train loss: 1.484, Test loss: 1.616, Test accuracy: 84.98 

Round  90, Train loss: 1.482, Test loss: 1.615, Test accuracy: 84.97 

Round  91, Train loss: 1.478, Test loss: 1.615, Test accuracy: 84.99 

Round  92, Train loss: 1.483, Test loss: 1.615, Test accuracy: 84.98 

Round  93, Train loss: 1.480, Test loss: 1.615, Test accuracy: 85.02 

Round  94, Train loss: 1.479, Test loss: 1.615, Test accuracy: 85.03 

Round  95, Train loss: 1.479, Test loss: 1.615, Test accuracy: 85.05 

Round  96, Train loss: 1.479, Test loss: 1.615, Test accuracy: 84.99 

Round  97, Train loss: 1.481, Test loss: 1.615, Test accuracy: 85.02 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 1.480, Test loss: 1.615, Test accuracy: 85.03 

Round  99, Train loss: 1.480, Test loss: 1.615, Test accuracy: 85.04 

Final Round, Train loss: 1.481, Test loss: 1.615, Test accuracy: 85.03 

Average accuracy final 10 rounds: 85.0115 

1368.4895243644714
[0.9360063076019287, 1.7741923332214355, 2.5363667011260986, 3.366760015487671, 4.214015960693359, 5.045633792877197, 5.881027698516846, 6.704463481903076, 7.543071269989014, 8.376037120819092, 9.209162712097168, 10.048389434814453, 10.895455360412598, 11.725019216537476, 12.576491594314575, 13.412883043289185, 14.261253356933594, 15.10545802116394, 15.950447797775269, 16.789649724960327, 17.596259832382202, 18.433396816253662, 19.126200437545776, 19.977777004241943, 20.829867124557495, 21.66858959197998, 22.4961359500885, 23.318268060684204, 24.00799322128296, 24.839732885360718, 25.69179606437683, 26.530172109603882, 27.341081380844116, 28.176079273223877, 29.019756078720093, 29.858181953430176, 30.707194805145264, 31.547969818115234, 32.23149847984314, 33.07436156272888, 33.910563468933105, 34.75685095787048, 35.589919328689575, 36.283639430999756, 37.07418441772461, 37.9325065612793, 38.60862135887146, 39.30812668800354, 40.141632318496704, 40.998894453048706, 41.83811092376709, 42.69898080825806, 43.52356195449829, 44.26641917228699, 45.094507932662964, 45.949212074279785, 46.775352239608765, 47.62990474700928, 48.45734214782715, 49.14489698410034, 49.968188524246216, 50.81573939323425, 51.47555899620056, 52.31122159957886, 53.13583326339722, 53.91132688522339, 54.74743700027466, 55.59512281417847, 56.43138241767883, 57.27248501777649, 58.09162640571594, 58.93767189979553, 59.76718044281006, 60.605806827545166, 61.43139719963074, 62.2654492855072, 63.09829139709473, 63.937607288360596, 64.77547121047974, 65.52313375473022, 66.36091470718384, 67.18967509269714, 67.87670135498047, 68.70273518562317, 69.39219617843628, 70.21612071990967, 71.06055927276611, 71.7241587638855, 72.56912136077881, 73.38391280174255, 74.07169699668884, 74.90005731582642, 75.74983811378479, 76.57402515411377, 77.41755723953247, 78.25816082954407, 78.94978427886963, 79.78305220603943, 80.55874752998352, 81.39646172523499, 82.94937944412231]
[28.905, 30.1525, 26.3275, 26.2625, 36.2575, 49.365, 58.465, 66.0225, 69.8525, 71.3675, 72.93, 74.35, 75.31, 75.83, 76.0225, 76.08, 76.165, 76.28, 77.67, 77.73, 77.705, 77.76, 77.725, 77.765, 78.0825, 81.44, 82.2775, 82.82, 83.2525, 83.4075, 83.9725, 84.3925, 84.485, 84.54, 84.66, 84.82, 84.795, 84.7775, 84.82, 84.7725, 84.805, 84.8025, 84.8325, 84.8125, 84.815, 84.8375, 84.8575, 84.8575, 84.8625, 84.8625, 84.815, 84.8625, 84.89, 84.8975, 84.905, 84.8975, 84.88, 84.87, 84.88, 84.915, 84.905, 84.8975, 84.9125, 84.9225, 84.9325, 84.945, 84.93, 84.93, 84.9425, 84.9475, 84.9475, 84.9575, 84.95, 84.9475, 84.965, 84.9575, 84.9725, 84.9475, 84.975, 84.9525, 84.97, 84.9225, 84.9225, 84.9525, 84.9625, 84.9625, 84.965, 84.9775, 84.98, 84.9775, 84.97, 84.99, 84.98, 85.02, 85.03, 85.045, 84.99, 85.015, 85.035, 85.04, 85.03]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.525, Test loss: 1.957, Test accuracy: 63.26
Round   1, Train loss: 1.294, Test loss: 1.794, Test accuracy: 73.28
Round   2, Train loss: 1.263, Test loss: 1.770, Test accuracy: 74.51
Round   3, Train loss: 1.231, Test loss: 1.731, Test accuracy: 78.64
Round   4, Train loss: 1.218, Test loss: 1.717, Test accuracy: 79.75
Round   5, Train loss: 1.206, Test loss: 1.710, Test accuracy: 80.31
Round   6, Train loss: 1.207, Test loss: 1.707, Test accuracy: 80.48
Round   7, Train loss: 1.199, Test loss: 1.703, Test accuracy: 80.93
Round   8, Train loss: 1.196, Test loss: 1.701, Test accuracy: 81.41
Round   9, Train loss: 1.196, Test loss: 1.699, Test accuracy: 82.27
Round  10, Train loss: 1.192, Test loss: 1.697, Test accuracy: 82.56
Round  11, Train loss: 1.190, Test loss: 1.697, Test accuracy: 82.65
Round  12, Train loss: 1.190, Test loss: 1.697, Test accuracy: 82.65
Round  13, Train loss: 1.187, Test loss: 1.696, Test accuracy: 82.67
Round  14, Train loss: 1.185, Test loss: 1.696, Test accuracy: 82.56
Round  15, Train loss: 1.187, Test loss: 1.699, Test accuracy: 82.44
Round  16, Train loss: 1.189, Test loss: 1.699, Test accuracy: 82.40
Round  17, Train loss: 1.182, Test loss: 1.699, Test accuracy: 82.39
Round  18, Train loss: 1.183, Test loss: 1.700, Test accuracy: 82.28
Round  19, Train loss: 1.183, Test loss: 1.700, Test accuracy: 82.21
Round  20, Train loss: 1.178, Test loss: 1.701, Test accuracy: 82.17
Round  21, Train loss: 1.183, Test loss: 1.701, Test accuracy: 82.11
Round  22, Train loss: 1.182, Test loss: 1.703, Test accuracy: 82.08
Round  23, Train loss: 1.181, Test loss: 1.705, Test accuracy: 81.94
Round  24, Train loss: 1.178, Test loss: 1.706, Test accuracy: 81.89
Round  25, Train loss: 1.179, Test loss: 1.707, Test accuracy: 81.78
Round  26, Train loss: 1.179, Test loss: 1.707, Test accuracy: 81.72
Round  27, Train loss: 1.175, Test loss: 1.709, Test accuracy: 81.67
Round  28, Train loss: 1.179, Test loss: 1.710, Test accuracy: 81.67
Round  29, Train loss: 1.180, Test loss: 1.711, Test accuracy: 81.53
Round  30, Train loss: 1.176, Test loss: 1.713, Test accuracy: 81.43
Round  31, Train loss: 1.179, Test loss: 1.715, Test accuracy: 81.29
Round  32, Train loss: 1.176, Test loss: 1.715, Test accuracy: 81.27
Round  33, Train loss: 1.176, Test loss: 1.716, Test accuracy: 81.25
Round  34, Train loss: 1.176, Test loss: 1.717, Test accuracy: 81.19
Round  35, Train loss: 1.178, Test loss: 1.719, Test accuracy: 81.02
Round  36, Train loss: 1.174, Test loss: 1.720, Test accuracy: 80.86
Round  37, Train loss: 1.177, Test loss: 1.721, Test accuracy: 80.80
Round  38, Train loss: 1.176, Test loss: 1.723, Test accuracy: 80.65
Round  39, Train loss: 1.178, Test loss: 1.723, Test accuracy: 80.64
Round  40, Train loss: 1.176, Test loss: 1.725, Test accuracy: 80.58
Round  41, Train loss: 1.175, Test loss: 1.726, Test accuracy: 80.49
Round  42, Train loss: 1.157, Test loss: 1.724, Test accuracy: 82.31
Round  43, Train loss: 1.122, Test loss: 1.712, Test accuracy: 83.89
Round  44, Train loss: 1.115, Test loss: 1.704, Test accuracy: 85.15
Round  45, Train loss: 1.121, Test loss: 1.701, Test accuracy: 85.76
Round  46, Train loss: 1.113, Test loss: 1.695, Test accuracy: 86.89
Round  47, Train loss: 1.119, Test loss: 1.694, Test accuracy: 86.87
Round  48, Train loss: 1.106, Test loss: 1.693, Test accuracy: 86.95
Round  49, Train loss: 1.110, Test loss: 1.690, Test accuracy: 87.17
Round  50, Train loss: 1.105, Test loss: 1.690, Test accuracy: 87.08
Round  51, Train loss: 1.106, Test loss: 1.690, Test accuracy: 87.45
Round  52, Train loss: 1.103, Test loss: 1.690, Test accuracy: 87.41
Round  53, Train loss: 1.104, Test loss: 1.690, Test accuracy: 87.37
Round  54, Train loss: 1.108, Test loss: 1.687, Test accuracy: 87.68
Round  55, Train loss: 1.107, Test loss: 1.687, Test accuracy: 87.80
Round  56, Train loss: 1.105, Test loss: 1.687, Test accuracy: 87.69
Round  57, Train loss: 1.103, Test loss: 1.688, Test accuracy: 87.55
Round  58, Train loss: 1.105, Test loss: 1.687, Test accuracy: 87.58
Round  59, Train loss: 1.102, Test loss: 1.688, Test accuracy: 87.64
Round  60, Train loss: 1.103, Test loss: 1.688, Test accuracy: 87.55
Round  61, Train loss: 1.104, Test loss: 1.689, Test accuracy: 87.51
Round  62, Train loss: 1.104, Test loss: 1.690, Test accuracy: 87.32
Round  63, Train loss: 1.103, Test loss: 1.691, Test accuracy: 87.22
Round  64, Train loss: 1.103, Test loss: 1.691, Test accuracy: 87.14
Round  65, Train loss: 1.102, Test loss: 1.692, Test accuracy: 87.11
Round  66, Train loss: 1.101, Test loss: 1.693, Test accuracy: 86.98
Round  67, Train loss: 1.103, Test loss: 1.693, Test accuracy: 86.94
Round  68, Train loss: 1.103, Test loss: 1.693, Test accuracy: 86.89
Round  69, Train loss: 1.104, Test loss: 1.695, Test accuracy: 86.78
Round  70, Train loss: 1.103, Test loss: 1.694, Test accuracy: 86.82
Round  71, Train loss: 1.103, Test loss: 1.695, Test accuracy: 86.64
Round  72, Train loss: 1.103, Test loss: 1.696, Test accuracy: 86.61
Round  73, Train loss: 1.102, Test loss: 1.696, Test accuracy: 86.58
Round  74, Train loss: 1.103, Test loss: 1.697, Test accuracy: 86.63
Round  75, Train loss: 1.102, Test loss: 1.697, Test accuracy: 86.55
Round  76, Train loss: 1.101, Test loss: 1.698, Test accuracy: 86.42
Round  77, Train loss: 1.103, Test loss: 1.699, Test accuracy: 86.30
Round  78, Train loss: 1.102, Test loss: 1.699, Test accuracy: 86.33
Round  79, Train loss: 1.103, Test loss: 1.700, Test accuracy: 86.30
Round  80, Train loss: 1.103, Test loss: 1.700, Test accuracy: 86.22
Round  81, Train loss: 1.101, Test loss: 1.700, Test accuracy: 86.22
Round  82, Train loss: 1.102, Test loss: 1.702, Test accuracy: 86.07
Round  83, Train loss: 1.101, Test loss: 1.703, Test accuracy: 85.92
Round  84, Train loss: 1.102, Test loss: 1.703, Test accuracy: 85.89
Round  85, Train loss: 1.102, Test loss: 1.703, Test accuracy: 85.85
Round  86, Train loss: 1.101, Test loss: 1.704, Test accuracy: 85.70
Round  87, Train loss: 1.103, Test loss: 1.705, Test accuracy: 85.71
Round  88, Train loss: 1.102, Test loss: 1.705, Test accuracy: 85.63
Round  89, Train loss: 1.102, Test loss: 1.706, Test accuracy: 85.59
Round  90, Train loss: 1.102, Test loss: 1.707, Test accuracy: 85.55
Round  91, Train loss: 1.102, Test loss: 1.707, Test accuracy: 85.56
Round  92, Train loss: 1.101, Test loss: 1.708, Test accuracy: 85.47
Round  93, Train loss: 1.101, Test loss: 1.707, Test accuracy: 85.52
Round  94, Train loss: 1.102, Test loss: 1.708, Test accuracy: 85.45
Round  95, Train loss: 1.102, Test loss: 1.708, Test accuracy: 85.42
Round  96, Train loss: 1.101, Test loss: 1.708, Test accuracy: 85.36
Round  97, Train loss: 1.100, Test loss: 1.708, Test accuracy: 85.31
Round  98, Train loss: 1.102, Test loss: 1.710, Test accuracy: 85.25
Round  99, Train loss: 1.101, Test loss: 1.711, Test accuracy: 85.19
Final Round, Train loss: 1.101, Test loss: 1.713, Test accuracy: 85.05
Average accuracy final 10 rounds: 85.40825000000001
4391.693391561508
[]
[63.2575, 73.285, 74.5125, 78.645, 79.75, 80.3075, 80.485, 80.93, 81.4075, 82.2675, 82.5575, 82.6525, 82.6475, 82.665, 82.5625, 82.435, 82.4, 82.3875, 82.275, 82.2125, 82.17, 82.105, 82.0775, 81.935, 81.89, 81.7825, 81.725, 81.67, 81.675, 81.525, 81.4275, 81.2875, 81.27, 81.25, 81.185, 81.0175, 80.8625, 80.7975, 80.6475, 80.6375, 80.575, 80.49, 82.3125, 83.895, 85.1525, 85.76, 86.8875, 86.8675, 86.95, 87.165, 87.08, 87.4525, 87.4075, 87.3675, 87.68, 87.8, 87.685, 87.5475, 87.5825, 87.64, 87.5525, 87.5075, 87.3225, 87.215, 87.135, 87.11, 86.985, 86.94, 86.8875, 86.7825, 86.8175, 86.64, 86.615, 86.58, 86.6275, 86.545, 86.425, 86.295, 86.33, 86.2975, 86.22, 86.2175, 86.0675, 85.9175, 85.8875, 85.85, 85.705, 85.7125, 85.63, 85.5875, 85.545, 85.565, 85.475, 85.5175, 85.455, 85.42, 85.3625, 85.305, 85.245, 85.1925, 85.0475]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.281, Test loss: 2.269, Test accuracy: 25.14
Round   0: Global train loss: 2.281, Global test loss: 2.301, Global test accuracy: 19.94
Round   1, Train loss: 2.140, Test loss: 2.204, Test accuracy: 34.27
Round   1: Global train loss: 2.140, Global test loss: 2.298, Global test accuracy: 28.98
Round   2, Train loss: 2.014, Test loss: 2.134, Test accuracy: 41.02
Round   2: Global train loss: 2.014, Global test loss: 2.293, Global test accuracy: 39.06
Round   3, Train loss: 1.788, Test loss: 2.014, Test accuracy: 51.49
Round   3: Global train loss: 1.788, Global test loss: 2.275, Global test accuracy: 53.24
Round   4, Train loss: 1.845, Test loss: 1.979, Test accuracy: 53.53
Round   4: Global train loss: 1.845, Global test loss: 2.249, Global test accuracy: 55.53
Round   5, Train loss: 1.790, Test loss: 1.949, Test accuracy: 56.98
Round   5: Global train loss: 1.790, Global test loss: 2.209, Global test accuracy: 55.95
Round   6, Train loss: 1.632, Test loss: 1.895, Test accuracy: 59.24
Round   6: Global train loss: 1.632, Global test loss: 2.144, Global test accuracy: 56.55
Round   7, Train loss: 1.520, Test loss: 1.867, Test accuracy: 60.34
Round   7: Global train loss: 1.520, Global test loss: 2.082, Global test accuracy: 57.12
Round   8, Train loss: 1.552, Test loss: 1.837, Test accuracy: 63.31
Round   8: Global train loss: 1.552, Global test loss: 2.024, Global test accuracy: 57.65
Round   9, Train loss: 1.480, Test loss: 1.831, Test accuracy: 63.65
Round   9: Global train loss: 1.480, Global test loss: 1.982, Global test accuracy: 58.13
Round  10, Train loss: 1.772, Test loss: 1.819, Test accuracy: 65.19
Round  10: Global train loss: 1.772, Global test loss: 1.947, Global test accuracy: 58.91
Round  11, Train loss: 1.648, Test loss: 1.782, Test accuracy: 69.42
Round  11: Global train loss: 1.648, Global test loss: 1.926, Global test accuracy: 60.47
Round  12, Train loss: 1.617, Test loss: 1.738, Test accuracy: 74.27
Round  12: Global train loss: 1.617, Global test loss: 1.904, Global test accuracy: 61.91
Round  13, Train loss: 1.562, Test loss: 1.744, Test accuracy: 73.06
Round  13: Global train loss: 1.562, Global test loss: 1.886, Global test accuracy: 63.28
Round  14, Train loss: 1.402, Test loss: 1.726, Test accuracy: 74.69
Round  14: Global train loss: 1.402, Global test loss: 1.868, Global test accuracy: 64.00
Round  15, Train loss: 1.264, Test loss: 1.745, Test accuracy: 72.61
Round  15: Global train loss: 1.264, Global test loss: 1.857, Global test accuracy: 64.48
Round  16, Train loss: 1.475, Test loss: 1.712, Test accuracy: 75.86
Round  16: Global train loss: 1.475, Global test loss: 1.847, Global test accuracy: 64.88
Round  17, Train loss: 1.262, Test loss: 1.696, Test accuracy: 77.49
Round  17: Global train loss: 1.262, Global test loss: 1.836, Global test accuracy: 65.12
Round  18, Train loss: 1.263, Test loss: 1.707, Test accuracy: 76.20
Round  18: Global train loss: 1.263, Global test loss: 1.827, Global test accuracy: 65.40
Round  19, Train loss: 1.234, Test loss: 1.698, Test accuracy: 77.15
Round  19: Global train loss: 1.234, Global test loss: 1.821, Global test accuracy: 65.47
Round  20, Train loss: 0.935, Test loss: 1.675, Test accuracy: 79.36
Round  20: Global train loss: 0.935, Global test loss: 1.813, Global test accuracy: 65.86
Round  21, Train loss: 1.254, Test loss: 1.673, Test accuracy: 79.45
Round  21: Global train loss: 1.254, Global test loss: 1.803, Global test accuracy: 67.01
Round  22, Train loss: 0.930, Test loss: 1.668, Test accuracy: 79.89
Round  22: Global train loss: 0.930, Global test loss: 1.796, Global test accuracy: 67.64
Round  23, Train loss: 1.254, Test loss: 1.662, Test accuracy: 80.44
Round  23: Global train loss: 1.254, Global test loss: 1.791, Global test accuracy: 68.61
Round  24, Train loss: 0.610, Test loss: 1.639, Test accuracy: 82.64
Round  24: Global train loss: 0.610, Global test loss: 1.782, Global test accuracy: 69.38
Round  25, Train loss: 0.950, Test loss: 1.633, Test accuracy: 83.22
Round  25: Global train loss: 0.950, Global test loss: 1.777, Global test accuracy: 69.82
Round  26, Train loss: 1.036, Test loss: 1.643, Test accuracy: 82.07
Round  26: Global train loss: 1.036, Global test loss: 1.771, Global test accuracy: 70.47
Round  27, Train loss: 1.048, Test loss: 1.656, Test accuracy: 80.94
Round  27: Global train loss: 1.048, Global test loss: 1.769, Global test accuracy: 70.73
Round  28, Train loss: 0.900, Test loss: 1.645, Test accuracy: 82.03
Round  28: Global train loss: 0.900, Global test loss: 1.763, Global test accuracy: 71.31
Round  29, Train loss: 0.968, Test loss: 1.647, Test accuracy: 81.81
Round  29: Global train loss: 0.968, Global test loss: 1.759, Global test accuracy: 71.94
Round  30, Train loss: 0.831, Test loss: 1.653, Test accuracy: 81.18
Round  30: Global train loss: 0.831, Global test loss: 1.755, Global test accuracy: 72.11
Round  31, Train loss: 0.880, Test loss: 1.641, Test accuracy: 82.31
Round  31: Global train loss: 0.880, Global test loss: 1.749, Global test accuracy: 72.54
Round  32, Train loss: 0.683, Test loss: 1.626, Test accuracy: 83.88
Round  32: Global train loss: 0.683, Global test loss: 1.744, Global test accuracy: 72.74
Round  33, Train loss: 0.819, Test loss: 1.619, Test accuracy: 84.49
Round  33: Global train loss: 0.819, Global test loss: 1.739, Global test accuracy: 73.00
Round  34, Train loss: 0.622, Test loss: 1.611, Test accuracy: 85.23
Round  34: Global train loss: 0.622, Global test loss: 1.732, Global test accuracy: 73.52
Round  35, Train loss: 0.578, Test loss: 1.599, Test accuracy: 86.41
Round  35: Global train loss: 0.578, Global test loss: 1.723, Global test accuracy: 74.61
Round  36, Train loss: 0.733, Test loss: 1.593, Test accuracy: 87.03
Round  36: Global train loss: 0.733, Global test loss: 1.711, Global test accuracy: 76.06
Round  37, Train loss: 0.754, Test loss: 1.590, Test accuracy: 87.42
Round  37: Global train loss: 0.754, Global test loss: 1.700, Global test accuracy: 77.56
Round  38, Train loss: 0.673, Test loss: 1.583, Test accuracy: 88.14
Round  38: Global train loss: 0.673, Global test loss: 1.683, Global test accuracy: 79.55
Round  39, Train loss: 0.631, Test loss: 1.578, Test accuracy: 88.60
Round  39: Global train loss: 0.631, Global test loss: 1.666, Global test accuracy: 81.45
Round  40, Train loss: 0.745, Test loss: 1.578, Test accuracy: 88.56
Round  40: Global train loss: 0.745, Global test loss: 1.662, Global test accuracy: 81.88
Round  41, Train loss: 0.641, Test loss: 1.576, Test accuracy: 88.75
Round  41: Global train loss: 0.641, Global test loss: 1.656, Global test accuracy: 82.39
Round  42, Train loss: 0.457, Test loss: 1.570, Test accuracy: 89.36
Round  42: Global train loss: 0.457, Global test loss: 1.643, Global test accuracy: 83.64
Round  43, Train loss: 0.478, Test loss: 1.569, Test accuracy: 89.36
Round  43: Global train loss: 0.478, Global test loss: 1.638, Global test accuracy: 84.31
Round  44, Train loss: 0.626, Test loss: 1.568, Test accuracy: 89.47
Round  44: Global train loss: 0.626, Global test loss: 1.628, Global test accuracy: 85.36
Round  45, Train loss: 0.435, Test loss: 1.568, Test accuracy: 89.53
Round  45: Global train loss: 0.435, Global test loss: 1.619, Global test accuracy: 86.23
Round  46, Train loss: 0.483, Test loss: 1.567, Test accuracy: 89.60
Round  46: Global train loss: 0.483, Global test loss: 1.616, Global test accuracy: 86.43
Round  47, Train loss: 0.538, Test loss: 1.566, Test accuracy: 89.70
Round  47: Global train loss: 0.538, Global test loss: 1.604, Global test accuracy: 87.53
Round  48, Train loss: 0.385, Test loss: 1.563, Test accuracy: 90.01
Round  48: Global train loss: 0.385, Global test loss: 1.595, Global test accuracy: 88.21
Round  49, Train loss: 0.674, Test loss: 1.561, Test accuracy: 90.18
Round  49: Global train loss: 0.674, Global test loss: 1.589, Global test accuracy: 88.92
Round  50, Train loss: 0.432, Test loss: 1.559, Test accuracy: 90.47
Round  50: Global train loss: 0.432, Global test loss: 1.583, Global test accuracy: 89.47
Round  51, Train loss: 0.503, Test loss: 1.558, Test accuracy: 90.50
Round  51: Global train loss: 0.503, Global test loss: 1.577, Global test accuracy: 89.95
Round  52, Train loss: 0.662, Test loss: 1.561, Test accuracy: 90.20
Round  52: Global train loss: 0.662, Global test loss: 1.576, Global test accuracy: 90.04
Round  53, Train loss: 0.452, Test loss: 1.562, Test accuracy: 90.21
Round  53: Global train loss: 0.452, Global test loss: 1.574, Global test accuracy: 90.36
Round  54, Train loss: 0.565, Test loss: 1.564, Test accuracy: 89.97
Round  54: Global train loss: 0.565, Global test loss: 1.577, Global test accuracy: 89.89
Round  55, Train loss: 0.480, Test loss: 1.562, Test accuracy: 90.14
Round  55: Global train loss: 0.480, Global test loss: 1.576, Global test accuracy: 90.06
Round  56, Train loss: 0.568, Test loss: 1.561, Test accuracy: 90.19
Round  56: Global train loss: 0.568, Global test loss: 1.572, Global test accuracy: 90.43
Round  57, Train loss: 0.614, Test loss: 1.559, Test accuracy: 90.43
Round  57: Global train loss: 0.614, Global test loss: 1.570, Global test accuracy: 90.74
Round  58, Train loss: 0.449, Test loss: 1.558, Test accuracy: 90.48
Round  58: Global train loss: 0.449, Global test loss: 1.566, Global test accuracy: 91.09
Round  59, Train loss: 0.454, Test loss: 1.559, Test accuracy: 90.43
Round  59: Global train loss: 0.454, Global test loss: 1.564, Global test accuracy: 91.23
Round  60, Train loss: 0.736, Test loss: 1.557, Test accuracy: 90.53
Round  60: Global train loss: 0.736, Global test loss: 1.564, Global test accuracy: 91.20
Round  61, Train loss: 0.451, Test loss: 1.559, Test accuracy: 90.44
Round  61: Global train loss: 0.451, Global test loss: 1.563, Global test accuracy: 91.23
Round  62, Train loss: 0.615, Test loss: 1.556, Test accuracy: 90.67
Round  62: Global train loss: 0.615, Global test loss: 1.562, Global test accuracy: 91.28
Round  63, Train loss: 0.426, Test loss: 1.555, Test accuracy: 90.87
Round  63: Global train loss: 0.426, Global test loss: 1.558, Global test accuracy: 91.48
Round  64, Train loss: 0.407, Test loss: 1.556, Test accuracy: 90.72
Round  64: Global train loss: 0.407, Global test loss: 1.557, Global test accuracy: 91.48
Round  65, Train loss: 0.490, Test loss: 1.558, Test accuracy: 90.44
Round  65: Global train loss: 0.490, Global test loss: 1.557, Global test accuracy: 91.50
Round  66, Train loss: 0.601, Test loss: 1.558, Test accuracy: 90.47
Round  66: Global train loss: 0.601, Global test loss: 1.557, Global test accuracy: 91.51
Round  67, Train loss: 0.519, Test loss: 1.557, Test accuracy: 90.55
Round  67: Global train loss: 0.519, Global test loss: 1.556, Global test accuracy: 91.63
Round  68, Train loss: 0.577, Test loss: 1.554, Test accuracy: 90.86
Round  68: Global train loss: 0.577, Global test loss: 1.555, Global test accuracy: 91.79
Round  69, Train loss: 0.319, Test loss: 1.553, Test accuracy: 90.90
Round  69: Global train loss: 0.319, Global test loss: 1.554, Global test accuracy: 91.88
Round  70, Train loss: 0.407, Test loss: 1.554, Test accuracy: 90.87
Round  70: Global train loss: 0.407, Global test loss: 1.552, Global test accuracy: 91.87
Round  71, Train loss: 0.494, Test loss: 1.553, Test accuracy: 90.97
Round  71: Global train loss: 0.494, Global test loss: 1.551, Global test accuracy: 91.96
Round  72, Train loss: 0.584, Test loss: 1.554, Test accuracy: 90.86
Round  72: Global train loss: 0.584, Global test loss: 1.551, Global test accuracy: 91.98
Round  73, Train loss: 0.474, Test loss: 1.553, Test accuracy: 90.99
Round  73: Global train loss: 0.474, Global test loss: 1.550, Global test accuracy: 91.98
Round  74, Train loss: 0.499, Test loss: 1.553, Test accuracy: 91.01
Round  74: Global train loss: 0.499, Global test loss: 1.549, Global test accuracy: 92.06
Round  75, Train loss: 0.503, Test loss: 1.554, Test accuracy: 90.95
Round  75: Global train loss: 0.503, Global test loss: 1.549, Global test accuracy: 92.08
Round  76, Train loss: 0.541, Test loss: 1.553, Test accuracy: 91.05
Round  76: Global train loss: 0.541, Global test loss: 1.548, Global test accuracy: 92.22
Round  77, Train loss: 0.495, Test loss: 1.553, Test accuracy: 91.08
Round  77: Global train loss: 0.495, Global test loss: 1.547, Global test accuracy: 92.33
Round  78, Train loss: 0.415, Test loss: 1.550, Test accuracy: 91.29
Round  78: Global train loss: 0.415, Global test loss: 1.546, Global test accuracy: 92.36
Round  79, Train loss: 0.609, Test loss: 1.550, Test accuracy: 91.32
Round  79: Global train loss: 0.609, Global test loss: 1.545, Global test accuracy: 92.40
Round  80, Train loss: 0.390, Test loss: 1.551, Test accuracy: 91.20
Round  80: Global train loss: 0.390, Global test loss: 1.545, Global test accuracy: 92.48
Round  81, Train loss: 0.712, Test loss: 1.551, Test accuracy: 91.26
Round  81: Global train loss: 0.712, Global test loss: 1.545, Global test accuracy: 92.50
Round  82, Train loss: 0.544, Test loss: 1.550, Test accuracy: 91.31
Round  82: Global train loss: 0.544, Global test loss: 1.544, Global test accuracy: 92.58
Round  83, Train loss: 0.553, Test loss: 1.550, Test accuracy: 91.30
Round  83: Global train loss: 0.553, Global test loss: 1.543, Global test accuracy: 92.60
Round  84, Train loss: 0.254, Test loss: 1.549, Test accuracy: 91.42
Round  84: Global train loss: 0.254, Global test loss: 1.542, Global test accuracy: 92.70
Round  85, Train loss: 0.645, Test loss: 1.550, Test accuracy: 91.25
Round  85: Global train loss: 0.645, Global test loss: 1.542, Global test accuracy: 92.71
Round  86, Train loss: 0.476, Test loss: 1.550, Test accuracy: 91.34
Round  86: Global train loss: 0.476, Global test loss: 1.542, Global test accuracy: 92.76
Round  87, Train loss: 0.374, Test loss: 1.551, Test accuracy: 91.21
Round  87: Global train loss: 0.374, Global test loss: 1.541, Global test accuracy: 92.77
Round  88, Train loss: 0.571, Test loss: 1.552, Test accuracy: 91.19
Round  88: Global train loss: 0.571, Global test loss: 1.541, Global test accuracy: 92.74
Round  89, Train loss: 0.671, Test loss: 1.552, Test accuracy: 91.08
Round  89: Global train loss: 0.671, Global test loss: 1.541, Global test accuracy: 92.69
Round  90, Train loss: 0.500, Test loss: 1.552, Test accuracy: 91.08
Round  90: Global train loss: 0.500, Global test loss: 1.541, Global test accuracy: 92.73
Round  91, Train loss: 0.650, Test loss: 1.551, Test accuracy: 91.18
Round  91: Global train loss: 0.650, Global test loss: 1.540, Global test accuracy: 92.80
Round  92, Train loss: 0.585, Test loss: 1.551, Test accuracy: 91.17
Round  92: Global train loss: 0.585, Global test loss: 1.540, Global test accuracy: 92.81
Round  93, Train loss: 0.792, Test loss: 1.551, Test accuracy: 91.14
Round  93: Global train loss: 0.792, Global test loss: 1.540, Global test accuracy: 92.87
Round  94, Train loss: 0.569, Test loss: 1.551, Test accuracy: 91.24
Round  94: Global train loss: 0.569, Global test loss: 1.539, Global test accuracy: 92.82
Round  95, Train loss: 0.571, Test loss: 1.549, Test accuracy: 91.42
Round  95: Global train loss: 0.571, Global test loss: 1.538, Global test accuracy: 93.00
Round  96, Train loss: 0.572, Test loss: 1.550, Test accuracy: 91.34
Round  96: Global train loss: 0.572, Global test loss: 1.538, Global test accuracy: 92.97
Round  97, Train loss: 0.506, Test loss: 1.550, Test accuracy: 91.29
Round  97: Global train loss: 0.506, Global test loss: 1.538, Global test accuracy: 93.02
Round  98, Train loss: 0.675, Test loss: 1.548, Test accuracy: 91.47
Round  98: Global train loss: 0.675, Global test loss: 1.537, Global test accuracy: 93.02
Round  99, Train loss: 0.499, Test loss: 1.549, Test accuracy: 91.42
Round  99: Global train loss: 0.499, Global test loss: 1.537, Global test accuracy: 92.99/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round: Train loss: 1.506, Test loss: 1.537, Test accuracy: 92.86
Final Round: Global train loss: 1.506, Global test loss: 1.536, Global test accuracy: 93.11
Average accuracy final 10 rounds: 91.27525
Average global accuracy final 10 rounds: 92.90375
4429.828166246414
[]
[25.14, 34.2675, 41.015, 51.495, 53.5275, 56.985, 59.245, 60.3425, 63.31, 63.6475, 65.195, 69.4225, 74.265, 73.0625, 74.6925, 72.615, 75.86, 77.4875, 76.2, 77.1475, 79.355, 79.4525, 79.8875, 80.445, 82.6375, 83.215, 82.07, 80.935, 82.03, 81.815, 81.1775, 82.31, 83.8825, 84.4925, 85.23, 86.4125, 87.025, 87.4225, 88.1375, 88.6, 88.565, 88.745, 89.355, 89.365, 89.47, 89.535, 89.6, 89.6975, 90.0125, 90.1775, 90.4675, 90.5, 90.1975, 90.2075, 89.97, 90.135, 90.1925, 90.43, 90.4775, 90.43, 90.535, 90.445, 90.675, 90.8675, 90.7225, 90.435, 90.4725, 90.5475, 90.86, 90.9, 90.8675, 90.97, 90.86, 90.9925, 91.01, 90.9475, 91.0475, 91.0825, 91.2875, 91.3225, 91.205, 91.2575, 91.3125, 91.3, 91.4175, 91.255, 91.345, 91.2075, 91.1875, 91.0775, 91.0775, 91.1775, 91.1725, 91.1375, 91.2425, 91.42, 91.345, 91.29, 91.47, 91.42, 92.855]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.302, Test accuracy: 7.87 

Round   0, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 7.86 

Round   1, Train loss: 2.303, Test loss: 2.302, Test accuracy: 7.88 

Round   1, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 7.91 

Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 7.93 

Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.00 

Round   3, Train loss: 2.303, Test loss: 2.302, Test accuracy: 7.98 

Round   3, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 7.99 

Round   4, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.02 

Round   4, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.02 

Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.06 

Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.11 

Round   6, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.07 

Round   6, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.14 

Round   7, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.11 

Round   7, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.15 

Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.16 

Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.19 

Round   9, Train loss: 2.303, Test loss: 2.302, Test accuracy: 8.16 

Round   9, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 8.20 

Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.19 

Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.20 

Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.23 

Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.23 

Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.25 

Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.30 

Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.26 

Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.29 

Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.27 

Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.34 

Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.29 

Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.36 

Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.30 

Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.38 

Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.34 

Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.44 

Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.37 

Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.42 

Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.38 

Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.41 

Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.39 

Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.41 

Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.43 

Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.45 

Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.48 

Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.49 

Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.50 

Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.54 

Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.52 

Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.59 

Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.55 

Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.65 

Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.62 

Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.71 

Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.69 

Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.75 

Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.73 

Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.84 

Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.78 

Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.93 

Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.88 

Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.99 

Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.90 

Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.11 

Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.98 

Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.17 

Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.03 

Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.19 

Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.08 

Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.21 

Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.11 

Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.27 

Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.21 

Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.31 

Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.28 

Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.36 

Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.30 

Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.44 

Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.37 

Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.40 

Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.40 

Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49 

Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.42 

Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.52 

Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47 

Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.61 

Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51 

Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.70 

Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.58 

Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.73 

Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.64 

Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.83 

Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.71 

Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.78 

Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.75 

Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.80 

Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.76 

Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.90 

Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.80 

Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.94 

Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.86 

Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.98 

Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.89 

Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.04 

Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.96 

Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.04 

Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.96 

Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.12 

Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.03 

Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.16 

Round  55, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.04 

Round  55, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 10.16 

Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.10 

Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.22 

Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.16 

Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.25 

Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.22 

Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 10.30 

Round  59, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.27 

Round  59, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.38 

Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.30 

Round  60, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.46 

Round  61, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.36 

Round  61, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.51 

Round  62, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.39 

Round  62, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.55 

Round  63, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.44 

Round  63, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.60 

Round  64, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.50 

Round  64, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.62 

Round  65, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.54 

Round  65, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.68 

Round  66, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.60 

Round  66, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.75 

Round  67, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.66 

Round  67, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 10.79 

Round  68, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.70 

Round  68, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.83 

Round  69, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.75 

Round  69, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.86 

Round  70, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.83 

Round  70, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 10.92 

Round  71, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.91 

Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.01 

Round  72, Train loss: 2.301, Test loss: 2.301, Test accuracy: 10.95 

Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.04 

Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.00 

Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.11 

Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.06 

Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.14 

Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.10 

Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.19 

Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.15 

Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.23 

Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.21 

Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.29 

Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.26 

Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.40 

Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.32 

Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.46 

Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.37 

Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.56 

Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.42 

Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.62 

Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.53 

Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.70 

Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.62 

Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.79 

Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.65 

Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.85 

Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.77 

Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.01 

Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.84 

Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.08 

Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.90 

Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.13 

Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.98 

Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.30 

Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.11 

Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.45 

Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.27 

Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.53 

Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.38 

Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.53 

Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.44 

Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.60 

Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.50 

Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.73 

Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.59 

Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.79 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.72 

Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.93 

Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.80 

Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.07 

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.91 

Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.23 

Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.02 

Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.34 

Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.17 

Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.46 

Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.74 

Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.46 

Average accuracy final 10 rounds: 12.6785 

Average global accuracy final 10 rounds: 12.919 

2275.320079803467
[0.9134328365325928, 1.7476811408996582, 2.587533712387085, 3.411050796508789, 4.2901036739349365, 5.191343069076538, 6.09203028678894, 6.999796628952026, 7.921382188796997, 8.837188243865967, 9.741232633590698, 10.646501064300537, 11.560506343841553, 12.465511322021484, 13.36952018737793, 14.275283575057983, 15.181607246398926, 16.086124897003174, 16.99398183822632, 17.90315294265747, 18.808520555496216, 19.710662126541138, 20.607545137405396, 21.506170511245728, 22.413264274597168, 23.316511631011963, 24.220834970474243, 25.12795376777649, 26.03243923187256, 26.932573556900024, 27.837576866149902, 28.75550413131714, 29.662826776504517, 30.570371627807617, 31.482927083969116, 32.39054870605469, 33.302438497543335, 34.216771364212036, 35.14006209373474, 36.051769733428955, 36.96550130844116, 37.87518930435181, 38.787352323532104, 39.71214485168457, 40.620850801467896, 41.52826809883118, 42.43519425392151, 43.34361743927002, 44.25290036201477, 45.159560680389404, 46.062899112701416, 46.969064474105835, 47.87455606460571, 48.779587745666504, 49.68964219093323, 50.590582609176636, 51.48631548881531, 52.38737416267395, 53.12771487236023, 53.87502717971802, 54.62926506996155, 55.383922815322876, 56.138771295547485, 56.88701391220093, 57.64196228981018, 58.40816068649292, 59.154813051223755, 59.90068435668945, 60.65381073951721, 61.41166067123413, 62.178465604782104, 62.936930656433105, 63.68416976928711, 64.43293404579163, 65.19590759277344, 65.96407771110535, 66.71097135543823, 67.46193242073059, 68.208078622818, 68.97377848625183, 69.74060893058777, 70.48699164390564, 71.23570799827576, 71.981605052948, 72.74700379371643, 73.51049327850342, 74.25667428970337, 75.00178265571594, 75.74745297431946, 76.50780463218689, 77.2615692615509, 78.00941610336304, 78.75501871109009, 79.49725389480591, 80.26068162918091, 81.00672793388367, 81.74063634872437, 82.47735071182251, 83.22216749191284, 83.97906565666199, 85.52653479576111]
[7.865, 7.885, 7.9325, 7.98, 8.0225, 8.06, 8.0725, 8.1075, 8.1575, 8.1625, 8.19, 8.2275, 8.2475, 8.2575, 8.265, 8.2875, 8.2975, 8.3425, 8.37, 8.3775, 8.3925, 8.4325, 8.4825, 8.5025, 8.515, 8.555, 8.625, 8.69, 8.7325, 8.78, 8.8775, 8.9025, 8.9775, 9.025, 9.0775, 9.105, 9.2125, 9.2825, 9.3, 9.37, 9.3975, 9.4225, 9.475, 9.505, 9.58, 9.64, 9.7075, 9.7525, 9.755, 9.805, 9.8625, 9.8925, 9.96, 9.965, 10.025, 10.0425, 10.1025, 10.165, 10.2225, 10.27, 10.2975, 10.3625, 10.3875, 10.4375, 10.4975, 10.5425, 10.5975, 10.665, 10.7025, 10.75, 10.8325, 10.9125, 10.9475, 11.0025, 11.0625, 11.1, 11.155, 11.205, 11.255, 11.325, 11.3725, 11.4175, 11.525, 11.615, 11.655, 11.77, 11.84, 11.9025, 11.9825, 12.105, 12.265, 12.3775, 12.4375, 12.5, 12.5875, 12.72, 12.805, 12.9075, 13.0175, 13.1675, 13.7375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 14.06
Round   1, Train loss: 2.298, Test loss: 2.293, Test accuracy: 30.95
Round   2, Train loss: 2.293, Test loss: 2.280, Test accuracy: 50.11
Round   3, Train loss: 2.278, Test loss: 2.217, Test accuracy: 42.68
Round   4, Train loss: 2.214, Test loss: 1.977, Test accuracy: 55.26
Round   5, Train loss: 1.999, Test loss: 1.805, Test accuracy: 71.19
Round   6, Train loss: 1.820, Test loss: 1.735, Test accuracy: 75.67
Round   7, Train loss: 1.804, Test loss: 1.680, Test accuracy: 81.38
Round   8, Train loss: 1.752, Test loss: 1.660, Test accuracy: 82.20
Round   9, Train loss: 1.696, Test loss: 1.649, Test accuracy: 82.66
Round  10, Train loss: 1.663, Test loss: 1.641, Test accuracy: 83.18
Round  11, Train loss: 1.671, Test loss: 1.635, Test accuracy: 83.51
Round  12, Train loss: 1.655, Test loss: 1.628, Test accuracy: 83.81
Round  13, Train loss: 1.623, Test loss: 1.593, Test accuracy: 88.48
Round  14, Train loss: 1.609, Test loss: 1.583, Test accuracy: 88.92
Round  15, Train loss: 1.577, Test loss: 1.575, Test accuracy: 89.73
Round  16, Train loss: 1.566, Test loss: 1.572, Test accuracy: 89.86
Round  17, Train loss: 1.562, Test loss: 1.568, Test accuracy: 90.10
Round  18, Train loss: 1.547, Test loss: 1.564, Test accuracy: 90.50
Round  19, Train loss: 1.540, Test loss: 1.563, Test accuracy: 90.65
Round  20, Train loss: 1.549, Test loss: 1.561, Test accuracy: 90.74
Round  21, Train loss: 1.536, Test loss: 1.559, Test accuracy: 90.84
Round  22, Train loss: 1.546, Test loss: 1.556, Test accuracy: 91.17
Round  23, Train loss: 1.540, Test loss: 1.555, Test accuracy: 91.19
Round  24, Train loss: 1.530, Test loss: 1.553, Test accuracy: 91.23
Round  25, Train loss: 1.523, Test loss: 1.552, Test accuracy: 91.27
Round  26, Train loss: 1.522, Test loss: 1.551, Test accuracy: 91.44
Round  27, Train loss: 1.522, Test loss: 1.550, Test accuracy: 91.49
Round  28, Train loss: 1.518, Test loss: 1.551, Test accuracy: 91.39
Round  29, Train loss: 1.513, Test loss: 1.549, Test accuracy: 91.62
Round  30, Train loss: 1.527, Test loss: 1.548, Test accuracy: 91.69
Round  31, Train loss: 1.513, Test loss: 1.547, Test accuracy: 91.71
Round  32, Train loss: 1.509, Test loss: 1.547, Test accuracy: 91.88
Round  33, Train loss: 1.517, Test loss: 1.547, Test accuracy: 91.88
Round  34, Train loss: 1.517, Test loss: 1.544, Test accuracy: 92.02
Round  35, Train loss: 1.511, Test loss: 1.544, Test accuracy: 91.99
Round  36, Train loss: 1.508, Test loss: 1.543, Test accuracy: 92.14
Round  37, Train loss: 1.518, Test loss: 1.542, Test accuracy: 92.29
Round  38, Train loss: 1.507, Test loss: 1.542, Test accuracy: 92.28
Round  39, Train loss: 1.512, Test loss: 1.542, Test accuracy: 92.22
Round  40, Train loss: 1.509, Test loss: 1.541, Test accuracy: 92.22
Round  41, Train loss: 1.507, Test loss: 1.540, Test accuracy: 92.48
Round  42, Train loss: 1.507, Test loss: 1.539, Test accuracy: 92.56
Round  43, Train loss: 1.510, Test loss: 1.538, Test accuracy: 92.66
Round  44, Train loss: 1.498, Test loss: 1.538, Test accuracy: 92.64
Round  45, Train loss: 1.506, Test loss: 1.539, Test accuracy: 92.61
Round  46, Train loss: 1.505, Test loss: 1.537, Test accuracy: 92.67
Round  47, Train loss: 1.507, Test loss: 1.538, Test accuracy: 92.61
Round  48, Train loss: 1.500, Test loss: 1.537, Test accuracy: 92.77
Round  49, Train loss: 1.497, Test loss: 1.536, Test accuracy: 92.81
Round  50, Train loss: 1.499, Test loss: 1.535, Test accuracy: 92.82
Round  51, Train loss: 1.497, Test loss: 1.535, Test accuracy: 92.82
Round  52, Train loss: 1.499, Test loss: 1.535, Test accuracy: 92.98
Round  53, Train loss: 1.497, Test loss: 1.534, Test accuracy: 92.91
Round  54, Train loss: 1.499, Test loss: 1.534, Test accuracy: 93.14
Round  55, Train loss: 1.492, Test loss: 1.534, Test accuracy: 92.96
Round  56, Train loss: 1.497, Test loss: 1.534, Test accuracy: 92.95
Round  57, Train loss: 1.495, Test loss: 1.533, Test accuracy: 93.16
Round  58, Train loss: 1.503, Test loss: 1.533, Test accuracy: 92.97
Round  59, Train loss: 1.501, Test loss: 1.533, Test accuracy: 93.08
Round  60, Train loss: 1.492, Test loss: 1.531, Test accuracy: 93.28
Round  61, Train loss: 1.498, Test loss: 1.531, Test accuracy: 93.29
Round  62, Train loss: 1.495, Test loss: 1.531, Test accuracy: 93.34
Round  63, Train loss: 1.495, Test loss: 1.531, Test accuracy: 93.36
Round  64, Train loss: 1.493, Test loss: 1.530, Test accuracy: 93.55
Round  65, Train loss: 1.492, Test loss: 1.530, Test accuracy: 93.47
Round  66, Train loss: 1.497, Test loss: 1.529, Test accuracy: 93.51
Round  67, Train loss: 1.496, Test loss: 1.530, Test accuracy: 93.39
Round  68, Train loss: 1.493, Test loss: 1.529, Test accuracy: 93.44
Round  69, Train loss: 1.495, Test loss: 1.529, Test accuracy: 93.42
Round  70, Train loss: 1.493, Test loss: 1.530, Test accuracy: 93.44
Round  71, Train loss: 1.488, Test loss: 1.528, Test accuracy: 93.44
Round  72, Train loss: 1.490, Test loss: 1.528, Test accuracy: 93.53
Round  73, Train loss: 1.488, Test loss: 1.528, Test accuracy: 93.43
Round  74, Train loss: 1.488, Test loss: 1.529, Test accuracy: 93.51
Round  75, Train loss: 1.487, Test loss: 1.526, Test accuracy: 93.82
Round  76, Train loss: 1.490, Test loss: 1.526, Test accuracy: 93.76
Round  77, Train loss: 1.487, Test loss: 1.527, Test accuracy: 93.64
Round  78, Train loss: 1.483, Test loss: 1.527, Test accuracy: 93.69
Round  79, Train loss: 1.491, Test loss: 1.526, Test accuracy: 93.77
Round  80, Train loss: 1.487, Test loss: 1.526, Test accuracy: 93.86
Round  81, Train loss: 1.485, Test loss: 1.525, Test accuracy: 93.82
Round  82, Train loss: 1.488, Test loss: 1.526, Test accuracy: 93.80
Round  83, Train loss: 1.486, Test loss: 1.525, Test accuracy: 93.83
Round  84, Train loss: 1.488, Test loss: 1.526, Test accuracy: 93.73
Round  85, Train loss: 1.484, Test loss: 1.525, Test accuracy: 93.82
Round  86, Train loss: 1.483, Test loss: 1.525, Test accuracy: 93.81
Round  87, Train loss: 1.490, Test loss: 1.525, Test accuracy: 93.83
Round  88, Train loss: 1.486, Test loss: 1.526, Test accuracy: 93.75
Round  89, Train loss: 1.490, Test loss: 1.526, Test accuracy: 93.71
Round  90, Train loss: 1.487, Test loss: 1.525, Test accuracy: 93.96
Round  91, Train loss: 1.492, Test loss: 1.524, Test accuracy: 93.91
Round  92, Train loss: 1.491, Test loss: 1.524, Test accuracy: 94.01
Round  93, Train loss: 1.483, Test loss: 1.523, Test accuracy: 94.04
Round  94, Train loss: 1.486, Test loss: 1.524, Test accuracy: 93.89
Round  95, Train loss: 1.485, Test loss: 1.524, Test accuracy: 93.95
Round  96, Train loss: 1.492, Test loss: 1.524, Test accuracy: 93.90
Round  97, Train loss: 1.488, Test loss: 1.524, Test accuracy: 93.91
Round  98, Train loss: 1.489, Test loss: 1.524, Test accuracy: 94.01
Round  99, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.11
Final Round, Train loss: 1.485, Test loss: 1.522, Test accuracy: 94.14
Average accuracy final 10 rounds: 93.969
2052.7701041698456
[2.2808620929718018, 4.409780263900757, 6.561227083206177, 8.732640504837036, 10.88418436050415, 13.015766143798828, 15.150876760482788, 17.323215007781982, 19.491358518600464, 21.63590908050537, 23.78477382659912, 25.918298721313477, 28.055388689041138, 30.180953979492188, 32.3193416595459, 34.45726251602173, 36.59326410293579, 38.72878575325012, 40.87490677833557, 43.00999426841736, 45.155044078826904, 47.292842626571655, 49.42798686027527, 51.5675528049469, 53.70269465446472, 55.831777572631836, 57.969139099121094, 60.11099457740784, 62.24078035354614, 64.37633681297302, 66.45078825950623, 68.29582214355469, 70.13741970062256, 71.9771134853363, 73.80268979072571, 75.64000201225281, 77.48067593574524, 79.31355476379395, 81.15194487571716, 82.98586416244507, 84.82346034049988, 86.65927910804749, 88.49111723899841, 90.32349252700806, 92.15772676467896, 93.98655390739441, 95.82457327842712, 97.65217590332031, 99.48626899719238, 101.31134176254272, 103.13768672943115, 104.97690939903259, 106.81999659538269, 108.65633034706116, 110.48422122001648, 112.32831454277039, 114.16723322868347, 116.01400637626648, 117.8618392944336, 119.69548177719116, 121.53446650505066, 123.38179898262024, 125.22391080856323, 127.0563063621521, 128.89796423912048, 130.73501229286194, 132.57314538955688, 134.41243600845337, 136.25868725776672, 138.0949215888977, 139.93013405799866, 141.76884245872498, 143.6058738231659, 145.45280003547668, 147.29109358787537, 149.11731481552124, 150.94949173927307, 152.78230094909668, 154.6149446964264, 156.44755625724792, 158.27483129501343, 160.10100984573364, 161.93067336082458, 163.7604341506958, 165.58873844146729, 167.41569566726685, 169.24274325370789, 171.0765540599823, 172.90776824951172, 174.743590593338, 176.56535124778748, 178.39224863052368, 180.20868372917175, 182.0214343070984, 183.8488199710846, 185.71078658103943, 187.5690951347351, 189.43874955177307, 191.2924301624298, 193.15022253990173, 195.02098560333252]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[14.0575, 30.9525, 50.11, 42.6825, 55.2575, 71.195, 75.6675, 81.38, 82.2025, 82.66, 83.1775, 83.51, 83.805, 88.48, 88.915, 89.735, 89.86, 90.1, 90.495, 90.6475, 90.7425, 90.8375, 91.1725, 91.195, 91.2325, 91.265, 91.445, 91.49, 91.385, 91.625, 91.69, 91.7125, 91.875, 91.8775, 92.02, 91.9875, 92.1425, 92.2875, 92.28, 92.215, 92.225, 92.48, 92.565, 92.6625, 92.6375, 92.6075, 92.6725, 92.61, 92.765, 92.805, 92.82, 92.8225, 92.9775, 92.9075, 93.1425, 92.9575, 92.955, 93.1625, 92.9675, 93.0775, 93.285, 93.29, 93.3425, 93.3625, 93.545, 93.47, 93.5125, 93.3925, 93.445, 93.425, 93.44, 93.44, 93.53, 93.4325, 93.51, 93.8225, 93.76, 93.6375, 93.6925, 93.765, 93.86, 93.8225, 93.795, 93.8275, 93.735, 93.8175, 93.81, 93.8275, 93.7525, 93.7075, 93.9625, 93.905, 94.0125, 94.0425, 93.895, 93.95, 93.9025, 93.9075, 94.0075, 94.105, 94.14]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.302, Test accuracy: 10.71
Round   1, Train loss: 2.303, Test loss: 2.302, Test accuracy: 12.05
Round   2, Train loss: 2.303, Test loss: 2.302, Test accuracy: 13.19
Round   3, Train loss: 2.303, Test loss: 2.302, Test accuracy: 14.21
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.52
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.77
Round   6, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.16
Round   7, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.10
Round   8, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.19
Round   9, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.88
Round  10, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.58
Round  11, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.55
Round  12, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.51
Round  13, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.34
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.40
Round  15, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.53
Round  16, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.55
Round  17, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.56
Round  18, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.55
Round  19, Train loss: 2.298, Test loss: 2.302, Test accuracy: 11.49
Round  20, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.59
Round  21, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.49
Round  22, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.48
Round  23, Train loss: 2.297, Test loss: 2.302, Test accuracy: 11.45
Round  24, Train loss: 2.297, Test loss: 2.302, Test accuracy: 11.30
Round  25, Train loss: 2.298, Test loss: 2.302, Test accuracy: 11.46
Round  26, Train loss: 2.297, Test loss: 2.302, Test accuracy: 11.38
Round  27, Train loss: 2.298, Test loss: 2.302, Test accuracy: 11.49
Round  28, Train loss: 2.298, Test loss: 2.302, Test accuracy: 11.42
Round  29, Train loss: 2.294, Test loss: 2.302, Test accuracy: 11.30
Round  30, Train loss: 2.295, Test loss: 2.303, Test accuracy: 11.26
Round  31, Train loss: 2.296, Test loss: 2.303, Test accuracy: 10.92
Round  32, Train loss: 2.296, Test loss: 2.303, Test accuracy: 11.06
Round  33, Train loss: 2.297, Test loss: 2.303, Test accuracy: 10.80
Round  34, Train loss: 2.293, Test loss: 2.303, Test accuracy: 11.01
Round  35, Train loss: 2.292, Test loss: 2.303, Test accuracy: 10.88
Round  36, Train loss: 2.289, Test loss: 2.303, Test accuracy: 10.71
Round  37, Train loss: 2.286, Test loss: 2.303, Test accuracy: 10.60
Round  38, Train loss: 2.298, Test loss: 2.303, Test accuracy: 10.47
Round  39, Train loss: 2.290, Test loss: 2.304, Test accuracy: 10.26
Round  40, Train loss: 2.291, Test loss: 2.304, Test accuracy: 10.27
Round  41, Train loss: 2.291, Test loss: 2.304, Test accuracy: 10.36
Round  42, Train loss: 2.289, Test loss: 2.304, Test accuracy: 10.46
Round  43, Train loss: 2.296, Test loss: 2.304, Test accuracy: 10.21
Round  44, Train loss: 2.291, Test loss: 2.304, Test accuracy: 10.09
Round  45, Train loss: 2.286, Test loss: 2.304, Test accuracy: 9.97
Round  46, Train loss: 2.289, Test loss: 2.304, Test accuracy: 9.96
Round  47, Train loss: 2.286, Test loss: 2.304, Test accuracy: 9.93
Round  48, Train loss: 2.289, Test loss: 2.304, Test accuracy: 9.90
Round  49, Train loss: 2.286, Test loss: 2.305, Test accuracy: 9.90
Round  50, Train loss: 2.292, Test loss: 2.305, Test accuracy: 9.92
Round  51, Train loss: 2.287, Test loss: 2.305, Test accuracy: 9.92
Round  52, Train loss: 2.282, Test loss: 2.305, Test accuracy: 10.00
Round  53, Train loss: 2.285, Test loss: 2.305, Test accuracy: 9.98
Round  54, Train loss: 2.282, Test loss: 2.305, Test accuracy: 10.17
Round  55, Train loss: 2.290, Test loss: 2.305, Test accuracy: 10.31
Round  56, Train loss: 2.284, Test loss: 2.305, Test accuracy: 10.49
Round  57, Train loss: 2.286, Test loss: 2.305, Test accuracy: 10.49
Round  58, Train loss: 2.283, Test loss: 2.305, Test accuracy: 10.46
Round  59, Train loss: 2.285, Test loss: 2.305, Test accuracy: 10.44
Round  60, Train loss: 2.275, Test loss: 2.305, Test accuracy: 10.43
Round  61, Train loss: 2.285, Test loss: 2.306, Test accuracy: 10.58
Round  62, Train loss: 2.281, Test loss: 2.306, Test accuracy: 10.61
Round  63, Train loss: 2.283, Test loss: 2.306, Test accuracy: 10.64
Round  64, Train loss: 2.288, Test loss: 2.306, Test accuracy: 10.53
Round  65, Train loss: 2.282, Test loss: 2.306, Test accuracy: 10.51
Round  66, Train loss: 2.285, Test loss: 2.306, Test accuracy: 10.57
Round  67, Train loss: 2.286, Test loss: 2.306, Test accuracy: 10.59
Round  68, Train loss: 2.280, Test loss: 2.306, Test accuracy: 10.56
Round  69, Train loss: 2.286, Test loss: 2.305, Test accuracy: 10.58
Round  70, Train loss: 2.285, Test loss: 2.305, Test accuracy: 10.33
Round  71, Train loss: 2.279, Test loss: 2.305, Test accuracy: 10.36
Round  72, Train loss: 2.289, Test loss: 2.305, Test accuracy: 10.37
Round  73, Train loss: 2.282, Test loss: 2.305, Test accuracy: 10.40
Round  74, Train loss: 2.282, Test loss: 2.305, Test accuracy: 10.43
Round  75, Train loss: 2.281, Test loss: 2.306, Test accuracy: 10.42
Round  76, Train loss: 2.280, Test loss: 2.306, Test accuracy: 10.34
Round  77, Train loss: 2.279, Test loss: 2.306, Test accuracy: 10.27
Round  78, Train loss: 2.280, Test loss: 2.306, Test accuracy: 10.32
Round  79, Train loss: 2.278, Test loss: 2.306, Test accuracy: 10.35
Round  80, Train loss: 2.285, Test loss: 2.306, Test accuracy: 10.41
Round  81, Train loss: 2.277, Test loss: 2.306, Test accuracy: 10.33
Round  82, Train loss: 2.281, Test loss: 2.306, Test accuracy: 10.34
Round  83, Train loss: 2.268, Test loss: 2.307, Test accuracy: 10.13
Round  84, Train loss: 2.270, Test loss: 2.306, Test accuracy: 10.19
Round  85, Train loss: 2.281, Test loss: 2.307, Test accuracy: 10.13
Round  86, Train loss: 2.276, Test loss: 2.307, Test accuracy: 10.08
Round  87, Train loss: 2.274, Test loss: 2.307, Test accuracy: 10.12
Round  88, Train loss: 2.276, Test loss: 2.308, Test accuracy: 10.05
Round  89, Train loss: 2.274, Test loss: 2.307, Test accuracy: 10.13
Round  90, Train loss: 2.277, Test loss: 2.308, Test accuracy: 10.11
Round  91, Train loss: 2.277, Test loss: 2.308, Test accuracy: 9.95
Round  92, Train loss: 2.268, Test loss: 2.308, Test accuracy: 9.99
Round  93, Train loss: 2.279, Test loss: 2.308, Test accuracy: 10.10
Round  94, Train loss: 2.270, Test loss: 2.309, Test accuracy: 10.05/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 2.271, Test loss: 2.309, Test accuracy: 10.12
Round  96, Train loss: 2.285, Test loss: 2.308, Test accuracy: 10.13
Round  97, Train loss: 2.266, Test loss: 2.309, Test accuracy: 10.13
Round  98, Train loss: 2.268, Test loss: 2.308, Test accuracy: 10.30
Round  99, Train loss: 2.278, Test loss: 2.307, Test accuracy: 10.43
Final Round, Train loss: 2.273, Test loss: 2.308, Test accuracy: 10.42
Average accuracy final 10 rounds: 10.1305
1430.5239827632904
[1.0220258235931396, 1.8576343059539795, 2.7637717723846436, 3.6591320037841797, 4.583588600158691, 5.508767604827881, 6.410452842712402, 7.329656600952148, 8.229150295257568, 9.145432710647583, 10.055168151855469, 10.968121767044067, 11.85384726524353, 12.745557069778442, 13.642980575561523, 14.532723903656006, 15.42247486114502, 16.339637517929077, 17.234471082687378, 18.163729906082153, 19.052882194519043, 19.96328330039978, 20.872095346450806, 21.780822277069092, 22.655303955078125, 23.482250928878784, 24.32213306427002, 25.155255556106567, 25.99522566795349, 26.83320379257202, 27.76230764389038, 28.677699089050293, 29.599228143692017, 30.495899438858032, 31.40686297416687, 32.315184593200684, 33.23724317550659, 34.14709496498108, 35.04857039451599, 35.95855665206909, 36.865413188934326, 37.78817677497864, 38.68847060203552, 39.602823972702026, 40.50535202026367, 41.424511194229126, 42.32894992828369, 43.23996043205261, 44.14280915260315, 45.04573607444763, 45.94977831840515, 46.849254846572876, 47.7638795375824, 48.66862106323242, 49.58559584617615, 50.48711705207825, 51.40005850791931, 52.30843806266785, 53.22451043128967, 54.08624219894409, 54.90697431564331, 55.73719143867493, 56.56794500350952, 57.399529218673706, 58.235084533691406, 59.067062854766846, 59.905478954315186, 60.747211933135986, 61.574798822402954, 62.471996545791626, 63.36424446105957, 64.28365468978882, 65.18787050247192, 66.0894238948822, 66.98901081085205, 67.89631247520447, 68.80884194374084, 69.70874428749084, 70.61798095703125, 71.52175521850586, 72.43952655792236, 73.33802366256714, 74.16547060012817, 75.06815195083618, 75.98301863670349, 76.880610704422, 77.78161334991455, 78.68822717666626, 79.60231685638428, 80.5354413986206, 81.44521284103394, 82.3507308959961, 83.2622754573822, 84.1959936618805, 85.09784650802612, 85.99401998519897, 86.89167714118958, 87.79052424430847, 88.70919489860535, 89.61872887611389, 90.97011423110962]
[10.705, 12.055, 13.1875, 14.205, 13.5175, 12.7675, 12.1625, 12.0975, 12.1925, 11.88, 11.5775, 11.55, 11.5075, 11.3425, 11.4, 11.53, 11.5525, 11.565, 11.5525, 11.4925, 11.5925, 11.4875, 11.485, 11.4475, 11.3, 11.465, 11.375, 11.495, 11.4175, 11.2975, 11.2575, 10.9225, 11.06, 10.7975, 11.0125, 10.8825, 10.7125, 10.6025, 10.4725, 10.255, 10.27, 10.3575, 10.46, 10.2125, 10.09, 9.9725, 9.955, 9.9275, 9.905, 9.9, 9.9175, 9.9175, 9.9975, 9.9825, 10.1675, 10.31, 10.495, 10.4875, 10.4575, 10.4375, 10.43, 10.5825, 10.61, 10.6425, 10.525, 10.5125, 10.5725, 10.5925, 10.5625, 10.5775, 10.3275, 10.3625, 10.37, 10.3975, 10.425, 10.4225, 10.335, 10.2725, 10.3225, 10.345, 10.4075, 10.3275, 10.3425, 10.13, 10.1925, 10.1275, 10.0825, 10.125, 10.055, 10.1325, 10.11, 9.95, 9.995, 10.095, 10.0475, 10.115, 10.1325, 10.13, 10.3, 10.43, 10.42]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.300, Test accuracy: 10.15
Round   1, Train loss: 2.313, Test loss: 2.295, Test accuracy: 28.09
Round   2, Train loss: 2.303, Test loss: 2.286, Test accuracy: 40.04
Round   3, Train loss: 2.287, Test loss: 2.263, Test accuracy: 30.21
Round   4, Train loss: 2.250, Test loss: 2.203, Test accuracy: 30.77
Round   5, Train loss: 2.186, Test loss: 2.142, Test accuracy: 40.85
Round   6, Train loss: 2.120, Test loss: 2.079, Test accuracy: 49.48
Round   7, Train loss: 2.049, Test loss: 2.008, Test accuracy: 57.75
Round   8, Train loss: 2.007, Test loss: 1.950, Test accuracy: 64.78
Round   9, Train loss: 1.933, Test loss: 1.912, Test accuracy: 68.33
Round  10, Train loss: 1.896, Test loss: 1.876, Test accuracy: 71.37
Round  11, Train loss: 1.886, Test loss: 1.846, Test accuracy: 72.75
Round  12, Train loss: 1.846, Test loss: 1.828, Test accuracy: 73.60
Round  13, Train loss: 1.844, Test loss: 1.812, Test accuracy: 74.25
Round  14, Train loss: 1.843, Test loss: 1.796, Test accuracy: 74.41
Round  15, Train loss: 1.830, Test loss: 1.784, Test accuracy: 74.84
Round  16, Train loss: 1.799, Test loss: 1.779, Test accuracy: 75.05
Round  17, Train loss: 1.783, Test loss: 1.775, Test accuracy: 75.28
Round  18, Train loss: 1.794, Test loss: 1.768, Test accuracy: 75.46
Round  19, Train loss: 1.801, Test loss: 1.754, Test accuracy: 75.65
Round  20, Train loss: 1.787, Test loss: 1.747, Test accuracy: 75.94
Round  21, Train loss: 1.763, Test loss: 1.746, Test accuracy: 76.83
Round  22, Train loss: 1.785, Test loss: 1.733, Test accuracy: 77.54
Round  23, Train loss: 1.752, Test loss: 1.727, Test accuracy: 79.02
Round  24, Train loss: 1.757, Test loss: 1.711, Test accuracy: 82.45
Round  25, Train loss: 1.725, Test loss: 1.699, Test accuracy: 83.59
Round  26, Train loss: 1.708, Test loss: 1.688, Test accuracy: 84.16
Round  27, Train loss: 1.710, Test loss: 1.676, Test accuracy: 84.69
Round  28, Train loss: 1.705, Test loss: 1.669, Test accuracy: 85.48
Round  29, Train loss: 1.680, Test loss: 1.663, Test accuracy: 86.79
Round  30, Train loss: 1.670, Test loss: 1.655, Test accuracy: 87.72
Round  31, Train loss: 1.667, Test loss: 1.647, Test accuracy: 88.62
Round  32, Train loss: 1.655, Test loss: 1.639, Test accuracy: 89.34
Round  33, Train loss: 1.659, Test loss: 1.630, Test accuracy: 89.97
Round  34, Train loss: 1.647, Test loss: 1.623, Test accuracy: 90.63
Round  35, Train loss: 1.644, Test loss: 1.616, Test accuracy: 90.91
Round  36, Train loss: 1.628, Test loss: 1.612, Test accuracy: 91.28
Round  37, Train loss: 1.631, Test loss: 1.607, Test accuracy: 91.78
Round  38, Train loss: 1.608, Test loss: 1.607, Test accuracy: 91.91
Round  39, Train loss: 1.630, Test loss: 1.598, Test accuracy: 92.22
Round  40, Train loss: 1.605, Test loss: 1.598, Test accuracy: 92.50
Round  41, Train loss: 1.619, Test loss: 1.591, Test accuracy: 92.77
Round  42, Train loss: 1.609, Test loss: 1.588, Test accuracy: 92.95
Round  43, Train loss: 1.614, Test loss: 1.582, Test accuracy: 93.14
Round  44, Train loss: 1.591, Test loss: 1.583, Test accuracy: 93.34
Round  45, Train loss: 1.582, Test loss: 1.584, Test accuracy: 93.48
Round  46, Train loss: 1.597, Test loss: 1.577, Test accuracy: 93.66
Round  47, Train loss: 1.592, Test loss: 1.575, Test accuracy: 93.75
Round  48, Train loss: 1.588, Test loss: 1.573, Test accuracy: 93.87
Round  49, Train loss: 1.586, Test loss: 1.572, Test accuracy: 94.04
Round  50, Train loss: 1.580, Test loss: 1.569, Test accuracy: 94.06
Round  51, Train loss: 1.594, Test loss: 1.563, Test accuracy: 94.23
Round  52, Train loss: 1.569, Test loss: 1.566, Test accuracy: 94.37
Round  53, Train loss: 1.575, Test loss: 1.564, Test accuracy: 94.46
Round  54, Train loss: 1.574, Test loss: 1.562, Test accuracy: 94.52
Round  55, Train loss: 1.570, Test loss: 1.560, Test accuracy: 94.62
Round  56, Train loss: 1.571, Test loss: 1.559, Test accuracy: 94.60
Round  57, Train loss: 1.566, Test loss: 1.559, Test accuracy: 94.71
Round  58, Train loss: 1.557, Test loss: 1.559, Test accuracy: 94.73
Round  59, Train loss: 1.553, Test loss: 1.559, Test accuracy: 94.88
Round  60, Train loss: 1.567, Test loss: 1.555, Test accuracy: 94.87
Round  61, Train loss: 1.559, Test loss: 1.552, Test accuracy: 94.94
Round  62, Train loss: 1.556, Test loss: 1.553, Test accuracy: 94.97
Round  63, Train loss: 1.553, Test loss: 1.552, Test accuracy: 95.05
Round  64, Train loss: 1.548, Test loss: 1.552, Test accuracy: 95.04
Round  65, Train loss: 1.554, Test loss: 1.551, Test accuracy: 95.00
Round  66, Train loss: 1.557, Test loss: 1.548, Test accuracy: 95.13
Round  67, Train loss: 1.542, Test loss: 1.551, Test accuracy: 95.24
Round  68, Train loss: 1.541, Test loss: 1.551, Test accuracy: 95.22
Round  69, Train loss: 1.556, Test loss: 1.545, Test accuracy: 95.26
Round  70, Train loss: 1.547, Test loss: 1.545, Test accuracy: 95.32
Round  71, Train loss: 1.541, Test loss: 1.547, Test accuracy: 95.42
Round  72, Train loss: 1.549, Test loss: 1.545, Test accuracy: 95.39
Round  73, Train loss: 1.539, Test loss: 1.547, Test accuracy: 95.50
Round  74, Train loss: 1.538, Test loss: 1.545, Test accuracy: 95.51
Round  75, Train loss: 1.548, Test loss: 1.541, Test accuracy: 95.53
Round  76, Train loss: 1.538, Test loss: 1.542, Test accuracy: 95.64
Round  77, Train loss: 1.541, Test loss: 1.541, Test accuracy: 95.61
Round  78, Train loss: 1.541, Test loss: 1.541, Test accuracy: 95.68
Round  79, Train loss: 1.536, Test loss: 1.541, Test accuracy: 95.67
Round  80, Train loss: 1.538, Test loss: 1.540, Test accuracy: 95.65
Round  81, Train loss: 1.531, Test loss: 1.540, Test accuracy: 95.77
Round  82, Train loss: 1.534, Test loss: 1.540, Test accuracy: 95.69
Round  83, Train loss: 1.535, Test loss: 1.538, Test accuracy: 95.78
Round  84, Train loss: 1.532, Test loss: 1.538, Test accuracy: 95.80
Round  85, Train loss: 1.530, Test loss: 1.537, Test accuracy: 95.86
Round  86, Train loss: 1.529, Test loss: 1.538, Test accuracy: 95.86
Round  87, Train loss: 1.529, Test loss: 1.538, Test accuracy: 95.93
Round  88, Train loss: 1.531, Test loss: 1.537, Test accuracy: 95.94
Round  89, Train loss: 1.526, Test loss: 1.538, Test accuracy: 95.91
Round  90, Train loss: 1.529, Test loss: 1.535, Test accuracy: 95.95
Round  91, Train loss: 1.522, Test loss: 1.536, Test accuracy: 95.92
Round  92, Train loss: 1.528, Test loss: 1.535, Test accuracy: 96.00
Round  93, Train loss: 1.531, Test loss: 1.533, Test accuracy: 96.03/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.522, Test loss: 1.535, Test accuracy: 96.07
Round  95, Train loss: 1.526, Test loss: 1.533, Test accuracy: 96.07
Round  96, Train loss: 1.523, Test loss: 1.534, Test accuracy: 96.08
Round  97, Train loss: 1.524, Test loss: 1.533, Test accuracy: 96.12
Round  98, Train loss: 1.522, Test loss: 1.532, Test accuracy: 96.14
Round  99, Train loss: 1.522, Test loss: 1.532, Test accuracy: 96.11
Final Round, Train loss: 1.495, Test loss: 1.527, Test accuracy: 96.11
Average accuracy final 10 rounds: 96.0485
1416.4770574569702
[1.009223222732544, 1.9293982982635498, 2.8311350345611572, 3.7169435024261475, 4.624570369720459, 5.523923397064209, 6.415523290634155, 7.30989670753479, 8.228379011154175, 9.135896921157837, 9.966280460357666, 10.789010047912598, 11.629343032836914, 12.45365595817566, 13.278507232666016, 14.125494956970215, 14.991653203964233, 15.838031768798828, 16.690823316574097, 17.54200839996338, 18.39049744606018, 19.23549723625183, 20.08317494392395, 20.928556442260742, 21.78859853744507, 22.66061234474182, 23.51271939277649, 24.358038902282715, 25.21243691444397, 26.077836513519287, 26.927464962005615, 27.759557485580444, 28.599977016448975, 29.43934679031372, 30.285158157348633, 31.128425359725952, 31.967389583587646, 32.79556703567505, 33.64880180358887, 34.47215151786804, 35.31321358680725, 36.138585567474365, 36.98497533798218, 37.80327749252319, 38.647194623947144, 39.47689080238342, 40.34696125984192, 41.17984986305237, 42.04951310157776, 42.898239850997925, 43.751774311065674, 44.58150815963745, 45.43695688247681, 46.26917862892151, 47.12373208999634, 47.97818470001221, 48.84236145019531, 49.702357053756714, 50.56644034385681, 51.41491651535034, 52.271092891693115, 53.106712102890015, 53.95630359649658, 54.784321546554565, 55.63970160484314, 56.47251343727112, 57.34199237823486, 58.18261098861694, 59.051462173461914, 59.8932466506958, 60.769980669021606, 61.60804319381714, 62.475399017333984, 63.30262780189514, 64.1566903591156, 64.99069952964783, 65.84801888465881, 66.68951272964478, 67.54331731796265, 68.38837623596191, 69.24455618858337, 70.09112215042114, 70.96143937110901, 71.81094193458557, 72.66539740562439, 73.49797701835632, 74.3720018863678, 75.20990633964539, 76.0748941898346, 76.90017437934875, 77.76985359191895, 78.6080334186554, 79.47131943702698, 80.31391286849976, 81.16796398162842, 82.01310992240906, 82.85853266716003, 83.70215702056885, 84.54801940917969, 85.39136171340942, 86.75837230682373]
[10.155, 28.0875, 40.0375, 30.2125, 30.7675, 40.8475, 49.4825, 57.7475, 64.775, 68.3325, 71.3675, 72.745, 73.6, 74.245, 74.405, 74.8375, 75.0525, 75.2825, 75.4575, 75.65, 75.9375, 76.825, 77.54, 79.0225, 82.4475, 83.5925, 84.1625, 84.6875, 85.48, 86.79, 87.7225, 88.6175, 89.3375, 89.9675, 90.6275, 90.9075, 91.2775, 91.785, 91.9075, 92.225, 92.495, 92.7675, 92.95, 93.145, 93.345, 93.4775, 93.66, 93.75, 93.87, 94.04, 94.0625, 94.235, 94.3725, 94.46, 94.515, 94.6175, 94.6025, 94.71, 94.73, 94.8825, 94.8675, 94.9375, 94.965, 95.0475, 95.0375, 95.0, 95.1325, 95.24, 95.215, 95.2625, 95.32, 95.42, 95.395, 95.495, 95.5125, 95.535, 95.6425, 95.61, 95.68, 95.6675, 95.6475, 95.765, 95.6925, 95.775, 95.7975, 95.865, 95.855, 95.9275, 95.945, 95.9125, 95.955, 95.92, 96.0025, 96.025, 96.07, 96.0725, 96.075, 96.125, 96.135, 96.105, 96.11]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.221, Test loss: 2.201, Test accuracy: 24.53 

Round   0, Global train loss: 2.221, Global test loss: 2.269, Global test accuracy: 16.67 

Round   1, Train loss: 1.931, Test loss: 2.017, Test accuracy: 47.17 

Round   1, Global train loss: 1.931, Global test loss: 2.161, Global test accuracy: 32.36 

Round   2, Train loss: 1.824, Test loss: 1.914, Test accuracy: 57.58 

Round   2, Global train loss: 1.824, Global test loss: 2.137, Global test accuracy: 32.46 

Round   3, Train loss: 1.778, Test loss: 1.811, Test accuracy: 65.08 

Round   3, Global train loss: 1.778, Global test loss: 2.200, Global test accuracy: 19.36 

Round   4, Train loss: 1.630, Test loss: 1.730, Test accuracy: 74.97 

Round   4, Global train loss: 1.630, Global test loss: 2.065, Global test accuracy: 42.95 

Round   5, Train loss: 1.607, Test loss: 1.669, Test accuracy: 80.81 

Round   5, Global train loss: 1.607, Global test loss: 2.183, Global test accuracy: 25.38 

Round   6, Train loss: 1.631, Test loss: 1.627, Test accuracy: 85.38 

Round   6, Global train loss: 1.631, Global test loss: 2.209, Global test accuracy: 22.73 

Round   7, Train loss: 1.580, Test loss: 1.603, Test accuracy: 87.86 

Round   7, Global train loss: 1.580, Global test loss: 2.105, Global test accuracy: 34.87 

Round   8, Train loss: 1.554, Test loss: 1.605, Test accuracy: 86.87 

Round   8, Global train loss: 1.554, Global test loss: 2.214, Global test accuracy: 20.18 

Round   9, Train loss: 1.652, Test loss: 1.578, Test accuracy: 89.71 

Round   9, Global train loss: 1.652, Global test loss: 2.254, Global test accuracy: 16.60 

Round  10, Train loss: 1.531, Test loss: 1.576, Test accuracy: 89.85 

Round  10, Global train loss: 1.531, Global test loss: 2.174, Global test accuracy: 23.38 

Round  11, Train loss: 1.645, Test loss: 1.565, Test accuracy: 90.93 

Round  11, Global train loss: 1.645, Global test loss: 2.113, Global test accuracy: 34.21 

Round  12, Train loss: 1.548, Test loss: 1.555, Test accuracy: 91.45 

Round  12, Global train loss: 1.548, Global test loss: 2.084, Global test accuracy: 36.11 

Round  13, Train loss: 1.584, Test loss: 1.554, Test accuracy: 91.46 

Round  13, Global train loss: 1.584, Global test loss: 2.073, Global test accuracy: 35.42 

Round  14, Train loss: 1.523, Test loss: 1.553, Test accuracy: 91.48 

Round  14, Global train loss: 1.523, Global test loss: 2.108, Global test accuracy: 31.80 

Round  15, Train loss: 1.518, Test loss: 1.542, Test accuracy: 92.72 

Round  15, Global train loss: 1.518, Global test loss: 2.157, Global test accuracy: 28.98 

Round  16, Train loss: 1.530, Test loss: 1.534, Test accuracy: 94.01 

Round  16, Global train loss: 1.530, Global test loss: 2.176, Global test accuracy: 27.18 

Round  17, Train loss: 1.476, Test loss: 1.533, Test accuracy: 94.08 

Round  17, Global train loss: 1.476, Global test loss: 2.219, Global test accuracy: 20.88 

Round  18, Train loss: 1.527, Test loss: 1.521, Test accuracy: 95.40 

Round  18, Global train loss: 1.527, Global test loss: 2.041, Global test accuracy: 43.92 

Round  19, Train loss: 1.469, Test loss: 1.520, Test accuracy: 95.40 

Round  19, Global train loss: 1.469, Global test loss: 2.084, Global test accuracy: 35.24 

Round  20, Train loss: 1.474, Test loss: 1.520, Test accuracy: 95.44 

Round  20, Global train loss: 1.474, Global test loss: 2.040, Global test accuracy: 40.27 

Round  21, Train loss: 1.470, Test loss: 1.519, Test accuracy: 95.42 

Round  21, Global train loss: 1.470, Global test loss: 2.115, Global test accuracy: 35.06 

Round  22, Train loss: 1.480, Test loss: 1.512, Test accuracy: 95.73 

Round  22, Global train loss: 1.480, Global test loss: 2.272, Global test accuracy: 16.08 

Round  23, Train loss: 1.475, Test loss: 1.511, Test accuracy: 95.66 

Round  23, Global train loss: 1.475, Global test loss: 2.133, Global test accuracy: 28.81 

Round  24, Train loss: 1.471, Test loss: 1.510, Test accuracy: 95.73 

Round  24, Global train loss: 1.471, Global test loss: 1.988, Global test accuracy: 53.12 

Round  25, Train loss: 1.469, Test loss: 1.510, Test accuracy: 95.80 

Round  25, Global train loss: 1.469, Global test loss: 2.040, Global test accuracy: 46.41 

Round  26, Train loss: 1.468, Test loss: 1.509, Test accuracy: 95.80 

Round  26, Global train loss: 1.468, Global test loss: 2.014, Global test accuracy: 46.73 

Round  27, Train loss: 1.478, Test loss: 1.505, Test accuracy: 96.14 

Round  27, Global train loss: 1.478, Global test loss: 2.089, Global test accuracy: 32.87 

Round  28, Train loss: 1.466, Test loss: 1.505, Test accuracy: 96.15 

Round  28, Global train loss: 1.466, Global test loss: 2.055, Global test accuracy: 37.55 

Round  29, Train loss: 1.470, Test loss: 1.504, Test accuracy: 96.12 

Round  29, Global train loss: 1.470, Global test loss: 2.134, Global test accuracy: 30.62 

Round  30, Train loss: 1.468, Test loss: 1.504, Test accuracy: 96.11 

Round  30, Global train loss: 1.468, Global test loss: 2.180, Global test accuracy: 22.65 

Round  31, Train loss: 1.468, Test loss: 1.504, Test accuracy: 96.13 

Round  31, Global train loss: 1.468, Global test loss: 2.090, Global test accuracy: 32.10 

Round  32, Train loss: 1.469, Test loss: 1.504, Test accuracy: 96.10 

Round  32, Global train loss: 1.469, Global test loss: 2.155, Global test accuracy: 25.57 

Round  33, Train loss: 1.466, Test loss: 1.504, Test accuracy: 96.12 

Round  33, Global train loss: 1.466, Global test loss: 2.085, Global test accuracy: 34.55 

Round  34, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.13 

Round  34, Global train loss: 1.467, Global test loss: 1.976, Global test accuracy: 52.56 

Round  35, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.11 

Round  35, Global train loss: 1.467, Global test loss: 2.194, Global test accuracy: 23.68 

Round  36, Train loss: 1.469, Test loss: 1.503, Test accuracy: 96.15 

Round  36, Global train loss: 1.469, Global test loss: 2.236, Global test accuracy: 18.85 

Round  37, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.16 

Round  37, Global train loss: 1.467, Global test loss: 2.051, Global test accuracy: 39.79 

Round  38, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.11 

Round  38, Global train loss: 1.467, Global test loss: 2.204, Global test accuracy: 20.10 

Round  39, Train loss: 1.468, Test loss: 1.503, Test accuracy: 96.12 

Round  39, Global train loss: 1.468, Global test loss: 2.052, Global test accuracy: 42.58 

Round  40, Train loss: 1.466, Test loss: 1.503, Test accuracy: 96.12 

Round  40, Global train loss: 1.466, Global test loss: 2.215, Global test accuracy: 21.84 

Round  41, Train loss: 1.467, Test loss: 1.503, Test accuracy: 96.14 

Round  41, Global train loss: 1.467, Global test loss: 2.126, Global test accuracy: 30.35 

Round  42, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.14 

Round  42, Global train loss: 1.468, Global test loss: 2.055, Global test accuracy: 43.60 

Round  43, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.16 

Round  43, Global train loss: 1.468, Global test loss: 2.099, Global test accuracy: 34.50 

Round  44, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.16 

Round  44, Global train loss: 1.468, Global test loss: 2.150, Global test accuracy: 29.88 

Round  45, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.16 

Round  45, Global train loss: 1.466, Global test loss: 2.072, Global test accuracy: 39.03 

Round  46, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.15 

Round  46, Global train loss: 1.466, Global test loss: 2.272, Global test accuracy: 15.41 

Round  47, Train loss: 1.468, Test loss: 1.502, Test accuracy: 96.12 

Round  47, Global train loss: 1.468, Global test loss: 2.259, Global test accuracy: 16.84 

Round  48, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.13 

Round  48, Global train loss: 1.466, Global test loss: 2.007, Global test accuracy: 44.82 

Round  49, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.14 

Round  49, Global train loss: 1.467, Global test loss: 2.040, Global test accuracy: 58.71 

Round  50, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.16 

Round  50, Global train loss: 1.467, Global test loss: 2.200, Global test accuracy: 21.33 

Round  51, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.18 

Round  51, Global train loss: 1.467, Global test loss: 2.150, Global test accuracy: 29.08 

Round  52, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.21 

Round  52, Global train loss: 1.467, Global test loss: 2.278, Global test accuracy: 16.07 

Round  53, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.18 

Round  53, Global train loss: 1.467, Global test loss: 2.116, Global test accuracy: 33.62 

Round  54, Train loss: 1.464, Test loss: 1.501, Test accuracy: 96.19 

Round  54, Global train loss: 1.464, Global test loss: 2.029, Global test accuracy: 41.37 

Round  55, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.19 

Round  55, Global train loss: 1.466, Global test loss: 2.095, Global test accuracy: 37.21 

Round  56, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.18 

Round  56, Global train loss: 1.465, Global test loss: 2.091, Global test accuracy: 37.17 

Round  57, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.20 

Round  57, Global train loss: 1.467, Global test loss: 2.142, Global test accuracy: 29.16 

Round  58, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.22 

Round  58, Global train loss: 1.466, Global test loss: 2.065, Global test accuracy: 39.63 

Round  59, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.22 

Round  59, Global train loss: 1.466, Global test loss: 2.131, Global test accuracy: 33.44 

Round  60, Train loss: 1.467, Test loss: 1.502, Test accuracy: 96.17 

Round  60, Global train loss: 1.467, Global test loss: 2.088, Global test accuracy: 39.71 

Round  61, Train loss: 1.466, Test loss: 1.502, Test accuracy: 96.18 

Round  61, Global train loss: 1.466, Global test loss: 2.106, Global test accuracy: 33.76 

Round  62, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.21 

Round  62, Global train loss: 1.467, Global test loss: 2.164, Global test accuracy: 26.44 

Round  63, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.21 

Round  63, Global train loss: 1.466, Global test loss: 2.202, Global test accuracy: 23.62 

Round  64, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.21 

Round  64, Global train loss: 1.465, Global test loss: 2.094, Global test accuracy: 36.83 

Round  65, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.17 

Round  65, Global train loss: 1.468, Global test loss: 2.100, Global test accuracy: 39.78 

Round  66, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.18 

Round  66, Global train loss: 1.468, Global test loss: 2.075, Global test accuracy: 39.11 

Round  67, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.17 

Round  67, Global train loss: 1.465, Global test loss: 2.086, Global test accuracy: 36.32 

Round  68, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.20 

Round  68, Global train loss: 1.466, Global test loss: 2.051, Global test accuracy: 39.16 

Round  69, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.21 

Round  69, Global train loss: 1.466, Global test loss: 2.040, Global test accuracy: 46.48 

Round  70, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.24 

Round  70, Global train loss: 1.466, Global test loss: 2.061, Global test accuracy: 34.38 

Round  71, Train loss: 1.464, Test loss: 1.501, Test accuracy: 96.22 

Round  71, Global train loss: 1.464, Global test loss: 2.054, Global test accuracy: 44.50 

Round  72, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.22 

Round  72, Global train loss: 1.468, Global test loss: 2.223, Global test accuracy: 20.52 

Round  73, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.22 

Round  73, Global train loss: 1.466, Global test loss: 2.213, Global test accuracy: 20.22 

Round  74, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.22 

Round  74, Global train loss: 1.467, Global test loss: 2.280, Global test accuracy: 16.78 

Round  75, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.22 

Round  75, Global train loss: 1.465, Global test loss: 2.105, Global test accuracy: 35.65 

Round  76, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.20 

Round  76, Global train loss: 1.465, Global test loss: 2.083, Global test accuracy: 36.28 

Round  77, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.21 

Round  77, Global train loss: 1.465, Global test loss: 2.134, Global test accuracy: 27.20 

Round  78, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.22 

Round  78, Global train loss: 1.467, Global test loss: 2.180, Global test accuracy: 25.44 

Round  79, Train loss: 1.464, Test loss: 1.501, Test accuracy: 96.22 

Round  79, Global train loss: 1.464, Global test loss: 2.145, Global test accuracy: 29.99 

Round  80, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.22 

Round  80, Global train loss: 1.467, Global test loss: 2.201, Global test accuracy: 23.24 

Round  81, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.22 

Round  81, Global train loss: 1.465, Global test loss: 2.116, Global test accuracy: 33.58 

Round  82, Train loss: 1.464, Test loss: 1.501, Test accuracy: 96.22 

Round  82, Global train loss: 1.464, Global test loss: 2.060, Global test accuracy: 38.31 

Round  83, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.22 

Round  83, Global train loss: 1.466, Global test loss: 2.055, Global test accuracy: 38.98 

Round  84, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.22 

Round  84, Global train loss: 1.467, Global test loss: 2.203, Global test accuracy: 23.24 

Round  85, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.22 

Round  85, Global train loss: 1.466, Global test loss: 2.107, Global test accuracy: 35.28 

Round  86, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.22 

Round  86, Global train loss: 1.467, Global test loss: 2.245, Global test accuracy: 18.00 

Round  87, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.22 

Round  87, Global train loss: 1.466, Global test loss: 2.024, Global test accuracy: 48.06 

Round  88, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.22 

Round  88, Global train loss: 1.465, Global test loss: 2.128, Global test accuracy: 33.23 

Round  89, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.22 

Round  89, Global train loss: 1.466, Global test loss: 2.040, Global test accuracy: 42.83 

Round  90, Train loss: 1.465, Test loss: 1.501, Test accuracy: 96.22 

Round  90, Global train loss: 1.465, Global test loss: 2.056, Global test accuracy: 42.26 

Round  91, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.22 

Round  91, Global train loss: 1.466, Global test loss: 2.143, Global test accuracy: 30.32 

Round  92, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.25 

Round  92, Global train loss: 1.466, Global test loss: 2.016, Global test accuracy: 41.36 

Round  93, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.24 

Round  93, Global train loss: 1.466, Global test loss: 2.171, Global test accuracy: 26.51 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.464, Test loss: 1.501, Test accuracy: 96.24 

Round  94, Global train loss: 1.464, Global test loss: 2.004, Global test accuracy: 43.07 

Round  95, Train loss: 1.464, Test loss: 1.501, Test accuracy: 96.23 

Round  95, Global train loss: 1.464, Global test loss: 2.109, Global test accuracy: 32.27 

Round  96, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.23 

Round  96, Global train loss: 1.466, Global test loss: 2.015, Global test accuracy: 50.35 

Round  97, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.23 

Round  97, Global train loss: 1.467, Global test loss: 2.213, Global test accuracy: 19.72 

Round  98, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.23 

Round  98, Global train loss: 1.466, Global test loss: 2.209, Global test accuracy: 22.52 

Round  99, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.24 

Round  99, Global train loss: 1.466, Global test loss: 2.125, Global test accuracy: 33.16 

Final Round, Train loss: 1.466, Test loss: 1.501, Test accuracy: 96.23 

Final Round, Global train loss: 1.466, Global test loss: 2.125, Global test accuracy: 33.16 

Average accuracy final 10 rounds: 96.235 

Average global accuracy final 10 rounds: 34.151666666666664 

1064.3470330238342
[0.9213323593139648, 1.7625374794006348, 2.6036994457244873, 3.437246799468994, 4.27883505821228, 5.108299970626831, 5.950667858123779, 6.79506516456604, 7.62575101852417, 8.467342138290405, 9.308733463287354, 10.160826683044434, 10.99538803100586, 11.82670545578003, 12.676459550857544, 13.516205072402954, 14.346791982650757, 15.184446573257446, 16.022711753845215, 16.86965537071228, 17.697810649871826, 18.532292366027832, 19.38291335105896, 20.22000217437744, 21.060142278671265, 21.89687967300415, 22.73774743080139, 23.577334880828857, 24.398637056350708, 25.22697353363037, 26.056862592697144, 26.885777473449707, 27.70880699157715, 28.530054092407227, 29.35887837409973, 30.182729482650757, 31.000096321105957, 31.825294733047485, 32.648207664489746, 33.48777508735657, 34.321797132492065, 35.152313232421875, 35.99149036407471, 36.83520793914795, 37.675166606903076, 38.51999473571777, 39.346049547195435, 40.18409872055054, 41.03003001213074, 41.866689920425415, 42.70476031303406, 43.532631635665894, 44.369903326034546, 45.21997404098511, 46.0412061214447, 46.881011962890625, 47.71869444847107, 48.5541889667511, 49.397422552108765, 50.222047567367554, 51.059977531433105, 51.895831823349, 52.71732139587402, 53.56216287612915, 54.39903402328491, 55.22690510749817, 56.06575417518616, 56.892399311065674, 57.73835778236389, 58.575371503829956, 59.40116477012634, 60.23497295379639, 61.064035177230835, 61.88504958152771, 62.71550989151001, 63.54455280303955, 64.37305450439453, 65.20460224151611, 66.02104306221008, 66.84778833389282, 67.67921161651611, 68.49533081054688, 69.32239484786987, 70.15346646308899, 70.97812175750732, 71.8197672367096, 72.64433550834656, 73.48935866355896, 74.32714915275574, 75.1539580821991, 75.99237585067749, 76.8255705833435, 77.64689564704895, 78.49135184288025, 79.31971263885498, 80.14718794822693, 80.97594618797302, 81.79275488853455, 82.63817429542542, 83.47178506851196, 85.14771032333374]
[24.533333333333335, 47.175, 57.575, 65.075, 74.96666666666667, 80.80833333333334, 85.38333333333334, 87.85833333333333, 86.86666666666666, 89.70833333333333, 89.85, 90.93333333333334, 91.45, 91.45833333333333, 91.48333333333333, 92.725, 94.00833333333334, 94.075, 95.4, 95.4, 95.44166666666666, 95.425, 95.73333333333333, 95.65833333333333, 95.73333333333333, 95.8, 95.8, 96.14166666666667, 96.15, 96.125, 96.10833333333333, 96.13333333333334, 96.1, 96.11666666666666, 96.13333333333334, 96.10833333333333, 96.15, 96.15833333333333, 96.10833333333333, 96.125, 96.11666666666666, 96.14166666666667, 96.14166666666667, 96.15833333333333, 96.15833333333333, 96.15833333333333, 96.15, 96.125, 96.13333333333334, 96.14166666666667, 96.15833333333333, 96.18333333333334, 96.20833333333333, 96.18333333333334, 96.19166666666666, 96.19166666666666, 96.18333333333334, 96.2, 96.21666666666667, 96.21666666666667, 96.175, 96.18333333333334, 96.20833333333333, 96.20833333333333, 96.20833333333333, 96.175, 96.18333333333334, 96.175, 96.2, 96.20833333333333, 96.24166666666666, 96.225, 96.21666666666667, 96.21666666666667, 96.21666666666667, 96.225, 96.2, 96.20833333333333, 96.21666666666667, 96.21666666666667, 96.21666666666667, 96.21666666666667, 96.21666666666667, 96.225, 96.21666666666667, 96.225, 96.225, 96.225, 96.225, 96.21666666666667, 96.21666666666667, 96.225, 96.25, 96.24166666666666, 96.24166666666666, 96.23333333333333, 96.23333333333333, 96.23333333333333, 96.23333333333333, 96.24166666666666, 96.23333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.231, Test loss: 2.205, Test accuracy: 27.41 

Round   0, Global train loss: 2.231, Global test loss: 2.287, Global test accuracy: 20.83 

Round   1, Train loss: 1.840, Test loss: 1.972, Test accuracy: 57.74 

Round   1, Global train loss: 1.840, Global test loss: 2.138, Global test accuracy: 48.92 

Round   2, Train loss: 1.728, Test loss: 1.842, Test accuracy: 64.06 

Round   2, Global train loss: 1.728, Global test loss: 2.026, Global test accuracy: 48.99 

Round   3, Train loss: 1.613, Test loss: 1.731, Test accuracy: 75.54 

Round   3, Global train loss: 1.613, Global test loss: 2.023, Global test accuracy: 45.63 

Round   4, Train loss: 1.545, Test loss: 1.675, Test accuracy: 79.09 

Round   4, Global train loss: 1.545, Global test loss: 1.926, Global test accuracy: 54.18 

Round   5, Train loss: 1.559, Test loss: 1.605, Test accuracy: 86.67 

Round   5, Global train loss: 1.559, Global test loss: 1.870, Global test accuracy: 59.87 

Round   6, Train loss: 1.533, Test loss: 1.566, Test accuracy: 90.92 

Round   6, Global train loss: 1.533, Global test loss: 1.810, Global test accuracy: 68.53 

Round   7, Train loss: 1.503, Test loss: 1.558, Test accuracy: 91.97 

Round   7, Global train loss: 1.503, Global test loss: 1.803, Global test accuracy: 68.22 

Round   8, Train loss: 1.522, Test loss: 1.558, Test accuracy: 91.47 

Round   8, Global train loss: 1.522, Global test loss: 1.794, Global test accuracy: 69.33 

Round   9, Train loss: 1.523, Test loss: 1.554, Test accuracy: 92.01 

Round   9, Global train loss: 1.523, Global test loss: 1.768, Global test accuracy: 72.80 

Round  10, Train loss: 1.522, Test loss: 1.537, Test accuracy: 93.39 

Round  10, Global train loss: 1.522, Global test loss: 1.790, Global test accuracy: 69.96 

Round  11, Train loss: 1.501, Test loss: 1.531, Test accuracy: 93.85 

Round  11, Global train loss: 1.501, Global test loss: 1.737, Global test accuracy: 74.58 

Round  12, Train loss: 1.517, Test loss: 1.507, Test accuracy: 96.05 

Round  12, Global train loss: 1.517, Global test loss: 1.736, Global test accuracy: 77.89 

Round  13, Train loss: 1.499, Test loss: 1.505, Test accuracy: 96.21 

Round  13, Global train loss: 1.499, Global test loss: 1.788, Global test accuracy: 69.32 

Round  14, Train loss: 1.492, Test loss: 1.505, Test accuracy: 96.22 

Round  14, Global train loss: 1.492, Global test loss: 1.733, Global test accuracy: 74.06 

Round  15, Train loss: 1.501, Test loss: 1.505, Test accuracy: 96.22 

Round  15, Global train loss: 1.501, Global test loss: 1.729, Global test accuracy: 74.66 

Round  16, Train loss: 1.503, Test loss: 1.504, Test accuracy: 96.30 

Round  16, Global train loss: 1.503, Global test loss: 1.718, Global test accuracy: 77.19 

Round  17, Train loss: 1.490, Test loss: 1.502, Test accuracy: 96.37 

Round  17, Global train loss: 1.490, Global test loss: 1.676, Global test accuracy: 81.53 

Round  18, Train loss: 1.491, Test loss: 1.502, Test accuracy: 96.40 

Round  18, Global train loss: 1.491, Global test loss: 1.682, Global test accuracy: 79.91 

Round  19, Train loss: 1.493, Test loss: 1.502, Test accuracy: 96.36 

Round  19, Global train loss: 1.493, Global test loss: 1.701, Global test accuracy: 76.32 

Round  20, Train loss: 1.492, Test loss: 1.501, Test accuracy: 96.50 

Round  20, Global train loss: 1.492, Global test loss: 1.670, Global test accuracy: 80.36 

Round  21, Train loss: 1.488, Test loss: 1.500, Test accuracy: 96.54 

Round  21, Global train loss: 1.488, Global test loss: 1.772, Global test accuracy: 70.71 

Round  22, Train loss: 1.486, Test loss: 1.499, Test accuracy: 96.61 

Round  22, Global train loss: 1.486, Global test loss: 1.623, Global test accuracy: 86.49 

Round  23, Train loss: 1.479, Test loss: 1.498, Test accuracy: 96.72 

Round  23, Global train loss: 1.479, Global test loss: 1.663, Global test accuracy: 81.03 

Round  24, Train loss: 1.486, Test loss: 1.498, Test accuracy: 96.67 

Round  24, Global train loss: 1.486, Global test loss: 1.630, Global test accuracy: 84.84 

Round  25, Train loss: 1.486, Test loss: 1.497, Test accuracy: 96.77 

Round  25, Global train loss: 1.486, Global test loss: 1.645, Global test accuracy: 83.69 

Round  26, Train loss: 1.476, Test loss: 1.496, Test accuracy: 96.83 

Round  26, Global train loss: 1.476, Global test loss: 1.671, Global test accuracy: 80.62 

Round  27, Train loss: 1.485, Test loss: 1.497, Test accuracy: 96.75 

Round  27, Global train loss: 1.485, Global test loss: 1.693, Global test accuracy: 77.38 

Round  28, Train loss: 1.481, Test loss: 1.496, Test accuracy: 96.85 

Round  28, Global train loss: 1.481, Global test loss: 1.635, Global test accuracy: 84.03 

Round  29, Train loss: 1.483, Test loss: 1.496, Test accuracy: 96.80 

Round  29, Global train loss: 1.483, Global test loss: 1.700, Global test accuracy: 76.42 

Round  30, Train loss: 1.482, Test loss: 1.496, Test accuracy: 96.86 

Round  30, Global train loss: 1.482, Global test loss: 1.655, Global test accuracy: 82.36 

Round  31, Train loss: 1.485, Test loss: 1.496, Test accuracy: 96.92 

Round  31, Global train loss: 1.485, Global test loss: 1.703, Global test accuracy: 76.29 

Round  32, Train loss: 1.477, Test loss: 1.496, Test accuracy: 96.90 

Round  32, Global train loss: 1.477, Global test loss: 1.715, Global test accuracy: 75.03 

Round  33, Train loss: 1.488, Test loss: 1.496, Test accuracy: 96.89 

Round  33, Global train loss: 1.488, Global test loss: 1.672, Global test accuracy: 80.93 

Round  34, Train loss: 1.480, Test loss: 1.496, Test accuracy: 96.88 

Round  34, Global train loss: 1.480, Global test loss: 1.669, Global test accuracy: 79.54 

Round  35, Train loss: 1.486, Test loss: 1.497, Test accuracy: 96.81 

Round  35, Global train loss: 1.486, Global test loss: 1.623, Global test accuracy: 85.85 

Round  36, Train loss: 1.474, Test loss: 1.497, Test accuracy: 96.80 

Round  36, Global train loss: 1.474, Global test loss: 1.614, Global test accuracy: 86.22 

Round  37, Train loss: 1.480, Test loss: 1.497, Test accuracy: 96.72 

Round  37, Global train loss: 1.480, Global test loss: 1.627, Global test accuracy: 84.97 

Round  38, Train loss: 1.484, Test loss: 1.496, Test accuracy: 96.86 

Round  38, Global train loss: 1.484, Global test loss: 1.628, Global test accuracy: 84.38 

Round  39, Train loss: 1.475, Test loss: 1.496, Test accuracy: 96.82 

Round  39, Global train loss: 1.475, Global test loss: 1.599, Global test accuracy: 87.36 

Round  40, Train loss: 1.477, Test loss: 1.495, Test accuracy: 96.91 

Round  40, Global train loss: 1.477, Global test loss: 1.604, Global test accuracy: 87.03 

Round  41, Train loss: 1.475, Test loss: 1.495, Test accuracy: 96.92 

Round  41, Global train loss: 1.475, Global test loss: 1.604, Global test accuracy: 86.72 

Round  42, Train loss: 1.476, Test loss: 1.495, Test accuracy: 96.95 

Round  42, Global train loss: 1.476, Global test loss: 1.642, Global test accuracy: 82.51 

Round  43, Train loss: 1.480, Test loss: 1.495, Test accuracy: 96.92 

Round  43, Global train loss: 1.480, Global test loss: 1.647, Global test accuracy: 81.97 

Round  44, Train loss: 1.476, Test loss: 1.494, Test accuracy: 96.94 

Round  44, Global train loss: 1.476, Global test loss: 1.593, Global test accuracy: 87.68 

Round  45, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.03 

Round  45, Global train loss: 1.474, Global test loss: 1.627, Global test accuracy: 83.99 

Round  46, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.02 

Round  46, Global train loss: 1.477, Global test loss: 1.658, Global test accuracy: 81.64 

Round  47, Train loss: 1.473, Test loss: 1.493, Test accuracy: 97.09 

Round  47, Global train loss: 1.473, Global test loss: 1.613, Global test accuracy: 85.85 

Round  48, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.08 

Round  48, Global train loss: 1.477, Global test loss: 1.638, Global test accuracy: 83.15 

Round  49, Train loss: 1.471, Test loss: 1.493, Test accuracy: 97.08 

Round  49, Global train loss: 1.471, Global test loss: 1.601, Global test accuracy: 87.63 

Round  50, Train loss: 1.475, Test loss: 1.493, Test accuracy: 97.03 

Round  50, Global train loss: 1.475, Global test loss: 1.649, Global test accuracy: 82.12 

Round  51, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.06 

Round  51, Global train loss: 1.474, Global test loss: 1.636, Global test accuracy: 83.24 

Round  52, Train loss: 1.473, Test loss: 1.493, Test accuracy: 97.06 

Round  52, Global train loss: 1.473, Global test loss: 1.633, Global test accuracy: 84.17 

Round  53, Train loss: 1.476, Test loss: 1.493, Test accuracy: 97.02 

Round  53, Global train loss: 1.476, Global test loss: 1.627, Global test accuracy: 84.76 

Round  54, Train loss: 1.473, Test loss: 1.493, Test accuracy: 97.12 

Round  54, Global train loss: 1.473, Global test loss: 1.622, Global test accuracy: 85.10 

Round  55, Train loss: 1.470, Test loss: 1.492, Test accuracy: 97.12 

Round  55, Global train loss: 1.470, Global test loss: 1.613, Global test accuracy: 85.72 

Round  56, Train loss: 1.473, Test loss: 1.491, Test accuracy: 97.18 

Round  56, Global train loss: 1.473, Global test loss: 1.630, Global test accuracy: 84.08 

Round  57, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.20 

Round  57, Global train loss: 1.472, Global test loss: 1.617, Global test accuracy: 85.64 

Round  58, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.19 

Round  58, Global train loss: 1.472, Global test loss: 1.604, Global test accuracy: 86.42 

Round  59, Train loss: 1.471, Test loss: 1.492, Test accuracy: 97.23 

Round  59, Global train loss: 1.471, Global test loss: 1.581, Global test accuracy: 89.11 

Round  60, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.17 

Round  60, Global train loss: 1.472, Global test loss: 1.586, Global test accuracy: 88.34 

Round  61, Train loss: 1.470, Test loss: 1.492, Test accuracy: 97.21 

Round  61, Global train loss: 1.470, Global test loss: 1.589, Global test accuracy: 88.48 

Round  62, Train loss: 1.472, Test loss: 1.492, Test accuracy: 97.20 

Round  62, Global train loss: 1.472, Global test loss: 1.660, Global test accuracy: 80.87 

Round  63, Train loss: 1.471, Test loss: 1.491, Test accuracy: 97.23 

Round  63, Global train loss: 1.471, Global test loss: 1.602, Global test accuracy: 87.33 

Round  64, Train loss: 1.470, Test loss: 1.491, Test accuracy: 97.26 

Round  64, Global train loss: 1.470, Global test loss: 1.597, Global test accuracy: 88.15 

Round  65, Train loss: 1.473, Test loss: 1.490, Test accuracy: 97.35 

Round  65, Global train loss: 1.473, Global test loss: 1.577, Global test accuracy: 89.33 

Round  66, Train loss: 1.473, Test loss: 1.490, Test accuracy: 97.33 

Round  66, Global train loss: 1.473, Global test loss: 1.576, Global test accuracy: 89.66 

Round  67, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.36 

Round  67, Global train loss: 1.471, Global test loss: 1.583, Global test accuracy: 88.61 

Round  68, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.39 

Round  68, Global train loss: 1.470, Global test loss: 1.586, Global test accuracy: 88.09 

Round  69, Train loss: 1.469, Test loss: 1.490, Test accuracy: 97.36 

Round  69, Global train loss: 1.469, Global test loss: 1.576, Global test accuracy: 89.34 

Round  70, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.38 

Round  70, Global train loss: 1.470, Global test loss: 1.590, Global test accuracy: 87.72 

Round  71, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.36 

Round  71, Global train loss: 1.469, Global test loss: 1.584, Global test accuracy: 88.28 

Round  72, Train loss: 1.468, Test loss: 1.490, Test accuracy: 97.33 

Round  72, Global train loss: 1.468, Global test loss: 1.575, Global test accuracy: 89.53 

Round  73, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.43 

Round  73, Global train loss: 1.470, Global test loss: 1.566, Global test accuracy: 90.78 

Round  74, Train loss: 1.467, Test loss: 1.489, Test accuracy: 97.44 

Round  74, Global train loss: 1.467, Global test loss: 1.599, Global test accuracy: 87.17 

Round  75, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.47 

Round  75, Global train loss: 1.471, Global test loss: 1.591, Global test accuracy: 88.39 

Round  76, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.39 

Round  76, Global train loss: 1.469, Global test loss: 1.572, Global test accuracy: 90.26 

Round  77, Train loss: 1.468, Test loss: 1.489, Test accuracy: 97.43 

Round  77, Global train loss: 1.468, Global test loss: 1.606, Global test accuracy: 86.46 

Round  78, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.39 

Round  78, Global train loss: 1.469, Global test loss: 1.584, Global test accuracy: 88.42 

Round  79, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.39 

Round  79, Global train loss: 1.470, Global test loss: 1.579, Global test accuracy: 88.91 

Round  80, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.43 

Round  80, Global train loss: 1.470, Global test loss: 1.564, Global test accuracy: 91.08 

Round  81, Train loss: 1.467, Test loss: 1.489, Test accuracy: 97.44 

Round  81, Global train loss: 1.467, Global test loss: 1.571, Global test accuracy: 90.08 

Round  82, Train loss: 1.468, Test loss: 1.489, Test accuracy: 97.44 

Round  82, Global train loss: 1.468, Global test loss: 1.611, Global test accuracy: 85.48 

Round  83, Train loss: 1.471, Test loss: 1.488, Test accuracy: 97.44 

Round  83, Global train loss: 1.471, Global test loss: 1.620, Global test accuracy: 84.49 

Round  84, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.46 

Round  84, Global train loss: 1.470, Global test loss: 1.570, Global test accuracy: 89.95 

Round  85, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.41 

Round  85, Global train loss: 1.470, Global test loss: 1.610, Global test accuracy: 86.23 

Round  86, Train loss: 1.468, Test loss: 1.489, Test accuracy: 97.43 

Round  86, Global train loss: 1.468, Global test loss: 1.603, Global test accuracy: 86.33 

Round  87, Train loss: 1.468, Test loss: 1.488, Test accuracy: 97.51 

Round  87, Global train loss: 1.468, Global test loss: 1.562, Global test accuracy: 90.92 

Round  88, Train loss: 1.467, Test loss: 1.488, Test accuracy: 97.48 

Round  88, Global train loss: 1.467, Global test loss: 1.571, Global test accuracy: 90.11 

Round  89, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.52 

Round  89, Global train loss: 1.470, Global test loss: 1.581, Global test accuracy: 88.36 

Round  90, Train loss: 1.468, Test loss: 1.487, Test accuracy: 97.56 

Round  90, Global train loss: 1.468, Global test loss: 1.579, Global test accuracy: 88.71 

Round  91, Train loss: 1.466, Test loss: 1.488, Test accuracy: 97.53 

Round  91, Global train loss: 1.466, Global test loss: 1.596, Global test accuracy: 87.06 

Round  92, Train loss: 1.467, Test loss: 1.488, Test accuracy: 97.49 

Round  92, Global train loss: 1.467, Global test loss: 1.570, Global test accuracy: 89.90 

Round  93, Train loss: 1.469, Test loss: 1.488, Test accuracy: 97.53 

Round  93, Global train loss: 1.469, Global test loss: 1.585, Global test accuracy: 88.03 
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.466, Test loss: 1.487, Test accuracy: 97.62 

Round  94, Global train loss: 1.466, Global test loss: 1.601, Global test accuracy: 86.65 

Round  95, Train loss: 1.469, Test loss: 1.487, Test accuracy: 97.62 

Round  95, Global train loss: 1.469, Global test loss: 1.595, Global test accuracy: 87.21 

Round  96, Train loss: 1.468, Test loss: 1.487, Test accuracy: 97.64 

Round  96, Global train loss: 1.468, Global test loss: 1.567, Global test accuracy: 90.54 

Round  97, Train loss: 1.469, Test loss: 1.488, Test accuracy: 97.58 

Round  97, Global train loss: 1.469, Global test loss: 1.557, Global test accuracy: 91.67 

Round  98, Train loss: 1.467, Test loss: 1.488, Test accuracy: 97.58 

Round  98, Global train loss: 1.467, Global test loss: 1.584, Global test accuracy: 88.23 

Round  99, Train loss: 1.467, Test loss: 1.488, Test accuracy: 97.60 

Round  99, Global train loss: 1.467, Global test loss: 1.621, Global test accuracy: 84.67 

Final Round, Train loss: 1.466, Test loss: 1.487, Test accuracy: 97.62 

Final Round, Global train loss: 1.466, Global test loss: 1.621, Global test accuracy: 84.67 

Average accuracy final 10 rounds: 97.57583333333334 

Average global accuracy final 10 rounds: 88.26750000000001 

993.5853264331818
[0.8700406551361084, 1.603689193725586, 2.378511428833008, 3.1407382488250732, 3.909593105316162, 4.712992906570435, 5.506587743759155, 6.298443555831909, 7.093909740447998, 7.876391649246216, 8.675646305084229, 9.481330633163452, 10.27492070198059, 11.083330392837524, 11.873280763626099, 12.672132730484009, 13.481078386306763, 14.27135181427002, 15.069600105285645, 15.865776538848877, 16.660616159439087, 17.459651231765747, 18.24595069885254, 19.048895835876465, 19.843117475509644, 20.632641792297363, 21.429682970046997, 22.228195190429688, 23.027665853500366, 23.822689056396484, 24.611899375915527, 25.414900302886963, 26.212282419204712, 27.015470266342163, 27.814350843429565, 28.600699186325073, 29.399048566818237, 30.198517322540283, 30.992502450942993, 31.795405387878418, 32.59154963493347, 33.38522672653198, 34.189231157302856, 34.9561653137207, 35.755719900131226, 36.524253368377686, 37.302255630493164, 38.09921455383301, 38.8920042514801, 39.689563035964966, 40.488813161849976, 41.28011894226074, 42.08150935173035, 42.888607025146484, 43.67467975616455, 44.46180295944214, 45.214863777160645, 45.96837019920349, 46.72314929962158, 47.478883504867554, 48.23836946487427, 48.99568510055542, 49.756813049316406, 50.521305561065674, 51.27639937400818, 52.04229712486267, 52.8000853061676, 53.554147481918335, 54.32059955596924, 55.085095167160034, 55.85439205169678, 56.624029874801636, 57.391528844833374, 58.1674702167511, 58.93008041381836, 59.68252968788147, 60.46108269691467, 61.22490334510803, 61.993571043014526, 62.775537729263306, 63.55088400840759, 64.31981945037842, 65.10323095321655, 65.8622567653656, 66.64070057868958, 67.4190821647644, 68.07358622550964, 68.74093174934387, 69.39773368835449, 70.0491533279419, 70.69741702079773, 71.32752513885498, 71.94458174705505, 72.56954884529114, 73.2050895690918, 73.83091974258423, 74.45107126235962, 75.08642029762268, 75.70811605453491, 76.32869076728821, 77.5909333229065]
[27.408333333333335, 57.74166666666667, 64.05833333333334, 75.54166666666667, 79.09166666666667, 86.66666666666667, 90.91666666666667, 91.96666666666667, 91.46666666666667, 92.00833333333334, 93.39166666666667, 93.85, 96.05, 96.20833333333333, 96.21666666666667, 96.225, 96.3, 96.36666666666666, 96.4, 96.35833333333333, 96.5, 96.54166666666667, 96.60833333333333, 96.725, 96.675, 96.76666666666667, 96.825, 96.75, 96.85, 96.8, 96.85833333333333, 96.91666666666667, 96.9, 96.89166666666667, 96.88333333333334, 96.80833333333334, 96.8, 96.725, 96.85833333333333, 96.81666666666666, 96.90833333333333, 96.925, 96.95, 96.925, 96.94166666666666, 97.03333333333333, 97.01666666666667, 97.09166666666667, 97.08333333333333, 97.075, 97.025, 97.05833333333334, 97.05833333333334, 97.01666666666667, 97.11666666666666, 97.125, 97.18333333333334, 97.2, 97.19166666666666, 97.23333333333333, 97.16666666666667, 97.20833333333333, 97.2, 97.23333333333333, 97.25833333333334, 97.35, 97.325, 97.35833333333333, 97.39166666666667, 97.35833333333333, 97.375, 97.35833333333333, 97.33333333333333, 97.43333333333334, 97.44166666666666, 97.46666666666667, 97.39166666666667, 97.43333333333334, 97.39166666666667, 97.39166666666667, 97.43333333333334, 97.44166666666666, 97.44166666666666, 97.44166666666666, 97.45833333333333, 97.40833333333333, 97.43333333333334, 97.50833333333334, 97.48333333333333, 97.51666666666667, 97.55833333333334, 97.53333333333333, 97.49166666666666, 97.53333333333333, 97.61666666666666, 97.625, 97.64166666666667, 97.575, 97.58333333333333, 97.6, 97.625]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.579, Test loss: 1.621, Test accuracy: 84.18
Average accuracy final 10 rounds: 84.19625 

2323.8710758686066
[0.7844748497009277, 1.5689496994018555, 2.2684483528137207, 2.967947006225586, 3.6626710891723633, 4.357395172119141, 5.058570146560669, 5.759745121002197, 6.455989837646484, 7.1522345542907715, 7.845001220703125, 8.537767887115479, 9.251843690872192, 9.965919494628906, 10.670886278152466, 11.375853061676025, 12.065821886062622, 12.755790710449219, 13.453586339950562, 14.151381969451904, 14.852152824401855, 15.552923679351807, 16.263914823532104, 16.974905967712402, 17.672372102737427, 18.36983823776245, 19.062312841415405, 19.75478744506836, 20.458956003189087, 21.163124561309814, 21.87961769104004, 22.596110820770264, 23.297679901123047, 23.99924898147583, 24.690226078033447, 25.381203174591064, 26.078238248825073, 26.775273323059082, 27.492868661880493, 28.210464000701904, 28.95501446723938, 29.699564933776855, 30.42803716659546, 31.156509399414062, 31.89997673034668, 32.6434440612793, 33.381062746047974, 34.11868143081665, 34.86041259765625, 35.60214376449585, 36.366079807281494, 37.13001585006714, 37.866520166397095, 38.60302448272705, 39.343204498291016, 40.08338451385498, 40.81589674949646, 41.54840898513794, 42.29269075393677, 43.036972522735596, 43.77032399177551, 44.50367546081543, 45.24963402748108, 45.99559259414673, 46.74679112434387, 47.497989654541016, 48.221970081329346, 48.945950508117676, 49.69344425201416, 50.440937995910645, 51.1847665309906, 51.92859506607056, 52.6591899394989, 53.389784812927246, 54.14048743247986, 54.89119005203247, 55.62537622451782, 56.359562397003174, 57.09934091567993, 57.83911943435669, 58.5820107460022, 59.324902057647705, 60.059890270233154, 60.7948784828186, 61.52605891227722, 62.25723934173584, 62.997159481048584, 63.73707962036133, 64.48736882209778, 65.23765802383423, 65.98077607154846, 66.7238941192627, 67.46457767486572, 68.20526123046875, 68.93223977088928, 69.65921831130981, 70.38073444366455, 71.10225057601929, 71.83331251144409, 72.5643744468689, 73.30901551246643, 74.05365657806396, 74.80123472213745, 75.54881286621094, 76.28582811355591, 77.02284336090088, 77.75136494636536, 78.47988653182983, 79.21481680870056, 79.94974708557129, 80.692063331604, 81.43437957763672, 82.20431637763977, 82.97425317764282, 83.72521734237671, 84.4761815071106, 85.20008873939514, 85.92399597167969, 86.65945672988892, 87.39491748809814, 88.1199414730072, 88.84496545791626, 89.601402759552, 90.35784006118774, 91.12132334709167, 91.8848066329956, 92.63032174110413, 93.37583684921265, 94.10466289520264, 94.83348894119263, 95.5547297000885, 96.27597045898438, 97.01500988006592, 97.75404930114746, 98.51462912559509, 99.27520895004272, 100.02652907371521, 100.7778491973877, 101.50100493431091, 102.22416067123413, 102.94949531555176, 103.67482995986938, 104.41363620758057, 105.15244245529175, 105.89830303192139, 106.64416360855103, 107.39549016952515, 108.14681673049927, 108.88283467292786, 109.61885261535645, 110.34327864646912, 111.06770467758179, 111.7961356639862, 112.52456665039062, 113.27231764793396, 114.0200686454773, 114.79051065444946, 115.56095266342163, 116.31266927719116, 117.0643858909607, 117.79872035980225, 118.5330548286438, 119.26281404495239, 119.99257326126099, 120.71691083908081, 121.44124841690063, 122.19666004180908, 122.95207166671753, 123.70129370689392, 124.45051574707031, 125.18638563156128, 125.92225551605225, 126.64863538742065, 127.37501525878906, 128.1034872531891, 128.8319592475891, 129.58904147148132, 130.34612369537354, 131.08348894119263, 131.82085418701172, 132.5625560283661, 133.30425786972046, 134.03801012039185, 134.77176237106323, 135.49205446243286, 136.2123465538025, 136.9633343219757, 137.71432209014893, 138.45362758636475, 139.19293308258057, 139.93902397155762, 140.68511486053467, 141.41799354553223, 142.15087223052979, 142.8745608329773, 143.5982494354248, 144.32723736763, 145.0562252998352, 145.7976438999176, 146.5390625, 147.86560368537903, 149.19214487075806]
[12.0575, 12.0575, 13.92, 13.92, 16.36, 16.36, 27.1325, 27.1325, 35.7775, 35.7775, 37.345, 37.345, 39.7925, 39.7925, 46.0475, 46.0475, 48.21, 48.21, 50.4225, 50.4225, 55.84, 55.84, 61.4175, 61.4175, 67.5175, 67.5175, 73.6675, 73.6675, 76.32, 76.32, 79.405, 79.405, 79.61, 79.61, 80.3725, 80.3725, 81.0625, 81.0625, 81.6675, 81.6675, 81.9025, 81.9025, 82.2625, 82.2625, 82.37, 82.37, 82.5225, 82.5225, 82.6675, 82.6675, 82.7725, 82.7725, 82.925, 82.925, 83.045, 83.045, 83.195, 83.195, 83.19, 83.19, 83.225, 83.225, 83.365, 83.365, 83.4025, 83.4025, 83.43, 83.43, 83.3825, 83.3825, 83.5125, 83.5125, 83.5725, 83.5725, 83.575, 83.575, 83.7225, 83.7225, 83.675, 83.675, 83.7275, 83.7275, 83.655, 83.655, 83.71, 83.71, 83.665, 83.665, 83.7575, 83.7575, 83.6975, 83.6975, 83.75, 83.75, 83.79, 83.79, 83.7825, 83.7825, 83.88, 83.88, 83.885, 83.885, 83.855, 83.855, 83.8975, 83.8975, 83.8475, 83.8475, 83.9025, 83.9025, 83.985, 83.985, 84.005, 84.005, 84.045, 84.045, 84.0125, 84.0125, 84.0075, 84.0075, 84.015, 84.015, 84.045, 84.045, 84.0825, 84.0825, 84.095, 84.095, 84.05, 84.05, 84.1525, 84.1525, 84.015, 84.015, 83.97, 83.97, 84.0, 84.0, 83.96, 83.96, 83.9625, 83.9625, 84.055, 84.055, 84.09, 84.09, 84.0025, 84.0025, 84.035, 84.035, 84.025, 84.025, 84.0625, 84.0625, 84.1075, 84.1075, 84.0775, 84.0775, 84.0825, 84.0825, 84.17, 84.17, 84.18, 84.18, 84.1825, 84.1825, 84.1175, 84.1175, 84.125, 84.125, 84.1075, 84.1075, 84.1125, 84.1125, 84.205, 84.205, 84.17, 84.17, 84.23, 84.23, 84.2275, 84.2275, 84.2475, 84.2475, 84.17, 84.17, 84.1975, 84.1975, 84.23, 84.23, 84.14, 84.14, 84.215, 84.215, 84.225, 84.225, 84.18, 84.18, 84.13, 84.13, 84.1775, 84.1775]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.572, Test loss: 1.616, Test accuracy: 84.53
Average accuracy final 10 rounds: 84.5375 

2402.7099351882935
[0.9160807132720947, 1.8321614265441895, 2.611677646636963, 3.3911938667297363, 4.163828372955322, 4.936462879180908, 5.769228458404541, 6.601994037628174, 7.419800281524658, 8.237606525421143, 9.056071996688843, 9.874537467956543, 10.706553936004639, 11.538570404052734, 12.344959259033203, 13.151348114013672, 13.967145919799805, 14.782943725585938, 15.605592727661133, 16.428241729736328, 17.245635986328125, 18.063030242919922, 18.88872742652893, 19.71442461013794, 20.53360414505005, 21.352783679962158, 22.160454750061035, 22.968125820159912, 23.79215431213379, 24.616182804107666, 25.44175934791565, 26.267335891723633, 27.09984016418457, 27.932344436645508, 28.76177191734314, 29.59119939804077, 30.41467046737671, 31.238141536712646, 32.059470653533936, 32.880799770355225, 33.69354224205017, 34.50628471374512, 35.34277081489563, 36.17925691604614, 37.00309634208679, 37.82693576812744, 38.649789571762085, 39.47264337539673, 40.30141806602478, 41.13019275665283, 41.94398212432861, 42.757771492004395, 43.579662799835205, 44.401554107666016, 45.20911407470703, 46.01667404174805, 46.84104299545288, 47.665411949157715, 48.49741816520691, 49.3294243812561, 50.16693902015686, 51.00445365905762, 51.83694505691528, 52.66943645477295, 53.483373403549194, 54.29731035232544, 55.10524845123291, 55.91318655014038, 56.73036003112793, 57.54753351211548, 58.367220640182495, 59.18690776824951, 60.03329825401306, 60.87968873977661, 61.71746039390564, 62.55523204803467, 63.372023582458496, 64.18881511688232, 65.00445222854614, 65.82008934020996, 66.63414096832275, 67.44819259643555, 68.26577186584473, 69.0833511352539, 69.90874600410461, 70.73414087295532, 71.56297278404236, 72.3918046951294, 73.22032618522644, 74.04884767532349, 74.85944533348083, 75.67004299163818, 76.49091029167175, 77.31177759170532, 78.12079644203186, 78.9298152923584, 79.75852084159851, 80.58722639083862, 81.42630791664124, 82.26538944244385, 83.08980441093445, 83.91421937942505, 84.74276328086853, 85.57130718231201, 86.38786911964417, 87.20443105697632, 88.01277256011963, 88.82111406326294, 89.64793038368225, 90.47474670410156, 91.29111886024475, 92.10749101638794, 92.92736101150513, 93.74723100662231, 94.56433272361755, 95.3814344406128, 96.19698452949524, 97.01253461837769, 97.81992077827454, 98.62730693817139, 99.44319272041321, 100.25907850265503, 101.07825469970703, 101.89743089675903, 102.71299028396606, 103.5285496711731, 104.34498190879822, 105.16141414642334, 105.97370600700378, 106.78599786758423, 107.59138512611389, 108.39677238464355, 109.20039510726929, 110.00401782989502, 110.82127451896667, 111.63853120803833, 112.46488380432129, 113.29123640060425, 114.1160569190979, 114.94087743759155, 115.72402453422546, 116.50717163085938, 117.28729009628296, 118.06740856170654, 118.85089635848999, 119.63438415527344, 120.40448689460754, 121.17458963394165, 121.98521327972412, 122.79583692550659, 123.61136031150818, 124.42688369750977, 125.224858045578, 126.02283239364624, 126.81050944328308, 127.59818649291992, 128.38842463493347, 129.17866277694702, 129.97970032691956, 130.7807378768921, 131.5932469367981, 132.4057559967041, 133.1955451965332, 133.9853343963623, 134.77598118782043, 135.56662797927856, 136.34874534606934, 137.1308627128601, 137.93373656272888, 138.73661041259766, 139.54328560829163, 140.3499608039856, 141.13932371139526, 141.92868661880493, 142.72601747512817, 143.52334833145142, 144.32312631607056, 145.1229043006897, 145.92291975021362, 146.72293519973755, 147.53554439544678, 148.348153591156, 149.1274116039276, 149.90666961669922, 150.70168447494507, 151.49669933319092, 152.29792881011963, 153.09915828704834, 153.89825797080994, 154.69735765457153, 155.5070662498474, 156.3167748451233, 157.1115686893463, 157.90636253356934, 158.6872968673706, 159.46823120117188, 160.25184655189514, 161.0354619026184, 161.83491349220276, 162.6343650817871, 163.9463131427765, 165.25826120376587]
[12.5, 12.5, 28.305, 28.305, 34.58, 34.58, 38.4575, 38.4575, 46.9525, 46.9525, 57.975, 57.975, 67.4125, 67.4125, 71.7325, 71.7325, 76.4225, 76.4225, 77.5275, 77.5275, 78.895, 78.895, 79.3675, 79.3675, 81.1925, 81.1925, 81.545, 81.545, 82.155, 82.155, 82.425, 82.425, 82.835, 82.835, 82.9325, 82.9325, 83.04, 83.04, 83.1575, 83.1575, 83.5625, 83.5625, 83.6, 83.6, 83.7975, 83.7975, 83.7625, 83.7625, 83.8225, 83.8225, 83.8725, 83.8725, 83.8275, 83.8275, 83.8625, 83.8625, 84.005, 84.005, 83.915, 83.915, 83.97, 83.97, 84.0525, 84.0525, 84.1025, 84.1025, 84.1125, 84.1125, 84.1125, 84.1125, 84.16, 84.16, 84.2325, 84.2325, 84.3125, 84.3125, 84.235, 84.235, 84.2825, 84.2825, 84.2275, 84.2275, 84.2675, 84.2675, 84.325, 84.325, 84.265, 84.265, 84.2975, 84.2975, 84.36, 84.36, 84.36, 84.36, 84.3925, 84.3925, 84.395, 84.395, 84.42, 84.42, 84.3925, 84.3925, 84.385, 84.385, 84.38, 84.38, 84.3125, 84.3125, 84.4725, 84.4725, 84.46, 84.46, 84.5075, 84.5075, 84.46, 84.46, 84.54, 84.54, 84.57, 84.57, 84.5425, 84.5425, 84.55, 84.55, 84.5175, 84.5175, 84.46, 84.46, 84.4625, 84.4625, 84.4975, 84.4975, 84.465, 84.465, 84.49, 84.49, 84.4975, 84.4975, 84.495, 84.495, 84.48, 84.48, 84.43, 84.43, 84.4725, 84.4725, 84.49, 84.49, 84.52, 84.52, 84.5, 84.5, 84.515, 84.515, 84.5075, 84.5075, 84.52, 84.52, 84.52, 84.52, 84.5525, 84.5525, 84.495, 84.495, 84.525, 84.525, 84.505, 84.505, 84.52, 84.52, 84.535, 84.535, 84.5325, 84.5325, 84.565, 84.565, 84.5625, 84.5625, 84.54, 84.54, 84.52, 84.52, 84.52, 84.52, 84.545, 84.545, 84.5475, 84.5475, 84.5725, 84.5725, 84.52, 84.52, 84.53, 84.53, 84.525, 84.525, 84.555, 84.555, 84.54, 84.54, 84.5325, 84.5325]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.478, Test loss: 1.616, Test accuracy: 84.96
Average accuracy final 10 rounds: 84.708 

2403.435688018799
[0.8537650108337402, 1.7075300216674805, 2.502964735031128, 3.2983994483947754, 4.0900092124938965, 4.881618976593018, 5.686837434768677, 6.492055892944336, 7.324731111526489, 8.157406330108643, 8.97522783279419, 9.793049335479736, 10.595471858978271, 11.397894382476807, 12.197919845581055, 12.997945308685303, 13.793043613433838, 14.588141918182373, 15.389619588851929, 16.191097259521484, 17.00416898727417, 17.817240715026855, 18.62733268737793, 19.437424659729004, 20.248859167099, 21.060293674468994, 21.862286806106567, 22.66427993774414, 23.459286212921143, 24.254292488098145, 25.077123165130615, 25.899953842163086, 26.70297622680664, 27.505998611450195, 28.28867506980896, 29.071351528167725, 29.876123905181885, 30.680896282196045, 31.489320278167725, 32.297744274139404, 33.0997200012207, 33.901695728302, 34.70045876502991, 35.49922180175781, 36.30756378173828, 37.11590576171875, 37.92232418060303, 38.728742599487305, 39.51340317726135, 40.2980637550354, 41.094664573669434, 41.89126539230347, 42.724496603012085, 43.5577278137207, 44.37517690658569, 45.192625999450684, 45.98900532722473, 46.78538465499878, 47.591898918151855, 48.39841318130493, 49.201457023620605, 50.00450086593628, 50.79697823524475, 51.58945560455322, 52.392553091049194, 53.195650577545166, 54.02224683761597, 54.84884309768677, 55.671708822250366, 56.494574546813965, 57.2980842590332, 58.10159397125244, 58.894972801208496, 59.68835163116455, 60.500168323516846, 61.31198501586914, 62.12741303443909, 62.94284105300903, 63.745558738708496, 64.54827642440796, 65.34795665740967, 66.14763689041138, 66.95681166648865, 67.76598644256592, 68.56523728370667, 69.36448812484741, 70.1505401134491, 70.93659210205078, 71.74613356590271, 72.55567502975464, 73.36120080947876, 74.16672658920288, 74.95749688148499, 75.74826717376709, 76.54957127571106, 77.35087537765503, 78.1518189907074, 78.95276260375977, 79.75981903076172, 80.56687545776367, 81.36782336235046, 82.16877126693726, 82.9547598361969, 83.74074840545654, 84.51967358589172, 85.2985987663269, 86.09271049499512, 86.88682222366333, 87.70238590240479, 88.51794958114624, 89.33366775512695, 90.14938592910767, 90.96889209747314, 91.78839826583862, 92.57371258735657, 93.35902690887451, 94.14932489395142, 94.93962287902832, 95.74673557281494, 96.55384826660156, 97.3528950214386, 98.15194177627563, 98.96632981300354, 99.78071784973145, 100.59728741645813, 101.41385698318481, 102.21325278282166, 103.0126485824585, 103.81352281570435, 104.6143970489502, 105.39231395721436, 106.17023086547852, 106.97264575958252, 107.77506065368652, 108.58755326271057, 109.40004587173462, 110.19742131233215, 110.99479675292969, 111.7902283668518, 112.58565998077393, 113.38986301422119, 114.19406604766846, 114.99918627738953, 115.8043065071106, 116.61723065376282, 117.43015480041504, 118.21904182434082, 119.0079288482666, 119.78622817993164, 120.56452751159668, 121.36994004249573, 122.17535257339478, 122.97716188430786, 123.77897119522095, 124.60154128074646, 125.42411136627197, 126.23403429985046, 127.04395723342896, 127.82964134216309, 128.61532545089722, 129.41292238235474, 130.21051931381226, 131.00613236427307, 131.8017454147339, 132.5906264781952, 133.3795075416565, 134.2004029750824, 135.0212984085083, 135.83880352973938, 136.65630865097046, 137.46520042419434, 138.2740921974182, 139.07706713676453, 139.88004207611084, 140.67056441307068, 141.46108675003052, 142.25328421592712, 143.04548168182373, 143.85097646713257, 144.6564712524414, 145.4570620059967, 146.257652759552, 147.05390787124634, 147.85016298294067, 148.65104866027832, 149.45193433761597, 150.24287128448486, 151.03380823135376, 151.83076310157776, 152.62771797180176, 153.42586541175842, 154.2240128517151, 155.00147008895874, 155.7789273262024, 156.58021569252014, 157.3815040588379, 158.18278527259827, 158.98406648635864, 159.7771360874176, 160.57020568847656, 162.06224036216736, 163.55427503585815]
[16.9025, 16.9025, 37.91, 37.91, 43.7675, 43.7675, 44.195, 44.195, 38.9275, 38.9275, 40.2375, 40.2375, 47.5675, 47.5675, 60.3775, 60.3775, 70.8575, 70.8575, 73.985, 73.985, 76.005, 76.005, 76.3825, 76.3825, 77.5275, 77.5275, 77.6675, 77.6675, 77.785, 77.785, 78.51, 78.51, 78.7275, 78.7275, 79.5475, 79.5475, 79.6375, 79.6375, 79.6825, 79.6825, 79.84, 79.84, 79.8975, 79.8975, 79.9925, 79.9925, 80.0325, 80.0325, 80.08, 80.08, 80.0925, 80.0925, 80.2075, 80.2075, 80.1825, 80.1825, 80.1975, 80.1975, 80.19, 80.19, 80.2575, 80.2575, 80.2225, 80.2225, 80.2375, 80.2375, 80.27, 80.27, 80.275, 80.275, 80.2775, 80.2775, 80.285, 80.285, 80.2875, 80.2875, 80.3125, 80.3125, 80.3025, 80.3025, 80.325, 80.325, 80.295, 80.295, 80.2825, 80.2825, 80.2675, 80.2675, 80.2625, 80.2625, 80.3075, 80.3075, 80.285, 80.285, 80.2675, 80.2675, 80.2725, 80.2725, 80.2475, 80.2475, 80.21, 80.21, 80.22, 80.22, 80.225, 80.225, 80.2475, 80.2475, 80.23, 80.23, 80.2125, 80.2125, 80.1675, 80.1675, 80.1825, 80.1825, 80.2075, 80.2075, 80.22, 80.22, 80.2275, 80.2275, 80.2325, 80.2325, 80.2225, 80.2225, 80.235, 80.235, 80.2425, 80.2425, 80.2425, 80.2425, 80.24, 80.24, 80.23, 80.23, 80.2375, 80.2375, 80.2125, 80.2125, 80.2175, 80.2175, 80.21, 80.21, 80.205, 80.205, 80.215, 80.215, 80.2225, 80.2225, 80.22, 80.22, 80.2325, 80.2325, 80.22, 80.22, 80.2325, 80.2325, 80.225, 80.225, 80.22, 80.22, 80.225, 80.225, 80.2175, 80.2175, 81.2525, 81.2525, 83.25, 83.25, 83.5775, 83.5775, 83.755, 83.755, 83.89, 83.89, 83.9925, 83.9925, 84.1075, 84.1075, 84.3925, 84.3925, 84.5275, 84.5275, 84.6075, 84.6075, 84.68, 84.68, 84.765, 84.765, 84.7925, 84.7925, 84.8175, 84.8175, 84.7975, 84.7975, 84.825, 84.825, 84.875, 84.875, 84.96, 84.96]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.528, Test loss: 1.957, Test accuracy: 64.97
Round   1, Train loss: 1.244, Test loss: 1.730, Test accuracy: 80.07
Round   2, Train loss: 1.201, Test loss: 1.683, Test accuracy: 85.17
Round   3, Train loss: 1.156, Test loss: 1.664, Test accuracy: 86.55
Round   4, Train loss: 1.147, Test loss: 1.652, Test accuracy: 87.63
Round   5, Train loss: 1.151, Test loss: 1.642, Test accuracy: 89.18
Round   6, Train loss: 1.138, Test loss: 1.638, Test accuracy: 89.44
Round   7, Train loss: 1.131, Test loss: 1.636, Test accuracy: 89.44
Round   8, Train loss: 1.129, Test loss: 1.634, Test accuracy: 89.49
Round   9, Train loss: 1.131, Test loss: 1.633, Test accuracy: 90.31
Round  10, Train loss: 1.125, Test loss: 1.632, Test accuracy: 90.47
Round  11, Train loss: 1.121, Test loss: 1.632, Test accuracy: 90.38
Round  12, Train loss: 1.121, Test loss: 1.633, Test accuracy: 90.44
Round  13, Train loss: 1.121, Test loss: 1.632, Test accuracy: 90.51
Round  14, Train loss: 1.120, Test loss: 1.633, Test accuracy: 90.35
Round  15, Train loss: 1.117, Test loss: 1.634, Test accuracy: 90.21
Round  16, Train loss: 1.116, Test loss: 1.633, Test accuracy: 90.37
Round  17, Train loss: 1.116, Test loss: 1.633, Test accuracy: 90.40
Round  18, Train loss: 1.113, Test loss: 1.634, Test accuracy: 90.40
Round  19, Train loss: 1.113, Test loss: 1.636, Test accuracy: 90.11
Round  20, Train loss: 1.113, Test loss: 1.637, Test accuracy: 90.04
Round  21, Train loss: 1.112, Test loss: 1.637, Test accuracy: 90.03
Round  22, Train loss: 1.111, Test loss: 1.638, Test accuracy: 89.92
Round  23, Train loss: 1.112, Test loss: 1.639, Test accuracy: 89.89
Round  24, Train loss: 1.111, Test loss: 1.639, Test accuracy: 89.75
Round  25, Train loss: 1.108, Test loss: 1.640, Test accuracy: 89.68
Round  26, Train loss: 1.111, Test loss: 1.641, Test accuracy: 89.56
Round  27, Train loss: 1.108, Test loss: 1.641, Test accuracy: 89.53
Round  28, Train loss: 1.108, Test loss: 1.643, Test accuracy: 89.43
Round  29, Train loss: 1.107, Test loss: 1.643, Test accuracy: 89.38
Round  30, Train loss: 1.108, Test loss: 1.643, Test accuracy: 89.31
Round  31, Train loss: 1.108, Test loss: 1.644, Test accuracy: 89.23
Round  32, Train loss: 1.107, Test loss: 1.646, Test accuracy: 89.05
Round  33, Train loss: 1.105, Test loss: 1.646, Test accuracy: 89.04
Round  34, Train loss: 1.106, Test loss: 1.647, Test accuracy: 88.89
Round  35, Train loss: 1.107, Test loss: 1.648, Test accuracy: 88.80
Round  36, Train loss: 1.105, Test loss: 1.649, Test accuracy: 88.70
Round  37, Train loss: 1.107, Test loss: 1.650, Test accuracy: 88.66
Round  38, Train loss: 1.106, Test loss: 1.651, Test accuracy: 88.59
Round  39, Train loss: 1.106, Test loss: 1.652, Test accuracy: 88.53
Round  40, Train loss: 1.105, Test loss: 1.653, Test accuracy: 88.46
Round  41, Train loss: 1.106, Test loss: 1.653, Test accuracy: 88.32
Round  42, Train loss: 1.105, Test loss: 1.654, Test accuracy: 88.14
Round  43, Train loss: 1.105, Test loss: 1.655, Test accuracy: 88.14
Round  44, Train loss: 1.105, Test loss: 1.658, Test accuracy: 87.81
Round  45, Train loss: 1.104, Test loss: 1.658, Test accuracy: 87.71
Round  46, Train loss: 1.105, Test loss: 1.658, Test accuracy: 87.83
Round  47, Train loss: 1.105, Test loss: 1.658, Test accuracy: 87.71
Round  48, Train loss: 1.104, Test loss: 1.659, Test accuracy: 87.67
Round  49, Train loss: 1.104, Test loss: 1.660, Test accuracy: 87.65
Round  50, Train loss: 1.103, Test loss: 1.661, Test accuracy: 87.55
Round  51, Train loss: 1.105, Test loss: 1.662, Test accuracy: 87.49
Round  52, Train loss: 1.104, Test loss: 1.662, Test accuracy: 87.44
Round  53, Train loss: 1.104, Test loss: 1.663, Test accuracy: 87.36
Round  54, Train loss: 1.104, Test loss: 1.663, Test accuracy: 87.28
Round  55, Train loss: 1.103, Test loss: 1.664, Test accuracy: 87.25
Round  56, Train loss: 1.103, Test loss: 1.665, Test accuracy: 87.19
Round  57, Train loss: 1.104, Test loss: 1.667, Test accuracy: 86.99
Round  58, Train loss: 1.103, Test loss: 1.667, Test accuracy: 86.96
Round  59, Train loss: 1.103, Test loss: 1.668, Test accuracy: 86.92
Round  60, Train loss: 1.104, Test loss: 1.668, Test accuracy: 86.87
Round  61, Train loss: 1.104, Test loss: 1.669, Test accuracy: 86.74
Round  62, Train loss: 1.103, Test loss: 1.669, Test accuracy: 86.72
Round  63, Train loss: 1.104, Test loss: 1.669, Test accuracy: 86.70
Round  64, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.64
Round  65, Train loss: 1.103, Test loss: 1.670, Test accuracy: 86.59
Round  66, Train loss: 1.103, Test loss: 1.671, Test accuracy: 86.56
Round  67, Train loss: 1.103, Test loss: 1.671, Test accuracy: 86.53
Round  68, Train loss: 1.102, Test loss: 1.672, Test accuracy: 86.46
Round  69, Train loss: 1.103, Test loss: 1.673, Test accuracy: 86.41
Round  70, Train loss: 1.103, Test loss: 1.674, Test accuracy: 86.33
Round  71, Train loss: 1.103, Test loss: 1.674, Test accuracy: 86.26
Round  72, Train loss: 1.102, Test loss: 1.674, Test accuracy: 86.26
Round  73, Train loss: 1.102, Test loss: 1.675, Test accuracy: 86.19
Round  74, Train loss: 1.103, Test loss: 1.676, Test accuracy: 86.12
Round  75, Train loss: 1.102, Test loss: 1.677, Test accuracy: 86.07
Round  76, Train loss: 1.103, Test loss: 1.677, Test accuracy: 86.01
Round  77, Train loss: 1.102, Test loss: 1.678, Test accuracy: 85.91
Round  78, Train loss: 1.103, Test loss: 1.678, Test accuracy: 85.89
Round  79, Train loss: 1.102, Test loss: 1.679, Test accuracy: 85.83
Round  80, Train loss: 1.102, Test loss: 1.680, Test accuracy: 85.74
Round  81, Train loss: 1.102, Test loss: 1.680, Test accuracy: 85.72
Round  82, Train loss: 1.102, Test loss: 1.680, Test accuracy: 85.68
Round  83, Train loss: 1.102, Test loss: 1.681, Test accuracy: 85.60
Round  84, Train loss: 1.102, Test loss: 1.683, Test accuracy: 85.51
Round  85, Train loss: 1.102, Test loss: 1.682, Test accuracy: 85.54
Round  86, Train loss: 1.103, Test loss: 1.683, Test accuracy: 85.55
Round  87, Train loss: 1.102, Test loss: 1.683, Test accuracy: 85.52
Round  88, Train loss: 1.102, Test loss: 1.685, Test accuracy: 85.41
Round  89, Train loss: 1.103, Test loss: 1.685, Test accuracy: 85.33
Round  90, Train loss: 1.102, Test loss: 1.686, Test accuracy: 85.31
Round  91, Train loss: 1.102, Test loss: 1.686, Test accuracy: 85.28
Round  92, Train loss: 1.102, Test loss: 1.686, Test accuracy: 85.22
Round  93, Train loss: 1.101, Test loss: 1.687, Test accuracy: 85.17
Round  94, Train loss: 1.102, Test loss: 1.687, Test accuracy: 85.22
Round  95, Train loss: 1.102, Test loss: 1.688, Test accuracy: 85.05
Round  96, Train loss: 1.102, Test loss: 1.688, Test accuracy: 85.17
Round  97, Train loss: 1.102, Test loss: 1.689, Test accuracy: 85.07
Round  98, Train loss: 1.102, Test loss: 1.689, Test accuracy: 85.07
Round  99, Train loss: 1.102, Test loss: 1.690, Test accuracy: 84.94
Final Round, Train loss: 1.102, Test loss: 1.692, Test accuracy: 84.84
Average accuracy final 10 rounds: 85.14975
4505.9217121601105
[]
[64.9675, 80.07, 85.1725, 86.545, 87.6275, 89.1825, 89.4375, 89.4425, 89.49, 90.315, 90.4725, 90.38, 90.4375, 90.5075, 90.35, 90.2125, 90.37, 90.3975, 90.4, 90.1125, 90.0425, 90.0275, 89.9225, 89.8875, 89.755, 89.6825, 89.565, 89.5325, 89.43, 89.375, 89.3125, 89.23, 89.0525, 89.0425, 88.885, 88.8025, 88.7, 88.66, 88.59, 88.525, 88.46, 88.32, 88.1375, 88.1375, 87.805, 87.71, 87.8275, 87.71, 87.665, 87.6475, 87.5475, 87.4925, 87.44, 87.3575, 87.275, 87.245, 87.19, 86.9925, 86.9575, 86.915, 86.8675, 86.7375, 86.7175, 86.7, 86.635, 86.59, 86.5575, 86.535, 86.46, 86.41, 86.3275, 86.26, 86.2625, 86.195, 86.1175, 86.0675, 86.0075, 85.9125, 85.895, 85.825, 85.74, 85.7175, 85.6775, 85.6025, 85.5125, 85.5375, 85.5525, 85.52, 85.41, 85.335, 85.305, 85.28, 85.2175, 85.17, 85.225, 85.0475, 85.165, 85.0725, 85.0725, 84.9425, 84.8425]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.503, Test loss: 1.530, Test accuracy: 93.58
Average accuracy final 10 rounds: 91.98675
Average global accuracy final 10 rounds: 91.98675
3458.612407684326
[]
[19.915, 24.88, 35.9575, 44.54, 59.385, 64.1875, 68.3175, 69.4325, 72.835, 72.7575, 77.1975, 78.97, 79.755, 80.3325, 84.0075, 85.1175, 85.315, 85.255, 84.185, 84.65, 86.8625, 86.915, 87.0075, 86.585, 86.605, 86.28, 86.245, 85.7275, 86.1825, 86.7825, 85.5375, 85.4125, 86.94, 87.5825, 86.9725, 87.64, 89.765, 90.01, 89.825, 90.02, 90.005, 90.065, 90.1825, 90.3475, 90.815, 90.8925, 90.9525, 90.845, 90.71, 90.795, 90.9725, 90.985, 90.8675, 91.0, 91.005, 91.025, 91.345, 91.5325, 91.2725, 91.4225, 91.115, 91.17, 91.0525, 91.0475, 91.125, 91.1625, 91.375, 91.3825, 91.425, 91.52, 91.42, 91.395, 91.56, 91.585, 91.635, 91.55, 91.3925, 91.6125, 91.6125, 91.79, 91.885, 91.795, 91.73, 91.86, 92.155, 92.06, 92.0225, 91.9925, 92.0225, 92.0325, 92.17, 92.0425, 92.0, 91.97, 91.9775, 91.945, 91.8525, 91.8375, 91.9925, 92.08, 93.5775]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.302, Test loss: 2.302, Test accuracy: 14.34
Final Round, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 14.25
Average accuracy final 10 rounds: 13.83725 

Average global accuracy final 10 rounds: 13.98425 

2471.794018983841
[1.0149385929107666, 1.9038808345794678, 2.7922708988189697, 3.676051139831543, 4.555256605148315, 5.439869403839111, 6.325283765792847, 7.2064619064331055, 8.080088376998901, 8.94821310043335, 9.803434133529663, 10.660604238510132, 11.530938148498535, 12.386327266693115, 13.279855251312256, 14.175041198730469, 15.051678895950317, 15.934123277664185, 16.817743062973022, 17.692659378051758, 18.576471090316772, 19.47135615348816, 20.361267566680908, 21.24193239212036, 22.137234449386597, 23.014517784118652, 23.896044492721558, 24.779398918151855, 25.663256645202637, 26.550624132156372, 27.429555892944336, 28.33056354522705, 29.24403142929077, 30.151829719543457, 31.055550813674927, 31.969517946243286, 32.86908745765686, 33.79083037376404, 34.703046560287476, 35.62716269493103, 36.542423725128174, 37.46236181259155, 38.25501871109009, 39.06263780593872, 39.881834268569946, 40.686362981796265, 41.47169542312622, 42.278403520584106, 43.08158564567566, 43.89094042778015, 44.698328495025635, 45.50449776649475, 46.30840229988098, 47.12361764907837, 47.90302133560181, 48.71686315536499, 49.52695178985596, 50.328826904296875, 51.13406753540039, 51.94114351272583, 52.74291181564331, 53.54424476623535, 54.321868658065796, 55.12362051010132, 55.93230652809143, 56.730767488479614, 57.53868079185486, 58.34417915344238, 59.14845538139343, 59.96133542060852, 60.74516677856445, 61.544533252716064, 62.34787368774414, 63.14794397354126, 63.94837713241577, 64.77105069160461, 65.57024097442627, 66.35770440101624, 67.13466143608093, 67.93750023841858, 68.74864149093628, 69.54766941070557, 70.3411214351654, 71.1480770111084, 71.9520013332367, 72.7635908126831, 73.55503726005554, 74.35880184173584, 75.14837861061096, 75.93775010108948, 76.74953174591064, 77.55651187896729, 78.36077070236206, 79.1527087688446, 79.97569751739502, 80.78487062454224, 81.58494782447815, 82.39180588722229, 83.20182371139526, 84.01172018051147, 85.64556765556335]
[9.24, 9.36, 9.3925, 9.4, 9.4525, 9.45, 9.475, 9.53, 9.5775, 9.625, 9.7025, 9.68, 9.7725, 9.845, 9.8775, 9.8825, 9.92, 9.945, 9.9775, 10.045, 10.06, 10.0875, 10.13, 10.1525, 10.1925, 10.265, 10.3375, 10.3975, 10.4025, 10.4875, 10.4975, 10.555, 10.5925, 10.6575, 10.735, 10.8125, 10.8875, 10.8975, 10.895, 10.965, 11.0325, 11.045, 11.0575, 11.095, 11.0825, 11.1125, 11.16, 11.215, 11.255, 11.29, 11.3325, 11.375, 11.4475, 11.455, 11.47, 11.5525, 11.61, 11.6525, 11.7225, 11.7275, 11.725, 11.755, 11.7775, 11.8275, 11.87, 11.9, 11.9525, 12.0075, 12.0575, 12.1375, 12.185, 12.2275, 12.305, 12.33, 12.39, 12.435, 12.4725, 12.52, 12.53, 12.63, 12.7625, 12.81, 12.905, 12.9475, 13.125, 13.155, 13.25, 13.3425, 13.43, 13.4975, 13.575, 13.62, 13.705, 13.7525, 13.8025, 13.82, 13.9125, 13.9975, 14.0475, 14.14, 14.335]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Final Round, Train loss: 1.482, Test loss: 1.521, Test accuracy: 94.22
Average accuracy final 10 rounds: 94.04950000000001
2401.731211423874
[2.2074179649353027, 4.436411142349243, 6.676523685455322, 8.954126834869385, 11.228010892868042, 13.485527276992798, 15.754892110824585, 18.023475646972656, 20.317655086517334, 22.590395212173462, 24.860499382019043, 27.133333444595337, 29.40633249282837, 31.684174299240112, 33.96005463600159, 36.21576404571533, 38.48118495941162, 40.7448456287384, 43.010021924972534, 45.288559436798096, 47.53268909454346, 49.791659116744995, 52.06727385520935, 54.366519927978516, 56.65123009681702, 58.93690013885498, 61.204789876937866, 63.470094442367554, 65.75704026222229, 68.0254008769989, 70.29711389541626, 72.55688047409058, 74.84642338752747, 77.1247935295105, 79.40209245681763, 81.68299794197083, 83.95115518569946, 86.2260570526123, 88.51846385002136, 90.80111002922058, 93.08008074760437, 95.34023785591125, 97.61633348464966, 99.90406799316406, 102.19451808929443, 104.47665929794312, 106.7452974319458, 109.00193977355957, 111.28986811637878, 113.58823490142822, 115.86863207817078, 118.13514137268066, 120.41072416305542, 122.6926257610321, 124.97520279884338, 127.25239062309265, 129.51863741874695, 131.775737285614, 134.06604623794556, 136.35576915740967, 138.64377355575562, 140.91541719436646, 143.1865315437317, 145.47801661491394, 147.7674536705017, 150.0525221824646, 152.3199291229248, 154.58106541633606, 156.8792736530304, 159.1749963760376, 161.465069770813, 163.73624682426453, 165.99909496307373, 168.29712748527527, 170.5948827266693, 172.8729271888733, 175.13309288024902, 177.39722347259521, 179.6801881790161, 181.9505479335785, 184.22738599777222, 186.499338388443, 188.75751996040344, 191.03209280967712, 193.31514692306519, 195.6004912853241, 197.87595963478088, 200.13711619377136, 202.42697739601135, 204.68286895751953, 206.9644317626953, 209.2369523048401, 211.50616073608398, 213.76793003082275, 216.0442578792572, 218.32222199440002, 220.60041999816895, 222.89247751235962, 225.1788182258606, 227.44909572601318, 229.7394232749939]
[10.385, 10.69, 21.035, 38.0175, 41.3325, 55.93, 66.365, 72.7775, 74.5125, 75.45, 75.5225, 80.45, 81.2975, 81.9825, 82.6325, 82.7925, 82.9725, 83.25, 83.3675, 83.475, 83.7125, 84.055, 84.08, 84.1325, 84.155, 84.19, 84.635, 84.505, 84.425, 84.6625, 84.745, 84.7525, 84.835, 84.8625, 84.8225, 85.005, 84.9275, 84.835, 85.1875, 85.2525, 85.2325, 85.3, 85.345, 85.3575, 85.5475, 85.505, 85.56, 85.5175, 85.5075, 85.5825, 85.6425, 85.57, 85.6125, 85.62, 85.6725, 85.5075, 85.6875, 85.725, 85.74, 85.8075, 85.77, 85.98, 85.825, 85.8525, 85.955, 85.8725, 85.9875, 90.3875, 92.2175, 92.4525, 92.6575, 92.81, 92.9475, 93.135, 93.1725, 93.3375, 93.4275, 93.5575, 93.4375, 93.6325, 93.6175, 93.67, 93.6575, 93.685, 93.6925, 93.795, 93.895, 93.9475, 93.86, 93.8975, 93.8675, 93.985, 94.03, 93.9725, 93.925, 93.9325, 94.2525, 94.2025, 94.2125, 94.115, 94.2225]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 2.296, Test loss: 2.305, Test accuracy: 9.55
Average accuracy final 10 rounds: 9.450500000000002
1583.2576467990875
[1.1030621528625488, 2.118122100830078, 3.000331163406372, 3.9241058826446533, 4.8255228996276855, 5.753021955490112, 6.642826080322266, 7.557981491088867, 8.481346607208252, 9.384801864624023, 10.264042139053345, 11.153800964355469, 12.068356275558472, 12.96337366104126, 13.84795331954956, 14.74367380142212, 15.662758350372314, 16.569151401519775, 17.463730096817017, 18.360064029693604, 19.28169846534729, 20.176026582717896, 21.09602451324463, 21.972625494003296, 22.8673574924469, 23.744994401931763, 24.651493787765503, 25.544506788253784, 26.517029523849487, 27.48173761367798, 28.46596074104309, 29.474153518676758, 30.45807719230652, 31.46157956123352, 32.45327925682068, 33.46835923194885, 34.44241261482239, 35.45964431762695, 36.42950081825256, 37.443816900253296, 38.39237332344055, 39.403254985809326, 40.385162115097046, 41.369750022888184, 42.3180787563324, 43.29900240898132, 44.30413055419922, 45.284871101379395, 46.294111490249634, 47.25785994529724, 48.28414511680603, 49.234477281570435, 50.24494981765747, 51.212730884552, 52.2201714515686, 53.17049598693848, 54.15163612365723, 55.14043712615967, 56.13027334213257, 57.132819175720215, 58.12493920326233, 59.13010549545288, 60.097633600234985, 61.10403752326965, 62.074888706207275, 63.08371353149414, 64.0163881778717, 65.0267686843872, 66.00057911872864, 67.00041556358337, 67.98234677314758, 68.9675543308258, 69.9495484828949, 70.88980340957642, 71.87250065803528, 72.84984850883484, 73.83124232292175, 74.77348327636719, 75.75546360015869, 76.744473695755, 77.71957612037659, 78.67012739181519, 79.63433456420898, 80.62125706672668, 81.58872032165527, 82.57663989067078, 83.54284620285034, 84.53770661354065, 85.50304079055786, 86.49315524101257, 87.47453927993774, 88.43888854980469, 89.4045979976654, 90.37406849861145, 91.39025449752808, 92.35871148109436, 93.37743353843689, 94.34215831756592, 95.3594479560852, 96.32635450363159, 97.81187725067139]
[8.6125, 8.1075, 8.3075, 8.0825, 8.0, 7.865, 8.0875, 8.775, 8.9925, 8.9275, 8.9225, 8.93, 8.705, 9.275, 9.2225, 9.3, 9.43, 9.39, 9.0425, 9.0825, 9.03, 9.0775, 9.1575, 9.2275, 9.26, 9.1825, 9.365, 9.165, 9.255, 9.245, 9.38, 9.38, 9.3675, 9.5625, 9.635, 9.6125, 9.73, 9.6125, 9.73, 9.775, 9.7525, 9.7825, 9.86, 9.735, 9.8125, 9.675, 9.5025, 9.625, 9.595, 9.7325, 9.6525, 9.685, 9.68, 9.74, 9.73, 9.74, 9.65, 9.655, 9.7275, 9.7575, 9.785, 9.685, 9.775, 9.8125, 9.8325, 9.84, 9.7925, 9.765, 9.635, 9.75, 9.6875, 9.7175, 9.715, 9.675, 9.675, 9.6225, 9.56, 9.5575, 9.4775, 9.4125, 9.2725, 9.24, 9.2775, 9.335, 9.1175, 9.17, 9.2325, 9.42, 9.4, 9.4025, 9.3875, 9.37, 9.375, 9.4075, 9.4025, 9.39, 9.555, 9.485, 9.555, 9.5775, 9.5525]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.494, Test loss: 1.525, Test accuracy: 96.33
Average accuracy final 10 rounds: 96.27725
2530.9865934848785
[1.1149065494537354, 2.2298130989074707, 3.271862030029297, 4.313910961151123, 5.349022150039673, 6.384133338928223, 7.420215845108032, 8.456298351287842, 9.4952712059021, 10.534244060516357, 11.586518287658691, 12.638792514801025, 13.726332902908325, 14.813873291015625, 15.861985683441162, 16.9100980758667, 17.84084987640381, 18.771601676940918, 19.70081877708435, 20.630035877227783, 21.514437198638916, 22.39883852005005, 23.316200256347656, 24.233561992645264, 25.14311909675598, 26.0526762008667, 26.981855154037476, 27.911034107208252, 28.867191553115845, 29.823348999023438, 30.77582287788391, 31.728296756744385, 32.661863565444946, 33.59543037414551, 34.52030158042908, 35.44517278671265, 36.34247422218323, 37.23977565765381, 38.13699197769165, 39.03420829772949, 39.948954582214355, 40.86370086669922, 41.847474575042725, 42.83124828338623, 43.82296586036682, 44.81468343734741, 45.71160578727722, 46.60852813720703, 47.50748634338379, 48.40644454956055, 49.27913522720337, 50.15182590484619, 51.02152919769287, 51.89123249053955, 52.75912928581238, 53.627026081085205, 54.52842998504639, 55.42983388900757, 56.31652498245239, 57.20321607589722, 58.136364698410034, 59.06951332092285, 59.94475197792053, 60.81999063491821, 61.71898818016052, 62.61798572540283, 63.508147954940796, 64.39831018447876, 65.3547465801239, 66.31118297576904, 67.18842101097107, 68.0656590461731, 68.9287040233612, 69.79174900054932, 70.78696179389954, 71.78217458724976, 72.75946879386902, 73.73676300048828, 74.72544860839844, 75.7141342163086, 76.69856286048889, 77.68299150466919, 78.65098094940186, 79.61897039413452, 80.5892596244812, 81.55954885482788, 82.51523399353027, 83.47091913223267, 84.43483781814575, 85.39875650405884, 86.36623907089233, 87.33372163772583, 88.31374931335449, 89.29377698898315, 90.2896888256073, 91.28560066223145, 92.24960494041443, 93.21360921859741, 94.18949961662292, 95.16539001464844, 96.15924119949341, 97.15309238433838, 98.10806512832642, 99.06303787231445, 100.02812695503235, 100.99321603775024, 101.94381022453308, 102.89440441131592, 103.87680697441101, 104.8592095375061, 105.85629963874817, 106.85338973999023, 107.8245050907135, 108.79562044143677, 109.77610421180725, 110.75658798217773, 111.70959544181824, 112.66260290145874, 113.63867902755737, 114.614755153656, 115.60571575164795, 116.59667634963989, 117.58914709091187, 118.58161783218384, 119.55218195915222, 120.5227460861206, 121.40453624725342, 122.28632640838623, 123.16992592811584, 124.05352544784546, 124.93324780464172, 125.81297016143799, 126.72798299789429, 127.64299583435059, 128.5462052822113, 129.44941473007202, 130.36582970619202, 131.282244682312, 132.2176477909088, 133.15305089950562, 134.04557013511658, 134.93808937072754, 135.82257604599, 136.70706272125244, 137.59723329544067, 138.4874038696289, 139.3617136478424, 140.2360234260559, 141.12181186676025, 142.0076003074646, 142.8932831287384, 143.7789659500122, 144.69943046569824, 145.61989498138428, 146.55469703674316, 147.48949909210205, 148.38450574874878, 149.2795124053955, 150.16049695014954, 151.04148149490356, 151.91055941581726, 152.77963733673096, 153.66008353233337, 154.5405297279358, 155.43928217887878, 156.33803462982178, 157.24924397468567, 158.16045331954956, 159.05915236473083, 159.9578514099121, 160.8346769809723, 161.71150255203247, 162.61616230010986, 163.52082204818726, 164.41204404830933, 165.3032660484314, 166.1901319026947, 167.076997756958, 167.96796321868896, 168.85892868041992, 169.73568391799927, 170.6124391555786, 171.51453685760498, 172.41663455963135, 173.31362128257751, 174.21060800552368, 175.11110186576843, 176.01159572601318, 176.91314697265625, 177.81469821929932, 178.71346855163574, 179.61223888397217, 180.48354196548462, 181.35484504699707, 182.21777439117432, 183.08070373535156, 183.96911644935608, 184.8575291633606, 185.72491836547852, 186.59230756759644, 188.0015094280243, 189.41071128845215]
[13.7025, 13.7025, 11.27, 11.27, 13.05, 13.05, 21.49, 21.49, 30.905, 30.905, 35.095, 35.095, 46.53, 46.53, 54.365, 54.365, 62.4625, 62.4625, 65.43, 65.43, 68.515, 68.515, 73.2825, 73.2825, 77.3425, 77.3425, 79.82, 79.82, 80.59, 80.59, 81.4625, 81.4625, 81.935, 81.935, 82.8875, 82.8875, 83.1975, 83.1975, 83.5625, 83.5625, 83.8275, 83.8275, 84.0975, 84.0975, 84.33, 84.33, 84.6225, 84.6225, 84.8825, 84.8825, 85.15, 85.15, 85.4325, 85.4325, 85.785, 85.785, 86.66, 86.66, 87.9825, 87.9825, 89.6575, 89.6575, 90.585, 90.585, 91.48, 91.48, 91.81, 91.81, 92.1325, 92.1325, 92.61, 92.61, 92.83, 92.83, 92.9325, 92.9325, 93.2425, 93.2425, 93.4525, 93.4525, 93.59, 93.59, 93.85, 93.85, 93.965, 93.965, 94.195, 94.195, 94.2475, 94.2475, 94.29, 94.29, 94.42, 94.42, 94.4975, 94.4975, 94.6325, 94.6325, 94.7, 94.7, 94.675, 94.675, 94.765, 94.765, 94.815, 94.815, 94.83, 94.83, 94.9675, 94.9675, 94.9175, 94.9175, 95.005, 95.005, 95.095, 95.095, 95.12, 95.12, 95.1725, 95.1725, 95.33, 95.33, 95.3375, 95.3375, 95.3875, 95.3875, 95.4025, 95.4025, 95.53, 95.53, 95.5075, 95.5075, 95.5675, 95.5675, 95.57, 95.57, 95.68, 95.68, 95.63, 95.63, 95.665, 95.665, 95.7675, 95.7675, 95.785, 95.785, 95.7725, 95.7725, 95.77, 95.77, 95.7525, 95.7525, 95.8525, 95.8525, 95.935, 95.935, 95.9775, 95.9775, 95.9375, 95.9375, 96.0275, 96.0275, 96.0825, 96.0825, 96.05, 96.05, 96.035, 96.035, 96.045, 96.045, 96.1675, 96.1675, 96.09, 96.09, 96.15, 96.15, 96.16, 96.16, 96.2275, 96.2275, 96.2325, 96.2325, 96.165, 96.165, 96.2175, 96.2175, 96.2125, 96.2125, 96.3125, 96.3125, 96.3175, 96.3175, 96.335, 96.335, 96.3125, 96.3125, 96.3325, 96.3325, 96.335, 96.335, 96.335, 96.335]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.465, Test loss: 1.502, Test accuracy: 96.04
Final Round, Global train loss: 1.465, Global test loss: 2.011, Global test accuracy: 44.42
Average accuracy final 10 rounds: 96.05083333333336 

Average global accuracy final 10 rounds: 41.084166666666675 

1330.5368626117706
[0.8915126323699951, 1.7830252647399902, 2.598970890045166, 3.414916515350342, 4.2295331954956055, 5.044149875640869, 5.846510171890259, 6.648870468139648, 7.463399171829224, 8.277927875518799, 9.072965621948242, 9.868003368377686, 10.677959680557251, 11.487915992736816, 12.29646611213684, 13.105016231536865, 13.925834655761719, 14.746653079986572, 15.554060935974121, 16.36146879196167, 17.177912712097168, 17.994356632232666, 18.805021286010742, 19.61568593978882, 20.427491903305054, 21.23929786682129, 22.038288354873657, 22.837278842926025, 23.64701199531555, 24.456745147705078, 25.271830320358276, 26.086915493011475, 26.874202966690063, 27.661490440368652, 28.471861839294434, 29.282233238220215, 30.090080976486206, 30.897928714752197, 31.68849468231201, 32.479060649871826, 33.28234076499939, 34.08562088012695, 34.90152382850647, 35.717426776885986, 36.51249384880066, 37.30756092071533, 38.08602476119995, 38.86448860168457, 39.66421437263489, 40.463940143585205, 41.261043071746826, 42.05814599990845, 42.86585259437561, 43.67355918884277, 44.48696208000183, 45.30036497116089, 46.11561989784241, 46.930874824523926, 47.74099850654602, 48.551122188568115, 49.35552453994751, 50.159926891326904, 50.970301151275635, 51.780675411224365, 52.58745503425598, 53.3942346572876, 54.202038526535034, 55.00984239578247, 55.8119113445282, 56.613980293273926, 57.431313037872314, 58.2486457824707, 59.04798936843872, 59.84733295440674, 60.638911485672, 61.430490016937256, 62.23077893257141, 63.031067848205566, 63.84470868110657, 64.65834951400757, 65.45136165618896, 66.24437379837036, 67.06167006492615, 67.87896633148193, 68.67676901817322, 69.4745717048645, 70.29226160049438, 71.10995149612427, 71.91092205047607, 72.71189260482788, 73.52390885353088, 74.33592510223389, 75.12733697891235, 75.91874885559082, 76.71452212333679, 77.51029539108276, 78.28615045547485, 79.06200551986694, 79.86319088935852, 80.6643762588501, 81.45912384986877, 82.25387144088745, 83.04809594154358, 83.8423204421997, 84.54780507087708, 85.25328969955444, 85.91090106964111, 86.56851243972778, 87.28425240516663, 87.99999237060547, 88.67116832733154, 89.34234428405762, 90.05003786087036, 90.7577314376831, 91.41824102401733, 92.07875061035156, 92.78851413726807, 93.49827766418457, 94.15838623046875, 94.81849479675293, 95.4757432937622, 96.13299179077148, 96.84191823005676, 97.55084466934204, 98.20817375183105, 98.86550283432007, 99.52992653846741, 100.19435024261475, 100.87567257881165, 101.55699491500854, 102.24669361114502, 102.9363923072815, 103.60234069824219, 104.26828908920288, 104.9481942653656, 105.62809944152832, 106.32010746002197, 107.01211547851562, 107.69269609451294, 108.37327671051025, 109.08340454101562, 109.793532371521, 110.46650171279907, 111.13947105407715, 111.82398748397827, 112.5085039138794, 113.20012426376343, 113.89174461364746, 114.55108737945557, 115.21043014526367, 115.89201068878174, 116.5735912322998, 117.2845413684845, 117.99549150466919, 118.6726438999176, 119.34979629516602, 120.02633213996887, 120.70286798477173, 121.4137659072876, 122.12466382980347, 122.81087136268616, 123.49707889556885, 124.16645860671997, 124.8358383178711, 125.51699614524841, 126.19815397262573, 126.86426782608032, 127.53038167953491, 128.21433234214783, 128.89828300476074, 129.57475399971008, 130.25122499465942, 130.93857717514038, 131.62592935562134, 132.31416368484497, 133.0023980140686, 133.65839433670044, 134.31439065933228, 134.9920938014984, 135.66979694366455, 136.33309054374695, 136.99638414382935, 137.692289352417, 138.38819456100464, 139.05212497711182, 139.716055393219, 140.42533254623413, 141.13460969924927, 141.8120560646057, 142.48950242996216, 143.1708881855011, 143.85227394104004, 144.5118579864502, 145.17144203186035, 145.83517909049988, 146.4989161491394, 147.16412997245789, 147.82934379577637, 148.4941656589508, 149.15898752212524, 150.58853340148926, 152.01807928085327]
[34.74166666666667, 34.74166666666667, 46.333333333333336, 46.333333333333336, 54.18333333333333, 54.18333333333333, 69.525, 69.525, 75.075, 75.075, 80.05833333333334, 80.05833333333334, 83.7, 83.7, 85.74166666666666, 85.74166666666666, 84.05833333333334, 84.05833333333334, 86.73333333333333, 86.73333333333333, 88.14166666666667, 88.14166666666667, 87.14166666666667, 87.14166666666667, 85.00833333333334, 85.00833333333334, 84.30833333333334, 84.30833333333334, 90.63333333333334, 90.63333333333334, 92.925, 92.925, 95.9, 95.9, 95.95, 95.95, 96.08333333333333, 96.08333333333333, 96.05, 96.05, 96.05833333333334, 96.05833333333334, 95.99166666666666, 95.99166666666666, 96.00833333333334, 96.00833333333334, 95.96666666666667, 95.96666666666667, 95.95833333333333, 95.95833333333333, 95.96666666666667, 95.96666666666667, 96.025, 96.025, 96.03333333333333, 96.03333333333333, 96.025, 96.025, 96.0, 96.0, 96.0, 96.0, 95.975, 95.975, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 95.98333333333333, 95.98333333333333, 96.0, 96.0, 96.025, 96.025, 96.025, 96.025, 96.03333333333333, 96.03333333333333, 96.025, 96.025, 96.01666666666667, 96.01666666666667, 95.98333333333333, 95.98333333333333, 95.98333333333333, 95.98333333333333, 95.99166666666666, 95.99166666666666, 96.0, 96.0, 96.0, 96.0, 95.98333333333333, 95.98333333333333, 95.99166666666666, 95.99166666666666, 95.975, 95.975, 95.98333333333333, 95.98333333333333, 95.98333333333333, 95.98333333333333, 95.98333333333333, 95.98333333333333, 95.98333333333333, 95.98333333333333, 96.00833333333334, 96.00833333333334, 95.99166666666666, 95.99166666666666, 96.0, 96.0, 96.0, 96.0, 96.01666666666667, 96.01666666666667, 96.00833333333334, 96.00833333333334, 96.025, 96.025, 96.03333333333333, 96.03333333333333, 96.025, 96.025, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.025, 96.025, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.025, 96.025, 96.01666666666667, 96.01666666666667, 96.025, 96.025, 96.025, 96.025, 96.025, 96.025, 96.04166666666667, 96.04166666666667, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.04166666666667, 96.04166666666667, 96.04166666666667, 96.04166666666667, 96.03333333333333, 96.03333333333333, 96.05, 96.05, 96.05833333333334, 96.05833333333334, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05, 96.05833333333334, 96.05833333333334, 96.05833333333334, 96.05833333333334, 96.05, 96.05, 96.05, 96.05, 96.04166666666667, 96.04166666666667, 96.04166666666667, 96.04166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.598, Test loss: 1.620, Test accuracy: 83.99
Final Round, Global train loss: 1.598, Global test loss: 1.950, Global test accuracy: 51.02
Average accuracy final 10 rounds: 84.91416666666666 

Average global accuracy final 10 rounds: 48.605000000000004 

1291.7924592494965
[0.9073858261108398, 1.8147716522216797, 2.6213691234588623, 3.427966594696045, 4.225931406021118, 5.023896217346191, 5.8428425788879395, 6.6617889404296875, 7.425727605819702, 8.189666271209717, 8.908802032470703, 9.62793779373169, 10.315954685211182, 11.003971576690674, 11.66480565071106, 12.325639724731445, 13.015953302383423, 13.7062668800354, 14.379382371902466, 15.052497863769531, 15.762083768844604, 16.471669673919678, 17.126359462738037, 17.781049251556396, 18.460325241088867, 19.139601230621338, 19.800563097000122, 20.461524963378906, 21.140784740447998, 21.82004451751709, 22.496037244796753, 23.172029972076416, 23.837462425231934, 24.50289487838745, 25.188924551010132, 25.874954223632812, 26.513811588287354, 27.152668952941895, 27.82425332069397, 28.495837688446045, 29.156176567077637, 29.81651544570923, 30.491040229797363, 31.165565013885498, 31.840595483779907, 32.515625953674316, 33.20522928237915, 33.894832611083984, 34.605852365493774, 35.316872119903564, 35.99305558204651, 36.66923904418945, 37.358468770980835, 38.04769849777222, 38.72225284576416, 39.3968071937561, 40.05345392227173, 40.71010065078735, 41.37890815734863, 42.04771566390991, 42.73319935798645, 43.41868305206299, 44.08694291114807, 44.755202770233154, 45.44133925437927, 46.12747573852539, 46.85368633270264, 47.57989692687988, 48.269824504852295, 48.95975208282471, 49.63705372810364, 50.31435537338257, 51.01930546760559, 51.72425556182861, 52.385780811309814, 53.047306060791016, 53.73251008987427, 54.41771411895752, 55.09804558753967, 55.778377056121826, 56.4747953414917, 57.17121362686157, 57.84505319595337, 58.518892765045166, 59.22612380981445, 59.93335485458374, 60.60467028617859, 61.27598571777344, 61.94095468521118, 62.605923652648926, 63.321638107299805, 64.03735256195068, 64.70025968551636, 65.36316680908203, 66.07558417320251, 66.788001537323, 67.46779942512512, 68.14759731292725, 68.85616421699524, 69.56473112106323, 70.2257673740387, 70.88680362701416, 71.59152913093567, 72.29625463485718, 72.96422934532166, 73.63220405578613, 74.29524874687195, 74.95829343795776, 75.64279246330261, 76.32729148864746, 76.98747110366821, 77.64765071868896, 78.36415243148804, 79.08065414428711, 79.75285339355469, 80.42505264282227, 81.13223099708557, 81.83940935134888, 82.49876523017883, 83.15812110900879, 83.88180422782898, 84.60548734664917, 85.27477550506592, 85.94406366348267, 86.62652659416199, 87.30898952484131, 87.98975610733032, 88.67052268981934, 89.34422612190247, 90.0179295539856, 90.69075012207031, 91.36357069015503, 92.03318095207214, 92.70279121398926, 93.41856932640076, 94.13434743881226, 94.78930377960205, 95.44426012039185, 96.17316699028015, 96.90207386016846, 97.55996537208557, 98.21785688400269, 98.8998703956604, 99.58188390731812, 100.24844980239868, 100.91501569747925, 101.58935475349426, 102.26369380950928, 102.92072224617004, 103.57775068283081, 104.24184465408325, 104.9059386253357, 105.59218406677246, 106.27842950820923, 106.93471646308899, 107.59100341796875, 108.2922203540802, 108.99343729019165, 109.64578247070312, 110.2981276512146, 110.99207139015198, 111.68601512908936, 112.33785891532898, 112.9897027015686, 113.66812252998352, 114.34654235839844, 114.99926114082336, 115.65197992324829, 116.3242826461792, 116.99658536911011, 117.66040134429932, 118.32421731948853, 118.98456573486328, 119.64491415023804, 120.32515048980713, 121.00538682937622, 121.6920690536499, 122.37875127792358, 123.06727433204651, 123.75579738616943, 124.41245412826538, 125.06911087036133, 125.77170062065125, 126.47429037094116, 127.1333179473877, 127.79234552383423, 128.47622752189636, 129.1601095199585, 129.84440970420837, 130.52870988845825, 131.1841812133789, 131.83965253829956, 132.52459597587585, 133.20953941345215, 133.86169528961182, 134.51385116577148, 135.17173504829407, 135.82961893081665, 136.4900188446045, 137.15041875839233, 138.52310371398926, 139.89578866958618]
[38.13333333333333, 38.13333333333333, 46.71666666666667, 46.71666666666667, 61.975, 61.975, 69.325, 69.325, 73.9, 73.9, 74.06666666666666, 74.06666666666666, 75.94166666666666, 75.94166666666666, 81.80833333333334, 81.80833333333334, 80.075, 80.075, 78.8, 78.8, 77.49166666666666, 77.49166666666666, 77.7, 77.7, 80.08333333333333, 80.08333333333333, 80.38333333333334, 80.38333333333334, 80.49166666666666, 80.49166666666666, 79.25833333333334, 79.25833333333334, 82.15833333333333, 82.15833333333333, 83.775, 83.775, 83.3, 83.3, 81.93333333333334, 81.93333333333334, 83.71666666666667, 83.71666666666667, 83.70833333333333, 83.70833333333333, 85.28333333333333, 85.28333333333333, 83.76666666666667, 83.76666666666667, 83.725, 83.725, 83.76666666666667, 83.76666666666667, 85.2, 85.2, 85.01666666666667, 85.01666666666667, 85.025, 85.025, 83.69166666666666, 83.69166666666666, 83.78333333333333, 83.78333333333333, 83.78333333333333, 83.78333333333333, 86.96666666666667, 86.96666666666667, 87.01666666666667, 87.01666666666667, 85.35, 85.35, 85.24166666666666, 85.24166666666666, 84.05, 84.05, 82.59166666666667, 82.59166666666667, 82.70833333333333, 82.70833333333333, 82.61666666666666, 82.61666666666666, 84.20833333333333, 84.20833333333333, 83.98333333333333, 83.98333333333333, 83.96666666666667, 83.96666666666667, 82.66666666666667, 82.66666666666667, 82.725, 82.725, 84.14166666666667, 84.14166666666667, 85.28333333333333, 85.28333333333333, 85.53333333333333, 85.53333333333333, 85.59166666666667, 85.59166666666667, 85.64166666666667, 85.64166666666667, 85.74166666666666, 85.74166666666666, 86.06666666666666, 86.06666666666666, 86.15833333333333, 86.15833333333333, 86.16666666666667, 86.16666666666667, 85.7, 85.7, 88.6, 88.6, 88.70833333333333, 88.70833333333333, 85.85833333333333, 85.85833333333333, 85.78333333333333, 85.78333333333333, 85.75833333333334, 85.75833333333334, 84.08333333333333, 84.08333333333333, 84.19166666666666, 84.19166666666666, 85.65, 85.65, 85.69166666666666, 85.69166666666666, 85.56666666666666, 85.56666666666666, 87.24166666666666, 87.24166666666666, 88.49166666666666, 88.49166666666666, 86.83333333333333, 86.83333333333333, 85.33333333333333, 85.33333333333333, 85.55, 85.55, 85.63333333333334, 85.63333333333334, 85.80833333333334, 85.80833333333334, 85.81666666666666, 85.81666666666666, 85.79166666666667, 85.79166666666667, 85.7, 85.7, 85.71666666666667, 85.71666666666667, 85.61666666666666, 85.61666666666666, 85.6, 85.6, 87.25833333333334, 87.25833333333334, 87.25, 87.25, 87.20833333333333, 87.20833333333333, 87.19166666666666, 87.19166666666666, 85.45833333333333, 85.45833333333333, 85.40833333333333, 85.40833333333333, 85.44166666666666, 85.44166666666666, 87.10833333333333, 87.10833333333333, 87.11666666666666, 87.11666666666666, 87.05833333333334, 87.05833333333334, 87.15833333333333, 87.15833333333333, 85.59166666666667, 85.59166666666667, 85.56666666666666, 85.56666666666666, 85.475, 85.475, 85.575, 85.575, 85.575, 85.575, 85.64166666666667, 85.64166666666667, 82.49166666666666, 82.49166666666666, 82.39166666666667, 82.39166666666667, 85.43333333333334, 85.43333333333334, 85.45, 85.45, 85.54166666666667, 85.54166666666667, 83.99166666666666, 83.99166666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 679, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 412, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 285, in train
    local_par_list = torch.cat((local_par_list, param.reshape(-1)), 0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 849, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 541, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 275, in <module>
    acc_test, loss_test = test_img_local_all(net_glob, args, dataset_test, dict_users_test,
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 133, in test_img_local_all
    a, b = test_img_local(net_local, dataset_test, args, user_idx=idx, idxs=dict_users_test[idx], concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 97, in test_img_local
    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.
../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2223, in train
    loss.backward()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 259, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 142, in _make_grads
    torch.ones_like(out, memory_format=torch.preserve_format)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.481, Test loss: 1.513, Test accuracy: 94.89
Final Round, Global train loss: 1.481, Global test loss: 1.982, Global test accuracy: 47.45
Average accuracy final 10 rounds: 94.4975 

Average global accuracy final 10 rounds: 47.611666666666665 

1336.1204550266266
[0.8788154125213623, 1.7576308250427246, 2.56227445602417, 3.3669180870056152, 4.162244558334351, 4.957571029663086, 5.772979974746704, 6.588388919830322, 7.393842458724976, 8.199295997619629, 8.998860120773315, 9.798424243927002, 10.605327367782593, 11.412230491638184, 12.230754375457764, 13.049278259277344, 13.86892056465149, 14.688562870025635, 15.509140014648438, 16.32971715927124, 17.149580001831055, 17.96944284439087, 18.790471076965332, 19.611499309539795, 20.41922664642334, 21.226953983306885, 22.038235187530518, 22.84951639175415, 23.664673328399658, 24.479830265045166, 25.298240184783936, 26.116650104522705, 26.928600549697876, 27.740550994873047, 28.55353617668152, 29.36652135848999, 30.162874460220337, 30.959227561950684, 31.765358209609985, 32.57148885726929, 33.3797721862793, 34.18805551528931, 35.002647399902344, 35.81723928451538, 36.62052035331726, 37.42380142211914, 38.23555564880371, 39.04730987548828, 39.84063506126404, 40.633960247039795, 41.454227685928345, 42.274495124816895, 43.07554888725281, 43.87660264968872, 44.69320511817932, 45.50980758666992, 46.31458878517151, 47.119369983673096, 47.92676019668579, 48.734150409698486, 49.539217948913574, 50.34428548812866, 51.16729426383972, 51.99030303955078, 52.799214363098145, 53.60812568664551, 54.42181420326233, 55.23550271987915, 56.053386211395264, 56.87126970291138, 57.67815160751343, 58.48503351211548, 59.30197763442993, 60.118921756744385, 60.92822241783142, 61.73752307891846, 62.553988218307495, 63.37045335769653, 64.17524909973145, 64.98004484176636, 65.79098057746887, 66.60191631317139, 67.3932318687439, 68.1845474243164, 69.01577615737915, 69.8470048904419, 70.65240120887756, 71.45779752731323, 72.2941038608551, 73.13041019439697, 73.93654012680054, 74.7426700592041, 75.56658458709717, 76.39049911499023, 77.18767237663269, 77.98484563827515, 78.8025553226471, 79.62026500701904, 80.41849112510681, 81.21671724319458, 82.05727124214172, 82.89782524108887, 83.70115423202515, 84.50448322296143, 85.33078813552856, 86.1570930480957, 86.95959854125977, 87.76210403442383, 88.587162733078, 89.41222143173218, 90.21727180480957, 91.02232217788696, 91.85021328926086, 92.67810440063477, 93.48567771911621, 94.29325103759766, 95.10978031158447, 95.92630958557129, 96.72851467132568, 97.53071975708008, 98.33860158920288, 99.14648342132568, 99.93903112411499, 100.7315788269043, 101.54138398170471, 102.35118913650513, 103.15712785720825, 103.96306657791138, 104.7669198513031, 105.57077312469482, 106.37676310539246, 107.18275308609009, 107.97461938858032, 108.76648569107056, 109.57713961601257, 110.38779354095459, 111.19644474983215, 112.00509595870972, 112.83622431755066, 113.6673526763916, 114.4766833782196, 115.28601408004761, 116.10458993911743, 116.92316579818726, 117.72246360778809, 118.52176141738892, 119.36289405822754, 120.20402669906616, 121.00865077972412, 121.81327486038208, 122.65715146064758, 123.50102806091309, 124.30791306495667, 125.11479806900024, 125.95353627204895, 126.79227447509766, 127.58090043067932, 128.369526386261, 129.02690243721008, 129.68427848815918, 130.37419080734253, 131.06410312652588, 131.74506545066833, 132.4260277748108, 133.08910369873047, 133.75217962265015, 134.4034366607666, 135.05469369888306, 135.71950888633728, 136.3843240737915, 137.04116940498352, 137.69801473617554, 138.36526083946228, 139.03250694274902, 139.68608474731445, 140.33966255187988, 140.9894196987152, 141.63917684555054, 142.29527163505554, 142.95136642456055, 143.6144871711731, 144.27760791778564, 144.93066668510437, 145.5837254524231, 146.2336184978485, 146.88351154327393, 147.56759762763977, 148.25168371200562, 148.89901065826416, 149.5463376045227, 150.2003412246704, 150.85434484481812, 151.50688529014587, 152.15942573547363, 152.81623578071594, 153.47304582595825, 154.13937091827393, 154.8056960105896, 155.44348049163818, 156.08126497268677, 157.40190505981445, 158.72254514694214]
[41.225, 41.225, 59.708333333333336, 59.708333333333336, 71.11666666666666, 71.11666666666666, 74.06666666666666, 74.06666666666666, 85.93333333333334, 85.93333333333334, 84.98333333333333, 84.98333333333333, 85.66666666666667, 85.66666666666667, 86.28333333333333, 86.28333333333333, 88.20833333333333, 88.20833333333333, 88.34166666666667, 88.34166666666667, 89.86666666666666, 89.86666666666666, 91.56666666666666, 91.56666666666666, 93.15833333333333, 93.15833333333333, 93.23333333333333, 93.23333333333333, 93.275, 93.275, 93.29166666666667, 93.29166666666667, 93.3, 93.3, 93.35833333333333, 93.35833333333333, 93.35, 93.35, 93.35833333333333, 93.35833333333333, 93.33333333333333, 93.33333333333333, 93.34166666666667, 93.34166666666667, 93.375, 93.375, 93.35833333333333, 93.35833333333333, 93.36666666666666, 93.36666666666666, 93.35, 93.35, 93.38333333333334, 93.38333333333334, 93.38333333333334, 93.38333333333334, 93.36666666666666, 93.36666666666666, 93.375, 93.375, 93.38333333333334, 93.38333333333334, 93.38333333333334, 93.38333333333334, 93.35, 93.35, 93.39166666666667, 93.39166666666667, 93.41666666666667, 93.41666666666667, 93.43333333333334, 93.43333333333334, 93.425, 93.425, 93.43333333333334, 93.43333333333334, 93.425, 93.425, 93.45, 93.45, 93.44166666666666, 93.44166666666666, 93.43333333333334, 93.43333333333334, 93.41666666666667, 93.41666666666667, 93.4, 93.4, 93.41666666666667, 93.41666666666667, 93.41666666666667, 93.41666666666667, 93.4, 93.4, 93.4, 93.4, 93.39166666666667, 93.39166666666667, 93.39166666666667, 93.39166666666667, 93.40833333333333, 93.40833333333333, 93.41666666666667, 93.41666666666667, 93.4, 93.4, 93.4, 93.4, 93.40833333333333, 93.40833333333333, 93.425, 93.425, 93.43333333333334, 93.43333333333334, 93.45, 93.45, 93.44166666666666, 93.44166666666666, 93.44166666666666, 93.44166666666666, 93.45833333333333, 93.45833333333333, 93.45833333333333, 93.45833333333333, 93.45, 93.45, 93.45, 93.45, 93.45833333333333, 93.45833333333333, 93.45, 93.45, 93.45, 93.45, 93.45, 93.45, 93.45, 93.45, 93.45, 93.45, 93.44166666666666, 93.44166666666666, 93.43333333333334, 93.43333333333334, 93.425, 93.425, 93.425, 93.425, 93.425, 93.425, 93.40833333333333, 93.40833333333333, 93.41666666666667, 93.41666666666667, 93.41666666666667, 93.41666666666667, 93.41666666666667, 93.41666666666667, 93.39166666666667, 93.39166666666667, 93.4, 93.4, 93.4, 93.4, 93.40833333333333, 93.40833333333333, 93.40833333333333, 93.40833333333333, 93.4, 93.4, 93.39166666666667, 93.39166666666667, 93.375, 93.375, 93.375, 93.375, 93.375, 93.375, 93.38333333333334, 93.38333333333334, 93.38333333333334, 93.38333333333334, 93.39166666666667, 93.39166666666667, 94.68333333333334, 94.68333333333334, 94.68333333333334, 94.68333333333334, 94.68333333333334, 94.68333333333334, 94.68333333333334, 94.68333333333334, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.86666666666666, 94.89166666666667, 94.89166666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.816, Test loss: 1.851, Test accuracy: 60.90
Final Round, Global train loss: 1.816, Global test loss: 2.266, Global test accuracy: 17.41
Average accuracy final 10 rounds: 57.7295 

Average global accuracy final 10 rounds: 16.40625 

3188.9276733398438
[0.8561737537384033, 1.7123475074768066, 2.4970827102661133, 3.28181791305542, 4.063526391983032, 4.8452348709106445, 5.622374057769775, 6.399513244628906, 7.180484056472778, 7.96145486831665, 8.743484497070312, 9.525514125823975, 10.3011634349823, 11.076812744140625, 11.848831415176392, 12.620850086212158, 13.394887685775757, 14.168925285339355, 14.946378231048584, 15.723831176757812, 16.50632929801941, 17.288827419281006, 18.07068681716919, 18.852546215057373, 19.64438533782959, 20.436224460601807, 21.206691026687622, 21.977157592773438, 22.751420736312866, 23.525683879852295, 24.300185441970825, 25.074687004089355, 25.851451635360718, 26.62821626663208, 27.40710139274597, 28.185986518859863, 28.980140686035156, 29.77429485321045, 30.55994176864624, 31.34558868408203, 32.1252715587616, 32.90495443344116, 33.68553948402405, 34.466124534606934, 35.2408766746521, 36.015628814697266, 36.78927564620972, 37.56292247772217, 38.337214946746826, 39.111507415771484, 39.886778593063354, 40.662049770355225, 41.44148254394531, 42.2209153175354, 43.00358271598816, 43.78625011444092, 44.579232692718506, 45.372215270996094, 46.14943051338196, 46.92664575576782, 47.69979763031006, 48.472949504852295, 49.25233244895935, 50.031715393066406, 50.804964542388916, 51.578213691711426, 52.350592613220215, 53.122971534729004, 53.901469469070435, 54.679967403411865, 55.469504594802856, 56.25904178619385, 57.041144609451294, 57.82324743270874, 58.60012125968933, 59.37699508666992, 60.149192571640015, 60.92139005661011, 61.69391918182373, 62.46644830703735, 63.239018201828, 64.01158809661865, 64.78820061683655, 65.56481313705444, 66.34858298301697, 67.13235282897949, 67.90952253341675, 68.686692237854, 69.45806384086609, 70.22943544387817, 70.99958443641663, 71.76973342895508, 72.5393054485321, 73.30887746810913, 74.079754114151, 74.85063076019287, 75.6355414390564, 76.42045211791992, 77.19750761985779, 77.97456312179565, 78.81249976158142, 79.65043640136719, 80.47281312942505, 81.29518985748291, 82.1200041770935, 82.9448184967041, 83.76973605155945, 84.5946536064148, 85.42591500282288, 86.25717639923096, 87.08369398117065, 87.91021156311035, 88.74476742744446, 89.57932329177856, 90.40709567070007, 91.23486804962158, 92.05995202064514, 92.8850359916687, 93.71048545837402, 94.53593492507935, 95.35901379585266, 96.18209266662598, 97.00633645057678, 97.83058023452759, 98.65436291694641, 99.47814559936523, 100.27417516708374, 101.07020473480225, 101.86269354820251, 102.65518236160278, 103.48895955085754, 104.3227367401123, 105.15493249893188, 105.98712825775146, 106.82070064544678, 107.65427303314209, 108.49196577072144, 109.32965850830078, 110.16847586631775, 111.00729322433472, 111.84452176094055, 112.68175029754639, 113.53053998947144, 114.37932968139648, 115.21330261230469, 116.04727554321289, 116.88479137420654, 117.7223072052002, 118.56517505645752, 119.40804290771484, 120.24207401275635, 121.07610511779785, 121.91354751586914, 122.75098991394043, 123.58743453025818, 124.42387914657593, 125.26343941688538, 126.10299968719482, 126.94093012809753, 127.77886056900024, 128.6174647808075, 129.45606899261475, 130.289457321167, 131.12284564971924, 131.95757007598877, 132.7922945022583, 133.6235556602478, 134.4548168182373, 135.29258847236633, 136.13036012649536, 136.96169471740723, 137.7930293083191, 138.63120865821838, 139.46938800811768, 140.30398988723755, 141.13859176635742, 141.9719591140747, 142.805326461792, 143.6385018825531, 144.4716773033142, 145.30847239494324, 146.14526748657227, 146.98012685775757, 147.81498622894287, 148.65452361106873, 149.49406099319458, 150.33552265167236, 151.17698431015015, 152.0143961906433, 152.85180807113647, 153.68852829933167, 154.52524852752686, 155.36288142204285, 156.20051431655884, 157.03128862380981, 157.8620629310608, 158.6945300102234, 159.526997089386, 160.36302995681763, 161.19906282424927, 162.8580288887024, 164.51699495315552]
[13.4975, 13.4975, 15.38, 15.38, 15.8775, 15.8775, 16.1525, 16.1525, 16.37, 16.37, 16.9775, 16.9775, 17.51, 17.51, 16.39, 16.39, 18.0225, 18.0225, 19.4875, 19.4875, 21.11, 21.11, 23.14, 23.14, 27.115, 27.115, 30.015, 30.015, 32.7525, 32.7525, 34.2225, 34.2225, 36.595, 36.595, 37.0325, 37.0325, 40.025, 40.025, 40.8525, 40.8525, 41.3525, 41.3525, 40.275, 40.275, 40.2775, 40.2775, 41.0475, 41.0475, 42.92, 42.92, 43.69, 43.69, 43.8125, 43.8125, 44.49, 44.49, 44.72, 44.72, 45.9225, 45.9225, 46.0675, 46.0675, 46.005, 46.005, 47.2725, 47.2725, 47.2625, 47.2625, 48.115, 48.115, 48.6625, 48.6625, 47.8775, 47.8775, 46.885, 46.885, 47.26, 47.26, 47.885, 47.885, 48.5325, 48.5325, 49.425, 49.425, 49.82, 49.82, 50.1775, 50.1775, 50.4825, 50.4825, 50.4525, 50.4525, 50.905, 50.905, 50.895, 50.895, 50.52, 50.52, 51.3325, 51.3325, 51.66, 51.66, 52.4475, 52.4475, 52.9375, 52.9375, 53.225, 53.225, 54.01, 54.01, 54.0175, 54.0175, 53.6375, 53.6375, 54.9175, 54.9175, 54.6425, 54.6425, 54.735, 54.735, 54.8975, 54.8975, 55.055, 55.055, 55.285, 55.285, 55.2775, 55.2775, 54.7275, 54.7275, 54.7325, 54.7325, 54.665, 54.665, 54.675, 54.675, 55.1575, 55.1575, 56.4025, 56.4025, 56.8475, 56.8475, 56.5825, 56.5825, 56.515, 56.515, 56.59, 56.59, 56.1525, 56.1525, 56.6025, 56.6025, 56.2325, 56.2325, 56.315, 56.315, 56.3675, 56.3675, 55.87, 55.87, 55.9875, 55.9875, 56.045, 56.045, 56.1825, 56.1825, 56.7625, 56.7625, 57.5175, 57.5175, 57.52, 57.52, 57.3825, 57.3825, 57.54, 57.54, 57.605, 57.605, 57.5975, 57.5975, 57.6675, 57.6675, 57.6625, 57.6625, 57.675, 57.675, 57.3575, 57.3575, 57.62, 57.62, 57.5125, 57.5125, 57.8, 57.8, 57.78, 57.78, 58.215, 58.215, 58.005, 58.005, 60.9, 60.9]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.825, Test loss: 1.871, Test accuracy: 58.85
Average accuracy final 10 rounds: 58.23925 

2226.999198913574
[0.8282504081726074, 1.6565008163452148, 2.3914380073547363, 3.126375198364258, 3.861264228820801, 4.596153259277344, 5.332543611526489, 6.068933963775635, 6.7993433475494385, 7.529752731323242, 8.275473594665527, 9.021194458007812, 9.759468793869019, 10.497743129730225, 11.24276089668274, 11.987778663635254, 12.724551916122437, 13.46132516860962, 14.191787719726562, 14.922250270843506, 15.656840324401855, 16.391430377960205, 17.123351573944092, 17.85527276992798, 18.607343435287476, 19.359414100646973, 20.103739738464355, 20.84806537628174, 21.592148303985596, 22.336231231689453, 23.098910093307495, 23.861588954925537, 24.608192682266235, 25.354796409606934, 26.11395812034607, 26.873119831085205, 27.624277114868164, 28.375434398651123, 29.1198673248291, 29.86430025100708, 30.606438159942627, 31.348576068878174, 32.09016275405884, 32.8317494392395, 33.577308177948, 34.322866916656494, 35.073694944381714, 35.824522972106934, 36.569199323654175, 37.313875675201416, 38.05258798599243, 38.79130029678345, 39.532241344451904, 40.27318239212036, 41.00848412513733, 41.7437858581543, 42.48893117904663, 43.234076499938965, 43.96989059448242, 44.70570468902588, 45.441012382507324, 46.17632007598877, 46.917240381240845, 47.65816068649292, 48.40359449386597, 49.149028301239014, 49.88635993003845, 50.62369155883789, 51.36327290534973, 52.10285425186157, 52.84984278678894, 53.59683132171631, 54.324540853500366, 55.052250385284424, 55.78692841529846, 56.5216064453125, 57.259758710861206, 57.99791097640991, 58.74336123466492, 59.48881149291992, 60.2304105758667, 60.97200965881348, 61.702982664108276, 62.433955669403076, 63.15856409072876, 63.88317251205444, 64.6219470500946, 65.36072158813477, 66.10452890396118, 66.8483362197876, 67.58761620521545, 68.32689619064331, 69.05260443687439, 69.77831268310547, 70.50741195678711, 71.23651123046875, 71.97387957572937, 72.71124792098999, 73.45897722244263, 74.20670652389526, 74.94779515266418, 75.6888837814331, 76.42378544807434, 77.15868711471558, 77.89302587509155, 78.62736463546753, 79.36775827407837, 80.10815191268921, 80.8522698879242, 81.59638786315918, 82.34827709197998, 83.10016632080078, 83.832754611969, 84.5653429031372, 85.29455590248108, 86.02376890182495, 86.75680470466614, 87.48984050750732, 88.2343897819519, 88.97893905639648, 89.72494125366211, 90.47094345092773, 91.20834946632385, 91.94575548171997, 92.67773795127869, 93.4097204208374, 94.13428497314453, 94.85884952545166, 95.60225582122803, 96.3456621170044, 97.08865118026733, 97.83164024353027, 98.57219982147217, 99.31275939941406, 100.05397844314575, 100.79519748687744, 101.5303053855896, 102.26541328430176, 103.00743746757507, 103.74946165084839, 104.49074864387512, 105.23203563690186, 105.96160387992859, 106.69117212295532, 107.41449737548828, 108.13782262802124, 108.87676286697388, 109.61570310592651, 110.35826635360718, 111.10082960128784, 111.84575915336609, 112.59068870544434, 113.31834554672241, 114.04600238800049, 114.77427911758423, 115.50255584716797, 116.23824763298035, 116.97393941879272, 117.7202718257904, 118.46660423278809, 119.20555806159973, 119.94451189041138, 120.6887743473053, 121.43303680419922, 122.16787075996399, 122.90270471572876, 123.63482475280762, 124.36694478988647, 125.10620713233948, 125.84546947479248, 126.5888819694519, 127.33229446411133, 128.0715000629425, 128.81070566177368, 129.53758454322815, 130.26446342468262, 131.0018401145935, 131.7392168045044, 132.47924423217773, 133.21927165985107, 133.96945691108704, 134.719642162323, 135.46591091156006, 136.21217966079712, 136.94598937034607, 137.67979907989502, 138.4205436706543, 139.16128826141357, 139.90853905677795, 140.65578985214233, 141.38411378860474, 142.11243772506714, 142.85783743858337, 143.6032371520996, 144.33575534820557, 145.06827354431152, 145.8059892654419, 146.54370498657227, 147.2865445613861, 148.02938413619995, 149.3400514125824, 150.65071868896484]
[10.23, 10.23, 10.8, 10.8, 12.37, 12.37, 13.4025, 13.4025, 13.525, 13.525, 13.6325, 13.6325, 13.825, 13.825, 13.935, 13.935, 14.39, 14.39, 14.32, 14.32, 14.4325, 14.4325, 14.47, 14.47, 15.065, 15.065, 15.2675, 15.2675, 15.015, 15.015, 16.1425, 16.1425, 16.6, 16.6, 15.955, 15.955, 15.845, 15.845, 18.145, 18.145, 19.35, 19.35, 19.84, 19.84, 20.575, 20.575, 21.2375, 21.2375, 22.99, 22.99, 25.15, 25.15, 26.8125, 26.8125, 29.9575, 29.9575, 31.53, 31.53, 34.69, 34.69, 37.79, 37.79, 39.845, 39.845, 41.4725, 41.4725, 43.02, 43.02, 44.0025, 44.0025, 45.4575, 45.4575, 47.06, 47.06, 48.1125, 48.1125, 48.2625, 48.2625, 48.99, 48.99, 51.2525, 51.2525, 51.495, 51.495, 52.2475, 52.2475, 52.955, 52.955, 53.1475, 53.1475, 53.5425, 53.5425, 54.0225, 54.0225, 54.0775, 54.0775, 54.195, 54.195, 54.4425, 54.4425, 54.525, 54.525, 54.63, 54.63, 54.645, 54.645, 54.6275, 54.6275, 54.74, 54.74, 54.71, 54.71, 54.935, 54.935, 54.83, 54.83, 54.94, 54.94, 55.3525, 55.3525, 55.47, 55.47, 55.4525, 55.4525, 55.51, 55.51, 55.555, 55.555, 55.89, 55.89, 56.3875, 56.3875, 56.475, 56.475, 56.3675, 56.3675, 56.3775, 56.3775, 56.325, 56.325, 56.3375, 56.3375, 56.4, 56.4, 56.5275, 56.5275, 56.67, 56.67, 56.685, 56.685, 56.645, 56.645, 56.745, 56.745, 56.8375, 56.8375, 56.9475, 56.9475, 56.9325, 56.9325, 57.2575, 57.2575, 57.39, 57.39, 57.4125, 57.4125, 57.515, 57.515, 57.625, 57.625, 57.655, 57.655, 57.635, 57.635, 57.935, 57.935, 57.97, 57.97, 58.0675, 58.0675, 58.195, 58.195, 58.1425, 58.1425, 58.25, 58.25, 58.23, 58.23, 58.2675, 58.2675, 58.255, 58.255, 58.28, 58.28, 58.2525, 58.2525, 58.235, 58.235, 58.285, 58.285, 58.85, 58.85]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.763, Test loss: 1.811, Test accuracy: 64.82
Average accuracy final 10 rounds: 64.708 

2248.9092705249786
[0.9177086353302002, 1.8354172706604004, 2.663755416870117, 3.492093563079834, 4.327076196670532, 5.1620588302612305, 5.983280181884766, 6.804501533508301, 7.626303434371948, 8.448105335235596, 9.267619848251343, 10.08713436126709, 10.909077644348145, 11.7310209274292, 12.559175252914429, 13.387329578399658, 14.206738471984863, 15.026147365570068, 15.845493078231812, 16.664838790893555, 17.481983423233032, 18.29912805557251, 19.123153686523438, 19.947179317474365, 20.770073175430298, 21.59296703338623, 22.414649724960327, 23.236332416534424, 24.065982580184937, 24.89563274383545, 25.7187602519989, 26.541887760162354, 27.35996675491333, 28.178045749664307, 29.001183032989502, 29.824320316314697, 30.645948886871338, 31.46757745742798, 32.299301624298096, 33.13102579116821, 33.95331597328186, 34.77560615539551, 35.59794497489929, 36.420283794403076, 37.24435567855835, 38.06842756271362, 38.897072553634644, 39.725717544555664, 40.56655025482178, 41.40738296508789, 42.24261021614075, 43.0778374671936, 43.90538024902344, 44.73292303085327, 45.55638027191162, 46.37983751296997, 47.204978227615356, 48.03011894226074, 48.85825562477112, 49.686392307281494, 50.511459827423096, 51.3365273475647, 52.16282296180725, 52.989118576049805, 53.822577238082886, 54.65603590011597, 55.481635093688965, 56.30723428726196, 57.13196086883545, 57.956687450408936, 58.78709554672241, 59.61750364303589, 60.44332671165466, 61.26914978027344, 62.10976266860962, 62.9503755569458, 63.7723286151886, 64.5942816734314, 65.41995811462402, 66.24563455581665, 67.07155179977417, 67.89746904373169, 68.72743892669678, 69.55740880966187, 70.39450192451477, 71.23159503936768, 72.05471324920654, 72.87783145904541, 73.70338106155396, 74.5289306640625, 75.359304189682, 76.18967771530151, 77.01451754570007, 77.83935737609863, 78.67390155792236, 79.5084457397461, 80.331702709198, 81.1549596786499, 81.96739888191223, 82.77983808517456, 83.59060049057007, 84.40136289596558, 85.21635365486145, 86.03134441375732, 86.85471177101135, 87.67807912826538, 88.5013382434845, 89.32459735870361, 90.14793014526367, 90.97126293182373, 91.79071736335754, 92.61017179489136, 93.42466711997986, 94.23916244506836, 95.05380392074585, 95.86844539642334, 96.68431115150452, 97.5001769065857, 98.31909227371216, 99.13800764083862, 99.95397353172302, 100.76993942260742, 101.58315348625183, 102.39636754989624, 103.2135534286499, 104.03073930740356, 104.83912205696106, 105.64750480651855, 106.46283793449402, 107.27817106246948, 108.09477496147156, 108.91137886047363, 109.72301125526428, 110.53464365005493, 111.35296654701233, 112.17128944396973, 112.9784483909607, 113.78560733795166, 114.6015853881836, 115.41756343841553, 116.23300290107727, 117.04844236373901, 117.85882019996643, 118.66919803619385, 119.4817726612091, 120.29434728622437, 121.10355567932129, 121.91276407241821, 122.73460721969604, 123.55645036697388, 124.37062644958496, 125.18480253219604, 125.99622654914856, 126.80765056610107, 127.62258052825928, 128.43751049041748, 129.249018907547, 130.0605273246765, 130.88355588912964, 131.70658445358276, 132.51677799224854, 133.3269715309143, 134.13345170021057, 134.93993186950684, 135.75900983810425, 136.57808780670166, 137.3880078792572, 138.19792795181274, 139.01538729667664, 139.83284664154053, 140.65735912322998, 141.48187160491943, 142.30323791503906, 143.1246042251587, 143.93696212768555, 144.7493200302124, 145.55832815170288, 146.36733627319336, 147.1755599975586, 147.98378372192383, 148.8023443222046, 149.62090492248535, 150.43932104110718, 151.257737159729, 152.06766057014465, 152.8775839805603, 153.70339393615723, 154.52920389175415, 155.34605145454407, 156.16289901733398, 156.98216152191162, 157.80142402648926, 158.61089324951172, 159.42036247253418, 160.23170804977417, 161.04305362701416, 161.8700454235077, 162.69703722000122, 163.51716208457947, 164.33728694915771, 165.62342476844788, 166.90956258773804]
[11.12, 11.12, 11.2675, 11.2675, 11.9525, 11.9525, 14.2525, 14.2525, 15.69, 15.69, 15.5025, 15.5025, 17.06, 17.06, 18.6775, 18.6775, 18.135, 18.135, 21.6925, 21.6925, 24.94, 24.94, 29.365, 29.365, 32.0325, 32.0325, 34.165, 34.165, 36.5525, 36.5525, 39.28, 39.28, 39.825, 39.825, 43.19, 43.19, 44.495, 44.495, 45.195, 45.195, 48.4825, 48.4825, 49.5725, 49.5725, 50.0675, 50.0675, 50.6375, 50.6375, 51.32, 51.32, 51.6175, 51.6175, 52.3525, 52.3525, 52.785, 52.785, 53.1725, 53.1725, 53.7775, 53.7775, 54.5525, 54.5525, 54.8175, 54.8175, 55.35, 55.35, 55.5475, 55.5475, 55.5225, 55.5225, 55.9225, 55.9225, 56.435, 56.435, 56.575, 56.575, 56.5, 56.5, 56.7125, 56.7125, 56.915, 56.915, 57.3975, 57.3975, 57.9275, 57.9275, 58.195, 58.195, 58.5375, 58.5375, 58.8125, 58.8125, 58.89, 58.89, 59.03, 59.03, 59.165, 59.165, 59.16, 59.16, 59.53, 59.53, 59.9, 59.9, 59.84, 59.84, 59.885, 59.885, 60.11, 60.11, 60.59, 60.59, 61.005, 61.005, 61.0225, 61.0225, 61.05, 61.05, 61.23, 61.23, 61.6125, 61.6125, 61.6825, 61.6825, 61.775, 61.775, 61.7225, 61.7225, 62.1675, 62.1675, 62.2625, 62.2625, 62.7225, 62.7225, 62.785, 62.785, 62.905, 62.905, 62.9425, 62.9425, 62.9925, 62.9925, 62.98, 62.98, 63.0525, 63.0525, 63.4075, 63.4075, 63.45, 63.45, 63.4225, 63.4225, 63.4875, 63.4875, 63.6125, 63.6125, 63.6525, 63.6525, 64.04, 64.04, 64.295, 64.295, 64.4075, 64.4075, 64.395, 64.395, 64.42, 64.42, 64.4475, 64.4475, 64.4925, 64.4925, 64.4875, 64.4875, 64.5575, 64.5575, 64.6175, 64.6175, 64.625, 64.625, 64.6475, 64.6475, 64.7025, 64.7025, 64.6725, 64.6725, 64.73, 64.73, 64.695, 64.695, 64.7025, 64.7025, 64.715, 64.715, 64.7675, 64.7675, 64.76, 64.76, 64.6875, 64.6875, 64.82, 64.82]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.2  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.489, Test loss: 1.633, Test accuracy: 83.37
Average accuracy final 10 rounds: 83.02000000000001 

2258.343775033951
[0.9190950393676758, 1.8381900787353516, 2.6676363945007324, 3.4970827102661133, 4.320876359939575, 5.144670009613037, 5.979847192764282, 6.815024375915527, 7.631710767745972, 8.448397159576416, 9.26770305633545, 10.087008953094482, 10.902562141418457, 11.718115329742432, 12.535868167877197, 13.353621006011963, 14.186936855316162, 15.020252704620361, 15.835749864578247, 16.651247024536133, 17.48004984855652, 18.308852672576904, 19.122565507888794, 19.936278343200684, 20.759134769439697, 21.58199119567871, 22.40149164199829, 23.22099208831787, 24.037587642669678, 24.854183197021484, 25.681195974349976, 26.508208751678467, 27.33228898048401, 28.15636920928955, 28.978787422180176, 29.8012056350708, 30.582153797149658, 31.363101959228516, 32.18081259727478, 32.998523235321045, 33.832202434539795, 34.665881633758545, 35.48951578140259, 36.31314992904663, 37.14179587364197, 37.970441818237305, 38.78907251358032, 39.60770320892334, 40.43088102340698, 41.254058837890625, 42.07579684257507, 42.89753484725952, 43.718623638153076, 44.53971242904663, 45.365424156188965, 46.1911358833313, 47.01889157295227, 47.84664726257324, 48.67215418815613, 49.497661113739014, 50.31998062133789, 51.14230012893677, 51.964457273483276, 52.786614418029785, 53.6114399433136, 54.43626546859741, 55.25600624084473, 56.07574701309204, 56.90472459793091, 57.733702182769775, 58.55366849899292, 59.373634815216064, 60.19562649726868, 61.01761817932129, 61.84428882598877, 62.67095947265625, 63.49554657936096, 64.32013368606567, 65.15290856361389, 65.98568344116211, 66.80445957183838, 67.62323570251465, 68.46046686172485, 69.29769802093506, 70.12655735015869, 70.95541667938232, 71.77131986618042, 72.58722305297852, 73.40531373023987, 74.22340440750122, 75.04114437103271, 75.85888433456421, 76.7028067111969, 77.54672908782959, 78.37431526184082, 79.20190143585205, 80.02165269851685, 80.84140396118164, 81.66210770606995, 82.48281145095825, 83.30354857444763, 84.12428569793701, 84.96929335594177, 85.81430101394653, 86.64171862602234, 87.46913623809814, 88.29969429969788, 89.13025236129761, 89.9499762058258, 90.769700050354, 91.59276938438416, 92.4158387184143, 93.2471694946289, 94.0785002708435, 94.8985743522644, 95.7186484336853, 96.54149580001831, 97.36434316635132, 98.18115973472595, 98.99797630310059, 99.82671761512756, 100.65545892715454, 101.48062372207642, 102.30578851699829, 103.12509322166443, 103.94439792633057, 104.76945900917053, 105.5945200920105, 106.41287136077881, 107.23122262954712, 108.06956815719604, 108.90791368484497, 109.7307517528534, 110.55358982086182, 111.37592244148254, 112.19825506210327, 113.019376039505, 113.84049701690674, 114.66981649398804, 115.49913597106934, 116.33335757255554, 117.16757917404175, 117.98999881744385, 118.81241846084595, 119.62693643569946, 120.44145441055298, 121.25673151016235, 122.07200860977173, 122.90414547920227, 123.73628234863281, 124.57782173156738, 125.41936111450195, 126.23175191879272, 127.0441427230835, 127.86402130126953, 128.68389987945557, 129.50524878501892, 130.32659769058228, 131.1589217185974, 131.99124574661255, 132.8155701160431, 133.63989448547363, 134.45453572273254, 135.26917695999146, 136.09095120429993, 136.9127254486084, 137.7357680797577, 138.55881071090698, 139.38707661628723, 140.21534252166748, 141.0470485687256, 141.8787546157837, 142.69839906692505, 143.5180435180664, 144.33976101875305, 145.1614785194397, 145.98300170898438, 146.80452489852905, 147.62806010246277, 148.45159530639648, 149.28635668754578, 150.12111806869507, 150.9416127204895, 151.76210737228394, 152.57992148399353, 153.39773559570312, 154.22159910202026, 155.0454626083374, 155.8630084991455, 156.6805543899536, 157.50202202796936, 158.3234896659851, 159.1534185409546, 159.98334741592407, 160.80451107025146, 161.62567472457886, 162.45385336875916, 163.28203201293945, 164.1007752418518, 164.91951847076416, 166.44659304618835, 167.97366762161255]
[11.1525, 11.1525, 13.13, 13.13, 15.9425, 15.9425, 21.1875, 21.1875, 23.065, 23.065, 23.6275, 23.6275, 24.0075, 24.0075, 27.655, 27.655, 30.385, 30.385, 35.4475, 35.4475, 40.51, 40.51, 45.3975, 45.3975, 50.2225, 50.2225, 53.3575, 53.3575, 55.87, 55.87, 57.38, 57.38, 60.115, 60.115, 60.375, 60.375, 61.8475, 61.8475, 62.095, 62.095, 63.75, 63.75, 64.5675, 64.5675, 65.955, 65.955, 67.165, 67.165, 67.4575, 67.4575, 70.085, 70.085, 70.855, 70.855, 71.395, 71.395, 72.12, 72.12, 72.8375, 72.8375, 73.2625, 73.2625, 73.4825, 73.4825, 74.0175, 74.0175, 74.8525, 74.8525, 75.7225, 75.7225, 76.845, 76.845, 77.755, 77.755, 78.32, 78.32, 78.95, 78.95, 79.4075, 79.4075, 80.415, 80.415, 80.925, 80.925, 81.045, 81.045, 81.2175, 81.2175, 81.2225, 81.2225, 81.35, 81.35, 81.5, 81.5, 81.735, 81.735, 81.845, 81.845, 81.9675, 81.9675, 82.0325, 82.0325, 82.0575, 82.0575, 82.0325, 82.0325, 82.0775, 82.0775, 82.205, 82.205, 82.2375, 82.2375, 82.25, 82.25, 82.285, 82.285, 82.2625, 82.2625, 82.4725, 82.4725, 82.6575, 82.6575, 82.645, 82.645, 82.6925, 82.6925, 82.71, 82.71, 82.725, 82.725, 82.705, 82.705, 82.78, 82.78, 82.7875, 82.7875, 82.745, 82.745, 82.7675, 82.7675, 82.7975, 82.7975, 82.7825, 82.7825, 82.8125, 82.8125, 82.84, 82.84, 82.8475, 82.8475, 82.835, 82.835, 82.8825, 82.8825, 82.85, 82.85, 82.89, 82.89, 82.91, 82.91, 82.915, 82.915, 82.9125, 82.9125, 82.9325, 82.9325, 82.9375, 82.9375, 82.94, 82.94, 82.9425, 82.9425, 82.91, 82.91, 82.9425, 82.9425, 82.945, 82.945, 82.9325, 82.9325, 82.9525, 82.9525, 82.9425, 82.9425, 82.94, 82.94, 82.965, 82.965, 82.9675, 82.9675, 82.9775, 82.9775, 82.9825, 82.9825, 82.98, 82.98, 83.2425, 83.2425, 83.25, 83.25, 83.3675, 83.3675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.533, Test loss: 2.239, Test accuracy: 26.22
Round   1, Train loss: 1.358, Test loss: 2.169, Test accuracy: 32.23
Round   2, Train loss: 1.399, Test loss: 2.113, Test accuracy: 38.34
Round   3, Train loss: 1.430, Test loss: 2.058, Test accuracy: 44.25
Round   4, Train loss: 1.319, Test loss: 2.015, Test accuracy: 48.57
Round   5, Train loss: 1.409, Test loss: 1.984, Test accuracy: 51.20
Round   6, Train loss: 1.365, Test loss: 1.958, Test accuracy: 53.74
Round   7, Train loss: 1.371, Test loss: 1.942, Test accuracy: 54.87
Round   8, Train loss: 1.352, Test loss: 1.928, Test accuracy: 56.00
Round   9, Train loss: 1.336, Test loss: 1.905, Test accuracy: 57.74
Round  10, Train loss: 1.317, Test loss: 1.895, Test accuracy: 58.90
Round  11, Train loss: 1.318, Test loss: 1.888, Test accuracy: 59.40
Round  12, Train loss: 1.353, Test loss: 1.866, Test accuracy: 61.58
Round  13, Train loss: 1.284, Test loss: 1.864, Test accuracy: 61.47
Round  14, Train loss: 1.298, Test loss: 1.857, Test accuracy: 61.90
Round  15, Train loss: 1.264, Test loss: 1.853, Test accuracy: 62.24
Round  16, Train loss: 1.345, Test loss: 1.850, Test accuracy: 62.62
Round  17, Train loss: 1.268, Test loss: 1.845, Test accuracy: 62.85
Round  18, Train loss: 1.260, Test loss: 1.841, Test accuracy: 63.29
Round  19, Train loss: 1.288, Test loss: 1.840, Test accuracy: 63.18
Round  20, Train loss: 1.264, Test loss: 1.839, Test accuracy: 63.12
Round  21, Train loss: 1.302, Test loss: 1.841, Test accuracy: 63.00
Round  22, Train loss: 1.319, Test loss: 1.838, Test accuracy: 63.32
Round  23, Train loss: 1.274, Test loss: 1.836, Test accuracy: 63.47
Round  24, Train loss: 1.236, Test loss: 1.833, Test accuracy: 63.60
Round  25, Train loss: 1.212, Test loss: 1.831, Test accuracy: 63.80
Round  26, Train loss: 1.248, Test loss: 1.830, Test accuracy: 63.83
Round  27, Train loss: 1.243, Test loss: 1.832, Test accuracy: 63.62
Round  28, Train loss: 1.316, Test loss: 1.824, Test accuracy: 64.48
Round  29, Train loss: 1.236, Test loss: 1.823, Test accuracy: 64.63
Round  30, Train loss: 1.280, Test loss: 1.821, Test accuracy: 64.79
Round  31, Train loss: 1.309, Test loss: 1.815, Test accuracy: 65.47
Round  32, Train loss: 1.304, Test loss: 1.815, Test accuracy: 65.39
Round  33, Train loss: 1.281, Test loss: 1.818, Test accuracy: 64.99
Round  34, Train loss: 1.282, Test loss: 1.820, Test accuracy: 64.82
Round  35, Train loss: 1.293, Test loss: 1.820, Test accuracy: 64.71
Round  36, Train loss: 1.270, Test loss: 1.821, Test accuracy: 64.66
Round  37, Train loss: 1.232, Test loss: 1.821, Test accuracy: 64.56
Round  38, Train loss: 1.275, Test loss: 1.814, Test accuracy: 65.48
Round  39, Train loss: 1.242, Test loss: 1.817, Test accuracy: 65.11
Round  40, Train loss: 1.271, Test loss: 1.820, Test accuracy: 64.74
Round  41, Train loss: 1.253, Test loss: 1.812, Test accuracy: 65.47
Round  42, Train loss: 1.204, Test loss: 1.814, Test accuracy: 65.25
Round  43, Train loss: 1.280, Test loss: 1.817, Test accuracy: 65.03
Round  44, Train loss: 1.277, Test loss: 1.820, Test accuracy: 64.66
Round  45, Train loss: 1.264, Test loss: 1.819, Test accuracy: 64.75
Round  46, Train loss: 1.290, Test loss: 1.824, Test accuracy: 64.23
Round  47, Train loss: 1.225, Test loss: 1.825, Test accuracy: 64.06
Round  48, Train loss: 1.230, Test loss: 1.824, Test accuracy: 64.10
Round  49, Train loss: 1.193, Test loss: 1.826, Test accuracy: 63.98
Round  50, Train loss: 1.263, Test loss: 1.823, Test accuracy: 64.38
Round  51, Train loss: 1.231, Test loss: 1.823, Test accuracy: 64.37
Round  52, Train loss: 1.205, Test loss: 1.828, Test accuracy: 63.65
Round  53, Train loss: 1.241, Test loss: 1.828, Test accuracy: 63.69
Round  54, Train loss: 1.208, Test loss: 1.830, Test accuracy: 63.47
Round  55, Train loss: 1.263, Test loss: 1.832, Test accuracy: 63.27
Round  56, Train loss: 1.204, Test loss: 1.833, Test accuracy: 63.17
Round  57, Train loss: 1.207, Test loss: 1.832, Test accuracy: 63.33
Round  58, Train loss: 1.230, Test loss: 1.834, Test accuracy: 63.01
Round  59, Train loss: 1.244, Test loss: 1.834, Test accuracy: 63.02
Round  60, Train loss: 1.266, Test loss: 1.835, Test accuracy: 62.80
Round  61, Train loss: 1.241, Test loss: 1.836, Test accuracy: 62.83
Round  62, Train loss: 1.216, Test loss: 1.833, Test accuracy: 63.17
Round  63, Train loss: 1.252, Test loss: 1.835, Test accuracy: 62.95
Round  64, Train loss: 1.255, Test loss: 1.825, Test accuracy: 63.98
Round  65, Train loss: 1.187, Test loss: 1.828, Test accuracy: 63.60
Round  66, Train loss: 1.243, Test loss: 1.829, Test accuracy: 63.41
Round  67, Train loss: 1.190, Test loss: 1.829, Test accuracy: 63.39
Round  68, Train loss: 1.204, Test loss: 1.827, Test accuracy: 63.68
Round  69, Train loss: 1.206, Test loss: 1.829, Test accuracy: 63.47
Round  70, Train loss: 1.263, Test loss: 1.828, Test accuracy: 63.53
Round  71, Train loss: 1.264, Test loss: 1.831, Test accuracy: 63.14
Round  72, Train loss: 1.212, Test loss: 1.831, Test accuracy: 63.13
Round  73, Train loss: 1.170, Test loss: 1.835, Test accuracy: 62.78
Round  74, Train loss: 1.239, Test loss: 1.835, Test accuracy: 62.71
Round  75, Train loss: 1.216, Test loss: 1.830, Test accuracy: 63.31
Round  76, Train loss: 1.274, Test loss: 1.827, Test accuracy: 63.61
Round  77, Train loss: 1.203, Test loss: 1.833, Test accuracy: 62.92
Round  78, Train loss: 1.226, Test loss: 1.835, Test accuracy: 62.60
Round  79, Train loss: 1.188, Test loss: 1.837, Test accuracy: 62.38
Round  80, Train loss: 1.239, Test loss: 1.843, Test accuracy: 61.88
Round  81, Train loss: 1.236, Test loss: 1.839, Test accuracy: 62.28
Round  82, Train loss: 1.265, Test loss: 1.840, Test accuracy: 62.13
Round  83, Train loss: 1.229, Test loss: 1.841, Test accuracy: 62.06
Round  84, Train loss: 1.276, Test loss: 1.844, Test accuracy: 61.65
Round  85, Train loss: 1.227, Test loss: 1.843, Test accuracy: 61.82
Round  86, Train loss: 1.239, Test loss: 1.843, Test accuracy: 61.72
Round  87, Train loss: 1.263, Test loss: 1.842, Test accuracy: 61.78
Round  88, Train loss: 1.255, Test loss: 1.841, Test accuracy: 61.98
Round  89, Train loss: 1.261, Test loss: 1.842, Test accuracy: 61.86
Round  90, Train loss: 1.203, Test loss: 1.840, Test accuracy: 62.04
Round  91, Train loss: 1.215, Test loss: 1.843, Test accuracy: 61.74
Round  92, Train loss: 1.204, Test loss: 1.846, Test accuracy: 61.45
Round  93, Train loss: 1.183, Test loss: 1.846, Test accuracy: 61.44
Round  94, Train loss: 1.216, Test loss: 1.842, Test accuracy: 61.86
Round  95, Train loss: 1.264, Test loss: 1.846, Test accuracy: 61.37
Round  96, Train loss: 1.227, Test loss: 1.850, Test accuracy: 61.11
Round  97, Train loss: 1.240, Test loss: 1.850, Test accuracy: 61.12
Round  98, Train loss: 1.229, Test loss: 1.851, Test accuracy: 60.92
Round  99, Train loss: 1.270, Test loss: 1.847, Test accuracy: 61.37
Final Round, Train loss: 1.226, Test loss: 1.859, Test accuracy: 60.13
Average accuracy final 10 rounds: 61.442
5241.538952112198
[]
[26.2225, 32.235, 38.345, 44.2525, 48.57, 51.1975, 53.745, 54.8675, 56.0, 57.74, 58.9025, 59.4025, 61.5825, 61.47, 61.9025, 62.2425, 62.625, 62.855, 63.2875, 63.1825, 63.115, 62.9975, 63.3175, 63.4675, 63.5975, 63.7975, 63.825, 63.615, 64.485, 64.6325, 64.79, 65.4725, 65.395, 64.99, 64.8175, 64.71, 64.6575, 64.5625, 65.485, 65.115, 64.7375, 65.4725, 65.25, 65.025, 64.655, 64.755, 64.2325, 64.0575, 64.1, 63.98, 64.3825, 64.3725, 63.6525, 63.6875, 63.47, 63.2675, 63.1725, 63.3325, 63.0125, 63.0225, 62.805, 62.825, 63.1725, 62.95, 63.985, 63.6025, 63.41, 63.3925, 63.6775, 63.47, 63.5275, 63.1425, 63.1325, 62.78, 62.7125, 63.3075, 63.6075, 62.9225, 62.605, 62.375, 61.8825, 62.2775, 62.135, 62.0575, 61.645, 61.82, 61.72, 61.78, 61.9775, 61.8575, 62.04, 61.74, 61.4475, 61.435, 61.8625, 61.3725, 61.1125, 61.115, 60.925, 61.37, 60.13]/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.822, Test loss: 1.785, Test accuracy: 67.67
Average accuracy final 10 rounds: 81.19025000000002
Average global accuracy final 10 rounds: 81.19025000000002
3230.4595789909363
[]
[18.72, 22.9475, 33.675, 25.215, 25.52, 30.705, 33.2925, 43.9575, 46.96, 44.49, 52.7775, 42.9725, 42.8, 42.5825, 51.6825, 55.7, 57.4225, 54.43, 56.205, 56.23, 58.2475, 61.2475, 62.8725, 64.8125, 67.87, 67.5825, 67.475, 66.9275, 68.445, 66.5975, 69.2675, 69.595, 69.1625, 70.4475, 71.515, 71.4625, 71.26, 71.4975, 72.825, 71.9025, 71.82, 72.045, 73.47, 72.6125, 73.1125, 74.6775, 74.505, 73.71, 74.1975, 73.3775, 75.3525, 75.535, 75.41, 74.51, 75.8575, 76.32, 77.7275, 77.5875, 77.0425, 76.0275, 79.2, 77.67, 78.0475, 78.18, 78.99, 77.9275, 79.14, 79.6325, 81.2125, 81.15, 79.5875, 78.805, 78.4675, 81.28, 80.8925, 79.76, 79.485, 81.575, 79.8375, 81.5825, 81.9575, 80.6975, 80.3275, 82.2575, 82.61, 82.6125, 82.7025, 83.32, 80.0475, 81.9875, 82.3375, 81.6775, 79.81, 79.8875, 80.8525, 81.015, 79.71, 81.0525, 82.2325, 83.3275, 67.6725]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.17
Final Round, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.12
Average accuracy final 10 rounds: 10.128499999999999 

Average global accuracy final 10 rounds: 10.0985 

2359.111440181732
[0.9749252796173096, 1.8853068351745605, 2.7952637672424316, 3.718672037124634, 4.636151313781738, 5.56502890586853, 6.498037576675415, 7.415283918380737, 8.333765029907227, 9.2465980052948, 10.165841341018677, 11.09115219116211, 12.018658876419067, 12.938913583755493, 13.853978872299194, 14.769006490707397, 15.692572116851807, 16.613264560699463, 17.53462791442871, 18.465952157974243, 19.370612621307373, 20.29097580909729, 21.20555090904236, 22.119187831878662, 23.03870940208435, 23.965438842773438, 24.892504453659058, 25.798141956329346, 26.710301160812378, 27.627185344696045, 28.54143214225769, 29.46967101097107, 30.394996881484985, 31.321362495422363, 32.24363851547241, 33.15874409675598, 34.06704044342041, 34.98096466064453, 35.89799404144287, 36.83255934715271, 37.75747489929199, 38.67163872718811, 39.59014582633972, 40.50565457344055, 41.43015098571777, 42.35781478881836, 43.28472137451172, 44.21854782104492, 45.13554811477661, 46.06046915054321, 46.97338604927063, 47.886144399642944, 48.802003145217896, 49.72377324104309, 50.64945697784424, 51.557905435562134, 52.4721941947937, 53.3909215927124, 54.30496048927307, 55.226962089538574, 56.16751480102539, 57.07558989524841, 57.99864196777344, 58.91144108772278, 59.82670331001282, 60.7410352230072, 61.666929960250854, 62.60548710823059, 63.518486738204956, 64.43349099159241, 65.36084842681885, 66.27837634086609, 67.1907000541687, 68.11729407310486, 69.0413019657135, 69.96546030044556, 70.88202929496765, 71.7982234954834, 72.72219157218933, 73.64303970336914, 74.56441116333008, 75.48748779296875, 76.40219116210938, 77.32573699951172, 78.23989939689636, 79.15253973007202, 80.07853960990906, 81.00423336029053, 81.93414974212646, 82.84865856170654, 83.76196432113647, 84.68970251083374, 85.60867047309875, 86.52438831329346, 87.46499705314636, 88.38775753974915, 89.30121421813965, 90.21092772483826, 91.1261796951294, 92.04220962524414, 93.88535380363464]
[9.9675, 9.965, 9.9725, 9.97, 9.9625, 9.9625, 9.9275, 9.935, 9.9425, 9.935, 9.965, 9.9775, 9.99, 9.99, 9.9625, 9.96, 9.9925, 10.0, 10.025, 10.0175, 10.015, 10.0325, 10.045, 10.04, 10.0275, 10.0175, 10.0375, 10.0275, 10.025, 10.035, 10.03, 10.0425, 10.0275, 10.045, 10.0425, 10.025, 10.0225, 10.035, 10.02, 10.025, 10.025, 9.995, 10.01, 10.005, 10.03, 10.025, 10.0225, 10.0275, 10.0425, 10.0525, 10.0425, 10.0625, 10.085, 10.0875, 10.09, 10.09, 10.0675, 10.0775, 10.0775, 10.0725, 10.0525, 10.07, 10.0675, 10.09, 10.0975, 10.08, 10.075, 10.0625, 10.045, 10.0925, 10.105, 10.1125, 10.1225, 10.1225, 10.125, 10.08, 10.065, 10.0875, 10.1025, 10.0775, 10.0625, 10.0975, 10.1, 10.1025, 10.1, 10.115, 10.1125, 10.1, 10.1225, 10.125, 10.1175, 10.1075, 10.105, 10.115, 10.12, 10.1425, 10.13, 10.1325, 10.1525, 10.1625, 10.17]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Final Round, Train loss: 1.734, Test loss: 2.250, Test accuracy: 18.99
Average accuracy final 10 rounds: 18.5355
2241.0567157268524
[2.306466579437256, 4.516334295272827, 6.746570348739624, 8.956136226654053, 11.155742883682251, 13.362169027328491, 15.574314832687378, 17.7836594581604, 20.011950969696045, 22.294023752212524, 24.50300145149231, 26.777802228927612, 28.966130018234253, 31.20405411720276, 33.37444567680359, 35.65556311607361, 37.84649872779846, 40.05059552192688, 42.239781618118286, 44.435797929763794, 46.62360215187073, 48.80820631980896, 51.01256275177002, 53.20874285697937, 55.46393179893494, 57.66675853729248, 59.93452429771423, 62.130178689956665, 64.39520788192749, 66.58162307739258, 68.80354237556458, 71.00830435752869, 73.2418999671936, 75.45313143730164, 77.66420388221741, 79.85881471633911, 82.06306982040405, 84.28583097457886, 86.50137257575989, 88.72717571258545, 90.91576600074768, 93.17071652412415, 95.34989857673645, 97.60831713676453, 99.81287050247192, 102.05574917793274, 104.26880192756653, 106.51668667793274, 108.7153856754303, 110.96023988723755, 113.1651840209961, 115.36870384216309, 117.57637619972229, 119.80059862136841, 122.00949573516846, 124.1983916759491, 126.43402457237244, 128.65846872329712, 130.8712854385376, 133.1022868156433, 135.3035225868225, 137.55765891075134, 139.77595210075378, 141.97922587394714, 144.20621061325073, 146.42774033546448, 148.63540244102478, 150.86092567443848, 153.06597590446472, 155.3274896144867, 157.56809782981873, 159.80496525764465, 162.02951383590698, 164.2780110836029, 166.4927475452423, 168.7777714729309, 170.9916570186615, 173.2027337551117, 175.38000535964966, 177.59912943840027, 179.79325461387634, 181.96690249443054, 184.1888711452484, 186.3727467060089, 188.58297324180603, 190.79646134376526, 192.9865243434906, 195.19558358192444, 197.42492938041687, 199.62358808517456, 201.81449222564697, 204.00788116455078, 206.19607520103455, 208.38761734962463, 210.576269865036, 212.77635836601257, 214.9340410232544, 217.11692881584167, 219.2859709262848, 221.45576977729797, 223.63999485969543]
[10.32, 10.4175, 10.5025, 10.4525, 10.5075, 10.655, 10.345, 10.51, 11.8425, 14.0675, 15.6225, 15.42, 15.7475, 15.495, 16.5875, 17.11, 17.4, 17.5825, 17.1925, 18.2625, 17.3125, 15.855, 17.2275, 18.0875, 17.9775, 17.7975, 16.8075, 17.625, 17.8225, 15.94, 15.695, 16.23, 16.695, 15.1975, 15.155, 15.045, 17.18, 16.98, 15.67, 16.66, 16.2425, 16.5425, 17.1025, 17.305, 17.91, 18.1575, 16.2825, 17.945, 16.9025, 17.1825, 18.165, 17.57, 17.6675, 18.3475, 17.3625, 17.865, 18.0475, 17.3, 17.5925, 17.5175, 18.3925, 18.0375, 17.4975, 17.325, 17.995, 18.76, 17.3925, 17.1, 18.7, 17.5025, 18.7, 19.5675, 18.43, 17.2275, 18.4275, 17.385, 17.575, 19.13, 18.295, 18.6525, 18.2825, 17.0525, 18.525, 19.215, 17.5325, 18.4975, 18.0375, 19.035, 19.16, 18.185, 19.2075, 18.12, 18.945, 18.4175, 18.4475, 18.5075, 19.0925, 18.055, 18.44, 18.1225, 18.9925]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 2.295, Test loss: 2.303, Test accuracy: 10.70
Average accuracy final 10 rounds: 10.946250000000001
1477.1555230617523
[1.1186606884002686, 2.0902955532073975, 3.0415587425231934, 3.983788013458252, 4.972137928009033, 5.922063589096069, 6.879536867141724, 7.826584100723267, 8.769288778305054, 9.73146390914917, 10.660804510116577, 11.624125719070435, 12.565032720565796, 13.509205102920532, 14.449535131454468, 15.387182235717773, 16.335977792739868, 17.29727578163147, 18.228991746902466, 19.184650182724, 20.125194311141968, 21.067282915115356, 22.021713972091675, 22.954951763153076, 23.90515637397766, 24.84477734565735, 25.813157081604004, 26.749692916870117, 27.69156241416931, 28.638837575912476, 29.592222213745117, 30.536611080169678, 31.485602855682373, 32.4204638004303, 33.36740708351135, 34.314229011535645, 35.269715309143066, 36.222758531570435, 37.17724418640137, 38.14406442642212, 39.0909481048584, 40.06336283683777, 41.013195753097534, 41.97880816459656, 42.921106815338135, 43.879258155822754, 44.84882187843323, 45.79321217536926, 46.74353623390198, 47.69693350791931, 48.64713191986084, 49.62391972541809, 50.575531244277954, 51.516988039016724, 52.479146003723145, 53.41593360900879, 54.386372089385986, 55.31577396392822, 56.252530574798584, 57.1949667930603, 58.12679600715637, 59.08003211021423, 60.02089214324951, 60.96199369430542, 61.89816331863403, 62.84416842460632, 63.789332151412964, 64.74627041816711, 65.68450784683228, 66.62455248832703, 67.55841565132141, 68.5228807926178, 69.46379685401917, 70.40251636505127, 71.33894491195679, 72.2765736579895, 73.2486081123352, 74.18651223182678, 75.13248610496521, 76.06260681152344, 77.02038407325745, 77.96615076065063, 78.90653347969055, 79.8423080444336, 80.78660345077515, 81.72540473937988, 82.69051694869995, 83.63838315010071, 84.596843957901, 85.54383087158203, 86.49204683303833, 87.46006178855896, 88.39849615097046, 89.37291407585144, 90.3108983039856, 91.25392556190491, 92.22226595878601, 93.16365027427673, 94.11538577079773, 95.05864262580872, 96.47466945648193]
[9.725, 9.8025, 9.855, 9.9225, 9.9425, 9.9575, 9.9775, 9.98, 9.98, 9.9875, 9.99, 9.99, 9.98, 9.99, 9.9875, 9.98, 9.9825, 9.9875, 9.99, 9.9825, 9.985, 10.0, 10.0425, 9.99, 9.9875, 9.9125, 9.9225, 9.9, 9.8225, 9.8375, 9.7175, 9.76, 9.5775, 9.69, 9.61, 9.52, 9.6075, 9.63, 9.62, 9.625, 9.655, 9.6925, 9.67, 9.685, 9.5425, 9.6625, 9.74, 9.7275, 9.8075, 9.8075, 9.985, 10.0525, 10.045, 10.24, 10.335, 10.28, 10.3125, 10.3975, 10.3675, 10.31, 10.485, 10.4325, 10.4525, 10.3925, 10.515, 10.605, 10.605, 10.5025, 10.515, 10.5775, 10.5, 10.5075, 10.5, 10.6375, 10.495, 10.5175, 10.545, 10.6575, 10.7075, 10.675, 10.6225, 10.5925, 10.565, 10.695, 10.71, 10.715, 10.75, 10.765, 10.7475, 10.795, 10.8875, 10.91, 10.855, 10.96, 10.9, 10.9125, 10.985, 11.035, 11.0325, 10.985, 10.7025]
/home/ChenSM/code/FL_HLS/utils/sampling.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Final Round, Train loss: 1.698, Test loss: 1.728, Test accuracy: 75.88
Average accuracy final 10 rounds: 74.23024999999998
2344.0700929164886
[1.0678491592407227, 2.1356983184814453, 3.0535435676574707, 3.971388816833496, 4.967177867889404, 5.9629669189453125, 6.868284463882446, 7.77360200881958, 8.73523211479187, 9.69686222076416, 10.631729125976562, 11.566596031188965, 12.473798990249634, 13.381001949310303, 14.313158988952637, 15.24531602859497, 16.166951179504395, 17.08858633041382, 18.010783195495605, 18.932980060577393, 19.84444308280945, 20.755906105041504, 21.690014362335205, 22.624122619628906, 23.59577965736389, 24.567436695098877, 25.50891423225403, 26.45039176940918, 27.40097188949585, 28.35155200958252, 29.30556058883667, 30.25956916809082, 31.208925485610962, 32.1582818031311, 33.12892508506775, 34.099568367004395, 35.03945350646973, 35.97933864593506, 36.919498920440674, 37.85965919494629, 38.81755566596985, 39.77545213699341, 40.72148680686951, 41.667521476745605, 42.66656494140625, 43.665608406066895, 44.60704231262207, 45.548476219177246, 46.493720293045044, 47.43896436691284, 48.39971733093262, 49.36047029495239, 50.30635476112366, 51.25223922729492, 52.21564793586731, 53.1790566444397, 54.130035638809204, 55.08101463317871, 56.03704476356506, 56.993074893951416, 57.98271942138672, 58.97236394882202, 59.922951221466064, 60.87353849411011, 61.82298493385315, 62.77243137359619, 63.71177959442139, 64.65112781524658, 65.5947482585907, 66.53836870193481, 67.50442910194397, 68.47048950195312, 69.42041325569153, 70.37033700942993, 71.31957030296326, 72.26880359649658, 73.24827790260315, 74.22775220870972, 75.1451256275177, 76.06249904632568, 77.05957555770874, 78.0566520690918, 79.00592398643494, 79.95519590377808, 80.89827418327332, 81.84135246276855, 82.83733177185059, 83.83331108093262, 84.7929573059082, 85.75260353088379, 86.69990849494934, 87.64721345901489, 88.63154077529907, 89.61586809158325, 90.55758309364319, 91.49929809570312, 92.44893145561218, 93.39856481552124, 94.35456347465515, 95.31056213378906, 96.26328659057617, 97.21601104736328, 98.20676183700562, 99.19751262664795, 100.1401526927948, 101.08279275894165, 102.02612733840942, 102.9694619178772, 103.92173862457275, 104.87401533126831, 105.85348439216614, 106.83295345306396, 107.7982804775238, 108.76360750198364, 109.70889568328857, 110.6541838645935, 111.60051274299622, 112.54684162139893, 113.48787331581116, 114.42890501022339, 115.38331151008606, 116.33771800994873, 117.31789374351501, 118.2980694770813, 119.2506091594696, 120.20314884185791, 121.14224672317505, 122.08134460449219, 123.03269147872925, 123.98403835296631, 124.92811369895935, 125.87218904495239, 126.81764006614685, 127.76309108734131, 128.72598099708557, 129.68887090682983, 130.62537932395935, 131.56188774108887, 132.52616620063782, 133.49044466018677, 134.4426474571228, 135.39485025405884, 136.34684944152832, 137.2988486289978, 138.2682180404663, 139.23758745193481, 140.16433095932007, 141.09107446670532, 142.0423710346222, 142.99366760253906, 143.95784997940063, 144.9220323562622, 145.8757839202881, 146.82953548431396, 147.77711844444275, 148.72470140457153, 149.6636927127838, 150.6026840209961, 151.54755973815918, 152.49243545532227, 153.45880651474, 154.42517757415771, 155.3742504119873, 156.3233232498169, 157.27499175071716, 158.22666025161743, 159.1746096611023, 160.12255907058716, 161.0685999393463, 162.01464080810547, 162.98089790344238, 163.9471549987793, 164.8906946182251, 165.8342342376709, 166.76732230186462, 167.70041036605835, 168.65752124786377, 169.6146321296692, 170.57537698745728, 171.53612184524536, 172.50620341300964, 173.47628498077393, 174.40936398506165, 175.34244298934937, 176.28291153907776, 177.22338008880615, 178.19502687454224, 179.16667366027832, 180.1125295162201, 181.05838537216187, 182.00006294250488, 182.9417405128479, 183.87554454803467, 184.80934858322144, 185.76255893707275, 186.71576929092407, 187.67853045463562, 188.64129161834717, 189.5849769115448, 190.52866220474243, 191.9403805732727, 193.35209894180298]
[10.6875, 10.6875, 11.0775, 11.0775, 11.6275, 11.6275, 12.3775, 12.3775, 12.555, 12.555, 13.0325, 13.0325, 12.6575, 12.6575, 12.645, 12.645, 11.8775, 11.8775, 12.635, 12.635, 13.5575, 13.5575, 13.67, 13.67, 14.125, 14.125, 15.77, 15.77, 18.1375, 18.1375, 19.81, 19.81, 20.0225, 20.0225, 21.685, 21.685, 23.1825, 23.1825, 26.335, 26.335, 27.7, 27.7, 29.925, 29.925, 31.445, 31.445, 33.655, 33.655, 35.3575, 35.3575, 37.885, 37.885, 38.7075, 38.7075, 39.9075, 39.9075, 40.8075, 40.8075, 42.9525, 42.9525, 44.265, 44.265, 45.155, 45.155, 45.8825, 45.8825, 46.4225, 46.4225, 48.18, 48.18, 49.18, 49.18, 49.93, 49.93, 50.6825, 50.6825, 51.3075, 51.3075, 51.8375, 51.8375, 52.44, 52.44, 53.1525, 53.1525, 53.8025, 53.8025, 54.3325, 54.3325, 55.185, 55.185, 56.0375, 56.0375, 56.5425, 56.5425, 57.4775, 57.4775, 57.7775, 57.7775, 58.6625, 58.6625, 58.9525, 58.9525, 59.6725, 59.6725, 60.565, 60.565, 61.0025, 61.0025, 61.2525, 61.2525, 61.795, 61.795, 61.98, 61.98, 62.4475, 62.4475, 63.1675, 63.1675, 63.8225, 63.8225, 64.17, 64.17, 64.2475, 64.2475, 64.4925, 64.4925, 64.8325, 64.8325, 64.905, 64.905, 65.67, 65.67, 66.15, 66.15, 66.4925, 66.4925, 67.2125, 67.2125, 67.235, 67.235, 67.4575, 67.4575, 67.9025, 67.9025, 68.3725, 68.3725, 68.805, 68.805, 69.05, 69.05, 69.4525, 69.4525, 69.4875, 69.4875, 69.56, 69.56, 69.66, 69.66, 69.66, 69.66, 69.885, 69.885, 69.9575, 69.9575, 70.1725, 70.1725, 70.4625, 70.4625, 71.3225, 71.3225, 71.5075, 71.5075, 71.965, 71.965, 72.265, 72.265, 72.6175, 72.6175, 72.9325, 72.9325, 73.18, 73.18, 73.2675, 73.2675, 73.49, 73.49, 74.265, 74.265, 74.52, 74.52, 74.52, 74.52, 74.5975, 74.5975, 74.7425, 74.7425, 74.855, 74.855, 74.865, 74.865, 75.8775, 75.8775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 44, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 57, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 1, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 132, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.342, Test loss: 1.720, Test accuracy: 60.97
Final Round, Global train loss: 0.342, Global test loss: 1.206, Global test accuracy: 67.95
Average accuracy final 10 rounds: 61.56200000000001 

Average global accuracy final 10 rounds: 67.44 

3842.854815721512
[1.4312183856964111, 2.8624367713928223, 4.064172029495239, 5.265907287597656, 6.4619300365448, 7.657952785491943, 8.859562158584595, 10.061171531677246, 11.266197204589844, 12.471222877502441, 13.673750638961792, 14.876278400421143, 16.075825452804565, 17.27537250518799, 18.478500843048096, 19.681629180908203, 20.883482217788696, 22.08533525466919, 23.288463354110718, 24.491591453552246, 25.695099353790283, 26.89860725402832, 28.10676336288452, 29.314919471740723, 30.516390085220337, 31.71786069869995, 32.92339324951172, 34.128925800323486, 35.34005618095398, 36.55118656158447, 37.75223898887634, 38.95329141616821, 40.155388832092285, 41.35748624801636, 42.5633909702301, 43.76929569244385, 44.95456624031067, 46.13983678817749, 47.33767604827881, 48.53551530838013, 49.716655254364014, 50.8977952003479, 51.96817469596863, 53.038554191589355, 54.10170912742615, 55.16486406326294, 56.23361563682556, 57.302367210388184, 58.36911582946777, 59.43586444854736, 60.50204658508301, 61.56822872161865, 62.63378667831421, 63.699344635009766, 64.76677918434143, 65.8342137336731, 66.89971232414246, 67.96521091461182, 69.03632164001465, 70.10743236541748, 71.16925191879272, 72.23107147216797, 73.29897880554199, 74.36688613891602, 75.4336805343628, 76.50047492980957, 77.56550550460815, 78.63053607940674, 79.69182085990906, 80.75310564041138, 81.81901407241821, 82.88492250442505, 83.92069792747498, 84.9564733505249, 85.98610877990723, 87.01574420928955, 88.04712414741516, 89.07850408554077, 90.1116771697998, 91.14485025405884, 92.17758202552795, 93.21031379699707, 94.28779029846191, 95.36526679992676, 96.41236710548401, 97.45946741104126, 98.6119315624237, 99.76439571380615, 101.02845668792725, 102.29251766204834, 103.53619456291199, 104.77987146377563, 106.02944588661194, 107.27902030944824, 108.53657484054565, 109.79412937164307, 111.04821586608887, 112.30230236053467, 113.5559766292572, 114.80965089797974, 116.07279539108276, 117.33593988418579, 118.6066381931305, 119.8773365020752, 121.1376485824585, 122.3979606628418, 123.65762090682983, 124.91728115081787, 126.17679023742676, 127.43629932403564, 128.69263648986816, 129.94897365570068, 131.23707604408264, 132.5251784324646, 133.784321308136, 135.04346418380737, 136.30647349357605, 137.56948280334473, 138.8303017616272, 140.09112071990967, 141.34933257102966, 142.60754442214966, 143.8673779964447, 145.12721157073975, 146.38833594322205, 147.64946031570435, 148.91835594177246, 150.18725156784058, 151.4492266178131, 152.71120166778564, 153.9821057319641, 155.25300979614258, 156.52400493621826, 157.79500007629395, 159.0628969669342, 160.33079385757446, 161.5997292995453, 162.8686647415161, 164.1367049217224, 165.4047451019287, 166.67417645454407, 167.94360780715942, 169.2148540019989, 170.48610019683838, 171.7517123222351, 173.01732444763184, 174.27768874168396, 175.53805303573608, 176.80490469932556, 178.07175636291504, 179.33773350715637, 180.6037106513977, 181.81657671928406, 183.0294427871704, 184.3134217262268, 185.5974006652832, 186.86374497413635, 188.1300892829895, 189.2429850101471, 190.3558807373047, 191.47597646713257, 192.59607219696045, 193.7738447189331, 194.95161724090576, 196.13573575019836, 197.31985425949097, 198.5002965927124, 199.68073892593384, 200.86314010620117, 202.0455412864685, 203.24029397964478, 204.43504667282104, 205.63024806976318, 206.82544946670532, 207.85585355758667, 208.88625764846802, 209.9119839668274, 210.93771028518677, 211.98884415626526, 213.03997802734375, 214.0820107460022, 215.12404346466064, 216.16281008720398, 217.20157670974731, 218.22751379013062, 219.25345087051392, 220.28908467292786, 221.3247184753418, 222.36156153678894, 223.39840459823608, 224.4418339729309, 225.48526334762573, 226.52282190322876, 227.5603804588318, 228.59467697143555, 229.6289734840393, 230.66608452796936, 231.7031955718994, 232.73899745941162, 233.77479934692383, 235.87626433372498, 237.97772932052612]
[22.7025, 22.7025, 28.2825, 28.2825, 29.8325, 29.8325, 31.975, 31.975, 34.8475, 34.8475, 36.7125, 36.7125, 37.6725, 37.6725, 39.1975, 39.1975, 40.01, 40.01, 42.775, 42.775, 43.9925, 43.9925, 45.0825, 45.0825, 45.3725, 45.3725, 46.0575, 46.0575, 46.99, 46.99, 47.615, 47.615, 48.875, 48.875, 49.2125, 49.2125, 49.64, 49.64, 50.3125, 50.3125, 50.6775, 50.6775, 51.075, 51.075, 52.0675, 52.0675, 52.7525, 52.7525, 52.7825, 52.7825, 53.2125, 53.2125, 53.9025, 53.9025, 54.185, 54.185, 54.905, 54.905, 55.3, 55.3, 55.53, 55.53, 55.885, 55.885, 55.815, 55.815, 56.09, 56.09, 55.8575, 55.8575, 56.13, 56.13, 56.35, 56.35, 56.1625, 56.1625, 56.645, 56.645, 56.6075, 56.6075, 57.1625, 57.1625, 57.105, 57.105, 57.125, 57.125, 57.2625, 57.2625, 57.7125, 57.7125, 57.9625, 57.9625, 58.125, 58.125, 58.2725, 58.2725, 58.1675, 58.1675, 58.0625, 58.0625, 57.8925, 57.8925, 58.0425, 58.0425, 58.4425, 58.4425, 58.825, 58.825, 59.0625, 59.0625, 59.155, 59.155, 59.0175, 59.0175, 58.805, 58.805, 59.305, 59.305, 59.285, 59.285, 59.515, 59.515, 59.6575, 59.6575, 59.445, 59.445, 59.605, 59.605, 59.625, 59.625, 59.75, 59.75, 60.2075, 60.2075, 60.6075, 60.6075, 60.88, 60.88, 60.7125, 60.7125, 60.47, 60.47, 60.135, 60.135, 60.255, 60.255, 60.6425, 60.6425, 60.365, 60.365, 60.47, 60.47, 60.51, 60.51, 60.5125, 60.5125, 60.6725, 60.6725, 60.4775, 60.4775, 60.2475, 60.2475, 60.305, 60.305, 60.84, 60.84, 61.07, 61.07, 61.48, 61.48, 61.2675, 61.2675, 61.015, 61.015, 61.28, 61.28, 61.045, 61.045, 61.0075, 61.0075, 61.57, 61.57, 61.5275, 61.5275, 61.145, 61.145, 61.3325, 61.3325, 61.65, 61.65, 61.6775, 61.6775, 61.64, 61.64, 61.82, 61.82, 61.7025, 61.7025, 61.555, 61.555, 60.965, 60.965]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.2  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.527, Test loss: 1.286, Test accuracy: 63.05
Average accuracy final 10 rounds: 62.4825 

2687.610002756119
[1.2846174240112305, 2.569234848022461, 3.6508123874664307, 4.7323899269104, 5.801991701126099, 6.871593475341797, 7.939345121383667, 9.007096767425537, 10.078687906265259, 11.15027904510498, 12.218721389770508, 13.287163734436035, 14.353949785232544, 15.420735836029053, 16.490973472595215, 17.561211109161377, 18.629606008529663, 19.69800090789795, 20.771787881851196, 21.845574855804443, 22.913395166397095, 23.981215476989746, 25.048197507858276, 26.115179538726807, 27.18300771713257, 28.25083589553833, 29.320263862609863, 30.389691829681396, 31.43028473854065, 32.4708776473999, 33.54665017127991, 34.62242269515991, 35.684701919555664, 36.746981143951416, 37.82113528251648, 38.89528942108154, 39.970516204833984, 41.045742988586426, 42.12040090560913, 43.195058822631836, 44.26782822608948, 45.34059762954712, 46.422542095184326, 47.50448656082153, 48.582359313964844, 49.660232067108154, 50.735970973968506, 51.81170988082886, 52.89794898033142, 53.984188079833984, 55.05120062828064, 56.118213176727295, 57.189016342163086, 58.25981950759888, 59.32719802856445, 60.39457654953003, 61.45838642120361, 62.5221962928772, 63.58725547790527, 64.65231466293335, 65.73739671707153, 66.82247877120972, 67.89599370956421, 68.9695086479187, 70.0457911491394, 71.12207365036011, 72.2003333568573, 73.27859306335449, 74.36113715171814, 75.44368124008179, 76.51972532272339, 77.59576940536499, 78.66876697540283, 79.74176454544067, 80.80540657043457, 81.86904859542847, 82.93740963935852, 84.00577068328857, 85.07926917076111, 86.15276765823364, 87.22435593605042, 88.29594421386719, 89.37230181694031, 90.44865942001343, 91.52245450019836, 92.5962495803833, 93.66272068023682, 94.72919178009033, 95.79570031166077, 96.8622088432312, 97.83177471160889, 98.80134057998657, 99.8027012348175, 100.80406188964844, 101.7799563407898, 102.75585079193115, 103.7311053276062, 104.70635986328125, 105.6702013015747, 106.63404273986816, 107.59753680229187, 108.56103086471558, 109.53227829933167, 110.50352573394775, 111.47101497650146, 112.43850421905518, 113.40670347213745, 114.37490272521973, 115.3437328338623, 116.31256294250488, 117.28404140472412, 118.25551986694336, 119.2246642112732, 120.19380855560303, 121.16504096984863, 122.13627338409424, 123.10308575630188, 124.06989812850952, 125.0354528427124, 126.00100755691528, 126.9807357788086, 127.9604640007019, 128.93560457229614, 129.91074514389038, 130.88130235671997, 131.85185956954956, 132.82111358642578, 133.790367603302, 134.76587963104248, 135.74139165878296, 136.71386742591858, 137.6863431930542, 138.65326976776123, 139.62019634246826, 140.5852427482605, 141.55028915405273, 142.519362449646, 143.48843574523926, 144.459566116333, 145.43069648742676, 146.4009711742401, 147.37124586105347, 148.34719467163086, 149.32314348220825, 150.29350805282593, 151.2638726234436, 152.23569560050964, 153.20751857757568, 154.20197296142578, 155.19642734527588, 156.16787457466125, 157.13932180404663, 158.10676860809326, 159.0742154121399, 160.0417766571045, 161.0093379020691, 161.98538327217102, 162.96142864227295, 163.9308888912201, 164.90034914016724, 165.885910987854, 166.87147283554077, 167.84241318702698, 168.81335353851318, 169.78418636322021, 170.75501918792725, 171.72531867027283, 172.6956181526184, 173.7102916240692, 174.72496509552002, 175.68668794631958, 176.64841079711914, 177.64805388450623, 178.6476969718933, 179.62169742584229, 180.59569787979126, 181.5724401473999, 182.54918241500854, 183.52098608016968, 184.4927897453308, 185.47272157669067, 186.45265340805054, 187.42359280586243, 188.39453220367432, 189.38015723228455, 190.36578226089478, 191.33791041374207, 192.31003856658936, 193.28025031089783, 194.2504620552063, 195.2228479385376, 196.1952338218689, 197.1674144268036, 198.13959503173828, 199.11858701705933, 200.09757900238037, 201.07422375679016, 202.05086851119995, 203.02888250350952, 204.0068964958191, 205.8347203731537, 207.66254425048828]
[15.385, 15.385, 25.2775, 25.2775, 28.5575, 28.5575, 32.1725, 32.1725, 33.4, 33.4, 36.035, 36.035, 38.205, 38.205, 39.645, 39.645, 41.545, 41.545, 42.5675, 42.5675, 43.5625, 43.5625, 44.3475, 44.3475, 44.5675, 44.5675, 44.6675, 44.6675, 46.0075, 46.0075, 46.4975, 46.4975, 47.425, 47.425, 47.735, 47.735, 48.7425, 48.7425, 48.97, 48.97, 49.6375, 49.6375, 49.12, 49.12, 50.275, 50.275, 50.055, 50.055, 50.1725, 50.1725, 51.75, 51.75, 52.795, 52.795, 52.77, 52.77, 53.195, 53.195, 53.835, 53.835, 54.4275, 54.4275, 54.67, 54.67, 55.2, 55.2, 55.5875, 55.5875, 55.8675, 55.8675, 56.53, 56.53, 56.9925, 56.9925, 56.7625, 56.7625, 57.275, 57.275, 57.685, 57.685, 57.805, 57.805, 57.5925, 57.5925, 57.6425, 57.6425, 57.4525, 57.4525, 57.705, 57.705, 58.24, 58.24, 58.3875, 58.3875, 58.5725, 58.5725, 58.755, 58.755, 58.63, 58.63, 58.5825, 58.5825, 58.7925, 58.7925, 58.8575, 58.8575, 59.585, 59.585, 59.2475, 59.2475, 59.715, 59.715, 59.345, 59.345, 59.79, 59.79, 60.035, 60.035, 60.285, 60.285, 60.3725, 60.3725, 60.3875, 60.3875, 60.3225, 60.3225, 60.8925, 60.8925, 60.47, 60.47, 61.05, 61.05, 61.1075, 61.1075, 60.895, 60.895, 61.3875, 61.3875, 60.665, 60.665, 60.855, 60.855, 61.24, 61.24, 60.9725, 60.9725, 61.5175, 61.5175, 61.36, 61.36, 61.9175, 61.9175, 62.1425, 62.1425, 61.8675, 61.8675, 62.0575, 62.0575, 62.45, 62.45, 62.4225, 62.4225, 62.885, 62.885, 62.6675, 62.6675, 62.2925, 62.2925, 62.62, 62.62, 62.46, 62.46, 61.6525, 61.6525, 62.2475, 62.2475, 62.5375, 62.5375, 62.2425, 62.2425, 62.1525, 62.1525, 62.515, 62.515, 62.0625, 62.0625, 62.8675, 62.8675, 62.5225, 62.5225, 62.5075, 62.5075, 62.4125, 62.4125, 62.3, 62.3, 62.83, 62.83, 62.655, 62.655, 63.05, 63.05]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.237, Test loss: 0.397, Test accuracy: 84.46
Average accuracy final 10 rounds: 84.3025
1491.623164653778
[1.8342244625091553, 3.6684489250183105, 5.072270631790161, 6.476092338562012, 7.89608359336853, 9.316074848175049, 10.717277765274048, 12.118480682373047, 13.528090238571167, 14.937699794769287, 16.342358589172363, 17.74701738357544, 19.15899109840393, 20.570964813232422, 21.965773105621338, 23.360581398010254, 24.77874183654785, 26.19690227508545, 27.595152378082275, 28.9934024810791, 30.40493106842041, 31.81645965576172, 33.207109689712524, 34.59775972366333, 35.9987268447876, 37.399693965911865, 38.802769899368286, 40.20584583282471, 41.61779522895813, 43.02974462509155, 44.41097688674927, 45.79220914840698, 47.18886041641235, 48.585511684417725, 49.991923093795776, 51.39833450317383, 52.799930810928345, 54.20152711868286, 55.604918241500854, 57.00830936431885, 58.41040778160095, 59.81250619888306, 61.20691442489624, 62.601322650909424, 64.00792002677917, 65.41451740264893, 66.83262777328491, 68.2507381439209, 69.6642735004425, 71.07780885696411, 72.49171090126038, 73.90561294555664, 75.32426881790161, 76.74292469024658, 78.16959428787231, 79.59626388549805, 81.01842951774597, 82.4405951499939, 83.85445499420166, 85.26831483840942, 86.67615485191345, 88.08399486541748, 89.49921798706055, 90.91444110870361, 92.32771182060242, 93.74098253250122, 95.15525698661804, 96.56953144073486, 97.98756456375122, 99.40559768676758, 100.82647109031677, 102.24734449386597, 103.66074109077454, 105.0741376876831, 106.48289561271667, 107.89165353775024, 109.30408930778503, 110.71652507781982, 112.12197542190552, 113.52742576599121, 114.93009519577026, 116.33276462554932, 117.74926948547363, 119.16577434539795, 120.56359314918518, 121.96141195297241, 123.37947797775269, 124.79754400253296, 126.19931936264038, 127.6010947227478, 129.01956629753113, 130.43803787231445, 131.84094905853271, 133.24386024475098, 134.66684293746948, 136.089825630188, 137.50030827522278, 138.91079092025757, 140.3368363380432, 141.76288175582886, 143.17157554626465, 144.58026933670044, 146.0092475414276, 147.43822574615479, 148.8406102657318, 150.24299478530884, 151.6654245853424, 153.08785438537598, 154.5041434764862, 155.92043256759644, 157.34253001213074, 158.76462745666504, 160.16962718963623, 161.57462692260742, 162.98889207839966, 164.4031572341919, 165.8063440322876, 167.2095308303833, 168.64188265800476, 170.07423448562622, 171.49318265914917, 172.91213083267212, 174.3298375606537, 175.74754428863525, 177.15799355506897, 178.56844282150269, 179.99366998672485, 181.41889715194702, 182.84034657478333, 184.26179599761963, 185.68099451065063, 187.10019302368164, 188.51346397399902, 189.9267349243164, 191.33110547065735, 192.7354760169983, 194.1373167037964, 195.53915739059448, 196.93907165527344, 198.3389859199524, 199.72982835769653, 201.12067079544067, 202.51517009735107, 203.90966939926147, 205.31890964508057, 206.72814989089966, 208.1299865245819, 209.53182315826416, 210.94000434875488, 212.3481855392456, 213.76279473304749, 215.17740392684937, 216.58197164535522, 217.98653936386108, 219.3915319442749, 220.79652452468872, 222.19685816764832, 223.5971918106079, 224.99437284469604, 226.39155387878418, 227.7915289402008, 229.19150400161743, 230.5901825428009, 231.98886108398438, 233.39774775505066, 234.80663442611694, 236.22002029418945, 237.63340616226196, 239.03261995315552, 240.43183374404907, 241.82473826408386, 243.21764278411865, 244.6148898601532, 246.01213693618774, 247.4047532081604, 248.79736948013306, 250.2025501728058, 251.60773086547852, 253.00108098983765, 254.39443111419678, 255.79432082176208, 257.1942105293274, 258.59302258491516, 259.99183464050293, 261.3962473869324, 262.8006601333618, 264.2117302417755, 265.6228003501892, 267.0239288806915, 268.42505741119385, 269.8233952522278, 271.2217330932617, 272.62514758110046, 274.0285620689392, 275.42203760147095, 276.8155131340027, 278.213885307312, 279.61225748062134, 281.0110914707184, 282.40992546081543, 284.53646755218506, 286.6630096435547]
[18.158333333333335, 18.158333333333335, 34.375, 34.375, 38.083333333333336, 38.083333333333336, 55.34166666666667, 55.34166666666667, 57.483333333333334, 57.483333333333334, 57.416666666666664, 57.416666666666664, 64.475, 64.475, 66.64166666666667, 66.64166666666667, 69.56666666666666, 69.56666666666666, 68.9, 68.9, 68.21666666666667, 68.21666666666667, 69.55833333333334, 69.55833333333334, 73.425, 73.425, 73.90833333333333, 73.90833333333333, 74.275, 74.275, 74.9, 74.9, 75.79166666666667, 75.79166666666667, 75.5, 75.5, 76.15833333333333, 76.15833333333333, 76.45833333333333, 76.45833333333333, 76.04166666666667, 76.04166666666667, 76.725, 76.725, 77.36666666666666, 77.36666666666666, 77.68333333333334, 77.68333333333334, 78.16666666666667, 78.16666666666667, 78.44166666666666, 78.44166666666666, 78.33333333333333, 78.33333333333333, 78.64166666666667, 78.64166666666667, 78.85, 78.85, 78.96666666666667, 78.96666666666667, 79.39166666666667, 79.39166666666667, 79.55, 79.55, 79.83333333333333, 79.83333333333333, 80.16666666666667, 80.16666666666667, 80.55, 80.55, 80.40833333333333, 80.40833333333333, 80.44166666666666, 80.44166666666666, 80.51666666666667, 80.51666666666667, 80.49166666666666, 80.49166666666666, 80.91666666666667, 80.91666666666667, 80.58333333333333, 80.58333333333333, 80.8, 80.8, 81.28333333333333, 81.28333333333333, 81.65833333333333, 81.65833333333333, 81.60833333333333, 81.60833333333333, 81.55, 81.55, 81.525, 81.525, 81.90833333333333, 81.90833333333333, 81.75833333333334, 81.75833333333334, 81.71666666666667, 81.71666666666667, 81.675, 81.675, 82.225, 82.225, 81.69166666666666, 81.69166666666666, 81.59166666666667, 81.59166666666667, 82.19166666666666, 82.19166666666666, 82.39166666666667, 82.39166666666667, 82.375, 82.375, 82.31666666666666, 82.31666666666666, 82.61666666666666, 82.61666666666666, 82.96666666666667, 82.96666666666667, 82.84166666666667, 82.84166666666667, 83.01666666666667, 83.01666666666667, 82.81666666666666, 82.81666666666666, 83.26666666666667, 83.26666666666667, 83.53333333333333, 83.53333333333333, 83.225, 83.225, 83.89166666666667, 83.89166666666667, 83.68333333333334, 83.68333333333334, 83.18333333333334, 83.18333333333334, 83.39166666666667, 83.39166666666667, 83.225, 83.225, 83.16666666666667, 83.16666666666667, 83.61666666666666, 83.61666666666666, 83.65833333333333, 83.65833333333333, 83.54166666666667, 83.54166666666667, 83.54166666666667, 83.54166666666667, 83.61666666666666, 83.61666666666666, 83.84166666666667, 83.84166666666667, 83.53333333333333, 83.53333333333333, 84.0, 84.0, 84.08333333333333, 84.08333333333333, 84.25833333333334, 84.25833333333334, 84.125, 84.125, 83.94166666666666, 83.94166666666666, 84.06666666666666, 84.06666666666666, 83.875, 83.875, 84.075, 84.075, 83.98333333333333, 83.98333333333333, 84.4, 84.4, 84.05, 84.05, 84.175, 84.175, 84.61666666666666, 84.61666666666666, 84.68333333333334, 84.68333333333334, 84.1, 84.1, 84.66666666666667, 84.66666666666667, 84.44166666666666, 84.44166666666666, 84.05, 84.05, 84.375, 84.375, 83.675, 83.675, 84.24166666666666, 84.24166666666666, 84.45833333333333, 84.45833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.323, Test loss: 0.741, Test accuracy: 78.79
Average accuracy final 10 rounds: 78.078
7663.773182153702
[11.817203760147095, 23.749037265777588, 35.564088582992554, 47.339749336242676, 58.992175817489624, 70.68261981010437, 82.3726978302002, 94.07061719894409, 105.75327491760254, 117.46225881576538, 129.15494346618652, 140.84215641021729, 152.55348992347717, 164.30526328086853, 176.045743227005, 187.76819491386414, 199.47498846054077, 211.1636688709259, 222.75713348388672, 233.36316227912903, 243.97481679916382, 254.6771321296692, 265.4117171764374, 276.13023352622986, 286.7974750995636, 297.52224016189575, 308.21285223960876, 318.9545028209686, 329.6534423828125, 340.36008167266846, 351.06196665763855, 361.7773835659027, 372.43092036247253, 383.0527799129486, 393.76329588890076, 404.47173261642456, 415.1995348930359, 425.7707118988037, 436.33965373039246, 447.03943824768066, 457.7588195800781, 468.48166131973267, 479.16580629348755, 489.8580815792084, 500.5601165294647, 511.2980959415436, 521.9616894721985, 532.6766493320465, 543.3192021846771, 554.0780489444733, 564.8080775737762, 575.504022359848, 586.2251088619232, 596.8895778656006, 607.756388425827, 618.5974824428558, 629.4012367725372, 640.1891708374023, 651.0863716602325, 661.9034554958344, 672.7646582126617, 683.5870802402496, 694.4601736068726, 705.2727701663971, 716.1788914203644, 727.0317845344543, 737.9248628616333, 748.734700679779, 759.6025745868683, 770.4724099636078, 781.3477880954742, 792.1630039215088, 803.0626068115234, 813.9474353790283, 824.7234082221985, 835.5410084724426, 846.3478467464447, 858.5788948535919, 870.6418118476868, 882.7216293811798, 894.7875683307648, 906.801117181778, 918.8530144691467, 930.9159824848175, 942.9730179309845, 955.0054907798767, 965.7856912612915, 976.6887440681458, 987.574227809906, 998.4726896286011, 1009.3704297542572, 1020.2685315608978, 1031.1330258846283, 1042.0214674472809, 1052.9322760105133, 1063.7253603935242, 1074.5964922904968, 1086.4405348300934, 1097.3151755332947, 1108.3026778697968, 1111.1707260608673]
[41.07, 47.7175, 52.3725, 56.105, 58.9775, 61.41, 63.5625, 64.0275, 65.3075, 67.2225, 68.26, 69.3925, 69.7575, 70.045, 70.83, 72.26, 72.49, 72.8225, 73.7225, 73.4125, 73.83, 74.3225, 73.44, 74.2475, 74.2325, 74.425, 75.4025, 75.3275, 75.7875, 76.34, 76.1425, 76.0875, 76.2, 76.23, 76.265, 76.5025, 76.0925, 76.7575, 76.7, 76.7625, 76.8125, 76.61, 76.565, 76.8125, 76.5475, 76.9125, 76.825, 76.8675, 76.81, 77.0675, 77.05, 77.1425, 77.14, 76.97, 77.47, 77.4775, 77.7325, 77.5125, 77.5775, 77.2325, 77.1375, 77.36, 77.5, 77.565, 77.6625, 77.7225, 78.0625, 77.41, 77.745, 78.1325, 77.49, 77.4175, 77.64, 77.8075, 77.8125, 78.14, 77.845, 77.835, 78.0375, 77.745, 77.715, 77.9375, 78.3925, 78.1075, 77.8875, 78.065, 78.4125, 78.18, 77.8475, 77.9275, 78.3175, 78.025, 77.54, 78.2025, 78.1875, 78.15, 78.0725, 78.225, 77.96, 78.1, 78.79]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

15144.837275505066
[4.8281614780426025, 9.25918197631836, 13.662594556808472, 18.384585857391357, 23.144519329071045, 27.73748517036438, 32.616578102111816, 37.239057779312134, 41.83070635795593, 46.611695766448975, 51.24718928337097, 55.84806036949158, 60.53779482841492, 65.24753785133362, 70.00004601478577, 74.59590935707092, 79.15088868141174, 83.70374321937561, 88.9015154838562, 94.06917715072632, 99.2962794303894, 103.95169568061829, 108.568186044693, 113.10274267196655, 117.67701721191406, 122.35699701309204, 126.97018194198608, 131.60710859298706, 136.26508593559265, 140.8540940284729, 145.48186469078064, 150.12392234802246, 154.70903944969177, 159.31306958198547, 163.9842188358307, 168.6409592628479, 173.29993271827698, 177.90726327896118, 182.49891066551208, 187.08517622947693, 191.6455340385437, 196.18999409675598, 200.73086810112, 205.27195692062378, 209.87925028800964, 214.4711627960205, 219.03870153427124, 223.54390358924866, 228.04728841781616, 232.57683634757996, 237.1255190372467, 241.7644522190094, 246.51454138755798, 251.13476467132568, 258.9311544895172, 265.48757004737854, 271.06381845474243, 276.0417945384979, 281.1014952659607, 286.2449836730957, 291.28699374198914, 296.42774629592896, 301.64388966560364, 306.6965935230255, 311.7569980621338, 316.88259840011597, 321.858758687973, 326.428671836853, 331.0257625579834, 335.5967490673065, 340.17045307159424, 344.7228400707245, 349.2686939239502, 353.8441481590271, 358.4585781097412, 363.005131483078, 367.58882451057434, 372.16731214523315, 376.87145733833313, 381.5742223262787, 386.31160950660706, 390.8907222747803, 395.4654767513275, 400.0883741378784, 404.72530126571655, 409.3369987010956, 414.09566712379456, 418.8066864013672, 423.4643166065216, 428.13326811790466, 432.82915925979614, 437.4197461605072, 442.073499917984, 446.6997628211975, 451.3320300579071, 456.02131485939026, 460.75451159477234, 465.34467363357544, 469.8923897743225, 474.5186071395874, 479.3720066547394, 484.2309765815735, 488.95365691185, 494.8604292869568, 501.6839406490326, 506.79571199417114, 511.87341833114624, 517.0053460597992, 522.159693479538, 526.7077238559723, 531.2752511501312, 535.7760119438171, 540.3266642093658, 544.862607717514, 549.601984500885, 554.7096378803253, 559.7952287197113, 564.9839868545532, 570.1664984226227, 575.2371051311493, 580.3583753108978, 585.3155496120453, 590.4266729354858, 595.4888432025909, 600.4921886920929, 605.4861168861389, 610.6398842334747, 615.7337548732758, 621.0022904872894, 626.1573202610016, 630.7876455783844, 635.4028041362762, 640.153154373169, 644.8741416931152, 649.7080790996552, 654.636611700058, 659.9220881462097, 665.1481277942657, 670.4707305431366, 675.8337228298187, 680.9395582675934, 686.0990438461304, 691.2623839378357, 696.3139774799347, 701.4873309135437, 706.6941890716553, 711.8882918357849, 717.1964447498322, 722.4381897449493, 727.711674451828, 732.9518074989319, 738.2169029712677, 743.3675887584686, 748.4408955574036, 753.5086803436279, 758.6779510974884, 763.8404459953308, 769.0650625228882, 774.2462861537933, 779.3965179920197, 784.3943004608154, 789.1168339252472, 793.9386670589447, 798.6705448627472, 803.7251350879669, 808.8955948352814, 813.8697891235352, 818.683176279068, 823.2380142211914, 827.7847955226898, 832.4393916130066, 836.9943516254425, 841.5469031333923, 846.1490476131439, 850.7676291465759, 855.321108341217, 860.0201835632324, 865.1829969882965, 870.3087379932404, 875.3317625522614, 880.4342341423035, 885.4342210292816, 890.5934212207794, 895.6380870342255, 900.754967212677, 905.2931020259857, 910.4171442985535, 915.5441527366638, 920.7397365570068, 925.380028963089, 929.9292631149292, 934.5107083320618, 939.0681488513947, 943.635942697525, 948.207887172699, 952.7550511360168, 957.3390052318573, 961.8957912921906, 966.4857461452484, 971.017657995224, 975.591349363327, 980.1587824821472, 985.105872631073, 989.7549245357513, 994.3052084445953, 998.866672039032, 1003.4605634212494, 1008.039110660553, 1012.6076054573059, 1017.1858706474304, 1021.7906501293182, 1026.3832910060883, 1030.9969682693481, 1035.584279537201, 1040.1857795715332, 1044.7506384849548, 1049.322551727295, 1053.8660945892334, 1058.4581878185272, 1063.0209896564484, 1067.608268737793, 1072.1736624240875, 1076.7197835445404, 1081.2639756202698, 1085.7793607711792, 1090.2990832328796, 1094.8644304275513, 1099.4045197963715, 1103.9153485298157, 1108.46249461174, 1113.0719134807587, 1117.6482677459717, 1122.2081921100616, 1126.7929594516754, 1131.354988336563, 1136.4272639751434, 1141.0614449977875, 1145.6966128349304, 1150.2373621463776, 1154.8785548210144, 1159.4664344787598, 1164.0990419387817, 1168.8916840553284, 1174.3067796230316, 1179.3566677570343, 1184.481260061264, 1189.7042164802551, 1194.9793939590454, 1200.1298871040344, 1205.2623646259308, 1210.3629620075226, 1215.501356601715, 1220.5718805789948, 1225.6937334537506, 1230.7750186920166, 1235.8686699867249, 1241.01047873497, 1245.7293946743011, 1250.743185043335, 1255.3904345035553, 1260.102870464325, 1265.1368725299835, 1269.7440383434296, 1274.2711203098297, 1278.8749127388, 1283.4335691928864, 1287.9861268997192, 1292.5305306911469, 1297.1109011173248, 1301.7030200958252, 1306.2335464954376, 1310.9160182476044, 1315.481452703476, 1320.019373178482, 1324.5305495262146, 1329.0453329086304, 1333.5598471164703, 1338.6815581321716, 1343.731651544571, 1348.728166103363, 1353.9753420352936, 1359.1884479522705, 1364.228963136673, 1369.2788462638855, 1374.4058272838593, 1378.9782354831696, 1383.5744426250458, 1388.1892051696777, 1392.8030037879944, 1397.4580247402191, 1402.5772829055786, 1407.7726027965546, 1412.9831380844116, 1418.1411542892456, 1423.257176399231, 1428.2832386493683, 1433.2778327465057, 1438.3745186328888, 1443.629242181778, 1448.7042970657349, 1451.31289601326]
[10.0, 10.0, 10.0, 10.0, 10.0025, 10.0025, 10.0025, 10.0025, 9.9975, 10.0, 9.9975, 9.9975, 10.0075, 10.0075, 10.0, 10.0025, 10.02, 10.08, 10.1025, 10.1375, 10.16, 10.18, 10.3125, 10.59, 10.795, 10.885, 10.8925, 11.09, 11.525, 11.7825, 12.155, 12.1475, 12.5125, 12.9525, 13.3625, 13.5875, 14.0375, 14.2175, 14.8175, 15.185, 15.615, 16.1625, 16.45, 16.6075, 16.6325, 16.545, 16.7425, 16.8175, 16.81, 16.68, 16.615, 16.6975, 16.805, 16.805, 16.8025, 16.67, 16.97, 17.0425, 17.0425, 16.9675, 17.0475, 17.13, 16.8175, 16.8325, 16.73, 16.96, 16.9675, 16.715, 16.665, 16.4575, 16.515, 16.605, 16.48, 16.675, 16.81, 16.5725, 16.4675, 14.5425, 13.5675, 11.815, 11.195, 10.4375, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.262, Test loss: 1.077, Test accuracy: 74.44
Final Round, Global train loss: 0.262, Global test loss: 1.177, Global test accuracy: 74.04
Average accuracy final 10 rounds: 73.34525 

Average global accuracy final 10 rounds: 73.943 

5871.173861026764
[4.921193838119507, 9.842387676239014, 14.637361764907837, 19.43233585357666, 24.22638201713562, 29.02042818069458, 33.76984214782715, 38.51925611495972, 43.1043004989624, 47.68934488296509, 52.48941946029663, 57.289494037628174, 61.49972915649414, 65.70996427536011, 69.94702696800232, 74.18408966064453, 78.41906237602234, 82.65403509140015, 86.9176709651947, 91.18130683898926, 95.42518281936646, 99.66905879974365, 103.90721869468689, 108.14537858963013, 112.27536344528198, 116.40534830093384, 120.54752469062805, 124.68970108032227, 128.85066628456116, 133.01163148880005, 137.1562578678131, 141.30088424682617, 145.4734399318695, 149.64599561691284, 153.78625321388245, 157.92651081085205, 162.08175897598267, 166.23700714111328, 170.39977169036865, 174.56253623962402, 178.73912358283997, 182.9157109260559, 187.06078219413757, 191.20585346221924, 195.39242315292358, 199.57899284362793, 203.7162594795227, 207.85352611541748, 211.98924255371094, 216.1249589920044, 220.25595235824585, 224.3869457244873, 228.51434993743896, 232.64175415039062, 236.76496267318726, 240.8881711959839, 245.02943229675293, 249.17069339752197, 253.2979612350464, 257.4252290725708, 261.56228494644165, 265.6993408203125, 269.8383057117462, 273.97727060317993, 278.122216463089, 282.26716232299805, 286.4018530845642, 290.53654384613037, 294.66348814964294, 298.7904324531555, 302.92827558517456, 307.0661187171936, 311.2110140323639, 315.3559093475342, 319.4854781627655, 323.6150469779968, 327.7655942440033, 331.91614151000977, 336.0836992263794, 340.251256942749, 344.39694595336914, 348.54263496398926, 352.70986008644104, 356.8770852088928, 361.0408401489258, 365.20459508895874, 369.35024642944336, 373.495897769928, 377.65180921554565, 381.80772066116333, 385.95931100845337, 390.1109013557434, 394.2665436267853, 398.42218589782715, 402.5829429626465, 406.7437000274658, 410.979718208313, 415.21573638916016, 419.4442059993744, 423.6726756095886, 427.9050302505493, 432.13738489151, 436.37863326072693, 440.61988162994385, 444.85795068740845, 449.09601974487305, 453.3086016178131, 457.5211834907532, 461.6641790866852, 465.8071746826172, 469.9793882369995, 474.15160179138184, 478.30226707458496, 482.4529323577881, 486.62183380126953, 490.790735244751, 495.0517535209656, 499.3127717971802, 503.60492062568665, 507.8970694541931, 512.1907863616943, 516.4845032691956, 520.7700200080872, 525.0555367469788, 529.3204641342163, 533.5853915214539, 537.8425953388214, 542.099799156189, 546.3027987480164, 550.5057983398438, 554.7173023223877, 558.9288063049316, 563.1347713470459, 567.3407363891602, 571.538191318512, 575.7356462478638, 579.9290611743927, 584.1224761009216, 588.3096063137054, 592.4967365264893, 596.687807559967, 600.8788785934448, 605.0602562427521, 609.2416338920593, 613.4424321651459, 617.6432304382324, 621.9593589305878, 626.2754874229431, 630.604701757431, 634.933916091919, 639.2465896606445, 643.5592632293701, 647.8514058589935, 652.143548488617, 656.4541819095612, 660.7648153305054, 665.0703806877136, 669.3759460449219, 673.5703179836273, 677.7646899223328, 681.9719843864441, 686.1792788505554, 690.3367502689362, 694.4942216873169, 698.6613693237305, 702.828516960144, 707.0059547424316, 711.1833925247192, 715.3445999622345, 719.5058073997498, 723.8070075511932, 728.1082077026367, 732.333270072937, 736.5583324432373, 740.735785484314, 744.9132385253906, 749.0753178596497, 753.2373971939087, 757.3866250514984, 761.5358529090881, 765.8713099956512, 770.2067670822144, 774.4168112277985, 778.6268553733826, 782.822829246521, 787.0188031196594, 791.2103860378265, 795.4019689559937, 799.6630351543427, 803.9241013526917, 808.1728765964508, 812.42165184021, 816.6274223327637, 820.8331928253174, 825.0560872554779, 829.2789816856384, 833.515346288681, 837.7517108917236, 842.6294040679932, 847.5070972442627, 849.8823523521423, 852.257607460022]
[35.905, 35.905, 42.0875, 42.0875, 44.0825, 44.0825, 47.9425, 47.9425, 49.3225, 49.3225, 52.1175, 52.1175, 53.8675, 53.8675, 54.7475, 54.7475, 56.9, 56.9, 57.4575, 57.4575, 59.75, 59.75, 60.8125, 60.8125, 61.4125, 61.4125, 62.52, 62.52, 62.9875, 62.9875, 63.83, 63.83, 63.9675, 63.9675, 64.7925, 64.7925, 65.395, 65.395, 65.7125, 65.7125, 65.9525, 65.9525, 66.4325, 66.4325, 67.165, 67.165, 67.575, 67.575, 67.6125, 67.6125, 67.7925, 67.7925, 68.1775, 68.1775, 68.38, 68.38, 68.5875, 68.5875, 68.76, 68.76, 69.03, 69.03, 69.4475, 69.4475, 69.2775, 69.2775, 69.5375, 69.5375, 70.0225, 70.0225, 70.08, 70.08, 70.565, 70.565, 70.4375, 70.4375, 70.375, 70.375, 70.785, 70.785, 70.785, 70.785, 71.05, 71.05, 70.89, 70.89, 71.1125, 71.1125, 71.3275, 71.3275, 71.3225, 71.3225, 71.6375, 71.6375, 71.2525, 71.2525, 71.45, 71.45, 71.695, 71.695, 72.0025, 72.0025, 71.9825, 71.9825, 72.175, 72.175, 72.0475, 72.0475, 72.235, 72.235, 72.295, 72.295, 72.23, 72.23, 72.585, 72.585, 72.4675, 72.4675, 72.465, 72.465, 72.64, 72.64, 72.7975, 72.7975, 72.7925, 72.7925, 72.5825, 72.5825, 72.6075, 72.6075, 72.59, 72.59, 72.51, 72.51, 72.7475, 72.7475, 73.1875, 73.1875, 73.405, 73.405, 73.3, 73.3, 73.3875, 73.3875, 73.265, 73.265, 73.2625, 73.2625, 73.31, 73.31, 73.2975, 73.2975, 73.25, 73.25, 72.92, 72.92, 73.26, 73.26, 73.25, 73.25, 73.0775, 73.0775, 73.1, 73.1, 73.1975, 73.1975, 73.325, 73.325, 73.3775, 73.3775, 73.5075, 73.5075, 73.6125, 73.6125, 73.76, 73.76, 73.7025, 73.7025, 73.46, 73.46, 73.3675, 73.3675, 73.305, 73.305, 73.4275, 73.4275, 73.18, 73.18, 73.24, 73.24, 73.035, 73.035, 73.4075, 73.4075, 73.2975, 73.2975, 73.55, 73.55, 73.6425, 73.6425, 74.4375, 74.4375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.209, Test loss: 0.359, Test accuracy: 86.22
Average accuracy final 10 rounds: 86.14916666666667 

1385.0611209869385
[1.5847651958465576, 3.1695303916931152, 4.466193914413452, 5.762857437133789, 7.060303449630737, 8.357749462127686, 9.653807640075684, 10.949865818023682, 12.24268126487732, 13.535496711730957, 14.83359956741333, 16.131702423095703, 17.436121702194214, 18.740540981292725, 20.00039505958557, 21.260249137878418, 22.556299448013306, 23.852349758148193, 25.105457067489624, 26.358564376831055, 27.60456395149231, 28.850563526153564, 30.110790729522705, 31.371017932891846, 32.62698578834534, 33.88295364379883, 35.186009645462036, 36.489065647125244, 37.75606298446655, 39.02306032180786, 40.286524295806885, 41.54998826980591, 42.8159966468811, 44.0820050239563, 45.3651602268219, 46.6483154296875, 47.94886493682861, 49.24941444396973, 50.538185358047485, 51.826956272125244, 53.11850333213806, 54.41005039215088, 55.69719409942627, 56.98433780670166, 58.28055477142334, 59.57677173614502, 60.8707377910614, 62.16470384597778, 63.468374729156494, 64.7720456123352, 66.07280373573303, 67.37356185913086, 68.67897820472717, 69.98439455032349, 71.28513526916504, 72.58587598800659, 73.8769760131836, 75.1680760383606, 76.46978044509888, 77.77148485183716, 79.08025312423706, 80.38902139663696, 81.6652500629425, 82.94147872924805, 84.23395824432373, 85.52643775939941, 86.81374025344849, 88.10104274749756, 89.36058473587036, 90.62012672424316, 91.90643501281738, 93.1927433013916, 94.49576735496521, 95.79879140853882, 97.03155541419983, 98.26431941986084, 99.50843143463135, 100.75254344940186, 101.96774888038635, 103.18295431137085, 104.42832970619202, 105.67370510101318, 106.9112811088562, 108.14885711669922, 109.36345744132996, 110.5780577659607, 111.79309701919556, 113.00813627243042, 114.24198913574219, 115.47584199905396, 116.6870219707489, 117.89820194244385, 119.12703227996826, 120.35586261749268, 121.58914256095886, 122.82242250442505, 124.11651849746704, 125.41061449050903, 126.61888575553894, 127.82715702056885, 129.03965735435486, 130.25215768814087, 131.53375792503357, 132.81535816192627, 134.09613394737244, 135.3769097328186, 136.67950415611267, 137.98209857940674, 139.25326824188232, 140.5244379043579, 141.6913435459137, 142.85824918746948, 144.02250981330872, 145.18677043914795, 146.34466886520386, 147.50256729125977, 148.77237105369568, 150.0421748161316, 151.30373740196228, 152.56529998779297, 153.71176028251648, 154.85822057724, 156.02792811393738, 157.19763565063477, 158.36158919334412, 159.52554273605347, 160.68603587150574, 161.846529006958, 163.00334334373474, 164.16015768051147, 165.3291139602661, 166.49807024002075, 167.67443346977234, 168.85079669952393, 170.1149399280548, 171.3790831565857, 172.5559539794922, 173.73282480239868, 174.89605927467346, 176.05929374694824, 177.2147922515869, 178.3702907562256, 179.6157751083374, 180.86125946044922, 181.9952130317688, 183.12916660308838, 184.2656054496765, 185.40204429626465, 186.53060746192932, 187.659170627594, 188.78903698921204, 189.91890335083008, 191.13762879371643, 192.35635423660278, 193.53780794143677, 194.71926164627075, 195.87318515777588, 197.027108669281, 198.27016639709473, 199.51322412490845, 200.7594428062439, 202.00566148757935, 203.1427719593048, 204.27988243103027, 205.47835421562195, 206.67682600021362, 207.89272260665894, 209.10861921310425, 210.27180528640747, 211.4349913597107, 212.5596113204956, 213.68423128128052, 214.82181978225708, 215.95940828323364, 217.08526349067688, 218.21111869812012, 219.34661960601807, 220.48212051391602, 221.62058234214783, 222.75904417037964, 223.88868069648743, 225.01831722259521, 226.15045952796936, 227.2826018333435, 228.40158867835999, 229.52057552337646, 230.6525321006775, 231.78448867797852, 232.91158413887024, 234.03867959976196, 235.16030645370483, 236.2819333076477, 237.4063537120819, 238.5307741165161, 239.65482783317566, 240.7788815498352, 241.89130544662476, 243.0037293434143, 244.12380146980286, 245.2438735961914, 246.9908926486969, 248.7379117012024]
[23.658333333333335, 23.658333333333335, 42.1, 42.1, 51.34166666666667, 51.34166666666667, 56.675, 56.675, 60.766666666666666, 60.766666666666666, 65.45, 65.45, 67.61666666666666, 67.61666666666666, 68.9, 68.9, 70.93333333333334, 70.93333333333334, 71.025, 71.025, 73.53333333333333, 73.53333333333333, 74.175, 74.175, 74.25, 74.25, 75.05833333333334, 75.05833333333334, 75.21666666666667, 75.21666666666667, 76.14166666666667, 76.14166666666667, 76.89166666666667, 76.89166666666667, 77.475, 77.475, 78.43333333333334, 78.43333333333334, 78.275, 78.275, 78.71666666666667, 78.71666666666667, 79.00833333333334, 79.00833333333334, 79.66666666666667, 79.66666666666667, 79.89166666666667, 79.89166666666667, 79.68333333333334, 79.68333333333334, 80.28333333333333, 80.28333333333333, 80.79166666666667, 80.79166666666667, 81.31666666666666, 81.31666666666666, 81.64166666666667, 81.64166666666667, 81.2, 81.2, 81.41666666666667, 81.41666666666667, 82.01666666666667, 82.01666666666667, 82.0, 82.0, 82.05, 82.05, 82.60833333333333, 82.60833333333333, 82.53333333333333, 82.53333333333333, 82.625, 82.625, 82.68333333333334, 82.68333333333334, 82.91666666666667, 82.91666666666667, 83.225, 83.225, 82.86666666666666, 82.86666666666666, 82.78333333333333, 82.78333333333333, 82.40833333333333, 82.40833333333333, 83.025, 83.025, 83.29166666666667, 83.29166666666667, 83.14166666666667, 83.14166666666667, 83.575, 83.575, 83.9, 83.9, 84.14166666666667, 84.14166666666667, 83.98333333333333, 83.98333333333333, 84.15, 84.15, 84.11666666666666, 84.11666666666666, 83.95, 83.95, 84.075, 84.075, 84.075, 84.075, 84.6, 84.6, 84.43333333333334, 84.43333333333334, 84.88333333333334, 84.88333333333334, 84.58333333333333, 84.58333333333333, 84.28333333333333, 84.28333333333333, 85.09166666666667, 85.09166666666667, 84.93333333333334, 84.93333333333334, 84.71666666666667, 84.71666666666667, 85.14166666666667, 85.14166666666667, 85.34166666666667, 85.34166666666667, 84.95, 84.95, 85.01666666666667, 85.01666666666667, 84.76666666666667, 84.76666666666667, 84.91666666666667, 84.91666666666667, 85.25833333333334, 85.25833333333334, 85.10833333333333, 85.10833333333333, 85.36666666666666, 85.36666666666666, 85.20833333333333, 85.20833333333333, 85.36666666666666, 85.36666666666666, 85.33333333333333, 85.33333333333333, 85.375, 85.375, 85.5, 85.5, 85.26666666666667, 85.26666666666667, 85.39166666666667, 85.39166666666667, 85.65833333333333, 85.65833333333333, 85.75833333333334, 85.75833333333334, 85.94166666666666, 85.94166666666666, 85.51666666666667, 85.51666666666667, 85.66666666666667, 85.66666666666667, 85.725, 85.725, 85.725, 85.725, 85.16666666666667, 85.16666666666667, 85.58333333333333, 85.58333333333333, 85.84166666666667, 85.84166666666667, 85.925, 85.925, 85.83333333333333, 85.83333333333333, 86.05833333333334, 86.05833333333334, 86.21666666666667, 86.21666666666667, 86.44166666666666, 86.44166666666666, 86.01666666666667, 86.01666666666667, 86.175, 86.175, 86.13333333333334, 86.13333333333334, 86.18333333333334, 86.18333333333334, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.225, 86.225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.400, Test loss: 0.704, Test accuracy: 76.92
Average accuracy final 10 rounds: 76.72500000000001
6069.5355525016785
[5.90776801109314, 11.81553602218628, 17.387317895889282, 22.959099769592285, 28.57028341293335, 34.181467056274414, 39.71841239929199, 45.25535774230957, 50.84178400039673, 56.42821025848389, 62.02165079116821, 67.61509132385254, 73.58679103851318, 79.55849075317383, 85.36666440963745, 91.17483806610107, 96.9964952468872, 102.81815242767334, 108.51217460632324, 114.20619678497314, 119.79637908935547, 125.3865613937378, 130.9876365661621, 136.58871173858643, 142.00259041786194, 147.41646909713745, 152.79501676559448, 158.1735644340515, 163.51861596107483, 168.86366748809814, 174.22277450561523, 179.58188152313232, 185.20529532432556, 190.8287091255188, 196.31326937675476, 201.79782962799072, 207.14202404022217, 212.4862184524536, 217.85921955108643, 223.23222064971924, 228.53081846237183, 233.8294162750244, 239.1872718334198, 244.54512739181519, 249.8584587574005, 255.17179012298584, 260.54749393463135, 265.92319774627686, 271.4961383342743, 277.06907892227173, 282.56098890304565, 288.0528988838196, 293.6505694389343, 299.2482399940491, 304.72587513923645, 310.2035102844238, 315.6168384552002, 321.03016662597656, 326.37661385536194, 331.7230610847473, 337.138055562973, 342.55305004119873, 347.9352798461914, 353.3175096511841, 358.7311096191406, 364.14470958709717, 369.7715194225311, 375.3983292579651, 381.0807235240936, 386.76311779022217, 392.4047429561615, 398.04636812210083, 403.5805187225342, 409.11466932296753, 414.60273241996765, 420.0907955169678, 425.60731172561646, 431.12382793426514, 436.68087363243103, 442.2379193305969, 447.56514978408813, 452.89238023757935, 458.2930998802185, 463.69381952285767, 469.0930142402649, 474.4922089576721, 480.35314440727234, 486.21407985687256, 492.16410183906555, 498.11412382125854, 503.4783070087433, 508.842490196228, 514.2665390968323, 519.6905879974365, 525.195858001709, 530.7011280059814, 536.4446001052856, 542.1880722045898, 547.7221629619598, 553.2562537193298, 558.6393306255341, 564.0224075317383, 569.4071183204651, 574.7918291091919, 580.1932384967804, 585.5946478843689, 591.3321552276611, 597.0696625709534, 602.6466372013092, 608.223611831665, 613.7679491043091, 619.3122863769531, 624.941787481308, 630.5712885856628, 636.5985295772552, 642.6257705688477, 649.2511675357819, 655.8765645027161, 662.5429604053497, 669.2093563079834, 675.7830550670624, 682.3567538261414, 689.068948507309, 695.7811431884766, 702.0689775943756, 708.3568120002747, 714.4586470127106, 720.5604820251465, 726.9932677745819, 733.4260535240173, 739.5767922401428, 745.7275309562683, 751.8411359786987, 757.9547410011292, 764.1439764499664, 770.3332118988037, 776.6030158996582, 782.8728199005127, 789.1896932125092, 795.5065665245056, 801.8313176631927, 808.1560688018799, 814.4268653392792, 820.6976618766785, 827.2395179271698, 833.7813739776611, 840.2955348491669, 846.8096957206726, 853.3750610351562, 859.9404263496399, 866.4424314498901, 872.9444365501404, 879.4511365890503, 885.9578366279602, 892.1703534126282, 898.3828701972961, 904.8974108695984, 911.4119515419006, 917.96142411232, 924.5108966827393, 930.8500354290009, 937.1891741752625, 943.4039170742035, 949.6186599731445, 955.7862224578857, 961.953784942627, 968.3978755474091, 974.8419661521912, 981.3189761638641, 987.7959861755371, 994.2512290477753, 1000.7064719200134, 1007.0543451309204, 1013.4022183418274, 1019.8392198085785, 1026.2762212753296, 1032.8331382274628, 1039.390055179596, 1045.9011447429657, 1052.4122343063354, 1058.5762062072754, 1064.7401781082153, 1070.973305940628, 1077.2064337730408, 1083.4104986190796, 1089.6145634651184, 1095.7458112239838, 1101.8770589828491, 1108.1904096603394, 1114.5037603378296, 1120.9871983528137, 1127.4706363677979, 1133.9243516921997, 1140.3780670166016, 1146.590805530548, 1152.8035440444946, 1159.3898873329163, 1165.976230621338, 1172.344168663025, 1178.712106704712, 1181.19864320755, 1183.6851797103882]
[32.025, 32.025, 39.6875, 39.6875, 44.8625, 44.8625, 48.0275, 48.0275, 51.925, 51.925, 54.6425, 54.6425, 57.4025, 57.4025, 59.67, 59.67, 60.0525, 60.0525, 61.8925, 61.8925, 62.9075, 62.9075, 63.365, 63.365, 64.695, 64.695, 64.6275, 64.6275, 65.625, 65.625, 66.8475, 66.8475, 68.135, 68.135, 68.765, 68.765, 68.95, 68.95, 70.045, 70.045, 70.275, 70.275, 70.1025, 70.1025, 70.8975, 70.8975, 71.3075, 71.3075, 71.515, 71.515, 71.79, 71.79, 72.1725, 72.1725, 72.085, 72.085, 73.0225, 73.0225, 73.04, 73.04, 73.2, 73.2, 73.1175, 73.1175, 73.93, 73.93, 73.7025, 73.7025, 74.0575, 74.0575, 73.9825, 73.9825, 73.94, 73.94, 74.3575, 74.3575, 73.8225, 73.8225, 74.255, 74.255, 74.1325, 74.1325, 74.6725, 74.6725, 74.7225, 74.7225, 74.8825, 74.8825, 74.5, 74.5, 74.4125, 74.4125, 74.995, 74.995, 74.9175, 74.9175, 75.0575, 75.0575, 75.21, 75.21, 74.88, 74.88, 74.8625, 74.8625, 75.09, 75.09, 75.3575, 75.3575, 75.7225, 75.7225, 75.8025, 75.8025, 75.9575, 75.9575, 75.7825, 75.7825, 76.0725, 76.0725, 75.505, 75.505, 75.99, 75.99, 75.745, 75.745, 75.8025, 75.8025, 76.2575, 76.2575, 76.135, 76.135, 75.955, 75.955, 76.34, 76.34, 76.26, 76.26, 76.0075, 76.0075, 76.1475, 76.1475, 76.3775, 76.3775, 76.2225, 76.2225, 76.12, 76.12, 76.48, 76.48, 76.4475, 76.4475, 76.725, 76.725, 76.715, 76.715, 76.58, 76.58, 76.8775, 76.8775, 77.105, 77.105, 77.07, 77.07, 76.6825, 76.6825, 77.1025, 77.1025, 76.395, 76.395, 76.945, 76.945, 77.0025, 77.0025, 77.1925, 77.1925, 76.79, 76.79, 76.675, 76.675, 77.015, 77.015, 76.765, 76.765, 76.43, 76.43, 76.4575, 76.4575, 76.5025, 76.5025, 77.065, 77.065, 76.8625, 76.8625, 76.555, 76.555, 76.9775, 76.9775, 76.785, 76.785, 76.85, 76.85, 76.925, 76.925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.138, Test loss: 1.091, Test accuracy: 67.12
Average accuracy final 10 rounds: 59.63722222222222
4308.623697280884
[6.793018102645874, 13.189595222473145, 19.770700693130493, 26.2218918800354, 32.7708625793457, 39.233930349349976, 45.35590887069702, 51.724902868270874, 57.97517514228821, 64.1347336769104, 70.30461263656616, 76.31660270690918, 82.46760368347168, 88.62878894805908, 94.5760235786438, 100.642737865448, 106.99835968017578, 113.29529976844788, 119.7306592464447, 126.20990490913391, 132.5287425518036, 138.94459700584412, 145.25691843032837, 151.57663226127625, 157.94685411453247, 164.25875234603882, 170.70801782608032, 177.11740493774414, 183.2516369819641, 189.32759022712708, 195.61439681053162, 201.7643964290619, 207.94769978523254, 214.2544629573822, 220.47343468666077, 226.50732159614563, 232.98189115524292, 238.95017218589783, 245.05485320091248, 251.26093173027039, 257.32572984695435, 263.2891619205475, 269.1606357097626, 275.18987822532654, 281.06753635406494, 286.9665458202362, 292.9679591655731, 298.8617105484009, 304.906231880188, 310.9011981487274, 316.77540373802185, 322.8014557361603, 328.77216696739197, 334.88853120803833, 340.841304063797, 346.834025144577, 352.8706884384155, 358.7906847000122, 364.77548599243164, 370.7923991680145, 376.7857344150543, 383.11128067970276, 389.17117953300476, 395.19294691085815, 401.5362503528595, 407.8083748817444, 414.2385959625244, 420.51346826553345, 427.01103258132935, 433.293790102005, 439.630802154541, 445.9161078929901, 452.32923769950867, 458.6413412094116, 465.03033804893494, 471.27177596092224, 477.63674211502075, 483.8333237171173, 490.0919988155365, 496.3686761856079, 502.5964343547821, 508.71990060806274, 514.7302250862122, 520.7861313819885, 526.7254712581635, 532.7758204936981, 538.8768968582153, 544.7502036094666, 550.7154664993286, 556.6637296676636, 562.58283162117, 568.5586225986481, 574.4906458854675, 580.5714666843414, 586.6729967594147, 592.5696370601654, 598.5401592254639, 604.6624355316162, 610.8308119773865, 617.0040907859802, 620.5069606304169]
[12.955555555555556, 26.333333333333332, 27.194444444444443, 28.377777777777776, 39.422222222222224, 35.59444444444444, 34.78333333333333, 42.1, 38.75, 45.87222222222222, 41.894444444444446, 48.81111111111111, 36.82222222222222, 49.455555555555556, 50.477777777777774, 50.85, 48.77777777777778, 46.4, 54.53888888888889, 53.74444444444445, 54.7, 49.93888888888889, 49.98888888888889, 46.355555555555554, 55.25, 53.41111111111111, 50.06111111111111, 56.55, 49.327777777777776, 54.93333333333333, 58.03888888888889, 53.42777777777778, 55.55555555555556, 58.31666666666667, 55.96111111111111, 54.38333333333333, 58.605555555555554, 54.53888888888889, 52.416666666666664, 60.81666666666667, 55.36666666666667, 60.42777777777778, 58.18333333333333, 53.68333333333333, 43.09444444444444, 58.94444444444444, 59.605555555555554, 51.044444444444444, 58.72222222222222, 55.02777777777778, 54.044444444444444, 62.57222222222222, 54.172222222222224, 55.37222222222222, 54.17777777777778, 59.35, 57.27777777777778, 58.82222222222222, 57.144444444444446, 59.46111111111111, 56.69444444444444, 56.333333333333336, 55.55555555555556, 58.56666666666667, 56.605555555555554, 59.12777777777778, 63.605555555555554, 59.422222222222224, 57.233333333333334, 53.605555555555554, 58.13333333333333, 57.55, 61.36666666666667, 59.35, 62.5, 60.605555555555554, 60.205555555555556, 62.016666666666666, 56.88333333333333, 62.977777777777774, 57.01111111111111, 65.18888888888888, 63.78333333333333, 64.31666666666666, 65.60555555555555, 58.76111111111111, 63.30555555555556, 61.93333333333333, 56.53333333333333, 60.11666666666667, 65.08888888888889, 60.57222222222222, 56.53333333333333, 56.477777777777774, 59.361111111111114, 63.43333333333333, 63.42777777777778, 60.455555555555556, 51.25555555555555, 59.766666666666666, 67.12222222222222]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 272, in <module>
    acc_test, loss_test = test_img_local_all(net_glob, args, dataset_test, dict_users_test,
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 133, in test_img_local_all
    a, b = test_img_local(net_local, dataset_test, args, user_idx=idx, idxs=dict_users_test[idx], concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/test.py", line 97, in test_img_local
    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.258, Test loss: 1.031, Test accuracy: 74.74
Final Round, Global train loss: 0.258, Global test loss: 1.728, Global test accuracy: 60.91
Average accuracy final 10 rounds: 73.49250000000002 

Average global accuracy final 10 rounds: 60.16825000000001 

6746.435334920883
[5.369055986404419, 10.738111972808838, 16.099080085754395, 21.46004819869995, 26.732122659683228, 32.004197120666504, 37.265416622161865, 42.52663612365723, 47.87730932235718, 53.22798252105713, 58.67235207557678, 64.11672163009644, 69.58535838127136, 75.05399513244629, 80.5879259109497, 86.12185668945312, 91.42829298973083, 96.73472929000854, 101.99051141738892, 107.24629354476929, 112.4895567893982, 117.7328200340271, 122.97942113876343, 128.22602224349976, 133.5372576713562, 138.84849309921265, 144.1843855381012, 149.52027797698975, 154.8261914253235, 160.13210487365723, 165.00259041786194, 169.87307596206665, 174.61476516723633, 179.356454372406, 184.74026036262512, 190.12406635284424, 195.50204586982727, 200.8800253868103, 206.0780463218689, 211.2760672569275, 216.5230197906494, 221.76997232437134, 227.05250453948975, 232.33503675460815, 237.58668303489685, 242.83832931518555, 248.09204745292664, 253.34576559066772, 258.6146891117096, 263.88361263275146, 269.2055094242096, 274.5274062156677, 279.830148935318, 285.13289165496826, 290.35600328445435, 295.57911491394043, 300.3296113014221, 305.0801076889038, 309.855872631073, 314.6316375732422, 319.85819840431213, 325.0847592353821, 330.35025668144226, 335.61575412750244, 340.6517696380615, 345.6877851486206, 350.79760694503784, 355.9074287414551, 361.1638684272766, 366.42030811309814, 371.5444676876068, 376.6686272621155, 381.7196698188782, 386.77071237564087, 391.8661422729492, 396.96157217025757, 402.0625126361847, 407.1634531021118, 412.33720231056213, 417.51095151901245, 422.68746304512024, 427.863974571228, 433.0968623161316, 438.32975006103516, 443.3875424861908, 448.44533491134644, 453.5864243507385, 458.7275137901306, 463.6779828071594, 468.62845182418823, 473.3170795440674, 478.00570726394653, 482.6346158981323, 487.2635245323181, 491.8819160461426, 496.50030755996704, 501.0901415348053, 505.67997550964355, 510.080194234848, 514.4804129600525, 518.7977693080902, 523.1151256561279, 527.4281075000763, 531.7410893440247, 536.0716676712036, 540.4022459983826, 544.7100481987, 549.0178503990173, 553.3219397068024, 557.6260290145874, 561.9174876213074, 566.2089462280273, 570.5463132858276, 574.8836803436279, 579.2452757358551, 583.6068711280823, 587.9383692741394, 592.2698674201965, 596.5788736343384, 600.8878798484802, 605.1866292953491, 609.485378742218, 613.8152439594269, 618.1451091766357, 622.506813287735, 626.8685173988342, 631.2242729663849, 635.5800285339355, 639.9690055847168, 644.357982635498, 648.7393176555634, 653.1206526756287, 657.4178004264832, 661.7149481773376, 666.0148041248322, 670.3146600723267, 674.625491142273, 678.9363222122192, 683.3025140762329, 687.6687059402466, 692.034835100174, 696.4009642601013, 700.7108883857727, 705.0208125114441, 709.337286233902, 713.6537599563599, 717.9939572811127, 722.3341546058655, 726.6746714115143, 731.0151882171631, 735.3428268432617, 739.6704654693604, 744.1046161651611, 748.5387668609619, 752.86767745018, 757.1965880393982, 761.5268964767456, 765.857204914093, 770.1950800418854, 774.5329551696777, 778.8404352664948, 783.1479153633118, 787.4633045196533, 791.7786936759949, 796.1147184371948, 800.4507431983948, 804.8137271404266, 809.1767110824585, 813.5508015155792, 817.9248919487, 822.2818493843079, 826.6388068199158, 830.9901907444, 835.3415746688843, 839.6502358913422, 843.9588971138, 848.3182134628296, 852.6775298118591, 857.0409488677979, 861.4043679237366, 865.7751173973083, 870.1458668708801, 874.5213680267334, 878.8968691825867, 883.2578203678131, 887.6187715530396, 891.960351228714, 896.3019309043884, 900.6248943805695, 904.9478578567505, 909.3038592338562, 913.6598606109619, 917.9989440441132, 922.3380274772644, 926.6622383594513, 930.9864492416382, 935.324285030365, 939.6621208190918, 943.9951293468475, 948.3281378746033, 950.5097422599792, 952.6913466453552]
[34.0525, 34.0525, 39.9125, 39.9125, 42.3275, 42.3275, 44.7225, 44.7225, 47.3575, 47.3575, 48.74, 48.74, 51.53, 51.53, 53.75, 53.75, 54.6575, 54.6575, 56.46, 56.46, 57.6475, 57.6475, 58.54, 58.54, 59.9325, 59.9325, 60.8775, 60.8775, 62.19, 62.19, 63.015, 63.015, 63.575, 63.575, 64.08, 64.08, 64.7425, 64.7425, 65.6275, 65.6275, 65.96, 65.96, 66.1775, 66.1775, 66.855, 66.855, 66.87, 66.87, 67.36, 67.36, 67.2975, 67.2975, 67.595, 67.595, 68.4325, 68.4325, 68.66, 68.66, 68.7975, 68.7975, 68.96, 68.96, 68.84, 68.84, 68.8975, 68.8975, 69.2625, 69.2625, 69.77, 69.77, 69.98, 69.98, 70.225, 70.225, 70.52, 70.52, 70.7275, 70.7275, 70.83, 70.83, 71.2325, 71.2325, 71.205, 71.205, 71.205, 71.205, 71.065, 71.065, 71.005, 71.005, 71.135, 71.135, 71.2075, 71.2075, 71.5, 71.5, 71.615, 71.615, 71.6025, 71.6025, 71.63, 71.63, 71.65, 71.65, 71.7075, 71.7075, 71.91, 71.91, 72.1425, 72.1425, 71.995, 71.995, 71.845, 71.845, 71.925, 71.925, 72.315, 72.315, 72.2075, 72.2075, 72.605, 72.605, 72.5175, 72.5175, 72.3825, 72.3825, 72.6275, 72.6275, 72.285, 72.285, 72.5, 72.5, 72.455, 72.455, 72.49, 72.49, 72.5775, 72.5775, 72.72, 72.72, 72.6625, 72.6625, 72.825, 72.825, 73.0275, 73.0275, 72.825, 72.825, 72.9725, 72.9725, 72.9475, 72.9475, 72.9325, 72.9325, 73.25, 73.25, 73.195, 73.195, 73.3425, 73.3425, 72.9525, 72.9525, 73.23, 73.23, 73.135, 73.135, 73.33, 73.33, 73.26, 73.26, 73.2525, 73.2525, 73.49, 73.49, 73.5075, 73.5075, 73.455, 73.455, 73.12, 73.12, 73.4125, 73.4125, 73.19, 73.19, 73.3675, 73.3675, 73.395, 73.395, 73.6675, 73.6675, 73.6225, 73.6225, 73.4425, 73.4425, 73.325, 73.325, 73.6225, 73.6225, 73.88, 73.88, 74.7375, 74.7375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.416, Test loss: 0.764, Test accuracy: 76.12
Average accuracy final 10 rounds: 75.66825 

4691.26194357872
[4.687952041625977, 9.375904083251953, 13.775710344314575, 18.175516605377197, 22.623318910598755, 27.071121215820312, 31.533511638641357, 35.9959020614624, 40.43160080909729, 44.86729955673218, 49.310139656066895, 53.75297975540161, 58.18441677093506, 62.615853786468506, 67.0513563156128, 71.48685884475708, 75.57534432411194, 79.6638298034668, 83.72395467758179, 87.78407955169678, 91.84242510795593, 95.90077066421509, 99.98293018341064, 104.0650897026062, 108.14921808242798, 112.23334646224976, 116.28903770446777, 120.34472894668579, 124.58206510543823, 128.81940126419067, 132.82958364486694, 136.8397660255432, 140.84914922714233, 144.85853242874146, 148.86348843574524, 152.86844444274902, 156.87776803970337, 160.88709163665771, 164.90205073356628, 168.91700983047485, 172.94488883018494, 176.97276782989502, 180.97535276412964, 184.97793769836426, 188.99772763252258, 193.0175175666809, 197.05851864814758, 201.09951972961426, 205.12370347976685, 209.14788722991943, 213.1507863998413, 217.15368556976318, 221.15443921089172, 225.15519285202026, 229.15840792655945, 233.16162300109863, 237.18396258354187, 241.2063021659851, 245.2566704750061, 249.3070387840271, 253.34823966026306, 257.389440536499, 261.42432284355164, 265.45920515060425, 269.47820711135864, 273.49720907211304, 277.54859614372253, 281.59998321533203, 285.65994095802307, 289.7198987007141, 293.7948660850525, 297.86983346939087, 301.909460067749, 305.9490866661072, 309.9720642566681, 313.995041847229, 318.030250787735, 322.06545972824097, 326.11285638809204, 330.1602530479431, 334.2100143432617, 338.2597756385803, 342.21934175491333, 346.17890787124634, 350.20073437690735, 354.22256088256836, 358.2241048812866, 362.2256488800049, 366.22537183761597, 370.22509479522705, 374.2553000450134, 378.2855052947998, 382.33872079849243, 386.39193630218506, 390.43679690361023, 394.4816575050354, 398.52234506607056, 402.5630326271057, 406.59916043281555, 410.6352882385254, 414.6574857234955, 418.6796832084656, 422.6831212043762, 426.68655920028687, 430.7487304210663, 434.8109016418457, 438.858553647995, 442.9062056541443, 446.8600163459778, 450.8138270378113, 454.81394839286804, 458.8140697479248, 462.84489583969116, 466.8757219314575, 470.9390540122986, 475.00238609313965, 479.00884795188904, 483.0153098106384, 487.04013991355896, 491.0649700164795, 495.0946202278137, 499.12427043914795, 503.1753616333008, 507.2264528274536, 511.29143929481506, 515.3564257621765, 519.3439083099365, 523.3313908576965, 527.281688451767, 531.2319860458374, 535.2031478881836, 539.1743097305298, 543.122195482254, 547.0700812339783, 551.0548150539398, 555.0395488739014, 559.0200729370117, 563.0005970001221, 567.0335912704468, 571.0665855407715, 575.0818603038788, 579.0971350669861, 583.1078090667725, 587.1184830665588, 591.1255042552948, 595.1325254440308, 599.1400351524353, 603.1475448608398, 607.2020964622498, 611.2566480636597, 615.3224241733551, 619.3882002830505, 623.3912279605865, 627.3942556381226, 631.3758177757263, 635.3573799133301, 639.3258063793182, 643.2942328453064, 647.2846646308899, 651.2750964164734, 655.2452399730682, 659.2153835296631, 663.1625611782074, 667.1097388267517, 671.1255528926849, 675.1413669586182, 679.1999578475952, 683.2585487365723, 687.2851781845093, 691.3118076324463, 695.3149890899658, 699.3181705474854, 703.3499064445496, 707.3816423416138, 711.3937344551086, 715.4058265686035, 719.3930017948151, 723.3801770210266, 727.403838634491, 731.4275002479553, 735.4137272834778, 739.3999543190002, 743.3487005233765, 747.2974467277527, 751.3281326293945, 755.3588185310364, 759.3863236904144, 763.4138288497925, 767.4136927127838, 771.4135565757751, 775.5706169605255, 779.7276773452759, 783.7279486656189, 787.7282199859619, 791.761593580246, 795.79496717453, 799.8122477531433, 803.8295283317566, 807.8644864559174, 811.8994445800781, 813.7928175926208, 815.6861906051636]
[27.0275, 27.0275, 35.3375, 35.3375, 40.86, 40.86, 45.2325, 45.2325, 48.595, 48.595, 51.36, 51.36, 53.775, 53.775, 55.675, 55.675, 57.295, 57.295, 58.0425, 58.0425, 58.545, 58.545, 59.885, 59.885, 61.6625, 61.6625, 62.925, 62.925, 63.6725, 63.6725, 65.5675, 65.5675, 66.0025, 66.0025, 66.2775, 66.2775, 68.01, 68.01, 67.1075, 67.1075, 67.845, 67.845, 68.7125, 68.7125, 69.1925, 69.1925, 68.96, 68.96, 69.8675, 69.8675, 70.3275, 70.3275, 70.6375, 70.6375, 70.375, 70.375, 70.6475, 70.6475, 70.8975, 70.8975, 71.2425, 71.2425, 71.385, 71.385, 71.4725, 71.4725, 71.0875, 71.0875, 71.58, 71.58, 72.235, 72.235, 72.005, 72.005, 72.9, 72.9, 72.4075, 72.4075, 72.285, 72.285, 72.215, 72.215, 72.75, 72.75, 73.4025, 73.4025, 73.4775, 73.4775, 73.24, 73.24, 73.1125, 73.1125, 73.5175, 73.5175, 73.1, 73.1, 73.22, 73.22, 73.885, 73.885, 74.0825, 74.0825, 73.9275, 73.9275, 74.455, 74.455, 74.5, 74.5, 73.8275, 73.8275, 74.1425, 74.1425, 74.0575, 74.0575, 73.8325, 73.8325, 73.6175, 73.6175, 74.075, 74.075, 74.685, 74.685, 74.4275, 74.4275, 74.4175, 74.4175, 74.7725, 74.7725, 74.865, 74.865, 74.6475, 74.6475, 74.925, 74.925, 74.7325, 74.7325, 74.99, 74.99, 75.0025, 75.0025, 74.605, 74.605, 74.99, 74.99, 75.49, 75.49, 75.2675, 75.2675, 74.955, 74.955, 74.7275, 74.7275, 75.08, 75.08, 74.765, 74.765, 74.8375, 74.8375, 74.875, 74.875, 75.14, 75.14, 75.225, 75.225, 75.1175, 75.1175, 74.8475, 74.8475, 75.275, 75.275, 75.3375, 75.3375, 75.3325, 75.3325, 75.2325, 75.2325, 75.4775, 75.4775, 75.6225, 75.6225, 75.865, 75.865, 75.65, 75.65, 75.6825, 75.6825, 75.545, 75.545, 75.1425, 75.1425, 75.395, 75.395, 75.815, 75.815, 75.56, 75.56, 76.0075, 76.0075, 76.02, 76.02, 76.125, 76.125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2226, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.357, Test loss: 1.361, Test accuracy: 59.89
Average accuracy final 10 rounds: 58.058499999999995
8325.738422393799
[12.460654020309448, 25.02053427696228, 37.68540620803833, 50.2503387928009, 62.857460260391235, 75.55994629859924, 88.26557278633118, 100.91665863990784, 113.52238178253174, 126.17245388031006, 138.81709337234497, 151.43360471725464, 164.04534816741943, 176.73245882987976, 189.33501720428467, 201.9109914302826, 214.5547103881836, 225.9384424686432, 237.3253195285797, 249.92333149909973, 262.5545859336853, 274.0370135307312, 285.4740946292877, 296.87306809425354, 308.272123336792, 319.6769528388977, 331.03929352760315, 342.449476480484, 353.8283591270447, 365.31505584716797, 376.7234025001526, 388.12335777282715, 399.59380197525024, 411.019296169281, 422.392005443573, 433.76942324638367, 446.20230174064636, 457.68248414993286, 469.12073516845703, 480.6093611717224, 492.0301582813263, 503.6043212413788, 515.0594818592072, 526.5326707363129, 537.9995768070221, 549.5283739566803, 561.0352010726929, 572.5425639152527, 585.3047280311584, 596.8207583427429, 608.3493003845215, 619.758702993393, 631.2516973018646, 642.8009383678436, 654.3100724220276, 665.8035476207733, 677.338880777359, 688.8402438163757, 700.3364772796631, 711.8045938014984, 723.3779654502869, 734.9503796100616, 746.4238505363464, 758.917319059372, 771.4917116165161, 782.9721403121948, 794.4077970981598, 805.8588514328003, 817.3186099529266, 828.8178360462189, 841.6136119365692, 853.1089296340942, 864.5718512535095, 876.0615437030792, 887.6522302627563, 899.1981699466705, 911.8887691497803, 924.5086352825165, 937.2236099243164, 949.8465569019318, 962.4801547527313, 975.1216161251068, 987.7684466838837, 1000.3992483615875, 1013.0721175670624, 1025.6847591400146, 1038.3991701602936, 1051.0332081317902, 1063.6905164718628, 1076.3622047901154, 1089.0299079418182, 1101.6890342235565, 1114.3129289150238, 1126.9680452346802, 1139.444251537323, 1151.7888181209564, 1164.5755734443665, 1177.3849177360535, 1190.1366369724274, 1202.9151329994202, 1206.1147999763489]
[33.09, 38.26, 42.78, 44.7475, 46.2525, 47.2975, 49.735, 50.975, 51.2475, 51.4675, 51.5725, 53.45, 51.495, 52.34, 54.5425, 52.2525, 54.175, 55.4225, 54.07, 55.2075, 54.25, 56.6125, 54.9775, 57.3025, 55.79, 56.2875, 56.8875, 56.2575, 56.83, 54.6625, 57.91, 55.6475, 57.6175, 57.6875, 58.145, 58.35, 56.34, 55.1375, 57.4925, 57.915, 56.7375, 57.7125, 57.325, 57.3675, 57.875, 58.5675, 55.8975, 55.94, 56.32, 56.0775, 57.89, 58.165, 57.495, 59.58, 57.925, 57.32, 57.065, 58.5325, 55.5225, 58.675, 58.185, 59.1825, 57.435, 58.795, 59.4725, 56.7275, 57.425, 56.96, 59.84, 57.3825, 58.42, 59.115, 57.635, 57.605, 58.805, 58.52, 56.825, 59.5275, 58.8475, 58.6125, 59.9175, 57.73, 57.9, 58.8725, 56.62, 58.0875, 57.6325, 59.38, 59.0175, 57.7125, 57.4775, 56.5825, 57.39, 58.385, 56.345, 58.3375, 59.125, 58.865, 58.4775, 59.6, 59.8875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Average accuracy final 10 rounds: 5.0 

Average global accuracy final 10 rounds: 5.0 

5132.6590666770935
[1.8309400081634521, 3.412886142730713, 5.002863645553589, 6.597237586975098, 8.175308465957642, 9.753061056137085, 11.33460783958435, 12.91037368774414, 14.49191164970398, 16.07190704345703, 17.653184413909912, 19.23763132095337, 20.803010940551758, 22.36516284942627, 23.930356740951538, 25.496779918670654, 27.05937623977661, 28.617181539535522, 30.18257474899292, 31.7506902217865, 33.311811447143555, 34.87451791763306, 36.44989037513733, 38.01788949966431, 39.58512616157532, 41.1337730884552, 42.52172303199768, 43.90135312080383, 45.28292369842529, 46.66570973396301, 48.057942152023315, 49.451786279678345, 50.840211629867554, 52.2342746257782, 53.62510895729065, 55.01077485084534, 56.40778207778931, 57.789329051971436, 59.17793345451355, 60.56168603897095, 61.94512057304382, 63.33135628700256, 64.72061848640442, 66.11238765716553, 67.49911332130432, 68.88947033882141, 70.28029584884644, 71.66630578041077, 73.05130887031555, 74.42967915534973, 75.81675481796265, 77.20861768722534, 78.58814215660095, 79.97406530380249, 81.36337804794312, 82.7568781375885, 84.40850162506104, 86.1549482345581, 87.90107154846191, 89.63810873031616, 91.37018942832947, 93.10720252990723, 94.82387137413025, 96.55282187461853, 98.2712152004242, 99.98348760604858, 101.71207094192505, 103.42121243476868, 105.13308143615723, 106.84056568145752, 108.55605554580688, 110.25602340698242, 111.96492171287537, 113.65900897979736, 115.3722608089447, 117.07774305343628, 118.77814483642578, 120.4768283367157, 122.18507432937622, 123.88993716239929, 125.5973174571991, 127.31411814689636, 129.02460479736328, 130.7394254207611, 132.46192479133606, 134.1779534816742, 135.8965609073639, 137.59700846672058, 139.28876972198486, 140.94951558113098, 142.60887575149536, 144.2832546234131, 145.98099875450134, 147.68584561347961, 149.38412833213806, 151.07613253593445, 152.77870178222656, 154.5013928413391, 156.23754286766052, 157.9612648487091, 159.67196083068848, 161.3903250694275, 163.09450244903564, 164.79880595207214, 166.51218605041504, 168.22320818901062, 169.93459558486938, 171.6522572040558, 173.3725290298462, 175.07813572883606, 176.78879046440125, 178.49701952934265, 180.2028660774231, 181.91373491287231, 183.6243495941162, 185.33426713943481, 187.04944729804993, 188.76753544807434, 190.47795009613037, 192.19205975532532, 193.89592146873474, 195.60333466529846, 197.32669878005981, 199.04361605644226, 200.76366353034973, 202.48055481910706, 204.199946641922, 205.9058403968811, 207.6125762462616, 209.32030415534973, 211.02662634849548, 212.74469661712646, 214.45883417129517, 216.177973985672, 217.9103171825409, 219.61887884140015, 221.32308673858643, 223.0310287475586, 224.73728775978088, 226.43033838272095, 228.05037593841553, 229.66184782981873, 231.28012871742249, 232.94794821739197, 234.64229893684387, 236.33716416358948, 238.03034949302673, 239.72596192359924, 241.41834378242493, 243.1035521030426, 244.7895064353943, 246.45071959495544, 248.13331365585327, 249.81020307540894, 251.50011730194092, 253.29431557655334, 255.00779724121094, 256.6513364315033, 258.29818391799927, 259.9486577510834, 261.6495041847229, 263.3615186214447, 265.07619881629944, 266.7825028896332, 268.51298451423645, 270.2463011741638, 271.98288464546204, 273.7133295536041, 275.4508216381073, 277.1684470176697, 278.9082672595978, 280.64443612098694, 282.3851013183594, 284.11476469039917, 285.84633779525757, 287.5475335121155, 289.2614531517029, 290.9909474849701, 292.7245943546295, 294.4296896457672, 296.0828158855438, 297.73146390914917, 299.3850338459015, 301.0973551273346, 302.82024788856506, 304.55159306526184, 306.2769479751587, 308.00137877464294, 309.7278425693512, 311.43838119506836, 313.15186858177185, 314.86791610717773, 316.60917353630066, 318.3548786640167, 320.08444714546204, 321.81543374061584, 323.52945494651794, 325.2418587207794, 326.95702958106995, 328.6781885623932, 330.3944933414459, 332.11109232902527, 333.84486413002014, 335.5630238056183, 337.2825050354004, 339.0039093494415, 340.72279357910156, 342.43449330329895, 344.1514537334442, 345.8629746437073, 347.5844943523407, 349.2974350452423, 351.01310634613037, 352.72901487350464, 354.4610233306885, 356.17769503593445, 357.8880705833435, 359.61540389060974, 361.3342390060425, 363.0588674545288, 364.78476428985596, 366.50802755355835, 368.0404198169708, 369.7462067604065, 371.37405490875244, 373.0231668949127, 374.6484854221344, 376.3618178367615, 378.08472204208374, 379.7937500476837, 381.5036907196045, 383.254976272583, 384.97145438194275, 386.6977262496948, 388.4138095378876, 390.1324303150177, 391.85211062431335, 393.57196378707886, 395.28786158561707, 397.0051021575928, 398.7211821079254, 400.43721055984497, 402.15918135643005, 403.9013569355011, 405.63232421875, 407.34172677993774, 409.077933549881, 410.80448365211487, 412.5154986381531, 414.2264828681946, 415.9405663013458, 417.65872955322266, 419.39587450027466, 421.1209888458252, 422.8270728588104, 424.56239104270935, 426.3044421672821, 428.01476192474365, 429.724547624588, 431.4493291378021, 433.1563551425934, 434.8726077079773, 436.60218048095703, 438.3393895626068, 440.07475233078003, 441.82288360595703, 443.55986285209656, 445.2781300544739, 447.0185112953186, 448.8158414363861, 450.548721075058, 452.2630925178528, 453.99777483940125, 455.7350115776062, 457.4617347717285, 459.1940743923187, 460.90435552597046, 462.63723969459534, 464.37495398521423, 466.0849552154541, 467.79913663864136, 469.5408661365509, 471.2566123008728, 472.9654152393341, 474.7004392147064, 476.40875816345215, 478.12874579429626, 479.944700717926, 481.6503267288208, 483.787517786026, 485.5199511051178, 487.2351200580597, 488.9751958847046, 490.72645831108093, 492.438777923584, 494.1782777309418, 495.9087905883789, 497.62047362327576, 499.35773372650146, 501.08638763427734, 503.9729619026184]
[10.8, 11.333333333333334, 9.833333333333334, 6.133333333333334, 9.466666666666667, 9.466666666666667, 8.325, 8.325, 8.325, 8.325, 8.325, 8.325, 8.325, 8.325, 8.325, 8.325, 6.658333333333333, 6.658333333333333, 6.658333333333333, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.304, Test loss: 1.053, Test accuracy: 74.08
Final Round, Global train loss: 0.304, Global test loss: 1.980, Global test accuracy: 46.69
Average accuracy final 10 rounds: 73.261 

Average global accuracy final 10 rounds: 46.497 

6139.05104136467
[5.047344923019409, 10.094689846038818, 15.040979862213135, 19.98726987838745, 24.97222065925598, 29.95717144012451, 34.96141839027405, 39.965665340423584, 44.996302366256714, 50.026939392089844, 54.95316004753113, 59.87938070297241, 64.82011675834656, 69.7608528137207, 74.31525945663452, 78.86966609954834, 83.18015384674072, 87.4906415939331, 91.82204055786133, 96.15343952178955, 100.50432467460632, 104.8552098274231, 109.19626641273499, 113.53732299804688, 118.14706325531006, 122.75680351257324, 127.0876133441925, 131.41842317581177, 135.7009847164154, 139.98354625701904, 144.6881721019745, 149.39279794692993, 153.72163367271423, 158.05046939849854, 162.36577773094177, 166.681086063385, 170.9960162639618, 175.31094646453857, 179.67532205581665, 184.03969764709473, 188.35512351989746, 192.6705493927002, 196.93991541862488, 201.20928144454956, 205.50085258483887, 209.79242372512817, 214.1088683605194, 218.42531299591064, 222.79436421394348, 227.16341543197632, 231.4270224571228, 235.6906294822693, 239.98441433906555, 244.27819919586182, 248.59851694107056, 252.9188346862793, 257.221976518631, 261.52511835098267, 265.8229217529297, 270.1207251548767, 274.4561710357666, 278.7916169166565, 283.0746591091156, 287.3577013015747, 291.65932631492615, 295.9609513282776, 300.2807152271271, 304.60047912597656, 308.959584236145, 313.3186893463135, 317.58552050590515, 321.8523516654968, 326.1687195301056, 330.48508739471436, 334.9464089870453, 339.4077305793762, 343.69059586524963, 347.97346115112305, 352.27276396751404, 356.57206678390503, 360.8658425807953, 365.15961837768555, 369.46243500709534, 373.7652516365051, 378.10658288002014, 382.44791412353516, 386.8545789718628, 391.26124382019043, 395.58059644699097, 399.8999490737915, 404.2360134124756, 408.57207775115967, 413.11144042015076, 417.65080308914185, 422.02165842056274, 426.39251375198364, 431.2243392467499, 436.0561647415161, 440.3505325317383, 444.64490032196045, 448.9570472240448, 453.26919412612915, 457.5737509727478, 461.87830781936646, 466.8360722064972, 471.79383659362793, 476.05863666534424, 480.32343673706055, 484.5828800201416, 488.84232330322266, 493.18816924095154, 497.5340151786804, 501.8748333454132, 506.215651512146, 510.5413250923157, 514.8669986724854, 519.1070840358734, 523.3471693992615, 527.6660051345825, 531.9848408699036, 536.2589230537415, 540.5330052375793, 544.7725281715393, 549.0120511054993, 553.2562944889069, 557.5005378723145, 561.7491521835327, 565.997766494751, 570.2148325443268, 574.4318985939026, 578.6820960044861, 582.9322934150696, 587.1983778476715, 591.4644622802734, 595.7278409004211, 599.9912195205688, 604.2746863365173, 608.5581531524658, 612.8501753807068, 617.1421976089478, 621.4707870483398, 625.7993764877319, 630.1170794963837, 634.4347825050354, 638.7405188083649, 643.0462551116943, 647.3507587909698, 651.6552624702454, 656.4550132751465, 661.2547640800476, 665.5478556156158, 669.8409471511841, 674.1104516983032, 678.3799562454224, 683.3672103881836, 688.3544645309448, 693.3045527935028, 698.2546410560608, 703.2188634872437, 708.1830859184265, 713.1552181243896, 718.1273503303528, 723.0683214664459, 728.0092926025391, 732.9906618595123, 737.9720311164856, 742.9252998828888, 747.878568649292, 752.8270387649536, 757.7755088806152, 762.7143259048462, 767.6531429290771, 772.6099259853363, 777.5667090415955, 782.5136296749115, 787.4605503082275, 791.7633895874023, 796.0662288665771, 800.3912925720215, 804.7163562774658, 809.0469126701355, 813.3774690628052, 817.7023043632507, 822.0271396636963, 826.2992236614227, 830.5713076591492, 834.8657557964325, 839.1602039337158, 843.4331569671631, 847.7061100006104, 851.9747722148895, 856.2434344291687, 860.5264191627502, 864.8094038963318, 869.1001918315887, 873.3909797668457, 877.7161817550659, 882.0413837432861, 886.3847572803497, 890.7281308174133, 892.8962435722351, 895.0643563270569]
[30.475, 30.475, 36.2725, 36.2725, 40.3625, 40.3625, 44.43, 44.43, 46.67, 46.67, 49.215, 49.215, 51.2425, 51.2425, 52.245, 52.245, 54.5775, 54.5775, 56.9625, 56.9625, 58.46, 58.46, 58.875, 58.875, 58.5425, 58.5425, 59.4175, 59.4175, 60.39, 60.39, 61.25, 61.25, 62.315, 62.315, 62.89, 62.89, 63.6825, 63.6825, 64.2275, 64.2275, 64.5875, 64.5875, 64.71, 64.71, 65.5275, 65.5275, 66.4725, 66.4725, 66.4725, 66.4725, 67.775, 67.775, 67.505, 67.505, 67.49, 67.49, 67.96, 67.96, 68.1625, 68.1625, 68.5425, 68.5425, 68.9075, 68.9075, 69.08, 69.08, 69.3875, 69.3875, 69.7025, 69.7025, 70.0625, 70.0625, 70.015, 70.015, 70.05, 70.05, 70.0525, 70.0525, 70.28, 70.28, 70.295, 70.295, 70.44, 70.44, 70.6075, 70.6075, 70.535, 70.535, 70.8225, 70.8225, 70.8975, 70.8975, 71.1875, 71.1875, 71.055, 71.055, 71.4325, 71.4325, 71.55, 71.55, 71.635, 71.635, 71.495, 71.495, 71.4325, 71.4325, 71.56, 71.56, 71.715, 71.715, 71.895, 71.895, 72.0525, 72.0525, 72.1775, 72.1775, 72.175, 72.175, 72.3075, 72.3075, 72.29, 72.29, 72.33, 72.33, 72.3425, 72.3425, 72.4375, 72.4375, 72.44, 72.44, 72.3925, 72.3925, 72.42, 72.42, 72.5775, 72.5775, 72.5275, 72.5275, 72.4825, 72.4825, 72.5775, 72.5775, 72.66, 72.66, 72.8525, 72.8525, 72.615, 72.615, 72.5, 72.5, 72.575, 72.575, 72.7125, 72.7125, 72.575, 72.575, 72.8325, 72.8325, 73.04, 73.04, 72.8525, 72.8525, 73.055, 73.055, 73.2425, 73.2425, 73.1325, 73.1325, 73.2525, 73.2525, 73.2425, 73.2425, 73.0675, 73.0675, 73.3575, 73.3575, 73.335, 73.335, 73.1475, 73.1475, 73.3, 73.3, 73.34, 73.34, 73.1875, 73.1875, 72.995, 72.995, 73.21, 73.21, 73.3375, 73.3375, 73.4575, 73.4575, 73.2, 73.2, 73.2025, 73.2025, 73.38, 73.38, 74.085, 74.085]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.408, Test loss: 0.770, Test accuracy: 76.39
Average accuracy final 10 rounds: 75.70325 

5098.890469312668
[4.932300090789795, 9.86460018157959, 14.526069164276123, 19.187538146972656, 23.846147060394287, 28.504755973815918, 33.14546084403992, 37.786165714263916, 42.431278705596924, 47.07639169692993, 51.742090702056885, 56.40778970718384, 61.05816149711609, 65.70853328704834, 70.35903644561768, 75.00953960418701, 79.66235733032227, 84.31517505645752, 88.9831166267395, 93.65105819702148, 98.29184746742249, 102.93263673782349, 107.5916998386383, 112.25076293945312, 116.92723965644836, 121.6037163734436, 126.2968897819519, 130.9900631904602, 135.64226770401, 140.29447221755981, 144.95855140686035, 149.6226305961609, 154.27565145492554, 158.92867231369019, 163.58179450035095, 168.23491668701172, 172.9004065990448, 177.56589651107788, 182.20582842826843, 186.84576034545898, 191.49171137809753, 196.13766241073608, 200.78605127334595, 205.4344401359558, 210.07658982276917, 214.71873950958252, 219.32440996170044, 223.93008041381836, 228.55804657936096, 233.18601274490356, 237.8292384147644, 242.47246408462524, 247.12336540222168, 251.77426671981812, 256.42704129219055, 261.079815864563, 265.74991941452026, 270.42002296447754, 275.11098194122314, 279.80194091796875, 284.4994432926178, 289.19694566726685, 293.8662202358246, 298.5354948043823, 303.1855173110962, 307.83553981781006, 312.4966051578522, 317.1576704978943, 321.82142877578735, 326.4851870536804, 331.14912366867065, 335.8130602836609, 340.49555015563965, 345.1780400276184, 349.8447606563568, 354.5114812850952, 359.13806986808777, 363.7646584510803, 368.41026496887207, 373.0558714866638, 377.6945209503174, 382.33317041397095, 386.9833483695984, 391.63352632522583, 396.27581763267517, 400.9181089401245, 405.53378343582153, 410.14945793151855, 414.78144216537476, 419.41342639923096, 424.06051111221313, 428.7075958251953, 433.36672258377075, 438.0258493423462, 442.63674330711365, 447.2476372718811, 451.8928277492523, 456.53801822662354, 461.1985032558441, 465.8589882850647, 470.6708297729492, 475.48267126083374, 480.19212555885315, 484.90157985687256, 489.620343208313, 494.3391065597534, 498.9282305240631, 503.5173544883728, 508.1885347366333, 512.8597149848938, 517.5063002109528, 522.1528854370117, 526.854514837265, 531.5561442375183, 536.2365462779999, 540.9169483184814, 545.5935180187225, 550.2700877189636, 554.984354019165, 559.6986203193665, 564.4584600925446, 569.2182998657227, 573.9373655319214, 578.6564311981201, 583.2550506591797, 587.8536701202393, 592.5085074901581, 597.1633448600769, 601.8237040042877, 606.4840631484985, 611.1386475563049, 615.7932319641113, 620.4468960762024, 625.1005601882935, 629.74054646492, 634.3805327415466, 638.9778707027435, 643.5752086639404, 648.2801721096039, 652.9851355552673, 657.672210931778, 662.3592863082886, 666.9866108894348, 671.613935470581, 676.2797017097473, 680.9454679489136, 685.3187310695648, 689.6919941902161, 694.3447117805481, 698.9974293708801, 703.6616730690002, 708.3259167671204, 712.9036259651184, 717.4813351631165, 722.0815942287445, 726.6818532943726, 731.2724752426147, 735.8630971908569, 740.4708874225616, 745.0786776542664, 749.6482841968536, 754.2178907394409, 758.8487572669983, 763.4796237945557, 768.1247470378876, 772.7698702812195, 777.4080119132996, 782.0461535453796, 786.6355633735657, 791.2249732017517, 795.816168308258, 800.4073634147644, 805.0197947025299, 809.6322259902954, 814.2224457263947, 818.8126654624939, 823.5114200115204, 828.2101745605469, 832.8639371395111, 837.5176997184753, 842.1418197154999, 846.7659397125244, 851.2851283550262, 855.8043169975281, 860.4986464977264, 865.1929759979248, 869.8721492290497, 874.5513224601746, 879.25421833992, 883.9571142196655, 888.5432815551758, 893.129448890686, 897.6841819286346, 902.2389149665833, 906.9297544956207, 911.6205940246582, 916.3262310028076, 921.031867980957, 925.6532027721405, 930.274537563324, 932.3841671943665, 934.4937968254089]
[25.1325, 25.1325, 31.79, 31.79, 36.7425, 36.7425, 40.3575, 40.3575, 44.94, 44.94, 47.93, 47.93, 50.43, 50.43, 52.0625, 52.0625, 55.1225, 55.1225, 57.755, 57.755, 58.465, 58.465, 60.17, 60.17, 61.63, 61.63, 62.3925, 62.3925, 64.4475, 64.4475, 64.9, 64.9, 64.88, 64.88, 65.815, 65.815, 66.69, 66.69, 67.39, 67.39, 68.025, 68.025, 68.27, 68.27, 68.5175, 68.5175, 69.2725, 69.2725, 69.9475, 69.9475, 70.25, 70.25, 70.5, 70.5, 70.6525, 70.6525, 70.6475, 70.6475, 71.2475, 71.2475, 71.645, 71.645, 71.7775, 71.7775, 71.575, 71.575, 71.8425, 71.8425, 72.345, 72.345, 72.9775, 72.9775, 73.1625, 73.1625, 73.24, 73.24, 72.89, 72.89, 73.0375, 73.0375, 73.0775, 73.0775, 73.625, 73.625, 73.2175, 73.2175, 73.6825, 73.6825, 73.915, 73.915, 74.335, 74.335, 74.16, 74.16, 74.0425, 74.0425, 74.045, 74.045, 74.6625, 74.6625, 74.4325, 74.4325, 74.415, 74.415, 74.715, 74.715, 74.7425, 74.7425, 74.8325, 74.8325, 74.9625, 74.9625, 74.57, 74.57, 74.7125, 74.7125, 74.7925, 74.7925, 75.1925, 75.1925, 74.2575, 74.2575, 74.7675, 74.7675, 74.5475, 74.5475, 74.715, 74.715, 74.9075, 74.9075, 74.785, 74.785, 74.285, 74.285, 74.795, 74.795, 74.86, 74.86, 74.745, 74.745, 75.35, 75.35, 74.9275, 74.9275, 75.16, 75.16, 75.1725, 75.1725, 75.18, 75.18, 75.0825, 75.0825, 75.0625, 75.0625, 75.0425, 75.0425, 75.2775, 75.2775, 74.7975, 74.7975, 75.4275, 75.4275, 75.3325, 75.3325, 75.05, 75.05, 75.7025, 75.7025, 75.915, 75.915, 75.56, 75.56, 75.5175, 75.5175, 75.14, 75.14, 75.3425, 75.3425, 75.455, 75.455, 75.5225, 75.5225, 75.555, 75.555, 75.58, 75.58, 75.89, 75.89, 75.785, 75.785, 75.6225, 75.6225, 75.7425, 75.7425, 75.7275, 75.7275, 75.9, 75.9, 75.7075, 75.7075, 76.395, 76.395]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.420, Test loss: 0.711, Test accuracy: 77.09
Average accuracy final 10 rounds: 77.20725
5662.736489772797
[6.457761526107788, 12.915523052215576, 18.805187940597534, 24.694852828979492, 30.582292079925537, 36.46973133087158, 42.374829053878784, 48.279926776885986, 53.654502153396606, 59.02907752990723, 64.39437294006348, 69.75966835021973, 75.1419882774353, 80.52430820465088, 85.96101236343384, 91.3977165222168, 96.86615014076233, 102.33458375930786, 107.73651051521301, 113.13843727111816, 118.5584089756012, 123.97838068008423, 129.32640600204468, 134.67443132400513, 140.03860545158386, 145.4027795791626, 150.6954345703125, 155.9880895614624, 161.3213930130005, 166.65469646453857, 172.049161195755, 177.44362592697144, 182.7979826927185, 188.15233945846558, 193.51898789405823, 198.88563632965088, 204.26803827285767, 209.65044021606445, 215.02278780937195, 220.39513540267944, 225.79688000679016, 231.19862461090088, 236.55489659309387, 241.91116857528687, 247.2843074798584, 252.65744638442993, 257.9982862472534, 263.3391261100769, 268.72576451301575, 274.1124029159546, 279.4321753978729, 284.75194787979126, 290.1026449203491, 295.453341960907, 300.8619501590729, 306.27055835723877, 311.6785981655121, 317.0866379737854, 322.4150562286377, 327.74347448349, 333.02175784111023, 338.30004119873047, 343.656507730484, 349.01297426223755, 354.3146653175354, 359.61635637283325, 364.92889738082886, 370.24143838882446, 375.6648008823395, 381.0881633758545, 386.46012568473816, 391.8320879936218, 397.2637195587158, 402.6953511238098, 408.0236737728119, 413.35199642181396, 418.77345752716064, 424.1949186325073, 429.5678217411041, 434.9407248497009, 440.320027589798, 445.699330329895, 450.9957914352417, 456.2922525405884, 461.6021842956543, 466.9121160507202, 472.28005290031433, 477.64798974990845, 483.0090992450714, 488.3702087402344, 493.7869288921356, 499.20364904403687, 504.57890915870667, 509.95416927337646, 515.4035074710846, 520.8528456687927, 526.3090348243713, 531.76522397995, 537.1721684932709, 542.5791130065918, 547.9892961978912, 553.3994793891907, 558.7562441825867, 564.1130089759827, 569.4537250995636, 574.7944412231445, 580.1866216659546, 585.5788021087646, 590.9629416465759, 596.3470811843872, 601.7832522392273, 607.2194232940674, 612.621750831604, 618.0240783691406, 623.4412815570831, 628.8584847450256, 634.206793308258, 639.5551018714905, 644.9297595024109, 650.3044171333313, 655.6344134807587, 660.964409828186, 666.3036820888519, 671.6429543495178, 677.0310287475586, 682.4191031455994, 687.7936615943909, 693.1682200431824, 698.610757112503, 704.0532941818237, 709.47975730896, 714.9062204360962, 720.3850531578064, 725.8638858795166, 731.2635893821716, 736.6632928848267, 742.0501623153687, 747.4370317459106, 752.8532919883728, 758.269552230835, 763.6553158760071, 769.0410795211792, 774.4572250843048, 779.8733706474304, 785.3343760967255, 790.7953815460205, 796.162095785141, 801.5288100242615, 806.9305255413055, 812.3322410583496, 817.7562463283539, 823.1802515983582, 828.6077148914337, 834.0351781845093, 839.9979190826416, 845.9606599807739, 851.4552357196808, 856.9498114585876, 862.5461673736572, 868.1425232887268, 874.1295249462128, 880.1165266036987, 886.1992104053497, 892.2818942070007, 898.2373974323273, 904.1929006576538, 910.1830379962921, 916.1731753349304, 922.1619529724121, 928.1507306098938, 934.1298673152924, 940.1090040206909, 946.0427911281586, 951.9765782356262, 958.0440368652344, 964.1114954948425, 970.1738662719727, 976.2362370491028, 982.3381087779999, 988.439980506897, 994.589361667633, 1000.7387428283691, 1006.8987908363342, 1013.0588388442993, 1019.1601266860962, 1025.261414527893, 1031.4175868034363, 1037.5737590789795, 1043.7879354953766, 1050.0021119117737, 1055.9401152133942, 1061.8781185150146, 1067.8373305797577, 1073.7965426445007, 1079.9103105068207, 1086.0240783691406, 1092.1891272068024, 1098.354176044464, 1103.890298128128, 1109.426420211792, 1111.5895190238953, 1113.7526178359985]
[23.8475, 23.8475, 31.295, 31.295, 36.995, 36.995, 43.405, 43.405, 47.3725, 47.3725, 50.6025, 50.6025, 53.2075, 53.2075, 55.2825, 55.2825, 55.7775, 55.7775, 58.065, 58.065, 61.225, 61.225, 61.8475, 61.8475, 63.1575, 63.1575, 63.7875, 63.7875, 64.4975, 64.4975, 65.24, 65.24, 67.0625, 67.0625, 67.7075, 67.7075, 68.63, 68.63, 68.9575, 68.9575, 69.525, 69.525, 69.7075, 69.7075, 70.63, 70.63, 71.21, 71.21, 71.4675, 71.4675, 71.45, 71.45, 72.0725, 72.0725, 72.145, 72.145, 72.39, 72.39, 72.18, 72.18, 72.6325, 72.6325, 73.325, 73.325, 73.365, 73.365, 73.73, 73.73, 73.6525, 73.6525, 73.7475, 73.7475, 73.96, 73.96, 73.545, 73.545, 74.0825, 74.0825, 74.425, 74.425, 74.915, 74.915, 74.2375, 74.2375, 74.61, 74.61, 74.785, 74.785, 75.0475, 75.0475, 75.5575, 75.5575, 74.9975, 74.9975, 75.46, 75.46, 75.585, 75.585, 75.6875, 75.6875, 75.3225, 75.3225, 75.3975, 75.3975, 75.7025, 75.7025, 75.89, 75.89, 75.9275, 75.9275, 75.595, 75.595, 75.66, 75.66, 75.9575, 75.9575, 75.7, 75.7, 76.135, 76.135, 76.1525, 76.1525, 76.2225, 76.2225, 76.4425, 76.4425, 76.0525, 76.0525, 76.205, 76.205, 76.5725, 76.5725, 76.6525, 76.6525, 76.4475, 76.4475, 76.3775, 76.3775, 76.5175, 76.5175, 76.255, 76.255, 75.9925, 75.9925, 76.1825, 76.1825, 76.22, 76.22, 76.25, 76.25, 76.4975, 76.4975, 76.7, 76.7, 76.83, 76.83, 76.7075, 76.7075, 76.6475, 76.6475, 76.7975, 76.7975, 76.8175, 76.8175, 76.7875, 76.7875, 77.1875, 77.1875, 77.0025, 77.0025, 76.5875, 76.5875, 77.2775, 77.2775, 77.0575, 77.0575, 77.07, 77.07, 77.0425, 77.0425, 77.4325, 77.4325, 77.0075, 77.0075, 77.4575, 77.4575, 77.0475, 77.0475, 77.14, 77.14, 77.155, 77.155, 76.84, 76.84, 77.2925, 77.2925, 77.55, 77.55, 77.15, 77.15, 77.09, 77.09]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.164, Test loss: 2.192, Test accuracy: 16.84
Round   1, Train loss: 0.960, Test loss: 2.072, Test accuracy: 32.69
Round   2, Train loss: 0.894, Test loss: 2.137, Test accuracy: 35.24
Round   3, Train loss: 0.835, Test loss: 2.112, Test accuracy: 35.63
Round   4, Train loss: 0.843, Test loss: 2.128, Test accuracy: 39.12
Round   5, Train loss: 0.744, Test loss: 2.055, Test accuracy: 41.76
Round   6, Train loss: 0.731, Test loss: 1.953, Test accuracy: 41.62
Round   7, Train loss: 0.789, Test loss: 1.967, Test accuracy: 40.08
Round   8, Train loss: 0.671, Test loss: 1.973, Test accuracy: 40.56
Round   9, Train loss: 0.750, Test loss: 1.915, Test accuracy: 42.52
Round  10, Train loss: 0.565, Test loss: 1.837, Test accuracy: 47.09
Round  11, Train loss: 0.600, Test loss: 1.918, Test accuracy: 45.23
Round  12, Train loss: 0.604, Test loss: 1.920, Test accuracy: 45.50
Round  13, Train loss: 0.583, Test loss: 1.778, Test accuracy: 46.52
Round  14, Train loss: 0.645, Test loss: 1.695, Test accuracy: 45.53
Round  15, Train loss: 0.536, Test loss: 1.871, Test accuracy: 45.32
Round  16, Train loss: 0.481, Test loss: 2.065, Test accuracy: 49.52
Round  17, Train loss: 0.550, Test loss: 1.926, Test accuracy: 47.49
Round  18, Train loss: 0.508, Test loss: 1.909, Test accuracy: 46.36
Round  19, Train loss: 0.518, Test loss: 1.871, Test accuracy: 46.97
Round  20, Train loss: 0.470, Test loss: 1.940, Test accuracy: 46.32
Round  21, Train loss: 0.509, Test loss: 1.815, Test accuracy: 48.96
Round  22, Train loss: 0.460, Test loss: 1.935, Test accuracy: 50.50
Round  23, Train loss: 0.468, Test loss: 1.652, Test accuracy: 48.77
Round  24, Train loss: 0.501, Test loss: 1.800, Test accuracy: 51.27
Round  25, Train loss: 0.473, Test loss: 1.626, Test accuracy: 49.67
Round  26, Train loss: 0.553, Test loss: 1.575, Test accuracy: 49.32
Round  27, Train loss: 0.495, Test loss: 1.733, Test accuracy: 46.31
Round  28, Train loss: 0.490, Test loss: 1.726, Test accuracy: 48.01
Round  29, Train loss: 0.406, Test loss: 1.705, Test accuracy: 50.27
Round  30, Train loss: 0.452, Test loss: 1.598, Test accuracy: 50.84
Round  31, Train loss: 0.421, Test loss: 1.819, Test accuracy: 48.54
Round  32, Train loss: 0.380, Test loss: 1.658, Test accuracy: 50.52
Round  33, Train loss: 0.483, Test loss: 1.555, Test accuracy: 50.88
Round  34, Train loss: 0.417, Test loss: 1.911, Test accuracy: 47.02
Round  35, Train loss: 0.413, Test loss: 1.586, Test accuracy: 48.98
Round  36, Train loss: 0.365, Test loss: 1.820, Test accuracy: 49.62
Round  37, Train loss: 0.315, Test loss: 1.779, Test accuracy: 48.30
Round  38, Train loss: 0.385, Test loss: 1.605, Test accuracy: 53.65
Round  39, Train loss: 0.444, Test loss: 1.623, Test accuracy: 49.11
Round  40, Train loss: 0.312, Test loss: 1.687, Test accuracy: 52.60
Round  41, Train loss: 0.316, Test loss: 1.735, Test accuracy: 50.59
Round  42, Train loss: 0.303, Test loss: 1.747, Test accuracy: 51.01
Round  43, Train loss: 0.388, Test loss: 1.671, Test accuracy: 51.15
Round  44, Train loss: 0.302, Test loss: 1.683, Test accuracy: 53.28
Round  45, Train loss: 0.341, Test loss: 1.622, Test accuracy: 50.15
Round  46, Train loss: 0.320, Test loss: 1.950, Test accuracy: 48.02
Round  47, Train loss: 0.378, Test loss: 2.132, Test accuracy: 45.22
Round  48, Train loss: 0.375, Test loss: 2.188, Test accuracy: 43.03
Round  49, Train loss: 0.286, Test loss: 1.970, Test accuracy: 49.85
Round  50, Train loss: 0.263, Test loss: 1.811, Test accuracy: 49.33
Round  51, Train loss: 0.251, Test loss: 1.863, Test accuracy: 50.18
Round  52, Train loss: 0.284, Test loss: 1.760, Test accuracy: 51.83
Round  53, Train loss: 0.231, Test loss: 1.894, Test accuracy: 50.58
Round  54, Train loss: 0.253, Test loss: 1.628, Test accuracy: 50.13
Round  55, Train loss: 0.309, Test loss: 1.820, Test accuracy: 48.98
Round  56, Train loss: 0.220, Test loss: 1.753, Test accuracy: 52.03
Round  57, Train loss: 0.293, Test loss: 1.771, Test accuracy: 51.21
Round  58, Train loss: 0.219, Test loss: 1.914, Test accuracy: 51.67
Round  59, Train loss: 0.294, Test loss: 2.107, Test accuracy: 51.62
Round  60, Train loss: 0.227, Test loss: 1.806, Test accuracy: 51.73
Round  61, Train loss: 0.224, Test loss: 1.791, Test accuracy: 49.48
Round  62, Train loss: 0.265, Test loss: 1.575, Test accuracy: 52.10
Round  63, Train loss: 0.292, Test loss: 1.893, Test accuracy: 48.85
Round  64, Train loss: 0.261, Test loss: 1.895, Test accuracy: 48.78
Round  65, Train loss: 0.236, Test loss: 2.072, Test accuracy: 50.11
Round  66, Train loss: 0.243, Test loss: 1.824, Test accuracy: 50.92
Round  67, Train loss: 0.230, Test loss: 1.714, Test accuracy: 49.68
Round  68, Train loss: 0.224, Test loss: 1.830, Test accuracy: 49.27
Round  69, Train loss: 0.325, Test loss: 1.718, Test accuracy: 52.82
Round  70, Train loss: 0.196, Test loss: 2.222, Test accuracy: 45.92
Round  71, Train loss: 0.213, Test loss: 1.815, Test accuracy: 50.25
Round  72, Train loss: 0.216, Test loss: 1.912, Test accuracy: 50.86
Round  73, Train loss: 0.306, Test loss: 1.681, Test accuracy: 53.61
Round  74, Train loss: 0.216, Test loss: 1.851, Test accuracy: 52.24
Round  75, Train loss: 0.160, Test loss: 1.781, Test accuracy: 51.63
Round  76, Train loss: 0.214, Test loss: 2.026, Test accuracy: 47.90
Round  77, Train loss: 0.183, Test loss: 2.031, Test accuracy: 48.18
Round  78, Train loss: 0.197, Test loss: 1.832, Test accuracy: 52.45
Round  79, Train loss: 0.186, Test loss: 1.902, Test accuracy: 52.58
Round  80, Train loss: 0.172, Test loss: 1.927, Test accuracy: 51.42
Round  81, Train loss: 0.187, Test loss: 1.817, Test accuracy: 50.92
Round  82, Train loss: 0.153, Test loss: 2.052, Test accuracy: 51.84
Round  83, Train loss: 0.265, Test loss: 1.753, Test accuracy: 53.02
Round  84, Train loss: 0.181, Test loss: 1.959, Test accuracy: 50.23
Round  85, Train loss: 0.162, Test loss: 1.914, Test accuracy: 52.70
Round  86, Train loss: 0.169, Test loss: 1.867, Test accuracy: 51.05
Round  87, Train loss: 0.272, Test loss: 1.796, Test accuracy: 53.02
Round  88, Train loss: 0.202, Test loss: 2.156, Test accuracy: 49.93
Round  89, Train loss: 0.198, Test loss: 1.811, Test accuracy: 51.08
Round  90, Train loss: 0.152, Test loss: 2.198, Test accuracy: 46.74
Round  91, Train loss: 0.168, Test loss: 1.810, Test accuracy: 51.27
Round  92, Train loss: 0.221, Test loss: 2.149, Test accuracy: 52.02
Round  93, Train loss: 0.156, Test loss: 1.829, Test accuracy: 51.77
Round  94, Train loss: 0.188, Test loss: 2.160, Test accuracy: 50.01
Round  95, Train loss: 0.150, Test loss: 2.067, Test accuracy: 51.00
Round  96, Train loss: 0.140, Test loss: 2.241, Test accuracy: 49.89
Round  97, Train loss: 0.147, Test loss: 1.869, Test accuracy: 52.32
Round  98, Train loss: 0.179, Test loss: 1.739, Test accuracy: 54.48
Round  99, Train loss: 0.252, Test loss: 1.749, Test accuracy: 47.70
Final Round, Train loss: 0.156, Test loss: 1.587, Test accuracy: 54.62
Average accuracy final 10 rounds: 50.72166666666667
2555.8149712085724
[4.299490213394165, 8.1143319606781, 11.992260694503784, 15.759220123291016, 19.653923749923706, 23.49654197692871, 27.366958618164062, 31.26176929473877, 35.0942862033844, 38.99090123176575, 42.645005226135254, 46.39745473861694, 50.17851376533508, 54.070029735565186, 57.89680504798889, 61.70030355453491, 65.52826642990112, 69.30535101890564, 73.1223452091217, 76.93254208564758, 80.75226473808289, 84.56658148765564, 88.36569595336914, 92.18666338920593, 96.01832985877991, 99.96401262283325, 103.75978589057922, 107.64379334449768, 111.42799806594849, 115.3618552684784, 119.32013940811157, 123.0374345779419, 126.72577357292175, 130.3876302242279, 134.12853121757507, 137.79284691810608, 141.3958022594452, 145.09573411941528, 148.84634041786194, 152.57773327827454, 156.33023476600647, 160.03863072395325, 163.9339084625244, 167.68700194358826, 171.47742247581482, 175.27189993858337, 179.03453421592712, 182.8362636566162, 186.58710527420044, 190.35730934143066, 194.16357398033142, 197.95139408111572, 201.74450969696045, 205.50759959220886, 209.3446033000946, 213.09058165550232, 216.88776350021362, 220.65439176559448, 224.42548370361328, 228.2086706161499, 231.94708847999573, 235.4119908809662, 238.8720998764038, 242.29080057144165, 245.73171401023865, 249.13504242897034, 252.56317472457886, 256.0014114379883, 259.37451338768005, 262.8145270347595, 266.2281324863434, 269.6363682746887, 273.01974511146545, 276.38289856910706, 279.8034071922302, 283.1674313545227, 286.76093554496765, 290.27002716064453, 293.6848797798157, 297.1115093231201, 300.6132113933563, 304.04145407676697, 307.4919857978821, 310.91747188568115, 314.36463928222656, 317.7653121948242, 321.1517117023468, 324.51630115509033, 327.85570788383484, 331.2718722820282, 334.6563596725464, 338.0808665752411, 341.4492473602295, 344.82247591018677, 348.1656725406647, 351.5327408313751, 354.8758728504181, 358.29423689842224, 361.6685194969177, 365.06016993522644, 367.90821075439453]
[16.841666666666665, 32.69166666666667, 35.24166666666667, 35.63333333333333, 39.11666666666667, 41.75833333333333, 41.61666666666667, 40.083333333333336, 40.55833333333333, 42.516666666666666, 47.09166666666667, 45.233333333333334, 45.5, 46.525, 45.53333333333333, 45.31666666666667, 49.516666666666666, 47.49166666666667, 46.358333333333334, 46.96666666666667, 46.31666666666667, 48.958333333333336, 50.5, 48.775, 51.266666666666666, 49.675, 49.31666666666667, 46.30833333333333, 48.00833333333333, 50.275, 50.84166666666667, 48.541666666666664, 50.525, 50.875, 47.016666666666666, 48.975, 49.61666666666667, 48.3, 53.65, 49.108333333333334, 52.6, 50.59166666666667, 51.00833333333333, 51.15, 53.28333333333333, 50.15, 48.016666666666666, 45.21666666666667, 43.03333333333333, 49.85, 49.333333333333336, 50.18333333333333, 51.825, 50.583333333333336, 50.13333333333333, 48.983333333333334, 52.03333333333333, 51.208333333333336, 51.666666666666664, 51.61666666666667, 51.725, 49.483333333333334, 52.1, 48.85, 48.78333333333333, 50.108333333333334, 50.925, 49.68333333333333, 49.266666666666666, 52.81666666666667, 45.925, 50.25, 50.858333333333334, 53.608333333333334, 52.24166666666667, 51.63333333333333, 47.9, 48.18333333333333, 52.45, 52.583333333333336, 51.425, 50.916666666666664, 51.84166666666667, 53.025, 50.225, 52.7, 51.05, 53.016666666666666, 49.93333333333333, 51.075, 46.74166666666667, 51.275, 52.025, 51.775, 50.00833333333333, 51.0, 49.891666666666666, 52.31666666666667, 54.483333333333334, 47.7, 54.625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.276, Test loss: 2.298, Test accuracy: 6.68
Round   0, Global train loss: 2.276, Global test loss: 2.300, Global test accuracy: 6.68
Round   1, Train loss: 2.284, Test loss: 2.296, Test accuracy: 6.72
Round   1, Global train loss: 2.284, Global test loss: 2.300, Global test accuracy: 6.68
Round   2, Train loss: 2.266, Test loss: 2.293, Test accuracy: 6.83
Round   2, Global train loss: 2.266, Global test loss: 2.299, Global test accuracy: 6.74
Round   3, Train loss: 2.283, Test loss: 2.293, Test accuracy: 6.91
Round   3, Global train loss: 2.283, Global test loss: 2.299, Global test accuracy: 6.66
Round   4, Train loss: 2.275, Test loss: 2.292, Test accuracy: 7.28
Round   4, Global train loss: 2.275, Global test loss: 2.299, Global test accuracy: 6.72
Round   5, Train loss: 2.256, Test loss: 2.292, Test accuracy: 9.49
Round   5, Global train loss: 2.256, Global test loss: 2.298, Global test accuracy: 6.93
Round   6, Train loss: 2.293, Test loss: 2.293, Test accuracy: 8.96
Round   6, Global train loss: 2.293, Global test loss: 2.299, Global test accuracy: 7.14
Round   7, Train loss: 2.245, Test loss: 2.292, Test accuracy: 11.97
Round   7, Global train loss: 2.245, Global test loss: 2.298, Global test accuracy: 10.22
Round   8, Train loss: 2.258, Test loss: 2.293, Test accuracy: 13.96
Round   8, Global train loss: 2.258, Global test loss: 2.297, Global test accuracy: 15.32
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 16.02
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 12.18
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 12.18
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 8.28
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 7.74
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 7.27
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 5.51
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 5.51
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 5.69
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 5.69
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 5.69
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 5.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 5.00
Average accuracy final 10 rounds: 5.0 

Average global accuracy final 10 rounds: 5.0 

4740.990002155304
[1.8311293125152588, 3.4092986583709717, 4.989552736282349, 6.588330030441284, 8.164502143859863, 9.74707841873169, 11.339690685272217, 12.921566009521484, 14.505355596542358, 16.090490579605103, 17.66967463493347, 19.25203251838684, 20.84105944633484, 22.429595947265625, 24.005434036254883, 25.590497255325317, 27.19154715538025, 28.816267490386963, 30.417305946350098, 32.0775351524353, 33.71305441856384, 35.29987573623657, 36.98320531845093, 38.63742518424988, 40.218196392059326, 41.8773775100708, 43.514435052871704, 45.177334785461426, 46.84578728675842, 48.49176549911499, 50.12452030181885, 51.73086857795715, 53.33654165267944, 54.99417281150818, 56.637863636016846, 58.212971448898315, 59.88680839538574, 61.528944969177246, 63.10291361808777, 64.73650813102722, 66.31965279579163, 67.89623832702637, 69.53131556510925, 71.11282420158386, 72.68774771690369, 74.30707740783691, 75.68246126174927, 77.08150959014893, 78.51145458221436, 79.93250727653503, 81.32410025596619, 82.73683023452759, 84.14233803749084, 85.56653094291687, 86.97231316566467, 88.3719310760498, 89.79525184631348, 91.20114088058472, 92.60074925422668, 94.01573467254639, 95.51186728477478, 96.91723704338074, 98.32908368110657, 99.84227085113525, 101.29549169540405, 102.71656370162964, 104.13103652000427, 105.62572741508484, 107.03118848800659, 108.5064024925232, 110.00080132484436, 111.39545631408691, 112.80004024505615, 114.1996443271637, 115.65172076225281, 117.11689233779907, 118.52167797088623, 119.9828588962555, 121.47308301925659, 122.93682765960693, 124.33742713928223, 125.82561802864075, 127.30017852783203, 128.7078607082367, 130.14922046661377, 131.6419656276703, 133.04573440551758, 134.45224452018738, 135.9444136619568, 137.39846563339233, 138.81090712547302, 140.26078343391418, 141.6728994846344, 143.07377433776855, 144.5478904247284, 145.96155977249146, 147.43664622306824, 148.8904685974121, 150.27857327461243, 151.76931142807007, 153.2782814502716, 154.6836404800415, 156.11098623275757, 157.63186264038086, 159.02770733833313, 160.43215441703796, 161.94016981124878, 163.38190984725952, 164.79670190811157, 166.18728280067444, 167.63435435295105, 169.03798723220825, 170.42577481269836, 171.80760979652405, 173.1916651725769, 174.57860708236694, 175.95796513557434, 177.34122347831726, 178.72659993171692, 180.10593271255493, 181.4922034740448, 182.90707564353943, 184.31633639335632, 185.72833609580994, 187.17313146591187, 188.57624053955078, 189.9788796901703, 191.40495610237122, 192.83785676956177, 194.25341248512268, 195.6760537624359, 197.12273383140564, 198.5419270992279, 199.952862739563, 201.36841201782227, 202.78284740447998, 204.19184803962708, 205.59810638427734, 207.0096275806427, 208.4378731250763, 209.85446047782898, 211.25551080703735, 212.67475700378418, 214.0783829689026, 215.48894381523132, 216.90084314346313, 218.32036638259888, 219.713228225708, 221.1242117881775, 222.53994059562683, 223.94112825393677, 225.33122038841248, 226.72724413871765, 228.12869477272034, 229.51210689544678, 230.90862035751343, 232.30425214767456, 233.70012211799622, 235.09999632835388, 236.48933124542236, 237.90354466438293, 239.3143756389618, 240.69734597206116, 242.09869575500488, 243.53656649589539, 244.92342019081116, 246.31581616401672, 247.75959157943726, 249.15140295028687, 250.55454015731812, 252.03218865394592, 253.45802879333496, 254.86176657676697, 256.27795910835266, 257.7620129585266, 259.1769781112671, 260.583425283432, 261.9940230846405, 263.38345289230347, 264.78017711639404, 266.1910767555237, 267.59623622894287, 269.01485681533813, 270.3968839645386, 271.7828516960144, 273.2402620315552, 274.64746856689453, 276.0410485267639, 277.4781062602997, 278.9828643798828, 280.38301849365234, 281.79472970962524, 283.30353450775146, 284.73077416419983, 286.119588136673, 287.6218409538269, 289.0770273208618, 290.51068806648254, 291.91827368736267, 293.33279633522034, 294.7564253807068, 296.1655607223511, 297.576762676239, 298.99682426452637, 300.419221162796, 301.8452808856964, 303.27831745147705, 304.7132318019867, 306.13043546676636, 307.55008006095886, 308.9652807712555, 310.3589816093445, 311.76578760147095, 313.18529891967773, 314.56969690322876, 315.9430491924286, 317.33663630485535, 318.7477538585663, 320.1316833496094, 321.517213344574, 322.9145476818085, 324.3110177516937, 325.69648361206055, 327.0754451751709, 328.46740102767944, 329.9099042415619, 331.312349319458, 332.7247486114502, 334.15370321273804, 335.58320474624634, 336.9881682395935, 338.38918375968933, 339.8143391609192, 341.24247789382935, 342.65964698791504, 344.0750844478607, 345.4741792678833, 346.9066286087036, 348.29608368873596, 349.6785304546356, 351.0553545951843, 352.4325785636902, 353.81869673728943, 355.2343785762787, 356.65114641189575, 358.05462741851807, 359.4559919834137, 360.8688020706177, 362.36866188049316, 363.78453731536865, 365.196408033371, 366.706814289093, 368.1278305053711, 369.5415108203888, 370.96594405174255, 372.46532011032104, 373.955358505249, 375.38011264801025, 376.88207960128784, 378.3046386241913, 379.7187442779541, 381.1585683822632, 382.5869879722595, 383.9643204212189, 385.33355498313904, 386.71042251586914, 388.08012795448303, 389.4602360725403, 390.8826689720154, 392.27211141586304, 393.6595199108124, 395.0426733493805, 396.4265878200531, 397.81466913223267, 399.21522974967957, 400.60107588768005, 402.0163059234619, 403.4057545661926, 404.80360555648804, 406.21732234954834, 407.6143341064453, 409.0409164428711, 410.5038516521454, 411.9921100139618, 413.447958946228, 414.8775918483734, 416.30869150161743, 417.7863883972168, 419.287766456604, 420.8750169277191, 422.2992866039276, 423.7456271648407, 425.213889837265, 426.6536226272583, 428.12892985343933, 429.55098724365234, 431.03393173217773, 432.4466049671173, 433.869740486145, 435.3553457260132, 437.7967104911804]
[6.683333333333334, 6.716666666666667, 6.825, 6.908333333333333, 7.275, 9.491666666666667, 8.958333333333334, 11.966666666666667, 13.958333333333334, 16.025, 12.183333333333334, 12.183333333333334, 8.283333333333333, 7.741666666666666, 7.266666666666667, 5.508333333333334, 5.508333333333334, 5.691666666666666, 5.691666666666666, 5.691666666666666, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.005, Test loss: 2.061, Test accuracy: 25.20
Round   0, Global train loss: 2.005, Global test loss: 2.169, Global test accuracy: 21.57
Round   1, Train loss: 1.743, Test loss: 1.905, Test accuracy: 31.61
Round   1, Global train loss: 1.743, Global test loss: 2.153, Global test accuracy: 24.29
Round   2, Train loss: 1.619, Test loss: 1.802, Test accuracy: 37.02
Round   2, Global train loss: 1.619, Global test loss: 2.242, Global test accuracy: 25.75
Round   3, Train loss: 1.520, Test loss: 1.711, Test accuracy: 39.63
Round   3, Global train loss: 1.520, Global test loss: 2.172, Global test accuracy: 27.05
Round   4, Train loss: 1.445, Test loss: 1.573, Test accuracy: 43.65
Round   4, Global train loss: 1.445, Global test loss: 2.078, Global test accuracy: 28.50
Round   5, Train loss: 1.365, Test loss: 1.499, Test accuracy: 46.72
Round   5, Global train loss: 1.365, Global test loss: 2.095, Global test accuracy: 28.68
Round   6, Train loss: 1.310, Test loss: 1.420, Test accuracy: 49.05
Round   6, Global train loss: 1.310, Global test loss: 2.131, Global test accuracy: 30.02
Round   7, Train loss: 1.252, Test loss: 1.339, Test accuracy: 52.00
Round   7, Global train loss: 1.252, Global test loss: 2.121, Global test accuracy: 29.66
Round   8, Train loss: 1.220, Test loss: 1.292, Test accuracy: 54.01
Round   8, Global train loss: 1.220, Global test loss: 2.068, Global test accuracy: 30.58
Round   9, Train loss: 1.168, Test loss: 1.260, Test accuracy: 55.34
Round   9, Global train loss: 1.168, Global test loss: 2.062, Global test accuracy: 29.87
Round  10, Train loss: 1.126, Test loss: 1.244, Test accuracy: 56.19
Round  10, Global train loss: 1.126, Global test loss: 2.105, Global test accuracy: 29.28
Round  11, Train loss: 1.120, Test loss: 1.218, Test accuracy: 57.25
Round  11, Global train loss: 1.120, Global test loss: 2.108, Global test accuracy: 30.69
Round  12, Train loss: 1.073, Test loss: 1.191, Test accuracy: 58.61
Round  12, Global train loss: 1.073, Global test loss: 2.155, Global test accuracy: 30.12
Round  13, Train loss: 1.029, Test loss: 1.174, Test accuracy: 59.23
Round  13, Global train loss: 1.029, Global test loss: 2.070, Global test accuracy: 31.46
Round  14, Train loss: 1.020, Test loss: 1.136, Test accuracy: 60.77
Round  14, Global train loss: 1.020, Global test loss: 2.085, Global test accuracy: 33.19
Round  15, Train loss: 0.996, Test loss: 1.113, Test accuracy: 61.94
Round  15, Global train loss: 0.996, Global test loss: 2.122, Global test accuracy: 30.25
Round  16, Train loss: 0.959, Test loss: 1.089, Test accuracy: 62.74
Round  16, Global train loss: 0.959, Global test loss: 2.122, Global test accuracy: 33.02
Round  17, Train loss: 0.938, Test loss: 1.088, Test accuracy: 62.84
Round  17, Global train loss: 0.938, Global test loss: 2.233, Global test accuracy: 30.92
Round  18, Train loss: 0.915, Test loss: 1.091, Test accuracy: 63.13
Round  18, Global train loss: 0.915, Global test loss: 2.302, Global test accuracy: 31.67
Round  19, Train loss: 0.903, Test loss: 1.069, Test accuracy: 63.70
Round  19, Global train loss: 0.903, Global test loss: 2.138, Global test accuracy: 30.97
Round  20, Train loss: 0.892, Test loss: 1.045, Test accuracy: 64.67
Round  20, Global train loss: 0.892, Global test loss: 2.231, Global test accuracy: 31.74
Round  21, Train loss: 0.866, Test loss: 1.036, Test accuracy: 65.01
Round  21, Global train loss: 0.866, Global test loss: 2.338, Global test accuracy: 31.68
Round  22, Train loss: 0.851, Test loss: 1.027, Test accuracy: 65.65
Round  22, Global train loss: 0.851, Global test loss: 2.111, Global test accuracy: 31.96
Round  23, Train loss: 0.849, Test loss: 1.028, Test accuracy: 65.82
Round  23, Global train loss: 0.849, Global test loss: 2.249, Global test accuracy: 33.52
Round  24, Train loss: 0.828, Test loss: 1.013, Test accuracy: 66.40
Round  24, Global train loss: 0.828, Global test loss: 2.159, Global test accuracy: 31.66
Round  25, Train loss: 0.803, Test loss: 1.008, Test accuracy: 66.81
Round  25, Global train loss: 0.803, Global test loss: 2.214, Global test accuracy: 32.01
Round  26, Train loss: 0.784, Test loss: 1.011, Test accuracy: 67.11
Round  26, Global train loss: 0.784, Global test loss: 2.302, Global test accuracy: 30.96
Round  27, Train loss: 0.769, Test loss: 1.004, Test accuracy: 67.12
Round  27, Global train loss: 0.769, Global test loss: 2.193, Global test accuracy: 31.37
Round  28, Train loss: 0.779, Test loss: 1.000, Test accuracy: 67.33
Round  28, Global train loss: 0.779, Global test loss: 2.204, Global test accuracy: 32.32
Round  29, Train loss: 0.763, Test loss: 0.992, Test accuracy: 67.82
Round  29, Global train loss: 0.763, Global test loss: 2.114, Global test accuracy: 32.10
Round  30, Train loss: 0.743, Test loss: 0.977, Test accuracy: 68.16
Round  30, Global train loss: 0.743, Global test loss: 2.274, Global test accuracy: 30.61
Round  31, Train loss: 0.728, Test loss: 0.983, Test accuracy: 68.32
Round  31, Global train loss: 0.728, Global test loss: 2.410, Global test accuracy: 32.23
Round  32, Train loss: 0.725, Test loss: 0.974, Test accuracy: 68.81
Round  32, Global train loss: 0.725, Global test loss: 2.411, Global test accuracy: 33.60
Round  33, Train loss: 0.715, Test loss: 0.973, Test accuracy: 68.95
Round  33, Global train loss: 0.715, Global test loss: 2.427, Global test accuracy: 34.87
Round  34, Train loss: 0.717, Test loss: 0.960, Test accuracy: 69.27
Round  34, Global train loss: 0.717, Global test loss: 2.269, Global test accuracy: 32.00
Round  35, Train loss: 0.678, Test loss: 0.969, Test accuracy: 69.14
Round  35, Global train loss: 0.678, Global test loss: 2.240, Global test accuracy: 34.04
Round  36, Train loss: 0.709, Test loss: 0.970, Test accuracy: 69.19
Round  36, Global train loss: 0.709, Global test loss: 2.219, Global test accuracy: 33.84
Round  37, Train loss: 0.687, Test loss: 0.969, Test accuracy: 69.20
Round  37, Global train loss: 0.687, Global test loss: 2.273, Global test accuracy: 32.06
Round  38, Train loss: 0.677, Test loss: 0.963, Test accuracy: 69.31
Round  38, Global train loss: 0.677, Global test loss: 2.309, Global test accuracy: 32.72
Round  39, Train loss: 0.670, Test loss: 0.967, Test accuracy: 69.22
Round  39, Global train loss: 0.670, Global test loss: 2.325, Global test accuracy: 33.73
Round  40, Train loss: 0.657, Test loss: 0.979, Test accuracy: 69.28
Round  40, Global train loss: 0.657, Global test loss: 2.155, Global test accuracy: 31.91
Round  41, Train loss: 0.669, Test loss: 0.974, Test accuracy: 69.44
Round  41, Global train loss: 0.669, Global test loss: 2.366, Global test accuracy: 31.84
Round  42, Train loss: 0.646, Test loss: 0.969, Test accuracy: 69.61
Round  42, Global train loss: 0.646, Global test loss: 2.250, Global test accuracy: 32.44
Round  43, Train loss: 0.648, Test loss: 0.963, Test accuracy: 70.01
Round  43, Global train loss: 0.648, Global test loss: 2.261, Global test accuracy: 30.92
Round  44, Train loss: 0.622, Test loss: 0.959, Test accuracy: 70.11
Round  44, Global train loss: 0.622, Global test loss: 2.256, Global test accuracy: 31.55
Round  45, Train loss: 0.607, Test loss: 0.950, Test accuracy: 70.50
Round  45, Global train loss: 0.607, Global test loss: 2.351, Global test accuracy: 32.39
Round  46, Train loss: 0.663, Test loss: 0.941, Test accuracy: 70.76
Round  46, Global train loss: 0.663, Global test loss: 2.359, Global test accuracy: 33.99
Round  47, Train loss: 0.621, Test loss: 0.950, Test accuracy: 70.65
Round  47, Global train loss: 0.621, Global test loss: 2.219, Global test accuracy: 31.61
Round  48, Train loss: 0.613, Test loss: 0.951, Test accuracy: 70.48
Round  48, Global train loss: 0.613, Global test loss: 2.292, Global test accuracy: 32.12
Round  49, Train loss: 0.595, Test loss: 0.949, Test accuracy: 70.65
Round  49, Global train loss: 0.595, Global test loss: 2.238, Global test accuracy: 31.17
Round  50, Train loss: 0.599, Test loss: 0.946, Test accuracy: 70.94
Round  50, Global train loss: 0.599, Global test loss: 2.375, Global test accuracy: 34.22
Round  51, Train loss: 0.628, Test loss: 0.946, Test accuracy: 71.19
Round  51, Global train loss: 0.628, Global test loss: 2.439, Global test accuracy: 31.98
Round  52, Train loss: 0.583, Test loss: 0.941, Test accuracy: 71.11
Round  52, Global train loss: 0.583, Global test loss: 2.299, Global test accuracy: 33.84
Round  53, Train loss: 0.603, Test loss: 0.946, Test accuracy: 71.26
Round  53, Global train loss: 0.603, Global test loss: 2.188, Global test accuracy: 34.47
Round  54, Train loss: 0.584, Test loss: 0.947, Test accuracy: 71.15
Round  54, Global train loss: 0.584, Global test loss: 2.439, Global test accuracy: 33.42
Round  55, Train loss: 0.555, Test loss: 0.948, Test accuracy: 71.33
Round  55, Global train loss: 0.555, Global test loss: 2.340, Global test accuracy: 31.41
Round  56, Train loss: 0.595, Test loss: 0.938, Test accuracy: 71.72
Round  56, Global train loss: 0.595, Global test loss: 2.517, Global test accuracy: 30.97
Round  57, Train loss: 0.562, Test loss: 0.932, Test accuracy: 71.99
Round  57, Global train loss: 0.562, Global test loss: 2.682, Global test accuracy: 34.45
Round  58, Train loss: 0.578, Test loss: 0.938, Test accuracy: 71.97
Round  58, Global train loss: 0.578, Global test loss: 2.519, Global test accuracy: 32.02
Round  59, Train loss: 0.552, Test loss: 0.956, Test accuracy: 71.68
Round  59, Global train loss: 0.552, Global test loss: 2.573, Global test accuracy: 35.48
Round  60, Train loss: 0.566, Test loss: 0.956, Test accuracy: 71.61
Round  60, Global train loss: 0.566, Global test loss: 2.447, Global test accuracy: 34.85
Round  61, Train loss: 0.570, Test loss: 0.942, Test accuracy: 71.81
Round  61, Global train loss: 0.570, Global test loss: 2.517, Global test accuracy: 36.13
Round  62, Train loss: 0.526, Test loss: 0.950, Test accuracy: 71.72
Round  62, Global train loss: 0.526, Global test loss: 2.300, Global test accuracy: 32.15
Round  63, Train loss: 0.519, Test loss: 0.963, Test accuracy: 71.42
Round  63, Global train loss: 0.519, Global test loss: 2.252, Global test accuracy: 33.01
Round  64, Train loss: 0.539, Test loss: 0.948, Test accuracy: 71.72
Round  64, Global train loss: 0.539, Global test loss: 2.516, Global test accuracy: 34.50
Round  65, Train loss: 0.506, Test loss: 0.948, Test accuracy: 71.96
Round  65, Global train loss: 0.506, Global test loss: 2.630, Global test accuracy: 30.97
Round  66, Train loss: 0.535, Test loss: 0.955, Test accuracy: 71.90
Round  66, Global train loss: 0.535, Global test loss: 2.291, Global test accuracy: 32.25
Round  67, Train loss: 0.515, Test loss: 0.944, Test accuracy: 72.17
Round  67, Global train loss: 0.515, Global test loss: 2.424, Global test accuracy: 33.04
Round  68, Train loss: 0.508, Test loss: 0.949, Test accuracy: 72.16
Round  68, Global train loss: 0.508, Global test loss: 2.495, Global test accuracy: 31.22
Round  69, Train loss: 0.552, Test loss: 0.951, Test accuracy: 72.25
Round  69, Global train loss: 0.552, Global test loss: 2.361, Global test accuracy: 32.19
Round  70, Train loss: 0.481, Test loss: 0.952, Test accuracy: 72.33
Round  70, Global train loss: 0.481, Global test loss: 2.396, Global test accuracy: 31.51
Round  71, Train loss: 0.548, Test loss: 0.950, Test accuracy: 72.55
Round  71, Global train loss: 0.548, Global test loss: 2.275, Global test accuracy: 34.56
Round  72, Train loss: 0.517, Test loss: 0.938, Test accuracy: 72.71
Round  72, Global train loss: 0.517, Global test loss: 2.406, Global test accuracy: 34.96
Round  73, Train loss: 0.507, Test loss: 0.951, Test accuracy: 72.47
Round  73, Global train loss: 0.507, Global test loss: 2.354, Global test accuracy: 33.17
Round  74, Train loss: 0.515, Test loss: 0.960, Test accuracy: 72.40
Round  74, Global train loss: 0.515, Global test loss: 2.464, Global test accuracy: 34.41
Round  75, Train loss: 0.497, Test loss: 0.961, Test accuracy: 72.31
Round  75, Global train loss: 0.497, Global test loss: 2.395, Global test accuracy: 33.78
Round  76, Train loss: 0.505, Test loss: 0.957, Test accuracy: 72.55
Round  76, Global train loss: 0.505, Global test loss: 2.325, Global test accuracy: 33.09
Round  77, Train loss: 0.549, Test loss: 0.968, Test accuracy: 72.73
Round  77, Global train loss: 0.549, Global test loss: 2.549, Global test accuracy: 32.05
Round  78, Train loss: 0.522, Test loss: 0.963, Test accuracy: 72.84
Round  78, Global train loss: 0.522, Global test loss: 2.562, Global test accuracy: 33.20
Round  79, Train loss: 0.531, Test loss: 0.964, Test accuracy: 72.84
Round  79, Global train loss: 0.531, Global test loss: 2.305, Global test accuracy: 35.53
Round  80, Train loss: 0.502, Test loss: 0.965, Test accuracy: 72.93
Round  80, Global train loss: 0.502, Global test loss: 2.496, Global test accuracy: 34.24
Round  81, Train loss: 0.521, Test loss: 0.986, Test accuracy: 72.54
Round  81, Global train loss: 0.521, Global test loss: 2.844, Global test accuracy: 35.46
Round  82, Train loss: 0.473, Test loss: 0.970, Test accuracy: 72.70
Round  82, Global train loss: 0.473, Global test loss: 2.366, Global test accuracy: 31.88
Round  83, Train loss: 0.490, Test loss: 0.967, Test accuracy: 72.75
Round  83, Global train loss: 0.490, Global test loss: 2.418, Global test accuracy: 33.69
Round  84, Train loss: 0.491, Test loss: 0.956, Test accuracy: 72.94
Round  84, Global train loss: 0.491, Global test loss: 2.501, Global test accuracy: 32.31
Round  85, Train loss: 0.498, Test loss: 0.957, Test accuracy: 73.05
Round  85, Global train loss: 0.498, Global test loss: 2.497, Global test accuracy: 33.70
Round  86, Train loss: 0.515, Test loss: 0.950, Test accuracy: 73.18
Round  86, Global train loss: 0.515, Global test loss: 2.501, Global test accuracy: 32.51
Round  87, Train loss: 0.481, Test loss: 0.952, Test accuracy: 73.18
Round  87, Global train loss: 0.481, Global test loss: 2.481, Global test accuracy: 35.41
Round  88, Train loss: 0.468, Test loss: 0.957, Test accuracy: 73.06
Round  88, Global train loss: 0.468, Global test loss: 2.442, Global test accuracy: 33.47
Round  89, Train loss: 0.496, Test loss: 0.954, Test accuracy: 73.28
Round  89, Global train loss: 0.496, Global test loss: 2.664, Global test accuracy: 32.64
Round  90, Train loss: 0.466, Test loss: 0.956, Test accuracy: 73.25
Round  90, Global train loss: 0.466, Global test loss: 2.624, Global test accuracy: 32.57
Round  91, Train loss: 0.454, Test loss: 0.968, Test accuracy: 73.29
Round  91, Global train loss: 0.454, Global test loss: 2.732, Global test accuracy: 32.89
Round  92, Train loss: 0.486, Test loss: 0.963, Test accuracy: 73.46
Round  92, Global train loss: 0.486, Global test loss: 2.552, Global test accuracy: 35.16
Round  93, Train loss: 0.460, Test loss: 0.964, Test accuracy: 73.58
Round  93, Global train loss: 0.460, Global test loss: 2.358, Global test accuracy: 30.35
Round  94, Train loss: 0.465, Test loss: 0.970, Test accuracy: 73.67
Round  94, Global train loss: 0.465, Global test loss: 2.472, Global test accuracy: 32.45
Round  95, Train loss: 0.440, Test loss: 0.968, Test accuracy: 73.67
Round  95, Global train loss: 0.440, Global test loss: 2.496, Global test accuracy: 31.07
Round  96, Train loss: 0.480, Test loss: 0.958, Test accuracy: 73.82
Round  96, Global train loss: 0.480, Global test loss: 2.438, Global test accuracy: 32.70
Round  97, Train loss: 0.439, Test loss: 0.953, Test accuracy: 73.89
Round  97, Global train loss: 0.439, Global test loss: 2.593, Global test accuracy: 32.71
Round  98, Train loss: 0.471, Test loss: 0.949, Test accuracy: 73.89
Round  98, Global train loss: 0.471, Global test loss: 2.507, Global test accuracy: 35.19
Round  99, Train loss: 0.437, Test loss: 0.951, Test accuracy: 73.95
Round  99, Global train loss: 0.437, Global test loss: 2.634, Global test accuracy: 34.32
Final Round, Train loss: 0.315, Test loss: 1.105, Test accuracy: 73.54
Final Round, Global train loss: 0.315, Global test loss: 2.634, Global test accuracy: 34.32
Average accuracy final 10 rounds: 73.648 

Average global accuracy final 10 rounds: 32.940749999999994 

6217.350820064545
[4.938530683517456, 9.877061367034912, 14.727161884307861, 19.57726240158081, 24.576136350631714, 29.575010299682617, 34.20775890350342, 38.84050750732422, 43.189512968063354, 47.53851842880249, 52.037840127944946, 56.5371618270874, 60.985527992248535, 65.43389415740967, 69.97445678710938, 74.51501941680908, 79.09783005714417, 83.68064069747925, 88.37305164337158, 93.06546258926392, 98.13517594337463, 103.20488929748535, 108.02882814407349, 112.85276699066162, 117.50473308563232, 122.15669918060303, 126.74894094467163, 131.34118270874023, 135.893079996109, 140.44497728347778, 145.02947449684143, 149.61397171020508, 154.12847590446472, 158.64298009872437, 163.19675469398499, 167.7505292892456, 172.65282821655273, 177.55512714385986, 182.6524360179901, 187.74974489212036, 193.0228614807129, 198.29597806930542, 203.35822582244873, 208.42047357559204, 213.35606741905212, 218.2916612625122, 222.96889209747314, 227.64612293243408, 232.23902034759521, 236.83191776275635, 241.51132464408875, 246.19073152542114, 250.91730737686157, 255.643883228302, 260.4628098011017, 265.28173637390137, 270.0670325756073, 274.85232877731323, 279.48266553878784, 284.11300230026245, 288.5369973182678, 292.9609923362732, 297.43266677856445, 301.9043412208557, 306.42096757888794, 310.93759393692017, 315.45707273483276, 319.97655153274536, 324.5179145336151, 329.05927753448486, 333.4977252483368, 337.9361729621887, 342.4517710208893, 346.96736907958984, 351.5021390914917, 356.03690910339355, 360.56770968437195, 365.09851026535034, 369.57688069343567, 374.055251121521, 378.6603493690491, 383.26544761657715, 387.831515789032, 392.3975839614868, 396.9552776813507, 401.5129714012146, 405.96887469291687, 410.42477798461914, 415.008451461792, 419.59212493896484, 424.0592534542084, 428.5263819694519, 433.03778290748596, 437.54918384552, 442.0767261981964, 446.6042685508728, 451.12874937057495, 455.6532301902771, 460.2682054042816, 464.88318061828613, 469.308167219162, 473.73315382003784, 478.18422293663025, 482.63529205322266, 487.15647172927856, 491.6776514053345, 496.11352252960205, 500.54939365386963, 505.1011288166046, 509.6528639793396, 514.2020008563995, 518.7511377334595, 523.2099578380585, 527.6687779426575, 532.1565372943878, 536.6442966461182, 541.1363070011139, 545.6283173561096, 549.9952363967896, 554.3621554374695, 558.714848279953, 563.0675411224365, 567.7375092506409, 572.4074773788452, 577.0728306770325, 581.7381839752197, 586.2001411914825, 590.6620984077454, 595.6507427692413, 600.6393871307373, 605.6921679973602, 610.7449488639832, 615.7892520427704, 620.8335552215576, 625.9193716049194, 631.0051879882812, 635.8950326442719, 640.7848773002625, 645.3388447761536, 649.8928122520447, 654.3098502159119, 658.726888179779, 663.0843870639801, 667.4418859481812, 671.8608777523041, 676.279869556427, 680.7738659381866, 685.2678623199463, 689.6935529708862, 694.1192436218262, 698.5304791927338, 702.9417147636414, 707.3683884143829, 711.7950620651245, 716.2117314338684, 720.6284008026123, 725.0030000209808, 729.3775992393494, 733.9050238132477, 738.432448387146, 742.983601808548, 747.53475522995, 752.5088059902191, 757.4828567504883, 762.2310302257538, 766.9792037010193, 771.5424604415894, 776.1057171821594, 780.6298744678497, 785.15403175354, 789.6579971313477, 794.1619625091553, 798.5070278644562, 802.8520932197571, 807.2853286266327, 811.7185640335083, 816.142924785614, 820.5672855377197, 825.0191333293915, 829.4709811210632, 833.9133405685425, 838.3557000160217, 842.7668535709381, 847.1780071258545, 851.6711857318878, 856.1643643379211, 861.2256288528442, 866.2868933677673, 870.7290754318237, 875.1712574958801, 879.6497802734375, 884.1283030509949, 888.5205631256104, 892.9128232002258, 897.6927537918091, 902.4726843833923, 907.6931538581848, 912.9136233329773, 918.1141965389252, 923.314769744873, 925.9983365535736, 928.6819033622742]
[25.2, 25.2, 31.615, 31.615, 37.02, 37.02, 39.6325, 39.6325, 43.6525, 43.6525, 46.715, 46.715, 49.0525, 49.0525, 52.0025, 52.0025, 54.01, 54.01, 55.345, 55.345, 56.1875, 56.1875, 57.2525, 57.2525, 58.61, 58.61, 59.225, 59.225, 60.77, 60.77, 61.935, 61.935, 62.74, 62.74, 62.84, 62.84, 63.13, 63.13, 63.705, 63.705, 64.6675, 64.6675, 65.01, 65.01, 65.6525, 65.6525, 65.8225, 65.8225, 66.3975, 66.3975, 66.805, 66.805, 67.11, 67.11, 67.1175, 67.1175, 67.325, 67.325, 67.82, 67.82, 68.155, 68.155, 68.32, 68.32, 68.8125, 68.8125, 68.95, 68.95, 69.265, 69.265, 69.135, 69.135, 69.185, 69.185, 69.1975, 69.1975, 69.31, 69.31, 69.225, 69.225, 69.28, 69.28, 69.44, 69.44, 69.6075, 69.6075, 70.0075, 70.0075, 70.11, 70.11, 70.5025, 70.5025, 70.7625, 70.7625, 70.65, 70.65, 70.4825, 70.4825, 70.6525, 70.6525, 70.94, 70.94, 71.195, 71.195, 71.11, 71.11, 71.2625, 71.2625, 71.1475, 71.1475, 71.335, 71.335, 71.72, 71.72, 71.9925, 71.9925, 71.97, 71.97, 71.68, 71.68, 71.605, 71.605, 71.805, 71.805, 71.725, 71.725, 71.425, 71.425, 71.715, 71.715, 71.9575, 71.9575, 71.8975, 71.8975, 72.175, 72.175, 72.16, 72.16, 72.2525, 72.2525, 72.33, 72.33, 72.5525, 72.5525, 72.7125, 72.7125, 72.465, 72.465, 72.4, 72.4, 72.305, 72.305, 72.55, 72.55, 72.7325, 72.7325, 72.84, 72.84, 72.8375, 72.8375, 72.9325, 72.9325, 72.5375, 72.5375, 72.7025, 72.7025, 72.755, 72.755, 72.945, 72.945, 73.0525, 73.0525, 73.1825, 73.1825, 73.1775, 73.1775, 73.065, 73.065, 73.285, 73.285, 73.255, 73.255, 73.2875, 73.2875, 73.4625, 73.4625, 73.58, 73.58, 73.675, 73.675, 73.665, 73.665, 73.82, 73.82, 73.895, 73.895, 73.8875, 73.8875, 73.9525, 73.9525, 73.54, 73.54]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.417, Test loss: 2.294, Test accuracy: 20.39
Round   1, Train loss: 0.983, Test loss: 1.713, Test accuracy: 36.23
Round   2, Train loss: 0.823, Test loss: 1.567, Test accuracy: 45.61
Round   3, Train loss: 0.824, Test loss: 1.071, Test accuracy: 55.73
Round   4, Train loss: 0.800, Test loss: 0.915, Test accuracy: 62.33
Round   5, Train loss: 0.717, Test loss: 0.748, Test accuracy: 66.12
Round   6, Train loss: 0.642, Test loss: 0.711, Test accuracy: 68.58
Round   7, Train loss: 0.708, Test loss: 0.690, Test accuracy: 69.03
Round   8, Train loss: 0.638, Test loss: 0.681, Test accuracy: 69.66
Round   9, Train loss: 0.640, Test loss: 0.625, Test accuracy: 71.57
Round  10, Train loss: 0.639, Test loss: 0.614, Test accuracy: 72.73
Round  11, Train loss: 0.602, Test loss: 0.599, Test accuracy: 73.42
Round  12, Train loss: 0.553, Test loss: 0.591, Test accuracy: 74.07
Round  13, Train loss: 0.621, Test loss: 0.585, Test accuracy: 74.14
Round  14, Train loss: 0.560, Test loss: 0.543, Test accuracy: 76.52
Round  15, Train loss: 0.574, Test loss: 0.546, Test accuracy: 76.75
Round  16, Train loss: 0.603, Test loss: 0.529, Test accuracy: 77.18
Round  17, Train loss: 0.526, Test loss: 0.536, Test accuracy: 77.31
Round  18, Train loss: 0.534, Test loss: 0.528, Test accuracy: 77.18
Round  19, Train loss: 0.543, Test loss: 0.504, Test accuracy: 78.72
Round  20, Train loss: 0.543, Test loss: 0.508, Test accuracy: 78.47
Round  21, Train loss: 0.527, Test loss: 0.490, Test accuracy: 79.50
Round  22, Train loss: 0.566, Test loss: 0.486, Test accuracy: 79.57
Round  23, Train loss: 0.457, Test loss: 0.475, Test accuracy: 80.14
Round  24, Train loss: 0.510, Test loss: 0.469, Test accuracy: 80.67
Round  25, Train loss: 0.442, Test loss: 0.463, Test accuracy: 80.84
Round  26, Train loss: 0.490, Test loss: 0.478, Test accuracy: 79.88
Round  27, Train loss: 0.440, Test loss: 0.460, Test accuracy: 80.70
Round  28, Train loss: 0.492, Test loss: 0.452, Test accuracy: 80.90
Round  29, Train loss: 0.470, Test loss: 0.452, Test accuracy: 81.03
Round  30, Train loss: 0.435, Test loss: 0.437, Test accuracy: 81.53
Round  31, Train loss: 0.402, Test loss: 0.437, Test accuracy: 81.89
Round  32, Train loss: 0.423, Test loss: 0.433, Test accuracy: 81.92
Round  33, Train loss: 0.414, Test loss: 0.427, Test accuracy: 82.33
Round  34, Train loss: 0.379, Test loss: 0.423, Test accuracy: 82.36
Round  35, Train loss: 0.466, Test loss: 0.416, Test accuracy: 82.99
Round  36, Train loss: 0.413, Test loss: 0.420, Test accuracy: 82.67
Round  37, Train loss: 0.462, Test loss: 0.413, Test accuracy: 82.73
Round  38, Train loss: 0.468, Test loss: 0.411, Test accuracy: 82.64
Round  39, Train loss: 0.412, Test loss: 0.408, Test accuracy: 82.88
Round  40, Train loss: 0.326, Test loss: 0.413, Test accuracy: 82.53
Round  41, Train loss: 0.331, Test loss: 0.415, Test accuracy: 82.57
Round  42, Train loss: 0.423, Test loss: 0.398, Test accuracy: 83.41
Round  43, Train loss: 0.408, Test loss: 0.395, Test accuracy: 83.90
Round  44, Train loss: 0.407, Test loss: 0.408, Test accuracy: 83.22
Round  45, Train loss: 0.450, Test loss: 0.400, Test accuracy: 83.54
Round  46, Train loss: 0.420, Test loss: 0.404, Test accuracy: 83.32
Round  47, Train loss: 0.414, Test loss: 0.391, Test accuracy: 83.81
Round  48, Train loss: 0.348, Test loss: 0.387, Test accuracy: 84.17
Round  49, Train loss: 0.401, Test loss: 0.391, Test accuracy: 83.80
Round  50, Train loss: 0.366, Test loss: 0.378, Test accuracy: 84.58
Round  51, Train loss: 0.375, Test loss: 0.382, Test accuracy: 84.60
Round  52, Train loss: 0.299, Test loss: 0.374, Test accuracy: 84.83
Round  53, Train loss: 0.339, Test loss: 0.376, Test accuracy: 84.83
Round  54, Train loss: 0.325, Test loss: 0.380, Test accuracy: 84.47
Round  55, Train loss: 0.376, Test loss: 0.374, Test accuracy: 84.83
Round  56, Train loss: 0.352, Test loss: 0.378, Test accuracy: 84.66
Round  57, Train loss: 0.329, Test loss: 0.368, Test accuracy: 85.16
Round  58, Train loss: 0.327, Test loss: 0.363, Test accuracy: 85.65
Round  59, Train loss: 0.361, Test loss: 0.362, Test accuracy: 85.42
Round  60, Train loss: 0.329, Test loss: 0.374, Test accuracy: 84.86
Round  61, Train loss: 0.322, Test loss: 0.366, Test accuracy: 85.09
Round  62, Train loss: 0.325, Test loss: 0.367, Test accuracy: 85.37
Round  63, Train loss: 0.344, Test loss: 0.361, Test accuracy: 85.59
Round  64, Train loss: 0.346, Test loss: 0.356, Test accuracy: 85.83
Round  65, Train loss: 0.276, Test loss: 0.369, Test accuracy: 85.20
Round  66, Train loss: 0.283, Test loss: 0.370, Test accuracy: 85.08
Round  67, Train loss: 0.278, Test loss: 0.369, Test accuracy: 85.22
Round  68, Train loss: 0.323, Test loss: 0.365, Test accuracy: 85.43
Round  69, Train loss: 0.273, Test loss: 0.361, Test accuracy: 85.64
Round  70, Train loss: 0.293, Test loss: 0.365, Test accuracy: 85.31
Round  71, Train loss: 0.270, Test loss: 0.363, Test accuracy: 85.49
Round  72, Train loss: 0.264, Test loss: 0.363, Test accuracy: 85.50
Round  73, Train loss: 0.312, Test loss: 0.370, Test accuracy: 85.63
Round  74, Train loss: 0.290, Test loss: 0.359, Test accuracy: 85.73
Round  75, Train loss: 0.359, Test loss: 0.364, Test accuracy: 85.85
Round  76, Train loss: 0.241, Test loss: 0.365, Test accuracy: 85.71
Round  77, Train loss: 0.290, Test loss: 0.364, Test accuracy: 85.64
Round  78, Train loss: 0.319, Test loss: 0.355, Test accuracy: 86.17
Round  79, Train loss: 0.296, Test loss: 0.354, Test accuracy: 86.17
Round  80, Train loss: 0.304, Test loss: 0.352, Test accuracy: 86.03
Round  81, Train loss: 0.279, Test loss: 0.361, Test accuracy: 85.98
Round  82, Train loss: 0.276, Test loss: 0.361, Test accuracy: 85.77
Round  83, Train loss: 0.251, Test loss: 0.362, Test accuracy: 85.87
Round  84, Train loss: 0.292, Test loss: 0.368, Test accuracy: 85.90
Round  85, Train loss: 0.216, Test loss: 0.363, Test accuracy: 85.89
Round  86, Train loss: 0.254, Test loss: 0.369, Test accuracy: 85.68
Round  87, Train loss: 0.239, Test loss: 0.367, Test accuracy: 86.27
Round  88, Train loss: 0.247, Test loss: 0.356, Test accuracy: 86.46
Round  89, Train loss: 0.231, Test loss: 0.352, Test accuracy: 86.27
Round  90, Train loss: 0.291, Test loss: 0.361, Test accuracy: 85.88
Round  91, Train loss: 0.274, Test loss: 0.365, Test accuracy: 85.96
Round  92, Train loss: 0.197, Test loss: 0.365, Test accuracy: 85.90
Round  93, Train loss: 0.257, Test loss: 0.355, Test accuracy: 86.22
Round  94, Train loss: 0.239, Test loss: 0.354, Test accuracy: 86.47
Round  95, Train loss: 0.250, Test loss: 0.357, Test accuracy: 86.34
Round  96, Train loss: 0.242, Test loss: 0.355, Test accuracy: 86.68
Round  97, Train loss: 0.232, Test loss: 0.364, Test accuracy: 86.24
Round  98, Train loss: 0.183, Test loss: 0.377, Test accuracy: 85.96
Round  99, Train loss: 0.240, Test loss: 0.352, Test accuracy: 86.63
Final Round, Train loss: 0.206, Test loss: 0.351, Test accuracy: 87.03
Average accuracy final 10 rounds: 86.22833333333332 

1557.9109842777252
[1.7819883823394775, 3.563976764678955, 4.915069580078125, 6.266162395477295, 7.663861513137817, 9.06156063079834, 10.53031873703003, 11.999076843261719, 13.40503978729248, 14.811002731323242, 16.18296217918396, 17.554921627044678, 18.915690898895264, 20.27646017074585, 21.670252323150635, 23.06404447555542, 24.485474586486816, 25.906904697418213, 27.370835781097412, 28.83476686477661, 30.288743257522583, 31.742719650268555, 33.1393826007843, 34.53604555130005, 35.885977029800415, 37.23590850830078, 38.65093159675598, 40.06595468521118, 41.43246817588806, 42.79898166656494, 44.18910884857178, 45.57923603057861, 46.90962219238281, 48.24000835418701, 49.6400785446167, 51.04014873504639, 52.44002890586853, 53.839909076690674, 55.151711225509644, 56.46351337432861, 57.81324601173401, 59.162978649139404, 60.5119514465332, 61.860924243927, 63.22633504867554, 64.59174585342407, 65.9922502040863, 67.39275455474854, 68.7321207523346, 70.07148694992065, 71.35996174812317, 72.64843654632568, 74.06127977371216, 75.47412300109863, 76.80406785011292, 78.1340126991272, 79.51296877861023, 80.89192485809326, 82.2764744758606, 83.66102409362793, 85.017019033432, 86.37301397323608, 87.74860095977783, 89.12418794631958, 90.49761962890625, 91.87105131149292, 93.25843048095703, 94.64580965042114, 96.01671361923218, 97.38761758804321, 98.72128415107727, 100.05495071411133, 101.38559365272522, 102.71623659133911, 103.95808720588684, 105.19993782043457, 106.48498845100403, 107.77003908157349, 109.04431176185608, 110.31858444213867, 111.68843293190002, 113.05828142166138, 114.4312334060669, 115.80418539047241, 117.17692995071411, 118.54967451095581, 119.95390319824219, 121.35813188552856, 122.7130012512207, 124.06787061691284, 125.39530539512634, 126.72274017333984, 128.12786078453064, 129.53298139572144, 130.89203763008118, 132.25109386444092, 133.6660656929016, 135.0810375213623, 136.41878247261047, 137.75652742385864, 139.09494590759277, 140.4333643913269, 141.82545638084412, 143.21754837036133, 144.51016330718994, 145.80277824401855, 147.10321307182312, 148.40364789962769, 149.73284006118774, 151.0620322227478, 152.31919956207275, 153.5763669013977, 154.95329546928406, 156.3302240371704, 157.64878797531128, 158.96735191345215, 160.23911499977112, 161.5108780860901, 162.88088250160217, 164.25088691711426, 165.62311697006226, 166.99534702301025, 168.4050416946411, 169.81473636627197, 171.14730381965637, 172.47987127304077, 173.8407576084137, 175.20164394378662, 176.5409698486328, 177.880295753479, 179.2465763092041, 180.6128568649292, 181.8577380180359, 183.10261917114258, 184.41302108764648, 185.7234230041504, 187.0477433204651, 188.37206363677979, 189.61610293388367, 190.86014223098755, 192.15073204040527, 193.441321849823, 194.75631403923035, 196.0713062286377, 197.36383533477783, 198.65636444091797, 199.92618560791016, 201.19600677490234, 202.53002166748047, 203.8640365600586, 205.19158172607422, 206.51912689208984, 207.8106849193573, 209.10224294662476, 210.4217438697815, 211.74124479293823, 213.01101088523865, 214.28077697753906, 215.5538182258606, 216.82685947418213, 218.11862468719482, 219.41038990020752, 220.68594455718994, 221.96149921417236, 223.2529375553131, 224.54437589645386, 225.8280792236328, 227.11178255081177, 228.3737862110138, 229.63578987121582, 230.92221879959106, 232.2086477279663, 233.5026433467865, 234.7966389656067, 236.08294892311096, 237.36925888061523, 238.79325938224792, 240.21725988388062, 241.6022367477417, 242.98721361160278, 244.36563777923584, 245.7440619468689, 247.16953015327454, 248.59499835968018, 249.88626265525818, 251.17752695083618, 252.4651277065277, 253.75272846221924, 255.1131772994995, 256.4736261367798, 257.77502179145813, 259.0764174461365, 260.3982288837433, 261.7200403213501, 262.9877462387085, 264.2554521560669, 265.56201934814453, 266.86858654022217, 268.1260914802551, 269.3835964202881, 271.4494671821594, 273.51533794403076]
[20.391666666666666, 20.391666666666666, 36.225, 36.225, 45.608333333333334, 45.608333333333334, 55.725, 55.725, 62.333333333333336, 62.333333333333336, 66.125, 66.125, 68.575, 68.575, 69.03333333333333, 69.03333333333333, 69.65833333333333, 69.65833333333333, 71.56666666666666, 71.56666666666666, 72.73333333333333, 72.73333333333333, 73.41666666666667, 73.41666666666667, 74.06666666666666, 74.06666666666666, 74.14166666666667, 74.14166666666667, 76.51666666666667, 76.51666666666667, 76.75, 76.75, 77.18333333333334, 77.18333333333334, 77.30833333333334, 77.30833333333334, 77.18333333333334, 77.18333333333334, 78.725, 78.725, 78.46666666666667, 78.46666666666667, 79.5, 79.5, 79.56666666666666, 79.56666666666666, 80.14166666666667, 80.14166666666667, 80.66666666666667, 80.66666666666667, 80.84166666666667, 80.84166666666667, 79.88333333333334, 79.88333333333334, 80.7, 80.7, 80.9, 80.9, 81.025, 81.025, 81.53333333333333, 81.53333333333333, 81.89166666666667, 81.89166666666667, 81.91666666666667, 81.91666666666667, 82.325, 82.325, 82.35833333333333, 82.35833333333333, 82.99166666666666, 82.99166666666666, 82.675, 82.675, 82.73333333333333, 82.73333333333333, 82.64166666666667, 82.64166666666667, 82.88333333333334, 82.88333333333334, 82.525, 82.525, 82.56666666666666, 82.56666666666666, 83.40833333333333, 83.40833333333333, 83.9, 83.9, 83.21666666666667, 83.21666666666667, 83.54166666666667, 83.54166666666667, 83.31666666666666, 83.31666666666666, 83.80833333333334, 83.80833333333334, 84.175, 84.175, 83.8, 83.8, 84.575, 84.575, 84.6, 84.6, 84.83333333333333, 84.83333333333333, 84.825, 84.825, 84.475, 84.475, 84.83333333333333, 84.83333333333333, 84.65833333333333, 84.65833333333333, 85.15833333333333, 85.15833333333333, 85.65, 85.65, 85.41666666666667, 85.41666666666667, 84.85833333333333, 84.85833333333333, 85.09166666666667, 85.09166666666667, 85.36666666666666, 85.36666666666666, 85.59166666666667, 85.59166666666667, 85.83333333333333, 85.83333333333333, 85.2, 85.2, 85.075, 85.075, 85.225, 85.225, 85.43333333333334, 85.43333333333334, 85.64166666666667, 85.64166666666667, 85.30833333333334, 85.30833333333334, 85.49166666666666, 85.49166666666666, 85.5, 85.5, 85.63333333333334, 85.63333333333334, 85.73333333333333, 85.73333333333333, 85.85, 85.85, 85.70833333333333, 85.70833333333333, 85.64166666666667, 85.64166666666667, 86.175, 86.175, 86.16666666666667, 86.16666666666667, 86.03333333333333, 86.03333333333333, 85.98333333333333, 85.98333333333333, 85.76666666666667, 85.76666666666667, 85.86666666666666, 85.86666666666666, 85.9, 85.9, 85.89166666666667, 85.89166666666667, 85.68333333333334, 85.68333333333334, 86.26666666666667, 86.26666666666667, 86.45833333333333, 86.45833333333333, 86.26666666666667, 86.26666666666667, 85.875, 85.875, 85.95833333333333, 85.95833333333333, 85.9, 85.9, 86.21666666666667, 86.21666666666667, 86.475, 86.475, 86.34166666666667, 86.34166666666667, 86.68333333333334, 86.68333333333334, 86.24166666666666, 86.24166666666666, 85.95833333333333, 85.95833333333333, 86.63333333333334, 86.63333333333334, 87.03333333333333, 87.03333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.511, Test loss: 2.341, Test accuracy: 14.69
Round   1, Train loss: 1.011, Test loss: 2.033, Test accuracy: 30.77
Round   2, Train loss: 0.904, Test loss: 1.513, Test accuracy: 38.51
Round   3, Train loss: 0.812, Test loss: 1.342, Test accuracy: 46.91
Round   4, Train loss: 0.780, Test loss: 1.075, Test accuracy: 55.55
Round   5, Train loss: 0.823, Test loss: 0.880, Test accuracy: 63.99
Round   6, Train loss: 0.693, Test loss: 0.825, Test accuracy: 65.85
Round   7, Train loss: 0.714, Test loss: 0.700, Test accuracy: 70.39
Round   8, Train loss: 0.630, Test loss: 0.676, Test accuracy: 72.44
Round   9, Train loss: 0.696, Test loss: 0.643, Test accuracy: 73.09
Round  10, Train loss: 0.567, Test loss: 0.634, Test accuracy: 74.09
Round  11, Train loss: 0.599, Test loss: 0.606, Test accuracy: 75.85
Round  12, Train loss: 0.620, Test loss: 0.577, Test accuracy: 77.42
Round  13, Train loss: 0.570, Test loss: 0.578, Test accuracy: 77.59
Round  14, Train loss: 0.598, Test loss: 0.563, Test accuracy: 77.86
Round  15, Train loss: 0.508, Test loss: 0.549, Test accuracy: 78.49
Round  16, Train loss: 0.538, Test loss: 0.538, Test accuracy: 79.31
Round  17, Train loss: 0.513, Test loss: 0.526, Test accuracy: 79.17
Round  18, Train loss: 0.517, Test loss: 0.525, Test accuracy: 79.78
Round  19, Train loss: 0.561, Test loss: 0.525, Test accuracy: 79.59
Round  20, Train loss: 0.529, Test loss: 0.521, Test accuracy: 79.59
Round  21, Train loss: 0.450, Test loss: 0.479, Test accuracy: 80.90
Round  22, Train loss: 0.531, Test loss: 0.478, Test accuracy: 80.72
Round  23, Train loss: 0.425, Test loss: 0.473, Test accuracy: 80.52
Round  24, Train loss: 0.508, Test loss: 0.467, Test accuracy: 81.12
Round  25, Train loss: 0.514, Test loss: 0.464, Test accuracy: 81.20
Round  26, Train loss: 0.493, Test loss: 0.453, Test accuracy: 81.55
Round  27, Train loss: 0.464, Test loss: 0.446, Test accuracy: 82.37
Round  28, Train loss: 0.477, Test loss: 0.440, Test accuracy: 82.14
Round  29, Train loss: 0.516, Test loss: 0.454, Test accuracy: 82.03
Round  30, Train loss: 0.481, Test loss: 0.435, Test accuracy: 82.42
Round  31, Train loss: 0.413, Test loss: 0.433, Test accuracy: 82.06
Round  32, Train loss: 0.457, Test loss: 0.429, Test accuracy: 82.08
Round  33, Train loss: 0.417, Test loss: 0.421, Test accuracy: 82.82
Round  34, Train loss: 0.405, Test loss: 0.414, Test accuracy: 83.00
Round  35, Train loss: 0.460, Test loss: 0.418, Test accuracy: 83.28
Round  36, Train loss: 0.453, Test loss: 0.406, Test accuracy: 83.60
Round  37, Train loss: 0.369, Test loss: 0.401, Test accuracy: 83.62
Round  38, Train loss: 0.382, Test loss: 0.405, Test accuracy: 83.71
Round  39, Train loss: 0.463, Test loss: 0.402, Test accuracy: 83.57
Round  40, Train loss: 0.371, Test loss: 0.396, Test accuracy: 83.89
Round  41, Train loss: 0.341, Test loss: 0.402, Test accuracy: 83.58
Round  42, Train loss: 0.359, Test loss: 0.401, Test accuracy: 83.78
Round  43, Train loss: 0.345, Test loss: 0.407, Test accuracy: 84.00
Round  44, Train loss: 0.381, Test loss: 0.391, Test accuracy: 84.31
Round  45, Train loss: 0.286, Test loss: 0.386, Test accuracy: 84.12
Round  46, Train loss: 0.407, Test loss: 0.386, Test accuracy: 84.60
Round  47, Train loss: 0.353, Test loss: 0.378, Test accuracy: 84.97
Round  48, Train loss: 0.373, Test loss: 0.377, Test accuracy: 84.57
Round  49, Train loss: 0.379, Test loss: 0.376, Test accuracy: 84.78
Round  50, Train loss: 0.381, Test loss: 0.372, Test accuracy: 85.31
Round  51, Train loss: 0.380, Test loss: 0.367, Test accuracy: 85.38
Round  52, Train loss: 0.333, Test loss: 0.367, Test accuracy: 85.04
Round  53, Train loss: 0.404, Test loss: 0.368, Test accuracy: 85.28
Round  54, Train loss: 0.396, Test loss: 0.365, Test accuracy: 85.53
Round  55, Train loss: 0.354, Test loss: 0.368, Test accuracy: 85.23
Round  56, Train loss: 0.370, Test loss: 0.367, Test accuracy: 85.13
Round  57, Train loss: 0.385, Test loss: 0.368, Test accuracy: 85.45
Round  58, Train loss: 0.363, Test loss: 0.361, Test accuracy: 85.61
Round  59, Train loss: 0.359, Test loss: 0.361, Test accuracy: 85.62
Round  60, Train loss: 0.336, Test loss: 0.365, Test accuracy: 85.64
Round  61, Train loss: 0.338, Test loss: 0.353, Test accuracy: 85.84
Round  62, Train loss: 0.275, Test loss: 0.346, Test accuracy: 86.05
Round  63, Train loss: 0.315, Test loss: 0.350, Test accuracy: 85.87
Round  64, Train loss: 0.281, Test loss: 0.350, Test accuracy: 86.25
Round  65, Train loss: 0.244, Test loss: 0.352, Test accuracy: 85.70
Round  66, Train loss: 0.306, Test loss: 0.348, Test accuracy: 85.80
Round  67, Train loss: 0.303, Test loss: 0.351, Test accuracy: 86.05
Round  68, Train loss: 0.299, Test loss: 0.349, Test accuracy: 86.34
Round  69, Train loss: 0.270, Test loss: 0.351, Test accuracy: 86.12
Round  70, Train loss: 0.329, Test loss: 0.345, Test accuracy: 86.28
Round  71, Train loss: 0.302, Test loss: 0.345, Test accuracy: 86.17
Round  72, Train loss: 0.328, Test loss: 0.346, Test accuracy: 86.40
Round  73, Train loss: 0.248, Test loss: 0.349, Test accuracy: 86.16
Round  74, Train loss: 0.288, Test loss: 0.346, Test accuracy: 86.33
Round  75, Train loss: 0.307, Test loss: 0.336, Test accuracy: 86.71
Round  76, Train loss: 0.295, Test loss: 0.336, Test accuracy: 86.63
Round  77, Train loss: 0.311, Test loss: 0.332, Test accuracy: 86.72
Round  78, Train loss: 0.275, Test loss: 0.334, Test accuracy: 87.01
Round  79, Train loss: 0.279, Test loss: 0.335, Test accuracy: 86.68
Round  80, Train loss: 0.291, Test loss: 0.336, Test accuracy: 86.82
Round  81, Train loss: 0.308, Test loss: 0.336, Test accuracy: 86.57
Round  82, Train loss: 0.287, Test loss: 0.335, Test accuracy: 86.61
Round  83, Train loss: 0.300, Test loss: 0.334, Test accuracy: 86.73
Round  84, Train loss: 0.270, Test loss: 0.328, Test accuracy: 86.85
Round  85, Train loss: 0.262, Test loss: 0.330, Test accuracy: 86.94
Round  86, Train loss: 0.216, Test loss: 0.328, Test accuracy: 86.84
Round  87, Train loss: 0.302, Test loss: 0.324, Test accuracy: 87.16
Round  88, Train loss: 0.244, Test loss: 0.327, Test accuracy: 87.25
Round  89, Train loss: 0.311, Test loss: 0.330, Test accuracy: 86.83
Round  90, Train loss: 0.287, Test loss: 0.326, Test accuracy: 87.10
Round  91, Train loss: 0.227, Test loss: 0.324, Test accuracy: 87.16
Round  92, Train loss: 0.270, Test loss: 0.325, Test accuracy: 87.02
Round  93, Train loss: 0.270, Test loss: 0.318, Test accuracy: 87.32
Round  94, Train loss: 0.195, Test loss: 0.325, Test accuracy: 87.28
Round  95, Train loss: 0.247, Test loss: 0.325, Test accuracy: 87.08
Round  96, Train loss: 0.240, Test loss: 0.320, Test accuracy: 87.36
Round  97, Train loss: 0.237, Test loss: 0.323, Test accuracy: 87.37
Round  98, Train loss: 0.206, Test loss: 0.319, Test accuracy: 87.52
Round  99, Train loss: 0.205, Test loss: 0.321, Test accuracy: 87.30
Final Round, Train loss: 0.203, Test loss: 0.320, Test accuracy: 87.44
Average accuracy final 10 rounds: 87.25
1807.5288181304932
[2.2495856285095215, 4.499171257019043, 6.3108673095703125, 8.122563362121582, 9.891026258468628, 11.659489154815674, 13.304458618164062, 14.949428081512451, 16.6175537109375, 18.28567934036255, 20.027642726898193, 21.769606113433838, 23.466311931610107, 25.163017749786377, 26.844244718551636, 28.525471687316895, 30.205970287322998, 31.8864688873291, 33.555147647857666, 35.22382640838623, 36.85046124458313, 38.47709608078003, 40.16934514045715, 41.86159420013428, 43.544092655181885, 45.22659111022949, 46.89656972885132, 48.566548347473145, 50.30698895454407, 52.04742956161499, 53.7137770652771, 55.38012456893921, 57.07681679725647, 58.77350902557373, 60.47097587585449, 62.168442726135254, 63.883920192718506, 65.59939765930176, 67.26327753067017, 68.92715740203857, 70.65702819824219, 72.3868989944458, 74.19832062721252, 76.00974225997925, 77.71840238571167, 79.42706251144409, 81.16293001174927, 82.89879751205444, 84.63056445121765, 86.36233139038086, 88.06487321853638, 89.7674150466919, 91.45111894607544, 93.13482284545898, 94.88336682319641, 96.63191080093384, 98.43825125694275, 100.24459171295166, 101.96964955329895, 103.69470739364624, 105.49257493019104, 107.29044246673584, 109.11002540588379, 110.92960834503174, 112.8009147644043, 114.67222118377686, 116.56139612197876, 118.45057106018066, 120.3261239528656, 122.20167684555054, 123.91347432136536, 125.62527179718018, 127.51061391830444, 129.3959560394287, 131.30522179603577, 133.21448755264282, 135.12866735458374, 137.04284715652466, 138.93263125419617, 140.82241535186768, 142.62219429016113, 144.4219732284546, 146.11464476585388, 147.80731630325317, 149.53881216049194, 151.2703080177307, 153.02679133415222, 154.78327465057373, 156.50201964378357, 158.2207646369934, 160.08926796913147, 161.95777130126953, 163.72089672088623, 165.48402214050293, 167.22796750068665, 168.97191286087036, 170.70557165145874, 172.43923044204712, 174.16751790046692, 175.89580535888672, 177.5895118713379, 179.28321838378906, 180.97892594337463, 182.6746335029602, 184.40340518951416, 186.13217687606812, 187.8495490550995, 189.56692123413086, 191.3574664592743, 193.14801168441772, 195.06456899642944, 196.98112630844116, 198.78566074371338, 200.5901951789856, 202.37365579605103, 204.15711641311646, 205.87681555747986, 207.59651470184326, 209.3044831752777, 211.01245164871216, 212.68668389320374, 214.3609161376953, 216.07013463974, 217.77935314178467, 219.5041263103485, 221.22889947891235, 223.01926183700562, 224.80962419509888, 226.5587375164032, 228.30785083770752, 230.04043412208557, 231.77301740646362, 233.47116708755493, 235.16931676864624, 236.91374039649963, 238.65816402435303, 240.46524786949158, 242.27233171463013, 243.97070622444153, 245.66908073425293, 247.37765526771545, 249.08622980117798, 250.80731296539307, 252.52839612960815, 254.38024640083313, 256.2320966720581, 258.10301065444946, 259.9739246368408, 261.83712816238403, 263.70033168792725, 265.5639524459839, 267.4275732040405, 269.30864453315735, 271.18971586227417, 272.92082691192627, 274.65193796157837, 276.36150884628296, 278.07107973098755, 279.8090088367462, 281.5469379425049, 283.25963377952576, 284.97232961654663, 286.7062888145447, 288.4402480125427, 290.10907554626465, 291.7779030799866, 293.48748254776, 295.19706201553345, 296.9223737716675, 298.6476855278015, 300.3803868293762, 302.1130881309509, 303.9972677230835, 305.88144731521606, 307.7596056461334, 309.6377639770508, 311.43942379951477, 313.24108362197876, 314.94182300567627, 316.6425623893738, 318.4347515106201, 320.22694063186646, 321.9764666557312, 323.72599267959595, 325.40864872932434, 327.09130477905273, 328.7693111896515, 330.44731760025024, 332.08599305152893, 333.7246685028076, 335.40719199180603, 337.08971548080444, 338.80949354171753, 340.5292716026306, 342.1193675994873, 343.709463596344, 345.3122925758362, 346.91512155532837, 348.57151532173157, 350.22790908813477, 352.4336624145508, 354.6394157409668]
[14.691666666666666, 14.691666666666666, 30.766666666666666, 30.766666666666666, 38.50833333333333, 38.50833333333333, 46.90833333333333, 46.90833333333333, 55.55, 55.55, 63.99166666666667, 63.99166666666667, 65.85, 65.85, 70.39166666666667, 70.39166666666667, 72.44166666666666, 72.44166666666666, 73.09166666666667, 73.09166666666667, 74.09166666666667, 74.09166666666667, 75.85, 75.85, 77.41666666666667, 77.41666666666667, 77.59166666666667, 77.59166666666667, 77.85833333333333, 77.85833333333333, 78.49166666666666, 78.49166666666666, 79.30833333333334, 79.30833333333334, 79.175, 79.175, 79.775, 79.775, 79.59166666666667, 79.59166666666667, 79.59166666666667, 79.59166666666667, 80.9, 80.9, 80.71666666666667, 80.71666666666667, 80.51666666666667, 80.51666666666667, 81.11666666666666, 81.11666666666666, 81.2, 81.2, 81.55, 81.55, 82.36666666666666, 82.36666666666666, 82.14166666666667, 82.14166666666667, 82.025, 82.025, 82.41666666666667, 82.41666666666667, 82.05833333333334, 82.05833333333334, 82.075, 82.075, 82.81666666666666, 82.81666666666666, 83.0, 83.0, 83.28333333333333, 83.28333333333333, 83.6, 83.6, 83.625, 83.625, 83.70833333333333, 83.70833333333333, 83.56666666666666, 83.56666666666666, 83.89166666666667, 83.89166666666667, 83.58333333333333, 83.58333333333333, 83.78333333333333, 83.78333333333333, 84.0, 84.0, 84.30833333333334, 84.30833333333334, 84.125, 84.125, 84.6, 84.6, 84.96666666666667, 84.96666666666667, 84.56666666666666, 84.56666666666666, 84.78333333333333, 84.78333333333333, 85.30833333333334, 85.30833333333334, 85.375, 85.375, 85.04166666666667, 85.04166666666667, 85.275, 85.275, 85.525, 85.525, 85.23333333333333, 85.23333333333333, 85.13333333333334, 85.13333333333334, 85.45, 85.45, 85.60833333333333, 85.60833333333333, 85.625, 85.625, 85.64166666666667, 85.64166666666667, 85.84166666666667, 85.84166666666667, 86.05, 86.05, 85.86666666666666, 85.86666666666666, 86.25, 86.25, 85.7, 85.7, 85.8, 85.8, 86.05, 86.05, 86.34166666666667, 86.34166666666667, 86.125, 86.125, 86.28333333333333, 86.28333333333333, 86.16666666666667, 86.16666666666667, 86.4, 86.4, 86.15833333333333, 86.15833333333333, 86.325, 86.325, 86.70833333333333, 86.70833333333333, 86.63333333333334, 86.63333333333334, 86.725, 86.725, 87.00833333333334, 87.00833333333334, 86.68333333333334, 86.68333333333334, 86.81666666666666, 86.81666666666666, 86.56666666666666, 86.56666666666666, 86.60833333333333, 86.60833333333333, 86.73333333333333, 86.73333333333333, 86.85, 86.85, 86.94166666666666, 86.94166666666666, 86.84166666666667, 86.84166666666667, 87.15833333333333, 87.15833333333333, 87.25, 87.25, 86.825, 86.825, 87.1, 87.1, 87.15833333333333, 87.15833333333333, 87.01666666666667, 87.01666666666667, 87.31666666666666, 87.31666666666666, 87.28333333333333, 87.28333333333333, 87.08333333333333, 87.08333333333333, 87.35833333333333, 87.35833333333333, 87.36666666666666, 87.36666666666666, 87.51666666666667, 87.51666666666667, 87.3, 87.3, 87.44166666666666, 87.44166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.179, Test loss: 2.153, Test accuracy: 21.22
Round   1, Train loss: 1.888, Test loss: 2.187, Test accuracy: 23.96
Round   2, Train loss: 1.745, Test loss: 2.146, Test accuracy: 25.57
Round   3, Train loss: 1.662, Test loss: 2.095, Test accuracy: 27.12
Round   4, Train loss: 1.567, Test loss: 2.098, Test accuracy: 26.93
Round   5, Train loss: 1.473, Test loss: 2.146, Test accuracy: 28.07
Round   6, Train loss: 1.387, Test loss: 2.091, Test accuracy: 29.65
Round   7, Train loss: 1.364, Test loss: 2.160, Test accuracy: 29.73
Round   8, Train loss: 1.285, Test loss: 2.080, Test accuracy: 28.25
Round   9, Train loss: 1.278, Test loss: 2.147, Test accuracy: 28.82
Round  10, Train loss: 1.221, Test loss: 2.167, Test accuracy: 28.80
Round  11, Train loss: 1.234, Test loss: 2.023, Test accuracy: 28.87
Round  12, Train loss: 1.146, Test loss: 2.133, Test accuracy: 29.62
Round  13, Train loss: 1.128, Test loss: 2.119, Test accuracy: 29.55
Round  14, Train loss: 1.096, Test loss: 2.198, Test accuracy: 29.32
Round  15, Train loss: 1.072, Test loss: 2.143, Test accuracy: 30.46
Round  16, Train loss: 1.055, Test loss: 2.081, Test accuracy: 29.89
Round  17, Train loss: 1.022, Test loss: 2.238, Test accuracy: 30.44
Round  18, Train loss: 0.982, Test loss: 2.126, Test accuracy: 29.47
Round  19, Train loss: 0.942, Test loss: 2.160, Test accuracy: 30.43
Round  20, Train loss: 0.955, Test loss: 2.182, Test accuracy: 30.03
Round  21, Train loss: 0.948, Test loss: 2.208, Test accuracy: 30.86
Round  22, Train loss: 0.947, Test loss: 2.145, Test accuracy: 31.64
Round  23, Train loss: 0.893, Test loss: 2.482, Test accuracy: 29.81
Round  24, Train loss: 0.921, Test loss: 2.229, Test accuracy: 29.45
Round  25, Train loss: 0.885, Test loss: 2.193, Test accuracy: 30.04
Round  26, Train loss: 0.869, Test loss: 2.255, Test accuracy: 31.02
Round  27, Train loss: 0.853, Test loss: 2.310, Test accuracy: 30.86
Round  28, Train loss: 0.838, Test loss: 2.223, Test accuracy: 29.64
Round  29, Train loss: 0.822, Test loss: 2.352, Test accuracy: 30.43
Round  30, Train loss: 0.819, Test loss: 2.189, Test accuracy: 31.03
Round  31, Train loss: 0.830, Test loss: 2.472, Test accuracy: 32.38
Round  32, Train loss: 0.814, Test loss: 2.336, Test accuracy: 32.79
Round  33, Train loss: 0.780, Test loss: 2.318, Test accuracy: 31.22
Round  34, Train loss: 0.781, Test loss: 2.165, Test accuracy: 31.32
Round  35, Train loss: 0.812, Test loss: 2.349, Test accuracy: 30.59
Round  36, Train loss: 0.775, Test loss: 2.233, Test accuracy: 33.53
Round  37, Train loss: 0.749, Test loss: 2.435, Test accuracy: 30.23
Round  38, Train loss: 0.747, Test loss: 2.242, Test accuracy: 30.32
Round  39, Train loss: 0.726, Test loss: 2.604, Test accuracy: 32.82
Round  40, Train loss: 0.707, Test loss: 2.304, Test accuracy: 30.20
Round  41, Train loss: 0.735, Test loss: 2.511, Test accuracy: 31.48
Round  42, Train loss: 0.703, Test loss: 2.358, Test accuracy: 32.15
Round  43, Train loss: 0.731, Test loss: 2.290, Test accuracy: 31.04
Round  44, Train loss: 0.663, Test loss: 2.762, Test accuracy: 33.92
Round  45, Train loss: 0.695, Test loss: 2.385, Test accuracy: 32.11
Round  46, Train loss: 0.709, Test loss: 2.255, Test accuracy: 32.60
Round  47, Train loss: 0.688, Test loss: 2.304, Test accuracy: 31.05
Round  48, Train loss: 0.703, Test loss: 2.208, Test accuracy: 30.01
Round  49, Train loss: 0.674, Test loss: 2.238, Test accuracy: 31.27
Round  50, Train loss: 0.640, Test loss: 2.289, Test accuracy: 29.80
Round  51, Train loss: 0.665, Test loss: 2.126, Test accuracy: 30.66
Round  52, Train loss: 0.642, Test loss: 2.187, Test accuracy: 29.61
Round  53, Train loss: 0.634, Test loss: 2.314, Test accuracy: 29.91
Round  54, Train loss: 0.659, Test loss: 2.352, Test accuracy: 31.33
Round  55, Train loss: 0.633, Test loss: 2.317, Test accuracy: 29.77
Round  56, Train loss: 0.650, Test loss: 2.484, Test accuracy: 32.44
Round  57, Train loss: 0.616, Test loss: 2.476, Test accuracy: 32.08
Round  58, Train loss: 0.634, Test loss: 2.455, Test accuracy: 30.95
Round  59, Train loss: 0.606, Test loss: 2.368, Test accuracy: 32.41
Round  60, Train loss: 0.604, Test loss: 2.362, Test accuracy: 32.02
Round  61, Train loss: 0.620, Test loss: 2.376, Test accuracy: 31.84
Round  62, Train loss: 0.583, Test loss: 2.716, Test accuracy: 33.01
Round  63, Train loss: 0.590, Test loss: 2.550, Test accuracy: 30.30
Round  64, Train loss: 0.594, Test loss: 2.436, Test accuracy: 31.55
Round  65, Train loss: 0.570, Test loss: 2.563, Test accuracy: 31.36
Round  66, Train loss: 0.597, Test loss: 2.401, Test accuracy: 28.96
Round  67, Train loss: 0.568, Test loss: 2.291, Test accuracy: 31.55
Round  68, Train loss: 0.559, Test loss: 2.478, Test accuracy: 31.05
Round  69, Train loss: 0.577, Test loss: 2.222, Test accuracy: 31.17
Round  70, Train loss: 0.592, Test loss: 2.387, Test accuracy: 29.49
Round  71, Train loss: 0.575, Test loss: 2.491, Test accuracy: 31.36
Round  72, Train loss: 0.541, Test loss: 2.548, Test accuracy: 32.78
Round  73, Train loss: 0.581, Test loss: 2.337, Test accuracy: 29.19
Round  74, Train loss: 0.548, Test loss: 2.455, Test accuracy: 31.05
Round  75, Train loss: 0.561, Test loss: 2.667, Test accuracy: 30.62
Round  76, Train loss: 0.528, Test loss: 2.469, Test accuracy: 33.88
Round  77, Train loss: 0.563, Test loss: 2.451, Test accuracy: 30.31
Round  78, Train loss: 0.519, Test loss: 2.390, Test accuracy: 31.38
Round  79, Train loss: 0.543, Test loss: 2.496, Test accuracy: 31.73
Round  80, Train loss: 0.579, Test loss: 2.547, Test accuracy: 30.28
Round  81, Train loss: 0.537, Test loss: 2.358, Test accuracy: 31.95
Round  82, Train loss: 0.488, Test loss: 2.353, Test accuracy: 29.80
Round  83, Train loss: 0.566, Test loss: 2.416, Test accuracy: 31.50
Round  84, Train loss: 0.551, Test loss: 3.035, Test accuracy: 32.12
Round  85, Train loss: 0.494, Test loss: 2.492, Test accuracy: 32.30
Round  86, Train loss: 0.513, Test loss: 2.285, Test accuracy: 31.72
Round  87, Train loss: 0.511, Test loss: 2.550, Test accuracy: 33.00
Round  88, Train loss: 0.514, Test loss: 2.482, Test accuracy: 30.06
Round  89, Train loss: 0.554, Test loss: 2.706, Test accuracy: 30.98
Round  90, Train loss: 0.515, Test loss: 2.559, Test accuracy: 31.09
Round  91, Train loss: 0.491, Test loss: 2.392, Test accuracy: 33.19
Round  92, Train loss: 0.490, Test loss: 2.476, Test accuracy: 32.45
Round  93, Train loss: 0.522, Test loss: 2.599, Test accuracy: 30.57
Round  94, Train loss: 0.487, Test loss: 2.455, Test accuracy: 30.95
Round  95, Train loss: 0.502, Test loss: 2.674, Test accuracy: 33.28
Round  96, Train loss: 0.505, Test loss: 2.337, Test accuracy: 29.31
Round  97, Train loss: 0.514, Test loss: 2.509, Test accuracy: 32.76
Round  98, Train loss: 0.475, Test loss: 2.640, Test accuracy: 31.35
Round  99, Train loss: 0.494, Test loss: 2.558, Test accuracy: 30.89
Final Round, Train loss: 0.418, Test loss: 2.022, Test accuracy: 33.87
Average accuracy final 10 rounds: 31.584250000000004
8749.313601970673
[12.238529920578003, 23.5769681930542, 35.9418888092041, 48.78535485267639, 60.747037172317505, 72.78640246391296, 84.87126183509827, 96.84554290771484, 108.79454803466797, 120.72258138656616, 132.5524022579193, 145.5523602962494, 157.99296593666077, 169.9690489768982, 181.82098960876465, 193.85860800743103, 205.79776239395142, 217.86260604858398, 230.59501194953918, 243.54330134391785, 256.63152503967285, 269.9849247932434, 282.7661395072937, 295.97402119636536, 309.06789898872375, 322.3130593299866, 335.79734802246094, 349.3962891101837, 362.49632954597473, 375.91120433807373, 388.43482971191406, 400.7073769569397, 413.1245210170746, 425.5278685092926, 437.71678924560547, 450.02610445022583, 462.2394895553589, 474.6005485057831, 487.50210881233215, 500.0735354423523, 512.6238062381744, 525.0487849712372, 537.4725031852722, 549.9561696052551, 562.5623867511749, 575.0982282161713, 587.3859310150146, 599.6882135868073, 612.367607831955, 625.0281069278717, 637.5759508609772, 650.8504114151001, 664.1959834098816, 677.2834756374359, 689.7558450698853, 702.2933661937714, 714.6388759613037, 727.5214726924896, 740.789027929306, 754.0501184463501, 767.2667317390442, 780.0625379085541, 792.6064565181732, 805.3788585662842, 818.0036880970001, 830.6279151439667, 843.1873269081116, 855.5879747867584, 868.0561370849609, 880.8610408306122, 894.2122294902802, 907.5091404914856, 921.0841856002808, 934.4914906024933, 947.6349630355835, 961.0299656391144, 973.8407690525055, 987.139119386673, 1000.5654845237732, 1013.6039700508118, 1026.8518996238708, 1039.8797252178192, 1053.0448620319366, 1066.689531803131, 1079.5615420341492, 1092.3884465694427, 1104.9452893733978, 1118.22554397583, 1131.577885389328, 1144.4162793159485, 1156.95982503891, 1169.6786489486694, 1182.4538583755493, 1195.1537628173828, 1207.8787953853607, 1220.5338008403778, 1233.3644955158234, 1246.0665140151978, 1258.8083283901215, 1271.496621131897, 1274.9176239967346]
[21.2225, 23.9575, 25.5725, 27.1175, 26.9275, 28.0725, 29.6475, 29.735, 28.2475, 28.8175, 28.805, 28.8675, 29.62, 29.555, 29.32, 30.46, 29.8875, 30.44, 29.47, 30.4275, 30.0325, 30.8575, 31.6375, 29.8125, 29.4525, 30.04, 31.0175, 30.86, 29.6425, 30.4325, 31.03, 32.385, 32.7925, 31.2175, 31.32, 30.5875, 33.53, 30.23, 30.3225, 32.82, 30.2, 31.475, 32.1475, 31.035, 33.925, 32.11, 32.6025, 31.055, 30.0125, 31.275, 29.795, 30.66, 29.61, 29.905, 31.3325, 29.77, 32.435, 32.08, 30.9525, 32.415, 32.025, 31.84, 33.0125, 30.305, 31.5475, 31.355, 28.965, 31.55, 31.0525, 31.1725, 29.49, 31.36, 32.7825, 29.1925, 31.055, 30.6175, 33.875, 30.3075, 31.375, 31.7275, 30.2775, 31.955, 29.7975, 31.5025, 32.1225, 32.3, 31.7175, 33.0, 30.0625, 30.975, 31.0925, 33.1875, 32.4475, 30.5675, 30.9525, 33.2825, 29.31, 32.76, 31.35, 30.8925, 33.8725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.228, Test loss: 2.306, Test accuracy: 10.62
Round   0, Global train loss: 2.228, Global test loss: 2.309, Global test accuracy: 9.99
Round   1, Train loss: 2.262, Test loss: 2.305, Test accuracy: 11.01
Round   1, Global train loss: 2.262, Global test loss: 2.309, Global test accuracy: 10.21
Round   2, Train loss: 2.241, Test loss: 2.305, Test accuracy: 12.22
Round   2, Global train loss: 2.241, Global test loss: 2.310, Global test accuracy: 10.30
Round   3, Train loss: 2.304, Test loss: 2.309, Test accuracy: 11.53
Round   3, Global train loss: 2.304, Global test loss: 2.312, Global test accuracy: 10.75
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 9.64
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 9.07
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 8.27
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 8.37
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 6.71
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 6.71
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 6.71
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 6.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 6.67
Average accuracy final 10 rounds: 6.666666666666669 

Average global accuracy final 10 rounds: 6.666666666666669 

15240.265696048737
[5.136213541030884, 10.09814453125, 14.985932350158691, 19.557259798049927, 24.499246835708618, 29.57417607307434, 34.59231495857239, 39.2955539226532, 43.83761954307556, 48.473259687423706, 53.030073404312134, 57.711777687072754, 62.33166527748108, 66.96015548706055, 71.56281352043152, 76.15725111961365, 80.81274771690369, 85.44763207435608, 89.98446249961853, 94.61885786056519, 99.10717678070068, 103.61847376823425, 108.49303841590881, 113.21732521057129, 118.27594017982483, 122.97349047660828, 127.47693538665771, 131.97343707084656, 136.48166060447693, 141.08784580230713, 145.67304801940918, 150.32053470611572, 154.87216806411743, 159.3314471244812, 163.8185384273529, 168.54017114639282, 173.50270318984985, 178.0924415588379, 182.645170211792, 187.15114569664001, 191.7035734653473, 196.23142075538635, 200.85182428359985, 205.35675168037415, 209.8444583415985, 214.4345645904541, 219.03213810920715, 223.5496256351471, 228.07481026649475, 232.58703017234802, 237.1028642654419, 241.5892641544342, 246.1663942337036, 250.82872223854065, 255.39222931861877, 259.93068981170654, 264.4969787597656, 269.059095621109, 273.6071798801422, 278.07422637939453, 282.6244785785675, 287.29854679107666, 291.8217251300812, 296.33560132980347, 300.86899971961975, 305.33336639404297, 309.9351770877838, 314.7426059246063, 319.36625266075134, 323.92240262031555, 328.54907059669495, 333.14352679252625, 338.1985664367676, 342.7978117465973, 347.20521664619446, 351.73496174812317, 356.3493378162384, 360.95808601379395, 365.45822954177856, 370.0175139904022, 374.95578360557556, 379.66423082351685, 384.3230028152466, 388.91989970207214, 393.4337213039398, 397.9044716358185, 402.4303629398346, 406.9536850452423, 411.44386076927185, 415.8805913925171, 420.34604120254517, 424.83264327049255, 429.3574936389923, 433.81995940208435, 438.27660179138184, 442.71255826950073, 447.1210627555847, 451.5585253238678, 456.0509076118469, 460.49944853782654, 464.9014184474945, 469.33678555488586, 473.81627011299133, 478.2473075389862, 482.70546221733093, 487.1285879611969, 491.54512095451355, 495.9849054813385, 500.44660544395447, 504.8695936203003, 509.31328415870667, 513.7247483730316, 518.1068348884583, 522.5648996829987, 527.0364739894867, 531.4821031093597, 535.898833990097, 540.2990899085999, 544.744190454483, 549.2670979499817, 553.7768034934998, 558.2322909832001, 562.6835358142853, 567.1063141822815, 571.5804028511047, 576.1010854244232, 580.7030928134918, 585.3310413360596, 589.7851564884186, 594.2409055233002, 598.7445175647736, 603.2937848567963, 607.8582167625427, 612.3527321815491, 616.8407833576202, 621.3512597084045, 625.8110573291779, 630.307608127594, 634.8028273582458, 639.3162906169891, 643.8244857788086, 648.3519155979156, 653.009505033493, 657.6534657478333, 662.225088596344, 666.7530555725098, 671.3201270103455, 675.9013073444366, 680.5596098899841, 685.0496382713318, 689.5503103733063, 694.0424523353577, 698.534294128418, 703.0758001804352, 707.5414335727692, 711.9944045543671, 716.4408116340637, 720.8465161323547, 725.3087346553802, 729.8286175727844, 734.3852043151855, 739.1308922767639, 744.1363236904144, 749.1348288059235, 754.1588037014008, 759.2075884342194, 764.3051645755768, 769.4118988513947, 774.0584161281586, 778.5983726978302, 783.0684478282928, 787.6127886772156, 792.1723372936249, 796.7288641929626, 801.2470288276672, 805.7074980735779, 810.3196516036987, 814.814530134201, 819.2707087993622, 823.7131428718567, 828.204644203186, 832.7296907901764, 837.2251386642456, 841.6774704456329, 846.1901836395264, 850.6746730804443, 855.1960031986237, 859.709406375885, 864.1906471252441, 868.6860451698303, 873.2023804187775, 877.7003841400146, 882.1947565078735, 886.645170211792, 891.1800122261047, 895.6390926837921, 900.1186563968658, 904.5878319740295, 909.1341247558594, 913.6647872924805, 918.1182963848114, 922.5797057151794, 927.0351877212524, 931.5295326709747, 936.0462927818298, 940.4797716140747, 944.9168741703033, 949.385392665863, 953.8986086845398, 958.3745369911194, 962.8236904144287, 967.3049736022949, 971.793386220932, 976.256208896637, 980.7307653427124, 985.225745677948, 989.719393491745, 994.2041780948639, 998.6783313751221, 1003.1478831768036, 1007.6467204093933, 1012.1507475376129, 1016.7346549034119, 1021.3063690662384, 1025.8433139324188, 1030.4459249973297, 1035.0403463840485, 1039.567228794098, 1044.097839832306, 1048.6709010601044, 1053.196572303772, 1057.729462146759, 1062.2540874481201, 1066.7871787548065, 1071.3565554618835, 1075.899509191513, 1080.387024641037, 1084.9582221508026, 1089.5549743175507, 1094.0841145515442, 1098.6718354225159, 1103.2532920837402, 1107.7796380519867, 1112.255072593689, 1116.8718955516815, 1121.4731810092926, 1126.0424613952637, 1130.6222145557404, 1135.2244930267334, 1139.8056089878082, 1144.3824231624603, 1148.9123628139496, 1153.514088153839, 1158.0548655986786, 1162.5639350414276, 1167.1001496315002, 1171.6738409996033, 1176.2013516426086, 1180.7275276184082, 1185.2573947906494, 1189.8185737133026, 1194.3672437667847, 1198.928085565567, 1203.5137040615082, 1208.082555770874, 1212.6314570903778, 1217.184900045395, 1221.7648267745972, 1226.3299186229706, 1230.857761144638, 1235.3954393863678, 1239.9288561344147, 1244.453055858612, 1249.00878572464, 1253.5413813591003, 1258.0911724567413, 1262.696876525879, 1267.2010571956635, 1271.7559716701508, 1276.3345022201538, 1280.884887456894, 1285.4241795539856, 1289.968589067459, 1294.577879667282, 1299.1189255714417, 1303.6974942684174, 1308.3771800994873, 1313.125467300415, 1317.770346403122, 1322.3801972866058, 1327.0707731246948, 1332.218774318695, 1337.5509004592896, 1342.6914222240448, 1347.879414319992, 1353.108755350113, 1358.312400817871, 1362.9057204723358, 1367.5234310626984, 1372.1643512248993, 1374.772915840149]
[10.616666666666667, 11.011111111111111, 12.219444444444445, 11.530555555555555, 9.644444444444444, 9.069444444444445, 8.266666666666667, 8.366666666666667, 6.708333333333334, 6.708333333333334, 6.708333333333334, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668, 6.666666666666668]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.178, Test loss: 1.928, Test accuracy: 22.15
Round   0, Global train loss: 1.178, Global test loss: 2.277, Global test accuracy: 14.33
Round   1, Train loss: 1.063, Test loss: 1.747, Test accuracy: 37.60
Round   1, Global train loss: 1.063, Global test loss: 2.429, Global test accuracy: 22.33
Round   2, Train loss: 0.935, Test loss: 1.380, Test accuracy: 46.47
Round   2, Global train loss: 0.935, Global test loss: 2.334, Global test accuracy: 25.92
Round   3, Train loss: 0.925, Test loss: 1.246, Test accuracy: 49.40
Round   3, Global train loss: 0.925, Global test loss: 2.167, Global test accuracy: 23.33
Round   4, Train loss: 0.844, Test loss: 1.078, Test accuracy: 54.52
Round   4, Global train loss: 0.844, Global test loss: 1.942, Global test accuracy: 28.57
Round   5, Train loss: 0.774, Test loss: 1.057, Test accuracy: 56.82
Round   5, Global train loss: 0.774, Global test loss: 2.151, Global test accuracy: 28.33
Round   6, Train loss: 0.806, Test loss: 0.849, Test accuracy: 63.45
Round   6, Global train loss: 0.806, Global test loss: 1.831, Global test accuracy: 33.77
Round   7, Train loss: 0.727, Test loss: 0.728, Test accuracy: 67.55
Round   7, Global train loss: 0.727, Global test loss: 1.846, Global test accuracy: 36.27
Round   8, Train loss: 0.755, Test loss: 0.752, Test accuracy: 66.23
Round   8, Global train loss: 0.755, Global test loss: 1.849, Global test accuracy: 32.57
Round   9, Train loss: 0.769, Test loss: 0.727, Test accuracy: 67.63
Round   9, Global train loss: 0.769, Global test loss: 1.966, Global test accuracy: 32.78
Round  10, Train loss: 0.784, Test loss: 0.718, Test accuracy: 68.58
Round  10, Global train loss: 0.784, Global test loss: 1.919, Global test accuracy: 31.28
Round  11, Train loss: 0.748, Test loss: 0.703, Test accuracy: 69.33
Round  11, Global train loss: 0.748, Global test loss: 2.134, Global test accuracy: 34.28
Round  12, Train loss: 0.721, Test loss: 0.733, Test accuracy: 68.48
Round  12, Global train loss: 0.721, Global test loss: 2.009, Global test accuracy: 35.02
Round  13, Train loss: 0.769, Test loss: 0.724, Test accuracy: 69.03
Round  13, Global train loss: 0.769, Global test loss: 1.720, Global test accuracy: 36.90
Round  14, Train loss: 0.651, Test loss: 0.699, Test accuracy: 69.68
Round  14, Global train loss: 0.651, Global test loss: 1.782, Global test accuracy: 37.37
Round  15, Train loss: 0.688, Test loss: 0.709, Test accuracy: 68.87
Round  15, Global train loss: 0.688, Global test loss: 1.782, Global test accuracy: 36.15
Round  16, Train loss: 0.638, Test loss: 0.695, Test accuracy: 69.55
Round  16, Global train loss: 0.638, Global test loss: 1.751, Global test accuracy: 38.95
Round  17, Train loss: 0.668, Test loss: 0.691, Test accuracy: 70.22
Round  17, Global train loss: 0.668, Global test loss: 1.744, Global test accuracy: 41.30
Round  18, Train loss: 0.773, Test loss: 0.682, Test accuracy: 70.48
Round  18, Global train loss: 0.773, Global test loss: 2.197, Global test accuracy: 30.77
Round  19, Train loss: 0.662, Test loss: 0.653, Test accuracy: 72.27
Round  19, Global train loss: 0.662, Global test loss: 1.621, Global test accuracy: 40.78
Round  20, Train loss: 0.676, Test loss: 0.654, Test accuracy: 72.43
Round  20, Global train loss: 0.676, Global test loss: 2.137, Global test accuracy: 30.05
Round  21, Train loss: 0.624, Test loss: 0.651, Test accuracy: 72.42
Round  21, Global train loss: 0.624, Global test loss: 1.705, Global test accuracy: 39.07
Round  22, Train loss: 0.675, Test loss: 0.656, Test accuracy: 72.50
Round  22, Global train loss: 0.675, Global test loss: 1.872, Global test accuracy: 37.33
Round  23, Train loss: 0.618, Test loss: 0.651, Test accuracy: 72.77
Round  23, Global train loss: 0.618, Global test loss: 1.596, Global test accuracy: 42.92
Round  24, Train loss: 0.628, Test loss: 0.647, Test accuracy: 73.63
Round  24, Global train loss: 0.628, Global test loss: 1.556, Global test accuracy: 43.53
Round  25, Train loss: 0.586, Test loss: 0.671, Test accuracy: 72.92
Round  25, Global train loss: 0.586, Global test loss: 1.761, Global test accuracy: 39.05
Round  26, Train loss: 0.542, Test loss: 0.676, Test accuracy: 72.78
Round  26, Global train loss: 0.542, Global test loss: 1.715, Global test accuracy: 42.85
Round  27, Train loss: 0.575, Test loss: 0.651, Test accuracy: 73.40
Round  27, Global train loss: 0.575, Global test loss: 1.624, Global test accuracy: 42.73
Round  28, Train loss: 0.600, Test loss: 0.643, Test accuracy: 73.90
Round  28, Global train loss: 0.600, Global test loss: 1.544, Global test accuracy: 43.63
Round  29, Train loss: 0.588, Test loss: 0.641, Test accuracy: 74.00
Round  29, Global train loss: 0.588, Global test loss: 1.648, Global test accuracy: 44.02
Round  30, Train loss: 0.607, Test loss: 0.624, Test accuracy: 74.27
Round  30, Global train loss: 0.607, Global test loss: 1.770, Global test accuracy: 39.58
Round  31, Train loss: 0.598, Test loss: 0.614, Test accuracy: 74.72
Round  31, Global train loss: 0.598, Global test loss: 1.690, Global test accuracy: 40.60
Round  32, Train loss: 0.546, Test loss: 0.609, Test accuracy: 75.30
Round  32, Global train loss: 0.546, Global test loss: 1.833, Global test accuracy: 42.15
Round  33, Train loss: 0.627, Test loss: 0.601, Test accuracy: 75.30
Round  33, Global train loss: 0.627, Global test loss: 1.643, Global test accuracy: 42.60
Round  34, Train loss: 0.619, Test loss: 0.599, Test accuracy: 75.63
Round  34, Global train loss: 0.619, Global test loss: 1.578, Global test accuracy: 41.90
Round  35, Train loss: 0.523, Test loss: 0.597, Test accuracy: 75.68
Round  35, Global train loss: 0.523, Global test loss: 1.404, Global test accuracy: 48.50
Round  36, Train loss: 0.529, Test loss: 0.629, Test accuracy: 74.88
Round  36, Global train loss: 0.529, Global test loss: 1.470, Global test accuracy: 46.48
Round  37, Train loss: 0.576, Test loss: 0.618, Test accuracy: 75.23
Round  37, Global train loss: 0.576, Global test loss: 1.381, Global test accuracy: 50.48
Round  38, Train loss: 0.523, Test loss: 0.613, Test accuracy: 75.80
Round  38, Global train loss: 0.523, Global test loss: 1.425, Global test accuracy: 48.83
Round  39, Train loss: 0.457, Test loss: 0.601, Test accuracy: 76.05
Round  39, Global train loss: 0.457, Global test loss: 1.486, Global test accuracy: 48.78
Round  40, Train loss: 0.515, Test loss: 0.597, Test accuracy: 76.73
Round  40, Global train loss: 0.515, Global test loss: 1.634, Global test accuracy: 44.17
Round  41, Train loss: 0.465, Test loss: 0.596, Test accuracy: 76.80
Round  41, Global train loss: 0.465, Global test loss: 1.499, Global test accuracy: 48.18
Round  42, Train loss: 0.490, Test loss: 0.615, Test accuracy: 76.27
Round  42, Global train loss: 0.490, Global test loss: 1.381, Global test accuracy: 50.07
Round  43, Train loss: 0.412, Test loss: 0.620, Test accuracy: 76.23
Round  43, Global train loss: 0.412, Global test loss: 1.841, Global test accuracy: 42.22
Round  44, Train loss: 0.464, Test loss: 0.616, Test accuracy: 76.37
Round  44, Global train loss: 0.464, Global test loss: 1.997, Global test accuracy: 40.02
Round  45, Train loss: 0.498, Test loss: 0.604, Test accuracy: 76.97
Round  45, Global train loss: 0.498, Global test loss: 1.624, Global test accuracy: 42.82
Round  46, Train loss: 0.534, Test loss: 0.608, Test accuracy: 76.28
Round  46, Global train loss: 0.534, Global test loss: 1.393, Global test accuracy: 48.88
Round  47, Train loss: 0.462, Test loss: 0.589, Test accuracy: 77.07
Round  47, Global train loss: 0.462, Global test loss: 1.544, Global test accuracy: 47.13
Round  48, Train loss: 0.458, Test loss: 0.608, Test accuracy: 76.68
Round  48, Global train loss: 0.458, Global test loss: 1.615, Global test accuracy: 47.82
Round  49, Train loss: 0.461, Test loss: 0.613, Test accuracy: 76.67
Round  49, Global train loss: 0.461, Global test loss: 1.852, Global test accuracy: 43.02
Round  50, Train loss: 0.469, Test loss: 0.592, Test accuracy: 77.57
Round  50, Global train loss: 0.469, Global test loss: 1.401, Global test accuracy: 49.85
Round  51, Train loss: 0.438, Test loss: 0.604, Test accuracy: 77.08
Round  51, Global train loss: 0.438, Global test loss: 1.559, Global test accuracy: 47.63
Round  52, Train loss: 0.458, Test loss: 0.608, Test accuracy: 77.27
Round  52, Global train loss: 0.458, Global test loss: 1.384, Global test accuracy: 51.00
Round  53, Train loss: 0.370, Test loss: 0.610, Test accuracy: 76.92
Round  53, Global train loss: 0.370, Global test loss: 1.569, Global test accuracy: 47.45
Round  54, Train loss: 0.355, Test loss: 0.609, Test accuracy: 77.05
Round  54, Global train loss: 0.355, Global test loss: 1.654, Global test accuracy: 46.88
Round  55, Train loss: 0.408, Test loss: 0.631, Test accuracy: 76.32
Round  55, Global train loss: 0.408, Global test loss: 1.671, Global test accuracy: 47.23
Round  56, Train loss: 0.428, Test loss: 0.641, Test accuracy: 76.18
Round  56, Global train loss: 0.428, Global test loss: 1.378, Global test accuracy: 52.45
Round  57, Train loss: 0.431, Test loss: 0.610, Test accuracy: 77.32
Round  57, Global train loss: 0.431, Global test loss: 1.439, Global test accuracy: 48.90
Round  58, Train loss: 0.409, Test loss: 0.617, Test accuracy: 76.95
Round  58, Global train loss: 0.409, Global test loss: 1.529, Global test accuracy: 49.52
Round  59, Train loss: 0.350, Test loss: 0.631, Test accuracy: 76.68
Round  59, Global train loss: 0.350, Global test loss: 1.540, Global test accuracy: 50.73
Round  60, Train loss: 0.447, Test loss: 0.644, Test accuracy: 76.83
Round  60, Global train loss: 0.447, Global test loss: 1.408, Global test accuracy: 50.65
Round  61, Train loss: 0.371, Test loss: 0.626, Test accuracy: 77.40
Round  61, Global train loss: 0.371, Global test loss: 1.385, Global test accuracy: 52.95
Round  62, Train loss: 0.439, Test loss: 0.601, Test accuracy: 78.13
Round  62, Global train loss: 0.439, Global test loss: 1.968, Global test accuracy: 43.28
Round  63, Train loss: 0.357, Test loss: 0.610, Test accuracy: 77.98
Round  63, Global train loss: 0.357, Global test loss: 1.614, Global test accuracy: 44.85
Round  64, Train loss: 0.348, Test loss: 0.609, Test accuracy: 77.53
Round  64, Global train loss: 0.348, Global test loss: 1.503, Global test accuracy: 49.43
Round  65, Train loss: 0.350, Test loss: 0.614, Test accuracy: 77.50
Round  65, Global train loss: 0.350, Global test loss: 1.566, Global test accuracy: 48.70
Round  66, Train loss: 0.349, Test loss: 0.599, Test accuracy: 77.83
Round  66, Global train loss: 0.349, Global test loss: 1.419, Global test accuracy: 50.95
Round  67, Train loss: 0.382, Test loss: 0.588, Test accuracy: 78.13
Round  67, Global train loss: 0.382, Global test loss: 1.396, Global test accuracy: 52.12
Round  68, Train loss: 0.302, Test loss: 0.598, Test accuracy: 77.87
Round  68, Global train loss: 0.302, Global test loss: 1.647, Global test accuracy: 49.97
Round  69, Train loss: 0.375, Test loss: 0.599, Test accuracy: 78.07
Round  69, Global train loss: 0.375, Global test loss: 1.815, Global test accuracy: 43.93
Round  70, Train loss: 0.332, Test loss: 0.593, Test accuracy: 78.53
Round  70, Global train loss: 0.332, Global test loss: 1.415, Global test accuracy: 53.52
Round  71, Train loss: 0.355, Test loss: 0.612, Test accuracy: 77.60
Round  71, Global train loss: 0.355, Global test loss: 1.734, Global test accuracy: 47.42
Round  72, Train loss: 0.442, Test loss: 0.583, Test accuracy: 78.27
Round  72, Global train loss: 0.442, Global test loss: 1.666, Global test accuracy: 49.23
Round  73, Train loss: 0.294, Test loss: 0.618, Test accuracy: 77.82
Round  73, Global train loss: 0.294, Global test loss: 2.197, Global test accuracy: 39.93
Round  74, Train loss: 0.277, Test loss: 0.602, Test accuracy: 78.33
Round  74, Global train loss: 0.277, Global test loss: 1.613, Global test accuracy: 51.15
Round  75, Train loss: 0.353, Test loss: 0.610, Test accuracy: 78.27
Round  75, Global train loss: 0.353, Global test loss: 1.424, Global test accuracy: 51.43
Round  76, Train loss: 0.376, Test loss: 0.632, Test accuracy: 77.48
Round  76, Global train loss: 0.376, Global test loss: 1.583, Global test accuracy: 48.72
Round  77, Train loss: 0.358, Test loss: 0.663, Test accuracy: 77.00
Round  77, Global train loss: 0.358, Global test loss: 1.452, Global test accuracy: 51.50
Round  78, Train loss: 0.376, Test loss: 0.672, Test accuracy: 76.93
Round  78, Global train loss: 0.376, Global test loss: 1.457, Global test accuracy: 51.62
Round  79, Train loss: 0.300, Test loss: 0.669, Test accuracy: 77.22
Round  79, Global train loss: 0.300, Global test loss: 1.637, Global test accuracy: 49.15
Round  80, Train loss: 0.321, Test loss: 0.654, Test accuracy: 77.20
Round  80, Global train loss: 0.321, Global test loss: 1.639, Global test accuracy: 49.75
Round  81, Train loss: 0.364, Test loss: 0.618, Test accuracy: 77.85
Round  81, Global train loss: 0.364, Global test loss: 1.392, Global test accuracy: 52.12
Round  82, Train loss: 0.325, Test loss: 0.606, Test accuracy: 78.33
Round  82, Global train loss: 0.325, Global test loss: 1.462, Global test accuracy: 51.57
Round  83, Train loss: 0.244, Test loss: 0.611, Test accuracy: 78.42
Round  83, Global train loss: 0.244, Global test loss: 1.438, Global test accuracy: 54.02
Round  84, Train loss: 0.291, Test loss: 0.614, Test accuracy: 78.20
Round  84, Global train loss: 0.291, Global test loss: 1.447, Global test accuracy: 51.93
Round  85, Train loss: 0.342, Test loss: 0.635, Test accuracy: 77.95
Round  85, Global train loss: 0.342, Global test loss: 1.657, Global test accuracy: 48.97
Round  86, Train loss: 0.342, Test loss: 0.644, Test accuracy: 78.05
Round  86, Global train loss: 0.342, Global test loss: 1.437, Global test accuracy: 52.68
Round  87, Train loss: 0.328, Test loss: 0.671, Test accuracy: 77.95
Round  87, Global train loss: 0.328, Global test loss: 2.511, Global test accuracy: 41.65
Round  88, Train loss: 0.315, Test loss: 0.655, Test accuracy: 78.35
Round  88, Global train loss: 0.315, Global test loss: 1.512, Global test accuracy: 50.72
Round  89, Train loss: 0.279, Test loss: 0.674, Test accuracy: 77.92
Round  89, Global train loss: 0.279, Global test loss: 1.475, Global test accuracy: 52.22
Round  90, Train loss: 0.290, Test loss: 0.648, Test accuracy: 78.58
Round  90, Global train loss: 0.290, Global test loss: 1.390, Global test accuracy: 54.88
Round  91, Train loss: 0.256, Test loss: 0.609, Test accuracy: 79.38
Round  91, Global train loss: 0.256, Global test loss: 1.587, Global test accuracy: 52.65
Round  92, Train loss: 0.348, Test loss: 0.609, Test accuracy: 79.58
Round  92, Global train loss: 0.348, Global test loss: 1.411, Global test accuracy: 52.07
Round  93, Train loss: 0.366, Test loss: 0.609, Test accuracy: 80.23
Round  93, Global train loss: 0.366, Global test loss: 1.569, Global test accuracy: 49.35
Round  94, Train loss: 0.294, Test loss: 0.618, Test accuracy: 79.90
Round  94, Global train loss: 0.294, Global test loss: 1.458, Global test accuracy: 54.10
Round  95, Train loss: 0.269, Test loss: 0.618, Test accuracy: 79.38
Round  95, Global train loss: 0.269, Global test loss: 1.373, Global test accuracy: 55.22
Round  96, Train loss: 0.312, Test loss: 0.615, Test accuracy: 79.33
Round  96, Global train loss: 0.312, Global test loss: 1.547, Global test accuracy: 52.05
Round  97, Train loss: 0.247, Test loss: 0.633, Test accuracy: 78.92
Round  97, Global train loss: 0.247, Global test loss: 1.560, Global test accuracy: 53.63
Round  98, Train loss: 0.278, Test loss: 0.608, Test accuracy: 79.70
Round  98, Global train loss: 0.278, Global test loss: 1.423, Global test accuracy: 53.98
Round  99, Train loss: 0.281, Test loss: 0.597, Test accuracy: 80.07
Round  99, Global train loss: 0.281, Global test loss: 1.584, Global test accuracy: 49.27
Final Round, Train loss: 0.218, Test loss: 0.705, Test accuracy: 79.45
Final Round, Global train loss: 0.218, Global test loss: 1.584, Global test accuracy: 49.27
Average accuracy final 10 rounds: 79.50833333333333 

Average global accuracy final 10 rounds: 52.72 

1091.3718802928925
[1.0559659004211426, 2.111931800842285, 2.9010913372039795, 3.690250873565674, 4.518563985824585, 5.346877098083496, 6.160520315170288, 6.97416353225708, 7.756738901138306, 8.539314270019531, 9.320587396621704, 10.101860523223877, 10.905998706817627, 11.710136890411377, 12.524526357650757, 13.338915824890137, 14.162555694580078, 14.98619556427002, 15.778757095336914, 16.57131862640381, 17.367571592330933, 18.163824558258057, 18.957245349884033, 19.75066614151001, 20.548286199569702, 21.345906257629395, 22.163127422332764, 22.980348587036133, 23.811811923980713, 24.643275260925293, 25.4634370803833, 26.28359889984131, 27.079585790634155, 27.875572681427002, 28.69702672958374, 29.51848077774048, 30.333807706832886, 31.149134635925293, 31.955318689346313, 32.761502742767334, 33.569966554641724, 34.37843036651611, 35.19671678543091, 36.0150032043457, 36.76282215118408, 37.51064109802246, 38.24725794792175, 38.983874797821045, 39.676246881484985, 40.368618965148926, 41.06151580810547, 41.75441265106201, 42.48677039146423, 43.219128131866455, 43.94719362258911, 44.67525911331177, 45.375155448913574, 46.07505178451538, 46.784037590026855, 47.49302339553833, 48.20036602020264, 48.90770864486694, 49.63550853729248, 50.36330842971802, 51.09331774711609, 51.82332706451416, 52.52214002609253, 53.2209529876709, 53.944047689437866, 54.667142391204834, 55.36493635177612, 56.06273031234741, 56.76466417312622, 57.46659803390503, 58.171945333480835, 58.87729263305664, 59.57176327705383, 60.266233921051025, 60.975228786468506, 61.684223651885986, 62.420050859451294, 63.1558780670166, 63.892035722732544, 64.62819337844849, 65.32008838653564, 66.0119833946228, 66.69845914840698, 67.38493490219116, 68.08742189407349, 68.78990888595581, 69.48515963554382, 70.18041038513184, 70.9088180065155, 71.63722562789917, 72.36966896057129, 73.10211229324341, 73.79786682128906, 74.49362134933472, 75.19059944152832, 75.88757753372192, 76.59053993225098, 77.29350233078003, 77.98603057861328, 78.67855882644653, 79.39655661582947, 80.1145544052124, 80.84786558151245, 81.5811767578125, 82.30690932273865, 83.0326418876648, 83.73393535614014, 84.43522882461548, 85.12939262390137, 85.82355642318726, 86.51881456375122, 87.21407270431519, 87.9085693359375, 88.60306596755981, 89.29135990142822, 89.97965383529663, 90.67576932907104, 91.37188482284546, 92.0626118183136, 92.75333881378174, 93.4444797039032, 94.13562059402466, 94.82591772079468, 95.5162148475647, 96.20482659339905, 96.8934383392334, 97.58147668838501, 98.26951503753662, 98.94930601119995, 99.62909698486328, 100.32264876365662, 101.01620054244995, 101.70691919326782, 102.3976378440857, 103.08980679512024, 103.78197574615479, 104.47962951660156, 105.17728328704834, 105.8759913444519, 106.57469940185547, 107.31014752388, 108.04559564590454, 108.76964259147644, 109.49368953704834, 110.23267316818237, 110.9716567993164, 111.73048090934753, 112.48930501937866, 113.23382234573364, 113.97833967208862, 114.72530364990234, 115.47226762771606, 116.19974684715271, 116.92722606658936, 117.66055655479431, 118.39388704299927, 119.12491989135742, 119.85595273971558, 120.6110737323761, 121.36619472503662, 122.09052515029907, 122.81485557556152, 123.55210733413696, 124.2893590927124, 125.02118611335754, 125.75301313400269, 126.49225187301636, 127.23149061203003, 127.9664056301117, 128.70132064819336, 129.43712878227234, 130.17293691635132, 130.86677980422974, 131.56062269210815, 132.29231572151184, 133.02400875091553, 133.75728511810303, 134.49056148529053, 135.1908564567566, 135.89115142822266, 136.61934566497803, 137.3475399017334, 138.0786714553833, 138.8098030090332, 139.5616853237152, 140.31356763839722, 141.0194284915924, 141.7252893447876, 142.4635078907013, 143.201726436615, 143.8791422843933, 144.55655813217163, 145.23014163970947, 145.90372514724731, 146.63975191116333, 147.37577867507935, 148.8457794189453, 150.31578016281128]
[22.15, 22.15, 37.6, 37.6, 46.46666666666667, 46.46666666666667, 49.4, 49.4, 54.516666666666666, 54.516666666666666, 56.81666666666667, 56.81666666666667, 63.45, 63.45, 67.55, 67.55, 66.23333333333333, 66.23333333333333, 67.63333333333334, 67.63333333333334, 68.58333333333333, 68.58333333333333, 69.33333333333333, 69.33333333333333, 68.48333333333333, 68.48333333333333, 69.03333333333333, 69.03333333333333, 69.68333333333334, 69.68333333333334, 68.86666666666666, 68.86666666666666, 69.55, 69.55, 70.21666666666667, 70.21666666666667, 70.48333333333333, 70.48333333333333, 72.26666666666667, 72.26666666666667, 72.43333333333334, 72.43333333333334, 72.41666666666667, 72.41666666666667, 72.5, 72.5, 72.76666666666667, 72.76666666666667, 73.63333333333334, 73.63333333333334, 72.91666666666667, 72.91666666666667, 72.78333333333333, 72.78333333333333, 73.4, 73.4, 73.9, 73.9, 74.0, 74.0, 74.26666666666667, 74.26666666666667, 74.71666666666667, 74.71666666666667, 75.3, 75.3, 75.3, 75.3, 75.63333333333334, 75.63333333333334, 75.68333333333334, 75.68333333333334, 74.88333333333334, 74.88333333333334, 75.23333333333333, 75.23333333333333, 75.8, 75.8, 76.05, 76.05, 76.73333333333333, 76.73333333333333, 76.8, 76.8, 76.26666666666667, 76.26666666666667, 76.23333333333333, 76.23333333333333, 76.36666666666666, 76.36666666666666, 76.96666666666667, 76.96666666666667, 76.28333333333333, 76.28333333333333, 77.06666666666666, 77.06666666666666, 76.68333333333334, 76.68333333333334, 76.66666666666667, 76.66666666666667, 77.56666666666666, 77.56666666666666, 77.08333333333333, 77.08333333333333, 77.26666666666667, 77.26666666666667, 76.91666666666667, 76.91666666666667, 77.05, 77.05, 76.31666666666666, 76.31666666666666, 76.18333333333334, 76.18333333333334, 77.31666666666666, 77.31666666666666, 76.95, 76.95, 76.68333333333334, 76.68333333333334, 76.83333333333333, 76.83333333333333, 77.4, 77.4, 78.13333333333334, 78.13333333333334, 77.98333333333333, 77.98333333333333, 77.53333333333333, 77.53333333333333, 77.5, 77.5, 77.83333333333333, 77.83333333333333, 78.13333333333334, 78.13333333333334, 77.86666666666666, 77.86666666666666, 78.06666666666666, 78.06666666666666, 78.53333333333333, 78.53333333333333, 77.6, 77.6, 78.26666666666667, 78.26666666666667, 77.81666666666666, 77.81666666666666, 78.33333333333333, 78.33333333333333, 78.26666666666667, 78.26666666666667, 77.48333333333333, 77.48333333333333, 77.0, 77.0, 76.93333333333334, 76.93333333333334, 77.21666666666667, 77.21666666666667, 77.2, 77.2, 77.85, 77.85, 78.33333333333333, 78.33333333333333, 78.41666666666667, 78.41666666666667, 78.2, 78.2, 77.95, 77.95, 78.05, 78.05, 77.95, 77.95, 78.35, 78.35, 77.91666666666667, 77.91666666666667, 78.58333333333333, 78.58333333333333, 79.38333333333334, 79.38333333333334, 79.58333333333333, 79.58333333333333, 80.23333333333333, 80.23333333333333, 79.9, 79.9, 79.38333333333334, 79.38333333333334, 79.33333333333333, 79.33333333333333, 78.91666666666667, 78.91666666666667, 79.7, 79.7, 80.06666666666666, 80.06666666666666, 79.45, 79.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.602, Test loss: 2.579, Test accuracy: 16.45
Round   1, Train loss: 1.033, Test loss: 1.808, Test accuracy: 30.65
Round   2, Train loss: 0.949, Test loss: 1.751, Test accuracy: 36.00
Round   3, Train loss: 0.892, Test loss: 1.385, Test accuracy: 44.27
Round   4, Train loss: 0.966, Test loss: 1.376, Test accuracy: 42.42
Round   5, Train loss: 0.834, Test loss: 1.329, Test accuracy: 49.17
Round   6, Train loss: 0.934, Test loss: 1.196, Test accuracy: 49.23
Round   7, Train loss: 0.904, Test loss: 1.118, Test accuracy: 55.15
Round   8, Train loss: 0.816, Test loss: 0.959, Test accuracy: 57.05
Round   9, Train loss: 0.712, Test loss: 0.955, Test accuracy: 59.48
Round  10, Train loss: 0.807, Test loss: 0.870, Test accuracy: 59.48
Round  11, Train loss: 0.723, Test loss: 0.811, Test accuracy: 63.75
Round  12, Train loss: 0.668, Test loss: 0.871, Test accuracy: 63.70
Round  13, Train loss: 0.747, Test loss: 0.803, Test accuracy: 64.35
Round  14, Train loss: 0.680, Test loss: 0.754, Test accuracy: 64.95
Round  15, Train loss: 0.744, Test loss: 0.770, Test accuracy: 67.42
Round  16, Train loss: 0.707, Test loss: 0.835, Test accuracy: 66.17
Round  17, Train loss: 0.743, Test loss: 0.691, Test accuracy: 69.10
Round  18, Train loss: 0.672, Test loss: 0.684, Test accuracy: 69.27
Round  19, Train loss: 0.644, Test loss: 0.672, Test accuracy: 70.18
Round  20, Train loss: 0.707, Test loss: 0.651, Test accuracy: 71.08
Round  21, Train loss: 0.636, Test loss: 0.645, Test accuracy: 70.78
Round  22, Train loss: 0.614, Test loss: 0.649, Test accuracy: 71.37
Round  23, Train loss: 0.610, Test loss: 0.645, Test accuracy: 71.20
Round  24, Train loss: 0.486, Test loss: 0.624, Test accuracy: 72.05
Round  25, Train loss: 0.616, Test loss: 0.623, Test accuracy: 72.13
Round  26, Train loss: 0.581, Test loss: 0.621, Test accuracy: 72.42
Round  27, Train loss: 0.593, Test loss: 0.622, Test accuracy: 72.12
Round  28, Train loss: 0.679, Test loss: 0.609, Test accuracy: 72.75
Round  29, Train loss: 0.636, Test loss: 0.612, Test accuracy: 73.40
Round  30, Train loss: 0.579, Test loss: 0.593, Test accuracy: 74.38
Round  31, Train loss: 0.578, Test loss: 0.592, Test accuracy: 74.37
Round  32, Train loss: 0.535, Test loss: 0.590, Test accuracy: 74.48
Round  33, Train loss: 0.611, Test loss: 0.591, Test accuracy: 74.10
Round  34, Train loss: 0.546, Test loss: 0.591, Test accuracy: 74.25
Round  35, Train loss: 0.603, Test loss: 0.588, Test accuracy: 74.30
Round  36, Train loss: 0.613, Test loss: 0.585, Test accuracy: 74.53
Round  37, Train loss: 0.551, Test loss: 0.569, Test accuracy: 75.42
Round  38, Train loss: 0.512, Test loss: 0.561, Test accuracy: 75.40
Round  39, Train loss: 0.581, Test loss: 0.576, Test accuracy: 75.18
Round  40, Train loss: 0.488, Test loss: 0.573, Test accuracy: 75.83
Round  41, Train loss: 0.513, Test loss: 0.566, Test accuracy: 75.75
Round  42, Train loss: 0.547, Test loss: 0.586, Test accuracy: 74.68
Round  43, Train loss: 0.511, Test loss: 0.572, Test accuracy: 75.70
Round  44, Train loss: 0.559, Test loss: 0.557, Test accuracy: 76.48
Round  45, Train loss: 0.535, Test loss: 0.547, Test accuracy: 76.78
Round  46, Train loss: 0.566, Test loss: 0.547, Test accuracy: 76.45
Round  47, Train loss: 0.548, Test loss: 0.543, Test accuracy: 76.50
Round  48, Train loss: 0.484, Test loss: 0.541, Test accuracy: 76.55
Round  49, Train loss: 0.430, Test loss: 0.540, Test accuracy: 77.33
Round  50, Train loss: 0.418, Test loss: 0.551, Test accuracy: 76.38
Round  51, Train loss: 0.454, Test loss: 0.543, Test accuracy: 77.22
Round  52, Train loss: 0.509, Test loss: 0.536, Test accuracy: 77.45
Round  53, Train loss: 0.555, Test loss: 0.543, Test accuracy: 76.50
Round  54, Train loss: 0.486, Test loss: 0.540, Test accuracy: 76.90
Round  55, Train loss: 0.404, Test loss: 0.537, Test accuracy: 77.33
Round  56, Train loss: 0.463, Test loss: 0.547, Test accuracy: 76.85
Round  57, Train loss: 0.510, Test loss: 0.541, Test accuracy: 77.02
Round  58, Train loss: 0.545, Test loss: 0.532, Test accuracy: 76.97
Round  59, Train loss: 0.495, Test loss: 0.532, Test accuracy: 77.40
Round  60, Train loss: 0.417, Test loss: 0.523, Test accuracy: 77.68
Round  61, Train loss: 0.455, Test loss: 0.530, Test accuracy: 77.97
Round  62, Train loss: 0.422, Test loss: 0.531, Test accuracy: 77.78
Round  63, Train loss: 0.494, Test loss: 0.517, Test accuracy: 78.32
Round  64, Train loss: 0.399, Test loss: 0.516, Test accuracy: 78.50
Round  65, Train loss: 0.484, Test loss: 0.512, Test accuracy: 78.27
Round  66, Train loss: 0.495, Test loss: 0.515, Test accuracy: 78.10
Round  67, Train loss: 0.459, Test loss: 0.521, Test accuracy: 77.92
Round  68, Train loss: 0.360, Test loss: 0.525, Test accuracy: 77.70
Round  69, Train loss: 0.398, Test loss: 0.513, Test accuracy: 78.33
Round  70, Train loss: 0.388, Test loss: 0.513, Test accuracy: 78.85
Round  71, Train loss: 0.362, Test loss: 0.503, Test accuracy: 79.47
Round  72, Train loss: 0.431, Test loss: 0.503, Test accuracy: 79.02
Round  73, Train loss: 0.370, Test loss: 0.509, Test accuracy: 78.82
Round  74, Train loss: 0.360, Test loss: 0.507, Test accuracy: 79.12
Round  75, Train loss: 0.484, Test loss: 0.501, Test accuracy: 79.37
Round  76, Train loss: 0.404, Test loss: 0.502, Test accuracy: 79.22
Round  77, Train loss: 0.374, Test loss: 0.507, Test accuracy: 78.58
Round  78, Train loss: 0.439, Test loss: 0.500, Test accuracy: 79.12
Round  79, Train loss: 0.464, Test loss: 0.502, Test accuracy: 79.52
Round  80, Train loss: 0.415, Test loss: 0.514, Test accuracy: 78.83
Round  81, Train loss: 0.401, Test loss: 0.515, Test accuracy: 78.58
Round  82, Train loss: 0.329, Test loss: 0.503, Test accuracy: 79.10
Round  83, Train loss: 0.427, Test loss: 0.491, Test accuracy: 79.55
Round  84, Train loss: 0.405, Test loss: 0.492, Test accuracy: 79.67
Round  85, Train loss: 0.398, Test loss: 0.488, Test accuracy: 79.78
Round  86, Train loss: 0.357, Test loss: 0.498, Test accuracy: 79.57
Round  87, Train loss: 0.414, Test loss: 0.507, Test accuracy: 79.30
Round  88, Train loss: 0.333, Test loss: 0.491, Test accuracy: 80.12
Round  89, Train loss: 0.286, Test loss: 0.495, Test accuracy: 80.12
Round  90, Train loss: 0.379, Test loss: 0.499, Test accuracy: 79.87
Round  91, Train loss: 0.419, Test loss: 0.509, Test accuracy: 79.60
Round  92, Train loss: 0.324, Test loss: 0.502, Test accuracy: 79.32
Round  93, Train loss: 0.334, Test loss: 0.499, Test accuracy: 79.63
Round  94, Train loss: 0.339, Test loss: 0.500, Test accuracy: 79.68
Round  95, Train loss: 0.299, Test loss: 0.501, Test accuracy: 79.97
Round  96, Train loss: 0.372, Test loss: 0.506, Test accuracy: 79.75
Round  97, Train loss: 0.332, Test loss: 0.491, Test accuracy: 79.98
Round  98, Train loss: 0.274, Test loss: 0.503, Test accuracy: 80.63
Round  99, Train loss: 0.314, Test loss: 0.497, Test accuracy: 79.97
Final Round, Train loss: 0.300, Test loss: 0.502, Test accuracy: 79.78
Average accuracy final 10 rounds: 79.84000000000002 

886.2496676445007
[1.048642873764038, 2.097285747528076, 2.8429269790649414, 3.5885682106018066, 4.3420469760894775, 5.095525741577148, 5.838763475418091, 6.582001209259033, 7.320467472076416, 8.058933734893799, 8.807522296905518, 9.556110858917236, 10.311336278915405, 11.066561698913574, 11.82081913948059, 12.575076580047607, 13.326009511947632, 14.076942443847656, 14.827371835708618, 15.57780122756958, 16.334072828292847, 17.090344429016113, 17.86374020576477, 18.637135982513428, 19.403477668762207, 20.169819355010986, 20.924402236938477, 21.678985118865967, 22.423481225967407, 23.167977333068848, 23.91969060897827, 24.671403884887695, 25.416104078292847, 26.160804271697998, 26.90870976448059, 27.656615257263184, 28.42044234275818, 29.184269428253174, 29.95123529434204, 30.718201160430908, 31.47649121284485, 32.23478126525879, 32.97708439826965, 33.71938753128052, 34.46906399726868, 35.218740463256836, 35.97708010673523, 36.73541975021362, 37.49430751800537, 38.25319528579712, 39.004257678985596, 39.75532007217407, 40.526020765304565, 41.29672145843506, 42.079806089401245, 42.86289072036743, 43.65446448326111, 44.446038246154785, 45.19062089920044, 45.935203552246094, 46.652297496795654, 47.369391441345215, 48.09191393852234, 48.81443643569946, 49.530534982681274, 50.246633529663086, 50.9663941860199, 51.68615484237671, 52.46326994895935, 53.24038505554199, 53.9893114566803, 54.7382378578186, 55.492350816726685, 56.246463775634766, 56.98870062828064, 57.730937480926514, 58.47666096687317, 59.222384452819824, 59.96495580673218, 60.70752716064453, 61.457245111465454, 62.20696306228638, 62.953314542770386, 63.699666023254395, 64.46592879295349, 65.23219156265259, 66.01144623756409, 66.79070091247559, 67.55237460136414, 68.31404829025269, 69.0526270866394, 69.79120588302612, 70.52513980865479, 71.25907373428345, 71.99603319168091, 72.73299264907837, 73.48492813110352, 74.23686361312866, 74.99077105522156, 75.74467849731445, 76.50984025001526, 77.27500200271606, 78.02890181541443, 78.7828016281128, 79.54261302947998, 80.30242443084717, 81.03685712814331, 81.77128982543945, 82.5240490436554, 83.27680826187134, 84.02250170707703, 84.76819515228271, 85.50970220565796, 86.2512092590332, 86.99405121803284, 87.73689317703247, 88.50268864631653, 89.26848411560059, 90.03013730049133, 90.79179048538208, 91.54673361778259, 92.3016767501831, 93.04868221282959, 93.79568767547607, 94.54508924484253, 95.29449081420898, 96.03941345214844, 96.78433609008789, 97.53058242797852, 98.27682876586914, 99.0360860824585, 99.79534339904785, 100.56069850921631, 101.32605361938477, 102.08289694786072, 102.83974027633667, 103.60580348968506, 104.37186670303345, 105.1166729927063, 105.86147928237915, 106.60342979431152, 107.3453803062439, 108.08956289291382, 108.83374547958374, 109.58475399017334, 110.33576250076294, 111.08845400810242, 111.8411455154419, 112.61665153503418, 113.39215755462646, 114.1466588973999, 114.90116024017334, 115.66329312324524, 116.42542600631714, 117.15772104263306, 117.89001607894897, 118.63711142539978, 119.38420677185059, 120.1298303604126, 120.87545394897461, 121.6246976852417, 122.37394142150879, 123.13279914855957, 123.89165687561035, 124.65011882781982, 125.4085807800293, 126.16656494140625, 126.9245491027832, 127.68015956878662, 128.43577003479004, 129.18542194366455, 129.93507385253906, 130.6879744529724, 131.44087505340576, 132.1811397075653, 132.92140436172485, 133.65843510627747, 134.39546585083008, 135.15187907218933, 135.90829229354858, 136.66834354400635, 137.4283947944641, 138.18887901306152, 138.94936323165894, 139.70940446853638, 140.46944570541382, 141.21180748939514, 141.95416927337646, 142.69995760917664, 143.4457459449768, 144.1804964542389, 144.91524696350098, 145.66207814216614, 146.4089093208313, 147.1588671207428, 147.9088249206543, 148.6530740261078, 149.39732313156128, 150.13307905197144, 150.8688349723816, 152.20459294319153, 153.54035091400146]
[16.45, 16.45, 30.65, 30.65, 36.0, 36.0, 44.266666666666666, 44.266666666666666, 42.416666666666664, 42.416666666666664, 49.166666666666664, 49.166666666666664, 49.233333333333334, 49.233333333333334, 55.15, 55.15, 57.05, 57.05, 59.483333333333334, 59.483333333333334, 59.483333333333334, 59.483333333333334, 63.75, 63.75, 63.7, 63.7, 64.35, 64.35, 64.95, 64.95, 67.41666666666667, 67.41666666666667, 66.16666666666667, 66.16666666666667, 69.1, 69.1, 69.26666666666667, 69.26666666666667, 70.18333333333334, 70.18333333333334, 71.08333333333333, 71.08333333333333, 70.78333333333333, 70.78333333333333, 71.36666666666666, 71.36666666666666, 71.2, 71.2, 72.05, 72.05, 72.13333333333334, 72.13333333333334, 72.41666666666667, 72.41666666666667, 72.11666666666666, 72.11666666666666, 72.75, 72.75, 73.4, 73.4, 74.38333333333334, 74.38333333333334, 74.36666666666666, 74.36666666666666, 74.48333333333333, 74.48333333333333, 74.1, 74.1, 74.25, 74.25, 74.3, 74.3, 74.53333333333333, 74.53333333333333, 75.41666666666667, 75.41666666666667, 75.4, 75.4, 75.18333333333334, 75.18333333333334, 75.83333333333333, 75.83333333333333, 75.75, 75.75, 74.68333333333334, 74.68333333333334, 75.7, 75.7, 76.48333333333333, 76.48333333333333, 76.78333333333333, 76.78333333333333, 76.45, 76.45, 76.5, 76.5, 76.55, 76.55, 77.33333333333333, 77.33333333333333, 76.38333333333334, 76.38333333333334, 77.21666666666667, 77.21666666666667, 77.45, 77.45, 76.5, 76.5, 76.9, 76.9, 77.33333333333333, 77.33333333333333, 76.85, 76.85, 77.01666666666667, 77.01666666666667, 76.96666666666667, 76.96666666666667, 77.4, 77.4, 77.68333333333334, 77.68333333333334, 77.96666666666667, 77.96666666666667, 77.78333333333333, 77.78333333333333, 78.31666666666666, 78.31666666666666, 78.5, 78.5, 78.26666666666667, 78.26666666666667, 78.1, 78.1, 77.91666666666667, 77.91666666666667, 77.7, 77.7, 78.33333333333333, 78.33333333333333, 78.85, 78.85, 79.46666666666667, 79.46666666666667, 79.01666666666667, 79.01666666666667, 78.81666666666666, 78.81666666666666, 79.11666666666666, 79.11666666666666, 79.36666666666666, 79.36666666666666, 79.21666666666667, 79.21666666666667, 78.58333333333333, 78.58333333333333, 79.11666666666666, 79.11666666666666, 79.51666666666667, 79.51666666666667, 78.83333333333333, 78.83333333333333, 78.58333333333333, 78.58333333333333, 79.1, 79.1, 79.55, 79.55, 79.66666666666667, 79.66666666666667, 79.78333333333333, 79.78333333333333, 79.56666666666666, 79.56666666666666, 79.3, 79.3, 80.11666666666666, 80.11666666666666, 80.11666666666666, 80.11666666666666, 79.86666666666666, 79.86666666666666, 79.6, 79.6, 79.31666666666666, 79.31666666666666, 79.63333333333334, 79.63333333333334, 79.68333333333334, 79.68333333333334, 79.96666666666667, 79.96666666666667, 79.75, 79.75, 79.98333333333333, 79.98333333333333, 80.63333333333334, 80.63333333333334, 79.96666666666667, 79.96666666666667, 79.78333333333333, 79.78333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.734, Test loss: 2.212, Test accuracy: 23.93
Round   1, Train loss: 1.072, Test loss: 1.772, Test accuracy: 37.60
Round   2, Train loss: 0.983, Test loss: 1.406, Test accuracy: 43.90
Round   3, Train loss: 1.008, Test loss: 1.350, Test accuracy: 49.48
Round   4, Train loss: 0.960, Test loss: 1.164, Test accuracy: 54.87
Round   5, Train loss: 0.891, Test loss: 0.965, Test accuracy: 57.68
Round   6, Train loss: 0.882, Test loss: 0.922, Test accuracy: 60.03
Round   7, Train loss: 0.801, Test loss: 0.915, Test accuracy: 61.37
Round   8, Train loss: 0.803, Test loss: 0.815, Test accuracy: 66.25
Round   9, Train loss: 0.811, Test loss: 0.789, Test accuracy: 67.15
Round  10, Train loss: 0.717, Test loss: 0.768, Test accuracy: 67.90
Round  11, Train loss: 0.773, Test loss: 0.747, Test accuracy: 68.82
Round  12, Train loss: 0.776, Test loss: 0.726, Test accuracy: 68.53
Round  13, Train loss: 0.673, Test loss: 0.712, Test accuracy: 70.32
Round  14, Train loss: 0.666, Test loss: 0.695, Test accuracy: 70.58
Round  15, Train loss: 0.637, Test loss: 0.680, Test accuracy: 71.23
Round  16, Train loss: 0.737, Test loss: 0.680, Test accuracy: 71.75
Round  17, Train loss: 0.702, Test loss: 0.658, Test accuracy: 72.40
Round  18, Train loss: 0.722, Test loss: 0.663, Test accuracy: 72.17
Round  19, Train loss: 0.687, Test loss: 0.688, Test accuracy: 71.58
Round  20, Train loss: 0.673, Test loss: 0.660, Test accuracy: 71.80
Round  21, Train loss: 0.679, Test loss: 0.652, Test accuracy: 72.82
Round  22, Train loss: 0.627, Test loss: 0.640, Test accuracy: 73.58
Round  23, Train loss: 0.674, Test loss: 0.652, Test accuracy: 72.82
Round  24, Train loss: 0.632, Test loss: 0.626, Test accuracy: 74.03
Round  25, Train loss: 0.604, Test loss: 0.623, Test accuracy: 74.22
Round  26, Train loss: 0.577, Test loss: 0.622, Test accuracy: 74.65
Round  27, Train loss: 0.631, Test loss: 0.623, Test accuracy: 74.72
Round  28, Train loss: 0.649, Test loss: 0.617, Test accuracy: 74.67
Round  29, Train loss: 0.620, Test loss: 0.613, Test accuracy: 74.50
Round  30, Train loss: 0.619, Test loss: 0.597, Test accuracy: 75.08
Round  31, Train loss: 0.603, Test loss: 0.599, Test accuracy: 75.20
Round  32, Train loss: 0.603, Test loss: 0.586, Test accuracy: 75.83
Round  33, Train loss: 0.523, Test loss: 0.584, Test accuracy: 76.02
Round  34, Train loss: 0.538, Test loss: 0.586, Test accuracy: 75.77
Round  35, Train loss: 0.579, Test loss: 0.576, Test accuracy: 76.42
Round  36, Train loss: 0.473, Test loss: 0.569, Test accuracy: 76.20
Round  37, Train loss: 0.602, Test loss: 0.567, Test accuracy: 76.38
Round  38, Train loss: 0.484, Test loss: 0.566, Test accuracy: 76.38
Round  39, Train loss: 0.528, Test loss: 0.550, Test accuracy: 76.68
Round  40, Train loss: 0.591, Test loss: 0.553, Test accuracy: 76.78
Round  41, Train loss: 0.474, Test loss: 0.553, Test accuracy: 77.08
Round  42, Train loss: 0.571, Test loss: 0.547, Test accuracy: 77.25
Round  43, Train loss: 0.507, Test loss: 0.546, Test accuracy: 77.07
Round  44, Train loss: 0.539, Test loss: 0.548, Test accuracy: 78.08
Round  45, Train loss: 0.475, Test loss: 0.542, Test accuracy: 77.78
Round  46, Train loss: 0.500, Test loss: 0.539, Test accuracy: 77.57
Round  47, Train loss: 0.539, Test loss: 0.531, Test accuracy: 77.78
Round  48, Train loss: 0.500, Test loss: 0.529, Test accuracy: 78.20
Round  49, Train loss: 0.531, Test loss: 0.531, Test accuracy: 78.40
Round  50, Train loss: 0.563, Test loss: 0.520, Test accuracy: 78.33
Round  51, Train loss: 0.534, Test loss: 0.522, Test accuracy: 78.30
Round  52, Train loss: 0.403, Test loss: 0.516, Test accuracy: 78.68
Round  53, Train loss: 0.495, Test loss: 0.516, Test accuracy: 78.68
Round  54, Train loss: 0.532, Test loss: 0.513, Test accuracy: 78.68
Round  55, Train loss: 0.459, Test loss: 0.509, Test accuracy: 78.98
Round  56, Train loss: 0.492, Test loss: 0.515, Test accuracy: 78.58
Round  57, Train loss: 0.500, Test loss: 0.505, Test accuracy: 79.12
Round  58, Train loss: 0.462, Test loss: 0.505, Test accuracy: 79.25
Round  59, Train loss: 0.411, Test loss: 0.494, Test accuracy: 79.53
Round  60, Train loss: 0.461, Test loss: 0.497, Test accuracy: 79.78
Round  61, Train loss: 0.439, Test loss: 0.494, Test accuracy: 80.12
Round  62, Train loss: 0.506, Test loss: 0.501, Test accuracy: 80.00
Round  63, Train loss: 0.485, Test loss: 0.491, Test accuracy: 79.85
Round  64, Train loss: 0.410, Test loss: 0.482, Test accuracy: 80.28
Round  65, Train loss: 0.430, Test loss: 0.486, Test accuracy: 80.27
Round  66, Train loss: 0.523, Test loss: 0.480, Test accuracy: 80.28
Round  67, Train loss: 0.484, Test loss: 0.483, Test accuracy: 80.13
Round  68, Train loss: 0.403, Test loss: 0.476, Test accuracy: 80.63
Round  69, Train loss: 0.464, Test loss: 0.471, Test accuracy: 80.68
Round  70, Train loss: 0.364, Test loss: 0.474, Test accuracy: 80.48
Round  71, Train loss: 0.445, Test loss: 0.477, Test accuracy: 80.87
Round  72, Train loss: 0.418, Test loss: 0.467, Test accuracy: 81.25
Round  73, Train loss: 0.460, Test loss: 0.475, Test accuracy: 80.73
Round  74, Train loss: 0.431, Test loss: 0.473, Test accuracy: 80.50
Round  75, Train loss: 0.419, Test loss: 0.469, Test accuracy: 80.65
Round  76, Train loss: 0.415, Test loss: 0.465, Test accuracy: 80.87
Round  77, Train loss: 0.391, Test loss: 0.460, Test accuracy: 81.37
Round  78, Train loss: 0.486, Test loss: 0.474, Test accuracy: 80.60
Round  79, Train loss: 0.379, Test loss: 0.467, Test accuracy: 81.18
Round  80, Train loss: 0.362, Test loss: 0.464, Test accuracy: 80.85
Round  81, Train loss: 0.375, Test loss: 0.458, Test accuracy: 81.20
Round  82, Train loss: 0.406, Test loss: 0.472, Test accuracy: 80.63
Round  83, Train loss: 0.342, Test loss: 0.457, Test accuracy: 81.62
Round  84, Train loss: 0.374, Test loss: 0.467, Test accuracy: 81.35
Round  85, Train loss: 0.373, Test loss: 0.468, Test accuracy: 81.02
Round  86, Train loss: 0.284, Test loss: 0.456, Test accuracy: 81.37
Round  87, Train loss: 0.345, Test loss: 0.454, Test accuracy: 81.83
Round  88, Train loss: 0.334, Test loss: 0.455, Test accuracy: 81.37
Round  89, Train loss: 0.396, Test loss: 0.441, Test accuracy: 82.15
Round  90, Train loss: 0.297, Test loss: 0.449, Test accuracy: 82.23
Round  91, Train loss: 0.413, Test loss: 0.447, Test accuracy: 82.33
Round  92, Train loss: 0.333, Test loss: 0.438, Test accuracy: 82.40
Round  93, Train loss: 0.341, Test loss: 0.447, Test accuracy: 81.97
Round  94, Train loss: 0.300, Test loss: 0.444, Test accuracy: 81.68
Round  95, Train loss: 0.392, Test loss: 0.443, Test accuracy: 81.97
Round  96, Train loss: 0.345, Test loss: 0.455, Test accuracy: 81.33
Round  97, Train loss: 0.312, Test loss: 0.449, Test accuracy: 81.82
Round  98, Train loss: 0.351, Test loss: 0.449, Test accuracy: 81.70
Round  99, Train loss: 0.358, Test loss: 0.453, Test accuracy: 81.28
Final Round, Train loss: 0.290, Test loss: 0.443, Test accuracy: 82.07
Average accuracy final 10 rounds: 81.87166666666667
941.4381170272827
[1.1988344192504883, 2.3976688385009766, 3.3278260231018066, 4.257983207702637, 5.170297384262085, 6.082611560821533, 7.009081125259399, 7.935550689697266, 8.797717809677124, 9.659884929656982, 10.559956550598145, 11.460028171539307, 12.343228816986084, 13.226429462432861, 14.098988771438599, 14.971548080444336, 15.910051822662354, 16.84855556488037, 17.77709150314331, 18.70562744140625, 19.644348621368408, 20.583069801330566, 21.50487756729126, 22.426685333251953, 23.305951595306396, 24.18521785736084, 25.03865957260132, 25.892101287841797, 26.73263645172119, 27.573171615600586, 28.444881916046143, 29.3165922164917, 30.20721411705017, 31.097836017608643, 31.97515320777893, 32.85247039794922, 33.739219665527344, 34.62596893310547, 35.4910843372345, 36.356199741363525, 37.22873497009277, 38.10127019882202, 38.9804744720459, 39.859678745269775, 40.728759765625, 41.597840785980225, 42.46105074882507, 43.32426071166992, 44.18643641471863, 45.048612117767334, 45.92430901527405, 46.80000591278076, 47.67929744720459, 48.55858898162842, 49.42593264579773, 50.29327630996704, 51.158347845077515, 52.02341938018799, 52.887202978134155, 53.75098657608032, 54.62342095375061, 55.4958553314209, 56.37510538101196, 57.25435543060303, 58.13250017166138, 59.01064491271973, 59.88366460800171, 60.75668430328369, 61.60951519012451, 62.46234607696533, 63.33164310455322, 64.20094013214111, 65.06553530693054, 65.93013048171997, 66.79625654220581, 67.66238260269165, 68.55908846855164, 69.45579433441162, 70.33180689811707, 71.20781946182251, 72.1027672290802, 72.99771499633789, 73.84853029251099, 74.69934558868408, 75.55524063110352, 76.41113567352295, 77.27318096160889, 78.13522624969482, 79.00338888168335, 79.87155151367188, 80.74400472640991, 81.61645793914795, 82.49714541435242, 83.37783288955688, 84.26087427139282, 85.14391565322876, 86.03950428962708, 86.93509292602539, 87.79939484596252, 88.66369676589966, 89.53643798828125, 90.40917921066284, 91.26563286781311, 92.12208652496338, 92.96796202659607, 93.81383752822876, 94.71984529495239, 95.62585306167603, 96.51335668563843, 97.40086030960083, 98.32174396514893, 99.24262762069702, 100.1073431968689, 100.97205877304077, 101.8304443359375, 102.68882989883423, 103.55211758613586, 104.4154052734375, 105.27463912963867, 106.13387298583984, 107.05652594566345, 107.97917890548706, 108.86915850639343, 109.7591381072998, 110.61817717552185, 111.4772162437439, 112.33446860313416, 113.19172096252441, 114.06397533416748, 114.93622970581055, 115.80431985855103, 116.6724100112915, 117.54185128211975, 118.411292552948, 119.26378893852234, 120.11628532409668, 120.99295258522034, 121.869619846344, 122.74243330955505, 123.61524677276611, 124.48934674263, 125.3634467124939, 126.22651243209839, 127.08957815170288, 127.96239280700684, 128.8352074623108, 129.7066297531128, 130.5780520439148, 131.44027543067932, 132.30249881744385, 133.1941101551056, 134.08572149276733, 134.96148037910461, 135.8372392654419, 136.74644255638123, 137.65564584732056, 138.47729802131653, 139.2989501953125, 140.11768770217896, 140.9364252090454, 141.7654583454132, 142.594491481781, 143.41777682304382, 144.24106216430664, 145.09111261367798, 145.94116306304932, 146.78350520133972, 147.62584733963013, 148.45142817497253, 149.27700901031494, 150.1107633113861, 150.94451761245728, 151.76532816886902, 152.58613872528076, 153.42198991775513, 154.2578411102295, 155.08594679832458, 155.91405248641968, 156.73298263549805, 157.55191278457642, 158.3822832107544, 159.21265363693237, 160.03055906295776, 160.84846448898315, 161.68720889091492, 162.52595329284668, 163.35384106636047, 164.18172883987427, 165.0124659538269, 165.84320306777954, 166.66291618347168, 167.48262929916382, 168.301824092865, 169.12101888656616, 169.9557318687439, 170.79044485092163, 171.63089323043823, 172.47134160995483, 173.31121730804443, 174.15109300613403, 175.43524169921875, 176.71939039230347]
[23.933333333333334, 23.933333333333334, 37.6, 37.6, 43.9, 43.9, 49.483333333333334, 49.483333333333334, 54.86666666666667, 54.86666666666667, 57.68333333333333, 57.68333333333333, 60.03333333333333, 60.03333333333333, 61.36666666666667, 61.36666666666667, 66.25, 66.25, 67.15, 67.15, 67.9, 67.9, 68.81666666666666, 68.81666666666666, 68.53333333333333, 68.53333333333333, 70.31666666666666, 70.31666666666666, 70.58333333333333, 70.58333333333333, 71.23333333333333, 71.23333333333333, 71.75, 71.75, 72.4, 72.4, 72.16666666666667, 72.16666666666667, 71.58333333333333, 71.58333333333333, 71.8, 71.8, 72.81666666666666, 72.81666666666666, 73.58333333333333, 73.58333333333333, 72.81666666666666, 72.81666666666666, 74.03333333333333, 74.03333333333333, 74.21666666666667, 74.21666666666667, 74.65, 74.65, 74.71666666666667, 74.71666666666667, 74.66666666666667, 74.66666666666667, 74.5, 74.5, 75.08333333333333, 75.08333333333333, 75.2, 75.2, 75.83333333333333, 75.83333333333333, 76.01666666666667, 76.01666666666667, 75.76666666666667, 75.76666666666667, 76.41666666666667, 76.41666666666667, 76.2, 76.2, 76.38333333333334, 76.38333333333334, 76.38333333333334, 76.38333333333334, 76.68333333333334, 76.68333333333334, 76.78333333333333, 76.78333333333333, 77.08333333333333, 77.08333333333333, 77.25, 77.25, 77.06666666666666, 77.06666666666666, 78.08333333333333, 78.08333333333333, 77.78333333333333, 77.78333333333333, 77.56666666666666, 77.56666666666666, 77.78333333333333, 77.78333333333333, 78.2, 78.2, 78.4, 78.4, 78.33333333333333, 78.33333333333333, 78.3, 78.3, 78.68333333333334, 78.68333333333334, 78.68333333333334, 78.68333333333334, 78.68333333333334, 78.68333333333334, 78.98333333333333, 78.98333333333333, 78.58333333333333, 78.58333333333333, 79.11666666666666, 79.11666666666666, 79.25, 79.25, 79.53333333333333, 79.53333333333333, 79.78333333333333, 79.78333333333333, 80.11666666666666, 80.11666666666666, 80.0, 80.0, 79.85, 79.85, 80.28333333333333, 80.28333333333333, 80.26666666666667, 80.26666666666667, 80.28333333333333, 80.28333333333333, 80.13333333333334, 80.13333333333334, 80.63333333333334, 80.63333333333334, 80.68333333333334, 80.68333333333334, 80.48333333333333, 80.48333333333333, 80.86666666666666, 80.86666666666666, 81.25, 81.25, 80.73333333333333, 80.73333333333333, 80.5, 80.5, 80.65, 80.65, 80.86666666666666, 80.86666666666666, 81.36666666666666, 81.36666666666666, 80.6, 80.6, 81.18333333333334, 81.18333333333334, 80.85, 80.85, 81.2, 81.2, 80.63333333333334, 80.63333333333334, 81.61666666666666, 81.61666666666666, 81.35, 81.35, 81.01666666666667, 81.01666666666667, 81.36666666666666, 81.36666666666666, 81.83333333333333, 81.83333333333333, 81.36666666666666, 81.36666666666666, 82.15, 82.15, 82.23333333333333, 82.23333333333333, 82.33333333333333, 82.33333333333333, 82.4, 82.4, 81.96666666666667, 81.96666666666667, 81.68333333333334, 81.68333333333334, 81.96666666666667, 81.96666666666667, 81.33333333333333, 81.33333333333333, 81.81666666666666, 81.81666666666666, 81.7, 81.7, 81.28333333333333, 81.28333333333333, 82.06666666666666, 82.06666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.209, Test loss: 2.304, Test accuracy: 21.30
Round   1, Train loss: 1.037, Test loss: 2.270, Test accuracy: 22.18
Round   2, Train loss: 1.032, Test loss: 2.210, Test accuracy: 17.33
Round   3, Train loss: 0.911, Test loss: 2.209, Test accuracy: 25.72
Round   4, Train loss: 0.931, Test loss: 2.085, Test accuracy: 25.67
Round   5, Train loss: 0.878, Test loss: 2.362, Test accuracy: 20.20
Round   6, Train loss: 0.766, Test loss: 2.128, Test accuracy: 26.18
Round   7, Train loss: 0.841, Test loss: 2.049, Test accuracy: 24.32
Round   8, Train loss: 0.747, Test loss: 1.854, Test accuracy: 32.35
Round   9, Train loss: 0.772, Test loss: 1.932, Test accuracy: 30.55
Round  10, Train loss: 0.705, Test loss: 2.027, Test accuracy: 32.55
Round  11, Train loss: 0.767, Test loss: 1.860, Test accuracy: 33.85
Round  12, Train loss: 0.769, Test loss: 1.872, Test accuracy: 34.77
Round  13, Train loss: 0.795, Test loss: 1.969, Test accuracy: 35.67
Round  14, Train loss: 0.728, Test loss: 1.593, Test accuracy: 42.13
Round  15, Train loss: 0.636, Test loss: 1.811, Test accuracy: 36.83
Round  16, Train loss: 0.704, Test loss: 1.539, Test accuracy: 44.83
Round  17, Train loss: 0.722, Test loss: 2.080, Test accuracy: 30.73
Round  18, Train loss: 0.632, Test loss: 1.715, Test accuracy: 36.03
Round  19, Train loss: 0.629, Test loss: 1.740, Test accuracy: 38.75
Round  20, Train loss: 0.583, Test loss: 1.801, Test accuracy: 39.27
Round  21, Train loss: 0.549, Test loss: 1.865, Test accuracy: 38.68
Round  22, Train loss: 0.650, Test loss: 1.555, Test accuracy: 43.45
Round  23, Train loss: 0.617, Test loss: 1.635, Test accuracy: 44.10
Round  24, Train loss: 0.568, Test loss: 1.612, Test accuracy: 43.82
Round  25, Train loss: 0.576, Test loss: 1.759, Test accuracy: 39.17
Round  26, Train loss: 0.625, Test loss: 1.570, Test accuracy: 43.25
Round  27, Train loss: 0.574, Test loss: 1.548, Test accuracy: 42.47
Round  28, Train loss: 0.541, Test loss: 1.672, Test accuracy: 39.83
Round  29, Train loss: 0.491, Test loss: 1.589, Test accuracy: 42.85
Round  30, Train loss: 0.602, Test loss: 1.496, Test accuracy: 46.50
Round  31, Train loss: 0.589, Test loss: 1.555, Test accuracy: 45.38
Round  32, Train loss: 0.517, Test loss: 1.664, Test accuracy: 41.57
Round  33, Train loss: 0.454, Test loss: 1.503, Test accuracy: 47.18
Round  34, Train loss: 0.505, Test loss: 1.584, Test accuracy: 45.78
Round  35, Train loss: 0.473, Test loss: 1.396, Test accuracy: 50.90
Round  36, Train loss: 0.515, Test loss: 1.650, Test accuracy: 41.98
Round  37, Train loss: 0.503, Test loss: 1.933, Test accuracy: 39.73
Round  38, Train loss: 0.456, Test loss: 1.531, Test accuracy: 46.80
Round  39, Train loss: 0.494, Test loss: 2.286, Test accuracy: 33.28
Round  40, Train loss: 0.532, Test loss: 1.442, Test accuracy: 48.90
Round  41, Train loss: 0.505, Test loss: 1.556, Test accuracy: 46.55
Round  42, Train loss: 0.379, Test loss: 1.612, Test accuracy: 48.77
Round  43, Train loss: 0.435, Test loss: 1.697, Test accuracy: 44.48
Round  44, Train loss: 0.447, Test loss: 1.571, Test accuracy: 45.08
Round  45, Train loss: 0.401, Test loss: 1.360, Test accuracy: 52.33
Round  46, Train loss: 0.413, Test loss: 1.551, Test accuracy: 45.37
Round  47, Train loss: 0.462, Test loss: 1.471, Test accuracy: 51.08
Round  48, Train loss: 0.332, Test loss: 1.922, Test accuracy: 41.00
Round  49, Train loss: 0.378, Test loss: 1.443, Test accuracy: 50.40
Round  50, Train loss: 0.338, Test loss: 1.596, Test accuracy: 46.62
Round  51, Train loss: 0.371, Test loss: 1.463, Test accuracy: 50.02
Round  52, Train loss: 0.409, Test loss: 1.574, Test accuracy: 47.70
Round  53, Train loss: 0.374, Test loss: 1.367, Test accuracy: 53.50
Round  54, Train loss: 0.430, Test loss: 1.990, Test accuracy: 40.83
Round  55, Train loss: 0.335, Test loss: 1.632, Test accuracy: 45.90
Round  56, Train loss: 0.398, Test loss: 1.544, Test accuracy: 47.78
Round  57, Train loss: 0.291, Test loss: 1.453, Test accuracy: 50.70
Round  58, Train loss: 0.335, Test loss: 1.505, Test accuracy: 51.38
Round  59, Train loss: 0.354, Test loss: 1.546, Test accuracy: 47.75
Round  60, Train loss: 0.410, Test loss: 1.406, Test accuracy: 50.30
Round  61, Train loss: 0.332, Test loss: 1.460, Test accuracy: 51.15
Round  62, Train loss: 0.358, Test loss: 1.347, Test accuracy: 53.58
Round  63, Train loss: 0.277, Test loss: 1.442, Test accuracy: 52.27
Round  64, Train loss: 0.299, Test loss: 1.598, Test accuracy: 50.00
Round  65, Train loss: 0.273, Test loss: 1.468, Test accuracy: 50.03
Round  66, Train loss: 0.272, Test loss: 1.476, Test accuracy: 53.53
Round  67, Train loss: 0.238, Test loss: 1.723, Test accuracy: 45.93
Round  68, Train loss: 0.367, Test loss: 1.370, Test accuracy: 52.47
Round  69, Train loss: 0.292, Test loss: 1.422, Test accuracy: 52.07
Round  70, Train loss: 0.322, Test loss: 1.381, Test accuracy: 53.33
Round  71, Train loss: 0.230, Test loss: 1.504, Test accuracy: 51.13
Round  72, Train loss: 0.298, Test loss: 1.507, Test accuracy: 49.35
Round  73, Train loss: 0.238, Test loss: 1.366, Test accuracy: 54.85
Round  74, Train loss: 0.358, Test loss: 2.057, Test accuracy: 44.35
Round  75, Train loss: 0.298, Test loss: 1.371, Test accuracy: 54.22
Round  76, Train loss: 0.291, Test loss: 1.289, Test accuracy: 57.08
Round  77, Train loss: 0.270, Test loss: 1.448, Test accuracy: 52.17
Round  78, Train loss: 0.262, Test loss: 1.373, Test accuracy: 53.50
Round  79, Train loss: 0.244, Test loss: 1.484, Test accuracy: 52.72
Round  80, Train loss: 0.283, Test loss: 1.567, Test accuracy: 52.55
Round  81, Train loss: 0.220, Test loss: 1.445, Test accuracy: 54.02
Round  82, Train loss: 0.285, Test loss: 1.729, Test accuracy: 46.73
Round  83, Train loss: 0.169, Test loss: 1.681, Test accuracy: 50.18
Round  84, Train loss: 0.214, Test loss: 1.517, Test accuracy: 54.92
Round  85, Train loss: 0.234, Test loss: 1.742, Test accuracy: 47.62
Round  86, Train loss: 0.229, Test loss: 1.703, Test accuracy: 50.20
Round  87, Train loss: 0.258, Test loss: 1.647, Test accuracy: 49.48
Round  88, Train loss: 0.261, Test loss: 1.422, Test accuracy: 53.52
Round  89, Train loss: 0.275, Test loss: 1.514, Test accuracy: 50.67
Round  90, Train loss: 0.165, Test loss: 1.490, Test accuracy: 54.27
Round  91, Train loss: 0.240, Test loss: 1.615, Test accuracy: 48.82
Round  92, Train loss: 0.217, Test loss: 1.688, Test accuracy: 48.37
Round  93, Train loss: 0.190, Test loss: 1.368, Test accuracy: 56.03
Round  94, Train loss: 0.231, Test loss: 1.706, Test accuracy: 48.78
Round  95, Train loss: 0.189, Test loss: 1.801, Test accuracy: 48.25
Round  96, Train loss: 0.122, Test loss: 1.874, Test accuracy: 48.27
Round  97, Train loss: 0.192, Test loss: 1.540, Test accuracy: 53.57
Round  98, Train loss: 0.173, Test loss: 1.500, Test accuracy: 55.52
Round  99, Train loss: 0.230, Test loss: 1.624, Test accuracy: 51.75
Final Round, Train loss: 0.202, Test loss: 1.286, Test accuracy: 58.82
Average accuracy final 10 rounds: 51.36166666666667
1339.1016280651093
[2.274071216583252, 4.298113584518433, 6.307745695114136, 8.30928659439087, 10.291242122650146, 12.3139488697052, 14.361600160598755, 16.379883527755737, 18.382004499435425, 20.412115812301636, 22.436214923858643, 24.429036140441895, 26.429527759552002, 28.430678606033325, 30.436991930007935, 32.459064245224, 34.45700001716614, 36.466413736343384, 38.456377029418945, 40.48005127906799, 42.50292754173279, 44.50061845779419, 46.481157064437866, 48.51504731178284, 50.552998781204224, 52.58318781852722, 54.59068012237549, 56.595842599868774, 58.60821199417114, 60.415029764175415, 62.25814914703369, 64.06568098068237, 65.86660742759705, 67.68025493621826, 69.52906560897827, 71.34751272201538, 73.15147662162781, 74.98158764839172, 76.81721234321594, 78.64970922470093, 80.45883846282959, 82.27951073646545, 84.09076046943665, 85.90822196006775, 87.7399754524231, 89.57888007164001, 91.41151118278503, 93.2162356376648, 95.01891827583313, 96.85474300384521, 98.684241771698, 100.48111414909363, 102.3122570514679, 104.19240617752075, 106.05363893508911, 107.91744947433472, 109.70449018478394, 111.52605938911438, 113.32628130912781, 115.14134788513184, 116.96502947807312, 118.79359340667725, 120.59053349494934, 122.44910287857056, 124.35452818870544, 126.22665476799011, 128.03237581253052, 129.85369873046875, 131.720308303833, 133.58869290351868, 135.427330493927, 137.3438220024109, 139.15515303611755, 140.94824743270874, 142.76327514648438, 144.5582308769226, 146.34567999839783, 148.13524079322815, 149.9611098766327, 151.78988933563232, 153.608740568161, 155.43905448913574, 157.32082748413086, 159.18111395835876, 160.95744514465332, 162.79088234901428, 164.6401789188385, 166.51402139663696, 168.37962412834167, 170.2466540336609, 172.14607310295105, 173.93817329406738, 175.79695653915405, 177.69547128677368, 179.61543035507202, 181.5197672843933, 183.40750908851624, 185.26114153862, 187.0473153591156, 188.8861517906189, 190.79975867271423]
[21.3, 22.183333333333334, 17.333333333333332, 25.716666666666665, 25.666666666666668, 20.2, 26.183333333333334, 24.316666666666666, 32.35, 30.55, 32.55, 33.85, 34.766666666666666, 35.666666666666664, 42.13333333333333, 36.833333333333336, 44.833333333333336, 30.733333333333334, 36.03333333333333, 38.75, 39.266666666666666, 38.68333333333333, 43.45, 44.1, 43.81666666666667, 39.166666666666664, 43.25, 42.46666666666667, 39.833333333333336, 42.85, 46.5, 45.38333333333333, 41.56666666666667, 47.18333333333333, 45.78333333333333, 50.9, 41.983333333333334, 39.733333333333334, 46.8, 33.28333333333333, 48.9, 46.55, 48.766666666666666, 44.483333333333334, 45.083333333333336, 52.333333333333336, 45.36666666666667, 51.083333333333336, 41.0, 50.4, 46.61666666666667, 50.016666666666666, 47.7, 53.5, 40.833333333333336, 45.9, 47.78333333333333, 50.7, 51.38333333333333, 47.75, 50.3, 51.15, 53.583333333333336, 52.266666666666666, 50.0, 50.03333333333333, 53.53333333333333, 45.93333333333333, 52.46666666666667, 52.06666666666667, 53.333333333333336, 51.13333333333333, 49.35, 54.85, 44.35, 54.21666666666667, 57.083333333333336, 52.166666666666664, 53.5, 52.71666666666667, 52.55, 54.016666666666666, 46.733333333333334, 50.18333333333333, 54.916666666666664, 47.61666666666667, 50.2, 49.483333333333334, 53.516666666666666, 50.666666666666664, 54.266666666666666, 48.81666666666667, 48.36666666666667, 56.03333333333333, 48.78333333333333, 48.25, 48.266666666666666, 53.56666666666667, 55.516666666666666, 51.75, 58.81666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.249, Test loss: 2.303, Test accuracy: 16.03
Round   0, Global train loss: 2.249, Global test loss: 2.305, Global test accuracy: 14.92
Round   1, Train loss: 2.277, Test loss: 2.301, Test accuracy: 17.30
Round   1, Global train loss: 2.277, Global test loss: 2.304, Global test accuracy: 13.48
Round   2, Train loss: 2.291, Test loss: 2.304, Test accuracy: 16.37
Round   2, Global train loss: 2.291, Global test loss: 2.307, Global test accuracy: 12.15
Round   3, Train loss: 2.353, Test loss: 2.315, Test accuracy: 14.78
Round   3, Global train loss: 2.353, Global test loss: 2.309, Global test accuracy: 11.72
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 11.55
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 12.93
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 12.93
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 11.07
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 9.70
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 8.45
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 10.07
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Average accuracy final 10 rounds: 11.666666666666663 

Average global accuracy final 10 rounds: 11.666666666666663 

2543.900179862976
[1.0784831047058105, 1.8872017860412598, 2.7087290287017822, 3.5260980129241943, 4.366840839385986, 5.2130396366119385, 6.0623557567596436, 6.894237995147705, 7.715674161911011, 8.542105913162231, 9.36080551147461, 10.172372817993164, 11.000876665115356, 11.816945552825928, 12.623632431030273, 13.44838547706604, 14.248084545135498, 15.062800645828247, 15.864699840545654, 16.6667218208313, 17.474514961242676, 18.297995567321777, 19.108709812164307, 19.933013677597046, 20.74271297454834, 21.570208311080933, 22.406555891036987, 23.217853546142578, 24.02373719215393, 24.82323670387268, 25.629642486572266, 26.439568042755127, 27.246857404708862, 28.059221029281616, 28.857872486114502, 29.672420263290405, 30.48642587661743, 31.29327130317688, 32.101717472076416, 32.92352533340454, 33.739015340805054, 34.55534768104553, 35.35912227630615, 36.16179704666138, 36.966315269470215, 37.76569175720215, 38.563239336013794, 39.382322549819946, 40.213475465774536, 41.030539989471436, 41.849246978759766, 42.66915321350098, 43.483890533447266, 44.29163098335266, 45.10908341407776, 45.91399693489075, 46.72391390800476, 47.54040813446045, 48.359052896499634, 49.172268867492676, 49.980998516082764, 50.79662489891052, 51.60755252838135, 52.42413854598999, 53.22751975059509, 54.04050350189209, 54.852606534957886, 55.6795449256897, 56.48633909225464, 57.303362131118774, 58.11701822280884, 58.93025207519531, 59.73279166221619, 60.54432535171509, 61.35615372657776, 62.16733360290527, 62.98391151428223, 63.792675733566284, 64.6128556728363, 65.42086291313171, 66.21681332588196, 67.0235493183136, 67.82578134536743, 68.62618350982666, 69.43998312950134, 70.2518961429596, 70.99413919448853, 71.8054838180542, 72.62549948692322, 73.45728492736816, 74.26743483543396, 75.09822201728821, 75.91114377975464, 76.72631692886353, 77.53554844856262, 78.34990191459656, 79.17234897613525, 79.98479151725769, 80.80977010726929, 81.62021827697754, 82.4367265701294, 83.2478358745575, 84.06520318984985, 84.87435817718506, 85.69717121124268, 86.51402950286865, 87.32896375656128, 88.15711641311646, 88.9676103591919, 89.78294348716736, 90.58897519111633, 91.40886187553406, 92.22132849693298, 93.03336954116821, 93.86438727378845, 94.67661309242249, 95.49822092056274, 96.31938147544861, 97.14110231399536, 97.96323895454407, 98.77613735198975, 99.61159491539001, 100.43153691291809, 101.25389742851257, 102.07108116149902, 102.89522337913513, 103.71919655799866, 104.53722357749939, 105.36901211738586, 106.19026327133179, 107.0104877948761, 107.83352828025818, 108.64550137519836, 109.46482491493225, 110.2870831489563, 111.11602973937988, 111.9447979927063, 112.75742220878601, 113.58747935295105, 114.39782047271729, 115.21997427940369, 116.04592323303223, 116.8559672832489, 117.66959238052368, 118.48344445228577, 119.31460046768188, 120.13125729560852, 120.93900227546692, 121.74170708656311, 122.55913066864014, 123.36798048019409, 124.17986917495728, 124.98150134086609, 125.7900071144104, 126.5973892211914, 127.41118383407593, 128.23213124275208, 129.04321455955505, 129.87518906593323, 130.68919110298157, 131.51124668121338, 132.32522249221802, 133.13724541664124, 133.95958638191223, 134.77073860168457, 135.59194993972778, 136.40519976615906, 137.2275424003601, 138.0462031364441, 138.86042523384094, 139.68128085136414, 140.50328373908997, 141.31569957733154, 142.1308844089508, 142.95060658454895, 143.7060296535492, 144.47888731956482, 145.20170760154724, 145.92564940452576, 146.6492578983307, 147.3661687374115, 148.09976840019226, 148.804584980011, 149.5146405696869, 150.23124742507935, 150.9438726902008, 151.70088863372803, 152.4153037071228, 153.13185000419617, 153.86344408988953, 154.57406783103943, 155.28871631622314, 156.0016655921936, 156.71292304992676, 157.42656326293945, 158.15573811531067, 158.8806915283203, 159.5945508480072, 160.3181483745575, 161.02920413017273, 161.75619077682495, 162.49283719062805, 163.20546054840088, 163.92809653282166, 164.64847469329834, 165.36827063560486, 166.08286094665527, 166.80591344833374, 167.53219628334045, 168.24661374092102, 168.97519373893738, 169.70257186889648, 170.42214250564575, 171.14462971687317, 171.89913845062256, 172.61641240119934, 173.3269374370575, 174.03454780578613, 174.75478601455688, 175.4733591079712, 176.19930171966553, 176.91677165031433, 177.68285369873047, 178.40042734146118, 179.1264808177948, 179.83426547050476, 180.54957604408264, 181.27116060256958, 181.9807584285736, 182.69874215126038, 183.410325050354, 184.12026953697205, 184.83992385864258, 185.55078840255737, 186.27480936050415, 186.9834485054016, 187.70487689971924, 188.42891216278076, 189.13580179214478, 189.86069869995117, 190.58005809783936, 191.30155038833618, 192.0127148628235, 192.72373151779175, 193.44710779190063, 194.1632740497589, 194.88201475143433, 195.60047888755798, 196.32095050811768, 197.05083346366882, 197.7630250453949, 198.4859573841095, 199.19331097602844, 199.91512632369995, 200.61760258674622, 201.33319067955017, 202.05660700798035, 202.78147435188293, 203.51138305664062, 204.224506855011, 204.94681596755981, 205.6676881313324, 206.37544202804565, 207.1002380847931, 207.8247091770172, 208.5383722782135, 209.2554578781128, 209.97710156440735, 210.69886422157288, 211.4164674282074, 212.1353120803833, 212.85090708732605, 213.57703971862793, 214.30318975448608, 215.02109718322754, 215.74737811088562, 216.4632761478424, 217.17969465255737, 217.88594031333923, 218.59943079948425, 219.36348700523376, 220.07621216773987, 220.79806900024414, 221.515531539917, 222.23341846466064, 222.95057606697083, 223.65647172927856, 224.3745048046112, 225.09116435050964, 225.8031234741211, 226.51499724388123, 227.236665725708, 227.94544649124146, 228.65623259544373, 229.36563968658447, 230.0729055404663, 230.78612875938416, 231.5097200870514, 232.22021055221558, 232.93151330947876, 234.36192154884338]
[16.033333333333335, 17.3, 16.366666666666667, 14.783333333333333, 11.55, 12.933333333333334, 12.933333333333334, 11.066666666666666, 9.7, 8.45, 10.066666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.094, Test loss: 1.966, Test accuracy: 24.37
Round   0, Global train loss: 1.094, Global test loss: 2.282, Global test accuracy: 15.66
Round   1, Train loss: 0.888, Test loss: 1.634, Test accuracy: 35.27
Round   1, Global train loss: 0.888, Global test loss: 2.172, Global test accuracy: 18.03
Round   2, Train loss: 0.782, Test loss: 1.282, Test accuracy: 49.17
Round   2, Global train loss: 0.782, Global test loss: 2.011, Global test accuracy: 25.85
Round   3, Train loss: 0.729, Test loss: 0.973, Test accuracy: 61.84
Round   3, Global train loss: 0.729, Global test loss: 1.830, Global test accuracy: 34.13
Round   4, Train loss: 0.754, Test loss: 0.980, Test accuracy: 61.52
Round   4, Global train loss: 0.754, Global test loss: 1.880, Global test accuracy: 34.13
Round   5, Train loss: 0.720, Test loss: 0.813, Test accuracy: 66.57
Round   5, Global train loss: 0.720, Global test loss: 1.850, Global test accuracy: 32.68
Round   6, Train loss: 0.664, Test loss: 0.739, Test accuracy: 70.08
Round   6, Global train loss: 0.664, Global test loss: 1.949, Global test accuracy: 28.83
Round   7, Train loss: 0.683, Test loss: 0.657, Test accuracy: 72.08
Round   7, Global train loss: 0.683, Global test loss: 1.697, Global test accuracy: 40.44
Round   8, Train loss: 0.626, Test loss: 0.605, Test accuracy: 74.78
Round   8, Global train loss: 0.626, Global test loss: 1.804, Global test accuracy: 30.32
Round   9, Train loss: 0.693, Test loss: 0.604, Test accuracy: 74.91
Round   9, Global train loss: 0.693, Global test loss: 1.784, Global test accuracy: 39.02
Round  10, Train loss: 0.620, Test loss: 0.581, Test accuracy: 75.93
Round  10, Global train loss: 0.620, Global test loss: 1.993, Global test accuracy: 29.44
Round  11, Train loss: 0.604, Test loss: 0.582, Test accuracy: 75.62
Round  11, Global train loss: 0.604, Global test loss: 1.823, Global test accuracy: 33.93
Round  12, Train loss: 0.550, Test loss: 0.571, Test accuracy: 76.20
Round  12, Global train loss: 0.550, Global test loss: 1.824, Global test accuracy: 39.82
Round  13, Train loss: 0.546, Test loss: 0.562, Test accuracy: 76.62
Round  13, Global train loss: 0.546, Global test loss: 1.506, Global test accuracy: 44.77
Round  14, Train loss: 0.503, Test loss: 0.562, Test accuracy: 76.94
Round  14, Global train loss: 0.503, Global test loss: 1.452, Global test accuracy: 46.62
Round  15, Train loss: 0.563, Test loss: 0.548, Test accuracy: 77.62
Round  15, Global train loss: 0.563, Global test loss: 1.519, Global test accuracy: 47.00
Round  16, Train loss: 0.542, Test loss: 0.544, Test accuracy: 78.12
Round  16, Global train loss: 0.542, Global test loss: 1.396, Global test accuracy: 52.27
Round  17, Train loss: 0.523, Test loss: 0.545, Test accuracy: 78.57
Round  17, Global train loss: 0.523, Global test loss: 1.450, Global test accuracy: 46.77
Round  18, Train loss: 0.512, Test loss: 0.539, Test accuracy: 78.69
Round  18, Global train loss: 0.512, Global test loss: 1.489, Global test accuracy: 47.46
Round  19, Train loss: 0.552, Test loss: 0.518, Test accuracy: 79.53
Round  19, Global train loss: 0.552, Global test loss: 1.433, Global test accuracy: 48.99
Round  20, Train loss: 0.504, Test loss: 0.517, Test accuracy: 79.20
Round  20, Global train loss: 0.504, Global test loss: 1.409, Global test accuracy: 49.96
Round  21, Train loss: 0.461, Test loss: 0.503, Test accuracy: 79.66
Round  21, Global train loss: 0.461, Global test loss: 1.379, Global test accuracy: 50.66
Round  22, Train loss: 0.471, Test loss: 0.519, Test accuracy: 79.58
Round  22, Global train loss: 0.471, Global test loss: 1.343, Global test accuracy: 53.84
Round  23, Train loss: 0.479, Test loss: 0.513, Test accuracy: 79.58
Round  23, Global train loss: 0.479, Global test loss: 1.378, Global test accuracy: 52.27
Round  24, Train loss: 0.489, Test loss: 0.508, Test accuracy: 79.81
Round  24, Global train loss: 0.489, Global test loss: 1.367, Global test accuracy: 52.23
Round  25, Train loss: 0.426, Test loss: 0.488, Test accuracy: 80.46
Round  25, Global train loss: 0.426, Global test loss: 1.357, Global test accuracy: 56.16
Round  26, Train loss: 0.438, Test loss: 0.489, Test accuracy: 80.52
Round  26, Global train loss: 0.438, Global test loss: 1.589, Global test accuracy: 47.03
Round  27, Train loss: 0.427, Test loss: 0.478, Test accuracy: 81.01
Round  27, Global train loss: 0.427, Global test loss: 1.359, Global test accuracy: 56.63
Round  28, Train loss: 0.417, Test loss: 0.487, Test accuracy: 80.91
Round  28, Global train loss: 0.417, Global test loss: 1.257, Global test accuracy: 57.18
Round  29, Train loss: 0.388, Test loss: 0.482, Test accuracy: 81.09
Round  29, Global train loss: 0.388, Global test loss: 1.176, Global test accuracy: 58.68
Round  30, Train loss: 0.397, Test loss: 0.469, Test accuracy: 81.51
Round  30, Global train loss: 0.397, Global test loss: 1.286, Global test accuracy: 55.34
Round  31, Train loss: 0.410, Test loss: 0.456, Test accuracy: 82.38
Round  31, Global train loss: 0.410, Global test loss: 1.358, Global test accuracy: 54.31
Round  32, Train loss: 0.414, Test loss: 0.471, Test accuracy: 81.83
Round  32, Global train loss: 0.414, Global test loss: 1.458, Global test accuracy: 53.33
Round  33, Train loss: 0.409, Test loss: 0.469, Test accuracy: 82.11
Round  33, Global train loss: 0.409, Global test loss: 1.317, Global test accuracy: 54.74
Round  34, Train loss: 0.458, Test loss: 0.462, Test accuracy: 82.54
Round  34, Global train loss: 0.458, Global test loss: 1.266, Global test accuracy: 55.29
Round  35, Train loss: 0.417, Test loss: 0.445, Test accuracy: 82.92
Round  35, Global train loss: 0.417, Global test loss: 1.165, Global test accuracy: 59.23
Round  36, Train loss: 0.388, Test loss: 0.436, Test accuracy: 83.19
Round  36, Global train loss: 0.388, Global test loss: 1.525, Global test accuracy: 49.51
Round  37, Train loss: 0.335, Test loss: 0.441, Test accuracy: 83.05
Round  37, Global train loss: 0.335, Global test loss: 1.214, Global test accuracy: 58.61
Round  38, Train loss: 0.340, Test loss: 0.438, Test accuracy: 83.42
Round  38, Global train loss: 0.340, Global test loss: 1.163, Global test accuracy: 57.86
Round  39, Train loss: 0.369, Test loss: 0.438, Test accuracy: 83.51
Round  39, Global train loss: 0.369, Global test loss: 1.296, Global test accuracy: 55.64
Round  40, Train loss: 0.359, Test loss: 0.432, Test accuracy: 83.83
Round  40, Global train loss: 0.359, Global test loss: 1.339, Global test accuracy: 56.47
Round  41, Train loss: 0.345, Test loss: 0.434, Test accuracy: 83.59
Round  41, Global train loss: 0.345, Global test loss: 1.384, Global test accuracy: 55.78
Round  42, Train loss: 0.313, Test loss: 0.433, Test accuracy: 83.70
Round  42, Global train loss: 0.313, Global test loss: 1.210, Global test accuracy: 58.62
Round  43, Train loss: 0.346, Test loss: 0.438, Test accuracy: 83.58
Round  43, Global train loss: 0.346, Global test loss: 1.057, Global test accuracy: 63.54
Round  44, Train loss: 0.345, Test loss: 0.436, Test accuracy: 83.92
Round  44, Global train loss: 0.345, Global test loss: 1.250, Global test accuracy: 56.31
Round  45, Train loss: 0.336, Test loss: 0.438, Test accuracy: 84.01
Round  45, Global train loss: 0.336, Global test loss: 1.439, Global test accuracy: 52.19
Round  46, Train loss: 0.326, Test loss: 0.425, Test accuracy: 84.25
Round  46, Global train loss: 0.326, Global test loss: 1.336, Global test accuracy: 56.33
Round  47, Train loss: 0.358, Test loss: 0.433, Test accuracy: 84.20
Round  47, Global train loss: 0.358, Global test loss: 1.241, Global test accuracy: 57.15
Round  48, Train loss: 0.309, Test loss: 0.436, Test accuracy: 84.12
Round  48, Global train loss: 0.309, Global test loss: 1.174, Global test accuracy: 59.08
Round  49, Train loss: 0.305, Test loss: 0.431, Test accuracy: 84.25
Round  49, Global train loss: 0.305, Global test loss: 1.358, Global test accuracy: 56.40
Round  50, Train loss: 0.315, Test loss: 0.433, Test accuracy: 84.29
Round  50, Global train loss: 0.315, Global test loss: 1.366, Global test accuracy: 55.27
Round  51, Train loss: 0.274, Test loss: 0.421, Test accuracy: 84.81
Round  51, Global train loss: 0.274, Global test loss: 1.372, Global test accuracy: 57.35
Round  52, Train loss: 0.308, Test loss: 0.431, Test accuracy: 84.89
Round  52, Global train loss: 0.308, Global test loss: 1.197, Global test accuracy: 59.60
Round  53, Train loss: 0.293, Test loss: 0.438, Test accuracy: 84.67
Round  53, Global train loss: 0.293, Global test loss: 1.255, Global test accuracy: 58.33
Round  54, Train loss: 0.276, Test loss: 0.439, Test accuracy: 84.61
Round  54, Global train loss: 0.276, Global test loss: 1.491, Global test accuracy: 54.01
Round  55, Train loss: 0.299, Test loss: 0.420, Test accuracy: 85.03
Round  55, Global train loss: 0.299, Global test loss: 1.402, Global test accuracy: 53.96
Round  56, Train loss: 0.287, Test loss: 0.418, Test accuracy: 85.07
Round  56, Global train loss: 0.287, Global test loss: 1.125, Global test accuracy: 61.94
Round  57, Train loss: 0.258, Test loss: 0.422, Test accuracy: 85.07
Round  57, Global train loss: 0.258, Global test loss: 1.563, Global test accuracy: 53.96
Round  58, Train loss: 0.278, Test loss: 0.415, Test accuracy: 85.54
Round  58, Global train loss: 0.278, Global test loss: 1.373, Global test accuracy: 58.63
Round  59, Train loss: 0.258, Test loss: 0.427, Test accuracy: 85.27
Round  59, Global train loss: 0.258, Global test loss: 1.149, Global test accuracy: 61.89
Round  60, Train loss: 0.282, Test loss: 0.424, Test accuracy: 85.11
Round  60, Global train loss: 0.282, Global test loss: 1.157, Global test accuracy: 59.52
Round  61, Train loss: 0.271, Test loss: 0.423, Test accuracy: 85.24
Round  61, Global train loss: 0.271, Global test loss: 1.203, Global test accuracy: 59.98
Round  62, Train loss: 0.310, Test loss: 0.419, Test accuracy: 85.41
Round  62, Global train loss: 0.310, Global test loss: 1.058, Global test accuracy: 64.13
Round  63, Train loss: 0.254, Test loss: 0.441, Test accuracy: 84.78
Round  63, Global train loss: 0.254, Global test loss: 1.219, Global test accuracy: 59.93
Round  64, Train loss: 0.272, Test loss: 0.442, Test accuracy: 85.08
Round  64, Global train loss: 0.272, Global test loss: 1.262, Global test accuracy: 61.00
Round  65, Train loss: 0.213, Test loss: 0.440, Test accuracy: 84.96
Round  65, Global train loss: 0.213, Global test loss: 1.518, Global test accuracy: 55.77
Round  66, Train loss: 0.267, Test loss: 0.430, Test accuracy: 85.32
Round  66, Global train loss: 0.267, Global test loss: 1.164, Global test accuracy: 62.90
Round  67, Train loss: 0.268, Test loss: 0.426, Test accuracy: 85.06
Round  67, Global train loss: 0.268, Global test loss: 1.205, Global test accuracy: 60.88
Round  68, Train loss: 0.258, Test loss: 0.435, Test accuracy: 84.97
Round  68, Global train loss: 0.258, Global test loss: 1.210, Global test accuracy: 61.66
Round  69, Train loss: 0.278, Test loss: 0.435, Test accuracy: 85.03
Round  69, Global train loss: 0.278, Global test loss: 1.065, Global test accuracy: 64.43
Round  70, Train loss: 0.245, Test loss: 0.433, Test accuracy: 84.92
Round  70, Global train loss: 0.245, Global test loss: 1.286, Global test accuracy: 60.88
Round  71, Train loss: 0.251, Test loss: 0.437, Test accuracy: 85.19
Round  71, Global train loss: 0.251, Global test loss: 1.146, Global test accuracy: 62.12
Round  72, Train loss: 0.237, Test loss: 0.452, Test accuracy: 84.67
Round  72, Global train loss: 0.237, Global test loss: 1.188, Global test accuracy: 61.53
Round  73, Train loss: 0.259, Test loss: 0.456, Test accuracy: 84.65
Round  73, Global train loss: 0.259, Global test loss: 1.240, Global test accuracy: 60.76
Round  74, Train loss: 0.228, Test loss: 0.457, Test accuracy: 85.03
Round  74, Global train loss: 0.228, Global test loss: 1.231, Global test accuracy: 60.53
Round  75, Train loss: 0.247, Test loss: 0.456, Test accuracy: 84.97
Round  75, Global train loss: 0.247, Global test loss: 1.354, Global test accuracy: 59.04
Round  76, Train loss: 0.227, Test loss: 0.452, Test accuracy: 85.38
Round  76, Global train loss: 0.227, Global test loss: 1.347, Global test accuracy: 58.06
Round  77, Train loss: 0.251, Test loss: 0.450, Test accuracy: 85.44
Round  77, Global train loss: 0.251, Global test loss: 1.135, Global test accuracy: 63.69
Round  78, Train loss: 0.201, Test loss: 0.461, Test accuracy: 85.18
Round  78, Global train loss: 0.201, Global test loss: 1.470, Global test accuracy: 57.54
Round  79, Train loss: 0.214, Test loss: 0.447, Test accuracy: 85.33
Round  79, Global train loss: 0.214, Global test loss: 1.215, Global test accuracy: 62.04
Round  80, Train loss: 0.201, Test loss: 0.444, Test accuracy: 85.45
Round  80, Global train loss: 0.201, Global test loss: 1.143, Global test accuracy: 62.74
Round  81, Train loss: 0.216, Test loss: 0.462, Test accuracy: 84.91
Round  81, Global train loss: 0.216, Global test loss: 1.253, Global test accuracy: 59.16
Round  82, Train loss: 0.212, Test loss: 0.456, Test accuracy: 85.17
Round  82, Global train loss: 0.212, Global test loss: 1.405, Global test accuracy: 60.48
Round  83, Train loss: 0.182, Test loss: 0.443, Test accuracy: 85.53
Round  83, Global train loss: 0.182, Global test loss: 1.173, Global test accuracy: 61.63
Round  84, Train loss: 0.250, Test loss: 0.450, Test accuracy: 85.68
Round  84, Global train loss: 0.250, Global test loss: 1.247, Global test accuracy: 62.25
Round  85, Train loss: 0.197, Test loss: 0.446, Test accuracy: 85.85
Round  85, Global train loss: 0.197, Global test loss: 1.358, Global test accuracy: 60.48
Round  86, Train loss: 0.215, Test loss: 0.440, Test accuracy: 86.12
Round  86, Global train loss: 0.215, Global test loss: 1.250, Global test accuracy: 62.72
Round  87, Train loss: 0.226, Test loss: 0.450, Test accuracy: 85.95
Round  87, Global train loss: 0.226, Global test loss: 1.113, Global test accuracy: 64.03
Round  88, Train loss: 0.193, Test loss: 0.453, Test accuracy: 86.18
Round  88, Global train loss: 0.193, Global test loss: 1.231, Global test accuracy: 63.79
Round  89, Train loss: 0.229, Test loss: 0.451, Test accuracy: 85.99
Round  89, Global train loss: 0.229, Global test loss: 1.043, Global test accuracy: 65.77
Round  90, Train loss: 0.217, Test loss: 0.437, Test accuracy: 85.79
Round  90, Global train loss: 0.217, Global test loss: 1.147, Global test accuracy: 63.10
Round  91, Train loss: 0.196, Test loss: 0.434, Test accuracy: 86.02
Round  91, Global train loss: 0.196, Global test loss: 1.157, Global test accuracy: 63.66
Round  92, Train loss: 0.192, Test loss: 0.444, Test accuracy: 85.80
Round  92, Global train loss: 0.192, Global test loss: 1.228, Global test accuracy: 61.38
Round  93, Train loss: 0.178, Test loss: 0.454, Test accuracy: 85.67
Round  93, Global train loss: 0.178, Global test loss: 1.159, Global test accuracy: 65.03
Round  94, Train loss: 0.220, Test loss: 0.477, Test accuracy: 85.03
Round  94, Global train loss: 0.220, Global test loss: 1.082, Global test accuracy: 65.16
Round  95, Train loss: 0.179, Test loss: 0.480, Test accuracy: 84.88
Round  95, Global train loss: 0.179, Global test loss: 1.175, Global test accuracy: 65.66
Round  96, Train loss: 0.190, Test loss: 0.469, Test accuracy: 85.17
Round  96, Global train loss: 0.190, Global test loss: 1.316, Global test accuracy: 59.55
Round  97, Train loss: 0.185, Test loss: 0.477, Test accuracy: 85.25
Round  97, Global train loss: 0.185, Global test loss: 1.206, Global test accuracy: 64.80
Round  98, Train loss: 0.204, Test loss: 0.464, Test accuracy: 85.46
Round  98, Global train loss: 0.204, Global test loss: 1.212, Global test accuracy: 63.34
Round  99, Train loss: 0.159, Test loss: 0.470, Test accuracy: 85.63
Round  99, Global train loss: 0.159, Global test loss: 1.183, Global test accuracy: 63.98
Final Round, Train loss: 0.145, Test loss: 0.487, Test accuracy: 86.23
Final Round, Global train loss: 0.145, Global test loss: 1.183, Global test accuracy: 63.98
Average accuracy final 10 rounds: 85.47 

Average global accuracy final 10 rounds: 63.565 

1990.0995035171509
[1.7285072803497314, 3.457014560699463, 4.9879491329193115, 6.51888370513916, 8.053872346878052, 9.588860988616943, 11.123823404312134, 12.658785820007324, 14.183412313461304, 15.708038806915283, 17.227052688598633, 18.746066570281982, 20.26455807685852, 21.78304958343506, 23.301191806793213, 24.819334030151367, 26.35236883163452, 27.885403633117676, 29.413222551345825, 30.941041469573975, 32.462743043899536, 33.9844446182251, 35.44181847572327, 36.899192333221436, 38.34709143638611, 39.79499053955078, 41.25679659843445, 42.718602657318115, 44.15684747695923, 45.59509229660034, 46.92703056335449, 48.25896883010864, 49.579524517059326, 50.90008020401001, 52.220165491104126, 53.54025077819824, 54.85957407951355, 56.17889738082886, 57.521957874298096, 58.865018367767334, 60.19734716415405, 61.52967596054077, 62.8628716468811, 64.19606733322144, 65.54866123199463, 66.90125513076782, 68.23449492454529, 69.56773471832275, 70.89561724662781, 72.22349977493286, 73.55881214141846, 74.89412450790405, 76.22702527046204, 77.55992603302002, 78.88819766044617, 80.21646928787231, 81.51551127433777, 82.81455326080322, 84.13616681098938, 85.45778036117554, 86.7962474822998, 88.13471460342407, 89.4739158153534, 90.81311702728271, 92.14960551261902, 93.48609399795532, 94.80737543106079, 96.12865686416626, 97.47522854804993, 98.8218002319336, 100.18611431121826, 101.55042839050293, 102.86997580528259, 104.18952322006226, 105.55305337905884, 106.91658353805542, 108.25704669952393, 109.59750986099243, 110.92682600021362, 112.25614213943481, 113.5930004119873, 114.9298586845398, 116.26547598838806, 117.60109329223633, 118.91596794128418, 120.23084259033203, 121.5992922782898, 122.96774196624756, 124.32608389854431, 125.68442583084106, 127.0250985622406, 128.36577129364014, 129.6923291683197, 131.01888704299927, 132.36059498786926, 133.70230293273926, 135.02891159057617, 136.3555202484131, 137.70608854293823, 139.05665683746338, 140.40536975860596, 141.75408267974854, 143.13631081581116, 144.51853895187378, 145.8676815032959, 147.21682405471802, 148.59302186965942, 149.96921968460083, 151.4013226032257, 152.8334255218506, 154.23471546173096, 155.63600540161133, 157.05158376693726, 158.46716213226318, 159.89038944244385, 161.3136167526245, 162.72506022453308, 164.13650369644165, 165.56469821929932, 166.99289274215698, 168.39729118347168, 169.80168962478638, 171.1587414741516, 172.51579332351685, 173.8469820022583, 175.17817068099976, 176.5173077583313, 177.85644483566284, 179.18276405334473, 180.5090832710266, 181.83066511154175, 183.15224695205688, 184.47960877418518, 185.80697059631348, 187.1263120174408, 188.44565343856812, 189.84014129638672, 191.23462915420532, 192.58014345169067, 193.92565774917603, 195.26203107833862, 196.59840440750122, 198.02440309524536, 199.4504017829895, 200.77652788162231, 202.10265398025513, 203.4388861656189, 204.77511835098267, 206.18545627593994, 207.59579420089722, 208.97777128219604, 210.35974836349487, 211.74684119224548, 213.1339340209961, 214.54533696174622, 215.95673990249634, 217.3128798007965, 218.66901969909668, 219.97556591033936, 221.28211212158203, 222.614403963089, 223.94669580459595, 225.27938294410706, 226.61207008361816, 228.0191400051117, 229.42620992660522, 230.82153868675232, 232.2168674468994, 233.61160802841187, 235.00634860992432, 236.38222336769104, 237.75809812545776, 239.18946838378906, 240.62083864212036, 242.0045781135559, 243.38831758499146, 244.7168791294098, 246.04544067382812, 247.38084936141968, 248.71625804901123, 250.07315111160278, 251.43004417419434, 252.8370225429535, 254.24400091171265, 255.6113817691803, 256.97876262664795, 258.3657433986664, 259.7527241706848, 261.1456649303436, 262.53860569000244, 263.8896164894104, 265.24062728881836, 266.5897181034088, 267.93880891799927, 269.27136063575745, 270.6039123535156, 271.9827826023102, 273.36165285110474, 274.6950149536133, 276.0283770561218, 278.4150779247284, 280.80177879333496]
[24.366666666666667, 24.366666666666667, 35.275, 35.275, 49.166666666666664, 49.166666666666664, 61.84166666666667, 61.84166666666667, 61.525, 61.525, 66.56666666666666, 66.56666666666666, 70.075, 70.075, 72.08333333333333, 72.08333333333333, 74.78333333333333, 74.78333333333333, 74.90833333333333, 74.90833333333333, 75.93333333333334, 75.93333333333334, 75.625, 75.625, 76.2, 76.2, 76.625, 76.625, 76.94166666666666, 76.94166666666666, 77.61666666666666, 77.61666666666666, 78.11666666666666, 78.11666666666666, 78.56666666666666, 78.56666666666666, 78.69166666666666, 78.69166666666666, 79.53333333333333, 79.53333333333333, 79.2, 79.2, 79.65833333333333, 79.65833333333333, 79.58333333333333, 79.58333333333333, 79.575, 79.575, 79.80833333333334, 79.80833333333334, 80.45833333333333, 80.45833333333333, 80.51666666666667, 80.51666666666667, 81.00833333333334, 81.00833333333334, 80.90833333333333, 80.90833333333333, 81.09166666666667, 81.09166666666667, 81.50833333333334, 81.50833333333334, 82.375, 82.375, 81.825, 81.825, 82.10833333333333, 82.10833333333333, 82.54166666666667, 82.54166666666667, 82.925, 82.925, 83.19166666666666, 83.19166666666666, 83.05, 83.05, 83.425, 83.425, 83.50833333333334, 83.50833333333334, 83.825, 83.825, 83.59166666666667, 83.59166666666667, 83.7, 83.7, 83.58333333333333, 83.58333333333333, 83.91666666666667, 83.91666666666667, 84.00833333333334, 84.00833333333334, 84.25, 84.25, 84.2, 84.2, 84.125, 84.125, 84.25, 84.25, 84.29166666666667, 84.29166666666667, 84.80833333333334, 84.80833333333334, 84.89166666666667, 84.89166666666667, 84.66666666666667, 84.66666666666667, 84.60833333333333, 84.60833333333333, 85.03333333333333, 85.03333333333333, 85.06666666666666, 85.06666666666666, 85.06666666666666, 85.06666666666666, 85.54166666666667, 85.54166666666667, 85.26666666666667, 85.26666666666667, 85.10833333333333, 85.10833333333333, 85.24166666666666, 85.24166666666666, 85.40833333333333, 85.40833333333333, 84.78333333333333, 84.78333333333333, 85.08333333333333, 85.08333333333333, 84.95833333333333, 84.95833333333333, 85.31666666666666, 85.31666666666666, 85.05833333333334, 85.05833333333334, 84.96666666666667, 84.96666666666667, 85.03333333333333, 85.03333333333333, 84.925, 84.925, 85.19166666666666, 85.19166666666666, 84.66666666666667, 84.66666666666667, 84.65, 84.65, 85.03333333333333, 85.03333333333333, 84.975, 84.975, 85.375, 85.375, 85.44166666666666, 85.44166666666666, 85.18333333333334, 85.18333333333334, 85.325, 85.325, 85.45, 85.45, 84.90833333333333, 84.90833333333333, 85.175, 85.175, 85.525, 85.525, 85.68333333333334, 85.68333333333334, 85.85, 85.85, 86.11666666666666, 86.11666666666666, 85.95, 85.95, 86.18333333333334, 86.18333333333334, 85.99166666666666, 85.99166666666666, 85.79166666666667, 85.79166666666667, 86.01666666666667, 86.01666666666667, 85.8, 85.8, 85.66666666666667, 85.66666666666667, 85.03333333333333, 85.03333333333333, 84.875, 84.875, 85.175, 85.175, 85.25, 85.25, 85.45833333333333, 85.45833333333333, 85.63333333333334, 85.63333333333334, 86.23333333333333, 86.23333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.467, Test loss: 2.147, Test accuracy: 25.77
Round   1, Train loss: 0.954, Test loss: 2.017, Test accuracy: 32.95
Round   2, Train loss: 0.857, Test loss: 1.604, Test accuracy: 42.76
Round   3, Train loss: 0.821, Test loss: 1.314, Test accuracy: 48.73
Round   4, Train loss: 0.795, Test loss: 1.218, Test accuracy: 56.92
Round   5, Train loss: 0.723, Test loss: 1.155, Test accuracy: 55.75
Round   6, Train loss: 0.709, Test loss: 0.934, Test accuracy: 60.62
Round   7, Train loss: 0.660, Test loss: 0.796, Test accuracy: 66.88
Round   8, Train loss: 0.725, Test loss: 0.844, Test accuracy: 68.71
Round   9, Train loss: 0.656, Test loss: 0.705, Test accuracy: 68.98
Round  10, Train loss: 0.588, Test loss: 0.785, Test accuracy: 68.84
Round  11, Train loss: 0.614, Test loss: 0.725, Test accuracy: 70.74
Round  12, Train loss: 0.677, Test loss: 0.587, Test accuracy: 75.09
Round  13, Train loss: 0.606, Test loss: 0.579, Test accuracy: 75.88
Round  14, Train loss: 0.599, Test loss: 0.566, Test accuracy: 76.70
Round  15, Train loss: 0.601, Test loss: 0.542, Test accuracy: 77.80
Round  16, Train loss: 0.567, Test loss: 0.564, Test accuracy: 76.57
Round  17, Train loss: 0.548, Test loss: 0.557, Test accuracy: 76.87
Round  18, Train loss: 0.582, Test loss: 0.552, Test accuracy: 76.86
Round  19, Train loss: 0.514, Test loss: 0.552, Test accuracy: 77.08
Round  20, Train loss: 0.547, Test loss: 0.539, Test accuracy: 77.80
Round  21, Train loss: 0.572, Test loss: 0.508, Test accuracy: 78.89
Round  22, Train loss: 0.528, Test loss: 0.497, Test accuracy: 79.50
Round  23, Train loss: 0.512, Test loss: 0.496, Test accuracy: 79.95
Round  24, Train loss: 0.491, Test loss: 0.490, Test accuracy: 79.87
Round  25, Train loss: 0.464, Test loss: 0.493, Test accuracy: 79.81
Round  26, Train loss: 0.492, Test loss: 0.482, Test accuracy: 79.99
Round  27, Train loss: 0.451, Test loss: 0.499, Test accuracy: 79.16
Round  28, Train loss: 0.462, Test loss: 0.487, Test accuracy: 79.40
Round  29, Train loss: 0.456, Test loss: 0.473, Test accuracy: 80.08
Round  30, Train loss: 0.447, Test loss: 0.470, Test accuracy: 80.56
Round  31, Train loss: 0.463, Test loss: 0.461, Test accuracy: 80.72
Round  32, Train loss: 0.468, Test loss: 0.453, Test accuracy: 81.20
Round  33, Train loss: 0.453, Test loss: 0.442, Test accuracy: 81.90
Round  34, Train loss: 0.429, Test loss: 0.433, Test accuracy: 82.46
Round  35, Train loss: 0.463, Test loss: 0.431, Test accuracy: 82.71
Round  36, Train loss: 0.443, Test loss: 0.424, Test accuracy: 82.92
Round  37, Train loss: 0.452, Test loss: 0.414, Test accuracy: 83.38
Round  38, Train loss: 0.446, Test loss: 0.418, Test accuracy: 83.48
Round  39, Train loss: 0.425, Test loss: 0.418, Test accuracy: 83.07
Round  40, Train loss: 0.435, Test loss: 0.413, Test accuracy: 83.02
Round  41, Train loss: 0.409, Test loss: 0.423, Test accuracy: 82.61
Round  42, Train loss: 0.430, Test loss: 0.407, Test accuracy: 83.53
Round  43, Train loss: 0.402, Test loss: 0.405, Test accuracy: 83.89
Round  44, Train loss: 0.382, Test loss: 0.406, Test accuracy: 83.63
Round  45, Train loss: 0.362, Test loss: 0.407, Test accuracy: 83.84
Round  46, Train loss: 0.416, Test loss: 0.398, Test accuracy: 83.83
Round  47, Train loss: 0.384, Test loss: 0.397, Test accuracy: 83.84
Round  48, Train loss: 0.381, Test loss: 0.397, Test accuracy: 83.91
Round  49, Train loss: 0.364, Test loss: 0.394, Test accuracy: 84.08
Round  50, Train loss: 0.353, Test loss: 0.387, Test accuracy: 84.37
Round  51, Train loss: 0.338, Test loss: 0.392, Test accuracy: 84.12
Round  52, Train loss: 0.337, Test loss: 0.396, Test accuracy: 83.72
Round  53, Train loss: 0.341, Test loss: 0.389, Test accuracy: 84.11
Round  54, Train loss: 0.348, Test loss: 0.397, Test accuracy: 83.73
Round  55, Train loss: 0.386, Test loss: 0.394, Test accuracy: 84.24
Round  56, Train loss: 0.376, Test loss: 0.384, Test accuracy: 84.72
Round  57, Train loss: 0.328, Test loss: 0.379, Test accuracy: 85.03
Round  58, Train loss: 0.341, Test loss: 0.374, Test accuracy: 85.08
Round  59, Train loss: 0.357, Test loss: 0.378, Test accuracy: 84.79
Round  60, Train loss: 0.375, Test loss: 0.378, Test accuracy: 85.13
Round  61, Train loss: 0.337, Test loss: 0.379, Test accuracy: 85.15
Round  62, Train loss: 0.324, Test loss: 0.375, Test accuracy: 84.67
Round  63, Train loss: 0.351, Test loss: 0.371, Test accuracy: 85.32
Round  64, Train loss: 0.298, Test loss: 0.382, Test accuracy: 85.11
Round  65, Train loss: 0.320, Test loss: 0.373, Test accuracy: 85.17
Round  66, Train loss: 0.318, Test loss: 0.374, Test accuracy: 84.97
Round  67, Train loss: 0.299, Test loss: 0.371, Test accuracy: 85.23
Round  68, Train loss: 0.309, Test loss: 0.369, Test accuracy: 85.33
Round  69, Train loss: 0.289, Test loss: 0.366, Test accuracy: 85.50
Round  70, Train loss: 0.280, Test loss: 0.365, Test accuracy: 85.81
Round  71, Train loss: 0.326, Test loss: 0.368, Test accuracy: 85.77
Round  72, Train loss: 0.309, Test loss: 0.369, Test accuracy: 85.43
Round  73, Train loss: 0.302, Test loss: 0.366, Test accuracy: 85.48
Round  74, Train loss: 0.306, Test loss: 0.365, Test accuracy: 86.08
Round  75, Train loss: 0.258, Test loss: 0.362, Test accuracy: 85.88
Round  76, Train loss: 0.309, Test loss: 0.357, Test accuracy: 85.89
Round  77, Train loss: 0.268, Test loss: 0.365, Test accuracy: 85.85
Round  78, Train loss: 0.250, Test loss: 0.361, Test accuracy: 86.22
Round  79, Train loss: 0.276, Test loss: 0.365, Test accuracy: 86.12
Round  80, Train loss: 0.277, Test loss: 0.359, Test accuracy: 86.04
Round  81, Train loss: 0.273, Test loss: 0.363, Test accuracy: 86.16
Round  82, Train loss: 0.282, Test loss: 0.362, Test accuracy: 86.15
Round  83, Train loss: 0.231, Test loss: 0.366, Test accuracy: 86.05
Round  84, Train loss: 0.258, Test loss: 0.366, Test accuracy: 86.08
Round  85, Train loss: 0.274, Test loss: 0.363, Test accuracy: 86.08
Round  86, Train loss: 0.283, Test loss: 0.359, Test accuracy: 86.00
Round  87, Train loss: 0.228, Test loss: 0.361, Test accuracy: 86.18
Round  88, Train loss: 0.232, Test loss: 0.365, Test accuracy: 85.88
Round  89, Train loss: 0.255, Test loss: 0.361, Test accuracy: 86.17
Round  90, Train loss: 0.225, Test loss: 0.360, Test accuracy: 86.33
Round  91, Train loss: 0.251, Test loss: 0.363, Test accuracy: 85.90
Round  92, Train loss: 0.240, Test loss: 0.362, Test accuracy: 86.34
Round  93, Train loss: 0.261, Test loss: 0.361, Test accuracy: 86.64
Round  94, Train loss: 0.213, Test loss: 0.366, Test accuracy: 86.24
Round  95, Train loss: 0.260, Test loss: 0.365, Test accuracy: 86.05
Round  96, Train loss: 0.247, Test loss: 0.370, Test accuracy: 86.33
Round  97, Train loss: 0.220, Test loss: 0.377, Test accuracy: 86.08
Round  98, Train loss: 0.227, Test loss: 0.364, Test accuracy: 86.55
Round  99, Train loss: 0.271, Test loss: 0.363, Test accuracy: 86.57
Final Round, Train loss: 0.203, Test loss: 0.367, Test accuracy: 86.85
Average accuracy final 10 rounds: 86.3025 

1565.842553138733
[1.6734395027160645, 3.346879005432129, 4.749506235122681, 6.152133464813232, 7.549726486206055, 8.947319507598877, 10.353503465652466, 11.759687423706055, 13.152779340744019, 14.545871257781982, 15.962052345275879, 17.378233432769775, 18.70885467529297, 20.039475917816162, 21.384766340255737, 22.730056762695312, 24.090741634368896, 25.45142650604248, 26.800307989120483, 28.149189472198486, 29.506452322006226, 30.863715171813965, 32.14100217819214, 33.41828918457031, 34.731693744659424, 36.045098304748535, 37.378570318222046, 38.71204233169556, 40.06922388076782, 41.42640542984009, 42.69150638580322, 43.95660734176636, 45.21572542190552, 46.47484350204468, 47.721346855163574, 48.96785020828247, 50.233339071273804, 51.49882793426514, 52.780909299850464, 54.06299066543579, 55.31621479988098, 56.56943893432617, 57.84599590301514, 59.1225528717041, 60.42550015449524, 61.72844743728638, 63.01019501686096, 64.29194259643555, 65.56248760223389, 66.83303260803223, 68.07703185081482, 69.32103109359741, 70.58591938018799, 71.85080766677856, 73.14192700386047, 74.43304634094238, 75.72584939002991, 77.01865243911743, 78.28601574897766, 79.55337905883789, 80.80847644805908, 82.06357383728027, 83.32264399528503, 84.5817141532898, 85.84562635421753, 87.10953855514526, 88.37182140350342, 89.63410425186157, 90.8913037776947, 92.14850330352783, 93.4052324295044, 94.66196155548096, 95.93519330024719, 97.20842504501343, 98.47879314422607, 99.74916124343872, 101.00080919265747, 102.25245714187622, 103.51464033126831, 104.7768235206604, 106.04256916046143, 107.30831480026245, 108.59023237228394, 109.87214994430542, 111.13370442390442, 112.39525890350342, 113.67886281013489, 114.96246671676636, 116.25340867042542, 117.54435062408447, 118.83915424346924, 120.133957862854, 121.45671534538269, 122.77947282791138, 124.04904055595398, 125.31860828399658, 126.62989091873169, 127.9411735534668, 129.24377155303955, 130.5463695526123, 131.84506845474243, 133.14376735687256, 134.43391609191895, 135.72406482696533, 137.0351722240448, 138.34627962112427, 139.62482047080994, 140.9033613204956, 142.16152811050415, 143.4196949005127, 144.6961042881012, 145.9725136756897, 147.2455072402954, 148.51850080490112, 149.815655708313, 151.11281061172485, 152.39705896377563, 153.68130731582642, 154.94446897506714, 156.20763063430786, 157.47007417678833, 158.7325177192688, 160.00392937660217, 161.27534103393555, 162.5439965724945, 163.81265211105347, 165.07059240341187, 166.32853269577026, 167.60431838035583, 168.8801040649414, 170.1500744819641, 171.42004489898682, 172.70959615707397, 173.99914741516113, 175.2712218761444, 176.54329633712769, 177.8224539756775, 179.1016116142273, 180.37044835090637, 181.63928508758545, 182.93032455444336, 184.22136402130127, 185.50678515434265, 186.79220628738403, 188.07903671264648, 189.36586713790894, 190.65561485290527, 191.9453625679016, 193.23148941993713, 194.51761627197266, 195.79384779930115, 197.07007932662964, 198.34631609916687, 199.6225528717041, 200.8985595703125, 202.1745662689209, 203.4466621875763, 204.7187581062317, 206.0273413658142, 207.33592462539673, 208.59933471679688, 209.86274480819702, 211.1347930431366, 212.40684127807617, 213.68171095848083, 214.9565806388855, 216.2371108531952, 217.51764106750488, 218.790682554245, 220.0637240409851, 221.35463070869446, 222.6455373764038, 223.93927097320557, 225.23300457000732, 226.52954745292664, 227.82609033584595, 229.12658715248108, 230.4270839691162, 231.6934814453125, 232.9598789215088, 234.2295446395874, 235.49921035766602, 236.8113112449646, 238.12341213226318, 239.39821529388428, 240.67301845550537, 241.97601771354675, 243.27901697158813, 244.554274559021, 245.82953214645386, 247.11932826042175, 248.40912437438965, 249.67951846122742, 250.94991254806519, 252.22419691085815, 253.49848127365112, 254.75900173187256, 256.019522190094, 257.2978720664978, 258.5762219429016, 260.5964684486389, 262.6167149543762]
[25.775, 25.775, 32.95, 32.95, 42.75833333333333, 42.75833333333333, 48.725, 48.725, 56.925, 56.925, 55.75, 55.75, 60.61666666666667, 60.61666666666667, 66.88333333333334, 66.88333333333334, 68.70833333333333, 68.70833333333333, 68.98333333333333, 68.98333333333333, 68.84166666666667, 68.84166666666667, 70.74166666666666, 70.74166666666666, 75.09166666666667, 75.09166666666667, 75.88333333333334, 75.88333333333334, 76.7, 76.7, 77.8, 77.8, 76.56666666666666, 76.56666666666666, 76.86666666666666, 76.86666666666666, 76.85833333333333, 76.85833333333333, 77.08333333333333, 77.08333333333333, 77.8, 77.8, 78.89166666666667, 78.89166666666667, 79.5, 79.5, 79.95, 79.95, 79.86666666666666, 79.86666666666666, 79.80833333333334, 79.80833333333334, 79.99166666666666, 79.99166666666666, 79.15833333333333, 79.15833333333333, 79.4, 79.4, 80.075, 80.075, 80.55833333333334, 80.55833333333334, 80.71666666666667, 80.71666666666667, 81.2, 81.2, 81.9, 81.9, 82.45833333333333, 82.45833333333333, 82.70833333333333, 82.70833333333333, 82.925, 82.925, 83.38333333333334, 83.38333333333334, 83.48333333333333, 83.48333333333333, 83.06666666666666, 83.06666666666666, 83.01666666666667, 83.01666666666667, 82.60833333333333, 82.60833333333333, 83.53333333333333, 83.53333333333333, 83.89166666666667, 83.89166666666667, 83.63333333333334, 83.63333333333334, 83.84166666666667, 83.84166666666667, 83.825, 83.825, 83.84166666666667, 83.84166666666667, 83.90833333333333, 83.90833333333333, 84.08333333333333, 84.08333333333333, 84.36666666666666, 84.36666666666666, 84.11666666666666, 84.11666666666666, 83.71666666666667, 83.71666666666667, 84.10833333333333, 84.10833333333333, 83.73333333333333, 83.73333333333333, 84.24166666666666, 84.24166666666666, 84.71666666666667, 84.71666666666667, 85.03333333333333, 85.03333333333333, 85.075, 85.075, 84.79166666666667, 84.79166666666667, 85.13333333333334, 85.13333333333334, 85.15, 85.15, 84.66666666666667, 84.66666666666667, 85.31666666666666, 85.31666666666666, 85.10833333333333, 85.10833333333333, 85.175, 85.175, 84.96666666666667, 84.96666666666667, 85.23333333333333, 85.23333333333333, 85.33333333333333, 85.33333333333333, 85.5, 85.5, 85.80833333333334, 85.80833333333334, 85.76666666666667, 85.76666666666667, 85.43333333333334, 85.43333333333334, 85.48333333333333, 85.48333333333333, 86.08333333333333, 86.08333333333333, 85.875, 85.875, 85.89166666666667, 85.89166666666667, 85.85, 85.85, 86.21666666666667, 86.21666666666667, 86.11666666666666, 86.11666666666666, 86.04166666666667, 86.04166666666667, 86.15833333333333, 86.15833333333333, 86.15, 86.15, 86.05, 86.05, 86.075, 86.075, 86.08333333333333, 86.08333333333333, 86.0, 86.0, 86.18333333333334, 86.18333333333334, 85.875, 85.875, 86.16666666666667, 86.16666666666667, 86.325, 86.325, 85.9, 85.9, 86.34166666666667, 86.34166666666667, 86.64166666666667, 86.64166666666667, 86.24166666666666, 86.24166666666666, 86.05, 86.05, 86.33333333333333, 86.33333333333333, 86.075, 86.075, 86.55, 86.55, 86.56666666666666, 86.56666666666666, 86.85, 86.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.498, Test loss: 1.996, Test accuracy: 22.52
Round   1, Train loss: 1.021, Test loss: 1.681, Test accuracy: 37.75
Round   2, Train loss: 0.912, Test loss: 1.725, Test accuracy: 44.68
Round   3, Train loss: 0.855, Test loss: 1.328, Test accuracy: 49.73
Round   4, Train loss: 0.777, Test loss: 1.064, Test accuracy: 59.08
Round   5, Train loss: 0.738, Test loss: 0.767, Test accuracy: 69.37
Round   6, Train loss: 0.718, Test loss: 0.730, Test accuracy: 71.48
Round   7, Train loss: 0.684, Test loss: 0.728, Test accuracy: 71.88
Round   8, Train loss: 0.712, Test loss: 0.652, Test accuracy: 74.60
Round   9, Train loss: 0.667, Test loss: 0.651, Test accuracy: 74.82
Round  10, Train loss: 0.657, Test loss: 0.617, Test accuracy: 76.78
Round  11, Train loss: 0.689, Test loss: 0.601, Test accuracy: 76.77
Round  12, Train loss: 0.622, Test loss: 0.580, Test accuracy: 78.02
Round  13, Train loss: 0.609, Test loss: 0.557, Test accuracy: 78.33
Round  14, Train loss: 0.599, Test loss: 0.556, Test accuracy: 78.66
Round  15, Train loss: 0.542, Test loss: 0.561, Test accuracy: 78.68
Round  16, Train loss: 0.558, Test loss: 0.529, Test accuracy: 79.78
Round  17, Train loss: 0.551, Test loss: 0.525, Test accuracy: 80.28
Round  18, Train loss: 0.515, Test loss: 0.522, Test accuracy: 80.33
Round  19, Train loss: 0.561, Test loss: 0.515, Test accuracy: 80.47
Round  20, Train loss: 0.535, Test loss: 0.502, Test accuracy: 80.74
Round  21, Train loss: 0.510, Test loss: 0.474, Test accuracy: 81.62
Round  22, Train loss: 0.525, Test loss: 0.467, Test accuracy: 81.81
Round  23, Train loss: 0.483, Test loss: 0.476, Test accuracy: 82.00
Round  24, Train loss: 0.510, Test loss: 0.476, Test accuracy: 81.34
Round  25, Train loss: 0.468, Test loss: 0.455, Test accuracy: 82.02
Round  26, Train loss: 0.475, Test loss: 0.447, Test accuracy: 82.14
Round  27, Train loss: 0.417, Test loss: 0.453, Test accuracy: 82.38
Round  28, Train loss: 0.477, Test loss: 0.446, Test accuracy: 82.42
Round  29, Train loss: 0.460, Test loss: 0.442, Test accuracy: 82.98
Round  30, Train loss: 0.485, Test loss: 0.439, Test accuracy: 82.76
Round  31, Train loss: 0.458, Test loss: 0.435, Test accuracy: 83.05
Round  32, Train loss: 0.417, Test loss: 0.430, Test accuracy: 82.86
Round  33, Train loss: 0.462, Test loss: 0.425, Test accuracy: 83.58
Round  34, Train loss: 0.403, Test loss: 0.432, Test accuracy: 83.03
Round  35, Train loss: 0.404, Test loss: 0.424, Test accuracy: 83.47
Round  36, Train loss: 0.444, Test loss: 0.408, Test accuracy: 83.89
Round  37, Train loss: 0.403, Test loss: 0.409, Test accuracy: 84.00
Round  38, Train loss: 0.421, Test loss: 0.400, Test accuracy: 84.33
Round  39, Train loss: 0.394, Test loss: 0.404, Test accuracy: 84.18
Round  40, Train loss: 0.407, Test loss: 0.394, Test accuracy: 84.39
Round  41, Train loss: 0.385, Test loss: 0.401, Test accuracy: 84.38
Round  42, Train loss: 0.353, Test loss: 0.394, Test accuracy: 84.57
Round  43, Train loss: 0.415, Test loss: 0.399, Test accuracy: 84.44
Round  44, Train loss: 0.390, Test loss: 0.390, Test accuracy: 84.71
Round  45, Train loss: 0.390, Test loss: 0.395, Test accuracy: 84.77
Round  46, Train loss: 0.392, Test loss: 0.391, Test accuracy: 84.90
Round  47, Train loss: 0.379, Test loss: 0.380, Test accuracy: 85.25
Round  48, Train loss: 0.345, Test loss: 0.373, Test accuracy: 85.27
Round  49, Train loss: 0.341, Test loss: 0.373, Test accuracy: 85.47
Round  50, Train loss: 0.348, Test loss: 0.376, Test accuracy: 85.37
Round  51, Train loss: 0.344, Test loss: 0.371, Test accuracy: 85.19
Round  52, Train loss: 0.393, Test loss: 0.369, Test accuracy: 85.70
Round  53, Train loss: 0.349, Test loss: 0.367, Test accuracy: 85.83
Round  54, Train loss: 0.389, Test loss: 0.369, Test accuracy: 85.72
Round  55, Train loss: 0.339, Test loss: 0.365, Test accuracy: 85.69
Round  56, Train loss: 0.362, Test loss: 0.365, Test accuracy: 85.69
Round  57, Train loss: 0.329, Test loss: 0.366, Test accuracy: 85.58
Round  58, Train loss: 0.348, Test loss: 0.365, Test accuracy: 85.71
Round  59, Train loss: 0.290, Test loss: 0.360, Test accuracy: 85.72
Round  60, Train loss: 0.324, Test loss: 0.362, Test accuracy: 85.91
Round  61, Train loss: 0.311, Test loss: 0.363, Test accuracy: 85.88
Round  62, Train loss: 0.322, Test loss: 0.360, Test accuracy: 86.06
Round  63, Train loss: 0.266, Test loss: 0.364, Test accuracy: 85.52
Round  64, Train loss: 0.309, Test loss: 0.356, Test accuracy: 86.05
Round  65, Train loss: 0.320, Test loss: 0.352, Test accuracy: 85.94
Round  66, Train loss: 0.275, Test loss: 0.361, Test accuracy: 86.01
Round  67, Train loss: 0.287, Test loss: 0.356, Test accuracy: 86.00
Round  68, Train loss: 0.305, Test loss: 0.350, Test accuracy: 86.21
Round  69, Train loss: 0.275, Test loss: 0.350, Test accuracy: 86.17
Round  70, Train loss: 0.266, Test loss: 0.349, Test accuracy: 86.24
Round  71, Train loss: 0.289, Test loss: 0.351, Test accuracy: 86.55
Round  72, Train loss: 0.324, Test loss: 0.350, Test accuracy: 86.15
Round  73, Train loss: 0.286, Test loss: 0.346, Test accuracy: 86.80
Round  74, Train loss: 0.278, Test loss: 0.345, Test accuracy: 86.54
Round  75, Train loss: 0.269, Test loss: 0.344, Test accuracy: 86.44
Round  76, Train loss: 0.262, Test loss: 0.340, Test accuracy: 86.92
Round  77, Train loss: 0.291, Test loss: 0.339, Test accuracy: 86.76
Round  78, Train loss: 0.277, Test loss: 0.340, Test accuracy: 86.71
Round  79, Train loss: 0.265, Test loss: 0.341, Test accuracy: 86.72
Round  80, Train loss: 0.246, Test loss: 0.338, Test accuracy: 86.64
Round  81, Train loss: 0.260, Test loss: 0.333, Test accuracy: 86.90
Round  82, Train loss: 0.265, Test loss: 0.337, Test accuracy: 87.11
Round  83, Train loss: 0.249, Test loss: 0.338, Test accuracy: 86.72
Round  84, Train loss: 0.255, Test loss: 0.339, Test accuracy: 86.78
Round  85, Train loss: 0.280, Test loss: 0.338, Test accuracy: 86.58
Round  86, Train loss: 0.250, Test loss: 0.334, Test accuracy: 86.78
Round  87, Train loss: 0.297, Test loss: 0.330, Test accuracy: 87.51
Round  88, Train loss: 0.248, Test loss: 0.330, Test accuracy: 87.11
Round  89, Train loss: 0.248, Test loss: 0.338, Test accuracy: 86.92
Round  90, Train loss: 0.277, Test loss: 0.336, Test accuracy: 86.91
Round  91, Train loss: 0.237, Test loss: 0.333, Test accuracy: 87.38
Round  92, Train loss: 0.240, Test loss: 0.333, Test accuracy: 86.83
Round  93, Train loss: 0.234, Test loss: 0.337, Test accuracy: 86.92
Round  94, Train loss: 0.238, Test loss: 0.337, Test accuracy: 86.80
Round  95, Train loss: 0.236, Test loss: 0.342, Test accuracy: 86.65
Round  96, Train loss: 0.234, Test loss: 0.341, Test accuracy: 86.97
Round  97, Train loss: 0.222, Test loss: 0.332, Test accuracy: 87.27
Round  98, Train loss: 0.235, Test loss: 0.332, Test accuracy: 87.02
Round  99, Train loss: 0.263, Test loss: 0.332, Test accuracy: 87.12
Final Round, Train loss: 0.185, Test loss: 0.330, Test accuracy: 87.47
Average accuracy final 10 rounds: 86.98583333333333
1809.8589494228363
[2.1192879676818848, 4.2385759353637695, 6.076601505279541, 7.9146270751953125, 9.701347589492798, 11.488068103790283, 13.323064804077148, 15.158061504364014, 16.841960668563843, 18.525859832763672, 20.20941424369812, 21.89296865463257, 23.594077587127686, 25.295186519622803, 26.979193925857544, 28.663201332092285, 30.385780572891235, 32.108359813690186, 33.83480715751648, 35.56125450134277, 37.27282738685608, 38.984400272369385, 40.714874505996704, 42.44534873962402, 44.180824756622314, 45.916300773620605, 47.59312701225281, 49.26995325088501, 50.967002153396606, 52.6640510559082, 54.336503744125366, 56.00895643234253, 57.69887733459473, 59.388798236846924, 61.074331521987915, 62.759864807128906, 64.44643354415894, 66.13300228118896, 67.82542157173157, 69.51784086227417, 71.21035385131836, 72.90286684036255, 74.57258319854736, 76.24229955673218, 77.94648861885071, 79.65067768096924, 81.35289931297302, 83.0551209449768, 84.74432516098022, 86.43352937698364, 88.09877347946167, 89.7640175819397, 91.44570660591125, 93.12739562988281, 94.84625911712646, 96.56512260437012, 98.25575351715088, 99.94638442993164, 101.63230752944946, 103.31823062896729, 105.03478026390076, 106.75132989883423, 108.45360016822815, 110.15587043762207, 111.84229850769043, 113.52872657775879, 115.20435881614685, 116.87999105453491, 118.56604862213135, 120.25210618972778, 121.9600157737732, 123.6679253578186, 125.36366653442383, 127.05940771102905, 128.7364158630371, 130.41342401504517, 132.1302845478058, 133.8471450805664, 135.52628779411316, 137.2054305076599, 138.88151383399963, 140.55759716033936, 142.2375774383545, 143.91755771636963, 145.59317660331726, 147.2687954902649, 148.9570770263672, 150.64535856246948, 152.31435585021973, 153.98335313796997, 155.67300248146057, 157.36265182495117, 159.04912853240967, 160.73560523986816, 162.393789768219, 164.05197429656982, 165.73304915428162, 167.4141240119934, 169.08776688575745, 170.76140975952148, 172.43352818489075, 174.10564661026, 175.79907608032227, 177.49250555038452, 179.164391040802, 180.83627653121948, 182.50127148628235, 184.16626644134521, 185.8228178024292, 187.47936916351318, 189.14378714561462, 190.80820512771606, 192.48384022712708, 194.1594753265381, 195.82908129692078, 197.49868726730347, 199.1537835597992, 200.80887985229492, 202.47261214256287, 204.1363444328308, 205.81354594230652, 207.49074745178223, 209.15816235542297, 210.82557725906372, 212.47132849693298, 214.11707973480225, 215.7679934501648, 217.41890716552734, 219.09627223014832, 220.7736372947693, 222.44981575012207, 224.12599420547485, 225.80345249176025, 227.48091077804565, 229.1640055179596, 230.84710025787354, 232.52418565750122, 234.2012710571289, 235.8788435459137, 237.5564160346985, 239.2262623310089, 240.89610862731934, 242.585839509964, 244.27557039260864, 245.96244597434998, 247.6493215560913, 249.33197474479675, 251.0146279335022, 252.694420337677, 254.3742127418518, 256.06747245788574, 257.7607321739197, 259.4261095523834, 261.09148693084717, 262.76919651031494, 264.4469060897827, 266.1231801509857, 267.7994542121887, 269.503142118454, 271.20683002471924, 272.8949730396271, 274.5831160545349, 276.2609474658966, 277.9387788772583, 279.61597776412964, 281.293176651001, 282.9716420173645, 284.650107383728, 286.30915808677673, 287.96820878982544, 289.63408756256104, 291.29996633529663, 292.97399163246155, 294.64801692962646, 296.3506989479065, 298.0533809661865, 299.7349190711975, 301.4164571762085, 303.12188506126404, 304.8273129463196, 306.54518389701843, 308.2630548477173, 309.93501925468445, 311.6069836616516, 313.3004288673401, 314.99387407302856, 316.6769781112671, 318.3600821495056, 320.03982949256897, 321.7195768356323, 323.3759014606476, 325.03222608566284, 326.6873631477356, 328.34250020980835, 330.00570487976074, 331.66890954971313, 333.3591990470886, 335.0494885444641, 336.72294092178345, 338.3963932991028, 340.6620898246765, 342.92778635025024]
[22.525, 22.525, 37.75, 37.75, 44.68333333333333, 44.68333333333333, 49.733333333333334, 49.733333333333334, 59.075, 59.075, 69.36666666666666, 69.36666666666666, 71.48333333333333, 71.48333333333333, 71.88333333333334, 71.88333333333334, 74.6, 74.6, 74.81666666666666, 74.81666666666666, 76.78333333333333, 76.78333333333333, 76.76666666666667, 76.76666666666667, 78.01666666666667, 78.01666666666667, 78.33333333333333, 78.33333333333333, 78.65833333333333, 78.65833333333333, 78.68333333333334, 78.68333333333334, 79.78333333333333, 79.78333333333333, 80.28333333333333, 80.28333333333333, 80.325, 80.325, 80.46666666666667, 80.46666666666667, 80.74166666666666, 80.74166666666666, 81.61666666666666, 81.61666666666666, 81.80833333333334, 81.80833333333334, 82.0, 82.0, 81.34166666666667, 81.34166666666667, 82.01666666666667, 82.01666666666667, 82.14166666666667, 82.14166666666667, 82.38333333333334, 82.38333333333334, 82.425, 82.425, 82.98333333333333, 82.98333333333333, 82.75833333333334, 82.75833333333334, 83.05, 83.05, 82.85833333333333, 82.85833333333333, 83.575, 83.575, 83.03333333333333, 83.03333333333333, 83.475, 83.475, 83.89166666666667, 83.89166666666667, 84.0, 84.0, 84.33333333333333, 84.33333333333333, 84.18333333333334, 84.18333333333334, 84.39166666666667, 84.39166666666667, 84.38333333333334, 84.38333333333334, 84.56666666666666, 84.56666666666666, 84.44166666666666, 84.44166666666666, 84.70833333333333, 84.70833333333333, 84.76666666666667, 84.76666666666667, 84.9, 84.9, 85.25, 85.25, 85.26666666666667, 85.26666666666667, 85.475, 85.475, 85.36666666666666, 85.36666666666666, 85.19166666666666, 85.19166666666666, 85.7, 85.7, 85.825, 85.825, 85.71666666666667, 85.71666666666667, 85.69166666666666, 85.69166666666666, 85.69166666666666, 85.69166666666666, 85.575, 85.575, 85.70833333333333, 85.70833333333333, 85.71666666666667, 85.71666666666667, 85.90833333333333, 85.90833333333333, 85.88333333333334, 85.88333333333334, 86.05833333333334, 86.05833333333334, 85.51666666666667, 85.51666666666667, 86.05, 86.05, 85.94166666666666, 85.94166666666666, 86.00833333333334, 86.00833333333334, 86.0, 86.0, 86.20833333333333, 86.20833333333333, 86.175, 86.175, 86.24166666666666, 86.24166666666666, 86.55, 86.55, 86.15, 86.15, 86.8, 86.8, 86.54166666666667, 86.54166666666667, 86.44166666666666, 86.44166666666666, 86.91666666666667, 86.91666666666667, 86.75833333333334, 86.75833333333334, 86.70833333333333, 86.70833333333333, 86.725, 86.725, 86.64166666666667, 86.64166666666667, 86.9, 86.9, 87.10833333333333, 87.10833333333333, 86.71666666666667, 86.71666666666667, 86.78333333333333, 86.78333333333333, 86.575, 86.575, 86.78333333333333, 86.78333333333333, 87.50833333333334, 87.50833333333334, 87.10833333333333, 87.10833333333333, 86.925, 86.925, 86.90833333333333, 86.90833333333333, 87.375, 87.375, 86.83333333333333, 86.83333333333333, 86.91666666666667, 86.91666666666667, 86.8, 86.8, 86.65, 86.65, 86.96666666666667, 86.96666666666667, 87.26666666666667, 87.26666666666667, 87.01666666666667, 87.01666666666667, 87.125, 87.125, 87.46666666666667, 87.46666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.177, Test loss: 1.752, Test accuracy: 37.27
Round   1, Train loss: 1.782, Test loss: 1.465, Test accuracy: 47.02
Round   2, Train loss: 1.605, Test loss: 1.336, Test accuracy: 52.37
Round   3, Train loss: 1.477, Test loss: 1.233, Test accuracy: 56.17
Round   4, Train loss: 1.388, Test loss: 1.162, Test accuracy: 59.48
Round   5, Train loss: 1.333, Test loss: 1.107, Test accuracy: 61.09
Round   6, Train loss: 1.251, Test loss: 1.055, Test accuracy: 63.41
Round   7, Train loss: 1.194, Test loss: 1.016, Test accuracy: 64.51
Round   8, Train loss: 1.167, Test loss: 0.978, Test accuracy: 65.82
Round   9, Train loss: 1.110, Test loss: 0.945, Test accuracy: 67.31
Round  10, Train loss: 1.056, Test loss: 0.922, Test accuracy: 68.17
Round  11, Train loss: 1.020, Test loss: 0.887, Test accuracy: 69.29
Round  12, Train loss: 0.988, Test loss: 0.875, Test accuracy: 69.92
Round  13, Train loss: 0.948, Test loss: 0.848, Test accuracy: 70.83
Round  14, Train loss: 0.911, Test loss: 0.834, Test accuracy: 71.54
Round  15, Train loss: 0.897, Test loss: 0.818, Test accuracy: 71.59
Round  16, Train loss: 0.903, Test loss: 0.817, Test accuracy: 71.69
Round  17, Train loss: 0.846, Test loss: 0.807, Test accuracy: 72.28
Round  18, Train loss: 0.858, Test loss: 0.782, Test accuracy: 73.06
Round  19, Train loss: 0.811, Test loss: 0.774, Test accuracy: 73.13
Round  20, Train loss: 0.772, Test loss: 0.786, Test accuracy: 72.99
Round  21, Train loss: 0.798, Test loss: 0.768, Test accuracy: 73.51
Round  22, Train loss: 0.799, Test loss: 0.768, Test accuracy: 73.66
Round  23, Train loss: 0.777, Test loss: 0.769, Test accuracy: 73.81
Round  24, Train loss: 0.737, Test loss: 0.761, Test accuracy: 74.09
Round  25, Train loss: 0.734, Test loss: 0.761, Test accuracy: 73.68
Round  26, Train loss: 0.737, Test loss: 0.744, Test accuracy: 74.46
Round  27, Train loss: 0.699, Test loss: 0.743, Test accuracy: 73.95
Round  28, Train loss: 0.701, Test loss: 0.743, Test accuracy: 74.50
Round  29, Train loss: 0.675, Test loss: 0.747, Test accuracy: 74.81
Round  30, Train loss: 0.673, Test loss: 0.738, Test accuracy: 74.94
Round  31, Train loss: 0.649, Test loss: 0.741, Test accuracy: 74.83
Round  32, Train loss: 0.641, Test loss: 0.732, Test accuracy: 75.23
Round  33, Train loss: 0.671, Test loss: 0.735, Test accuracy: 75.93
Round  34, Train loss: 0.626, Test loss: 0.734, Test accuracy: 75.88
Round  35, Train loss: 0.654, Test loss: 0.718, Test accuracy: 76.10
Round  36, Train loss: 0.651, Test loss: 0.722, Test accuracy: 75.93
Round  37, Train loss: 0.626, Test loss: 0.722, Test accuracy: 75.78
Round  38, Train loss: 0.633, Test loss: 0.717, Test accuracy: 76.27
Round  39, Train loss: 0.616, Test loss: 0.724, Test accuracy: 76.03
Round  40, Train loss: 0.635, Test loss: 0.718, Test accuracy: 75.75
Round  41, Train loss: 0.594, Test loss: 0.723, Test accuracy: 75.98
Round  42, Train loss: 0.582, Test loss: 0.719, Test accuracy: 76.20
Round  43, Train loss: 0.598, Test loss: 0.705, Test accuracy: 76.81
Round  44, Train loss: 0.567, Test loss: 0.719, Test accuracy: 76.27
Round  45, Train loss: 0.544, Test loss: 0.710, Test accuracy: 76.94
Round  46, Train loss: 0.613, Test loss: 0.711, Test accuracy: 76.37
Round  47, Train loss: 0.562, Test loss: 0.711, Test accuracy: 76.83
Round  48, Train loss: 0.541, Test loss: 0.719, Test accuracy: 76.31
Round  49, Train loss: 0.539, Test loss: 0.718, Test accuracy: 76.91
Round  50, Train loss: 0.527, Test loss: 0.724, Test accuracy: 76.75
Round  51, Train loss: 0.541, Test loss: 0.715, Test accuracy: 76.95
Round  52, Train loss: 0.519, Test loss: 0.712, Test accuracy: 77.05
Round  53, Train loss: 0.534, Test loss: 0.724, Test accuracy: 76.68
Round  54, Train loss: 0.581, Test loss: 0.708, Test accuracy: 76.68
Round  55, Train loss: 0.553, Test loss: 0.720, Test accuracy: 77.10
Round  56, Train loss: 0.518, Test loss: 0.717, Test accuracy: 76.70
Round  57, Train loss: 0.542, Test loss: 0.710, Test accuracy: 76.77
Round  58, Train loss: 0.535, Test loss: 0.713, Test accuracy: 77.11
Round  59, Train loss: 0.516, Test loss: 0.705, Test accuracy: 77.48
Round  60, Train loss: 0.495, Test loss: 0.717, Test accuracy: 77.22
Round  61, Train loss: 0.524, Test loss: 0.726, Test accuracy: 76.95
Round  62, Train loss: 0.488, Test loss: 0.720, Test accuracy: 77.33
Round  63, Train loss: 0.492, Test loss: 0.714, Test accuracy: 77.66
Round  64, Train loss: 0.498, Test loss: 0.718, Test accuracy: 77.02
Round  65, Train loss: 0.489, Test loss: 0.715, Test accuracy: 77.57
Round  66, Train loss: 0.525, Test loss: 0.701, Test accuracy: 77.80
Round  67, Train loss: 0.479, Test loss: 0.719, Test accuracy: 77.58
Round  68, Train loss: 0.479, Test loss: 0.710, Test accuracy: 77.61
Round  69, Train loss: 0.455, Test loss: 0.715, Test accuracy: 77.70
Round  70, Train loss: 0.440, Test loss: 0.714, Test accuracy: 77.74
Round  71, Train loss: 0.465, Test loss: 0.714, Test accuracy: 77.82
Round  72, Train loss: 0.441, Test loss: 0.718, Test accuracy: 78.25
Round  73, Train loss: 0.467, Test loss: 0.722, Test accuracy: 77.54
Round  74, Train loss: 0.454, Test loss: 0.712, Test accuracy: 77.95
Round  75, Train loss: 0.473, Test loss: 0.710, Test accuracy: 78.20
Round  76, Train loss: 0.448, Test loss: 0.726, Test accuracy: 78.20
Round  77, Train loss: 0.429, Test loss: 0.716, Test accuracy: 77.58
Round  78, Train loss: 0.443, Test loss: 0.721, Test accuracy: 78.04
Round  79, Train loss: 0.475, Test loss: 0.712, Test accuracy: 78.36
Round  80, Train loss: 0.419, Test loss: 0.713, Test accuracy: 78.60
Round  81, Train loss: 0.477, Test loss: 0.700, Test accuracy: 78.31
Round  82, Train loss: 0.453, Test loss: 0.715, Test accuracy: 77.98
Round  83, Train loss: 0.429, Test loss: 0.722, Test accuracy: 78.20
Round  84, Train loss: 0.435, Test loss: 0.727, Test accuracy: 78.19
Round  85, Train loss: 0.434, Test loss: 0.725, Test accuracy: 78.15
Round  86, Train loss: 0.465, Test loss: 0.716, Test accuracy: 78.04
Round  87, Train loss: 0.419, Test loss: 0.710, Test accuracy: 78.79
Round  88, Train loss: 0.416, Test loss: 0.722, Test accuracy: 78.36
Round  89, Train loss: 0.411, Test loss: 0.732, Test accuracy: 78.38
Round  90, Train loss: 0.449, Test loss: 0.727, Test accuracy: 78.43
Round  91, Train loss: 0.413, Test loss: 0.726, Test accuracy: 78.56
Round  92, Train loss: 0.440, Test loss: 0.711, Test accuracy: 78.50
Round  93, Train loss: 0.426, Test loss: 0.720, Test accuracy: 78.18
Round  94, Train loss: 0.408, Test loss: 0.720, Test accuracy: 78.16
Round  95, Train loss: 0.437, Test loss: 0.714, Test accuracy: 78.42
Round  96, Train loss: 0.415, Test loss: 0.713, Test accuracy: 78.65
Round  97, Train loss: 0.405, Test loss: 0.725, Test accuracy: 78.60
Round  98, Train loss: 0.407, Test loss: 0.723, Test accuracy: 78.45
Round  99, Train loss: 0.404, Test loss: 0.713, Test accuracy: 78.60
Final Round, Train loss: 0.322, Test loss: 0.725, Test accuracy: 79.16
Average accuracy final 10 rounds: 78.45350000000002
8432.12907743454
[13.369304418563843, 26.299339532852173, 38.4466826915741, 50.52797198295593, 62.58712291717529, 74.86281967163086, 87.25695776939392, 99.66393065452576, 111.84219861030579, 124.11370825767517, 136.31162428855896, 148.56913447380066, 160.73524379730225, 172.96464467048645, 185.25477647781372, 197.3619041442871, 209.06831407546997, 220.82384324073792, 232.60711216926575, 244.59658885002136, 256.34668827056885, 268.2441794872284, 280.141916513443, 291.93817496299744, 303.7970914840698, 315.7815546989441, 327.52292919158936, 339.57435369491577, 351.45363688468933, 363.33375811576843, 375.22437810897827, 387.0764241218567, 398.88139748573303, 410.7309160232544, 422.6157603263855, 434.3710482120514, 446.1443569660187, 457.99997425079346, 470.1041865348816, 481.7707920074463, 493.5021140575409, 505.30301690101624, 517.0368006229401, 528.8570907115936, 540.7967965602875, 552.7432839870453, 564.5141258239746, 576.393652677536, 588.2246296405792, 600.4761612415314, 612.3086159229279, 624.007022857666, 636.2462129592896, 648.4158136844635, 660.1565551757812, 672.0139625072479, 684.14910364151, 695.9877846240997, 707.7865755558014, 719.7268149852753, 731.7714757919312, 743.8887236118317, 756.2151899337769, 768.0014991760254, 779.7421543598175, 791.430406332016, 804.419926404953, 817.7013928890228, 831.0761868953705, 844.3758888244629, 857.6967587471008, 870.9932398796082, 884.2825508117676, 897.4162628650665, 910.5045778751373, 923.8413603305817, 935.6989064216614, 947.5665366649628, 959.498459815979, 971.3481860160828, 983.6899411678314, 995.5412633419037, 1007.4287331104279, 1019.5226519107819, 1031.541392326355, 1043.6018965244293, 1055.9045860767365, 1068.1432242393494, 1080.435010433197, 1092.9273138046265, 1105.1624455451965, 1117.3447897434235, 1130.9996507167816, 1144.7253804206848, 1158.3916356563568, 1171.6739764213562, 1183.7344152927399, 1195.8060808181763, 1208.0632963180542, 1220.11754155159, 1223.1805489063263]
[37.275, 47.025, 52.3675, 56.1725, 59.48, 61.09, 63.415, 64.51, 65.8175, 67.3125, 68.165, 69.2925, 69.9225, 70.83, 71.54, 71.5925, 71.69, 72.2775, 73.0625, 73.1275, 72.9925, 73.5075, 73.66, 73.8125, 74.095, 73.68, 74.4625, 73.955, 74.5, 74.81, 74.9425, 74.835, 75.2275, 75.93, 75.875, 76.0975, 75.9325, 75.78, 76.2675, 76.0275, 75.755, 75.9775, 76.1975, 76.81, 76.2725, 76.935, 76.3725, 76.8325, 76.31, 76.9125, 76.7525, 76.9525, 77.0475, 76.68, 76.6825, 77.1025, 76.6975, 76.7675, 77.115, 77.4825, 77.225, 76.9525, 77.3325, 77.6625, 77.0225, 77.5675, 77.7975, 77.58, 77.615, 77.7, 77.7375, 77.82, 78.245, 77.5375, 77.9475, 78.2, 78.2025, 77.575, 78.0425, 78.36, 78.6, 78.3125, 77.9825, 78.1975, 78.195, 78.1475, 78.04, 78.7875, 78.355, 78.375, 78.43, 78.555, 78.495, 78.18, 78.1575, 78.415, 78.6525, 78.6, 78.4475, 78.6025, 79.155]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.288, Test loss: 2.325, Test accuracy: 9.12
Round   0, Global train loss: 2.288, Global test loss: 2.326, Global test accuracy: 8.33
Round   1, Train loss: 2.335, Test loss: 2.324, Test accuracy: 10.27
Round   1, Global train loss: 2.335, Global test loss: 2.327, Global test accuracy: 8.25
Round   2, Train loss: 2.334, Test loss: 2.328, Test accuracy: 10.20
Round   2, Global train loss: 2.334, Global test loss: 2.331, Global test accuracy: 7.84
Round   3, Train loss: 2.341, Test loss: 2.331, Test accuracy: 9.99
Round   3, Global train loss: 2.341, Global test loss: 2.334, Global test accuracy: 8.22
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 12.49
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 10.82
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 10.30
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 8.70
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 8.34
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 8.34
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Average accuracy final 10 rounds: 11.666666666666663 

Average global accuracy final 10 rounds: 11.666666666666663 

5261.056010723114
[1.6897647380828857, 3.1104650497436523, 4.5367701053619385, 5.945721864700317, 7.372172832489014, 8.873396396636963, 10.296069383621216, 11.996683359146118, 13.72555160522461, 15.4724600315094, 17.224971294403076, 18.96814799308777, 20.70923686027527, 22.456308126449585, 24.1839280128479, 25.912931442260742, 27.642659187316895, 29.38943099975586, 31.104740858078003, 32.82296013832092, 34.55055117607117, 36.24919772148132, 37.953608751297, 39.70026469230652, 41.43212127685547, 43.16308832168579, 44.89879322052002, 46.638153314590454, 48.38321590423584, 50.13969612121582, 51.878164529800415, 53.63905143737793, 55.389256954193115, 57.13638782501221, 58.879406452178955, 60.61341309547424, 62.3493378162384, 64.07933115959167, 65.82010626792908, 67.5627179145813, 69.29274606704712, 71.02691149711609, 72.76233172416687, 74.49793362617493, 76.23272514343262, 77.94383525848389, 79.63821768760681, 81.3429183959961, 83.05426502227783, 84.77774047851562, 86.5536060333252, 88.26939940452576, 89.98022603988647, 91.70159339904785, 93.42458868026733, 95.14420199394226, 96.85185146331787, 98.568124294281, 100.28289771080017, 102.00534129142761, 103.72691226005554, 105.46718168258667, 107.18534851074219, 108.89849400520325, 110.61130928993225, 112.34310507774353, 114.07656598091125, 115.80286502838135, 117.5364978313446, 119.26338362693787, 120.99636244773865, 122.72097516059875, 124.44674563407898, 126.17792463302612, 127.8960874080658, 129.6155345439911, 131.3472080230713, 133.06343293190002, 134.79218649864197, 136.51524186134338, 138.2517282962799, 139.97557258605957, 141.70085430145264, 143.41739964485168, 145.14724588394165, 146.8677761554718, 148.5979323387146, 150.30866980552673, 152.03211855888367, 153.59225916862488, 155.27833676338196, 157.01007723808289, 158.73174905776978, 160.4358446598053, 162.15481567382812, 163.87055015563965, 165.6058406829834, 167.3206582069397, 169.03324699401855, 170.78251695632935, 172.49928855895996, 174.2094829082489, 175.9344823360443, 177.65036368370056, 179.37653183937073, 181.0923240184784, 182.79265236854553, 184.53528952598572, 186.2640106678009, 187.9837486743927, 189.71450090408325, 191.43729662895203, 193.17304039001465, 194.91154098510742, 196.63787317276, 198.38002061843872, 200.10271978378296, 201.8244321346283, 203.539165019989, 205.2407922744751, 206.95602679252625, 208.66735363006592, 210.37881588935852, 212.0967981815338, 213.80138754844666, 215.50519132614136, 217.23519802093506, 218.95864033699036, 220.7274785041809, 222.45433902740479, 224.19535398483276, 225.9242742061615, 227.65837478637695, 229.38873314857483, 231.1160569190979, 232.84638810157776, 234.57054471969604, 236.29470205307007, 238.0181655883789, 239.7446460723877, 241.4693694114685, 243.20249891281128, 244.93210411071777, 246.65949773788452, 248.39587426185608, 250.1249439716339, 251.85376262664795, 253.58400201797485, 255.31063723564148, 257.06107211112976, 258.79591846466064, 260.52634382247925, 262.2537612915039, 263.98015666007996, 265.71046924591064, 267.4335901737213, 269.15309882164, 270.8716309070587, 272.59154438972473, 274.30814385414124, 276.02984857559204, 277.7571177482605, 279.47315645217896, 281.1843845844269, 282.8777847290039, 284.57969760894775, 286.2651216983795, 287.9674687385559, 289.66156697273254, 291.361004114151, 293.0896303653717, 294.8154275417328, 296.5452208518982, 298.27651381492615, 300.0191659927368, 301.74277782440186, 303.4496660232544, 305.1507353782654, 306.84356570243835, 308.5510058403015, 310.28257989883423, 311.99458599090576, 313.7277252674103, 315.46709752082825, 317.1996912956238, 318.9188597202301, 320.634729385376, 322.34973764419556, 324.04978466033936, 325.76888966560364, 327.4416422843933, 329.13071489334106, 330.8384578227997, 332.53916025161743, 334.24903750419617, 335.98081064224243, 337.6815574169159, 339.37360978126526, 341.0692207813263, 342.7747097015381, 344.46418833732605, 346.15976190567017, 347.86749601364136, 349.58205580711365, 351.2877378463745, 352.99252939224243, 354.71901631355286, 356.4369955062866, 358.152813911438, 359.87897849082947, 361.59250807762146, 363.29058384895325, 364.9957752227783, 366.76552176475525, 368.4730043411255, 370.1649491786957, 371.8642635345459, 373.5844373703003, 375.31236124038696, 377.0234761238098, 378.71497082710266, 380.41899037361145, 382.1225383281708, 383.83049392700195, 385.5326404571533, 387.2496404647827, 388.9758710861206, 390.6786251068115, 392.402126789093, 394.10523867607117, 395.81296706199646, 397.5384590625763, 399.27134108543396, 400.98433351516724, 402.6425449848175, 404.27848196029663, 405.89906764030457, 407.58667159080505, 409.276682138443, 410.99808287620544, 412.6999042034149, 414.3934898376465, 416.0996196269989, 417.80518341064453, 419.48402190208435, 421.17879581451416, 422.87560749053955, 424.5630738735199, 426.2552011013031, 427.95428228378296, 429.65563344955444, 431.36610746383667, 433.0512852668762, 434.74078035354614, 436.4472599029541, 438.13442182540894, 439.8057346343994, 441.51188945770264, 443.2078788280487, 444.887423992157, 446.5689835548401, 448.2611927986145, 449.95740509033203, 451.64179825782776, 453.3429491519928, 455.0434536933899, 456.72666668891907, 458.4128632545471, 460.11239862442017, 461.7952878475189, 463.48556900024414, 465.19233894348145, 466.89017605781555, 468.57989859580994, 470.27685618400574, 471.9773635864258, 473.68380331993103, 475.37782526016235, 477.06900334358215, 478.7560782432556, 480.44239354133606, 482.1374559402466, 483.8291244506836, 485.51328682899475, 487.21284008026123, 488.8956792354584, 490.5951087474823, 492.2994599342346, 493.97850155830383, 495.67129158973694, 497.38404726982117, 499.0636131763458, 500.76168751716614, 502.4514603614807, 504.1441857814789, 505.83083391189575, 507.50616812705994, 509.2108747959137, 510.90049266815186, 512.5796272754669, 515.4311234951019]
[9.125, 10.266666666666667, 10.2, 9.991666666666667, 12.491666666666667, 10.816666666666666, 10.3, 8.7, 8.341666666666667, 8.341666666666667, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.985, Test loss: 1.845, Test accuracy: 31.17
Round   0, Global train loss: 0.985, Global test loss: 2.236, Global test accuracy: 21.37
Round   1, Train loss: 0.869, Test loss: 1.453, Test accuracy: 46.93
Round   1, Global train loss: 0.869, Global test loss: 2.135, Global test accuracy: 25.61
Round   2, Train loss: 0.790, Test loss: 1.171, Test accuracy: 57.04
Round   2, Global train loss: 0.790, Global test loss: 1.941, Global test accuracy: 37.09
Round   3, Train loss: 0.801, Test loss: 0.946, Test accuracy: 61.95
Round   3, Global train loss: 0.801, Global test loss: 1.747, Global test accuracy: 40.03
Round   4, Train loss: 0.732, Test loss: 0.866, Test accuracy: 63.08
Round   4, Global train loss: 0.732, Global test loss: 1.861, Global test accuracy: 29.49
Round   5, Train loss: 0.696, Test loss: 0.811, Test accuracy: 65.23
Round   5, Global train loss: 0.696, Global test loss: 1.817, Global test accuracy: 33.24
Round   6, Train loss: 0.627, Test loss: 0.684, Test accuracy: 70.83
Round   6, Global train loss: 0.627, Global test loss: 1.718, Global test accuracy: 45.57
Round   7, Train loss: 0.573, Test loss: 0.706, Test accuracy: 71.55
Round   7, Global train loss: 0.573, Global test loss: 2.067, Global test accuracy: 42.98
Round   8, Train loss: 0.692, Test loss: 0.669, Test accuracy: 72.71
Round   8, Global train loss: 0.692, Global test loss: 1.438, Global test accuracy: 49.37
Round   9, Train loss: 0.657, Test loss: 0.618, Test accuracy: 74.35
Round   9, Global train loss: 0.657, Global test loss: 1.884, Global test accuracy: 38.86
Round  10, Train loss: 0.655, Test loss: 0.643, Test accuracy: 73.12
Round  10, Global train loss: 0.655, Global test loss: 1.705, Global test accuracy: 40.07
Round  11, Train loss: 0.640, Test loss: 0.574, Test accuracy: 75.83
Round  11, Global train loss: 0.640, Global test loss: 1.576, Global test accuracy: 44.74
Round  12, Train loss: 0.522, Test loss: 0.581, Test accuracy: 75.61
Round  12, Global train loss: 0.522, Global test loss: 1.514, Global test accuracy: 48.71
Round  13, Train loss: 0.608, Test loss: 0.558, Test accuracy: 76.92
Round  13, Global train loss: 0.608, Global test loss: 1.342, Global test accuracy: 51.77
Round  14, Train loss: 0.535, Test loss: 0.545, Test accuracy: 77.66
Round  14, Global train loss: 0.535, Global test loss: 1.707, Global test accuracy: 39.51
Round  15, Train loss: 0.537, Test loss: 0.538, Test accuracy: 77.72
Round  15, Global train loss: 0.537, Global test loss: 1.454, Global test accuracy: 49.80
Round  16, Train loss: 0.597, Test loss: 0.545, Test accuracy: 77.38
Round  16, Global train loss: 0.597, Global test loss: 1.398, Global test accuracy: 51.27
Round  17, Train loss: 0.515, Test loss: 0.544, Test accuracy: 77.41
Round  17, Global train loss: 0.515, Global test loss: 1.395, Global test accuracy: 52.46
Round  18, Train loss: 0.558, Test loss: 0.509, Test accuracy: 79.08
Round  18, Global train loss: 0.558, Global test loss: 1.435, Global test accuracy: 50.18
Round  19, Train loss: 0.496, Test loss: 0.506, Test accuracy: 79.42
Round  19, Global train loss: 0.496, Global test loss: 1.276, Global test accuracy: 54.09
Round  20, Train loss: 0.478, Test loss: 0.501, Test accuracy: 79.54
Round  20, Global train loss: 0.478, Global test loss: 1.374, Global test accuracy: 53.88
Round  21, Train loss: 0.409, Test loss: 0.498, Test accuracy: 80.03
Round  21, Global train loss: 0.409, Global test loss: 1.259, Global test accuracy: 56.63
Round  22, Train loss: 0.546, Test loss: 0.491, Test accuracy: 80.37
Round  22, Global train loss: 0.546, Global test loss: 1.431, Global test accuracy: 50.73
Round  23, Train loss: 0.386, Test loss: 0.488, Test accuracy: 80.49
Round  23, Global train loss: 0.386, Global test loss: 1.502, Global test accuracy: 54.64
Round  24, Train loss: 0.499, Test loss: 0.494, Test accuracy: 80.42
Round  24, Global train loss: 0.499, Global test loss: 1.359, Global test accuracy: 53.09
Round  25, Train loss: 0.431, Test loss: 0.489, Test accuracy: 80.52
Round  25, Global train loss: 0.431, Global test loss: 1.253, Global test accuracy: 55.76
Round  26, Train loss: 0.369, Test loss: 0.483, Test accuracy: 80.81
Round  26, Global train loss: 0.369, Global test loss: 1.408, Global test accuracy: 50.61
Round  27, Train loss: 0.490, Test loss: 0.495, Test accuracy: 80.51
Round  27, Global train loss: 0.490, Global test loss: 1.203, Global test accuracy: 57.74
Round  28, Train loss: 0.352, Test loss: 0.495, Test accuracy: 80.52
Round  28, Global train loss: 0.352, Global test loss: 1.297, Global test accuracy: 56.65
Round  29, Train loss: 0.473, Test loss: 0.496, Test accuracy: 80.46
Round  29, Global train loss: 0.473, Global test loss: 1.349, Global test accuracy: 53.42
Round  30, Train loss: 0.360, Test loss: 0.493, Test accuracy: 80.60
Round  30, Global train loss: 0.360, Global test loss: 1.211, Global test accuracy: 58.92
Round  31, Train loss: 0.462, Test loss: 0.488, Test accuracy: 81.30
Round  31, Global train loss: 0.462, Global test loss: 1.251, Global test accuracy: 56.49
Round  32, Train loss: 0.353, Test loss: 0.477, Test accuracy: 81.67
Round  32, Global train loss: 0.353, Global test loss: 1.148, Global test accuracy: 60.86
Round  33, Train loss: 0.448, Test loss: 0.483, Test accuracy: 81.69
Round  33, Global train loss: 0.448, Global test loss: 1.184, Global test accuracy: 59.84
Round  34, Train loss: 0.434, Test loss: 0.496, Test accuracy: 81.39
Round  34, Global train loss: 0.434, Global test loss: 1.313, Global test accuracy: 53.90
Round  35, Train loss: 0.476, Test loss: 0.492, Test accuracy: 81.93
Round  35, Global train loss: 0.476, Global test loss: 1.450, Global test accuracy: 52.68
Round  36, Train loss: 0.337, Test loss: 0.485, Test accuracy: 82.11
Round  36, Global train loss: 0.337, Global test loss: 1.109, Global test accuracy: 61.33
Round  37, Train loss: 0.389, Test loss: 0.483, Test accuracy: 82.21
Round  37, Global train loss: 0.389, Global test loss: 1.264, Global test accuracy: 59.26
Round  38, Train loss: 0.343, Test loss: 0.483, Test accuracy: 82.03
Round  38, Global train loss: 0.343, Global test loss: 1.526, Global test accuracy: 54.22
Round  39, Train loss: 0.439, Test loss: 0.474, Test accuracy: 82.42
Round  39, Global train loss: 0.439, Global test loss: 1.385, Global test accuracy: 52.78
Round  40, Train loss: 0.373, Test loss: 0.474, Test accuracy: 82.67
Round  40, Global train loss: 0.373, Global test loss: 1.094, Global test accuracy: 62.85
Round  41, Train loss: 0.322, Test loss: 0.483, Test accuracy: 82.34
Round  41, Global train loss: 0.322, Global test loss: 1.375, Global test accuracy: 56.67
Round  42, Train loss: 0.349, Test loss: 0.488, Test accuracy: 82.07
Round  42, Global train loss: 0.349, Global test loss: 1.349, Global test accuracy: 56.27
Round  43, Train loss: 0.290, Test loss: 0.483, Test accuracy: 82.37
Round  43, Global train loss: 0.290, Global test loss: 1.115, Global test accuracy: 64.12
Round  44, Train loss: 0.382, Test loss: 0.489, Test accuracy: 82.10
Round  44, Global train loss: 0.382, Global test loss: 1.161, Global test accuracy: 61.01
Round  45, Train loss: 0.318, Test loss: 0.477, Test accuracy: 82.41
Round  45, Global train loss: 0.318, Global test loss: 1.448, Global test accuracy: 55.27
Round  46, Train loss: 0.292, Test loss: 0.474, Test accuracy: 82.37
Round  46, Global train loss: 0.292, Global test loss: 1.349, Global test accuracy: 59.11
Round  47, Train loss: 0.377, Test loss: 0.478, Test accuracy: 82.23
Round  47, Global train loss: 0.377, Global test loss: 1.190, Global test accuracy: 61.12
Round  48, Train loss: 0.308, Test loss: 0.466, Test accuracy: 83.04
Round  48, Global train loss: 0.308, Global test loss: 1.321, Global test accuracy: 55.81
Round  49, Train loss: 0.235, Test loss: 0.461, Test accuracy: 83.11
Round  49, Global train loss: 0.235, Global test loss: 1.212, Global test accuracy: 60.76
Round  50, Train loss: 0.332, Test loss: 0.484, Test accuracy: 82.59
Round  50, Global train loss: 0.332, Global test loss: 1.322, Global test accuracy: 57.82
Round  51, Train loss: 0.337, Test loss: 0.470, Test accuracy: 83.11
Round  51, Global train loss: 0.337, Global test loss: 1.601, Global test accuracy: 53.33
Round  52, Train loss: 0.330, Test loss: 0.485, Test accuracy: 82.70
Round  52, Global train loss: 0.330, Global test loss: 1.059, Global test accuracy: 64.23
Round  53, Train loss: 0.291, Test loss: 0.492, Test accuracy: 82.73
Round  53, Global train loss: 0.291, Global test loss: 1.212, Global test accuracy: 60.07
Round  54, Train loss: 0.235, Test loss: 0.488, Test accuracy: 82.44
Round  54, Global train loss: 0.235, Global test loss: 1.439, Global test accuracy: 58.71
Round  55, Train loss: 0.303, Test loss: 0.486, Test accuracy: 82.55
Round  55, Global train loss: 0.303, Global test loss: 1.181, Global test accuracy: 61.65
Round  56, Train loss: 0.343, Test loss: 0.491, Test accuracy: 82.67
Round  56, Global train loss: 0.343, Global test loss: 1.181, Global test accuracy: 62.37
Round  57, Train loss: 0.345, Test loss: 0.492, Test accuracy: 83.10
Round  57, Global train loss: 0.345, Global test loss: 1.076, Global test accuracy: 65.12
Round  58, Train loss: 0.273, Test loss: 0.505, Test accuracy: 82.59
Round  58, Global train loss: 0.273, Global test loss: 1.065, Global test accuracy: 64.96
Round  59, Train loss: 0.278, Test loss: 0.503, Test accuracy: 82.88
Round  59, Global train loss: 0.278, Global test loss: 1.325, Global test accuracy: 58.89
Round  60, Train loss: 0.263, Test loss: 0.489, Test accuracy: 83.13
Round  60, Global train loss: 0.263, Global test loss: 1.211, Global test accuracy: 60.92
Round  61, Train loss: 0.280, Test loss: 0.480, Test accuracy: 83.37
Round  61, Global train loss: 0.280, Global test loss: 1.476, Global test accuracy: 58.54
Round  62, Train loss: 0.297, Test loss: 0.473, Test accuracy: 83.68
Round  62, Global train loss: 0.297, Global test loss: 1.522, Global test accuracy: 56.97
Round  63, Train loss: 0.237, Test loss: 0.473, Test accuracy: 83.61
Round  63, Global train loss: 0.237, Global test loss: 1.017, Global test accuracy: 66.63
Round  64, Train loss: 0.218, Test loss: 0.484, Test accuracy: 83.57
Round  64, Global train loss: 0.218, Global test loss: 1.246, Global test accuracy: 62.45
Round  65, Train loss: 0.212, Test loss: 0.491, Test accuracy: 83.44
Round  65, Global train loss: 0.212, Global test loss: 1.077, Global test accuracy: 65.81
Round  66, Train loss: 0.261, Test loss: 0.491, Test accuracy: 83.36
Round  66, Global train loss: 0.261, Global test loss: 1.245, Global test accuracy: 62.91
Round  67, Train loss: 0.313, Test loss: 0.495, Test accuracy: 83.47
Round  67, Global train loss: 0.313, Global test loss: 1.245, Global test accuracy: 61.67
Round  68, Train loss: 0.239, Test loss: 0.498, Test accuracy: 83.41
Round  68, Global train loss: 0.239, Global test loss: 1.290, Global test accuracy: 62.03
Round  69, Train loss: 0.255, Test loss: 0.487, Test accuracy: 83.59
Round  69, Global train loss: 0.255, Global test loss: 1.237, Global test accuracy: 62.17
Round  70, Train loss: 0.280, Test loss: 0.488, Test accuracy: 83.79
Round  70, Global train loss: 0.280, Global test loss: 1.213, Global test accuracy: 61.30
Round  71, Train loss: 0.254, Test loss: 0.477, Test accuracy: 84.16
Round  71, Global train loss: 0.254, Global test loss: 1.056, Global test accuracy: 65.47
Round  72, Train loss: 0.222, Test loss: 0.483, Test accuracy: 84.21
Round  72, Global train loss: 0.222, Global test loss: 1.113, Global test accuracy: 65.68
Round  73, Train loss: 0.268, Test loss: 0.487, Test accuracy: 84.16
Round  73, Global train loss: 0.268, Global test loss: 1.171, Global test accuracy: 63.25
Round  74, Train loss: 0.317, Test loss: 0.477, Test accuracy: 84.44
Round  74, Global train loss: 0.317, Global test loss: 1.262, Global test accuracy: 61.05
Round  75, Train loss: 0.201, Test loss: 0.496, Test accuracy: 84.12
Round  75, Global train loss: 0.201, Global test loss: 1.109, Global test accuracy: 65.32
Round  76, Train loss: 0.228, Test loss: 0.496, Test accuracy: 84.23
Round  76, Global train loss: 0.228, Global test loss: 1.272, Global test accuracy: 62.89
Round  77, Train loss: 0.225, Test loss: 0.488, Test accuracy: 84.33
Round  77, Global train loss: 0.225, Global test loss: 1.035, Global test accuracy: 67.52
Round  78, Train loss: 0.235, Test loss: 0.495, Test accuracy: 84.39
Round  78, Global train loss: 0.235, Global test loss: 1.268, Global test accuracy: 63.33
Round  79, Train loss: 0.208, Test loss: 0.501, Test accuracy: 84.21
Round  79, Global train loss: 0.208, Global test loss: 1.108, Global test accuracy: 66.91
Round  80, Train loss: 0.221, Test loss: 0.496, Test accuracy: 84.27
Round  80, Global train loss: 0.221, Global test loss: 1.158, Global test accuracy: 65.62
Round  81, Train loss: 0.302, Test loss: 0.511, Test accuracy: 83.80
Round  81, Global train loss: 0.302, Global test loss: 1.701, Global test accuracy: 51.42
Round  82, Train loss: 0.204, Test loss: 0.509, Test accuracy: 83.92
Round  82, Global train loss: 0.204, Global test loss: 1.204, Global test accuracy: 62.64
Round  83, Train loss: 0.192, Test loss: 0.496, Test accuracy: 84.13
Round  83, Global train loss: 0.192, Global test loss: 1.130, Global test accuracy: 64.00
Round  84, Train loss: 0.212, Test loss: 0.510, Test accuracy: 83.98
Round  84, Global train loss: 0.212, Global test loss: 1.562, Global test accuracy: 57.52
Round  85, Train loss: 0.168, Test loss: 0.512, Test accuracy: 84.07
Round  85, Global train loss: 0.168, Global test loss: 1.771, Global test accuracy: 55.63
Round  86, Train loss: 0.149, Test loss: 0.522, Test accuracy: 84.07
Round  86, Global train loss: 0.149, Global test loss: 1.303, Global test accuracy: 64.65
Round  87, Train loss: 0.202, Test loss: 0.512, Test accuracy: 84.01
Round  87, Global train loss: 0.202, Global test loss: 1.934, Global test accuracy: 54.77
Round  88, Train loss: 0.289, Test loss: 0.514, Test accuracy: 84.12
Round  88, Global train loss: 0.289, Global test loss: 1.352, Global test accuracy: 61.53
Round  89, Train loss: 0.213, Test loss: 0.515, Test accuracy: 84.31
Round  89, Global train loss: 0.213, Global test loss: 1.153, Global test accuracy: 64.37
Round  90, Train loss: 0.162, Test loss: 0.510, Test accuracy: 84.43
Round  90, Global train loss: 0.162, Global test loss: 1.566, Global test accuracy: 61.44
Round  91, Train loss: 0.242, Test loss: 0.519, Test accuracy: 84.33
Round  91, Global train loss: 0.242, Global test loss: 1.473, Global test accuracy: 59.28
Round  92, Train loss: 0.214, Test loss: 0.513, Test accuracy: 84.29
Round  92, Global train loss: 0.214, Global test loss: 1.405, Global test accuracy: 60.39
Round  93, Train loss: 0.163, Test loss: 0.507, Test accuracy: 84.27
Round  93, Global train loss: 0.163, Global test loss: 1.731, Global test accuracy: 59.03
Round  94, Train loss: 0.191, Test loss: 0.503, Test accuracy: 84.51
Round  94, Global train loss: 0.191, Global test loss: 1.255, Global test accuracy: 63.96
Round  95, Train loss: 0.179, Test loss: 0.527, Test accuracy: 84.18
Round  95, Global train loss: 0.179, Global test loss: 1.179, Global test accuracy: 65.23
Round  96, Train loss: 0.227, Test loss: 0.512, Test accuracy: 84.26
Round  96, Global train loss: 0.227, Global test loss: 1.073, Global test accuracy: 67.37
Round  97, Train loss: 0.173, Test loss: 0.519, Test accuracy: 83.88
Round  97, Global train loss: 0.173, Global test loss: 1.244, Global test accuracy: 64.89
Round  98, Train loss: 0.248, Test loss: 0.513, Test accuracy: 83.93
Round  98, Global train loss: 0.248, Global test loss: 1.328, Global test accuracy: 63.31
Round  99, Train loss: 0.222, Test loss: 0.502, Test accuracy: 84.09
Round  99, Global train loss: 0.222, Global test loss: 1.742, Global test accuracy: 56.87
Final Round, Train loss: 0.153, Test loss: 0.575, Test accuracy: 84.45
Final Round, Global train loss: 0.153, Global test loss: 1.742, Global test accuracy: 56.87
Average accuracy final 10 rounds: 84.21833333333333 

Average global accuracy final 10 rounds: 62.17555555555555 

2833.134821653366
[2.4016149044036865, 4.803229808807373, 7.012286424636841, 9.221343040466309, 11.455860376358032, 13.690377712249756, 15.906191349029541, 18.122004985809326, 20.36532187461853, 22.608638763427734, 24.836382150650024, 27.064125537872314, 29.273868799209595, 31.483612060546875, 33.71494460105896, 35.946277141571045, 38.145962715148926, 40.34564828872681, 42.52066731452942, 44.69568634033203, 46.9120237827301, 49.128361225128174, 51.33857536315918, 53.548789501190186, 55.72874426841736, 57.90869903564453, 60.118372440338135, 62.32804584503174, 64.5115692615509, 66.69509267807007, 68.8934395313263, 71.09178638458252, 73.29396176338196, 75.4961371421814, 77.68141412734985, 79.86669111251831, 82.06453776359558, 84.26238441467285, 86.48195028305054, 88.70151615142822, 90.88584661483765, 93.07017707824707, 95.29557061195374, 97.5209641456604, 99.70928597450256, 101.89760780334473, 104.08822441101074, 106.27884101867676, 108.2003812789917, 110.12192153930664, 112.02437090873718, 113.92682027816772, 115.84733986854553, 117.76785945892334, 119.67775869369507, 121.5876579284668, 123.50976586341858, 125.43187379837036, 127.34127902984619, 129.25068426132202, 131.1902756690979, 133.12986707687378, 135.02187657356262, 136.91388607025146, 138.84124088287354, 140.7685956954956, 142.70142555236816, 144.63425540924072, 146.5515158176422, 148.4687762260437, 150.39155960083008, 152.31434297561646, 154.21547865867615, 156.11661434173584, 158.06829810142517, 160.0199818611145, 161.91736674308777, 163.81475162506104, 165.7339587211609, 167.65316581726074, 169.59036231040955, 171.52755880355835, 173.4397828578949, 175.35200691223145, 177.28317213058472, 179.214337348938, 181.125821352005, 183.03730535507202, 184.97380542755127, 186.91030550003052, 188.83478307724, 190.75926065444946, 192.66176557540894, 194.5642704963684, 196.5001950263977, 198.436119556427, 200.32905292510986, 202.22198629379272, 204.13728284835815, 206.05257940292358, 207.97217988967896, 209.89178037643433, 211.79194808006287, 213.6921157836914, 215.62221837043762, 217.55232095718384, 219.45832705497742, 221.364333152771, 223.2823190689087, 225.2003049850464, 227.11099886894226, 229.02169275283813, 230.96651220321655, 232.91133165359497, 234.82343697547913, 236.73554229736328, 238.648015499115, 240.5604887008667, 242.49922466278076, 244.43796062469482, 246.35991954803467, 248.2818784713745, 250.20813703536987, 252.13439559936523, 254.06607270240784, 255.99774980545044, 257.92362785339355, 259.84950590133667, 261.7741479873657, 263.6987900733948, 265.6449739933014, 267.591157913208, 269.5257065296173, 271.4602551460266, 273.381094455719, 275.3019337654114, 277.225355386734, 279.14877700805664, 281.0663642883301, 282.9839515686035, 284.90277767181396, 286.8216037750244, 288.7192049026489, 290.61680603027344, 292.5353116989136, 294.4538173675537, 296.3963325023651, 298.3388476371765, 300.23215198516846, 302.1254563331604, 304.06534218788147, 306.00522804260254, 307.90225982666016, 309.7992916107178, 311.72638463974, 313.6534776687622, 315.579466342926, 317.50545501708984, 319.4234821796417, 321.3415093421936, 323.28668212890625, 325.2318549156189, 327.1441180706024, 329.05638122558594, 330.97965598106384, 332.90293073654175, 334.81693744659424, 336.73094415664673, 338.6494610309601, 340.56797790527344, 342.5142502784729, 344.46052265167236, 346.6971640586853, 348.93380546569824, 351.2412836551666, 353.548761844635, 355.8669226169586, 358.1850833892822, 360.4893078804016, 362.793532371521, 365.090172290802, 367.386812210083, 369.73772644996643, 372.08864068984985, 374.38285779953003, 376.6770749092102, 378.95676851272583, 381.23646211624146, 383.449969291687, 385.66347646713257, 387.89108633995056, 390.11869621276855, 392.3456857204437, 394.5726752281189, 396.8675887584686, 399.16250228881836, 401.45608949661255, 403.74967670440674, 406.0513560771942, 408.3530354499817, 411.0249307155609, 413.69682598114014]
[31.166666666666668, 31.166666666666668, 46.93333333333333, 46.93333333333333, 57.03888888888889, 57.03888888888889, 61.95, 61.95, 63.077777777777776, 63.077777777777776, 65.23333333333333, 65.23333333333333, 70.83333333333333, 70.83333333333333, 71.55, 71.55, 72.71111111111111, 72.71111111111111, 74.35, 74.35, 73.12222222222222, 73.12222222222222, 75.82777777777778, 75.82777777777778, 75.61111111111111, 75.61111111111111, 76.91666666666667, 76.91666666666667, 77.66111111111111, 77.66111111111111, 77.72222222222223, 77.72222222222223, 77.38333333333334, 77.38333333333334, 77.40555555555555, 77.40555555555555, 79.07777777777778, 79.07777777777778, 79.41666666666667, 79.41666666666667, 79.54444444444445, 79.54444444444445, 80.02777777777777, 80.02777777777777, 80.37222222222222, 80.37222222222222, 80.49444444444444, 80.49444444444444, 80.41666666666667, 80.41666666666667, 80.52222222222223, 80.52222222222223, 80.81111111111112, 80.81111111111112, 80.5111111111111, 80.5111111111111, 80.51666666666667, 80.51666666666667, 80.46111111111111, 80.46111111111111, 80.6, 80.6, 81.3, 81.3, 81.67222222222222, 81.67222222222222, 81.68888888888888, 81.68888888888888, 81.39444444444445, 81.39444444444445, 81.93333333333334, 81.93333333333334, 82.11111111111111, 82.11111111111111, 82.20555555555555, 82.20555555555555, 82.02777777777777, 82.02777777777777, 82.42222222222222, 82.42222222222222, 82.66666666666667, 82.66666666666667, 82.34444444444445, 82.34444444444445, 82.06666666666666, 82.06666666666666, 82.36666666666666, 82.36666666666666, 82.1, 82.1, 82.40555555555555, 82.40555555555555, 82.37222222222222, 82.37222222222222, 82.22777777777777, 82.22777777777777, 83.04444444444445, 83.04444444444445, 83.10555555555555, 83.10555555555555, 82.58888888888889, 82.58888888888889, 83.10555555555555, 83.10555555555555, 82.7, 82.7, 82.72777777777777, 82.72777777777777, 82.43888888888888, 82.43888888888888, 82.55, 82.55, 82.66666666666667, 82.66666666666667, 83.1, 83.1, 82.59444444444445, 82.59444444444445, 82.88333333333334, 82.88333333333334, 83.12777777777778, 83.12777777777778, 83.36666666666666, 83.36666666666666, 83.68333333333334, 83.68333333333334, 83.61111111111111, 83.61111111111111, 83.56666666666666, 83.56666666666666, 83.44444444444444, 83.44444444444444, 83.35555555555555, 83.35555555555555, 83.46666666666667, 83.46666666666667, 83.40555555555555, 83.40555555555555, 83.58888888888889, 83.58888888888889, 83.78888888888889, 83.78888888888889, 84.15555555555555, 84.15555555555555, 84.21111111111111, 84.21111111111111, 84.15555555555555, 84.15555555555555, 84.43888888888888, 84.43888888888888, 84.12222222222222, 84.12222222222222, 84.22777777777777, 84.22777777777777, 84.33333333333333, 84.33333333333333, 84.39444444444445, 84.39444444444445, 84.21111111111111, 84.21111111111111, 84.26666666666667, 84.26666666666667, 83.8, 83.8, 83.92222222222222, 83.92222222222222, 84.12777777777778, 84.12777777777778, 83.98333333333333, 83.98333333333333, 84.07222222222222, 84.07222222222222, 84.06666666666666, 84.06666666666666, 84.00555555555556, 84.00555555555556, 84.11666666666666, 84.11666666666666, 84.31111111111112, 84.31111111111112, 84.43333333333334, 84.43333333333334, 84.33333333333333, 84.33333333333333, 84.29444444444445, 84.29444444444445, 84.27222222222223, 84.27222222222223, 84.5111111111111, 84.5111111111111, 84.17777777777778, 84.17777777777778, 84.25555555555556, 84.25555555555556, 83.88333333333334, 83.88333333333334, 83.92777777777778, 83.92777777777778, 84.09444444444445, 84.09444444444445, 84.45, 84.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.386, Test loss: 2.311, Test accuracy: 19.50
Round   1, Train loss: 0.996, Test loss: 2.339, Test accuracy: 24.98
Round   2, Train loss: 0.861, Test loss: 1.446, Test accuracy: 42.94
Round   3, Train loss: 0.865, Test loss: 1.221, Test accuracy: 50.91
Round   4, Train loss: 0.729, Test loss: 1.144, Test accuracy: 57.19
Round   5, Train loss: 0.740, Test loss: 0.931, Test accuracy: 61.53
Round   6, Train loss: 0.649, Test loss: 0.845, Test accuracy: 65.26
Round   7, Train loss: 0.698, Test loss: 0.907, Test accuracy: 61.50
Round   8, Train loss: 0.629, Test loss: 0.867, Test accuracy: 66.22
Round   9, Train loss: 0.646, Test loss: 0.789, Test accuracy: 67.51
Round  10, Train loss: 0.694, Test loss: 0.857, Test accuracy: 65.70
Round  11, Train loss: 0.657, Test loss: 0.768, Test accuracy: 70.74
Round  12, Train loss: 0.588, Test loss: 0.832, Test accuracy: 68.27
Round  13, Train loss: 0.611, Test loss: 0.803, Test accuracy: 69.78
Round  14, Train loss: 0.470, Test loss: 0.713, Test accuracy: 72.62
Round  15, Train loss: 0.554, Test loss: 0.845, Test accuracy: 68.22
Round  16, Train loss: 0.536, Test loss: 0.850, Test accuracy: 69.05
Round  17, Train loss: 0.573, Test loss: 0.758, Test accuracy: 70.21
Round  18, Train loss: 0.561, Test loss: 0.675, Test accuracy: 73.63
Round  19, Train loss: 0.523, Test loss: 0.593, Test accuracy: 74.52
Round  20, Train loss: 0.475, Test loss: 0.634, Test accuracy: 74.43
Round  21, Train loss: 0.582, Test loss: 0.518, Test accuracy: 78.64
Round  22, Train loss: 0.443, Test loss: 0.511, Test accuracy: 78.86
Round  23, Train loss: 0.602, Test loss: 0.507, Test accuracy: 79.22
Round  24, Train loss: 0.428, Test loss: 0.499, Test accuracy: 79.22
Round  25, Train loss: 0.446, Test loss: 0.487, Test accuracy: 79.94
Round  26, Train loss: 0.487, Test loss: 0.487, Test accuracy: 79.61
Round  27, Train loss: 0.441, Test loss: 0.489, Test accuracy: 79.54
Round  28, Train loss: 0.490, Test loss: 0.484, Test accuracy: 80.03
Round  29, Train loss: 0.510, Test loss: 0.476, Test accuracy: 80.47
Round  30, Train loss: 0.524, Test loss: 0.466, Test accuracy: 80.62
Round  31, Train loss: 0.451, Test loss: 0.461, Test accuracy: 81.19
Round  32, Train loss: 0.527, Test loss: 0.460, Test accuracy: 81.64
Round  33, Train loss: 0.423, Test loss: 0.456, Test accuracy: 81.22
Round  34, Train loss: 0.537, Test loss: 0.455, Test accuracy: 81.68
Round  35, Train loss: 0.388, Test loss: 0.446, Test accuracy: 81.84
Round  36, Train loss: 0.343, Test loss: 0.451, Test accuracy: 81.52
Round  37, Train loss: 0.451, Test loss: 0.445, Test accuracy: 82.06
Round  38, Train loss: 0.390, Test loss: 0.446, Test accuracy: 82.02
Round  39, Train loss: 0.454, Test loss: 0.443, Test accuracy: 82.07
Round  40, Train loss: 0.432, Test loss: 0.440, Test accuracy: 82.17
Round  41, Train loss: 0.381, Test loss: 0.431, Test accuracy: 82.94
Round  42, Train loss: 0.323, Test loss: 0.430, Test accuracy: 82.69
Round  43, Train loss: 0.335, Test loss: 0.428, Test accuracy: 82.68
Round  44, Train loss: 0.382, Test loss: 0.427, Test accuracy: 82.73
Round  45, Train loss: 0.361, Test loss: 0.425, Test accuracy: 82.94
Round  46, Train loss: 0.404, Test loss: 0.426, Test accuracy: 83.19
Round  47, Train loss: 0.377, Test loss: 0.435, Test accuracy: 82.72
Round  48, Train loss: 0.480, Test loss: 0.431, Test accuracy: 83.11
Round  49, Train loss: 0.319, Test loss: 0.420, Test accuracy: 83.27
Round  50, Train loss: 0.441, Test loss: 0.422, Test accuracy: 83.28
Round  51, Train loss: 0.315, Test loss: 0.414, Test accuracy: 83.57
Round  52, Train loss: 0.286, Test loss: 0.410, Test accuracy: 83.76
Round  53, Train loss: 0.299, Test loss: 0.412, Test accuracy: 83.67
Round  54, Train loss: 0.385, Test loss: 0.407, Test accuracy: 83.75
Round  55, Train loss: 0.389, Test loss: 0.410, Test accuracy: 83.62
Round  56, Train loss: 0.385, Test loss: 0.406, Test accuracy: 84.16
Round  57, Train loss: 0.318, Test loss: 0.410, Test accuracy: 84.07
Round  58, Train loss: 0.378, Test loss: 0.421, Test accuracy: 83.39
Round  59, Train loss: 0.384, Test loss: 0.404, Test accuracy: 84.22
Round  60, Train loss: 0.431, Test loss: 0.408, Test accuracy: 84.18
Round  61, Train loss: 0.328, Test loss: 0.413, Test accuracy: 84.00
Round  62, Train loss: 0.343, Test loss: 0.409, Test accuracy: 84.07
Round  63, Train loss: 0.304, Test loss: 0.397, Test accuracy: 84.55
Round  64, Train loss: 0.312, Test loss: 0.404, Test accuracy: 84.35
Round  65, Train loss: 0.339, Test loss: 0.397, Test accuracy: 84.57
Round  66, Train loss: 0.343, Test loss: 0.393, Test accuracy: 84.76
Round  67, Train loss: 0.311, Test loss: 0.397, Test accuracy: 84.51
Round  68, Train loss: 0.245, Test loss: 0.391, Test accuracy: 84.84
Round  69, Train loss: 0.273, Test loss: 0.398, Test accuracy: 84.85
Round  70, Train loss: 0.263, Test loss: 0.396, Test accuracy: 84.61
Round  71, Train loss: 0.238, Test loss: 0.398, Test accuracy: 84.66
Round  72, Train loss: 0.286, Test loss: 0.408, Test accuracy: 84.18
Round  73, Train loss: 0.269, Test loss: 0.397, Test accuracy: 84.75
Round  74, Train loss: 0.300, Test loss: 0.405, Test accuracy: 84.24
Round  75, Train loss: 0.322, Test loss: 0.408, Test accuracy: 84.36
Round  76, Train loss: 0.353, Test loss: 0.398, Test accuracy: 84.79
Round  77, Train loss: 0.302, Test loss: 0.403, Test accuracy: 84.59
Round  78, Train loss: 0.300, Test loss: 0.405, Test accuracy: 84.47
Round  79, Train loss: 0.265, Test loss: 0.403, Test accuracy: 84.84
Round  80, Train loss: 0.347, Test loss: 0.395, Test accuracy: 84.96
Round  81, Train loss: 0.294, Test loss: 0.406, Test accuracy: 84.62
Round  82, Train loss: 0.205, Test loss: 0.404, Test accuracy: 84.83
Round  83, Train loss: 0.294, Test loss: 0.402, Test accuracy: 84.89
Round  84, Train loss: 0.304, Test loss: 0.401, Test accuracy: 84.88
Round  85, Train loss: 0.235, Test loss: 0.399, Test accuracy: 85.01
Round  86, Train loss: 0.272, Test loss: 0.400, Test accuracy: 85.04
Round  87, Train loss: 0.325, Test loss: 0.397, Test accuracy: 85.23
Round  88, Train loss: 0.296, Test loss: 0.405, Test accuracy: 84.94
Round  89, Train loss: 0.260, Test loss: 0.406, Test accuracy: 84.73
Round  90, Train loss: 0.207, Test loss: 0.398, Test accuracy: 85.43
Round  91, Train loss: 0.292, Test loss: 0.404, Test accuracy: 84.78
Round  92, Train loss: 0.225, Test loss: 0.398, Test accuracy: 84.92
Round  93, Train loss: 0.212, Test loss: 0.400, Test accuracy: 85.25
Round  94, Train loss: 0.205, Test loss: 0.408, Test accuracy: 85.04
Round  95, Train loss: 0.257, Test loss: 0.400, Test accuracy: 85.14
Round  96, Train loss: 0.260, Test loss: 0.405, Test accuracy: 85.18
Round  97, Train loss: 0.227, Test loss: 0.393, Test accuracy: 85.28
Round  98, Train loss: 0.274, Test loss: 0.402, Test accuracy: 85.03
Round  99, Train loss: 0.261, Test loss: 0.406, Test accuracy: 85.13
Final Round, Train loss: 0.214, Test loss: 0.405, Test accuracy: 85.07
Average accuracy final 10 rounds: 85.11777777777777 

2333.3446278572083
[2.365551233291626, 4.731102466583252, 6.847913980484009, 8.964725494384766, 11.071794509887695, 13.178863525390625, 15.26019835472107, 17.341533184051514, 19.452868700027466, 21.564204216003418, 23.658418655395508, 25.752633094787598, 27.867848873138428, 29.983064651489258, 32.086771965026855, 34.19047927856445, 36.28501868247986, 38.379558086395264, 40.47948360443115, 42.57940912246704, 44.661837577819824, 46.74426603317261, 48.84394407272339, 50.94362211227417, 53.045536279678345, 55.14745044708252, 57.22991228103638, 59.312374114990234, 61.43362998962402, 63.55488586425781, 65.63088417053223, 67.70688247680664, 69.81917572021484, 71.93146896362305, 74.02909898757935, 76.12672901153564, 78.22069048881531, 80.31465196609497, 82.4161627292633, 84.51767349243164, 86.62031674385071, 88.72295999526978, 90.81126523017883, 92.89957046508789, 95.0066487789154, 97.11372709274292, 99.20278406143188, 101.29184103012085, 103.38247561454773, 105.47311019897461, 107.47196745872498, 109.47082471847534, 111.56369376182556, 113.65656280517578, 115.74385929107666, 117.83115577697754, 119.90162706375122, 121.9720983505249, 124.07439517974854, 126.17669200897217, 128.28869128227234, 130.4006905555725, 132.50739407539368, 134.61409759521484, 136.72972083091736, 138.84534406661987, 140.9349389076233, 143.0245337486267, 145.12181115150452, 147.21908855438232, 149.31430077552795, 151.40951299667358, 153.49825167655945, 155.5869903564453, 157.7097873687744, 159.83258438110352, 161.92433786392212, 164.01609134674072, 166.11687231063843, 168.21765327453613, 170.31695199012756, 172.416250705719, 174.50038695335388, 176.58452320098877, 178.68400025367737, 180.78347730636597, 182.89016199111938, 184.9968466758728, 187.07195854187012, 189.14707040786743, 191.13644576072693, 193.12582111358643, 195.19857168197632, 197.2713222503662, 199.36014533042908, 201.44896841049194, 203.55793690681458, 205.6669054031372, 207.78287291526794, 209.89884042739868, 212.0047755241394, 214.11071062088013, 216.21255683898926, 218.3144030570984, 220.40955233573914, 222.50470161437988, 224.63091230392456, 226.75712299346924, 228.85412788391113, 230.95113277435303, 233.05226182937622, 235.1533908843994, 237.26278471946716, 239.3721785545349, 241.47625279426575, 243.58032703399658, 245.67740416526794, 247.7744812965393, 249.88902711868286, 252.00357294082642, 254.0978262424469, 256.1920795440674, 258.31344866752625, 260.4348177909851, 262.5433158874512, 264.65181398391724, 266.7475907802582, 268.8433675765991, 270.966748714447, 273.0901298522949, 275.1825795173645, 277.2750291824341, 279.3846549987793, 281.4942808151245, 283.6054811477661, 285.7166814804077, 287.81133008003235, 289.905978679657, 292.0249996185303, 294.14402055740356, 296.2478108406067, 298.3516011238098, 300.4518163204193, 302.5520315170288, 304.65479135513306, 306.7575511932373, 308.8631684780121, 310.96878576278687, 313.08976101875305, 315.21073627471924, 317.30731105804443, 319.40388584136963, 321.5157208442688, 323.62755584716797, 325.7092397212982, 327.79092359542847, 329.887193441391, 331.9834632873535, 334.1081893444061, 336.23291540145874, 338.3375988006592, 340.4422821998596, 342.5553696155548, 344.66845703125, 346.7646493911743, 348.86084175109863, 350.9385344982147, 353.0162272453308, 355.1419322490692, 357.2676372528076, 359.38115072250366, 361.4946641921997, 363.6038918495178, 365.71311950683594, 367.81796526908875, 369.92281103134155, 372.0096769332886, 374.0965428352356, 376.21461248397827, 378.33268213272095, 380.43697023391724, 382.5412583351135, 384.6439573764801, 386.7466564178467, 388.8668439388275, 390.98703145980835, 393.08245372772217, 395.177875995636, 397.28905415534973, 399.4002323150635, 401.51558661460876, 403.63094091415405, 405.7206199169159, 407.81029891967773, 409.90278124809265, 411.99526357650757, 414.114399433136, 416.2335352897644, 418.34508204460144, 420.4566287994385, 422.5461587905884, 424.6356887817383]
[19.5, 19.5, 24.983333333333334, 24.983333333333334, 42.93888888888889, 42.93888888888889, 50.91111111111111, 50.91111111111111, 57.19444444444444, 57.19444444444444, 61.52777777777778, 61.52777777777778, 65.25555555555556, 65.25555555555556, 61.5, 61.5, 66.22222222222223, 66.22222222222223, 67.50555555555556, 67.50555555555556, 65.7, 65.7, 70.74444444444444, 70.74444444444444, 68.26666666666667, 68.26666666666667, 69.77777777777777, 69.77777777777777, 72.61666666666666, 72.61666666666666, 68.22222222222223, 68.22222222222223, 69.05, 69.05, 70.21111111111111, 70.21111111111111, 73.62777777777778, 73.62777777777778, 74.51666666666667, 74.51666666666667, 74.43333333333334, 74.43333333333334, 78.64444444444445, 78.64444444444445, 78.85555555555555, 78.85555555555555, 79.21666666666667, 79.21666666666667, 79.21666666666667, 79.21666666666667, 79.93888888888888, 79.93888888888888, 79.61111111111111, 79.61111111111111, 79.54444444444445, 79.54444444444445, 80.02777777777777, 80.02777777777777, 80.46666666666667, 80.46666666666667, 80.62222222222222, 80.62222222222222, 81.19444444444444, 81.19444444444444, 81.64444444444445, 81.64444444444445, 81.21666666666667, 81.21666666666667, 81.68333333333334, 81.68333333333334, 81.83888888888889, 81.83888888888889, 81.52222222222223, 81.52222222222223, 82.06111111111112, 82.06111111111112, 82.01666666666667, 82.01666666666667, 82.06666666666666, 82.06666666666666, 82.16666666666667, 82.16666666666667, 82.94444444444444, 82.94444444444444, 82.69444444444444, 82.69444444444444, 82.68333333333334, 82.68333333333334, 82.73333333333333, 82.73333333333333, 82.93888888888888, 82.93888888888888, 83.18888888888888, 83.18888888888888, 82.71666666666667, 82.71666666666667, 83.10555555555555, 83.10555555555555, 83.26666666666667, 83.26666666666667, 83.27777777777777, 83.27777777777777, 83.56666666666666, 83.56666666666666, 83.75555555555556, 83.75555555555556, 83.67222222222222, 83.67222222222222, 83.75, 83.75, 83.61666666666666, 83.61666666666666, 84.16111111111111, 84.16111111111111, 84.06666666666666, 84.06666666666666, 83.39444444444445, 83.39444444444445, 84.21666666666667, 84.21666666666667, 84.18333333333334, 84.18333333333334, 84.0, 84.0, 84.07222222222222, 84.07222222222222, 84.55, 84.55, 84.35, 84.35, 84.56666666666666, 84.56666666666666, 84.75555555555556, 84.75555555555556, 84.5111111111111, 84.5111111111111, 84.84444444444445, 84.84444444444445, 84.85, 84.85, 84.60555555555555, 84.60555555555555, 84.66111111111111, 84.66111111111111, 84.17777777777778, 84.17777777777778, 84.75, 84.75, 84.2388888888889, 84.2388888888889, 84.36111111111111, 84.36111111111111, 84.79444444444445, 84.79444444444445, 84.58888888888889, 84.58888888888889, 84.46666666666667, 84.46666666666667, 84.83888888888889, 84.83888888888889, 84.96111111111111, 84.96111111111111, 84.61666666666666, 84.61666666666666, 84.82777777777778, 84.82777777777778, 84.88888888888889, 84.88888888888889, 84.87777777777778, 84.87777777777778, 85.0111111111111, 85.0111111111111, 85.04444444444445, 85.04444444444445, 85.23333333333333, 85.23333333333333, 84.93888888888888, 84.93888888888888, 84.72777777777777, 84.72777777777777, 85.43333333333334, 85.43333333333334, 84.77777777777777, 84.77777777777777, 84.92222222222222, 84.92222222222222, 85.25, 85.25, 85.04444444444445, 85.04444444444445, 85.13888888888889, 85.13888888888889, 85.17777777777778, 85.17777777777778, 85.27777777777777, 85.27777777777777, 85.02777777777777, 85.02777777777777, 85.12777777777778, 85.12777777777778, 85.06666666666666, 85.06666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.569, Test loss: 2.043, Test accuracy: 32.29
Round   1, Train loss: 0.972, Test loss: 1.733, Test accuracy: 41.02
Round   2, Train loss: 0.857, Test loss: 1.479, Test accuracy: 47.14
Round   3, Train loss: 0.875, Test loss: 1.081, Test accuracy: 57.25
Round   4, Train loss: 0.789, Test loss: 1.001, Test accuracy: 59.90
Round   5, Train loss: 0.645, Test loss: 0.835, Test accuracy: 65.46
Round   6, Train loss: 0.781, Test loss: 0.802, Test accuracy: 67.49
Round   7, Train loss: 0.671, Test loss: 0.734, Test accuracy: 70.48
Round   8, Train loss: 0.716, Test loss: 0.648, Test accuracy: 73.30
Round   9, Train loss: 0.590, Test loss: 0.645, Test accuracy: 73.47
Round  10, Train loss: 0.636, Test loss: 0.639, Test accuracy: 73.92
Round  11, Train loss: 0.694, Test loss: 0.614, Test accuracy: 75.70
Round  12, Train loss: 0.594, Test loss: 0.590, Test accuracy: 75.85
Round  13, Train loss: 0.593, Test loss: 0.579, Test accuracy: 76.19
Round  14, Train loss: 0.585, Test loss: 0.567, Test accuracy: 77.38
Round  15, Train loss: 0.623, Test loss: 0.555, Test accuracy: 77.98
Round  16, Train loss: 0.569, Test loss: 0.536, Test accuracy: 78.58
Round  17, Train loss: 0.670, Test loss: 0.527, Test accuracy: 79.39
Round  18, Train loss: 0.554, Test loss: 0.541, Test accuracy: 79.30
Round  19, Train loss: 0.554, Test loss: 0.513, Test accuracy: 80.08
Round  20, Train loss: 0.568, Test loss: 0.511, Test accuracy: 80.25
Round  21, Train loss: 0.543, Test loss: 0.496, Test accuracy: 80.23
Round  22, Train loss: 0.497, Test loss: 0.511, Test accuracy: 80.42
Round  23, Train loss: 0.505, Test loss: 0.497, Test accuracy: 81.12
Round  24, Train loss: 0.524, Test loss: 0.489, Test accuracy: 81.38
Round  25, Train loss: 0.511, Test loss: 0.485, Test accuracy: 81.50
Round  26, Train loss: 0.488, Test loss: 0.470, Test accuracy: 81.81
Round  27, Train loss: 0.432, Test loss: 0.457, Test accuracy: 82.27
Round  28, Train loss: 0.372, Test loss: 0.445, Test accuracy: 82.66
Round  29, Train loss: 0.363, Test loss: 0.453, Test accuracy: 82.54
Round  30, Train loss: 0.493, Test loss: 0.446, Test accuracy: 82.92
Round  31, Train loss: 0.380, Test loss: 0.436, Test accuracy: 82.95
Round  32, Train loss: 0.413, Test loss: 0.428, Test accuracy: 83.23
Round  33, Train loss: 0.484, Test loss: 0.429, Test accuracy: 83.42
Round  34, Train loss: 0.390, Test loss: 0.421, Test accuracy: 83.59
Round  35, Train loss: 0.321, Test loss: 0.420, Test accuracy: 83.83
Round  36, Train loss: 0.328, Test loss: 0.414, Test accuracy: 84.01
Round  37, Train loss: 0.355, Test loss: 0.409, Test accuracy: 84.22
Round  38, Train loss: 0.400, Test loss: 0.405, Test accuracy: 84.21
Round  39, Train loss: 0.422, Test loss: 0.402, Test accuracy: 84.53
Round  40, Train loss: 0.383, Test loss: 0.400, Test accuracy: 84.78
Round  41, Train loss: 0.434, Test loss: 0.402, Test accuracy: 84.47
Round  42, Train loss: 0.393, Test loss: 0.389, Test accuracy: 84.73
Round  43, Train loss: 0.365, Test loss: 0.396, Test accuracy: 84.41
Round  44, Train loss: 0.375, Test loss: 0.390, Test accuracy: 84.88
Round  45, Train loss: 0.357, Test loss: 0.392, Test accuracy: 84.80
Round  46, Train loss: 0.321, Test loss: 0.377, Test accuracy: 85.52
Round  47, Train loss: 0.303, Test loss: 0.389, Test accuracy: 85.22
Round  48, Train loss: 0.322, Test loss: 0.377, Test accuracy: 85.57
Round  49, Train loss: 0.382, Test loss: 0.377, Test accuracy: 85.56
Round  50, Train loss: 0.365, Test loss: 0.373, Test accuracy: 85.67
Round  51, Train loss: 0.298, Test loss: 0.374, Test accuracy: 85.83
Round  52, Train loss: 0.305, Test loss: 0.365, Test accuracy: 85.76
Round  53, Train loss: 0.350, Test loss: 0.363, Test accuracy: 85.95
Round  54, Train loss: 0.366, Test loss: 0.361, Test accuracy: 85.81
Round  55, Train loss: 0.353, Test loss: 0.362, Test accuracy: 85.65
Round  56, Train loss: 0.343, Test loss: 0.363, Test accuracy: 85.68
Round  57, Train loss: 0.294, Test loss: 0.358, Test accuracy: 86.27
Round  58, Train loss: 0.289, Test loss: 0.360, Test accuracy: 86.32
Round  59, Train loss: 0.303, Test loss: 0.357, Test accuracy: 86.08
Round  60, Train loss: 0.286, Test loss: 0.356, Test accuracy: 86.23
Round  61, Train loss: 0.294, Test loss: 0.352, Test accuracy: 86.50
Round  62, Train loss: 0.289, Test loss: 0.352, Test accuracy: 86.41
Round  63, Train loss: 0.298, Test loss: 0.353, Test accuracy: 86.22
Round  64, Train loss: 0.274, Test loss: 0.351, Test accuracy: 86.42
Round  65, Train loss: 0.245, Test loss: 0.345, Test accuracy: 86.62
Round  66, Train loss: 0.309, Test loss: 0.342, Test accuracy: 86.87
Round  67, Train loss: 0.310, Test loss: 0.350, Test accuracy: 86.26
Round  68, Train loss: 0.289, Test loss: 0.348, Test accuracy: 86.61
Round  69, Train loss: 0.256, Test loss: 0.349, Test accuracy: 86.55
Round  70, Train loss: 0.247, Test loss: 0.343, Test accuracy: 86.68
Round  71, Train loss: 0.227, Test loss: 0.339, Test accuracy: 87.21
Round  72, Train loss: 0.388, Test loss: 0.337, Test accuracy: 86.94
Round  73, Train loss: 0.220, Test loss: 0.330, Test accuracy: 87.32
Round  74, Train loss: 0.247, Test loss: 0.335, Test accuracy: 87.08
Round  75, Train loss: 0.257, Test loss: 0.333, Test accuracy: 87.23
Round  76, Train loss: 0.385, Test loss: 0.337, Test accuracy: 87.04
Round  77, Train loss: 0.297, Test loss: 0.330, Test accuracy: 87.05
Round  78, Train loss: 0.257, Test loss: 0.337, Test accuracy: 86.79
Round  79, Train loss: 0.320, Test loss: 0.333, Test accuracy: 87.24
Round  80, Train loss: 0.236, Test loss: 0.329, Test accuracy: 87.33
Round  81, Train loss: 0.257, Test loss: 0.328, Test accuracy: 87.28
Round  82, Train loss: 0.170, Test loss: 0.335, Test accuracy: 87.12
Round  83, Train loss: 0.239, Test loss: 0.330, Test accuracy: 87.26
Round  84, Train loss: 0.285, Test loss: 0.330, Test accuracy: 87.49
Round  85, Train loss: 0.230, Test loss: 0.335, Test accuracy: 86.93
Round  86, Train loss: 0.216, Test loss: 0.329, Test accuracy: 87.48
Round  87, Train loss: 0.256, Test loss: 0.329, Test accuracy: 87.38
Round  88, Train loss: 0.236, Test loss: 0.331, Test accuracy: 87.17
Round  89, Train loss: 0.196, Test loss: 0.327, Test accuracy: 87.57
Round  90, Train loss: 0.237, Test loss: 0.319, Test accuracy: 87.92
Round  91, Train loss: 0.338, Test loss: 0.326, Test accuracy: 87.64
Round  92, Train loss: 0.241, Test loss: 0.329, Test accuracy: 87.46
Round  93, Train loss: 0.205, Test loss: 0.326, Test accuracy: 87.78
Round  94, Train loss: 0.232, Test loss: 0.332, Test accuracy: 87.32
Round  95, Train loss: 0.241, Test loss: 0.326, Test accuracy: 87.38
Round  96, Train loss: 0.300, Test loss: 0.322, Test accuracy: 87.76
Round  97, Train loss: 0.236, Test loss: 0.330, Test accuracy: 87.58
Round  98, Train loss: 0.255, Test loss: 0.330, Test accuracy: 87.42
Round  99, Train loss: 0.262, Test loss: 0.327, Test accuracy: 87.62
Final Round, Train loss: 0.186, Test loss: 0.326, Test accuracy: 87.72
Average accuracy final 10 rounds: 87.58666666666669
1832.1386976242065
[2.249763011932373, 4.499526023864746, 6.3290886878967285, 8.158651351928711, 9.987602710723877, 11.816554069519043, 13.64635419845581, 15.476154327392578, 17.215982675552368, 18.955811023712158, 20.738991498947144, 22.52217197418213, 24.370408535003662, 26.218645095825195, 27.98804998397827, 29.757454872131348, 31.552801847457886, 33.348148822784424, 35.15301728248596, 36.9578857421875, 38.7768280506134, 40.59577035903931, 42.42637252807617, 44.25697469711304, 46.07093262672424, 47.88489055633545, 49.722687005996704, 51.56048345565796, 53.39793801307678, 55.235392570495605, 57.060667753219604, 58.8859429359436, 60.72085094451904, 62.55575895309448, 64.39178919792175, 66.22781944274902, 68.06129431724548, 69.89476919174194, 71.73817014694214, 73.58157110214233, 75.41786623001099, 77.25416135787964, 79.09085369110107, 80.92754602432251, 82.75223159790039, 84.57691717147827, 86.41985845565796, 88.26279973983765, 90.11657166481018, 91.97034358978271, 93.79968094825745, 95.62901830673218, 97.47020077705383, 99.31138324737549, 101.16530251502991, 103.01922178268433, 104.84561347961426, 106.67200517654419, 108.53128695487976, 110.39056873321533, 112.25258874893188, 114.11460876464844, 115.94674253463745, 117.77887630462646, 119.60911250114441, 121.43934869766235, 123.26531887054443, 125.09128904342651, 126.94058346748352, 128.78987789154053, 130.6288125514984, 132.4677472114563, 134.2756371498108, 136.08352708816528, 137.930330991745, 139.7771348953247, 141.63544750213623, 143.49376010894775, 145.32472705841064, 147.15569400787354, 148.99863839149475, 150.84158277511597, 152.67040014266968, 154.4992175102234, 156.32602882385254, 158.1528401374817, 159.98624110221863, 161.81964206695557, 163.65691781044006, 165.49419355392456, 167.3439588546753, 169.19372415542603, 171.03068780899048, 172.86765146255493, 174.71495294570923, 176.56225442886353, 178.40275502204895, 180.24325561523438, 182.07878398895264, 183.9143123626709, 185.75746512413025, 187.6006178855896, 189.42790842056274, 191.2551989555359, 193.09253430366516, 194.92986965179443, 196.77576398849487, 198.6216583251953, 200.46115970611572, 202.30066108703613, 204.15889930725098, 206.01713752746582, 207.88047194480896, 209.7438063621521, 211.58083128929138, 213.41785621643066, 215.26652669906616, 217.11519718170166, 218.97721767425537, 220.83923816680908, 222.6544759273529, 224.46971368789673, 226.31460523605347, 228.1594967842102, 229.99041056632996, 231.8213243484497, 233.6211724281311, 235.4210205078125, 237.28058505058289, 239.14014959335327, 240.9813859462738, 242.82262229919434, 244.6414647102356, 246.46030712127686, 248.2891914844513, 250.11807584762573, 251.94214868545532, 253.7662215232849, 255.57648301124573, 257.38674449920654, 259.2162654399872, 261.0457863807678, 262.8674991130829, 264.68921184539795, 266.52648973464966, 268.36376762390137, 270.19703698158264, 272.0303063392639, 273.8689193725586, 275.70753240585327, 277.53695726394653, 279.3663821220398, 281.2070596218109, 283.04773712158203, 284.8910393714905, 286.7343416213989, 288.58574962615967, 290.4371576309204, 292.2598602771759, 294.0825629234314, 295.92234802246094, 297.7621331214905, 299.5787351131439, 301.39533710479736, 303.1435375213623, 304.89173793792725, 306.6864233016968, 308.4811086654663, 310.27520513534546, 312.0693016052246, 313.90711760520935, 315.7449336051941, 317.5468556880951, 319.3487777709961, 321.1493842601776, 322.94999074935913, 324.85571551322937, 326.7614402770996, 328.54672050476074, 330.3320007324219, 332.188823223114, 334.04564571380615, 335.87248373031616, 337.6993217468262, 339.4939432144165, 341.28856468200684, 343.10835576057434, 344.92814683914185, 346.76160168647766, 348.5950565338135, 350.38977670669556, 352.18449687957764, 354.0317816734314, 355.87906646728516, 357.7226622104645, 359.5662579536438, 361.37443804740906, 363.1826181411743, 365.0174765586853, 366.8523349761963, 369.1066198348999, 371.3609046936035]
[32.291666666666664, 32.291666666666664, 41.025, 41.025, 47.141666666666666, 47.141666666666666, 57.25, 57.25, 59.9, 59.9, 65.45833333333333, 65.45833333333333, 67.49166666666666, 67.49166666666666, 70.48333333333333, 70.48333333333333, 73.3, 73.3, 73.46666666666667, 73.46666666666667, 73.925, 73.925, 75.7, 75.7, 75.85, 75.85, 76.19166666666666, 76.19166666666666, 77.375, 77.375, 77.98333333333333, 77.98333333333333, 78.58333333333333, 78.58333333333333, 79.39166666666667, 79.39166666666667, 79.3, 79.3, 80.075, 80.075, 80.25, 80.25, 80.23333333333333, 80.23333333333333, 80.41666666666667, 80.41666666666667, 81.11666666666666, 81.11666666666666, 81.38333333333334, 81.38333333333334, 81.5, 81.5, 81.80833333333334, 81.80833333333334, 82.26666666666667, 82.26666666666667, 82.65833333333333, 82.65833333333333, 82.54166666666667, 82.54166666666667, 82.925, 82.925, 82.95, 82.95, 83.23333333333333, 83.23333333333333, 83.41666666666667, 83.41666666666667, 83.59166666666667, 83.59166666666667, 83.83333333333333, 83.83333333333333, 84.00833333333334, 84.00833333333334, 84.225, 84.225, 84.20833333333333, 84.20833333333333, 84.525, 84.525, 84.78333333333333, 84.78333333333333, 84.475, 84.475, 84.73333333333333, 84.73333333333333, 84.40833333333333, 84.40833333333333, 84.88333333333334, 84.88333333333334, 84.8, 84.8, 85.51666666666667, 85.51666666666667, 85.21666666666667, 85.21666666666667, 85.56666666666666, 85.56666666666666, 85.55833333333334, 85.55833333333334, 85.675, 85.675, 85.83333333333333, 85.83333333333333, 85.75833333333334, 85.75833333333334, 85.95, 85.95, 85.80833333333334, 85.80833333333334, 85.65, 85.65, 85.68333333333334, 85.68333333333334, 86.26666666666667, 86.26666666666667, 86.31666666666666, 86.31666666666666, 86.075, 86.075, 86.23333333333333, 86.23333333333333, 86.5, 86.5, 86.40833333333333, 86.40833333333333, 86.225, 86.225, 86.41666666666667, 86.41666666666667, 86.61666666666666, 86.61666666666666, 86.86666666666666, 86.86666666666666, 86.25833333333334, 86.25833333333334, 86.60833333333333, 86.60833333333333, 86.55, 86.55, 86.68333333333334, 86.68333333333334, 87.20833333333333, 87.20833333333333, 86.94166666666666, 86.94166666666666, 87.31666666666666, 87.31666666666666, 87.08333333333333, 87.08333333333333, 87.23333333333333, 87.23333333333333, 87.04166666666667, 87.04166666666667, 87.05, 87.05, 86.79166666666667, 86.79166666666667, 87.24166666666666, 87.24166666666666, 87.325, 87.325, 87.275, 87.275, 87.125, 87.125, 87.25833333333334, 87.25833333333334, 87.49166666666666, 87.49166666666666, 86.93333333333334, 86.93333333333334, 87.48333333333333, 87.48333333333333, 87.38333333333334, 87.38333333333334, 87.16666666666667, 87.16666666666667, 87.56666666666666, 87.56666666666666, 87.91666666666667, 87.91666666666667, 87.64166666666667, 87.64166666666667, 87.45833333333333, 87.45833333333333, 87.775, 87.775, 87.31666666666666, 87.31666666666666, 87.38333333333334, 87.38333333333334, 87.75833333333334, 87.75833333333334, 87.575, 87.575, 87.41666666666667, 87.41666666666667, 87.625, 87.625, 87.725, 87.725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.101, Test loss: 2.339, Test accuracy: 24.63
Round   1, Train loss: 0.997, Test loss: 2.139, Test accuracy: 35.29
Round   2, Train loss: 0.876, Test loss: 2.124, Test accuracy: 35.82
Round   3, Train loss: 0.893, Test loss: 2.223, Test accuracy: 31.79
Round   4, Train loss: 0.738, Test loss: 2.050, Test accuracy: 39.26
Round   5, Train loss: 0.760, Test loss: 1.906, Test accuracy: 41.46
Round   6, Train loss: 0.766, Test loss: 2.237, Test accuracy: 30.38
Round   7, Train loss: 0.724, Test loss: 1.742, Test accuracy: 43.22
Round   8, Train loss: 0.683, Test loss: 1.579, Test accuracy: 44.55
Round   9, Train loss: 0.714, Test loss: 2.046, Test accuracy: 38.95
Round  10, Train loss: 0.633, Test loss: 1.596, Test accuracy: 46.92
Round  11, Train loss: 0.668, Test loss: 1.526, Test accuracy: 46.34
Round  12, Train loss: 0.727, Test loss: 1.827, Test accuracy: 40.15
Round  13, Train loss: 0.531, Test loss: 1.653, Test accuracy: 44.00
Round  14, Train loss: 0.506, Test loss: 1.626, Test accuracy: 47.79
Round  15, Train loss: 0.609, Test loss: 1.369, Test accuracy: 51.83
Round  16, Train loss: 0.520, Test loss: 1.479, Test accuracy: 50.48
Round  17, Train loss: 0.527, Test loss: 1.527, Test accuracy: 49.17
Round  18, Train loss: 0.539, Test loss: 1.720, Test accuracy: 43.68
Round  19, Train loss: 0.660, Test loss: 1.790, Test accuracy: 44.83
Round  20, Train loss: 0.445, Test loss: 1.312, Test accuracy: 55.83
Round  21, Train loss: 0.417, Test loss: 1.275, Test accuracy: 55.62
Round  22, Train loss: 0.534, Test loss: 1.396, Test accuracy: 51.90
Round  23, Train loss: 0.537, Test loss: 1.485, Test accuracy: 50.80
Round  24, Train loss: 0.511, Test loss: 1.357, Test accuracy: 56.35
Round  25, Train loss: 0.370, Test loss: 1.567, Test accuracy: 53.34
Round  26, Train loss: 0.500, Test loss: 1.510, Test accuracy: 51.21
Round  27, Train loss: 0.360, Test loss: 1.393, Test accuracy: 55.13
Round  28, Train loss: 0.443, Test loss: 1.599, Test accuracy: 50.93
Round  29, Train loss: 0.433, Test loss: 1.338, Test accuracy: 54.23
Round  30, Train loss: 0.394, Test loss: 1.116, Test accuracy: 61.77
Round  31, Train loss: 0.469, Test loss: 1.380, Test accuracy: 52.14
Round  32, Train loss: 0.375, Test loss: 1.365, Test accuracy: 56.25
Round  33, Train loss: 0.400, Test loss: 1.356, Test accuracy: 57.73
Round  34, Train loss: 0.345, Test loss: 1.359, Test accuracy: 56.25
Round  35, Train loss: 0.375, Test loss: 1.201, Test accuracy: 60.77
Round  36, Train loss: 0.362, Test loss: 1.426, Test accuracy: 56.25
Round  37, Train loss: 0.368, Test loss: 1.364, Test accuracy: 55.31
Round  38, Train loss: 0.353, Test loss: 1.299, Test accuracy: 58.48
Round  39, Train loss: 0.349, Test loss: 1.262, Test accuracy: 57.98
Round  40, Train loss: 0.365, Test loss: 1.150, Test accuracy: 60.49
Round  41, Train loss: 0.291, Test loss: 1.288, Test accuracy: 60.17
Round  42, Train loss: 0.320, Test loss: 1.212, Test accuracy: 60.21
Round  43, Train loss: 0.318, Test loss: 1.246, Test accuracy: 60.93
Round  44, Train loss: 0.418, Test loss: 1.445, Test accuracy: 54.50
Round  45, Train loss: 0.289, Test loss: 1.529, Test accuracy: 57.48
Round  46, Train loss: 0.309, Test loss: 1.222, Test accuracy: 61.56
Round  47, Train loss: 0.282, Test loss: 1.544, Test accuracy: 55.35
Round  48, Train loss: 0.275, Test loss: 1.327, Test accuracy: 57.74
Round  49, Train loss: 0.388, Test loss: 1.573, Test accuracy: 51.34
Round  50, Train loss: 0.341, Test loss: 1.245, Test accuracy: 58.87
Round  51, Train loss: 0.243, Test loss: 1.171, Test accuracy: 63.06
Round  52, Train loss: 0.248, Test loss: 1.254, Test accuracy: 62.01
Round  53, Train loss: 0.259, Test loss: 1.258, Test accuracy: 60.67
Round  54, Train loss: 0.259, Test loss: 1.140, Test accuracy: 64.08
Round  55, Train loss: 0.255, Test loss: 1.176, Test accuracy: 62.46
Round  56, Train loss: 0.270, Test loss: 1.237, Test accuracy: 61.52
Round  57, Train loss: 0.211, Test loss: 1.324, Test accuracy: 61.30
Round  58, Train loss: 0.197, Test loss: 1.610, Test accuracy: 55.60
Round  59, Train loss: 0.317, Test loss: 1.204, Test accuracy: 60.52
Round  60, Train loss: 0.273, Test loss: 1.224, Test accuracy: 59.69
Round  61, Train loss: 0.258, Test loss: 1.297, Test accuracy: 60.24
Round  62, Train loss: 0.274, Test loss: 1.188, Test accuracy: 62.96
Round  63, Train loss: 0.204, Test loss: 1.204, Test accuracy: 63.44
Round  64, Train loss: 0.231, Test loss: 1.195, Test accuracy: 62.45
Round  65, Train loss: 0.209, Test loss: 1.058, Test accuracy: 66.16
Round  66, Train loss: 0.278, Test loss: 1.232, Test accuracy: 61.28
Round  67, Train loss: 0.219, Test loss: 1.270, Test accuracy: 61.89
Round  68, Train loss: 0.276, Test loss: 1.470, Test accuracy: 58.20
Round  69, Train loss: 0.233, Test loss: 1.440, Test accuracy: 59.64
Round  70, Train loss: 0.149, Test loss: 1.254, Test accuracy: 63.10
Round  71, Train loss: 0.202, Test loss: 1.498, Test accuracy: 58.90
Round  72, Train loss: 0.163, Test loss: 1.579, Test accuracy: 58.74
Round  73, Train loss: 0.259, Test loss: 1.228, Test accuracy: 62.68
Round  74, Train loss: 0.245, Test loss: 1.086, Test accuracy: 65.12
Round  75, Train loss: 0.219, Test loss: 1.265, Test accuracy: 61.84
Round  76, Train loss: 0.229, Test loss: 1.061, Test accuracy: 66.14
Round  77, Train loss: 0.197, Test loss: 1.286, Test accuracy: 63.35
Round  78, Train loss: 0.223, Test loss: 1.119, Test accuracy: 64.85
Round  79, Train loss: 0.153, Test loss: 1.581, Test accuracy: 58.58
Round  80, Train loss: 0.182, Test loss: 1.225, Test accuracy: 63.33
Round  81, Train loss: 0.218, Test loss: 1.173, Test accuracy: 65.30
Round  82, Train loss: 0.156, Test loss: 1.166, Test accuracy: 64.12
Round  83, Train loss: 0.134, Test loss: 1.310, Test accuracy: 63.73
Round  84, Train loss: 0.233, Test loss: 1.303, Test accuracy: 61.86
Round  85, Train loss: 0.241, Test loss: 1.173, Test accuracy: 64.98
Round  86, Train loss: 0.245, Test loss: 1.216, Test accuracy: 64.62
Round  87, Train loss: 0.185, Test loss: 1.150, Test accuracy: 64.60
Round  88, Train loss: 0.145, Test loss: 1.249, Test accuracy: 62.70
Round  89, Train loss: 0.165, Test loss: 1.116, Test accuracy: 66.73
Round  90, Train loss: 0.216, Test loss: 1.304, Test accuracy: 61.73
Round  91, Train loss: 0.181, Test loss: 1.403, Test accuracy: 60.12
Round  92, Train loss: 0.122, Test loss: 1.404, Test accuracy: 61.87
Round  93, Train loss: 0.213, Test loss: 1.077, Test accuracy: 66.26
Round  94, Train loss: 0.150, Test loss: 1.198, Test accuracy: 64.97
Round  95, Train loss: 0.191, Test loss: 1.304, Test accuracy: 62.28
Round  96, Train loss: 0.149, Test loss: 1.356, Test accuracy: 63.48
Round  97, Train loss: 0.132, Test loss: 1.491, Test accuracy: 61.24
Round  98, Train loss: 0.189, Test loss: 1.426, Test accuracy: 59.85
Round  99, Train loss: 0.204, Test loss: 1.511, Test accuracy: 60.23
Final Round, Train loss: 0.151, Test loss: 1.072, Test accuracy: 67.05
Average accuracy final 10 rounds: 62.20250000000001
2820.943195581436
[4.2410595417022705, 8.338789463043213, 12.430410385131836, 16.47349715232849, 20.534561157226562, 24.57644510269165, 28.655865907669067, 32.69224524497986, 36.721925020217896, 40.76685452461243, 44.99931812286377, 49.03613257408142, 53.05749821662903, 57.075292348861694, 61.11047673225403, 65.15578579902649, 69.16938710212708, 73.18999481201172, 77.1818482875824, 81.20033288002014, 85.22706151008606, 89.2233259677887, 93.23303318023682, 97.23385834693909, 101.30126953125, 105.36000275611877, 109.39299321174622, 113.39951276779175, 117.4509584903717, 121.5018150806427, 125.57878756523132, 129.62694931030273, 133.65069246292114, 137.80579662322998, 141.8987820148468, 145.93873262405396, 149.95315527915955, 153.97888231277466, 158.0036838054657, 162.04184341430664, 166.0855643749237, 170.19879341125488, 174.28866696357727, 178.38832187652588, 182.54143953323364, 186.6454997062683, 190.75812411308289, 194.8610782623291, 198.97654008865356, 203.08311557769775, 207.21426057815552, 211.31462740898132, 215.4906554222107, 219.61411023139954, 223.74891638755798, 227.85549354553223, 231.99348211288452, 236.12891697883606, 240.2471306324005, 244.3709752559662, 248.51758861541748, 252.6319341659546, 256.804071187973, 260.93159556388855, 265.06902408599854, 269.1832904815674, 273.36156940460205, 277.52730679512024, 281.64946484565735, 285.8446373939514, 289.9697034358978, 294.1106038093567, 298.24753642082214, 302.35491967201233, 306.4898204803467, 310.60512709617615, 314.7145822048187, 318.84999108314514, 322.9226927757263, 326.9598560333252, 330.97533917427063, 334.981143951416, 339.014732837677, 343.0372805595398, 347.09600353240967, 351.1255798339844, 355.13709902763367, 359.0578844547272, 363.0221903324127, 366.98486828804016, 370.99905729293823, 375.0167906284332, 379.02764415740967, 383.0327425003052, 387.0088760852814, 391.0085802078247, 394.998028755188, 398.9856503009796, 402.94460892677307, 406.91411209106445, 410.2807734012604]
[24.633333333333333, 35.291666666666664, 35.81666666666667, 31.791666666666668, 39.25833333333333, 41.458333333333336, 30.383333333333333, 43.21666666666667, 44.55, 38.95, 46.925, 46.34166666666667, 40.15, 44.0, 47.791666666666664, 51.825, 50.475, 49.166666666666664, 43.68333333333333, 44.825, 55.833333333333336, 55.61666666666667, 51.9, 50.8, 56.35, 53.34166666666667, 51.208333333333336, 55.13333333333333, 50.93333333333333, 54.233333333333334, 61.775, 52.141666666666666, 56.25, 57.725, 56.25, 60.775, 56.25, 55.30833333333333, 58.475, 57.975, 60.49166666666667, 60.166666666666664, 60.208333333333336, 60.93333333333333, 54.5, 57.475, 61.55833333333333, 55.35, 57.74166666666667, 51.34166666666667, 58.86666666666667, 63.05833333333333, 62.00833333333333, 60.675, 64.075, 62.458333333333336, 61.516666666666666, 61.3, 55.6, 60.525, 59.69166666666667, 60.24166666666667, 62.958333333333336, 63.44166666666667, 62.45, 66.15833333333333, 61.28333333333333, 61.891666666666666, 58.2, 59.641666666666666, 63.1, 58.9, 58.74166666666667, 62.68333333333333, 65.125, 61.84166666666667, 66.14166666666667, 63.35, 64.85, 58.583333333333336, 63.333333333333336, 65.3, 64.11666666666666, 63.733333333333334, 61.858333333333334, 64.98333333333333, 64.61666666666666, 64.6, 62.7, 66.73333333333333, 61.733333333333334, 60.125, 61.86666666666667, 66.25833333333334, 64.96666666666667, 62.28333333333333, 63.475, 61.24166666666667, 59.85, 60.225, 67.05]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.302, Test accuracy: 11.84
Round   0, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.87
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.95
Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.05
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.06
Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.07
Round   3, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.06
Round   3, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.07
Round   4, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.17
Round   4, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.18
Round   5, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.23
Round   5, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.21
Round   6, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.18
Round   6, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 12.30
Round   7, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.16
Round   7, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 12.27
Round   8, Train loss: 2.300, Test loss: 2.300, Test accuracy: 12.24
Round   8, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 12.23
Round   9, Train loss: 2.300, Test loss: 2.300, Test accuracy: 12.28
Round   9, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 12.17
Round  10, Train loss: 2.300, Test loss: 2.300, Test accuracy: 12.29
Round  10, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 12.29
Round  11, Train loss: 2.299, Test loss: 2.299, Test accuracy: 12.36
Round  11, Global train loss: 2.299, Global test loss: 2.299, Global test accuracy: 12.56
Round  12, Train loss: 2.299, Test loss: 2.299, Test accuracy: 12.52
Round  12, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 12.65
Round  13, Train loss: 2.299, Test loss: 2.299, Test accuracy: 12.65
Round  13, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 12.79
Round  14, Train loss: 2.298, Test loss: 2.299, Test accuracy: 12.70
Round  14, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 12.95
Round  15, Train loss: 2.298, Test loss: 2.298, Test accuracy: 12.89
Round  15, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 13.21
Round  16, Train loss: 2.298, Test loss: 2.298, Test accuracy: 13.10
Round  16, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 13.51
Round  17, Train loss: 2.297, Test loss: 2.298, Test accuracy: 13.29
Round  17, Global train loss: 2.297, Global test loss: 2.297, Global test accuracy: 13.67
Round  18, Train loss: 2.297, Test loss: 2.297, Test accuracy: 13.48
Round  18, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 13.69
Round  19, Train loss: 2.297, Test loss: 2.297, Test accuracy: 13.76
Round  19, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 14.31
Round  20, Train loss: 2.297, Test loss: 2.297, Test accuracy: 13.82
Round  20, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 14.25
Round  21, Train loss: 2.296, Test loss: 2.296, Test accuracy: 14.35
Round  21, Global train loss: 2.296, Global test loss: 2.295, Global test accuracy: 15.46
Round  22, Train loss: 2.296, Test loss: 2.296, Test accuracy: 14.76
Round  22, Global train loss: 2.296, Global test loss: 2.295, Global test accuracy: 15.96
Round  23, Train loss: 2.295, Test loss: 2.296, Test accuracy: 14.88
Round  23, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 15.37
Round  24, Train loss: 2.295, Test loss: 2.295, Test accuracy: 15.41
Round  24, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 16.51
Round  25, Train loss: 2.295, Test loss: 2.295, Test accuracy: 16.10
Round  25, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 16.79
Round  26, Train loss: 2.294, Test loss: 2.294, Test accuracy: 16.55
Round  26, Global train loss: 2.294, Global test loss: 2.293, Global test accuracy: 16.72
Round  27, Train loss: 2.294, Test loss: 2.293, Test accuracy: 16.55
Round  27, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 16.52
Round  28, Train loss: 2.293, Test loss: 2.293, Test accuracy: 16.32
Round  28, Global train loss: 2.293, Global test loss: 2.292, Global test accuracy: 16.45
Round  29, Train loss: 2.294, Test loss: 2.292, Test accuracy: 16.22
Round  29, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 16.58
Round  30, Train loss: 2.293, Test loss: 2.292, Test accuracy: 16.48
Round  30, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 16.76
Round  31, Train loss: 2.293, Test loss: 2.292, Test accuracy: 16.64
Round  31, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 17.00
Round  32, Train loss: 2.292, Test loss: 2.291, Test accuracy: 16.55
Round  32, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 17.46
Round  33, Train loss: 2.291, Test loss: 2.291, Test accuracy: 16.73
Round  33, Global train loss: 2.291, Global test loss: 2.289, Global test accuracy: 16.86
Round  34, Train loss: 2.291, Test loss: 2.290, Test accuracy: 16.75
Round  34, Global train loss: 2.291, Global test loss: 2.289, Global test accuracy: 16.85
Round  35, Train loss: 2.291, Test loss: 2.289, Test accuracy: 16.72
Round  35, Global train loss: 2.291, Global test loss: 2.288, Global test accuracy: 17.07
Round  36, Train loss: 2.290, Test loss: 2.289, Test accuracy: 16.57
Round  36, Global train loss: 2.290, Global test loss: 2.288, Global test accuracy: 16.93
Round  37, Train loss: 2.290, Test loss: 2.288, Test accuracy: 17.04
Round  37, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 17.59
Round  38, Train loss: 2.290, Test loss: 2.288, Test accuracy: 17.04
Round  38, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 16.90
Round  39, Train loss: 2.289, Test loss: 2.287, Test accuracy: 16.80
Round  39, Global train loss: 2.289, Global test loss: 2.286, Global test accuracy: 16.46
Round  40, Train loss: 2.289, Test loss: 2.287, Test accuracy: 16.80
Round  40, Global train loss: 2.289, Global test loss: 2.286, Global test accuracy: 17.47
Round  41, Train loss: 2.288, Test loss: 2.287, Test accuracy: 17.29
Round  41, Global train loss: 2.288, Global test loss: 2.285, Global test accuracy: 17.26
Round  42, Train loss: 2.288, Test loss: 2.286, Test accuracy: 17.23
Round  42, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 16.97
Round  43, Train loss: 2.286, Test loss: 2.285, Test accuracy: 16.82
Round  43, Global train loss: 2.286, Global test loss: 2.284, Global test accuracy: 16.49
Round  44, Train loss: 2.287, Test loss: 2.285, Test accuracy: 17.10
Round  44, Global train loss: 2.287, Global test loss: 2.283, Global test accuracy: 17.70
Round  45, Train loss: 2.286, Test loss: 2.284, Test accuracy: 17.35
Round  45, Global train loss: 2.286, Global test loss: 2.282, Global test accuracy: 19.00
Round  46, Train loss: 2.285, Test loss: 2.284, Test accuracy: 17.80
Round  46, Global train loss: 2.285, Global test loss: 2.282, Global test accuracy: 18.46
Round  47, Train loss: 2.285, Test loss: 2.283, Test accuracy: 18.11
Round  47, Global train loss: 2.285, Global test loss: 2.281, Global test accuracy: 18.84
Round  48, Train loss: 2.284, Test loss: 2.282, Test accuracy: 18.44
Round  48, Global train loss: 2.284, Global test loss: 2.280, Global test accuracy: 19.13
Round  49, Train loss: 2.284, Test loss: 2.281, Test accuracy: 18.40
Round  49, Global train loss: 2.284, Global test loss: 2.280, Global test accuracy: 18.46
Round  50, Train loss: 2.283, Test loss: 2.281, Test accuracy: 18.14
Round  50, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 18.08
Round  51, Train loss: 2.282, Test loss: 2.280, Test accuracy: 18.20
Round  51, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 19.20
Round  52, Train loss: 2.282, Test loss: 2.279, Test accuracy: 18.12
Round  52, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 18.78
Round  53, Train loss: 2.281, Test loss: 2.278, Test accuracy: 18.44
Round  53, Global train loss: 2.281, Global test loss: 2.277, Global test accuracy: 19.43
Round  54, Train loss: 2.280, Test loss: 2.278, Test accuracy: 18.92
Round  54, Global train loss: 2.280, Global test loss: 2.276, Global test accuracy: 19.60
Round  55, Train loss: 2.281, Test loss: 2.277, Test accuracy: 18.71
Round  55, Global train loss: 2.281, Global test loss: 2.276, Global test accuracy: 19.43
Round  56, Train loss: 2.280, Test loss: 2.277, Test accuracy: 19.19
Round  56, Global train loss: 2.280, Global test loss: 2.275, Global test accuracy: 19.88
Round  57, Train loss: 2.279, Test loss: 2.276, Test accuracy: 19.41
Round  57, Global train loss: 2.279, Global test loss: 2.274, Global test accuracy: 19.79
Round  58, Train loss: 2.279, Test loss: 2.275, Test accuracy: 19.26
Round  58, Global train loss: 2.279, Global test loss: 2.274, Global test accuracy: 19.73
Round  59, Train loss: 2.278, Test loss: 2.275, Test accuracy: 19.48
Round  59, Global train loss: 2.278, Global test loss: 2.273, Global test accuracy: 20.08
Round  60, Train loss: 2.277, Test loss: 2.274, Test accuracy: 19.57
Round  60, Global train loss: 2.277, Global test loss: 2.272, Global test accuracy: 20.55
Round  61, Train loss: 2.276, Test loss: 2.273, Test accuracy: 19.89
Round  61, Global train loss: 2.276, Global test loss: 2.271, Global test accuracy: 21.26
Round  62, Train loss: 2.275, Test loss: 2.272, Test accuracy: 20.32
Round  62, Global train loss: 2.275, Global test loss: 2.269, Global test accuracy: 21.32
Round  63, Train loss: 2.275, Test loss: 2.270, Test accuracy: 20.32
Round  63, Global train loss: 2.275, Global test loss: 2.268, Global test accuracy: 21.11
Round  64, Train loss: 2.274, Test loss: 2.269, Test accuracy: 20.50
Round  64, Global train loss: 2.274, Global test loss: 2.266, Global test accuracy: 21.26
Round  65, Train loss: 2.272, Test loss: 2.268, Test accuracy: 20.56
Round  65, Global train loss: 2.272, Global test loss: 2.265, Global test accuracy: 21.05
Round  66, Train loss: 2.271, Test loss: 2.266, Test accuracy: 20.47
Round  66, Global train loss: 2.271, Global test loss: 2.264, Global test accuracy: 20.06
Round  67, Train loss: 2.270, Test loss: 2.266, Test accuracy: 20.39
Round  67, Global train loss: 2.270, Global test loss: 2.263, Global test accuracy: 20.04
Round  68, Train loss: 2.269, Test loss: 2.265, Test accuracy: 20.52
Round  68, Global train loss: 2.269, Global test loss: 2.263, Global test accuracy: 20.64
Round  69, Train loss: 2.268, Test loss: 2.264, Test accuracy: 20.37
Round  69, Global train loss: 2.268, Global test loss: 2.261, Global test accuracy: 20.05
Round  70, Train loss: 2.266, Test loss: 2.263, Test accuracy: 20.34
Round  70, Global train loss: 2.266, Global test loss: 2.260, Global test accuracy: 20.72
Round  71, Train loss: 2.266, Test loss: 2.261, Test accuracy: 20.14
Round  71, Global train loss: 2.266, Global test loss: 2.258, Global test accuracy: 20.12
Round  72, Train loss: 2.264, Test loss: 2.260, Test accuracy: 20.50
Round  72, Global train loss: 2.264, Global test loss: 2.257, Global test accuracy: 20.91
Round  73, Train loss: 2.264, Test loss: 2.259, Test accuracy: 20.76
Round  73, Global train loss: 2.264, Global test loss: 2.256, Global test accuracy: 21.56
Round  74, Train loss: 2.264, Test loss: 2.257, Test accuracy: 21.03
Round  74, Global train loss: 2.264, Global test loss: 2.254, Global test accuracy: 21.18
Round  75, Train loss: 2.261, Test loss: 2.257, Test accuracy: 20.90
Round  75, Global train loss: 2.261, Global test loss: 2.254, Global test accuracy: 21.23
Round  76, Train loss: 2.262, Test loss: 2.255, Test accuracy: 20.74
Round  76, Global train loss: 2.262, Global test loss: 2.252, Global test accuracy: 20.25
Round  77, Train loss: 2.261, Test loss: 2.254, Test accuracy: 20.64
Round  77, Global train loss: 2.261, Global test loss: 2.251, Global test accuracy: 20.74
Round  78, Train loss: 2.260, Test loss: 2.253, Test accuracy: 20.40
Round  78, Global train loss: 2.260, Global test loss: 2.251, Global test accuracy: 20.47
Round  79, Train loss: 2.260, Test loss: 2.252, Test accuracy: 20.18
Round  79, Global train loss: 2.260, Global test loss: 2.249, Global test accuracy: 19.96
Round  80, Train loss: 2.258, Test loss: 2.251, Test accuracy: 20.10
Round  80, Global train loss: 2.258, Global test loss: 2.248, Global test accuracy: 20.34
Round  81, Train loss: 2.258, Test loss: 2.250, Test accuracy: 20.18
Round  81, Global train loss: 2.258, Global test loss: 2.248, Global test accuracy: 20.89
Round  82, Train loss: 2.257, Test loss: 2.249, Test accuracy: 20.31
Round  82, Global train loss: 2.257, Global test loss: 2.246, Global test accuracy: 20.98
Round  83, Train loss: 2.257, Test loss: 2.248, Test accuracy: 20.29
Round  83, Global train loss: 2.257, Global test loss: 2.244, Global test accuracy: 20.60
Round  84, Train loss: 2.256, Test loss: 2.247, Test accuracy: 20.41
Round  84, Global train loss: 2.256, Global test loss: 2.242, Global test accuracy: 20.09
Round  85, Train loss: 2.253, Test loss: 2.245, Test accuracy: 20.07
Round  85, Global train loss: 2.253, Global test loss: 2.240, Global test accuracy: 19.54
Round  86, Train loss: 2.250, Test loss: 2.243, Test accuracy: 20.03
Round  86, Global train loss: 2.250, Global test loss: 2.237, Global test accuracy: 19.85
Round  87, Train loss: 2.250, Test loss: 2.241, Test accuracy: 19.85
Round  87, Global train loss: 2.250, Global test loss: 2.237, Global test accuracy: 20.31
Round  88, Train loss: 2.249, Test loss: 2.239, Test accuracy: 20.05
Round  88, Global train loss: 2.249, Global test loss: 2.236, Global test accuracy: 20.98
Round  89, Train loss: 2.250, Test loss: 2.237, Test accuracy: 20.23
Round  89, Global train loss: 2.250, Global test loss: 2.233, Global test accuracy: 20.78
Round  90, Train loss: 2.248, Test loss: 2.235, Test accuracy: 20.30
Round  90, Global train loss: 2.248, Global test loss: 2.231, Global test accuracy: 20.89
Round  91, Train loss: 2.248, Test loss: 2.234, Test accuracy: 20.30
Round  91, Global train loss: 2.248, Global test loss: 2.230, Global test accuracy: 20.33
Round  92, Train loss: 2.247, Test loss: 2.232, Test accuracy: 20.24
Round  92, Global train loss: 2.247, Global test loss: 2.228, Global test accuracy: 20.81
Round  93, Train loss: 2.245, Test loss: 2.230, Test accuracy: 20.32
Round  93, Global train loss: 2.245, Global test loss: 2.227, Global test accuracy: 20.70
Round  94, Train loss: 2.246, Test loss: 2.229, Test accuracy: 20.54
Round  94, Global train loss: 2.246, Global test loss: 2.224, Global test accuracy: 21.34
Round  95, Train loss: 2.244, Test loss: 2.226, Test accuracy: 20.98
Round  95, Global train loss: 2.244, Global test loss: 2.222, Global test accuracy: 21.60
Round  96, Train loss: 2.243, Test loss: 2.225, Test accuracy: 21.18
Round  96, Global train loss: 2.243, Global test loss: 2.221, Global test accuracy: 21.77
Round  97, Train loss: 2.242, Test loss: 2.223, Test accuracy: 21.23
Round  97, Global train loss: 2.242, Global test loss: 2.219, Global test accuracy: 21.53
Round  98, Train loss: 2.240, Test loss: 2.222, Test accuracy: 21.21
Round  98, Global train loss: 2.240, Global test loss: 2.218, Global test accuracy: 21.57
Round  99, Train loss: 2.238, Test loss: 2.221, Test accuracy: 21.43
Round  99, Global train loss: 2.238, Global test loss: 2.217, Global test accuracy: 21.75
Round 100, Train loss: 2.239, Test loss: 2.219, Test accuracy: 21.42
Round 100, Global train loss: 2.239, Global test loss: 2.215, Global test accuracy: 21.31
Round 101, Train loss: 2.237, Test loss: 2.217, Test accuracy: 21.27
Round 101, Global train loss: 2.237, Global test loss: 2.214, Global test accuracy: 21.24
Round 102, Train loss: 2.236, Test loss: 2.216, Test accuracy: 21.16
Round 102, Global train loss: 2.236, Global test loss: 2.214, Global test accuracy: 21.33
Round 103, Train loss: 2.235, Test loss: 2.215, Test accuracy: 20.94
Round 103, Global train loss: 2.235, Global test loss: 2.212, Global test accuracy: 20.96
Round 104, Train loss: 2.236, Test loss: 2.213, Test accuracy: 21.06
Round 104, Global train loss: 2.236, Global test loss: 2.209, Global test accuracy: 20.87
Round 105, Train loss: 2.234, Test loss: 2.212, Test accuracy: 21.05
Round 105, Global train loss: 2.234, Global test loss: 2.208, Global test accuracy: 21.05
Round 106, Train loss: 2.230, Test loss: 2.210, Test accuracy: 21.02
Round 106, Global train loss: 2.230, Global test loss: 2.207, Global test accuracy: 21.50
Round 107, Train loss: 2.230, Test loss: 2.208, Test accuracy: 21.09
Round 107, Global train loss: 2.230, Global test loss: 2.203, Global test accuracy: 21.20
Round 108, Train loss: 2.233, Test loss: 2.207, Test accuracy: 21.06
Round 108, Global train loss: 2.233, Global test loss: 2.202, Global test accuracy: 21.15
Round 109, Train loss: 2.232, Test loss: 2.206, Test accuracy: 21.14
Round 109, Global train loss: 2.232, Global test loss: 2.202, Global test accuracy: 21.64
Round 110, Train loss: 2.229, Test loss: 2.205, Test accuracy: 21.20
Round 110, Global train loss: 2.229, Global test loss: 2.203, Global test accuracy: 21.96
Round 111, Train loss: 2.228, Test loss: 2.204, Test accuracy: 21.40
Round 111, Global train loss: 2.228, Global test loss: 2.202, Global test accuracy: 22.29
Round 112, Train loss: 2.226, Test loss: 2.202, Test accuracy: 21.55
Round 112, Global train loss: 2.226, Global test loss: 2.201, Global test accuracy: 22.38
Round 113, Train loss: 2.226, Test loss: 2.202, Test accuracy: 21.84
Round 113, Global train loss: 2.226, Global test loss: 2.200, Global test accuracy: 22.23
Round 114, Train loss: 2.221, Test loss: 2.201, Test accuracy: 21.95
Round 114, Global train loss: 2.221, Global test loss: 2.199, Global test accuracy: 22.26
Round 115, Train loss: 2.225, Test loss: 2.201, Test accuracy: 22.05
Round 115, Global train loss: 2.225, Global test loss: 2.197, Global test accuracy: 22.38
Round 116, Train loss: 2.225, Test loss: 2.199, Test accuracy: 22.10
Round 116, Global train loss: 2.225, Global test loss: 2.194, Global test accuracy: 22.23
Round 117, Train loss: 2.225, Test loss: 2.197, Test accuracy: 21.93
Round 117, Global train loss: 2.225, Global test loss: 2.193, Global test accuracy: 22.07
Round 118, Train loss: 2.224, Test loss: 2.195, Test accuracy: 21.93
Round 118, Global train loss: 2.224, Global test loss: 2.191, Global test accuracy: 21.51
Round 119, Train loss: 2.222, Test loss: 2.193, Test accuracy: 21.73
Round 119, Global train loss: 2.222, Global test loss: 2.190, Global test accuracy: 21.49
Round 120, Train loss: 2.221, Test loss: 2.193, Test accuracy: 21.71
Round 120, Global train loss: 2.221, Global test loss: 2.188, Global test accuracy: 21.47
Round 121, Train loss: 2.219, Test loss: 2.192, Test accuracy: 21.39
Round 121, Global train loss: 2.219, Global test loss: 2.188, Global test accuracy: 21.21
Round 122, Train loss: 2.219, Test loss: 2.191, Test accuracy: 21.42
Round 122, Global train loss: 2.219, Global test loss: 2.186, Global test accuracy: 21.47
Round 123, Train loss: 2.221, Test loss: 2.189, Test accuracy: 21.59
Round 123, Global train loss: 2.221, Global test loss: 2.186, Global test accuracy: 21.73
Round 124, Train loss: 2.215, Test loss: 2.188, Test accuracy: 21.51
Round 124, Global train loss: 2.215, Global test loss: 2.185, Global test accuracy: 21.92
Round 125, Train loss: 2.218, Test loss: 2.186, Test accuracy: 21.55
Round 125, Global train loss: 2.218, Global test loss: 2.182, Global test accuracy: 21.70
Round 126, Train loss: 2.220, Test loss: 2.185, Test accuracy: 21.75
Round 126, Global train loss: 2.220, Global test loss: 2.181, Global test accuracy: 21.97
Round 127, Train loss: 2.212, Test loss: 2.185, Test accuracy: 21.79
Round 127, Global train loss: 2.212, Global test loss: 2.181, Global test accuracy: 22.11
Round 128, Train loss: 2.217, Test loss: 2.183, Test accuracy: 21.85
Round 128, Global train loss: 2.217, Global test loss: 2.179, Global test accuracy: 21.79
Round 129, Train loss: 2.213, Test loss: 2.181, Test accuracy: 21.82
Round 129, Global train loss: 2.213, Global test loss: 2.177, Global test accuracy: 21.56
Round 130, Train loss: 2.212, Test loss: 2.181, Test accuracy: 21.77
Round 130, Global train loss: 2.212, Global test loss: 2.176, Global test accuracy: 21.77
Round 131, Train loss: 2.215, Test loss: 2.179, Test accuracy: 21.89
Round 131, Global train loss: 2.215, Global test loss: 2.173, Global test accuracy: 21.82
Round 132, Train loss: 2.217, Test loss: 2.176, Test accuracy: 21.68
Round 132, Global train loss: 2.217, Global test loss: 2.172, Global test accuracy: 21.62
Round 133, Train loss: 2.215, Test loss: 2.175, Test accuracy: 21.53
Round 133, Global train loss: 2.215, Global test loss: 2.171, Global test accuracy: 21.41
Round 134, Train loss: 2.214, Test loss: 2.173, Test accuracy: 21.48
Round 134, Global train loss: 2.214, Global test loss: 2.168, Global test accuracy: 21.69
Round 135, Train loss: 2.207, Test loss: 2.170, Test accuracy: 21.49
Round 135, Global train loss: 2.207, Global test loss: 2.166, Global test accuracy: 21.56
Round 136, Train loss: 2.216, Test loss: 2.169, Test accuracy: 21.51
Round 136, Global train loss: 2.216, Global test loss: 2.165, Global test accuracy: 21.89
Round 137, Train loss: 2.210, Test loss: 2.166, Test accuracy: 21.50
Round 137, Global train loss: 2.210, Global test loss: 2.161, Global test accuracy: 21.68
Round 138, Train loss: 2.211, Test loss: 2.166, Test accuracy: 21.48
Round 138, Global train loss: 2.211, Global test loss: 2.160, Global test accuracy: 21.71
Round 139, Train loss: 2.209, Test loss: 2.164, Test accuracy: 21.62
Round 139, Global train loss: 2.209, Global test loss: 2.157, Global test accuracy: 21.71
Round 140, Train loss: 2.212, Test loss: 2.162, Test accuracy: 21.81
Round 140, Global train loss: 2.212, Global test loss: 2.157, Global test accuracy: 21.53
Round 141, Train loss: 2.215, Test loss: 2.161, Test accuracy: 21.72
Round 141, Global train loss: 2.215, Global test loss: 2.155, Global test accuracy: 21.35
Round 142, Train loss: 2.219, Test loss: 2.158, Test accuracy: 21.73
Round 142, Global train loss: 2.219, Global test loss: 2.154, Global test accuracy: 21.47
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 21.23
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 17.59
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 14.62
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 13.47
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 11.73
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00