nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.150, Test loss: 2.661, Test accuracy: 57.67
Final Round, Global train loss: 0.150, Global test loss: 1.182, Global test accuracy: 60.95
Average accuracy final 10 rounds: 57.138499999999986 

Average global accuracy final 10 rounds: 60.023250000000004 

6067.594718456268
[4.742488622665405, 9.48497724533081, 14.156854391098022, 18.828731536865234, 23.530823945999146, 28.232916355133057, 33.18820238113403, 38.14348840713501, 43.11849617958069, 48.09350395202637, 53.055196046829224, 58.01688814163208, 63.00375318527222, 67.99061822891235, 72.95457172393799, 77.91852521896362, 82.94308567047119, 87.96764612197876, 92.93814635276794, 97.90864658355713, 102.87864875793457, 107.84865093231201, 112.81712746620178, 117.78560400009155, 122.7602686882019, 127.73493337631226, 132.71235251426697, 137.68977165222168, 142.66095089912415, 147.6321301460266, 152.60137629508972, 157.57062244415283, 162.53549122810364, 167.50036001205444, 172.46792674064636, 177.43549346923828, 182.40482378005981, 187.37415409088135, 192.34690356254578, 197.3196530342102, 202.28999042510986, 207.26032781600952, 212.25064086914062, 217.24095392227173, 222.23947477340698, 227.23799562454224, 232.20551705360413, 237.17303848266602, 242.1219606399536, 247.0708827972412, 252.0395143032074, 257.0081458091736, 262.0011372566223, 266.99412870407104, 272.001261472702, 277.008394241333, 282.00214171409607, 286.99588918685913, 291.9788098335266, 296.9617304801941, 301.9529883861542, 306.94424629211426, 311.9292619228363, 316.91427755355835, 321.9144685268402, 326.91465950012207, 331.9014618396759, 336.88826417922974, 341.87231516838074, 346.85636615753174, 351.8372585773468, 356.81815099716187, 361.81300497055054, 366.8078589439392, 371.79375410079956, 376.7796492576599, 381.7644338607788, 386.7492184638977, 391.74130606651306, 396.7333936691284, 401.7278063297272, 406.7222189903259, 411.7268431186676, 416.7314672470093, 421.7281494140625, 426.7248315811157, 431.72469449043274, 436.72455739974976, 441.72472977638245, 446.72490215301514, 451.71745586395264, 456.71000957489014, 461.6777639389038, 466.6455183029175, 471.62974977493286, 476.61398124694824, 481.5824074745178, 486.5508337020874, 491.4987816810608, 496.4467296600342, 501.3980407714844, 506.34935188293457, 511.2813241481781, 516.2132964134216, 521.1569397449493, 526.100583076477, 531.0647823810577, 536.0289816856384, 541.0289542675018, 546.0289268493652, 551.0162489414215, 556.0035710334778, 560.9910957813263, 565.9786205291748, 570.9609124660492, 575.9432044029236, 580.9355270862579, 585.9278497695923, 590.9083850383759, 595.8889203071594, 600.8645992279053, 605.8402781486511, 610.8277721405029, 615.8152661323547, 620.7724874019623, 625.7297086715698, 630.7161529064178, 635.7025971412659, 640.6796271800995, 645.6566572189331, 650.6379172801971, 655.6191773414612, 660.5931930541992, 665.5672087669373, 670.5730817317963, 675.5789546966553, 680.5822672843933, 685.5855798721313, 690.5994212627411, 695.6132626533508, 700.6703410148621, 705.7274193763733, 710.7651975154877, 715.802975654602, 720.8658363819122, 725.9286971092224, 730.944141626358, 735.9595861434937, 740.9742560386658, 745.9889259338379, 751.0138900279999, 756.0388541221619, 761.0437526702881, 766.0486512184143, 771.0618033409119, 776.0749554634094, 781.095329284668, 786.1157031059265, 791.1232149600983, 796.13072681427, 801.1352398395538, 806.1397528648376, 811.1505856513977, 816.1614184379578, 821.1848964691162, 826.2083745002747, 831.2215855121613, 836.2347965240479, 841.2500998973846, 846.2654032707214, 851.2960114479065, 856.3266196250916, 861.3270697593689, 866.3275198936462, 871.3490364551544, 876.3705530166626, 881.3824076652527, 886.3942623138428, 891.420825958252, 896.4473896026611, 901.4574041366577, 906.4674186706543, 911.493093252182, 916.5187678337097, 921.5996322631836, 926.6804966926575, 931.6914637088776, 936.7024307250977, 941.495246887207, 946.2880630493164, 951.0847399234772, 955.8814167976379, 960.640453338623, 965.3994898796082, 970.2669620513916, 975.134434223175, 980.0063991546631, 984.8783640861511, 989.9108626842499, 994.9433612823486, 997.4769344329834, 1000.0105075836182]
[38.23, 38.23, 43.2225, 43.2225, 44.98, 44.98, 46.475, 46.475, 47.4625, 47.4625, 48.1225, 48.1225, 49.5825, 49.5825, 50.32, 50.32, 50.28, 50.28, 50.795, 50.795, 51.7875, 51.7875, 52.545, 52.545, 53.305, 53.305, 53.3875, 53.3875, 53.86, 53.86, 54.2825, 54.2825, 54.4875, 54.4875, 54.78, 54.78, 55.05, 55.05, 55.245, 55.245, 55.525, 55.525, 55.6425, 55.6425, 55.815, 55.815, 56.0925, 56.0925, 56.3025, 56.3025, 56.3175, 56.3175, 56.2575, 56.2575, 56.21, 56.21, 56.4425, 56.4425, 56.3425, 56.3425, 56.7, 56.7, 56.7725, 56.7725, 56.035, 56.035, 56.02, 56.02, 55.8475, 55.8475, 55.8125, 55.8125, 55.6175, 55.6175, 55.4825, 55.4825, 55.945, 55.945, 56.4275, 56.4275, 56.4825, 56.4825, 56.4775, 56.4775, 56.505, 56.505, 56.665, 56.665, 56.63, 56.63, 56.515, 56.515, 56.4775, 56.4775, 56.43, 56.43, 56.8075, 56.8075, 56.8975, 56.8975, 56.85, 56.85, 56.6625, 56.6625, 56.8725, 56.8725, 56.74, 56.74, 56.5575, 56.5575, 56.53, 56.53, 56.75, 56.75, 56.8825, 56.8825, 57.0175, 57.0175, 56.7125, 56.7125, 56.245, 56.245, 56.25, 56.25, 56.215, 56.215, 56.525, 56.525, 56.1275, 56.1275, 56.1975, 56.1975, 56.5175, 56.5175, 56.5325, 56.5325, 56.79, 56.79, 56.7775, 56.7775, 56.9475, 56.9475, 56.845, 56.845, 56.86, 56.86, 56.845, 56.845, 56.785, 56.785, 56.73, 56.73, 56.7575, 56.7575, 56.6875, 56.6875, 56.6425, 56.6425, 56.94, 56.94, 56.7075, 56.7075, 56.82, 56.82, 57.13, 57.13, 57.1525, 57.1525, 57.11, 57.11, 57.125, 57.125, 57.01, 57.01, 57.21, 57.21, 57.095, 57.095, 57.1075, 57.1075, 57.005, 57.005, 57.2925, 57.2925, 57.3025, 57.3025, 57.27, 57.27, 57.1475, 57.1475, 56.9125, 56.9125, 56.7375, 56.7375, 56.9975, 56.9975, 57.36, 57.36, 57.36, 57.36, 57.6675, 57.6675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.180, Test loss: 0.595, Test accuracy: 83.07
Final Round, Global train loss: 0.180, Global test loss: 1.563, Global test accuracy: 56.68
Average accuracy final 10 rounds: 82.16499999999999 

Average global accuracy final 10 rounds: 57.739999999999995 

899.0198812484741
[0.9226284027099609, 1.8452568054199219, 2.5264298915863037, 3.2076029777526855, 3.884532928466797, 4.561462879180908, 5.276228189468384, 5.990993499755859, 6.710627317428589, 7.430261135101318, 8.148753881454468, 8.867246627807617, 9.585114240646362, 10.302981853485107, 11.021570682525635, 11.740159511566162, 12.462019205093384, 13.183878898620605, 13.910226583480835, 14.636574268341064, 15.35160207748413, 16.066629886627197, 16.78230595588684, 17.497982025146484, 18.21539068222046, 18.932799339294434, 19.651270627975464, 20.369741916656494, 21.087677240371704, 21.805612564086914, 22.52765965461731, 23.249706745147705, 23.963576316833496, 24.677445888519287, 25.393879890441895, 26.110313892364502, 26.83025050163269, 27.55018711090088, 28.265772819519043, 28.981358528137207, 29.696735382080078, 30.41211223602295, 31.127750396728516, 31.843388557434082, 32.56274199485779, 33.282095432281494, 33.99316954612732, 34.704243659973145, 35.419711112976074, 36.135178565979004, 36.85340356826782, 37.57162857055664, 38.294235706329346, 39.01684284210205, 39.73447799682617, 40.45211315155029, 41.17451858520508, 41.89692401885986, 42.60977578163147, 43.322627544403076, 44.04047679901123, 44.758326053619385, 45.47840476036072, 46.19848346710205, 46.92324995994568, 47.64801645278931, 48.36507534980774, 49.08213424682617, 49.79875111579895, 50.51536798477173, 51.234041690826416, 51.9527153968811, 52.675750970840454, 53.398786544799805, 54.115275859832764, 54.83176517486572, 55.552809953689575, 56.27385473251343, 56.99305462837219, 57.71225452423096, 58.427273988723755, 59.14229345321655, 59.856001138687134, 60.569708824157715, 61.286991357803345, 62.004273891448975, 62.72489547729492, 63.44551706314087, 64.16266441345215, 64.87981176376343, 65.58807873725891, 66.2963457107544, 67.00255966186523, 67.70877361297607, 68.41463470458984, 69.12049579620361, 69.83387541770935, 70.54725503921509, 71.26173233985901, 71.97620964050293, 72.6941602230072, 73.41211080551147, 74.12684607505798, 74.84158134460449, 75.55791854858398, 76.27425575256348, 76.9872817993164, 77.70030784606934, 78.31814813613892, 78.9359884262085, 79.56072664260864, 80.18546485900879, 80.79854989051819, 81.41163492202759, 82.03572154045105, 82.65980815887451, 83.28528499603271, 83.91076183319092, 84.5240547657013, 85.13734769821167, 85.74960541725159, 86.3618631362915, 86.97329711914062, 87.58473110198975, 88.20769429206848, 88.83065748214722, 89.44559907913208, 90.06054067611694, 90.67989206314087, 91.2992434501648, 91.91626214981079, 92.53328084945679, 93.15097856521606, 93.76867628097534, 94.3836886882782, 94.99870109558105, 95.61585712432861, 96.23301315307617, 96.85536599159241, 97.47771883010864, 98.09476852416992, 98.7118182182312, 99.33052015304565, 99.94922208786011, 100.56776833534241, 101.1863145828247, 101.804203748703, 102.4220929145813, 103.03778171539307, 103.65347051620483, 104.27073836326599, 104.88800621032715, 105.5055205821991, 106.12303495407104, 106.73828506469727, 107.35353517532349, 107.97633266448975, 108.599130153656, 109.21719026565552, 109.83525037765503, 110.45270299911499, 111.07015562057495, 111.68667984008789, 112.30320405960083, 112.92113518714905, 113.53906631469727, 114.15240502357483, 114.76574373245239, 115.38332104682922, 116.00089836120605, 116.61565041542053, 117.23040246963501, 117.8458092212677, 118.46121597290039, 119.0804169178009, 119.69961786270142, 120.3181140422821, 120.9366102218628, 121.55324459075928, 122.16987895965576, 122.78827428817749, 123.40666961669922, 124.02244114875793, 124.63821268081665, 125.25507593154907, 125.8719391822815, 126.49033331871033, 127.10872745513916, 127.72310853004456, 128.33748960494995, 128.9519453048706, 129.56640100479126, 130.18528270721436, 130.80416440963745, 131.4262354373932, 132.04830646514893, 132.6681351661682, 133.2879638671875, 133.90709686279297, 134.52622985839844, 135.7627980709076, 136.99936628341675]
[26.716666666666665, 26.716666666666665, 41.61666666666667, 41.61666666666667, 43.86666666666667, 43.86666666666667, 53.1, 53.1, 57.65, 57.65, 60.5, 60.5, 64.63333333333334, 64.63333333333334, 65.0, 65.0, 69.06666666666666, 69.06666666666666, 72.55, 72.55, 72.51666666666667, 72.51666666666667, 72.71666666666667, 72.71666666666667, 73.46666666666667, 73.46666666666667, 73.65, 73.65, 73.03333333333333, 73.03333333333333, 73.81666666666666, 73.81666666666666, 74.68333333333334, 74.68333333333334, 74.78333333333333, 74.78333333333333, 75.01666666666667, 75.01666666666667, 75.85, 75.85, 76.36666666666666, 76.36666666666666, 76.1, 76.1, 76.88333333333334, 76.88333333333334, 76.85, 76.85, 77.6, 77.6, 77.71666666666667, 77.71666666666667, 77.63333333333334, 77.63333333333334, 78.03333333333333, 78.03333333333333, 77.23333333333333, 77.23333333333333, 77.35, 77.35, 77.53333333333333, 77.53333333333333, 79.25, 79.25, 79.5, 79.5, 79.76666666666667, 79.76666666666667, 79.18333333333334, 79.18333333333334, 79.65, 79.65, 79.73333333333333, 79.73333333333333, 80.05, 80.05, 80.1, 80.1, 80.2, 80.2, 79.85, 79.85, 79.61666666666666, 79.61666666666666, 79.56666666666666, 79.56666666666666, 79.95, 79.95, 80.33333333333333, 80.33333333333333, 80.41666666666667, 80.41666666666667, 80.4, 80.4, 79.16666666666667, 79.16666666666667, 81.05, 81.05, 81.05, 81.05, 80.7, 80.7, 80.85, 80.85, 81.8, 81.8, 81.31666666666666, 81.31666666666666, 80.95, 80.95, 81.28333333333333, 81.28333333333333, 81.35, 81.35, 81.46666666666667, 81.46666666666667, 81.56666666666666, 81.56666666666666, 80.63333333333334, 80.63333333333334, 81.0, 81.0, 80.28333333333333, 80.28333333333333, 80.46666666666667, 80.46666666666667, 80.61666666666666, 80.61666666666666, 80.85, 80.85, 81.26666666666667, 81.26666666666667, 80.83333333333333, 80.83333333333333, 80.96666666666667, 80.96666666666667, 80.98333333333333, 80.98333333333333, 81.48333333333333, 81.48333333333333, 81.4, 81.4, 81.11666666666666, 81.11666666666666, 81.51666666666667, 81.51666666666667, 81.75, 81.75, 81.68333333333334, 81.68333333333334, 81.7, 81.7, 81.18333333333334, 81.18333333333334, 82.05, 82.05, 81.88333333333334, 81.88333333333334, 81.8, 81.8, 82.1, 82.1, 81.28333333333333, 81.28333333333333, 81.3, 81.3, 81.7, 81.7, 82.68333333333334, 82.68333333333334, 82.63333333333334, 82.63333333333334, 82.65, 82.65, 82.26666666666667, 82.26666666666667, 81.36666666666666, 81.36666666666666, 81.98333333333333, 81.98333333333333, 82.13333333333334, 82.13333333333334, 82.4, 82.4, 82.68333333333334, 82.68333333333334, 81.56666666666666, 81.56666666666666, 82.0, 82.0, 82.46666666666667, 82.46666666666667, 82.51666666666667, 82.51666666666667, 82.48333333333333, 82.48333333333333, 81.8, 81.8, 81.6, 81.6, 83.06666666666666, 83.06666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.247, Test loss: 0.433, Test accuracy: 82.85
Average accuracy final 10 rounds: 82.52833333333334 

734.816445350647
[0.9066381454467773, 1.8132762908935547, 2.4688310623168945, 3.1243858337402344, 3.7769086360931396, 4.429431438446045, 5.089171648025513, 5.7489118576049805, 6.40894627571106, 7.068980693817139, 7.725963354110718, 8.382946014404297, 9.036772012710571, 9.690598011016846, 10.346153259277344, 11.001708507537842, 11.653510570526123, 12.305312633514404, 12.962798595428467, 13.62028455734253, 14.281194686889648, 14.942104816436768, 15.605681419372559, 16.26925802230835, 16.93216848373413, 17.595078945159912, 18.259077787399292, 18.923076629638672, 19.58505916595459, 20.247041702270508, 20.904507637023926, 21.561973571777344, 22.21959662437439, 22.877219676971436, 23.531081199645996, 24.184942722320557, 24.835586071014404, 25.486229419708252, 26.13531517982483, 26.784400939941406, 27.440388679504395, 28.096376419067383, 28.7541561126709, 29.411935806274414, 30.06708860397339, 30.722241401672363, 31.37909984588623, 32.0359582901001, 32.697224378585815, 33.35849046707153, 34.01450228691101, 34.67051410675049, 35.326345443725586, 35.982176780700684, 36.63953948020935, 37.29690217971802, 37.95320677757263, 38.609511375427246, 39.266693353652954, 39.92387533187866, 40.58037495613098, 41.2368745803833, 41.89144515991211, 42.54601573944092, 43.199303150177, 43.852590560913086, 44.502418756484985, 45.152246952056885, 45.804445028305054, 46.45664310455322, 47.10715889930725, 47.75767469406128, 48.41017746925354, 49.0626802444458, 49.71540832519531, 50.368136405944824, 51.01779222488403, 51.66744804382324, 52.32162261009216, 52.975797176361084, 53.632189989089966, 54.28858280181885, 54.94442415237427, 55.60026550292969, 56.253467082977295, 56.9066686630249, 57.560285329818726, 58.21390199661255, 58.86849308013916, 59.52308416366577, 60.1774582862854, 60.83183240890503, 61.48340582847595, 62.134979248046875, 62.78460502624512, 63.43423080444336, 64.07987856864929, 64.72552633285522, 65.37285828590393, 66.02019023895264, 66.67294406890869, 67.32569789886475, 67.97594857215881, 68.62619924545288, 69.27414798736572, 69.92209672927856, 70.57216954231262, 71.22224235534668, 71.87428259849548, 72.52632284164429, 73.18380093574524, 73.84127902984619, 74.4935245513916, 75.14577007293701, 75.79782009124756, 76.4498701095581, 77.10213899612427, 77.75440788269043, 78.40907144546509, 79.06373500823975, 79.71899938583374, 80.37426376342773, 81.02411675453186, 81.67396974563599, 82.3276059627533, 82.9812421798706, 83.6359510421753, 84.29065990447998, 84.94821095466614, 85.6057620048523, 86.26306962966919, 86.92037725448608, 87.57110095024109, 88.2218246459961, 88.87197542190552, 89.52212619781494, 90.17320489883423, 90.82428359985352, 91.47955656051636, 92.1348295211792, 92.78551006317139, 93.43619060516357, 94.08477091789246, 94.73335123062134, 95.38270831108093, 96.03206539154053, 96.68636202812195, 97.34065866470337, 97.99669885635376, 98.65273904800415, 99.30497074127197, 99.9572024345398, 100.60963702201843, 101.26207160949707, 101.91914772987366, 102.57622385025024, 103.22604465484619, 103.87586545944214, 104.5311176776886, 105.18636989593506, 105.83996224403381, 106.49355459213257, 107.14479923248291, 107.79604387283325, 108.44669246673584, 109.09734106063843, 109.75456881523132, 110.41179656982422, 111.06971454620361, 111.72763252258301, 112.37801671028137, 113.02840089797974, 113.6785078048706, 114.32861471176147, 114.98565649986267, 115.64269828796387, 116.29853010177612, 116.95436191558838, 117.60653042793274, 118.2586989402771, 118.91086769104004, 119.56303644180298, 120.21685194969177, 120.87066745758057, 121.52229285240173, 122.1739182472229, 122.8247754573822, 123.4756326675415, 124.12478065490723, 124.77392864227295, 125.42569017410278, 126.07745170593262, 126.73284149169922, 127.38823127746582, 128.04182505607605, 128.69541883468628, 129.35824823379517, 130.02107763290405, 130.68354082107544, 131.34600400924683, 132.53681540489197, 133.7276268005371]
[26.433333333333334, 26.433333333333334, 31.066666666666666, 31.066666666666666, 36.266666666666666, 36.266666666666666, 45.31666666666667, 45.31666666666667, 46.266666666666666, 46.266666666666666, 52.71666666666667, 52.71666666666667, 62.38333333333333, 62.38333333333333, 62.6, 62.6, 65.25, 65.25, 64.7, 64.7, 65.25, 65.25, 68.01666666666667, 68.01666666666667, 69.01666666666667, 69.01666666666667, 69.85, 69.85, 70.51666666666667, 70.51666666666667, 71.1, 71.1, 71.2, 71.2, 71.8, 71.8, 72.28333333333333, 72.28333333333333, 72.63333333333334, 72.63333333333334, 72.25, 72.25, 72.55, 72.55, 73.86666666666666, 73.86666666666666, 74.88333333333334, 74.88333333333334, 74.73333333333333, 74.73333333333333, 74.41666666666667, 74.41666666666667, 76.5, 76.5, 76.73333333333333, 76.73333333333333, 76.75, 76.75, 76.05, 76.05, 76.66666666666667, 76.66666666666667, 76.45, 76.45, 76.95, 76.95, 77.86666666666666, 77.86666666666666, 77.85, 77.85, 77.58333333333333, 77.58333333333333, 77.56666666666666, 77.56666666666666, 78.2, 78.2, 77.46666666666667, 77.46666666666667, 77.68333333333334, 77.68333333333334, 78.03333333333333, 78.03333333333333, 78.33333333333333, 78.33333333333333, 78.43333333333334, 78.43333333333334, 79.11666666666666, 79.11666666666666, 79.08333333333333, 79.08333333333333, 79.3, 79.3, 79.25, 79.25, 79.65, 79.65, 79.55, 79.55, 79.31666666666666, 79.31666666666666, 79.56666666666666, 79.56666666666666, 80.33333333333333, 80.33333333333333, 80.1, 80.1, 80.31666666666666, 80.31666666666666, 80.03333333333333, 80.03333333333333, 81.03333333333333, 81.03333333333333, 80.5, 80.5, 80.66666666666667, 80.66666666666667, 80.61666666666666, 80.61666666666666, 81.28333333333333, 81.28333333333333, 80.65, 80.65, 80.7, 80.7, 80.71666666666667, 80.71666666666667, 80.98333333333333, 80.98333333333333, 81.53333333333333, 81.53333333333333, 81.96666666666667, 81.96666666666667, 81.8, 81.8, 81.61666666666666, 81.61666666666666, 81.63333333333334, 81.63333333333334, 81.86666666666666, 81.86666666666666, 81.36666666666666, 81.36666666666666, 82.2, 82.2, 81.68333333333334, 81.68333333333334, 82.28333333333333, 82.28333333333333, 82.28333333333333, 82.28333333333333, 82.36666666666666, 82.36666666666666, 81.85, 81.85, 81.35, 81.35, 81.31666666666666, 81.31666666666666, 81.55, 81.55, 81.56666666666666, 81.56666666666666, 82.23333333333333, 82.23333333333333, 82.55, 82.55, 82.33333333333333, 82.33333333333333, 82.16666666666667, 82.16666666666667, 82.11666666666666, 82.11666666666666, 82.73333333333333, 82.73333333333333, 82.41666666666667, 82.41666666666667, 82.46666666666667, 82.46666666666667, 82.33333333333333, 82.33333333333333, 81.58333333333333, 81.58333333333333, 82.66666666666667, 82.66666666666667, 82.7, 82.7, 82.4, 82.4, 82.8, 82.8, 82.53333333333333, 82.53333333333333, 82.6, 82.6, 82.43333333333334, 82.43333333333334, 82.8, 82.8, 82.76666666666667, 82.76666666666667, 82.85, 82.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.329, Test loss: 0.892, Test accuracy: 75.49
Average accuracy final 10 rounds: 75.7635 

4978.052330255508
[5.016364097595215, 10.03272819519043, 15.021852731704712, 20.010977268218994, 25.034399032592773, 30.057820796966553, 34.90882849693298, 39.759836196899414, 44.59508180618286, 49.43032741546631, 54.416077852249146, 59.40182828903198, 64.54967498779297, 69.69752168655396, 74.75168418884277, 79.80584669113159, 84.74154949188232, 89.67725229263306, 94.58444285392761, 99.49163341522217, 104.36101365089417, 109.23039388656616, 114.17147731781006, 119.11256074905396, 123.97526264190674, 128.83796453475952, 133.88061094284058, 138.92325735092163, 143.88637113571167, 148.8494849205017, 153.84309649467468, 158.83670806884766, 163.7175235748291, 168.59833908081055, 173.48394918441772, 178.3695592880249, 183.2554383277893, 188.1413173675537, 193.0747368335724, 198.00815629959106, 202.8911736011505, 207.77419090270996, 212.61869049072266, 217.46319007873535, 221.73857259750366, 226.01395511627197, 230.2473168373108, 234.4806785583496, 238.73281693458557, 242.98495531082153, 247.24767136573792, 251.5103874206543, 256.44268798828125, 261.3749885559082, 266.23819041252136, 271.1013922691345, 275.95155334472656, 280.8017144203186, 285.67646312713623, 290.55121183395386, 295.4610822200775, 300.3709526062012, 305.2804594039917, 310.1899662017822, 314.47093892097473, 318.75191164016724, 323.035197019577, 327.3184823989868, 331.5806770324707, 335.8428716659546, 340.0957589149475, 344.34864616394043, 348.65726041793823, 352.96587467193604, 357.29550075531006, 361.6251268386841, 365.96872425079346, 370.31232166290283, 374.58078122138977, 378.8492407798767, 383.135858297348, 387.42247581481934, 391.6760139465332, 395.92955207824707, 400.1881294250488, 404.4467067718506, 408.6939480304718, 412.941189289093, 417.19660782814026, 421.4520263671875, 425.65939116477966, 429.8667559623718, 434.1024887561798, 438.3382215499878, 442.62885069847107, 446.91947984695435, 451.15947127342224, 455.39946269989014, 459.6005084514618, 463.80155420303345, 468.06679582595825, 472.33203744888306, 477.2542231082916, 482.1764087677002, 487.11854219436646, 492.0606756210327, 496.9359793663025, 501.81128311157227, 507.2284183502197, 512.6455535888672, 518.3100275993347, 523.9745016098022, 528.6596302986145, 533.3447589874268, 538.2057814598083, 543.0668039321899, 547.8272640705109, 552.5877242088318, 557.4575109481812, 562.3272976875305, 566.8908259868622, 571.4543542861938, 576.039089679718, 580.6238250732422, 585.3540341854095, 590.0842432975769, 594.8915441036224, 599.698844909668, 604.4750990867615, 609.251353263855, 614.2089111804962, 619.1664690971375, 623.9562058448792, 628.7459425926208, 633.5803246498108, 638.4147067070007, 643.2011141777039, 647.987521648407, 652.8501620292664, 657.7128024101257, 662.1263394355774, 666.539876461029, 671.3507761955261, 676.1616759300232, 680.465677022934, 684.7696781158447, 689.1209251880646, 693.4721722602844, 697.774495601654, 702.0768189430237, 706.4717905521393, 710.8667621612549, 715.1990637779236, 719.5313653945923, 723.8259115219116, 728.120457649231, 732.3946084976196, 736.6687593460083, 741.0356528759003, 745.4025464057922, 749.7058577537537, 754.0091691017151, 758.3387532234192, 762.6683373451233, 766.9340143203735, 771.1996912956238, 775.551634311676, 779.9035773277283, 784.2263782024384, 788.5491790771484, 792.8751554489136, 797.2011318206787, 801.458464384079, 805.7157969474792, 809.9635057449341, 814.2112145423889, 818.5627727508545, 822.9143309593201, 827.2803149223328, 831.6462988853455, 835.9975986480713, 840.3488984107971, 844.6264789104462, 848.9040594100952, 853.1377351284027, 857.3714108467102, 861.7002296447754, 866.0290484428406, 870.4176051616669, 874.8061618804932, 879.1613688468933, 883.5165758132935, 887.7727463245392, 892.0289168357849, 896.6960990428925, 901.36328125, 906.1062226295471, 910.8491640090942, 915.5700781345367, 920.2909922599792, 922.2660989761353, 924.2412056922913]
[38.25, 38.25, 45.8775, 45.8775, 50.5125, 50.5125, 54.01, 54.01, 57.635, 57.635, 60.48, 60.48, 62.2475, 62.2475, 63.6175, 63.6175, 64.925, 64.925, 65.9675, 65.9675, 67.1775, 67.1775, 67.8375, 67.8375, 67.8025, 67.8025, 68.82, 68.82, 69.085, 69.085, 69.24, 69.24, 70.13, 70.13, 70.905, 70.905, 71.2625, 71.2625, 71.8375, 71.8375, 72.03, 72.03, 72.615, 72.615, 72.46, 72.46, 73.0575, 73.0575, 73.1875, 73.1875, 72.8225, 72.8225, 73.5175, 73.5175, 73.25, 73.25, 73.7775, 73.7775, 73.8825, 73.8825, 74.0725, 74.0725, 74.3725, 74.3725, 74.315, 74.315, 74.445, 74.445, 74.0475, 74.0475, 74.685, 74.685, 74.5475, 74.5475, 74.655, 74.655, 74.6025, 74.6025, 74.65, 74.65, 74.8725, 74.8725, 75.2475, 75.2475, 75.1125, 75.1125, 75.195, 75.195, 75.0925, 75.0925, 75.23, 75.23, 75.1675, 75.1675, 74.6525, 74.6525, 74.7975, 74.7975, 75.5775, 75.5775, 75.615, 75.615, 75.5325, 75.5325, 75.1425, 75.1425, 75.1525, 75.1525, 75.29, 75.29, 75.2525, 75.2525, 75.3575, 75.3575, 75.5475, 75.5475, 75.295, 75.295, 75.4925, 75.4925, 75.18, 75.18, 76.0675, 76.0675, 75.725, 75.725, 75.5275, 75.5275, 75.6675, 75.6675, 75.4475, 75.4475, 75.6475, 75.6475, 75.7425, 75.7425, 75.16, 75.16, 75.3575, 75.3575, 75.57, 75.57, 75.74, 75.74, 75.6125, 75.6125, 75.76, 75.76, 75.67, 75.67, 75.6575, 75.6575, 75.6025, 75.6025, 75.9225, 75.9225, 75.6925, 75.6925, 75.9325, 75.9325, 75.7875, 75.7875, 75.61, 75.61, 75.6, 75.6, 75.75, 75.75, 75.4775, 75.4775, 75.8725, 75.8725, 75.8475, 75.8475, 75.83, 75.83, 76.0825, 76.0825, 76.15, 76.15, 75.935, 75.935, 75.6, 75.6, 75.6325, 75.6325, 75.64, 75.64, 75.805, 75.805, 75.6275, 75.6275, 75.9075, 75.9075, 75.9175, 75.9175, 75.85, 75.85, 75.72, 75.72, 75.4875, 75.4875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.039, Test loss: 1.018, Test accuracy: 81.18
Average accuracy final 10 rounds: 80.81083333333332 

1525.32390832901
[1.6208698749542236, 3.2417397499084473, 4.632605075836182, 6.023470401763916, 7.437958478927612, 8.852446556091309, 10.263679504394531, 11.674912452697754, 13.057212829589844, 14.439513206481934, 15.820663213729858, 17.201813220977783, 18.627142667770386, 20.05247211456299, 21.680169343948364, 23.30786657333374, 25.020594120025635, 26.73332166671753, 28.57537007331848, 30.417418479919434, 32.126182317733765, 33.834946155548096, 35.61389994621277, 37.39285373687744, 39.12099885940552, 40.849143981933594, 42.47017979621887, 44.09121561050415, 45.49595856666565, 46.90070152282715, 48.22790336608887, 49.555105209350586, 50.89416265487671, 52.23322010040283, 53.59433317184448, 54.95544624328613, 56.27628827095032, 57.5971302986145, 58.91270303726196, 60.228275775909424, 61.538084268569946, 62.84789276123047, 64.17459273338318, 65.50129270553589, 66.84126138687134, 68.18123006820679, 69.49655318260193, 70.81187629699707, 72.12195348739624, 73.43203067779541, 74.75620698928833, 76.08038330078125, 77.44745135307312, 78.81451940536499, 80.1260416507721, 81.4375638961792, 82.7322256565094, 84.0268874168396, 85.34268736839294, 86.65848731994629, 88.00957608222961, 89.36066484451294, 90.70291900634766, 92.04517316818237, 93.48508954048157, 94.92500591278076, 96.37869691848755, 97.83238792419434, 99.31472063064575, 100.79705333709717, 102.2723708152771, 103.74768829345703, 105.22857928276062, 106.70947027206421, 108.20044040679932, 109.69141054153442, 111.17132759094238, 112.65124464035034, 114.00807642936707, 115.36490821838379, 116.83996319770813, 118.31501817703247, 119.71087574958801, 121.10673332214355, 122.40632963180542, 123.70592594146729, 125.0405957698822, 126.37526559829712, 127.67134428024292, 128.96742296218872, 130.2758502960205, 131.5842776298523, 132.90530061721802, 134.22632360458374, 135.5730893611908, 136.91985511779785, 138.20404243469238, 139.4882297515869, 140.78959250450134, 142.09095525741577, 143.38153886795044, 144.6721224784851, 145.98043298721313, 147.28874349594116, 148.58746194839478, 149.8861804008484, 151.2061631679535, 152.5261459350586, 153.8451235294342, 155.16410112380981, 156.4401822090149, 157.71626329421997, 159.01641654968262, 160.31656980514526, 161.62801051139832, 162.93945121765137, 164.23700499534607, 165.53455877304077, 166.8500461578369, 168.16553354263306, 169.49557447433472, 170.82561540603638, 172.13578009605408, 173.44594478607178, 174.74215602874756, 176.03836727142334, 177.37210726737976, 178.70584726333618, 180.03239059448242, 181.35893392562866, 182.63878870010376, 183.91864347457886, 185.20522737503052, 186.49181127548218, 187.8013458251953, 189.11088037490845, 190.40705180168152, 191.7032232284546, 192.98386406898499, 194.26450490951538, 195.56598472595215, 196.86746454238892, 198.1790406703949, 199.49061679840088, 200.78637671470642, 202.08213663101196, 203.35110759735107, 204.62007856369019, 205.92165899276733, 207.22323942184448, 208.518319606781, 209.81339979171753, 211.10063362121582, 212.3878674507141, 213.67630338668823, 214.96473932266235, 216.25551962852478, 217.5462999343872, 218.8150336742401, 220.08376741409302, 221.3598186969757, 222.6358699798584, 223.92673516273499, 225.21760034561157, 226.5041651725769, 227.79072999954224, 229.06721997261047, 230.3437099456787, 231.6251790523529, 232.9066481590271, 234.20207858085632, 235.49750900268555, 236.7833390235901, 238.06916904449463, 239.34013175964355, 240.61109447479248, 241.93008160591125, 243.24906873703003, 244.53820061683655, 245.82733249664307, 247.11425065994263, 248.4011688232422, 249.70358777046204, 251.00600671768188, 252.30924677848816, 253.61248683929443, 254.90816736221313, 256.20384788513184, 257.5241551399231, 258.84446239471436, 260.1410028934479, 261.4375433921814, 262.7369203567505, 264.0362973213196, 265.3390166759491, 266.6417360305786, 267.9190969467163, 269.196457862854, 270.4734652042389, 271.7504725456238, 273.86665654182434, 275.9828405380249]
[25.758333333333333, 25.758333333333333, 40.63333333333333, 40.63333333333333, 47.075, 47.075, 56.7, 56.7, 59.175, 59.175, 60.416666666666664, 60.416666666666664, 63.875, 63.875, 67.94166666666666, 67.94166666666666, 67.94166666666666, 67.94166666666666, 70.45, 70.45, 71.66666666666667, 71.66666666666667, 70.95, 70.95, 72.44166666666666, 72.44166666666666, 73.55, 73.55, 73.83333333333333, 73.83333333333333, 74.25, 74.25, 75.25833333333334, 75.25833333333334, 75.74166666666666, 75.74166666666666, 76.275, 76.275, 75.61666666666666, 75.61666666666666, 76.83333333333333, 76.83333333333333, 76.75833333333334, 76.75833333333334, 76.48333333333333, 76.48333333333333, 76.14166666666667, 76.14166666666667, 76.98333333333333, 76.98333333333333, 77.09166666666667, 77.09166666666667, 78.18333333333334, 78.18333333333334, 78.51666666666667, 78.51666666666667, 78.575, 78.575, 78.3, 78.3, 78.64166666666667, 78.64166666666667, 78.76666666666667, 78.76666666666667, 79.18333333333334, 79.18333333333334, 79.175, 79.175, 79.23333333333333, 79.23333333333333, 79.35833333333333, 79.35833333333333, 78.825, 78.825, 78.55833333333334, 78.55833333333334, 78.59166666666667, 78.59166666666667, 78.65833333333333, 78.65833333333333, 79.00833333333334, 79.00833333333334, 79.5, 79.5, 79.74166666666666, 79.74166666666666, 80.05, 80.05, 80.3, 80.3, 79.65833333333333, 79.65833333333333, 79.75833333333334, 79.75833333333334, 80.05833333333334, 80.05833333333334, 80.225, 80.225, 80.175, 80.175, 80.09166666666667, 80.09166666666667, 80.075, 80.075, 80.06666666666666, 80.06666666666666, 79.65833333333333, 79.65833333333333, 79.88333333333334, 79.88333333333334, 79.45, 79.45, 79.575, 79.575, 80.05, 80.05, 80.29166666666667, 80.29166666666667, 80.06666666666666, 80.06666666666666, 80.35833333333333, 80.35833333333333, 80.24166666666666, 80.24166666666666, 80.55, 80.55, 80.25833333333334, 80.25833333333334, 80.48333333333333, 80.48333333333333, 80.56666666666666, 80.56666666666666, 80.625, 80.625, 80.525, 80.525, 80.41666666666667, 80.41666666666667, 79.725, 79.725, 79.475, 79.475, 79.575, 79.575, 80.29166666666667, 80.29166666666667, 80.375, 80.375, 80.35, 80.35, 80.59166666666667, 80.59166666666667, 80.275, 80.275, 79.98333333333333, 79.98333333333333, 80.08333333333333, 80.08333333333333, 80.6, 80.6, 80.75, 80.75, 80.73333333333333, 80.73333333333333, 81.025, 81.025, 80.51666666666667, 80.51666666666667, 80.575, 80.575, 80.6, 80.6, 80.825, 80.825, 81.06666666666666, 81.06666666666666, 80.81666666666666, 80.81666666666666, 80.48333333333333, 80.48333333333333, 80.68333333333334, 80.68333333333334, 80.50833333333334, 80.50833333333334, 80.53333333333333, 80.53333333333333, 80.99166666666666, 80.99166666666666, 80.85833333333333, 80.85833333333333, 80.86666666666666, 80.86666666666666, 80.83333333333333, 80.83333333333333, 80.81666666666666, 80.81666666666666, 80.925, 80.925, 81.09166666666667, 81.09166666666667, 81.18333333333334, 81.18333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.049, Test loss: 0.746, Test accuracy: 68.45
Average accuracy final 10 rounds: 67.58583333333333
1839.7670996189117
[]
[29.808333333333334, 27.775, 34.11666666666667, 38.59166666666667, 46.80833333333333, 47.81666666666667, 44.65, 43.65833333333333, 44.53333333333333, 45.45, 45.30833333333333, 46.74166666666667, 51.166666666666664, 51.40833333333333, 52.108333333333334, 52.858333333333334, 51.65833333333333, 52.63333333333333, 54.81666666666667, 53.88333333333333, 55.35, 55.141666666666666, 53.05, 54.525, 54.775, 55.833333333333336, 55.975, 54.925, 56.358333333333334, 55.375, 54.74166666666667, 55.81666666666667, 55.75, 56.35, 56.166666666666664, 55.15833333333333, 58.166666666666664, 58.09166666666667, 58.583333333333336, 59.275, 58.625, 58.725, 59.61666666666667, 58.24166666666667, 57.875, 58.7, 60.34166666666667, 61.333333333333336, 61.19166666666667, 61.375, 62.583333333333336, 61.891666666666666, 61.38333333333333, 61.11666666666667, 62.05, 62.525, 63.46666666666667, 62.983333333333334, 62.916666666666664, 64.775, 64.75, 63.9, 64.20833333333333, 63.925, 65.59166666666667, 65.325, 65.05833333333334, 65.8, 64.8, 65.25833333333334, 64.29166666666667, 65.38333333333334, 64.29166666666667, 64.70833333333333, 65.29166666666667, 64.85, 66.2, 65.13333333333334, 66.54166666666667, 65.525, 64.61666666666666, 65.70833333333333, 65.9, 65.90833333333333, 66.89166666666667, 66.7, 65.725, 65.975, 65.225, 66.40833333333333, 66.44166666666666, 65.84166666666667, 65.50833333333334, 68.01666666666667, 68.05, 67.76666666666667, 69.19166666666666, 67.51666666666667, 68.89166666666667, 68.63333333333334, 68.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.573, Test loss: 0.558, Test accuracy: 77.44
Average accuracy final 10 rounds: 77.0725
Average global accuracy final 10 rounds: 77.0725
1357.46582198143
[]
[30.208333333333332, 44.84166666666667, 44.9, 51.03333333333333, 55.825, 62.50833333333333, 61.45, 62.625, 64.6, 65.84166666666667, 65.26666666666667, 64.99166666666666, 66.425, 68.48333333333333, 68.65833333333333, 68.2, 69.71666666666667, 70.18333333333334, 70.51666666666667, 70.325, 70.16666666666667, 70.80833333333334, 70.85, 70.99166666666666, 70.925, 71.775, 71.44166666666666, 70.53333333333333, 72.09166666666667, 72.525, 73.19166666666666, 72.25, 73.65833333333333, 74.75, 74.85833333333333, 74.65833333333333, 75.00833333333334, 75.28333333333333, 75.20833333333333, 74.90833333333333, 74.7, 74.35833333333333, 74.59166666666667, 74.5, 74.8, 74.20833333333333, 74.375, 74.34166666666667, 74.775, 74.85833333333333, 74.525, 74.1, 74.10833333333333, 74.175, 75.425, 75.10833333333333, 75.75, 76.15, 76.55, 76.96666666666667, 77.6, 77.45833333333333, 77.65833333333333, 77.48333333333333, 76.68333333333334, 76.44166666666666, 76.91666666666667, 76.65, 76.46666666666667, 76.2, 76.75833333333334, 76.76666666666667, 76.36666666666666, 75.81666666666666, 76.28333333333333, 76.35833333333333, 76.91666666666667, 77.375, 77.16666666666667, 77.1, 76.70833333333333, 76.98333333333333, 76.9, 77.44166666666666, 77.325, 77.425, 77.375, 77.06666666666666, 77.2, 77.3, 77.23333333333333, 77.09166666666667, 77.875, 78.03333333333333, 77.31666666666666, 76.775, 76.35833333333333, 76.71666666666667, 76.15, 77.175, 77.44166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.163, Test loss: 1.070, Test accuracy: 66.91
Average accuracy final 10 rounds: 62.64999999999999
2407.990555047989
[3.8178532123565674, 7.424312591552734, 10.79268193244934, 14.202137470245361, 17.60018038749695, 20.97981882095337, 24.30562424659729, 27.70861029624939, 31.088043689727783, 34.49903106689453, 37.88311982154846, 41.29109597206116, 44.67161750793457, 48.08990716934204, 51.50739765167236, 54.90304183959961, 58.29339027404785, 61.73516511917114, 65.15950775146484, 68.54207682609558, 71.93524408340454, 75.36072707176208, 78.75400280952454, 82.13612127304077, 85.50204253196716, 88.8886296749115, 92.29623222351074, 95.69445538520813, 99.04831075668335, 102.39633393287659, 105.80082058906555, 109.20182013511658, 112.64568281173706, 116.05007982254028, 119.47193765640259, 122.8953685760498, 126.35168027877808, 129.6994550228119, 133.11949372291565, 136.54532957077026, 139.99552512168884, 143.4227077960968, 146.85330486297607, 150.30621671676636, 153.81890869140625, 157.28755712509155, 160.88517022132874, 164.41802501678467, 167.81214904785156, 171.18428540229797, 174.55242943763733, 177.9549331665039, 181.36996793746948, 184.8591866493225, 188.17762923240662, 191.59168100357056, 194.94247269630432, 198.32164692878723, 201.64906287193298, 205.03868579864502, 208.46787095069885, 211.86685037612915, 215.24874448776245, 218.62479376792908, 222.04002594947815, 225.4133267402649, 228.83696150779724, 232.209144115448, 235.61276984214783, 239.0166666507721, 242.45007300376892, 245.84758734703064, 249.29664063453674, 252.63941025733948, 256.0370662212372, 259.4197027683258, 262.85014843940735, 266.25413703918457, 269.6597855091095, 273.06925201416016, 276.6849069595337, 280.2855968475342, 283.7042033672333, 287.06886863708496, 290.46539187431335, 293.7843236923218, 297.20274114608765, 300.52839255332947, 304.1619379520416, 307.77367329597473, 311.3371753692627, 314.62992906570435, 317.98955249786377, 321.6785349845886, 325.39179611206055, 329.0553059577942, 332.7420039176941, 336.36754989624023, 340.01090455055237, 343.4203300476074, 346.33541464805603]
[20.008333333333333, 27.033333333333335, 24.233333333333334, 28.558333333333334, 35.791666666666664, 34.56666666666667, 36.833333333333336, 38.166666666666664, 43.608333333333334, 36.325, 47.24166666666667, 46.05, 37.025, 45.96666666666667, 48.15833333333333, 46.166666666666664, 52.30833333333333, 44.925, 47.575, 52.55, 48.13333333333333, 50.44166666666667, 51.475, 46.583333333333336, 47.791666666666664, 53.85, 43.108333333333334, 53.7, 53.525, 54.833333333333336, 57.8, 56.71666666666667, 54.95, 51.358333333333334, 57.516666666666666, 55.1, 58.34166666666667, 55.075, 51.78333333333333, 53.983333333333334, 56.81666666666667, 56.43333333333333, 54.925, 56.36666666666667, 52.775, 55.625, 56.99166666666667, 60.675, 57.391666666666666, 55.00833333333333, 58.56666666666667, 63.18333333333333, 61.00833333333333, 62.375, 62.358333333333334, 58.3, 61.50833333333333, 57.43333333333333, 56.4, 59.583333333333336, 61.5, 62.416666666666664, 56.84166666666667, 59.541666666666664, 62.05833333333333, 61.641666666666666, 59.55, 61.43333333333333, 59.291666666666664, 58.80833333333333, 60.858333333333334, 62.833333333333336, 60.958333333333336, 62.233333333333334, 55.31666666666667, 56.68333333333333, 58.28333333333333, 62.075, 60.475, 60.391666666666666, 60.525, 62.416666666666664, 64.675, 62.19166666666667, 60.291666666666664, 60.4, 61.391666666666666, 60.225, 63.38333333333333, 63.458333333333336, 63.30833333333333, 63.34166666666667, 63.766666666666666, 57.541666666666664, 63.075, 64.325, 62.50833333333333, 61.358333333333334, 64.4, 62.875, 66.90833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 58235 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53709 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.215, Test loss: 0.373, Test accuracy: 85.83
Average accuracy final 10 rounds: 85.36333333333333
1649.208075761795
[2.1210596561431885, 4.242119312286377, 5.909784317016602, 7.577449321746826, 9.18256664276123, 10.787683963775635, 12.360110998153687, 13.932538032531738, 15.45419454574585, 16.97585105895996, 18.527546644210815, 20.07924222946167, 21.78557562828064, 23.49190902709961, 25.146480560302734, 26.80105209350586, 28.399035215377808, 29.997018337249756, 31.63840675354004, 33.27979516983032, 34.91660165786743, 36.55340814590454, 38.15125322341919, 39.74909830093384, 41.39940142631531, 43.04970455169678, 44.74054789543152, 46.43139123916626, 47.954999685287476, 49.47860813140869, 51.06847095489502, 52.65833377838135, 54.251904010772705, 55.84547424316406, 57.428446769714355, 59.01141929626465, 60.604586601257324, 62.19775390625, 63.82208490371704, 65.44641590118408, 66.97963571548462, 68.51285552978516, 70.10514616966248, 71.6974368095398, 73.34346842765808, 74.98950004577637, 76.55047535896301, 78.11145067214966, 79.64500164985657, 81.17855262756348, 82.75786113739014, 84.3371696472168, 85.90829396247864, 87.47941827774048, 89.01172184944153, 90.54402542114258, 92.13007664680481, 93.71612787246704, 95.2695426940918, 96.82295751571655, 98.33693027496338, 99.8509030342102, 101.3977038860321, 102.944504737854, 104.58964037895203, 106.23477602005005, 107.78309679031372, 109.33141756057739, 110.89743280410767, 112.46344804763794, 114.07480955123901, 115.68617105484009, 117.25644540786743, 118.82671976089478, 120.37913012504578, 121.93154048919678, 123.54799365997314, 125.16444683074951, 126.71835041046143, 128.27225399017334, 129.80702304840088, 131.34179210662842, 132.91081476211548, 134.47983741760254, 136.0504858493805, 137.62113428115845, 139.1550648212433, 140.68899536132812, 142.38507604599, 144.08115673065186, 145.7536027431488, 147.42604875564575, 148.97497463226318, 150.52390050888062, 152.08985686302185, 153.6558132171631, 155.22841596603394, 156.80101871490479, 158.3558759689331, 159.91073322296143, 161.4502923488617, 162.98985147476196, 164.56381630897522, 166.13778114318848, 167.71016883850098, 169.28255653381348, 170.92397952079773, 172.56540250778198, 174.22550177574158, 175.88560104370117, 177.53349566459656, 179.18139028549194, 180.8905544281006, 182.59971857070923, 184.31727766990662, 186.034836769104, 187.73688316345215, 189.4389295578003, 191.07892489433289, 192.71892023086548, 194.45369744300842, 196.18847465515137, 197.862774848938, 199.5370750427246, 201.21692609786987, 202.89677715301514, 204.62144780158997, 206.3461184501648, 208.07420778274536, 209.80229711532593, 211.49585461616516, 213.1894121170044, 214.89941787719727, 216.60942363739014, 218.31918168067932, 220.0289397239685, 221.73769521713257, 223.44645071029663, 225.19645953178406, 226.94646835327148, 228.5862340927124, 230.22599983215332, 231.78826332092285, 233.35052680969238, 234.92745757102966, 236.50438833236694, 238.0334050655365, 239.56242179870605, 241.0853295326233, 242.60823726654053, 244.15000700950623, 245.69177675247192, 247.22805333137512, 248.76432991027832, 250.28749299049377, 251.81065607070923, 253.3352768421173, 254.8598976135254, 256.38132524490356, 257.90275287628174, 259.44001483917236, 260.977276802063, 262.60183548927307, 264.22639417648315, 265.8518168926239, 267.47723960876465, 268.99873447418213, 270.5202293395996, 272.1016516685486, 273.68307399749756, 275.2586770057678, 276.8342800140381, 278.3497042655945, 279.8651285171509, 281.3862998485565, 282.90747117996216, 284.4321222305298, 285.9567732810974, 287.4823694229126, 289.0079655647278, 290.52937173843384, 292.0507779121399, 293.59717059135437, 295.14356327056885, 296.67238569259644, 298.201208114624, 299.7455995082855, 301.289990901947, 302.86350560188293, 304.43702030181885, 305.99471712112427, 307.5524139404297, 309.0825674533844, 310.6127209663391, 312.258731842041, 313.9047427177429, 315.54587864875793, 317.18701457977295, 318.79011034965515, 320.39320611953735, 322.5570077896118, 324.7208094596863]
[20.683333333333334, 20.683333333333334, 38.475, 38.475, 46.49166666666667, 46.49166666666667, 58.425, 58.425, 61.041666666666664, 61.041666666666664, 63.208333333333336, 63.208333333333336, 65.79166666666667, 65.79166666666667, 68.56666666666666, 68.56666666666666, 70.975, 70.975, 72.48333333333333, 72.48333333333333, 73.325, 73.325, 73.63333333333334, 73.63333333333334, 74.45833333333333, 74.45833333333333, 74.775, 74.775, 75.35, 75.35, 75.61666666666666, 75.61666666666666, 75.94166666666666, 75.94166666666666, 76.63333333333334, 76.63333333333334, 76.575, 76.575, 77.73333333333333, 77.73333333333333, 77.58333333333333, 77.58333333333333, 77.89166666666667, 77.89166666666667, 78.85, 78.85, 79.06666666666666, 79.06666666666666, 79.025, 79.025, 79.55, 79.55, 79.55, 79.55, 80.05833333333334, 80.05833333333334, 80.35, 80.35, 80.375, 80.375, 80.64166666666667, 80.64166666666667, 80.35, 80.35, 80.39166666666667, 80.39166666666667, 81.325, 81.325, 81.475, 81.475, 80.95833333333333, 80.95833333333333, 80.55, 80.55, 80.875, 80.875, 80.5, 80.5, 81.80833333333334, 81.80833333333334, 81.71666666666667, 81.71666666666667, 81.93333333333334, 81.93333333333334, 82.00833333333334, 82.00833333333334, 82.225, 82.225, 82.31666666666666, 82.31666666666666, 83.09166666666667, 83.09166666666667, 82.825, 82.825, 82.65833333333333, 82.65833333333333, 83.1, 83.1, 82.55, 82.55, 82.90833333333333, 82.90833333333333, 83.525, 83.525, 83.18333333333334, 83.18333333333334, 83.64166666666667, 83.64166666666667, 83.175, 83.175, 83.65, 83.65, 83.85, 83.85, 83.70833333333333, 83.70833333333333, 83.84166666666667, 83.84166666666667, 84.16666666666667, 84.16666666666667, 84.24166666666666, 84.24166666666666, 84.25833333333334, 84.25833333333334, 84.21666666666667, 84.21666666666667, 84.51666666666667, 84.51666666666667, 84.23333333333333, 84.23333333333333, 84.36666666666666, 84.36666666666666, 84.40833333333333, 84.40833333333333, 84.56666666666666, 84.56666666666666, 84.25833333333334, 84.25833333333334, 84.39166666666667, 84.39166666666667, 84.15833333333333, 84.15833333333333, 84.34166666666667, 84.34166666666667, 84.65, 84.65, 84.45833333333333, 84.45833333333333, 84.41666666666667, 84.41666666666667, 84.86666666666666, 84.86666666666666, 84.89166666666667, 84.89166666666667, 84.73333333333333, 84.73333333333333, 84.425, 84.425, 84.70833333333333, 84.70833333333333, 85.05833333333334, 85.05833333333334, 84.45, 84.45, 84.70833333333333, 84.70833333333333, 84.74166666666666, 84.74166666666666, 84.98333333333333, 84.98333333333333, 85.26666666666667, 85.26666666666667, 85.08333333333333, 85.08333333333333, 84.96666666666667, 84.96666666666667, 85.15833333333333, 85.15833333333333, 85.725, 85.725, 85.01666666666667, 85.01666666666667, 85.24166666666666, 85.24166666666666, 85.56666666666666, 85.56666666666666, 85.18333333333334, 85.18333333333334, 85.45, 85.45, 85.44166666666666, 85.44166666666666, 85.44166666666666, 85.44166666666666, 85.3, 85.3, 85.49166666666666, 85.49166666666666, 85.5, 85.5, 85.825, 85.825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.058, Test loss: 1.028, Test accuracy: 80.04
Final Round, Global train loss: 0.058, Global test loss: 1.989, Global test accuracy: 32.63
Average accuracy final 10 rounds: 79.59916666666666 

Average global accuracy final 10 rounds: 26.029999999999998 

1810.3538954257965
[1.609360694885254, 3.218721389770508, 4.5869140625, 5.955106735229492, 7.385869979858398, 8.816633224487305, 10.437080144882202, 12.0575270652771, 13.49044394493103, 14.923360824584961, 16.391185760498047, 17.859010696411133, 19.292522192001343, 20.726033687591553, 22.21331524848938, 23.700596809387207, 25.165102243423462, 26.629607677459717, 28.083008527755737, 29.536409378051758, 30.99995517730713, 32.4635009765625, 33.916221141815186, 35.36894130706787, 36.78381824493408, 38.19869518280029, 39.68005394935608, 41.161412715911865, 42.631410360336304, 44.10140800476074, 45.56819772720337, 47.034987449645996, 48.48869466781616, 49.94240188598633, 51.391117811203, 52.83983373641968, 54.278706789016724, 55.71757984161377, 57.18713450431824, 58.656689167022705, 60.12229132652283, 61.58789348602295, 63.040940046310425, 64.4939866065979, 65.95756435394287, 67.42114210128784, 68.82442283630371, 70.22770357131958, 71.67771863937378, 73.12773370742798, 74.61517429351807, 76.10261487960815, 77.54547882080078, 78.98834276199341, 80.40457725524902, 81.82081174850464, 83.11109495162964, 84.40137815475464, 85.68555355072021, 86.96972894668579, 88.2375316619873, 89.50533437728882, 90.77452182769775, 92.04370927810669, 93.33271193504333, 94.62171459197998, 95.92311501502991, 97.22451543807983, 98.49691653251648, 99.76931762695312, 101.03294324874878, 102.29656887054443, 103.58773016929626, 104.8788914680481, 106.19453954696655, 107.51018762588501, 108.79258584976196, 110.07498407363892, 111.38984417915344, 112.70470428466797, 114.01017117500305, 115.31563806533813, 116.5949215888977, 117.87420511245728, 119.18523287773132, 120.49626064300537, 121.78318476676941, 123.07010889053345, 124.33020377159119, 125.59029865264893, 126.86766910552979, 128.14503955841064, 129.42099142074585, 130.69694328308105, 131.98879528045654, 133.28064727783203, 134.57226634025574, 135.86388540267944, 137.1340527534485, 138.40422010421753, 139.68161153793335, 140.95900297164917, 142.25648522377014, 143.5539674758911, 144.81346988677979, 146.07297229766846, 147.33317828178406, 148.59338426589966, 149.88426899909973, 151.1751537322998, 152.44908237457275, 153.7230110168457, 155.01020693778992, 156.29740285873413, 157.5663321018219, 158.83526134490967, 160.10675621032715, 161.37825107574463, 162.66262650489807, 163.9470019340515, 165.23445844650269, 166.52191495895386, 167.8206114768982, 169.11930799484253, 170.42356991767883, 171.72783184051514, 173.02721667289734, 174.32660150527954, 175.60636448860168, 176.88612747192383, 178.1758735179901, 179.4656195640564, 180.7625377178192, 182.05945587158203, 183.36642241477966, 184.6733889579773, 185.9723606109619, 187.27133226394653, 188.54228281974792, 189.81323337554932, 191.07346606254578, 192.33369874954224, 193.60178756713867, 194.8698763847351, 196.15021634101868, 197.43055629730225, 198.69078493118286, 199.95101356506348, 201.21370315551758, 202.47639274597168, 203.74169826507568, 205.0070037841797, 206.28002738952637, 207.55305099487305, 208.80338406562805, 210.05371713638306, 211.29867005348206, 212.54362297058105, 213.7814223766327, 215.01922178268433, 216.27518391609192, 217.5311460494995, 218.77878189086914, 220.02641773223877, 221.2603404521942, 222.49426317214966, 223.7465488910675, 224.99883460998535, 226.24948859214783, 227.5001425743103, 228.74225902557373, 229.98437547683716, 231.223308801651, 232.46224212646484, 233.71166586875916, 234.96108961105347, 236.21267795562744, 237.46426630020142, 238.69967484474182, 239.93508338928223, 241.1787292957306, 242.42237520217896, 243.66723942756653, 244.9121036529541, 246.14795517921448, 247.38380670547485, 248.61320281028748, 249.8425989151001, 251.09008240699768, 252.33756589889526, 253.57844352722168, 254.8193211555481, 256.0584440231323, 257.29756689071655, 258.54084300994873, 259.7841191291809, 261.04276633262634, 262.3014135360718, 263.5741217136383, 264.84682989120483, 267.2532362937927, 269.6596426963806]
[33.0, 33.0, 47.166666666666664, 47.166666666666664, 52.275, 52.275, 61.13333333333333, 61.13333333333333, 60.19166666666667, 60.19166666666667, 62.833333333333336, 62.833333333333336, 61.516666666666666, 61.516666666666666, 65.15, 65.15, 63.975, 63.975, 68.08333333333333, 68.08333333333333, 66.9, 66.9, 67.025, 67.025, 71.45833333333333, 71.45833333333333, 67.71666666666667, 67.71666666666667, 69.60833333333333, 69.60833333333333, 70.875, 70.875, 75.7, 75.7, 75.975, 75.975, 76.225, 76.225, 75.76666666666667, 75.76666666666667, 75.725, 75.725, 75.48333333333333, 75.48333333333333, 75.46666666666667, 75.46666666666667, 75.80833333333334, 75.80833333333334, 75.91666666666667, 75.91666666666667, 76.81666666666666, 76.81666666666666, 77.1, 77.1, 77.55833333333334, 77.55833333333334, 77.39166666666667, 77.39166666666667, 77.41666666666667, 77.41666666666667, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.84166666666667, 77.84166666666667, 77.35, 77.35, 77.79166666666667, 77.79166666666667, 78.3, 78.3, 78.39166666666667, 78.39166666666667, 78.375, 78.375, 78.375, 78.375, 78.725, 78.725, 78.625, 78.625, 78.41666666666667, 78.41666666666667, 78.49166666666666, 78.49166666666666, 78.65, 78.65, 78.8, 78.8, 79.00833333333334, 79.00833333333334, 78.825, 78.825, 78.875, 78.875, 78.9, 78.9, 78.825, 78.825, 78.86666666666666, 78.86666666666666, 79.14166666666667, 79.14166666666667, 79.40833333333333, 79.40833333333333, 79.31666666666666, 79.31666666666666, 79.28333333333333, 79.28333333333333, 79.24166666666666, 79.24166666666666, 79.16666666666667, 79.16666666666667, 79.05833333333334, 79.05833333333334, 78.75833333333334, 78.75833333333334, 78.70833333333333, 78.70833333333333, 78.24166666666666, 78.24166666666666, 78.58333333333333, 78.58333333333333, 78.93333333333334, 78.93333333333334, 78.85833333333333, 78.85833333333333, 78.78333333333333, 78.78333333333333, 79.25833333333334, 79.25833333333334, 79.58333333333333, 79.58333333333333, 79.50833333333334, 79.50833333333334, 79.19166666666666, 79.19166666666666, 79.33333333333333, 79.33333333333333, 79.375, 79.375, 79.475, 79.475, 78.85, 78.85, 79.16666666666667, 79.16666666666667, 79.49166666666666, 79.49166666666666, 79.24166666666666, 79.24166666666666, 78.94166666666666, 78.94166666666666, 79.11666666666666, 79.11666666666666, 79.56666666666666, 79.56666666666666, 79.68333333333334, 79.68333333333334, 79.41666666666667, 79.41666666666667, 79.5, 79.5, 79.45833333333333, 79.45833333333333, 79.34166666666667, 79.34166666666667, 79.175, 79.175, 79.16666666666667, 79.16666666666667, 79.21666666666667, 79.21666666666667, 79.3, 79.3, 79.36666666666666, 79.36666666666666, 79.55833333333334, 79.55833333333334, 79.64166666666667, 79.64166666666667, 79.425, 79.425, 79.43333333333334, 79.43333333333334, 79.65, 79.65, 79.60833333333333, 79.60833333333333, 79.875, 79.875, 79.68333333333334, 79.68333333333334, 79.58333333333333, 79.58333333333333, 79.64166666666667, 79.64166666666667, 79.45, 79.45, 80.04166666666667, 80.04166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.152, Test loss: 0.495, Test accuracy: 86.00
Final Round, Global train loss: 0.152, Global test loss: 1.462, Global test accuracy: 59.88
Average accuracy final 10 rounds: 85.4775 

Average global accuracy final 10 rounds: 66.64916666666667 

1809.4219617843628
[1.6397664546966553, 3.2795329093933105, 4.659959077835083, 6.0403852462768555, 7.453839302062988, 8.867293357849121, 10.279869318008423, 11.692445278167725, 13.091654062271118, 14.490862846374512, 15.90920114517212, 17.327539443969727, 18.789653778076172, 20.251768112182617, 21.701507568359375, 23.151247024536133, 24.582286596298218, 26.013326168060303, 27.470691919326782, 28.92805767059326, 30.35317611694336, 31.778294563293457, 33.240174770355225, 34.70205497741699, 36.1061224937439, 37.5101900100708, 38.93978238105774, 40.36937475204468, 41.80388951301575, 43.238404273986816, 44.67634105682373, 46.114277839660645, 47.53536558151245, 48.95645332336426, 50.37647557258606, 51.79649782180786, 53.2147536277771, 54.63300943374634, 56.053404569625854, 57.47379970550537, 58.91081881523132, 60.347837924957275, 61.76723337173462, 63.18662881851196, 64.62022423744202, 66.05381965637207, 67.48720598220825, 68.92059230804443, 70.36402583122253, 71.80745935440063, 73.23584508895874, 74.66423082351685, 76.10158252716064, 77.53893423080444, 78.97589063644409, 80.41284704208374, 81.84371995925903, 83.27459287643433, 84.71752572059631, 86.1604585647583, 87.60700869560242, 89.05355882644653, 90.4875876903534, 91.92161655426025, 93.34927201271057, 94.77692747116089, 96.21775269508362, 97.65857791900635, 99.09440040588379, 100.53022289276123, 101.9706563949585, 103.41108989715576, 104.86259341239929, 106.31409692764282, 107.75558996200562, 109.19708299636841, 110.638742685318, 112.08040237426758, 113.52139663696289, 114.9623908996582, 116.39064598083496, 117.81890106201172, 119.24556469917297, 120.67222833633423, 122.10238528251648, 123.53254222869873, 124.96743988990784, 126.40233755111694, 127.82263350486755, 129.24292945861816, 130.67671823501587, 132.11050701141357, 133.54994368553162, 134.98938035964966, 136.42730450630188, 137.8652286529541, 139.29307556152344, 140.72092247009277, 141.9581413269043, 143.19536018371582, 144.4235188961029, 145.65167760849, 146.8777916431427, 148.1039056777954, 149.33883476257324, 150.57376384735107, 151.79204058647156, 153.01031732559204, 154.2300362586975, 155.44975519180298, 156.68337774276733, 157.9170002937317, 159.14023232460022, 160.36346435546875, 161.5948839187622, 162.82630348205566, 164.05385756492615, 165.28141164779663, 166.517502784729, 167.75359392166138, 168.98597836494446, 170.21836280822754, 171.43905019760132, 172.6597375869751, 173.88647508621216, 175.11321258544922, 176.3466558456421, 177.58009910583496, 178.80455422401428, 180.0290093421936, 181.26016402244568, 182.49131870269775, 183.7291555404663, 184.96699237823486, 186.1867651939392, 187.40653800964355, 188.63043022155762, 189.85432243347168, 191.0774998664856, 192.3006772994995, 193.51960921287537, 194.73854112625122, 195.9716923236847, 197.20484352111816, 198.4348418712616, 199.66484022140503, 200.89417910575867, 202.1235179901123, 203.35323429107666, 204.58295059204102, 205.81695675849915, 207.05096292495728, 208.27856922149658, 209.5061755180359, 210.73494124412537, 211.96370697021484, 213.19663667678833, 214.42956638336182, 215.66809391975403, 216.90662145614624, 218.14791917800903, 219.38921689987183, 220.61961603164673, 221.85001516342163, 223.09099125862122, 224.3319673538208, 225.56358003616333, 226.79519271850586, 228.02914571762085, 229.26309871673584, 230.4932360649109, 231.72337341308594, 232.9562873840332, 234.18920135498047, 235.41745257377625, 236.64570379257202, 237.87344908714294, 239.10119438171387, 240.3270444869995, 241.55289459228516, 242.78830814361572, 244.0237216949463, 245.24405670166016, 246.46439170837402, 247.6918272972107, 248.91926288604736, 250.13965725898743, 251.3600516319275, 252.58591151237488, 253.81177139282227, 255.0415506362915, 256.27132987976074, 257.50398778915405, 258.73664569854736, 260.2014629840851, 261.6662802696228, 263.10877561569214, 264.5512709617615, 265.9933488368988, 267.43542671203613, 269.844131231308, 272.25283575057983]
[29.383333333333333, 29.383333333333333, 34.666666666666664, 34.666666666666664, 49.59166666666667, 49.59166666666667, 53.69166666666667, 53.69166666666667, 57.375, 57.375, 68.44166666666666, 68.44166666666666, 66.55833333333334, 66.55833333333334, 69.40833333333333, 69.40833333333333, 68.45, 68.45, 72.175, 72.175, 72.75833333333334, 72.75833333333334, 73.84166666666667, 73.84166666666667, 73.36666666666666, 73.36666666666666, 74.99166666666666, 74.99166666666666, 75.65833333333333, 75.65833333333333, 75.71666666666667, 75.71666666666667, 78.15833333333333, 78.15833333333333, 78.99166666666666, 78.99166666666666, 79.69166666666666, 79.69166666666666, 79.71666666666667, 79.71666666666667, 79.925, 79.925, 80.69166666666666, 80.69166666666666, 80.83333333333333, 80.83333333333333, 80.63333333333334, 80.63333333333334, 80.825, 80.825, 81.31666666666666, 81.31666666666666, 81.425, 81.425, 81.93333333333334, 81.93333333333334, 82.00833333333334, 82.00833333333334, 82.35, 82.35, 82.73333333333333, 82.73333333333333, 82.75833333333334, 82.75833333333334, 82.43333333333334, 82.43333333333334, 82.15833333333333, 82.15833333333333, 82.50833333333334, 82.50833333333334, 82.625, 82.625, 82.71666666666667, 82.71666666666667, 82.95, 82.95, 83.0, 83.0, 83.38333333333334, 83.38333333333334, 83.48333333333333, 83.48333333333333, 84.04166666666667, 84.04166666666667, 83.46666666666667, 83.46666666666667, 83.125, 83.125, 83.48333333333333, 83.48333333333333, 83.23333333333333, 83.23333333333333, 83.225, 83.225, 83.15, 83.15, 83.99166666666666, 83.99166666666666, 84.26666666666667, 84.26666666666667, 84.25833333333334, 84.25833333333334, 84.54166666666667, 84.54166666666667, 84.175, 84.175, 83.975, 83.975, 84.10833333333333, 84.10833333333333, 84.48333333333333, 84.48333333333333, 84.45833333333333, 84.45833333333333, 84.35, 84.35, 84.13333333333334, 84.13333333333334, 84.575, 84.575, 84.175, 84.175, 84.275, 84.275, 84.59166666666667, 84.59166666666667, 83.76666666666667, 83.76666666666667, 83.45, 83.45, 83.85, 83.85, 84.275, 84.275, 84.05, 84.05, 84.63333333333334, 84.63333333333334, 84.75833333333334, 84.75833333333334, 84.78333333333333, 84.78333333333333, 85.21666666666667, 85.21666666666667, 85.28333333333333, 85.28333333333333, 85.43333333333334, 85.43333333333334, 85.30833333333334, 85.30833333333334, 85.16666666666667, 85.16666666666667, 84.65, 84.65, 84.525, 84.525, 84.08333333333333, 84.08333333333333, 84.08333333333333, 84.08333333333333, 83.65833333333333, 83.65833333333333, 84.15, 84.15, 84.325, 84.325, 84.53333333333333, 84.53333333333333, 84.66666666666667, 84.66666666666667, 84.95833333333333, 84.95833333333333, 85.23333333333333, 85.23333333333333, 84.825, 84.825, 84.925, 84.925, 85.41666666666667, 85.41666666666667, 85.75, 85.75, 85.73333333333333, 85.73333333333333, 85.5, 85.5, 85.59166666666667, 85.59166666666667, 85.65, 85.65, 85.66666666666667, 85.66666666666667, 85.25833333333334, 85.25833333333334, 85.23333333333333, 85.23333333333333, 85.23333333333333, 85.23333333333333, 85.15833333333333, 85.15833333333333, 86.0, 86.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.198, Test loss: 0.361, Test accuracy: 86.38
Average accuracy final 10 rounds: 86.1525 

1433.0313503742218
[1.4755096435546875, 2.951019287109375, 4.200707912445068, 5.450396537780762, 6.744216680526733, 8.038036823272705, 9.331805944442749, 10.625575065612793, 11.92225193977356, 13.218928813934326, 14.512876033782959, 15.806823253631592, 17.10746669769287, 18.40811014175415, 19.714025259017944, 21.01994037628174, 22.314350843429565, 23.608761310577393, 24.903976440429688, 26.199191570281982, 27.50011134147644, 28.8010311126709, 30.0981228351593, 31.395214557647705, 32.69799613952637, 34.00077772140503, 35.29912352561951, 36.597469329833984, 37.89577007293701, 39.19407081604004, 40.49856185913086, 41.80305290222168, 43.09563088417053, 44.388208866119385, 45.68792152404785, 46.98763418197632, 48.28886914253235, 49.59010410308838, 50.88963556289673, 52.18916702270508, 53.48998308181763, 54.790799140930176, 56.0921847820282, 57.39357042312622, 58.69026708602905, 59.986963748931885, 61.28575897216797, 62.58455419540405, 63.82748985290527, 65.0704255104065, 66.2511990070343, 67.43197250366211, 68.61703729629517, 69.80210208892822, 70.99278163909912, 72.18346118927002, 73.38140296936035, 74.57934474945068, 75.76774072647095, 76.95613670349121, 78.14191269874573, 79.32768869400024, 80.51622915267944, 81.70476961135864, 82.88314986228943, 84.06153011322021, 85.23700380325317, 86.41247749328613, 87.61003470420837, 88.80759191513062, 89.99589467048645, 91.18419742584229, 92.46706628799438, 93.74993515014648, 94.96737790107727, 96.18482065200806, 97.39849877357483, 98.6121768951416, 99.83559036254883, 101.05900382995605, 102.27837562561035, 103.49774742126465, 104.72874426841736, 105.95974111557007, 107.17912697792053, 108.398512840271, 109.60721826553345, 110.8159236907959, 112.03023767471313, 113.24455165863037, 114.46323466300964, 115.68191766738892, 116.89377641677856, 118.10563516616821, 119.32572770118713, 120.54582023620605, 121.77186393737793, 122.9979076385498, 124.213210105896, 125.42851257324219, 126.6313750743866, 127.834237575531, 129.02779936790466, 130.22136116027832, 131.42115235328674, 132.62094354629517, 133.8170313835144, 135.01311922073364, 136.21355414390564, 137.41398906707764, 138.60934925079346, 139.80470943450928, 141.00792503356934, 142.2111406326294, 143.4118595123291, 144.6125783920288, 145.81774520874023, 147.02291202545166, 148.22012424468994, 149.41733646392822, 150.58669257164001, 151.7560486793518, 152.94318962097168, 154.13033056259155, 155.32486844062805, 156.51940631866455, 157.697979927063, 158.87655353546143, 160.06118202209473, 161.24581050872803, 162.43342447280884, 163.62103843688965, 164.81826949119568, 166.0155005455017, 167.2058253288269, 168.3961501121521, 169.59420132637024, 170.79225254058838, 171.99920415878296, 173.20615577697754, 174.40605759620667, 175.6059594154358, 176.80620765686035, 178.0064558982849, 179.20330786705017, 180.40015983581543, 181.6003715991974, 182.80058336257935, 183.99808382987976, 185.19558429718018, 186.38034296035767, 187.56510162353516, 188.7550904750824, 189.94507932662964, 191.14783930778503, 192.35059928894043, 193.57190775871277, 194.7932162284851, 196.00668811798096, 197.2201600074768, 198.4513177871704, 199.682475566864, 200.90106463432312, 202.11965370178223, 203.34878826141357, 204.57792282104492, 205.81984901428223, 207.06177520751953, 208.2855474948883, 209.50931978225708, 210.72883415222168, 211.94834852218628, 213.19816064834595, 214.44797277450562, 215.6746950149536, 216.9014172554016, 218.10995721817017, 219.31849718093872, 220.52472448349, 221.73095178604126, 222.94525742530823, 224.1595630645752, 225.564546585083, 226.96953010559082, 228.2545042037964, 229.53947830200195, 230.92589592933655, 232.31231355667114, 233.6695272922516, 235.02674102783203, 236.42631483078003, 237.82588863372803, 239.21650218963623, 240.60711574554443, 241.96674704551697, 243.3263783454895, 244.71929502487183, 246.11221170425415, 247.50866603851318, 248.90512037277222, 250.9609739780426, 253.016827583313]
[23.433333333333334, 23.433333333333334, 32.1, 32.1, 47.075, 47.075, 47.15, 47.15, 57.93333333333333, 57.93333333333333, 61.833333333333336, 61.833333333333336, 63.625, 63.625, 67.68333333333334, 67.68333333333334, 69.54166666666667, 69.54166666666667, 68.15, 68.15, 68.28333333333333, 68.28333333333333, 69.56666666666666, 69.56666666666666, 74.99166666666666, 74.99166666666666, 76.41666666666667, 76.41666666666667, 77.175, 77.175, 77.36666666666666, 77.36666666666666, 76.725, 76.725, 77.56666666666666, 77.56666666666666, 77.09166666666667, 77.09166666666667, 78.28333333333333, 78.28333333333333, 78.19166666666666, 78.19166666666666, 78.29166666666667, 78.29166666666667, 79.225, 79.225, 78.8, 78.8, 79.73333333333333, 79.73333333333333, 80.225, 80.225, 79.99166666666666, 79.99166666666666, 80.45833333333333, 80.45833333333333, 79.81666666666666, 79.81666666666666, 81.175, 81.175, 81.2, 81.2, 81.225, 81.225, 81.73333333333333, 81.73333333333333, 81.425, 81.425, 81.325, 81.325, 81.66666666666667, 81.66666666666667, 82.10833333333333, 82.10833333333333, 82.06666666666666, 82.06666666666666, 82.75, 82.75, 82.725, 82.725, 82.95, 82.95, 83.14166666666667, 83.14166666666667, 82.66666666666667, 82.66666666666667, 83.49166666666666, 83.49166666666666, 83.58333333333333, 83.58333333333333, 82.88333333333334, 82.88333333333334, 83.70833333333333, 83.70833333333333, 83.91666666666667, 83.91666666666667, 84.125, 84.125, 83.525, 83.525, 84.34166666666667, 84.34166666666667, 84.425, 84.425, 84.275, 84.275, 84.45, 84.45, 84.41666666666667, 84.41666666666667, 84.26666666666667, 84.26666666666667, 84.39166666666667, 84.39166666666667, 84.74166666666666, 84.74166666666666, 84.38333333333334, 84.38333333333334, 84.45, 84.45, 84.49166666666666, 84.49166666666666, 84.2, 84.2, 84.96666666666667, 84.96666666666667, 85.14166666666667, 85.14166666666667, 84.675, 84.675, 84.84166666666667, 84.84166666666667, 84.94166666666666, 84.94166666666666, 84.81666666666666, 84.81666666666666, 85.0, 85.0, 84.975, 84.975, 84.99166666666666, 84.99166666666666, 85.075, 85.075, 85.49166666666666, 85.49166666666666, 85.51666666666667, 85.51666666666667, 85.69166666666666, 85.69166666666666, 85.7, 85.7, 85.325, 85.325, 85.40833333333333, 85.40833333333333, 85.45833333333333, 85.45833333333333, 85.63333333333334, 85.63333333333334, 86.04166666666667, 86.04166666666667, 85.9, 85.9, 85.86666666666666, 85.86666666666666, 85.78333333333333, 85.78333333333333, 85.8, 85.8, 85.14166666666667, 85.14166666666667, 85.8, 85.8, 86.175, 86.175, 85.81666666666666, 85.81666666666666, 85.98333333333333, 85.98333333333333, 86.15833333333333, 86.15833333333333, 86.03333333333333, 86.03333333333333, 86.225, 86.225, 86.00833333333334, 86.00833333333334, 86.4, 86.4, 86.25833333333334, 86.25833333333334, 86.125, 86.125, 85.95833333333333, 85.95833333333333, 86.18333333333334, 86.18333333333334, 86.175, 86.175, 86.375, 86.375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.134, Test loss: 0.403, Test accuracy: 86.56
Average accuracy final 10 rounds: 86.09083333333334 

1516.2543060779572
[1.7956461906433105, 3.591292381286621, 5.089701175689697, 6.588109970092773, 7.85438084602356, 9.120651721954346, 10.408975839614868, 11.69729995727539, 12.981102705001831, 14.264905452728271, 15.554450035095215, 16.843994617462158, 18.130687475204468, 19.417380332946777, 20.70384931564331, 21.990318298339844, 23.279972791671753, 24.569627285003662, 25.854844570159912, 27.140061855316162, 28.425593614578247, 29.711125373840332, 30.986578702926636, 32.26203203201294, 33.531495332717896, 34.80095863342285, 36.07835292816162, 37.35574722290039, 38.631632566452026, 39.90751791000366, 41.16582703590393, 42.4241361618042, 43.68896555900574, 44.953794956207275, 46.23128533363342, 47.50877571105957, 48.7774338722229, 50.04609203338623, 51.31737756729126, 52.58866310119629, 53.841341733932495, 55.0940203666687, 56.35911560058594, 57.624210834503174, 58.897464990615845, 60.170719146728516, 61.4506402015686, 62.73056125640869, 64.00493359565735, 65.279305934906, 66.55811810493469, 67.83693027496338, 69.12797236442566, 70.41901445388794, 71.78127861022949, 73.14354276657104, 74.44832968711853, 75.75311660766602, 77.00709700584412, 78.26107740402222, 79.52731394767761, 80.79355049133301, 82.04506993293762, 83.29658937454224, 84.55178737640381, 85.80698537826538, 87.23744225502014, 88.6678991317749, 90.14072871208191, 91.61355829238892, 93.05008220672607, 94.48660612106323, 95.95469307899475, 97.42278003692627, 98.87942337989807, 100.33606672286987, 101.82012557983398, 103.3041844367981, 104.76520013809204, 106.22621583938599, 107.70606637001038, 109.18591690063477, 110.66948652267456, 112.15305614471436, 113.63764929771423, 115.12224245071411, 116.62148094177246, 118.12071943283081, 119.60390281677246, 121.08708620071411, 122.55634379386902, 124.02560138702393, 125.3330614566803, 126.64052152633667, 127.94018530845642, 129.23984909057617, 130.52927088737488, 131.81869268417358, 133.1198136806488, 134.42093467712402, 135.8597092628479, 137.29848384857178, 138.73770213127136, 140.17692041397095, 141.61989545822144, 143.06287050247192, 144.51976084709167, 145.97665119171143, 147.46188855171204, 148.94712591171265, 150.44058275222778, 151.93403959274292, 153.39830422401428, 154.86256885528564, 156.31949615478516, 157.77642345428467, 159.23646759986877, 160.69651174545288, 162.14958810806274, 163.6026644706726, 165.05786848068237, 166.51307249069214, 167.9812662601471, 169.44946002960205, 170.9161353111267, 172.38281059265137, 173.85022854804993, 175.3176465034485, 176.77318835258484, 178.2287302017212, 179.6328477859497, 181.03696537017822, 182.41485285758972, 183.79274034500122, 185.17067337036133, 186.54860639572144, 187.93468356132507, 189.3207607269287, 190.77244639396667, 192.22413206100464, 193.6750476360321, 195.12596321105957, 196.58963799476624, 198.0533127784729, 199.51473760604858, 200.97616243362427, 202.430104970932, 203.88404750823975, 205.3448531627655, 206.80565881729126, 208.2714512348175, 209.73724365234375, 211.19639801979065, 212.65555238723755, 214.15092086791992, 215.6462893486023, 217.15879559516907, 218.67130184173584, 220.2025294303894, 221.73375701904297, 223.1816110610962, 224.6294651031494, 226.0863926410675, 227.5433201789856, 229.00704789161682, 230.47077560424805, 231.93016862869263, 233.3895616531372, 234.84454154968262, 236.29952144622803, 237.75826025009155, 239.21699905395508, 240.67382645606995, 242.13065385818481, 243.58878803253174, 245.04692220687866, 246.48352551460266, 247.92012882232666, 249.19037675857544, 250.46062469482422, 251.73296332359314, 253.00530195236206, 254.28241991996765, 255.55953788757324, 256.81009316444397, 258.0606484413147, 259.31260228157043, 260.5645561218262, 261.8153796195984, 263.0662031173706, 264.3133924007416, 265.56058168411255, 266.81202387809753, 268.0634660720825, 269.32081389427185, 270.5781617164612, 271.8379604816437, 273.0977592468262, 274.3610579967499, 275.6243567466736, 277.61983013153076, 279.61530351638794]
[26.4, 26.4, 35.90833333333333, 35.90833333333333, 43.858333333333334, 43.858333333333334, 59.40833333333333, 59.40833333333333, 60.55833333333333, 60.55833333333333, 60.825, 60.825, 67.25, 67.25, 71.8, 71.8, 72.29166666666667, 72.29166666666667, 72.23333333333333, 72.23333333333333, 73.625, 73.625, 74.49166666666666, 74.49166666666666, 75.53333333333333, 75.53333333333333, 73.8, 73.8, 76.09166666666667, 76.09166666666667, 76.98333333333333, 76.98333333333333, 78.775, 78.775, 79.35, 79.35, 79.11666666666666, 79.11666666666666, 78.75833333333334, 78.75833333333334, 78.86666666666666, 78.86666666666666, 80.29166666666667, 80.29166666666667, 81.24166666666666, 81.24166666666666, 81.01666666666667, 81.01666666666667, 81.91666666666667, 81.91666666666667, 81.80833333333334, 81.80833333333334, 81.60833333333333, 81.60833333333333, 82.03333333333333, 82.03333333333333, 81.80833333333334, 81.80833333333334, 83.04166666666667, 83.04166666666667, 82.45833333333333, 82.45833333333333, 82.9, 82.9, 83.325, 83.325, 83.48333333333333, 83.48333333333333, 83.84166666666667, 83.84166666666667, 83.88333333333334, 83.88333333333334, 84.05, 84.05, 84.05, 84.05, 84.29166666666667, 84.29166666666667, 84.4, 84.4, 84.14166666666667, 84.14166666666667, 84.80833333333334, 84.80833333333334, 84.10833333333333, 84.10833333333333, 85.1, 85.1, 84.4, 84.4, 84.525, 84.525, 84.99166666666666, 84.99166666666666, 84.725, 84.725, 85.1, 85.1, 85.1, 85.1, 85.18333333333334, 85.18333333333334, 85.2, 85.2, 85.25, 85.25, 85.09166666666667, 85.09166666666667, 85.20833333333333, 85.20833333333333, 85.725, 85.725, 85.68333333333334, 85.68333333333334, 85.45833333333333, 85.45833333333333, 85.6, 85.6, 85.23333333333333, 85.23333333333333, 85.65833333333333, 85.65833333333333, 85.5, 85.5, 85.70833333333333, 85.70833333333333, 85.58333333333333, 85.58333333333333, 86.00833333333334, 86.00833333333334, 85.60833333333333, 85.60833333333333, 86.03333333333333, 86.03333333333333, 85.59166666666667, 85.59166666666667, 85.89166666666667, 85.89166666666667, 85.48333333333333, 85.48333333333333, 85.60833333333333, 85.60833333333333, 85.71666666666667, 85.71666666666667, 85.63333333333334, 85.63333333333334, 85.825, 85.825, 85.75833333333334, 85.75833333333334, 85.825, 85.825, 85.325, 85.325, 85.68333333333334, 85.68333333333334, 85.2, 85.2, 85.93333333333334, 85.93333333333334, 85.525, 85.525, 85.39166666666667, 85.39166666666667, 85.94166666666666, 85.94166666666666, 86.03333333333333, 86.03333333333333, 86.08333333333333, 86.08333333333333, 86.09166666666667, 86.09166666666667, 85.85, 85.85, 86.28333333333333, 86.28333333333333, 85.95, 85.95, 86.38333333333334, 86.38333333333334, 86.00833333333334, 86.00833333333334, 86.275, 86.275, 86.11666666666666, 86.11666666666666, 86.0, 86.0, 86.25, 86.25, 86.04166666666667, 86.04166666666667, 85.85, 85.85, 86.01666666666667, 86.01666666666667, 86.26666666666667, 86.26666666666667, 86.08333333333333, 86.08333333333333, 86.55833333333334, 86.55833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.039, Test loss: 1.024, Test accuracy: 81.47
Average accuracy final 10 rounds: 81.13250000000001 

1604.8656768798828
[1.7969813346862793, 3.5939626693725586, 5.120118856430054, 6.646275043487549, 8.172393083572388, 9.698511123657227, 11.210453033447266, 12.722394943237305, 14.224781274795532, 15.72716760635376, 17.240439414978027, 18.753711223602295, 20.257832765579224, 21.761954307556152, 23.269670009613037, 24.777385711669922, 26.29940915107727, 27.82143259048462, 29.345702171325684, 30.869971752166748, 32.38777804374695, 33.90558433532715, 35.415860176086426, 36.9261360168457, 38.4373300075531, 39.9485239982605, 41.46706676483154, 42.98560953140259, 44.509315490722656, 46.033021450042725, 47.54535961151123, 49.057697772979736, 50.57048320770264, 52.08326864242554, 53.60569643974304, 55.12812423706055, 56.65041399002075, 58.17270374298096, 59.69227623939514, 61.211848735809326, 62.733927965164185, 64.25600719451904, 65.78424572944641, 67.31248426437378, 68.83303594589233, 70.35358762741089, 71.87055492401123, 73.38752222061157, 74.90781211853027, 76.42810201644897, 77.95488905906677, 79.48167610168457, 81.00399851799011, 82.52632093429565, 84.04654693603516, 85.56677293777466, 87.10035634040833, 88.63393974304199, 90.14591217041016, 91.65788459777832, 93.17038154602051, 94.6828784942627, 96.20209097862244, 97.72130346298218, 99.22564578056335, 100.72998809814453, 102.23423600196838, 103.73848390579224, 105.24636721611023, 106.75425052642822, 108.26986122131348, 109.78547191619873, 111.29747676849365, 112.80948162078857, 114.31955933570862, 115.82963705062866, 117.35249471664429, 118.87535238265991, 120.39486455917358, 121.91437673568726, 123.43125176429749, 124.94812679290771, 126.45550394058228, 127.96288108825684, 129.4738006591797, 130.98472023010254, 132.48633432388306, 133.98794841766357, 135.48137617111206, 136.97480392456055, 138.48857831954956, 140.00235271453857, 141.51864790916443, 143.03494310379028, 144.54151272773743, 146.04808235168457, 147.56853246688843, 149.08898258209229, 150.53289914131165, 151.976815700531, 153.42506980895996, 154.87332391738892, 156.3196985721588, 157.7660732269287, 159.22006559371948, 160.67405796051025, 162.12424278259277, 163.5744276046753, 165.0307137966156, 166.4869999885559, 167.9910478591919, 169.49509572982788, 170.95792031288147, 172.42074489593506, 173.9204969406128, 175.42024898529053, 176.92569088935852, 178.4311327934265, 179.94108390808105, 181.4510350227356, 182.95999693870544, 184.4689588546753, 185.9261772632599, 187.38339567184448, 188.85245513916016, 190.32151460647583, 191.7738425731659, 193.22617053985596, 194.7330219745636, 196.23987340927124, 197.76336860656738, 199.28686380386353, 200.80635261535645, 202.32584142684937, 203.8550832271576, 205.38432502746582, 206.91830825805664, 208.45229148864746, 209.96017169952393, 211.4680519104004, 212.98570704460144, 214.5033621788025, 216.01825881004333, 217.53315544128418, 218.99563360214233, 220.4581117630005, 221.96869039535522, 223.47926902770996, 224.9897656440735, 226.500262260437, 228.02217149734497, 229.54408073425293, 231.07269096374512, 232.6013011932373, 234.1197485923767, 235.6381959915161, 237.14821028709412, 238.65822458267212, 240.16645765304565, 241.6746907234192, 243.17918848991394, 244.6836862564087, 246.21524477005005, 247.7468032836914, 249.32965278625488, 250.91250228881836, 252.42833638191223, 253.9441704750061, 255.4669051170349, 256.9896397590637, 258.5038146972656, 260.01798963546753, 261.52624320983887, 263.0344967842102, 264.5427963733673, 266.0510959625244, 267.55970525741577, 269.06831455230713, 270.5859205722809, 272.10352659225464, 273.7028341293335, 275.30214166641235, 276.8217487335205, 278.34135580062866, 279.8177013397217, 281.2940468788147, 282.7761125564575, 284.25817823410034, 285.7363336086273, 287.2144889831543, 288.70549416542053, 290.19649934768677, 291.6814422607422, 293.1663851737976, 294.6586880683899, 296.1509909629822, 297.6381936073303, 299.12539625167847, 300.60366678237915, 302.08193731307983, 304.4156129360199, 306.74928855895996]
[28.908333333333335, 28.908333333333335, 43.71666666666667, 43.71666666666667, 49.916666666666664, 49.916666666666664, 56.625, 56.625, 61.175, 61.175, 63.25, 63.25, 66.60833333333333, 66.60833333333333, 68.40833333333333, 68.40833333333333, 72.725, 72.725, 72.85833333333333, 72.85833333333333, 73.14166666666667, 73.14166666666667, 74.0, 74.0, 73.775, 73.775, 73.19166666666666, 73.19166666666666, 73.23333333333333, 73.23333333333333, 73.675, 73.675, 74.05, 74.05, 75.04166666666667, 75.04166666666667, 76.03333333333333, 76.03333333333333, 76.43333333333334, 76.43333333333334, 76.325, 76.325, 76.525, 76.525, 77.25833333333334, 77.25833333333334, 77.35833333333333, 77.35833333333333, 77.29166666666667, 77.29166666666667, 77.36666666666666, 77.36666666666666, 77.48333333333333, 77.48333333333333, 78.43333333333334, 78.43333333333334, 78.53333333333333, 78.53333333333333, 78.65833333333333, 78.65833333333333, 78.54166666666667, 78.54166666666667, 78.60833333333333, 78.60833333333333, 78.59166666666667, 78.59166666666667, 79.3, 79.3, 79.06666666666666, 79.06666666666666, 78.725, 78.725, 79.05, 79.05, 78.89166666666667, 78.89166666666667, 79.05, 79.05, 79.31666666666666, 79.31666666666666, 79.06666666666666, 79.06666666666666, 78.94166666666666, 78.94166666666666, 79.075, 79.075, 79.75833333333334, 79.75833333333334, 79.4, 79.4, 79.18333333333334, 79.18333333333334, 79.25, 79.25, 79.125, 79.125, 79.23333333333333, 79.23333333333333, 79.46666666666667, 79.46666666666667, 79.50833333333334, 79.50833333333334, 79.3, 79.3, 79.45, 79.45, 79.25, 79.25, 79.625, 79.625, 79.81666666666666, 79.81666666666666, 79.65, 79.65, 79.85, 79.85, 80.53333333333333, 80.53333333333333, 80.15, 80.15, 80.49166666666666, 80.49166666666666, 80.725, 80.725, 80.18333333333334, 80.18333333333334, 80.66666666666667, 80.66666666666667, 80.65, 80.65, 80.9, 80.9, 80.925, 80.925, 80.73333333333333, 80.73333333333333, 80.125, 80.125, 80.31666666666666, 80.31666666666666, 80.38333333333334, 80.38333333333334, 80.48333333333333, 80.48333333333333, 80.35, 80.35, 80.79166666666667, 80.79166666666667, 80.83333333333333, 80.83333333333333, 81.13333333333334, 81.13333333333334, 81.03333333333333, 81.03333333333333, 81.10833333333333, 81.10833333333333, 80.83333333333333, 80.83333333333333, 80.85833333333333, 80.85833333333333, 80.41666666666667, 80.41666666666667, 80.50833333333334, 80.50833333333334, 80.46666666666667, 80.46666666666667, 80.91666666666667, 80.91666666666667, 80.675, 80.675, 81.24166666666666, 81.24166666666666, 81.76666666666667, 81.76666666666667, 81.41666666666667, 81.41666666666667, 81.35, 81.35, 81.575, 81.575, 81.70833333333333, 81.70833333333333, 81.525, 81.525, 81.525, 81.525, 81.34166666666667, 81.34166666666667, 81.06666666666666, 81.06666666666666, 81.14166666666667, 81.14166666666667, 80.975, 80.975, 80.70833333333333, 80.70833333333333, 80.71666666666667, 80.71666666666667, 80.61666666666666, 80.61666666666666, 81.475, 81.475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.123, Test loss: 1.661, Test accuracy: 45.12
Average accuracy final 10 rounds: 44.653749999999995
6737.586879968643
[]
[33.475, 33.585, 32.9175, 31.1975, 30.67, 32.07, 30.58, 28.7975, 28.71, 27.765, 28.635, 28.915, 29.115, 29.2775, 29.94, 29.8325, 30.1225, 30.78, 30.955, 32.1525, 32.61, 33.8625, 33.91, 34.0225, 34.0275, 35.0025, 34.7975, 35.685, 37.2425, 36.3, 37.3325, 36.835, 36.965, 37.485, 37.825, 38.5825, 37.9925, 38.04, 38.9375, 39.1775, 39.9475, 40.555, 40.6825, 40.825, 41.0175, 40.475, 40.6125, 40.6875, 41.365, 41.285, 41.9975, 41.37, 40.8925, 41.4475, 42.295, 42.02, 43.2, 42.1325, 42.99, 42.1325, 42.835, 43.375, 43.4975, 42.82, 43.83, 43.24, 44.0125, 44.2025, 43.2975, 43.94, 44.075, 43.7975, 43.9175, 43.6575, 44.36, 43.805, 43.4575, 43.955, 44.1525, 44.465, 43.825, 44.495, 44.8175, 44.29, 44.23, 44.6075, 44.42, 44.82, 44.255, 44.1975, 43.5875, 44.64, 44.5125, 44.2575, 45.055, 44.43, 45.3825, 44.9625, 45.175, 44.535, 45.115]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.507, Test loss: 0.495, Test accuracy: 80.39
Average accuracy final 10 rounds: 80.67277777777777
Average global accuracy final 10 rounds: 80.67277777777777
2470.4859805107117
[]
[29.783333333333335, 40.35, 50.37222222222222, 51.227777777777774, 59.455555555555556, 59.71666666666667, 66.93888888888888, 67.17222222222222, 67.82222222222222, 67.22777777777777, 68.50555555555556, 71.02777777777777, 70.17222222222222, 72.0, 72.44444444444444, 73.97222222222223, 74.06666666666666, 74.46666666666667, 74.25555555555556, 75.12222222222222, 75.11111111111111, 75.35555555555555, 76.34444444444445, 76.68888888888888, 77.12777777777778, 76.78333333333333, 76.36666666666666, 75.96666666666667, 76.97222222222223, 77.01666666666667, 77.77222222222223, 77.87777777777778, 77.39444444444445, 77.8, 77.55555555555556, 78.14444444444445, 78.17777777777778, 77.80555555555556, 78.2611111111111, 78.41666666666667, 78.58333333333333, 78.52222222222223, 78.9, 78.31111111111112, 78.54444444444445, 78.56666666666666, 78.37777777777778, 79.40555555555555, 79.31666666666666, 79.22222222222223, 78.79444444444445, 78.91666666666667, 78.57222222222222, 79.07777777777778, 78.56666666666666, 78.94444444444444, 78.63888888888889, 78.64444444444445, 78.74444444444444, 78.55555555555556, 79.16666666666667, 78.70555555555555, 79.27777777777777, 79.03888888888889, 79.08333333333333, 79.33333333333333, 79.64444444444445, 79.71666666666667, 79.7611111111111, 79.59444444444445, 79.53888888888889, 79.61111111111111, 79.7388888888889, 79.92222222222222, 79.52222222222223, 79.45, 79.67222222222222, 79.86111111111111, 79.69444444444444, 79.88333333333334, 80.11111111111111, 81.15, 81.1, 80.75, 80.97777777777777, 80.75, 80.59444444444445, 80.51666666666667, 80.48333333333333, 80.34444444444445, 80.10555555555555, 80.33333333333333, 80.79444444444445, 80.84444444444445, 80.82222222222222, 80.63333333333334, 80.58333333333333, 80.63888888888889, 81.15555555555555, 80.81666666666666, 80.38888888888889]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.136, Test loss: 0.996, Test accuracy: 69.29
Average accuracy final 10 rounds: 63.56166666666667
4318.522317647934
[6.889089345932007, 13.379938125610352, 19.871089935302734, 26.291999101638794, 32.7468945980072, 39.08646273612976, 45.45636463165283, 51.832624197006226, 57.79362392425537, 63.6944363117218, 69.61005139350891, 75.51584267616272, 81.54111886024475, 87.81118893623352, 94.10471057891846, 100.40024328231812, 106.59066343307495, 112.45386481285095, 118.36789727210999, 124.20775365829468, 130.05597376823425, 136.0465109348297, 141.9956169128418, 147.8767364025116, 153.8174431324005, 159.69649410247803, 165.6033639907837, 171.60775351524353, 177.82947635650635, 184.1214063167572, 190.5224530696869, 196.82574152946472, 203.037442445755, 209.33034992218018, 215.6127827167511, 222.0885512828827, 228.42728519439697, 234.73612880706787, 241.5247461795807, 247.7915849685669, 253.67447090148926, 259.9855263233185, 265.89516949653625, 271.82261538505554, 277.7512798309326, 283.74527406692505, 289.67381262779236, 295.61896300315857, 301.5474660396576, 307.5898985862732, 313.7143437862396, 319.6212785243988, 325.79940390586853, 332.28428387641907, 338.83065128326416, 344.81204080581665, 350.66956305503845, 356.57138562202454, 362.5058150291443, 368.5477213859558, 374.655136346817, 380.66278553009033, 386.6537821292877, 392.6176154613495, 398.68842697143555, 404.74337244033813, 411.10759687423706, 417.58724880218506, 423.8863413333893, 430.24070835113525, 436.33792185783386, 442.5197448730469, 448.65787625312805, 454.7097351551056, 460.89683651924133, 467.38024950027466, 473.84700107574463, 480.20548939704895, 486.2364785671234, 492.2860474586487, 498.2568094730377, 504.22540855407715, 510.4714992046356, 516.459629535675, 522.5456929206848, 528.5228555202484, 534.6221752166748, 540.60724568367, 546.5691208839417, 552.5870201587677, 558.653520822525, 564.6638054847717, 570.6421239376068, 576.6745908260345, 582.6424517631531, 588.6880135536194, 595.2505388259888, 601.66251039505, 607.996682882309, 614.295129776001, 617.9010043144226]
[21.988888888888887, 23.005555555555556, 33.31111111111111, 36.98888888888889, 37.077777777777776, 31.122222222222224, 44.794444444444444, 37.18888888888889, 48.85, 45.73888888888889, 50.138888888888886, 45.46111111111111, 48.85, 49.083333333333336, 50.51111111111111, 47.083333333333336, 49.11666666666667, 53.111111111111114, 52.62222222222222, 49.705555555555556, 58.21666666666667, 59.09444444444444, 54.22222222222222, 57.85, 57.15555555555556, 62.36666666666667, 51.111111111111114, 51.21666666666667, 62.544444444444444, 63.044444444444444, 61.71111111111111, 56.27777777777778, 54.81111111111111, 54.80555555555556, 57.59444444444444, 58.65, 50.13333333333333, 49.78888888888889, 61.96666666666667, 62.15, 62.62777777777778, 56.8, 60.388888888888886, 62.05, 56.81111111111111, 54.98888888888889, 62.727777777777774, 54.15, 57.611111111111114, 56.666666666666664, 60.94444444444444, 63.75, 57.516666666666666, 61.88333333333333, 62.47222222222222, 54.227777777777774, 62.266666666666666, 66.04444444444445, 58.23888888888889, 61.66111111111111, 59.65555555555556, 62.13333333333333, 62.96666666666667, 58.86666666666667, 67.55555555555556, 61.69444444444444, 54.88333333333333, 59.27777777777778, 61.016666666666666, 61.44444444444444, 62.33888888888889, 62.8, 63.25555555555555, 61.05, 61.68888888888889, 63.97222222222222, 58.02777777777778, 59.83888888888889, 67.21111111111111, 63.28888888888889, 59.17777777777778, 59.30555555555556, 65.70555555555555, 58.48888888888889, 66.52777777777777, 66.81111111111112, 64.34444444444445, 66.81666666666666, 64.89444444444445, 54.56111111111111, 65.85555555555555, 64.61111111111111, 62.977777777777774, 66.35555555555555, 64.86111111111111, 58.84444444444444, 57.85, 65.29444444444445, 64.5111111111111, 64.45555555555555, 69.29444444444445]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 51070 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 56394 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.048, Test loss: 0.898, Test accuracy: 82.44
Final Round, Global train loss: 0.048, Global test loss: 2.497, Global test accuracy: 15.87
Average accuracy final 10 rounds: 81.95083333333332 

Average global accuracy final 10 rounds: 13.294166666666664 

2237.1260335445404
[1.9257347583770752, 3.8514695167541504, 5.399789571762085, 6.9481096267700195, 8.596626281738281, 10.245142936706543, 11.870348453521729, 13.495553970336914, 15.154125690460205, 16.812697410583496, 18.50194811820984, 20.19119882583618, 21.833808660507202, 23.476418495178223, 25.137561321258545, 26.798704147338867, 28.4065945148468, 30.014484882354736, 31.628566026687622, 33.24264717102051, 34.82912731170654, 36.41560745239258, 38.05254316329956, 39.68947887420654, 41.282389879226685, 42.875300884246826, 44.469475746154785, 46.063650608062744, 47.692304611206055, 49.320958614349365, 50.93431901931763, 52.54767942428589, 54.18925905227661, 55.830838680267334, 57.41578698158264, 59.00073528289795, 60.56119465827942, 62.12165403366089, 63.71519374847412, 65.30873346328735, 66.92211866378784, 68.53550386428833, 70.10703015327454, 71.67855644226074, 73.24349689483643, 74.80843734741211, 76.26027297973633, 77.71210861206055, 79.16790509223938, 80.62370157241821, 82.09410619735718, 83.56451082229614, 85.0282084941864, 86.49190616607666, 87.9832706451416, 89.47463512420654, 90.9139986038208, 92.35336208343506, 93.80578565597534, 95.25820922851562, 96.73054528236389, 98.20288133621216, 99.6239070892334, 101.04493284225464, 102.48804664611816, 103.93116044998169, 105.40120577812195, 106.8712511062622, 108.34481143951416, 109.81837177276611, 111.2409462928772, 112.66352081298828, 114.098797082901, 115.53407335281372, 117.01771569252014, 118.50135803222656, 119.96499133110046, 121.42862462997437, 122.8809266090393, 124.33322858810425, 125.78838658332825, 127.24354457855225, 128.70283031463623, 130.16211605072021, 131.63138961791992, 133.10066318511963, 134.5407977104187, 135.98093223571777, 137.44453024864197, 138.90812826156616, 140.50158286094666, 142.09503746032715, 143.54506850242615, 144.99509954452515, 146.46613121032715, 147.93716287612915, 149.5235676765442, 151.10997247695923, 152.6839861869812, 154.25799989700317, 155.8249912261963, 157.3919825553894, 158.9674825668335, 160.5429825782776, 162.13970375061035, 163.73642492294312, 165.3066601753235, 166.87689542770386, 172.60875129699707, 178.34060716629028, 179.7999665737152, 181.25932598114014, 182.74484872817993, 184.23037147521973, 185.66117405891418, 187.09197664260864, 188.5869722366333, 190.08196783065796, 191.56374979019165, 193.04553174972534, 194.5009582042694, 195.95638465881348, 197.3896028995514, 198.8228211402893, 200.27343320846558, 201.72404527664185, 203.19583821296692, 204.667631149292, 206.14026355743408, 207.61289596557617, 209.0271508693695, 210.44140577316284, 211.87623405456543, 213.31106233596802, 214.774311542511, 216.23756074905396, 217.719895362854, 219.20222997665405, 220.66086339950562, 222.11949682235718, 223.55100083351135, 224.98250484466553, 226.44027090072632, 227.8980369567871, 229.36554646492004, 230.83305597305298, 232.26692485809326, 233.70079374313354, 235.21958947181702, 236.7383852005005, 238.2053782939911, 239.6723713874817, 241.12073373794556, 242.56909608840942, 244.02102327346802, 245.4729504585266, 246.91773986816406, 248.3625292778015, 249.81997108459473, 251.27741289138794, 252.72008347511292, 254.1627540588379, 255.6162428855896, 257.0697317123413, 258.5297849178314, 259.98983812332153, 261.4280424118042, 262.86624670028687, 264.3212308883667, 265.77621507644653, 267.22916769981384, 268.68212032318115, 270.16068935394287, 271.6392583847046, 273.08868312835693, 274.5381078720093, 275.9757618904114, 277.4134159088135, 278.8659303188324, 280.3184447288513, 281.72966265678406, 283.1408805847168, 284.6721079349518, 286.20333528518677, 287.7275366783142, 289.25173807144165, 290.7106468677521, 292.1695556640625, 293.69757747650146, 295.22559928894043, 296.63483452796936, 298.0440697669983, 299.6558289527893, 301.2675881385803, 302.8032190799713, 304.3388500213623, 305.74564576148987, 307.15244150161743, 308.7129635810852, 310.273485660553, 312.9373400211334, 315.60119438171387]
[23.916666666666668, 23.916666666666668, 39.63333333333333, 39.63333333333333, 50.766666666666666, 50.766666666666666, 57.34166666666667, 57.34166666666667, 60.825, 60.825, 67.58333333333333, 67.58333333333333, 67.925, 67.925, 71.23333333333333, 71.23333333333333, 73.6, 73.6, 74.35833333333333, 74.35833333333333, 75.03333333333333, 75.03333333333333, 74.90833333333333, 74.90833333333333, 74.825, 74.825, 75.775, 75.775, 76.925, 76.925, 77.25, 77.25, 77.65, 77.65, 77.58333333333333, 77.58333333333333, 77.5, 77.5, 77.65, 77.65, 77.70833333333333, 77.70833333333333, 78.10833333333333, 78.10833333333333, 78.93333333333334, 78.93333333333334, 78.95833333333333, 78.95833333333333, 79.175, 79.175, 78.71666666666667, 78.71666666666667, 79.20833333333333, 79.20833333333333, 79.75, 79.75, 79.73333333333333, 79.73333333333333, 79.95, 79.95, 79.95833333333333, 79.95833333333333, 79.88333333333334, 79.88333333333334, 80.00833333333334, 80.00833333333334, 80.25833333333334, 80.25833333333334, 79.8, 79.8, 79.24166666666666, 79.24166666666666, 79.675, 79.675, 80.33333333333333, 80.33333333333333, 80.3, 80.3, 80.2, 80.2, 80.475, 80.475, 80.15, 80.15, 80.35, 80.35, 80.29166666666667, 80.29166666666667, 80.34166666666667, 80.34166666666667, 80.55833333333334, 80.55833333333334, 80.78333333333333, 80.78333333333333, 80.81666666666666, 80.81666666666666, 80.81666666666666, 80.81666666666666, 81.26666666666667, 81.26666666666667, 81.60833333333333, 81.60833333333333, 81.225, 81.225, 81.525, 81.525, 81.80833333333334, 81.80833333333334, 82.18333333333334, 82.18333333333334, 82.16666666666667, 82.16666666666667, 82.44166666666666, 82.44166666666666, 81.975, 81.975, 81.925, 81.925, 81.9, 81.9, 81.68333333333334, 81.68333333333334, 81.85, 81.85, 81.9, 81.9, 81.70833333333333, 81.70833333333333, 81.6, 81.6, 81.49166666666666, 81.49166666666666, 81.625, 81.625, 81.16666666666667, 81.16666666666667, 80.86666666666666, 80.86666666666666, 81.35, 81.35, 81.16666666666667, 81.16666666666667, 81.18333333333334, 81.18333333333334, 81.70833333333333, 81.70833333333333, 81.875, 81.875, 81.58333333333333, 81.58333333333333, 81.85, 81.85, 81.71666666666667, 81.71666666666667, 81.7, 81.7, 81.825, 81.825, 81.96666666666667, 81.96666666666667, 82.05, 82.05, 82.18333333333334, 82.18333333333334, 81.85, 81.85, 82.075, 82.075, 81.875, 81.875, 82.025, 82.025, 81.58333333333333, 81.58333333333333, 81.79166666666667, 81.79166666666667, 82.0, 82.0, 81.88333333333334, 81.88333333333334, 82.1, 82.1, 82.35833333333333, 82.35833333333333, 81.94166666666666, 81.94166666666666, 81.875, 81.875, 81.94166666666666, 81.94166666666666, 81.93333333333334, 81.93333333333334, 81.925, 81.925, 81.75, 81.75, 81.73333333333333, 81.73333333333333, 81.95, 81.95, 82.44166666666666, 82.44166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.317, Test loss: 1.191, Test accuracy: 71.97
Final Round, Global train loss: 0.317, Global test loss: 3.211, Global test accuracy: 14.22
Average accuracy final 10 rounds: 71.815 

Average global accuracy final 10 rounds: 15.577499999999999 

6652.343185901642
[5.206949472427368, 10.413898944854736, 15.399176359176636, 20.384453773498535, 25.558358907699585, 30.732264041900635, 35.741981506347656, 40.75169897079468, 45.61420035362244, 50.476701736450195, 55.3486909866333, 60.220680236816406, 65.18267607688904, 70.14467191696167, 75.07249855995178, 80.0003252029419, 84.88485598564148, 89.76938676834106, 94.5079493522644, 99.24651193618774, 104.01252675056458, 108.7785415649414, 113.77869629859924, 118.77885103225708, 123.60765719413757, 128.43646335601807, 133.2918872833252, 138.14731121063232, 142.97089672088623, 147.79448223114014, 152.56023454666138, 157.32598686218262, 162.1428792476654, 166.9597716331482, 171.86259150505066, 176.76541137695312, 181.65081024169922, 186.5362091064453, 191.42760586738586, 196.31900262832642, 201.39650535583496, 206.4740080833435, 211.48750758171082, 216.50100708007812, 221.71723747253418, 226.93346786499023, 232.04008865356445, 237.14670944213867, 242.1528754234314, 247.15904140472412, 252.14587473869324, 257.13270807266235, 262.2162537574768, 267.29979944229126, 272.3068935871124, 277.3139877319336, 282.3581783771515, 287.4023690223694, 292.05458974838257, 296.70681047439575, 301.54318714141846, 306.37956380844116, 311.47803711891174, 316.5765104293823, 321.4637339115143, 326.35095739364624, 331.10910177230835, 335.86724615097046, 340.62966561317444, 345.3920850753784, 350.16734790802, 354.9426107406616, 360.10403633117676, 365.2654619216919, 370.3884434700012, 375.51142501831055, 380.5595769882202, 385.6077289581299, 390.6027035713196, 395.5976781845093, 400.4103653430939, 405.22305250167847, 410.0272991657257, 414.83154582977295, 419.6309030056, 424.430260181427, 429.26396322250366, 434.0976662635803, 438.89133167266846, 443.6849970817566, 448.46669816970825, 453.2483992576599, 458.0424041748047, 462.83640909194946, 467.6045820713043, 472.3727550506592, 477.13354301452637, 481.89433097839355, 486.7198419570923, 491.545352935791, 496.3397674560547, 501.13418197631836, 505.9273157119751, 510.72044944763184, 515.5023968219757, 520.2843441963196, 525.0723929405212, 529.8604416847229, 534.6932964324951, 539.5261511802673, 544.3367826938629, 549.1474142074585, 553.9594223499298, 558.7714304924011, 563.5392892360687, 568.3071479797363, 573.0835537910461, 577.859959602356, 582.6853182315826, 587.5106768608093, 592.3050093650818, 597.0993418693542, 601.8904409408569, 606.6815400123596, 611.4745564460754, 616.2675728797913, 621.0552341938019, 625.8428955078125, 630.6419410705566, 635.4409866333008, 640.2486510276794, 645.0563154220581, 649.850670337677, 654.6450252532959, 659.4610025882721, 664.2769799232483, 669.091899394989, 673.9068188667297, 678.7089493274689, 683.511079788208, 688.294290304184, 693.0775008201599, 697.8669764995575, 702.6564521789551, 707.4688124656677, 712.2811727523804, 717.0843765735626, 721.8875803947449, 726.6951060295105, 731.5026316642761, 736.2509694099426, 740.9993071556091, 745.7878239154816, 750.576340675354, 755.3573040962219, 760.1382675170898, 764.9437754154205, 769.7492833137512, 774.5702655315399, 779.3912477493286, 784.1494977474213, 788.9077477455139, 793.6737501621246, 798.4397525787354, 803.213353395462, 807.9869542121887, 812.7662436962128, 817.5455331802368, 822.3316271305084, 827.11772108078, 831.8942642211914, 836.6708073616028, 841.4477517604828, 846.2246961593628, 850.9982707500458, 855.7718453407288, 860.5481142997742, 865.3243832588196, 870.087753534317, 874.8511238098145, 879.6254065036774, 884.3996891975403, 889.1655902862549, 893.9314913749695, 898.7296853065491, 903.5278792381287, 908.3284559249878, 913.1290326118469, 917.9316971302032, 922.7343616485596, 927.5432524681091, 932.3521432876587, 937.1363778114319, 941.9206123352051, 946.732218503952, 951.543824672699, 956.3102865219116, 961.0767483711243, 965.8872895240784, 970.6978306770325, 973.1100273132324, 975.5222239494324]
[17.48, 17.48, 25.455, 25.455, 28.515, 28.515, 30.835, 30.835, 36.8325, 36.8325, 39.8525, 39.8525, 44.36, 44.36, 47.215, 47.215, 50.4075, 50.4075, 51.355, 51.355, 52.6475, 52.6475, 53.175, 53.175, 53.445, 53.445, 54.2, 54.2, 57.78, 57.78, 59.165, 59.165, 59.9025, 59.9025, 60.4, 60.4, 61.07, 61.07, 62.48, 62.48, 63.495, 63.495, 64.29, 64.29, 64.36, 64.36, 65.4775, 65.4775, 65.775, 65.775, 66.1125, 66.1125, 66.355, 66.355, 66.39, 66.39, 66.5925, 66.5925, 66.735, 66.735, 67.0075, 67.0075, 67.33, 67.33, 67.375, 67.375, 67.485, 67.485, 67.7825, 67.7825, 67.955, 67.955, 68.1625, 68.1625, 68.0825, 68.0825, 68.255, 68.255, 68.5025, 68.5025, 68.4675, 68.4675, 68.4425, 68.4425, 68.7, 68.7, 68.62, 68.62, 68.65, 68.65, 68.89, 68.89, 69.0825, 69.0825, 69.4625, 69.4625, 69.33, 69.33, 69.63, 69.63, 69.6525, 69.6525, 69.8225, 69.8225, 69.9575, 69.9575, 70.0875, 70.0875, 70.0875, 70.0875, 70.07, 70.07, 69.9225, 69.9225, 70.13, 70.13, 70.2725, 70.2725, 70.3625, 70.3625, 70.48, 70.48, 70.5175, 70.5175, 70.5975, 70.5975, 70.43, 70.43, 70.3475, 70.3475, 70.625, 70.625, 70.5525, 70.5525, 70.58, 70.58, 70.7075, 70.7075, 70.6675, 70.6675, 70.7, 70.7, 70.8725, 70.8725, 70.41, 70.41, 70.4425, 70.4425, 70.57, 70.57, 70.91, 70.91, 71.2925, 71.2925, 71.265, 71.265, 71.3475, 71.3475, 71.1925, 71.1925, 71.34, 71.34, 71.2775, 71.2775, 71.2925, 71.2925, 71.52, 71.52, 71.4325, 71.4325, 71.47, 71.47, 71.47, 71.47, 71.5925, 71.5925, 71.7875, 71.7875, 71.835, 71.835, 71.565, 71.565, 71.54, 71.54, 71.94, 71.94, 71.955, 71.955, 71.805, 71.805, 71.96, 71.96, 71.9075, 71.9075, 71.875, 71.875, 71.785, 71.785, 71.8175, 71.8175, 71.965, 71.965]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.396, Test loss: 0.752, Test accuracy: 76.48
Average accuracy final 10 rounds: 75.8665 

4643.868995189667
[4.474827766418457, 8.949655532836914, 13.359313726425171, 17.768971920013428, 22.175304412841797, 26.581636905670166, 30.99447512626648, 35.40731334686279, 39.83250713348389, 44.25770092010498, 48.68129205703735, 53.10488319396973, 57.103360176086426, 61.101837158203125, 65.10451769828796, 69.1071982383728, 73.12974119186401, 77.15228414535522, 81.19462442398071, 85.2369647026062, 89.26997494697571, 93.30298519134521, 97.32497692108154, 101.34696865081787, 105.368479013443, 109.38998937606812, 113.40540146827698, 117.42081356048584, 121.4017174243927, 125.38262128829956, 129.3714144229889, 133.36020755767822, 137.36423254013062, 141.368257522583, 145.34458327293396, 149.3209090232849, 153.30506443977356, 157.2892198562622, 161.32786345481873, 165.36650705337524, 169.4114580154419, 173.45640897750854, 177.49431943893433, 181.5322299003601, 185.5340449810028, 189.5358600616455, 193.55797481536865, 197.5800895690918, 201.58593273162842, 205.59177589416504, 209.61758065223694, 213.64338541030884, 217.69126892089844, 221.73915243148804, 225.6753284931183, 229.61150455474854, 233.6131556034088, 237.6148066520691, 241.58916020393372, 245.56351375579834, 249.5188934803009, 253.47427320480347, 257.5002360343933, 261.52619886398315, 265.5297944545746, 269.533390045166, 273.5689232349396, 277.60445642471313, 281.6141710281372, 285.6238856315613, 289.65786385536194, 293.6918420791626, 297.6770360469818, 301.662230014801, 305.63224172592163, 309.60225343704224, 313.60370111465454, 317.60514879226685, 321.5339765548706, 325.46280431747437, 329.4073247909546, 333.3518452644348, 337.35739612579346, 341.3629469871521, 345.3761706352234, 349.3893942832947, 353.4025378227234, 357.4156813621521, 361.4119324684143, 365.4081835746765, 369.43982696533203, 373.47147035598755, 377.4737377166748, 381.47600507736206, 385.51124835014343, 389.5464916229248, 393.55568265914917, 397.56487369537354, 401.5316939353943, 405.49851417541504, 409.4829580783844, 413.46740198135376, 417.4132270812988, 421.3590521812439, 425.31868600845337, 429.27831983566284, 433.24209856987, 437.20587730407715, 441.18972539901733, 445.1735734939575, 449.187504529953, 453.2014355659485, 457.1992597579956, 461.1970839500427, 465.17859411239624, 469.16010427474976, 473.1302001476288, 477.1002960205078, 481.08807015419006, 485.0758442878723, 489.11235785484314, 493.14887142181396, 497.15084958076477, 501.1528277397156, 505.08910846710205, 509.0253891944885, 513.0045411586761, 516.9836931228638, 520.9791853427887, 524.9746775627136, 528.9393746852875, 532.9040718078613, 536.871821641922, 540.8395714759827, 544.8281192779541, 548.8166670799255, 552.8387115001678, 556.8607559204102, 560.8546805381775, 564.8486051559448, 568.8197290897369, 572.790853023529, 576.7674434185028, 580.7440338134766, 584.7594137191772, 588.7747936248779, 592.7629754543304, 596.751157283783, 600.7003111839294, 604.6494650840759, 608.6132826805115, 612.577100276947, 616.5616323947906, 620.5461645126343, 624.5409154891968, 628.5356664657593, 632.5474035739899, 636.5591406822205, 640.5500736236572, 644.541006565094, 648.5225894451141, 652.5041723251343, 656.4699625968933, 660.4357528686523, 664.4412322044373, 668.4467115402222, 672.4490838050842, 676.4514560699463, 680.3881494998932, 684.3248429298401, 688.2671473026276, 692.209451675415, 696.1949682235718, 700.1804847717285, 704.1418528556824, 708.1032209396362, 712.0499622821808, 715.9967036247253, 719.9636046886444, 723.9305057525635, 727.940761089325, 731.9510164260864, 735.950624704361, 739.9502329826355, 743.9485599994659, 747.9468870162964, 751.9269440174103, 755.9070010185242, 759.8712801933289, 763.8355593681335, 767.8097031116486, 771.7838468551636, 775.7821755409241, 779.7805042266846, 783.7376370429993, 787.694769859314, 791.6392605304718, 795.5837512016296, 799.5954008102417, 803.6070504188538, 805.5177428722382, 807.4284353256226]
[12.2575, 12.2575, 18.2225, 18.2225, 24.045, 24.045, 31.9725, 31.9725, 35.7525, 35.7525, 40.0275, 40.0275, 45.08, 45.08, 48.42, 48.42, 52.33, 52.33, 56.3675, 56.3675, 58.4975, 58.4975, 60.105, 60.105, 61.405, 61.405, 62.7475, 62.7475, 63.7925, 63.7925, 64.815, 64.815, 65.44, 65.44, 66.2125, 66.2125, 66.6075, 66.6075, 65.59, 65.59, 67.18, 67.18, 68.13, 68.13, 68.3375, 68.3375, 68.635, 68.635, 69.1775, 69.1775, 69.4875, 69.4875, 69.685, 69.685, 70.33, 70.33, 70.825, 70.825, 71.1625, 71.1625, 71.78, 71.78, 72.215, 72.215, 72.35, 72.35, 72.23, 72.23, 72.4625, 72.4625, 72.54, 72.54, 72.525, 72.525, 72.735, 72.735, 73.2525, 73.2525, 73.5175, 73.5175, 73.48, 73.48, 73.1675, 73.1675, 73.3325, 73.3325, 73.5225, 73.5225, 73.79, 73.79, 73.8375, 73.8375, 73.5475, 73.5475, 73.8425, 73.8425, 73.81, 73.81, 74.35, 74.35, 74.395, 74.395, 74.175, 74.175, 74.53, 74.53, 74.2725, 74.2725, 74.8975, 74.8975, 74.3825, 74.3825, 74.3875, 74.3875, 74.7075, 74.7075, 74.7925, 74.7925, 74.84, 74.84, 74.595, 74.595, 74.8, 74.8, 75.1, 75.1, 75.0875, 75.0875, 74.875, 74.875, 75.045, 75.045, 75.0725, 75.0725, 75.2475, 75.2475, 74.7725, 74.7725, 74.83, 74.83, 74.8525, 74.8525, 75.6475, 75.6475, 75.46, 75.46, 75.2275, 75.2275, 75.2575, 75.2575, 75.13, 75.13, 75.095, 75.095, 74.925, 74.925, 75.705, 75.705, 75.69, 75.69, 75.4375, 75.4375, 75.505, 75.505, 75.6925, 75.6925, 75.675, 75.675, 75.7625, 75.7625, 75.48, 75.48, 75.0825, 75.0825, 75.7275, 75.7275, 75.7125, 75.7125, 75.445, 75.445, 75.49, 75.49, 75.8025, 75.8025, 75.57, 75.57, 76.235, 76.235, 75.615, 75.615, 76.1825, 76.1825, 76.035, 76.035, 75.8625, 75.8625, 75.7075, 75.7075, 76.165, 76.165, 76.4825, 76.4825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.306, Test loss: 0.838, Test accuracy: 76.53
Average accuracy final 10 rounds: 76.674 

5222.922810554504
[4.85055947303772, 9.70111894607544, 14.493492603302002, 19.285866260528564, 24.08557438850403, 28.885282516479492, 33.682504653930664, 38.479726791381836, 43.2538058757782, 48.02788496017456, 52.83842206001282, 57.648959159851074, 62.49572730064392, 67.34249544143677, 72.1919276714325, 77.04135990142822, 81.89384460449219, 86.74632930755615, 91.57309651374817, 96.39986371994019, 101.2474627494812, 106.09506177902222, 110.92895030975342, 115.76283884048462, 120.55913519859314, 125.35543155670166, 130.17822909355164, 135.0010266304016, 139.83089995384216, 144.66077327728271, 149.49689650535583, 154.33301973342896, 159.14685463905334, 163.96068954467773, 168.77234387397766, 173.5839982032776, 178.7618625164032, 183.9397268295288, 189.0700991153717, 194.2004714012146, 199.35057830810547, 204.50068521499634, 209.68380331993103, 214.86692142486572, 220.02922439575195, 225.19152736663818, 230.32964992523193, 235.46777248382568, 240.64759135246277, 245.82741022109985, 251.01208329200745, 256.19675636291504, 261.34810757637024, 266.49945878982544, 271.75900077819824, 277.01854276657104, 282.1785228252411, 287.33850288391113, 292.6119112968445, 297.88531970977783, 302.96417570114136, 308.0430316925049, 313.193320274353, 318.3436088562012, 323.55235981941223, 328.7611107826233, 333.9827461242676, 339.20438146591187, 344.4475848674774, 349.69078826904297, 354.902161359787, 360.113534450531, 365.3066346645355, 370.49973487854004, 375.6707820892334, 380.84182929992676, 386.0768768787384, 391.31192445755005, 396.49197697639465, 401.67202949523926, 406.8422529697418, 412.0124764442444, 417.1897542476654, 422.3670320510864, 427.5206537246704, 432.6742753982544, 437.8195035457611, 442.9647316932678, 448.1005370616913, 453.23634243011475, 458.3868749141693, 463.5374073982239, 468.7676351070404, 473.99786281585693, 479.2307391166687, 484.46361541748047, 489.67778158187866, 494.89194774627686, 500.1318256855011, 505.37170362472534, 510.5938594341278, 515.8160152435303, 521.0212733745575, 526.2265315055847, 530.9281349182129, 535.6297383308411, 542.2421262264252, 548.8545141220093, 554.0256249904633, 559.1967358589172, 564.3699407577515, 569.5431456565857, 574.7374374866486, 579.9317293167114, 585.1722111701965, 590.4126930236816, 595.6588203907013, 600.904947757721, 606.0768754482269, 611.2488031387329, 616.455178976059, 621.661554813385, 626.9351286888123, 632.2087025642395, 637.3914811611176, 642.5742597579956, 647.7836797237396, 652.9930996894836, 658.2068748474121, 663.4206500053406, 668.6517758369446, 673.8829016685486, 679.0732634067535, 684.2636251449585, 689.4891362190247, 694.7146472930908, 699.9454584121704, 705.17626953125, 710.1610231399536, 715.1457767486572, 720.0401709079742, 724.9345650672913, 729.7930750846863, 734.6515851020813, 739.3918154239655, 744.1320457458496, 748.3938665390015, 752.6556873321533, 757.015186548233, 761.3746857643127, 765.672801733017, 769.9709177017212, 774.2768585681915, 778.5827994346619, 782.8658194541931, 787.1488394737244, 791.4540274143219, 795.7592153549194, 800.0679948329926, 804.3767743110657, 808.6728904247284, 812.9690065383911, 817.2772207260132, 821.5854349136353, 825.8576350212097, 830.1298351287842, 834.5772342681885, 839.0246334075928, 843.3268666267395, 847.6290998458862, 851.9701368808746, 856.311173915863, 860.6174132823944, 864.9236526489258, 869.2402498722076, 873.5568470954895, 877.8982427120209, 882.2396383285522, 886.517083644867, 890.7945289611816, 895.0877387523651, 899.3809485435486, 903.728524684906, 908.0761008262634, 912.392637014389, 916.7091732025146, 921.0167238712311, 925.3242745399475, 929.6455283164978, 933.9667820930481, 938.3885226249695, 942.8102631568909, 947.1631455421448, 951.5160279273987, 955.8602566719055, 960.2044854164124, 964.5209624767303, 968.8374395370483, 973.1879858970642, 977.5385322570801, 979.5178127288818, 981.4970932006836]
[12.33, 12.33, 16.18, 16.18, 19.2925, 19.2925, 24.4175, 24.4175, 31.2525, 31.2525, 37.98, 37.98, 45.1525, 45.1525, 48.8075, 48.8075, 52.2275, 52.2275, 56.53, 56.53, 60.225, 60.225, 63.4325, 63.4325, 65.56, 65.56, 66.7, 66.7, 67.63, 67.63, 68.3225, 68.3225, 69.5075, 69.5075, 70.5925, 70.5925, 71.1275, 71.1275, 71.0575, 71.0575, 71.4725, 71.4725, 71.8325, 71.8325, 72.205, 72.205, 73.1075, 73.1075, 73.265, 73.265, 73.1075, 73.1075, 73.6975, 73.6975, 73.845, 73.845, 74.145, 74.145, 73.9625, 73.9625, 74.34, 74.34, 74.4425, 74.4425, 74.39, 74.39, 74.725, 74.725, 74.5325, 74.5325, 74.5925, 74.5925, 75.035, 75.035, 75.09, 75.09, 75.3225, 75.3225, 75.1975, 75.1975, 75.44, 75.44, 75.59, 75.59, 75.115, 75.115, 74.8275, 74.8275, 75.3675, 75.3675, 75.795, 75.795, 75.455, 75.455, 75.51, 75.51, 75.855, 75.855, 75.76, 75.76, 75.8025, 75.8025, 76.0375, 76.0375, 76.06, 76.06, 75.935, 75.935, 76.0775, 76.0775, 75.5925, 75.5925, 75.98, 75.98, 76.1575, 76.1575, 76.1075, 76.1075, 75.825, 75.825, 76.1075, 76.1075, 76.055, 76.055, 75.8575, 75.8575, 75.9325, 75.9325, 75.8625, 75.8625, 76.1075, 76.1075, 76.1425, 76.1425, 76.35, 76.35, 76.1575, 76.1575, 76.12, 76.12, 76.3175, 76.3175, 76.09, 76.09, 76.2875, 76.2875, 75.885, 75.885, 75.85, 75.85, 76.2425, 76.2425, 76.2, 76.2, 76.3025, 76.3025, 76.43, 76.43, 76.5925, 76.5925, 76.49, 76.49, 76.555, 76.555, 76.3675, 76.3675, 76.7975, 76.7975, 76.495, 76.495, 76.32, 76.32, 76.5075, 76.5075, 76.4725, 76.4725, 76.47, 76.47, 76.3275, 76.3275, 76.7425, 76.7425, 76.5875, 76.5875, 76.9325, 76.9325, 76.98, 76.98, 76.085, 76.085, 76.7575, 76.7575, 76.835, 76.835, 76.55, 76.55, 76.75, 76.75, 76.52, 76.52, 76.5325, 76.5325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.038, Test loss: 0.991, Test accuracy: 81.71
Average accuracy final 10 rounds: 81.43999999999998 

1520.711226940155
[1.810398817062378, 3.620797634124756, 5.167631149291992, 6.7144646644592285, 8.247933626174927, 9.781402587890625, 11.316368103027344, 12.851333618164062, 14.40787386894226, 15.964414119720459, 17.51363778114319, 19.062861442565918, 20.5873966217041, 22.111931800842285, 23.638489484786987, 25.16504716873169, 26.720673084259033, 28.276298999786377, 29.817112922668457, 31.357926845550537, 32.884809732437134, 34.41169261932373, 35.961631298065186, 37.51156997680664, 39.07858943939209, 40.64560890197754, 42.187907457351685, 43.73020601272583, 45.25007081031799, 46.769935607910156, 48.05704069137573, 49.34414577484131, 50.62887120246887, 51.913596630096436, 53.20301580429077, 54.49243497848511, 55.759114027023315, 57.02579307556152, 58.30340003967285, 59.58100700378418, 60.873884201049805, 62.16676139831543, 63.468284606933594, 64.76980781555176, 66.04031372070312, 67.31081962585449, 68.57219815254211, 69.83357667922974, 71.10696291923523, 72.38034915924072, 73.672842502594, 74.96533584594727, 76.25343179702759, 77.54152774810791, 78.80396699905396, 80.06640625, 81.33394694328308, 82.60148763656616, 83.88506865501404, 85.16864967346191, 86.46220254898071, 87.75575542449951, 89.03436279296875, 90.31297016143799, 91.57812905311584, 92.8432879447937, 94.22225403785706, 95.60122013092041, 96.89339804649353, 98.18557596206665, 99.47727060317993, 100.76896524429321, 102.02677941322327, 103.28459358215332, 104.58702492713928, 105.88945627212524, 107.17020511627197, 108.4509539604187, 109.73267436027527, 111.01439476013184, 112.28320145606995, 113.55200815200806, 114.83208990097046, 116.11217164993286, 117.42656898498535, 118.74096632003784, 120.01163792610168, 121.28230953216553, 122.55594444274902, 123.82957935333252, 125.1112539768219, 126.39292860031128, 127.67582321166992, 128.95871782302856, 130.24372673034668, 131.5287356376648, 132.80320024490356, 134.07766485214233, 135.35140109062195, 136.62513732910156, 137.9179563522339, 139.2107753753662, 140.50214767456055, 141.79351997375488, 143.06622433662415, 144.3389286994934, 145.62029480934143, 146.90166091918945, 148.18385863304138, 149.4660563468933, 150.74232387542725, 152.01859140396118, 153.2913269996643, 154.56406259536743, 155.8464343547821, 157.12880611419678, 158.4340262413025, 159.7392463684082, 161.0270311832428, 162.3148159980774, 163.58083605766296, 164.84685611724854, 166.10794162750244, 167.36902713775635, 168.66140222549438, 169.95377731323242, 171.23901271820068, 172.52424812316895, 173.7988088130951, 175.07336950302124, 176.3299524784088, 177.5865354537964, 178.88224411010742, 180.17795276641846, 181.48476839065552, 182.79158401489258, 184.08096837997437, 185.37035274505615, 186.6390838623047, 187.90781497955322, 189.18977165222168, 190.47172832489014, 191.7607672214508, 193.04980611801147, 194.3319127559662, 195.6140193939209, 196.8827667236328, 198.15151405334473, 199.420170545578, 200.68882703781128, 201.97306632995605, 203.25730562210083, 204.5326235294342, 205.80794143676758, 207.07544469833374, 208.3429479598999, 209.6096739768982, 210.87639999389648, 212.1684286594391, 213.4604573249817, 214.75110030174255, 216.04174327850342, 217.30158162117004, 218.56141996383667, 219.8250789642334, 221.08873796463013, 222.3806653022766, 223.6725926399231, 224.96614003181458, 226.25968742370605, 227.53735423088074, 228.81502103805542, 230.08350563049316, 231.3519902229309, 232.61933159828186, 233.8866729736328, 235.18177366256714, 236.47687435150146, 237.7538001537323, 239.03072595596313, 240.314701795578, 241.59867763519287, 242.8773307800293, 244.15598392486572, 245.4374759197235, 246.7189679145813, 248.00069880485535, 249.2824296951294, 250.5725393295288, 251.86264896392822, 253.14734482765198, 254.43204069137573, 255.71014714241028, 256.9882535934448, 258.25642800331116, 259.5246024131775, 260.7944645881653, 262.0643267631531, 263.33837246894836, 264.61241817474365, 266.7022023200989, 268.7919864654541]
[23.283333333333335, 23.283333333333335, 35.5, 35.5, 39.05833333333333, 39.05833333333333, 41.541666666666664, 41.541666666666664, 42.391666666666666, 42.391666666666666, 54.916666666666664, 54.916666666666664, 57.8, 57.8, 65.89166666666667, 65.89166666666667, 67.74166666666666, 67.74166666666666, 69.59166666666667, 69.59166666666667, 71.2, 71.2, 71.29166666666667, 71.29166666666667, 72.09166666666667, 72.09166666666667, 74.20833333333333, 74.20833333333333, 75.49166666666666, 75.49166666666666, 75.34166666666667, 75.34166666666667, 76.075, 76.075, 76.56666666666666, 76.56666666666666, 76.86666666666666, 76.86666666666666, 76.31666666666666, 76.31666666666666, 77.4, 77.4, 77.125, 77.125, 78.26666666666667, 78.26666666666667, 77.51666666666667, 77.51666666666667, 77.54166666666667, 77.54166666666667, 78.41666666666667, 78.41666666666667, 78.63333333333334, 78.63333333333334, 78.975, 78.975, 77.98333333333333, 77.98333333333333, 78.2, 78.2, 78.78333333333333, 78.78333333333333, 79.15, 79.15, 79.03333333333333, 79.03333333333333, 79.41666666666667, 79.41666666666667, 79.1, 79.1, 79.0, 79.0, 79.40833333333333, 79.40833333333333, 79.3, 79.3, 79.55, 79.55, 79.61666666666666, 79.61666666666666, 79.64166666666667, 79.64166666666667, 80.03333333333333, 80.03333333333333, 80.4, 80.4, 80.625, 80.625, 80.58333333333333, 80.58333333333333, 80.375, 80.375, 80.43333333333334, 80.43333333333334, 79.68333333333334, 79.68333333333334, 80.01666666666667, 80.01666666666667, 79.99166666666666, 79.99166666666666, 80.28333333333333, 80.28333333333333, 80.49166666666666, 80.49166666666666, 80.68333333333334, 80.68333333333334, 80.75833333333334, 80.75833333333334, 80.4, 80.4, 80.325, 80.325, 80.075, 80.075, 79.85, 79.85, 80.03333333333333, 80.03333333333333, 80.33333333333333, 80.33333333333333, 80.81666666666666, 80.81666666666666, 81.00833333333334, 81.00833333333334, 81.15833333333333, 81.15833333333333, 80.73333333333333, 80.73333333333333, 81.09166666666667, 81.09166666666667, 80.88333333333334, 80.88333333333334, 80.63333333333334, 80.63333333333334, 80.775, 80.775, 81.15833333333333, 81.15833333333333, 81.30833333333334, 81.30833333333334, 81.1, 81.1, 81.21666666666667, 81.21666666666667, 80.71666666666667, 80.71666666666667, 81.11666666666666, 81.11666666666666, 81.45, 81.45, 81.49166666666666, 81.49166666666666, 81.8, 81.8, 81.75833333333334, 81.75833333333334, 81.575, 81.575, 81.20833333333333, 81.20833333333333, 81.04166666666667, 81.04166666666667, 81.35833333333333, 81.35833333333333, 81.46666666666667, 81.46666666666667, 81.44166666666666, 81.44166666666666, 81.00833333333334, 81.00833333333334, 81.06666666666666, 81.06666666666666, 81.275, 81.275, 81.25, 81.25, 81.46666666666667, 81.46666666666667, 81.30833333333334, 81.30833333333334, 81.15, 81.15, 81.21666666666667, 81.21666666666667, 81.3, 81.3, 81.21666666666667, 81.21666666666667, 81.45, 81.45, 81.46666666666667, 81.46666666666667, 81.66666666666667, 81.66666666666667, 81.59166666666667, 81.59166666666667, 81.64166666666667, 81.64166666666667, 81.7, 81.7, 81.70833333333333, 81.70833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.068, Test loss: 0.764, Test accuracy: 67.43
Average accuracy final 10 rounds: 68.16166666666666
2179.865911245346
[]
[21.55, 34.766666666666666, 38.99166666666667, 41.1, 46.65, 50.575, 56.516666666666666, 59.225, 59.75, 60.24166666666667, 59.3, 60.2, 61.25833333333333, 61.43333333333333, 60.44166666666667, 60.9, 60.475, 61.8, 61.4, 61.65, 62.59166666666667, 63.49166666666667, 64.60833333333333, 64.075, 64.475, 64.925, 64.65833333333333, 64.98333333333333, 65.86666666666666, 66.49166666666666, 65.88333333333334, 64.99166666666666, 66.15, 65.88333333333334, 66.35, 65.96666666666667, 65.4, 66.03333333333333, 66.28333333333333, 67.29166666666667, 66.43333333333334, 67.30833333333334, 66.89166666666667, 67.39166666666667, 66.825, 67.525, 67.95, 68.3, 69.275, 69.26666666666667, 67.79166666666667, 68.48333333333333, 69.075, 68.48333333333333, 69.84166666666667, 70.175, 70.88333333333334, 69.86666666666666, 69.6, 70.49166666666666, 69.66666666666667, 68.49166666666666, 68.50833333333334, 69.09166666666667, 69.34166666666667, 69.275, 68.93333333333334, 67.95, 68.73333333333333, 68.74166666666666, 68.09166666666667, 68.01666666666667, 68.275, 67.61666666666666, 67.375, 67.475, 67.01666666666667, 66.425, 65.88333333333334, 66.90833333333333, 67.19166666666666, 68.03333333333333, 68.71666666666667, 69.025, 68.66666666666667, 68.775, 68.65833333333333, 68.93333333333334, 67.89166666666667, 68.28333333333333, 68.53333333333333, 68.8, 69.20833333333333, 68.8, 67.61666666666666, 68.38333333333334, 68.4, 66.975, 67.35833333333333, 67.54166666666667, 67.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.575, Test loss: 0.564, Test accuracy: 77.26
Average accuracy final 10 rounds: 78.05
Average global accuracy final 10 rounds: 78.05
1434.424687385559
[]
[20.975, 33.45, 41.233333333333334, 40.666666666666664, 46.55, 53.95, 57.583333333333336, 61.75, 61.208333333333336, 63.475, 65.4, 65.73333333333333, 67.43333333333334, 68.98333333333333, 69.06666666666666, 68.85833333333333, 70.10833333333333, 70.46666666666667, 72.30833333333334, 72.175, 72.13333333333334, 71.66666666666667, 71.425, 72.34166666666667, 73.00833333333334, 72.84166666666667, 72.90833333333333, 72.41666666666667, 73.44166666666666, 73.30833333333334, 73.50833333333334, 73.9, 73.69166666666666, 73.94166666666666, 73.475, 74.13333333333334, 74.74166666666666, 74.93333333333334, 75.35833333333333, 75.3, 75.16666666666667, 74.475, 74.775, 74.06666666666666, 74.25833333333334, 74.95833333333333, 75.81666666666666, 75.85, 76.825, 76.59166666666667, 76.20833333333333, 76.26666666666667, 76.05833333333334, 75.59166666666667, 76.33333333333333, 76.7, 76.05833333333334, 76.26666666666667, 77.025, 76.76666666666667, 76.225, 76.53333333333333, 76.48333333333333, 76.74166666666666, 76.44166666666666, 76.175, 76.7, 76.69166666666666, 76.25833333333334, 77.06666666666666, 77.40833333333333, 76.88333333333334, 77.30833333333334, 77.55, 77.775, 77.35, 77.30833333333334, 76.43333333333334, 75.88333333333334, 76.61666666666666, 76.60833333333333, 76.625, 76.29166666666667, 76.65833333333333, 76.83333333333333, 77.125, 77.25833333333334, 77.425, 77.925, 77.88333333333334, 78.125, 77.96666666666667, 77.96666666666667, 77.94166666666666, 78.05833333333334, 78.1, 78.5, 78.25833333333334, 77.89166666666667, 77.69166666666666, 77.25833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

1616.7553980350494
[1.7957804203033447, 3.363603115081787, 4.934453725814819, 6.546543836593628, 8.12610936164856, 9.697511672973633, 11.257782220840454, 12.824114799499512, 14.385583877563477, 15.938385963439941, 17.48066735267639, 19.03608512878418, 20.600558757781982, 22.165849208831787, 23.73994016647339, 25.312582969665527, 26.872599363327026, 28.434995889663696, 29.99668598175049, 31.553125143051147, 33.105048179626465, 34.64519119262695, 36.182689905166626, 37.7285053730011, 39.269891023635864, 40.81485033035278, 42.364293336868286, 43.92036318778992, 45.46657586097717, 47.017234802246094, 48.57058668136597, 50.11940884590149, 51.66435980796814, 53.20307469367981, 54.743789196014404, 56.28882026672363, 57.8437876701355, 59.400208473205566, 60.953566551208496, 62.5029673576355, 64.05904459953308, 65.61532282829285, 67.15802311897278, 68.71868991851807, 70.26294827461243, 71.80817651748657, 73.35948944091797, 74.91479539871216, 76.46474313735962, 78.00398349761963, 79.55581974983215, 81.11576771736145, 82.65692353248596, 84.20361113548279, 85.74970626831055, 87.29812026023865, 88.85014462471008, 90.40292453765869, 91.95628643035889, 93.50530934333801, 95.04551100730896, 96.59565472602844, 97.96990370750427, 99.35114455223083, 100.72536182403564, 102.1026566028595, 103.47041964530945, 104.85583829879761, 106.2298903465271, 107.602365732193, 108.97315287590027, 110.34665751457214, 111.7121570110321, 113.08705186843872, 114.4638876914978, 115.83863735198975, 117.209956407547, 118.56518816947937, 119.94321179389954, 121.33657598495483, 122.71650290489197, 124.09860610961914, 125.48173880577087, 126.85370230674744, 128.23856139183044, 129.61962485313416, 131.0037546157837, 132.3939483165741, 133.78257989883423, 135.15962171554565, 136.5459394454956, 137.94224762916565, 139.3396475315094, 140.73281335830688, 142.1275191307068, 143.52408623695374, 144.90820360183716, 146.28906083106995, 147.6788775920868, 149.05578351020813, 151.36073422431946]
[10.041666666666666, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.187, Test loss: 2.502, Test accuracy: 25.09
Average accuracy final 10 rounds: 22.730833333333333
2488.1307775974274
[3.702800750732422, 7.1673338413238525, 10.650895833969116, 14.115853309631348, 17.544371128082275, 21.370955228805542, 25.189160585403442, 29.001147985458374, 32.4481635093689, 35.88465642929077, 39.35194110870361, 42.81688356399536, 46.257253885269165, 49.696146726608276, 53.11839008331299, 56.5478937625885, 59.98455238342285, 63.417855978012085, 66.86990976333618, 70.30725145339966, 73.74441456794739, 77.20366644859314, 80.73477625846863, 84.23179244995117, 87.68786597251892, 91.199223279953, 94.68405747413635, 98.16055679321289, 101.645681142807, 105.10876965522766, 108.61303329467773, 112.09662628173828, 115.59626126289368, 119.07920718193054, 122.56576013565063, 126.06611275672913, 129.51969361305237, 133.0023958683014, 136.49626970291138, 139.9585838317871, 143.4545295238495, 146.93131017684937, 150.45903182029724, 153.99552416801453, 157.53095626831055, 161.08278250694275, 164.55534029006958, 168.0325584411621, 171.55859541893005, 175.0676999092102, 178.57216930389404, 182.10721254348755, 185.6713593006134, 189.22002387046814, 192.75122475624084, 196.307630777359, 199.8367052078247, 203.4045271873474, 207.0002088546753, 210.5769419670105, 214.15132999420166, 217.74465131759644, 221.32312417030334, 224.9087154865265, 228.44298768043518, 231.9875409603119, 235.52738642692566, 239.05574750900269, 242.56309270858765, 246.08927655220032, 249.6479902267456, 253.2026767730713, 256.6884367465973, 260.16636967658997, 263.6634714603424, 267.1536509990692, 270.670622587204, 274.23228001594543, 277.7708010673523, 281.3532724380493, 284.86366748809814, 288.39115953445435, 291.90020275115967, 295.41754245758057, 298.9925603866577, 302.54923939704895, 306.11913800239563, 309.6151473522186, 313.1399793624878, 316.62854385375977, 320.1332564353943, 323.6452012062073, 327.1975257396698, 330.7178695201874, 334.235769033432, 337.78889417648315, 341.33141112327576, 344.84884428977966, 348.38776659965515, 351.9505305290222, 354.8839204311371]
[13.341666666666667, 19.433333333333334, 19.025, 22.75, 22.016666666666666, 21.866666666666667, 23.066666666666666, 19.45, 22.075, 21.341666666666665, 23.225, 22.0, 21.791666666666668, 20.791666666666668, 21.008333333333333, 21.708333333333332, 23.825, 22.2, 21.208333333333332, 22.383333333333333, 22.483333333333334, 22.75, 20.958333333333332, 19.166666666666668, 20.616666666666667, 20.641666666666666, 20.825, 18.191666666666666, 21.7, 22.975, 23.391666666666666, 21.191666666666666, 22.416666666666668, 24.1, 23.441666666666666, 23.233333333333334, 22.958333333333332, 24.341666666666665, 23.75, 20.966666666666665, 22.458333333333332, 22.55, 22.825, 24.025, 25.383333333333333, 23.333333333333332, 23.933333333333334, 24.158333333333335, 23.075, 24.925, 24.066666666666666, 21.508333333333333, 22.383333333333333, 23.091666666666665, 22.6, 24.325, 23.316666666666666, 24.758333333333333, 24.891666666666666, 23.491666666666667, 23.358333333333334, 23.641666666666666, 22.933333333333334, 23.608333333333334, 22.35, 22.458333333333332, 24.425, 24.333333333333332, 22.633333333333333, 23.541666666666668, 20.258333333333333, 23.441666666666666, 25.191666666666666, 23.2, 22.083333333333332, 24.675, 21.8, 22.008333333333333, 22.141666666666666, 23.166666666666668, 21.7, 24.166666666666668, 24.975, 22.416666666666668, 22.425, 22.958333333333332, 21.641666666666666, 23.508333333333333, 23.575, 22.35, 21.558333333333334, 22.291666666666668, 23.166666666666668, 18.958333333333332, 21.833333333333332, 24.441666666666666, 23.325, 23.483333333333334, 24.658333333333335, 23.591666666666665, 25.091666666666665]
python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.203, Test loss: 0.343, Test accuracy: 87.17
Average accuracy final 10 rounds: 86.73166666666668
1826.8115682601929
[2.2585396766662598, 4.5170793533325195, 6.383645057678223, 8.250210762023926, 10.131341218948364, 12.012471675872803, 13.867666006088257, 15.722860336303711, 17.568108081817627, 19.413355827331543, 21.258291959762573, 23.103228092193604, 24.91394305229187, 26.724658012390137, 28.5796902179718, 30.434722423553467, 32.26464533805847, 34.09456825256348, 35.913376331329346, 37.732184410095215, 39.56690955162048, 41.40163469314575, 43.233163356781006, 45.06469202041626, 46.87876772880554, 48.692843437194824, 50.50407791137695, 52.31531238555908, 54.092246294021606, 55.86918020248413, 57.63658261299133, 59.403985023498535, 61.216637134552, 63.02928924560547, 64.84057569503784, 66.65186214447021, 68.47453141212463, 70.29720067977905, 72.12236189842224, 73.94752311706543, 75.77142763137817, 77.59533214569092, 79.43795585632324, 81.28057956695557, 83.09888291358948, 84.91718626022339, 86.7376651763916, 88.55814409255981, 90.39913487434387, 92.24012565612793, 94.07523822784424, 95.91035079956055, 97.72667837142944, 99.54300594329834, 101.37609100341797, 103.2091760635376, 105.04620099067688, 106.88322591781616, 108.72498321533203, 110.5667405128479, 112.4014687538147, 114.2361969947815, 116.09161043167114, 117.94702386856079, 119.7767767906189, 121.606529712677, 123.41922736167908, 125.23192501068115, 127.06318426132202, 128.8944435119629, 130.72902250289917, 132.56360149383545, 134.40446281433105, 136.24532413482666, 138.08168363571167, 139.91804313659668, 141.7531373500824, 143.58823156356812, 145.44829845428467, 147.30836534500122, 149.1420612335205, 150.9757571220398, 152.79434776306152, 154.61293840408325, 156.44595313072205, 158.27896785736084, 160.10922837257385, 161.93948888778687, 163.76972222328186, 165.59995555877686, 167.42218923568726, 169.24442291259766, 171.07346105575562, 172.90249919891357, 174.7565610408783, 176.61062288284302, 178.48073935508728, 180.35085582733154, 182.19921159744263, 184.0475673675537, 185.882803440094, 187.71803951263428, 189.5660228729248, 191.41400623321533, 193.26479816436768, 195.11559009552002, 196.95751643180847, 198.79944276809692, 200.64291143417358, 202.48638010025024, 204.3194077014923, 206.15243530273438, 208.00253200531006, 209.85262870788574, 211.69718503952026, 213.54174137115479, 215.37623834609985, 217.21073532104492, 219.05089664459229, 220.89105796813965, 222.73709726333618, 224.58313655853271, 226.44400715827942, 228.30487775802612, 230.14429473876953, 231.98371171951294, 233.81215023994446, 235.64058876037598, 237.49261283874512, 239.34463691711426, 241.18881034851074, 243.03298377990723, 244.86916995048523, 246.70535612106323, 248.54045033454895, 250.37554454803467, 252.22761750221252, 254.07969045639038, 255.9073302745819, 257.73497009277344, 259.55916714668274, 261.38336420059204, 263.2222821712494, 265.06120014190674, 266.878865480423, 268.6965308189392, 270.5303108692169, 272.36409091949463, 274.20201683044434, 276.03994274139404, 277.88354110717773, 279.7271394729614, 281.5445399284363, 283.36194038391113, 285.2245280742645, 287.0871157646179, 288.92115235328674, 290.75518894195557, 292.59805560112, 294.4409222602844, 296.26549100875854, 298.09005975723267, 299.9067916870117, 301.72352361679077, 303.53878569602966, 305.35404777526855, 307.1679615974426, 308.9818754196167, 310.7987711429596, 312.6156668663025, 314.43528270721436, 316.2548985481262, 318.06826853752136, 319.8816385269165, 321.7033395767212, 323.5250406265259, 325.33474564552307, 327.14445066452026, 328.9660692214966, 330.7876877784729, 332.6043176651001, 334.4209475517273, 336.2217321395874, 338.0225167274475, 339.85211753845215, 341.6817183494568, 343.502810716629, 345.32390308380127, 347.13706040382385, 348.95021772384644, 350.7798306941986, 352.6094436645508, 354.42859983444214, 356.2477560043335, 358.0743820667267, 359.9010081291199, 361.72971296310425, 363.5584177970886, 365.380074262619, 367.2017307281494, 369.44371128082275, 371.6856918334961]
[16.016666666666666, 16.016666666666666, 32.21666666666667, 32.21666666666667, 40.90833333333333, 40.90833333333333, 45.675, 45.675, 52.975, 52.975, 54.4, 54.4, 56.71666666666667, 56.71666666666667, 62.733333333333334, 62.733333333333334, 67.84166666666667, 67.84166666666667, 68.625, 68.625, 69.6, 69.6, 70.475, 70.475, 73.54166666666667, 73.54166666666667, 74.74166666666666, 74.74166666666666, 76.0, 76.0, 76.66666666666667, 76.66666666666667, 76.56666666666666, 76.56666666666666, 77.4, 77.4, 77.45, 77.45, 77.9, 77.9, 78.725, 78.725, 79.55, 79.55, 79.71666666666667, 79.71666666666667, 79.24166666666666, 79.24166666666666, 79.81666666666666, 79.81666666666666, 79.89166666666667, 79.89166666666667, 80.43333333333334, 80.43333333333334, 80.54166666666667, 80.54166666666667, 80.825, 80.825, 80.38333333333334, 80.38333333333334, 81.03333333333333, 81.03333333333333, 81.525, 81.525, 81.60833333333333, 81.60833333333333, 82.2, 82.2, 82.35, 82.35, 82.10833333333333, 82.10833333333333, 82.35, 82.35, 82.48333333333333, 82.48333333333333, 82.55833333333334, 82.55833333333334, 82.44166666666666, 82.44166666666666, 83.1, 83.1, 83.45833333333333, 83.45833333333333, 83.125, 83.125, 83.85833333333333, 83.85833333333333, 83.90833333333333, 83.90833333333333, 83.6, 83.6, 83.68333333333334, 83.68333333333334, 83.875, 83.875, 84.375, 84.375, 84.325, 84.325, 84.21666666666667, 84.21666666666667, 84.125, 84.125, 84.04166666666667, 84.04166666666667, 84.11666666666666, 84.11666666666666, 84.66666666666667, 84.66666666666667, 84.575, 84.575, 84.36666666666666, 84.36666666666666, 83.75833333333334, 83.75833333333334, 84.49166666666666, 84.49166666666666, 84.48333333333333, 84.48333333333333, 84.75, 84.75, 85.19166666666666, 85.19166666666666, 84.95833333333333, 84.95833333333333, 85.16666666666667, 85.16666666666667, 85.24166666666666, 85.24166666666666, 85.43333333333334, 85.43333333333334, 84.675, 84.675, 85.70833333333333, 85.70833333333333, 85.16666666666667, 85.16666666666667, 85.18333333333334, 85.18333333333334, 85.625, 85.625, 85.14166666666667, 85.14166666666667, 85.61666666666666, 85.61666666666666, 86.1, 86.1, 85.625, 85.625, 86.08333333333333, 86.08333333333333, 86.33333333333333, 86.33333333333333, 86.05833333333334, 86.05833333333334, 85.95833333333333, 85.95833333333333, 86.4, 86.4, 86.10833333333333, 86.10833333333333, 86.38333333333334, 86.38333333333334, 86.21666666666667, 86.21666666666667, 85.925, 85.925, 86.55, 86.55, 86.30833333333334, 86.30833333333334, 85.95833333333333, 85.95833333333333, 86.3, 86.3, 85.75, 85.75, 86.20833333333333, 86.20833333333333, 86.425, 86.425, 86.575, 86.575, 86.01666666666667, 86.01666666666667, 86.775, 86.775, 86.96666666666667, 86.96666666666667, 87.04166666666667, 87.04166666666667, 86.45833333333333, 86.45833333333333, 86.79166666666667, 86.79166666666667, 87.125, 87.125, 87.14166666666667, 87.14166666666667, 87.175, 87.175]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.569, Test loss: 1.635, Test accuracy: 82.81
Final Round, Global train loss: 1.569, Global test loss: 1.624, Global test accuracy: 83.87
Average accuracy final 10 rounds: 82.83824999999999 

Average global accuracy final 10 rounds: 83.8545 

5128.283280849457
[3.7181150913238525, 7.436230182647705, 10.977999687194824, 14.519769191741943, 18.094101905822754, 21.668434619903564, 25.183805227279663, 28.69917583465576, 32.28873586654663, 35.8782958984375, 39.40274786949158, 42.927199840545654, 46.37346291542053, 49.81972599029541, 53.38289952278137, 56.946073055267334, 60.47951674461365, 64.01296043395996, 67.60303044319153, 71.1931004524231, 74.92000675201416, 78.64691305160522, 82.28367042541504, 85.92042779922485, 89.46422481536865, 93.00802183151245, 96.08239006996155, 99.15675830841064, 102.26866579055786, 105.38057327270508, 108.47283267974854, 111.56509208679199, 114.6128671169281, 117.66064214706421, 120.82632803916931, 123.99201393127441, 127.15467548370361, 130.3173370361328, 133.29061341285706, 136.2638897895813, 139.1533169746399, 142.0427441596985, 145.03592228889465, 148.02910041809082, 150.98916602134705, 153.94923162460327, 156.886004447937, 159.82277727127075, 162.7527244091034, 165.68267154693604, 168.59157466888428, 171.50047779083252, 174.38895535469055, 177.27743291854858, 180.18263173103333, 183.08783054351807, 186.00352668762207, 188.91922283172607, 191.8543884754181, 194.7895541191101, 197.7096564769745, 200.62975883483887, 203.55509996414185, 206.48044109344482, 209.37005352973938, 212.25966596603394, 215.1530909538269, 218.04651594161987, 220.95928812026978, 223.87206029891968, 226.80947875976562, 229.74689722061157, 232.7947278022766, 235.84255838394165, 238.88722491264343, 241.93189144134521, 244.92565321922302, 247.91941499710083, 250.90645694732666, 253.8934988975525, 256.8913805484772, 259.88926219940186, 262.89466762542725, 265.90007305145264, 268.9222557544708, 271.944438457489, 274.8971083164215, 277.849778175354, 280.8033695220947, 283.75696086883545, 286.6512405872345, 289.54552030563354, 292.4418203830719, 295.33812046051025, 298.25298285484314, 301.167845249176, 304.10470366477966, 307.0415620803833, 310.03222823143005, 313.0228943824768, 315.973650932312, 318.9244074821472, 321.93254137039185, 324.9406752586365, 327.8877305984497, 330.83478593826294, 333.8164336681366, 336.79808139801025, 339.7386965751648, 342.67931175231934, 345.6760470867157, 348.67278242111206, 351.62199330329895, 354.57120418548584, 357.4320101737976, 360.2928161621094, 363.19631123542786, 366.09980630874634, 368.99814891815186, 371.8964915275574, 374.84274101257324, 377.7889904975891, 380.73648500442505, 383.683979511261, 386.7233898639679, 389.7628002166748, 392.7475163936615, 395.7322325706482, 398.7899615764618, 401.8476905822754, 404.7803418636322, 407.712993144989, 410.661493062973, 413.60999298095703, 416.562908411026, 419.51582384109497, 422.5420620441437, 425.5683002471924, 428.5717890262604, 431.57527780532837, 434.68694519996643, 437.7986125946045, 440.8445565700531, 443.8905005455017, 446.9338684082031, 449.97723627090454, 453.0345959663391, 456.0919556617737, 459.2335367202759, 462.3751177787781, 465.52579402923584, 468.6764702796936, 471.7769033908844, 474.8773365020752, 477.8677353858948, 480.85813426971436, 483.8186933994293, 486.7792525291443, 489.74434542655945, 492.7094383239746, 495.71014380455017, 498.71084928512573, 501.73432302474976, 504.7577967643738, 507.81712532043457, 510.87645387649536, 513.9371960163116, 516.9979381561279, 520.034793138504, 523.0716481208801, 526.1151752471924, 529.1587023735046, 532.3095026016235, 535.4603028297424, 538.6152992248535, 541.7702956199646, 544.9134206771851, 548.0565457344055, 551.1130766868591, 554.1696076393127, 557.1112158298492, 560.0528240203857, 562.881189584732, 565.7095551490784, 568.6180477142334, 571.5265402793884, 574.4895348548889, 577.4525294303894, 580.516428232193, 583.5803270339966, 586.6702740192413, 589.7602210044861, 592.748583316803, 595.7369456291199, 598.6441912651062, 601.5514369010925, 604.506382226944, 607.4613275527954, 610.445366859436, 613.4294061660767, 614.9254283905029, 616.4214506149292]
[36.67, 36.67, 65.56, 65.56, 73.2125, 73.2125, 74.3325, 74.3325, 75.0375, 75.0375, 77.0475, 77.0475, 77.4025, 77.4025, 77.54, 77.54, 77.6025, 77.6025, 77.75, 77.75, 78.48, 78.48, 78.55, 78.55, 80.7625, 80.7625, 80.8225, 80.8225, 81.12, 81.12, 81.13, 81.13, 81.18, 81.18, 81.255, 81.255, 81.46, 81.46, 81.4375, 81.4375, 81.4325, 81.4325, 81.865, 81.865, 81.865, 81.865, 81.87, 81.87, 81.915, 81.915, 81.9125, 81.9125, 81.9975, 81.9975, 81.9925, 81.9925, 82.3125, 82.3125, 82.3025, 82.3025, 82.3075, 82.3075, 82.3225, 82.3225, 82.4875, 82.4875, 82.52, 82.52, 82.53, 82.53, 82.5275, 82.5275, 82.555, 82.555, 82.615, 82.615, 82.6075, 82.6075, 82.6, 82.6, 82.58, 82.58, 82.5575, 82.5575, 82.5875, 82.5875, 82.5925, 82.5925, 82.59, 82.59, 82.61, 82.61, 82.655, 82.655, 82.6475, 82.6475, 82.68, 82.68, 82.65, 82.65, 82.6375, 82.6375, 82.675, 82.675, 82.67, 82.67, 82.685, 82.685, 82.71, 82.71, 82.705, 82.705, 82.7075, 82.7075, 82.735, 82.735, 82.715, 82.715, 82.7075, 82.7075, 82.715, 82.715, 82.715, 82.715, 82.715, 82.715, 82.7275, 82.7275, 82.7425, 82.7425, 82.7275, 82.7275, 82.735, 82.735, 82.75, 82.75, 82.7425, 82.7425, 82.7175, 82.7175, 82.7325, 82.7325, 82.74, 82.74, 82.77, 82.77, 82.7575, 82.7575, 82.765, 82.765, 82.75, 82.75, 82.795, 82.795, 82.7875, 82.7875, 82.815, 82.815, 82.8225, 82.8225, 82.8225, 82.8225, 82.8325, 82.8325, 82.815, 82.815, 82.83, 82.83, 82.815, 82.815, 82.8175, 82.8175, 82.84, 82.84, 82.8125, 82.8125, 82.8025, 82.8025, 82.8175, 82.8175, 82.835, 82.835, 82.8375, 82.8375, 82.845, 82.845, 82.8625, 82.8625, 82.85, 82.85, 82.835, 82.835, 82.825, 82.825, 82.85, 82.85, 82.8275, 82.8275, 82.815, 82.815, 82.8125, 82.8125]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.62
Final Round, Global train loss: 1.469, Global test loss: 1.496, Global test accuracy: 96.76
Average accuracy final 10 rounds: 96.61975000000001 

Average global accuracy final 10 rounds: 96.78474999999999 

5354.560883283615
[3.5008339881896973, 7.0016679763793945, 10.412813425064087, 13.82395887374878, 17.283701419830322, 20.743443965911865, 24.328362703323364, 27.913281440734863, 31.14948058128357, 34.385679721832275, 37.97315716743469, 41.56063461303711, 45.167555809020996, 48.77447700500488, 52.30479454994202, 55.83511209487915, 59.25495409965515, 62.67479610443115, 66.1460223197937, 69.61724853515625, 73.12502861022949, 76.63280868530273, 80.22500324249268, 83.81719779968262, 87.38355302810669, 90.94990825653076, 94.49768042564392, 98.04545259475708, 101.61843466758728, 105.19141674041748, 108.79081130027771, 112.39020586013794, 115.97012996673584, 119.55005407333374, 122.98296976089478, 126.41588544845581, 129.6868507862091, 132.9578161239624, 136.4694013595581, 139.9809865951538, 143.5393843650818, 147.09778213500977, 150.61608171463013, 154.1343812942505, 157.63135075569153, 161.12832021713257, 164.68414568901062, 168.23997116088867, 171.78532338142395, 175.33067560195923, 178.82609915733337, 182.32152271270752, 185.6961064338684, 189.0706901550293, 192.46092891693115, 195.851167678833, 199.31975984573364, 202.78835201263428, 206.2990210056305, 209.8096899986267, 213.3500518798828, 216.89041376113892, 220.40821647644043, 223.92601919174194, 227.43214678764343, 230.93827438354492, 234.5128173828125, 238.08736038208008, 241.62598943710327, 245.16461849212646, 248.69336533546448, 252.2221121788025, 255.59779977798462, 258.97348737716675, 262.43833684921265, 265.90318632125854, 269.4254958629608, 272.9478054046631, 276.5328321456909, 280.11785888671875, 283.6824417114258, 287.2470245361328, 290.7050635814667, 294.16310262680054, 297.6222252845764, 301.0813479423523, 304.5642759799957, 308.04720401763916, 311.49316334724426, 314.93912267684937, 318.44459199905396, 321.95006132125854, 325.4119553565979, 328.87384939193726, 332.400830745697, 335.9278120994568, 339.5812015533447, 343.23459100723267, 346.87479734420776, 350.51500368118286, 354.1167097091675, 357.7184157371521, 361.2245066165924, 364.7305974960327, 368.2139542102814, 371.69731092453003, 375.1585605144501, 378.6198101043701, 382.16866421699524, 385.71751832962036, 389.2553400993347, 392.7931618690491, 396.2649760246277, 399.7367901802063, 403.31472420692444, 406.8926582336426, 410.4324429035187, 413.9722275733948, 417.5040924549103, 421.0359573364258, 424.51775097846985, 427.9995446205139, 431.4662272930145, 434.93290996551514, 438.41764974594116, 441.9023895263672, 445.4406991004944, 448.9790086746216, 452.57916021347046, 456.17931175231934, 459.76827335357666, 463.357234954834, 466.9008162021637, 470.4443974494934, 473.9389750957489, 477.4335527420044, 480.93746995925903, 484.4413871765137, 487.9930028915405, 491.5446186065674, 495.0784909725189, 498.61236333847046, 502.1659846305847, 505.719605922699, 509.2662556171417, 512.8129053115845, 516.3498485088348, 519.8867917060852, 523.3421049118042, 526.7974181175232, 530.2535717487335, 533.7097253799438, 537.2359735965729, 540.7622218132019, 544.2885818481445, 547.8149418830872, 551.3233485221863, 554.8317551612854, 558.334748506546, 561.8377418518066, 565.3072893619537, 568.7768368721008, 572.2313084602356, 575.6857800483704, 579.2066020965576, 582.7274241447449, 586.2965993881226, 589.8657746315002, 593.4016563892365, 596.9375381469727, 600.0001397132874, 603.062741279602, 606.0157046318054, 608.9686679840088, 612.097772359848, 615.2268767356873, 618.3549697399139, 621.4830627441406, 624.6458878517151, 627.8087129592896, 631.0021600723267, 634.1956071853638, 637.2120006084442, 640.2283940315247, 643.1879603862762, 646.1475267410278, 649.3398802280426, 652.5322337150574, 655.7355213165283, 658.9388089179993, 662.0571999549866, 665.1755909919739, 668.1320102214813, 671.0884294509888, 673.9944620132446, 676.9004945755005, 679.9741809368134, 683.0478672981262, 686.1891684532166, 689.3304696083069, 690.9011540412903, 692.4718384742737]
[45.82, 45.82, 70.9325, 70.9325, 77.475, 77.475, 79.9675, 79.9675, 80.4625, 80.4625, 83.4775, 83.4775, 83.6775, 83.6775, 83.8575, 83.8575, 84.2175, 84.2175, 84.4625, 84.4625, 86.3875, 86.3875, 88.3725, 88.3725, 89.6475, 89.6475, 90.6, 90.6, 90.895, 90.895, 91.06, 91.06, 92.5025, 92.5025, 92.81, 92.81, 92.925, 92.925, 93.0175, 93.0175, 93.47, 93.47, 93.645, 93.645, 93.8525, 93.8525, 93.97, 93.97, 94.06, 94.06, 94.285, 94.285, 94.3625, 94.3625, 94.37, 94.37, 95.0375, 95.0375, 95.1775, 95.1775, 95.2325, 95.2325, 95.2725, 95.2725, 95.335, 95.335, 95.4375, 95.4375, 95.5375, 95.5375, 95.585, 95.585, 95.605, 95.605, 95.6725, 95.6725, 95.67, 95.67, 95.6775, 95.6775, 95.7475, 95.7475, 95.7875, 95.7875, 95.85, 95.85, 95.8825, 95.8825, 95.9, 95.9, 95.93, 95.93, 95.9675, 95.9675, 95.98, 95.98, 95.98, 95.98, 96.005, 96.005, 96.045, 96.045, 96.0825, 96.0825, 96.075, 96.075, 96.105, 96.105, 96.145, 96.145, 96.1975, 96.1975, 96.2175, 96.2175, 96.2375, 96.2375, 96.2375, 96.2375, 96.255, 96.255, 96.2575, 96.2575, 96.25, 96.25, 96.2775, 96.2775, 96.3275, 96.3275, 96.33, 96.33, 96.3375, 96.3375, 96.325, 96.325, 96.375, 96.375, 96.415, 96.415, 96.4475, 96.4475, 96.4625, 96.4625, 96.465, 96.465, 96.485, 96.485, 96.4925, 96.4925, 96.5225, 96.5225, 96.56, 96.56, 96.5625, 96.5625, 96.58, 96.58, 96.56, 96.56, 96.6, 96.6, 96.5825, 96.5825, 96.5575, 96.5575, 96.5625, 96.5625, 96.5725, 96.5725, 96.59, 96.59, 96.58, 96.58, 96.5575, 96.5575, 96.5375, 96.5375, 96.5725, 96.5725, 96.5775, 96.5775, 96.6, 96.6, 96.59, 96.59, 96.5975, 96.5975, 96.5975, 96.5975, 96.6325, 96.6325, 96.6425, 96.6425, 96.65, 96.65, 96.6325, 96.6325, 96.6175, 96.6175, 96.6375, 96.6375, 96.625, 96.625]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.473, Test loss: 1.504, Test accuracy: 95.84
Average accuracy final 10 rounds: 95.77475 

4080.53817486763
[3.5436933040618896, 7.087386608123779, 10.440189123153687, 13.792991638183594, 17.1362407207489, 20.47948980331421, 23.82097625732422, 27.16246271133423, 30.529860258102417, 33.897257804870605, 37.23268103599548, 40.56810426712036, 43.76357173919678, 46.95903921127319, 50.325594902038574, 53.692150592803955, 57.01488542556763, 60.3376202583313, 63.656582832336426, 66.97554540634155, 70.37677717208862, 73.7780089378357, 77.17382550239563, 80.56964206695557, 83.75983500480652, 86.95002794265747, 90.2533028125763, 93.55657768249512, 96.83024501800537, 100.10391235351562, 103.39430737495422, 106.68470239639282, 110.10418605804443, 113.52366971969604, 116.92540264129639, 120.32713556289673, 123.72880530357361, 127.13047504425049, 130.83212995529175, 134.533784866333, 137.89340686798096, 141.2530288696289, 144.58417749404907, 147.91532611846924, 151.29340720176697, 154.6714882850647, 157.89494919776917, 161.11841011047363, 164.53392934799194, 167.94944858551025, 171.34181880950928, 174.7341890335083, 178.0432004928589, 181.35221195220947, 184.51421236991882, 187.67621278762817, 191.05162262916565, 194.42703247070312, 197.63407397270203, 200.84111547470093, 204.15728974342346, 207.473464012146, 210.80966138839722, 214.14585876464844, 217.34055471420288, 220.53525066375732, 223.93181467056274, 227.32837867736816, 230.7351520061493, 234.14192533493042, 237.46690154075623, 240.79187774658203, 244.2737843990326, 247.75569105148315, 251.22439217567444, 254.69309329986572, 257.99252438545227, 261.2919554710388, 264.6553256511688, 268.0186958312988, 271.3389537334442, 274.6592116355896, 277.97889471054077, 281.29857778549194, 284.7529866695404, 288.20739555358887, 291.6759305000305, 295.14446544647217, 298.5122170448303, 301.8799686431885, 305.3331410884857, 308.78631353378296, 312.240713596344, 315.69511365890503, 319.05710458755493, 322.41909551620483, 325.8161129951477, 329.2131304740906, 332.5633313655853, 335.9135322570801, 339.22990798950195, 342.5462837219238, 345.8285219669342, 349.1107602119446, 352.4607939720154, 355.8108277320862, 359.03914642333984, 362.2674651145935, 365.72641348838806, 369.1853618621826, 372.6028401851654, 376.0203185081482, 379.227756023407, 382.43519353866577, 385.8123035430908, 389.18941354751587, 392.2593584060669, 395.3293032646179, 398.18514037132263, 401.04097747802734, 404.0670254230499, 407.0930733680725, 410.10745429992676, 413.121835231781, 416.1606559753418, 419.1994767189026, 422.2646095752716, 425.3297424316406, 428.41131591796875, 431.4928894042969, 434.3546621799469, 437.2164349555969, 440.21711921691895, 443.21780347824097, 446.1570100784302, 449.0962166786194, 452.27299547195435, 455.4497742652893, 458.5999126434326, 461.7500510215759, 464.65719270706177, 467.5643343925476, 470.5516526699066, 473.5389709472656, 476.60002756118774, 479.66108417510986, 482.5771265029907, 485.4931688308716, 488.5284321308136, 491.5636954307556, 494.6086754798889, 497.6536555290222, 500.620646238327, 503.58763694763184, 506.5511620044708, 509.5146870613098, 512.4027011394501, 515.2907152175903, 518.2113561630249, 521.1319971084595, 524.2038140296936, 527.2756309509277, 530.2502989768982, 533.2249670028687, 536.1133382320404, 539.0017094612122, 542.0308299064636, 545.0599503517151, 548.0369277000427, 551.0139050483704, 553.9788484573364, 556.9437918663025, 559.9554963111877, 562.967200756073, 565.9743139743805, 568.981427192688, 572.1659762859344, 575.3505253791809, 578.2098324298859, 581.0691394805908, 584.010175704956, 586.9512119293213, 590.0683906078339, 593.1855692863464, 596.2158017158508, 599.2460341453552, 602.147976398468, 605.0499186515808, 608.1548049449921, 611.2596912384033, 614.2541072368622, 617.248523235321, 620.1372134685516, 623.0259037017822, 626.0652313232422, 629.1045589447021, 632.0774574279785, 635.0503559112549, 638.0748624801636, 641.0993690490723, 642.5745513439178, 644.0497336387634]
[20.345, 20.345, 33.91, 33.91, 56.715, 56.715, 62.24, 62.24, 68.855, 68.855, 72.9075, 72.9075, 74.7175, 74.7175, 79.34, 79.34, 80.4375, 80.4375, 81.96, 81.96, 83.8, 83.8, 85.4875, 85.4875, 86.4975, 86.4975, 89.07, 89.07, 89.395, 89.395, 90.9625, 90.9625, 91.1175, 91.1175, 91.34, 91.34, 91.495, 91.495, 91.7825, 91.7825, 92.27, 92.27, 92.2275, 92.2275, 92.2775, 92.2775, 92.5975, 92.5975, 92.8075, 92.8075, 93.0725, 93.0725, 93.2525, 93.2525, 93.525, 93.525, 93.555, 93.555, 93.6, 93.6, 93.705, 93.705, 93.7525, 93.7525, 93.7725, 93.7725, 93.925, 93.925, 93.87, 93.87, 93.93, 93.93, 94.035, 94.035, 94.1475, 94.1475, 94.305, 94.305, 94.5, 94.5, 94.4675, 94.4675, 94.49, 94.49, 94.53, 94.53, 94.65, 94.65, 94.6775, 94.6775, 94.6375, 94.6375, 94.6975, 94.6975, 94.6975, 94.6975, 94.7375, 94.7375, 94.7675, 94.7675, 94.755, 94.755, 94.815, 94.815, 94.8825, 94.8825, 94.975, 94.975, 94.9725, 94.9725, 94.9275, 94.9275, 95.03, 95.03, 95.1375, 95.1375, 95.12, 95.12, 95.18, 95.18, 95.11, 95.11, 95.2175, 95.2175, 95.2975, 95.2975, 95.3325, 95.3325, 95.315, 95.315, 95.3475, 95.3475, 95.35, 95.35, 95.3475, 95.3475, 95.3975, 95.3975, 95.4075, 95.4075, 95.4075, 95.4075, 95.4875, 95.4875, 95.5075, 95.5075, 95.465, 95.465, 95.5475, 95.5475, 95.5475, 95.5475, 95.605, 95.605, 95.61, 95.61, 95.57, 95.57, 95.5575, 95.5575, 95.5775, 95.5775, 95.6225, 95.6225, 95.545, 95.545, 95.57, 95.57, 95.6525, 95.6525, 95.63, 95.63, 95.66, 95.66, 95.69, 95.69, 95.695, 95.695, 95.685, 95.685, 95.6875, 95.6875, 95.72, 95.72, 95.7375, 95.7375, 95.8125, 95.8125, 95.8, 95.8, 95.79, 95.79, 95.7925, 95.7925, 95.795, 95.795, 95.8025, 95.8025, 95.81, 95.81, 95.845, 95.845]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.55
Average accuracy final 10 rounds: 96.62175 

4229.800408601761
[3.590367317199707, 7.180734634399414, 10.63374924659729, 14.086763858795166, 17.64733362197876, 21.207903385162354, 24.75559663772583, 28.303289890289307, 31.854926586151123, 35.40656328201294, 38.976954221725464, 42.54734516143799, 45.785444259643555, 49.02354335784912, 52.197959661483765, 55.37237596511841, 58.56443166732788, 61.75648736953735, 65.06044292449951, 68.36439847946167, 71.40032196044922, 74.43624544143677, 77.63399767875671, 80.83174991607666, 84.0678939819336, 87.30403804779053, 90.44207072257996, 93.58010339736938, 96.7727427482605, 99.96538209915161, 103.13190340995789, 106.29842472076416, 109.3699300289154, 112.44143533706665, 115.61734390258789, 118.79325246810913, 121.8318190574646, 124.87038564682007, 128.14345479011536, 131.41652393341064, 134.66362953186035, 137.91073513031006, 140.93741250038147, 143.96408987045288, 147.10919094085693, 150.254292011261, 153.36098384857178, 156.46767568588257, 159.56624245643616, 162.66480922698975, 165.8441126346588, 169.02341604232788, 172.14095449447632, 175.25849294662476, 178.49903082847595, 181.73956871032715, 184.93387484550476, 188.12818098068237, 191.13447642326355, 194.14077186584473, 197.45638990402222, 200.7720079421997, 204.01421236991882, 207.25641679763794, 210.30851483345032, 213.3606128692627, 216.50065803527832, 219.64070320129395, 222.80652570724487, 225.9723482131958, 229.04899859428406, 232.12564897537231, 235.29710221290588, 238.46855545043945, 241.55801486968994, 244.64747428894043, 247.87190532684326, 251.0963363647461, 254.69994282722473, 258.30354928970337, 261.77280044555664, 265.2420516014099, 268.6493082046509, 272.05656480789185, 275.69129037857056, 279.32601594924927, 282.86303424835205, 286.40005254745483, 290.04158186912537, 293.6831111907959, 296.94104647636414, 300.1989817619324, 303.32330203056335, 306.44762229919434, 309.60967922210693, 312.77173614501953, 316.07175612449646, 319.3717761039734, 322.8943030834198, 326.4168300628662, 330.02551078796387, 333.6341915130615, 336.955445766449, 340.2767000198364, 343.42410612106323, 346.57151222229004, 349.78426003456116, 352.9970078468323, 356.1734402179718, 359.3498725891113, 362.42325592041016, 365.496639251709, 368.7380123138428, 371.97938537597656, 375.19241547584534, 378.4054455757141, 381.43768787384033, 384.46993017196655, 387.73009514808655, 390.99026012420654, 394.24316453933716, 397.4960689544678, 400.5908000469208, 403.6855311393738, 407.09891152381897, 410.51229190826416, 413.88115429878235, 417.25001668930054, 420.5684280395508, 423.886839389801, 427.1193127632141, 430.3517861366272, 433.81376671791077, 437.27574729919434, 440.61393666267395, 443.95212602615356, 447.2650291919708, 450.5779323577881, 453.9123909473419, 457.24684953689575, 460.6061806678772, 463.96551179885864, 467.4560477733612, 470.94658374786377, 474.2762379646301, 477.6058921813965, 480.86036443710327, 484.11483669281006, 487.4709575176239, 490.82707834243774, 494.2187285423279, 497.610378742218, 500.90689754486084, 504.20341634750366, 507.4922249317169, 510.7810335159302, 514.0918891429901, 517.40274477005, 520.6930696964264, 523.9833946228027, 527.2632184028625, 530.5430421829224, 534.1739695072174, 537.8048968315125, 541.1972715854645, 544.5896463394165, 547.9855587482452, 551.381471157074, 554.6648418903351, 557.9482126235962, 561.3023982048035, 564.6565837860107, 568.0519516468048, 571.4473195075989, 574.874589920044, 578.301860332489, 581.5905561447144, 584.8792519569397, 588.3006408214569, 591.7220296859741, 595.1115231513977, 598.5010166168213, 601.7803688049316, 605.059720993042, 608.371702671051, 611.6836843490601, 615.0168874263763, 618.3500905036926, 621.6385049819946, 624.9269194602966, 628.233728647232, 631.5405378341675, 634.8932204246521, 638.2459030151367, 641.5671257972717, 644.8883485794067, 648.0605247020721, 651.2327008247375, 654.3285365104675, 657.4243721961975, 659.0174729824066, 660.6105737686157]
[43.335, 43.335, 69.3925, 69.3925, 74.03, 74.03, 77.3525, 77.3525, 79.2675, 79.2675, 80.285, 80.285, 80.9825, 80.9825, 81.37, 81.37, 82.4075, 82.4075, 82.8025, 82.8025, 83.06, 83.06, 83.74, 83.74, 83.75, 83.75, 84.38, 84.38, 84.5325, 84.5325, 84.7475, 84.7475, 84.8, 84.8, 84.94, 84.94, 85.125, 85.125, 85.155, 85.155, 85.225, 85.225, 85.3, 85.3, 85.6, 85.6, 86.0925, 86.0925, 86.62, 86.62, 86.685, 86.685, 87.19, 87.19, 88.11, 88.11, 89.9225, 89.9225, 91.7425, 91.7425, 93.205, 93.205, 93.8125, 93.8125, 94.68, 94.68, 94.8425, 94.8425, 95.395, 95.395, 95.4825, 95.4825, 95.615, 95.615, 95.64, 95.64, 95.625, 95.625, 95.6875, 95.6875, 95.765, 95.765, 95.8675, 95.8675, 95.8925, 95.8925, 95.9125, 95.9125, 95.9925, 95.9925, 96.02, 96.02, 96.0675, 96.0675, 96.11, 96.11, 96.125, 96.125, 96.1, 96.1, 96.185, 96.185, 96.23, 96.23, 96.2375, 96.2375, 96.24, 96.24, 96.3075, 96.3075, 96.365, 96.365, 96.355, 96.355, 96.4, 96.4, 96.395, 96.395, 96.4075, 96.4075, 96.4125, 96.4125, 96.4225, 96.4225, 96.42, 96.42, 96.415, 96.415, 96.4025, 96.4025, 96.455, 96.455, 96.465, 96.465, 96.4375, 96.4375, 96.4975, 96.4975, 96.525, 96.525, 96.5025, 96.5025, 96.4675, 96.4675, 96.5375, 96.5375, 96.5325, 96.5325, 96.5175, 96.5175, 96.5475, 96.5475, 96.5175, 96.5175, 96.505, 96.505, 96.5025, 96.5025, 96.5475, 96.5475, 96.58, 96.58, 96.57, 96.57, 96.5525, 96.5525, 96.5675, 96.5675, 96.5925, 96.5925, 96.5775, 96.5775, 96.5825, 96.5825, 96.5575, 96.5575, 96.5575, 96.5575, 96.545, 96.545, 96.615, 96.615, 96.5975, 96.5975, 96.605, 96.605, 96.5925, 96.5925, 96.63, 96.63, 96.645, 96.645, 96.63, 96.63, 96.6375, 96.6375, 96.635, 96.635, 96.63, 96.63, 96.5525, 96.5525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.288, Test loss: 2.229, Test accuracy: 24.02
Round   1, Train loss: 1.998, Test loss: 1.808, Test accuracy: 70.13
Round   2, Train loss: 1.696, Test loss: 1.692, Test accuracy: 79.59
Round   3, Train loss: 1.648, Test loss: 1.657, Test accuracy: 81.32
Round   4, Train loss: 1.620, Test loss: 1.651, Test accuracy: 81.75
Round   5, Train loss: 1.621, Test loss: 1.645, Test accuracy: 82.23
Round   6, Train loss: 1.610, Test loss: 1.642, Test accuracy: 82.39
Round   7, Train loss: 1.605, Test loss: 1.640, Test accuracy: 82.54
Round   8, Train loss: 1.596, Test loss: 1.637, Test accuracy: 82.61
Round   9, Train loss: 1.530, Test loss: 1.587, Test accuracy: 88.48
Round  10, Train loss: 1.520, Test loss: 1.583, Test accuracy: 88.58
Round  11, Train loss: 1.517, Test loss: 1.577, Test accuracy: 89.16
Round  12, Train loss: 1.516, Test loss: 1.572, Test accuracy: 89.42
Round  13, Train loss: 1.498, Test loss: 1.570, Test accuracy: 89.66
Round  14, Train loss: 1.495, Test loss: 1.569, Test accuracy: 89.71
Round  15, Train loss: 1.499, Test loss: 1.568, Test accuracy: 89.69
Round  16, Train loss: 1.500, Test loss: 1.565, Test accuracy: 89.98
Round  17, Train loss: 1.496, Test loss: 1.565, Test accuracy: 90.01
Round  18, Train loss: 1.494, Test loss: 1.564, Test accuracy: 90.02
Round  19, Train loss: 1.500, Test loss: 1.563, Test accuracy: 90.17
Round  20, Train loss: 1.490, Test loss: 1.563, Test accuracy: 90.18
Round  21, Train loss: 1.491, Test loss: 1.563, Test accuracy: 90.15
Round  22, Train loss: 1.491, Test loss: 1.562, Test accuracy: 90.19
Round  23, Train loss: 1.490, Test loss: 1.562, Test accuracy: 90.19
Round  24, Train loss: 1.487, Test loss: 1.561, Test accuracy: 90.25
Round  25, Train loss: 1.486, Test loss: 1.561, Test accuracy: 90.27
Round  26, Train loss: 1.489, Test loss: 1.561, Test accuracy: 90.25
Round  27, Train loss: 1.488, Test loss: 1.561, Test accuracy: 90.25
Round  28, Train loss: 1.488, Test loss: 1.561, Test accuracy: 90.27
Round  29, Train loss: 1.488, Test loss: 1.561, Test accuracy: 90.28
Round  30, Train loss: 1.488, Test loss: 1.560, Test accuracy: 90.31
Round  31, Train loss: 1.490, Test loss: 1.560, Test accuracy: 90.26
Round  32, Train loss: 1.487, Test loss: 1.560, Test accuracy: 90.32
Round  33, Train loss: 1.485, Test loss: 1.561, Test accuracy: 90.31
Round  34, Train loss: 1.483, Test loss: 1.560, Test accuracy: 90.30
Round  35, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.30
Round  36, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.33
Round  37, Train loss: 1.486, Test loss: 1.560, Test accuracy: 90.36
Round  38, Train loss: 1.486, Test loss: 1.560, Test accuracy: 90.33
Round  39, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.31
Round  40, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.32
Round  41, Train loss: 1.488, Test loss: 1.560, Test accuracy: 90.33
Round  42, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.31
Round  43, Train loss: 1.482, Test loss: 1.560, Test accuracy: 90.32
Round  44, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.31
Round  45, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.31
Round  46, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.30
Round  47, Train loss: 1.483, Test loss: 1.560, Test accuracy: 90.29
Round  48, Train loss: 1.483, Test loss: 1.560, Test accuracy: 90.31
Round  49, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.33
Round  50, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.31
Round  51, Train loss: 1.487, Test loss: 1.559, Test accuracy: 90.31
Round  52, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.33
Round  53, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.33
Round  54, Train loss: 1.486, Test loss: 1.559, Test accuracy: 90.33
Round  55, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.32
Round  56, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.34
Round  57, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.34
Round  58, Train loss: 1.486, Test loss: 1.559, Test accuracy: 90.34
Round  59, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.31
Round  60, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.33
Round  61, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.32
Round  62, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.32
Round  63, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.34
Round  64, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.33
Round  65, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.34
Round  66, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.37
Round  67, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.38
Round  68, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.39
Round  69, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.40
Round  70, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.41
Round  71, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.40
Round  72, Train loss: 1.485, Test loss: 1.559, Test accuracy: 90.38
Round  73, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.40
Round  74, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.38
Round  75, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.37
Round  76, Train loss: 1.485, Test loss: 1.559, Test accuracy: 90.38
Round  77, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.39
Round  78, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.35
Round  79, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.36
Round  80, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.35
Round  81, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.38
Round  82, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.37
Round  83, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.36
Round  84, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.38
Round  85, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.42
Round  86, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.40
Round  87, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.41
Round  88, Train loss: 1.481, Test loss: 1.559, Test accuracy: 90.39
Round  89, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.40
Round  90, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.40
Round  91, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.39
Round  92, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.37
Round  93, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.36
Round  94, Train loss: 1.481, Test loss: 1.559, Test accuracy: 90.36
Round  95, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.36
Round  96, Train loss: 1.479, Test loss: 1.559, Test accuracy: 90.33
Round  97, Train loss: 1.479, Test loss: 1.559, Test accuracy: 90.34
Round  98, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.35
Round  99, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.36/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.34
Average accuracy final 10 rounds: 90.36250000000001 

4672.535085201263
[3.6477677822113037, 7.295535564422607, 11.023440599441528, 14.75134563446045, 18.457818508148193, 22.164291381835938, 25.91098189353943, 29.65767240524292, 33.35524797439575, 37.052823543548584, 40.82396054267883, 44.59509754180908, 48.339759349823, 52.084421157836914, 55.795087814331055, 59.505754470825195, 63.19267773628235, 66.8796010017395, 70.62231922149658, 74.36503744125366, 78.18448400497437, 82.00393056869507, 85.82644414901733, 89.6489577293396, 93.40296506881714, 97.15697240829468, 100.90161943435669, 104.6462664604187, 108.38823127746582, 112.13019609451294, 115.85307741165161, 119.57595872879028, 123.31617903709412, 127.05639934539795, 130.84057068824768, 134.6247420310974, 138.38329696655273, 142.14185190200806, 145.8845899105072, 149.62732791900635, 153.33928632736206, 157.05124473571777, 160.7976176738739, 164.54399061203003, 168.4353108406067, 172.32663106918335, 176.0479772090912, 179.76932334899902, 183.63338255882263, 187.49744176864624, 191.26957893371582, 195.0417160987854, 198.79753065109253, 202.55334520339966, 206.2267665863037, 209.90018796920776, 213.62999510765076, 217.35980224609375, 221.1559932231903, 224.95218420028687, 228.7015359401703, 232.4508876800537, 236.186616897583, 239.9223461151123, 243.69170928001404, 247.46107244491577, 251.22704148292542, 254.99301052093506, 258.73279881477356, 262.47258710861206, 266.16510462760925, 269.85762214660645, 273.60419511795044, 277.35076808929443, 281.17618703842163, 285.0016059875488, 288.7417709827423, 292.4819359779358, 296.21289134025574, 299.9438467025757, 303.6597716808319, 307.37569665908813, 311.16312885284424, 314.95056104660034, 318.77779626846313, 322.6050314903259, 326.4124286174774, 330.2198257446289, 334.1548888683319, 338.0899519920349, 342.0176899433136, 345.9454278945923, 349.76782298088074, 353.5902180671692, 357.42740654945374, 361.2645950317383, 365.1538531780243, 369.0431113243103, 372.9365222454071, 376.8299331665039, 380.6816260814667, 384.53331899642944, 388.3946123123169, 392.25590562820435, 396.0961844921112, 399.93646335601807, 403.7384407520294, 407.54041814804077, 411.46809911727905, 415.39578008651733, 419.31293201446533, 423.23008394241333, 427.2059087753296, 431.18173360824585, 435.13049483299255, 439.07925605773926, 443.0300512313843, 446.9808464050293, 450.9241564273834, 454.86746644973755, 458.85717272758484, 462.84687900543213, 466.7568004131317, 470.6667218208313, 474.59265303611755, 478.5185842514038, 482.5102422237396, 486.50190019607544, 490.45590829849243, 494.4099164009094, 498.34374356269836, 502.2775707244873, 506.26553606987, 510.2535014152527, 514.2384948730469, 518.2234883308411, 522.129894733429, 526.0363011360168, 529.9530589580536, 533.8698167800903, 537.9027469158173, 541.9356770515442, 546.0047438144684, 550.0738105773926, 554.008022069931, 557.9422335624695, 561.7785067558289, 565.6147799491882, 569.4475045204163, 573.2802290916443, 577.1063916683197, 580.9325542449951, 584.6909408569336, 588.4493274688721, 592.3298344612122, 596.2103414535522, 600.319256067276, 604.4281706809998, 608.448582649231, 612.4689946174622, 616.4017872810364, 620.3345799446106, 624.2719588279724, 628.2093377113342, 632.2160801887512, 636.2228226661682, 640.2274332046509, 644.2320437431335, 648.2218885421753, 652.211733341217, 656.2252488136292, 660.2387642860413, 664.2878377437592, 668.336911201477, 672.2978353500366, 676.2587594985962, 680.1104273796082, 683.9620952606201, 687.8248264789581, 691.6875576972961, 695.7054460048676, 699.723334312439, 703.7335298061371, 707.7437252998352, 711.6379017829895, 715.5320782661438, 719.4038910865784, 723.2757039070129, 727.1975705623627, 731.1194372177124, 735.0705251693726, 739.0216131210327, 742.9652879238129, 746.908962726593, 750.7862122058868, 754.6634616851807, 758.728150844574, 762.7928400039673, 766.7982473373413, 770.8036546707153, 772.6889853477478, 774.5743160247803]
[24.015, 24.015, 70.1325, 70.1325, 79.59, 79.59, 81.3175, 81.3175, 81.745, 81.745, 82.2325, 82.2325, 82.39, 82.39, 82.5425, 82.5425, 82.6075, 82.6075, 88.48, 88.48, 88.5775, 88.5775, 89.1625, 89.1625, 89.415, 89.415, 89.655, 89.655, 89.71, 89.71, 89.685, 89.685, 89.9775, 89.9775, 90.01, 90.01, 90.02, 90.02, 90.1725, 90.1725, 90.1825, 90.1825, 90.1525, 90.1525, 90.195, 90.195, 90.19, 90.19, 90.2525, 90.2525, 90.27, 90.27, 90.2525, 90.2525, 90.255, 90.255, 90.27, 90.27, 90.28, 90.28, 90.315, 90.315, 90.2575, 90.2575, 90.3175, 90.3175, 90.315, 90.315, 90.3025, 90.3025, 90.295, 90.295, 90.3325, 90.3325, 90.3625, 90.3625, 90.325, 90.325, 90.3075, 90.3075, 90.3225, 90.3225, 90.3275, 90.3275, 90.31, 90.31, 90.3225, 90.3225, 90.3075, 90.3075, 90.315, 90.315, 90.3025, 90.3025, 90.2925, 90.2925, 90.3075, 90.3075, 90.325, 90.325, 90.315, 90.315, 90.315, 90.315, 90.3275, 90.3275, 90.3325, 90.3325, 90.3275, 90.3275, 90.32, 90.32, 90.345, 90.345, 90.3425, 90.3425, 90.3375, 90.3375, 90.315, 90.315, 90.325, 90.325, 90.3175, 90.3175, 90.32, 90.32, 90.345, 90.345, 90.3325, 90.3325, 90.3425, 90.3425, 90.37, 90.37, 90.3775, 90.3775, 90.3925, 90.3925, 90.4025, 90.4025, 90.41, 90.41, 90.3975, 90.3975, 90.3775, 90.3775, 90.3975, 90.3975, 90.375, 90.375, 90.3725, 90.3725, 90.38, 90.38, 90.3875, 90.3875, 90.3525, 90.3525, 90.36, 90.36, 90.35, 90.35, 90.3775, 90.3775, 90.3725, 90.3725, 90.3625, 90.3625, 90.3775, 90.3775, 90.415, 90.415, 90.4025, 90.4025, 90.4075, 90.4075, 90.39, 90.39, 90.4, 90.4, 90.4, 90.4, 90.39, 90.39, 90.3675, 90.3675, 90.355, 90.355, 90.36, 90.36, 90.355, 90.355, 90.335, 90.335, 90.345, 90.345, 90.3525, 90.3525, 90.365, 90.365, 90.34, 90.34]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.711, Test loss: 2.295, Test accuracy: 17.27
Round   1, Train loss: 1.613, Test loss: 2.262, Test accuracy: 39.23
Round   2, Train loss: 1.460, Test loss: 2.172, Test accuracy: 50.86
Round   3, Train loss: 1.338, Test loss: 2.082, Test accuracy: 63.71
Round   4, Train loss: 1.254, Test loss: 2.014, Test accuracy: 71.25
Round   5, Train loss: 1.221, Test loss: 1.982, Test accuracy: 74.48
Round   6, Train loss: 1.216, Test loss: 1.960, Test accuracy: 77.58
Round   7, Train loss: 1.207, Test loss: 1.948, Test accuracy: 78.28
Round   8, Train loss: 1.197, Test loss: 1.938, Test accuracy: 79.77
Round   9, Train loss: 1.169, Test loss: 1.925, Test accuracy: 82.10
Round  10, Train loss: 1.152, Test loss: 1.913, Test accuracy: 82.79
Round  11, Train loss: 1.146, Test loss: 1.905, Test accuracy: 83.14
Round  12, Train loss: 1.146, Test loss: 1.898, Test accuracy: 84.08
Round  13, Train loss: 1.135, Test loss: 1.893, Test accuracy: 84.53
Round  14, Train loss: 1.130, Test loss: 1.887, Test accuracy: 84.96
Round  15, Train loss: 1.134, Test loss: 1.884, Test accuracy: 85.28
Round  16, Train loss: 1.127, Test loss: 1.880, Test accuracy: 85.28
Round  17, Train loss: 1.133, Test loss: 1.877, Test accuracy: 85.16
Round  18, Train loss: 1.128, Test loss: 1.876, Test accuracy: 84.88
Round  19, Train loss: 1.123, Test loss: 1.873, Test accuracy: 84.52
Round  20, Train loss: 1.128, Test loss: 1.873, Test accuracy: 84.30
Round  21, Train loss: 1.121, Test loss: 1.872, Test accuracy: 83.82
Round  22, Train loss: 1.120, Test loss: 1.870, Test accuracy: 83.75
Round  23, Train loss: 1.122, Test loss: 1.869, Test accuracy: 83.59
Round  24, Train loss: 1.123, Test loss: 1.870, Test accuracy: 83.08
Round  25, Train loss: 1.121, Test loss: 1.869, Test accuracy: 83.17
Round  26, Train loss: 1.116, Test loss: 1.868, Test accuracy: 83.05
Round  27, Train loss: 1.117, Test loss: 1.867, Test accuracy: 82.92
Round  28, Train loss: 1.118, Test loss: 1.867, Test accuracy: 82.53
Round  29, Train loss: 1.116, Test loss: 1.866, Test accuracy: 82.45
Round  30, Train loss: 1.116, Test loss: 1.864, Test accuracy: 82.38
Round  31, Train loss: 1.117, Test loss: 1.865, Test accuracy: 81.97
Round  32, Train loss: 1.118, Test loss: 1.866, Test accuracy: 81.68
Round  33, Train loss: 1.116, Test loss: 1.865, Test accuracy: 81.58
Round  34, Train loss: 1.117, Test loss: 1.866, Test accuracy: 81.35
Round  35, Train loss: 1.115, Test loss: 1.866, Test accuracy: 80.93
Round  36, Train loss: 1.115, Test loss: 1.866, Test accuracy: 80.80
Round  37, Train loss: 1.111, Test loss: 1.866, Test accuracy: 80.50
Round  38, Train loss: 1.115, Test loss: 1.866, Test accuracy: 80.27
Round  39, Train loss: 1.116, Test loss: 1.867, Test accuracy: 80.08
Round  40, Train loss: 1.113, Test loss: 1.866, Test accuracy: 79.97
Round  41, Train loss: 1.114, Test loss: 1.866, Test accuracy: 79.87
Round  42, Train loss: 1.115, Test loss: 1.867, Test accuracy: 79.53
Round  43, Train loss: 1.113, Test loss: 1.868, Test accuracy: 79.32
Round  44, Train loss: 1.115, Test loss: 1.869, Test accuracy: 78.96
Round  45, Train loss: 1.116, Test loss: 1.869, Test accuracy: 78.96
Round  46, Train loss: 1.114, Test loss: 1.869, Test accuracy: 78.66
Round  47, Train loss: 1.116, Test loss: 1.870, Test accuracy: 78.30
Round  48, Train loss: 1.113, Test loss: 1.871, Test accuracy: 78.19
Round  49, Train loss: 1.109, Test loss: 1.871, Test accuracy: 77.86
Round  50, Train loss: 1.114, Test loss: 1.873, Test accuracy: 77.57
Round  51, Train loss: 1.114, Test loss: 1.873, Test accuracy: 77.47
Round  52, Train loss: 1.114, Test loss: 1.874, Test accuracy: 77.33
Round  53, Train loss: 1.114, Test loss: 1.875, Test accuracy: 77.13
Round  54, Train loss: 1.110, Test loss: 1.875, Test accuracy: 77.14
Round  55, Train loss: 1.112, Test loss: 1.875, Test accuracy: 76.96
Round  56, Train loss: 1.113, Test loss: 1.875, Test accuracy: 76.97
Round  57, Train loss: 1.112, Test loss: 1.877, Test accuracy: 76.47
Round  58, Train loss: 1.112, Test loss: 1.877, Test accuracy: 76.48
Round  59, Train loss: 1.111, Test loss: 1.877, Test accuracy: 76.33
Round  60, Train loss: 1.108, Test loss: 1.877, Test accuracy: 76.08
Round  61, Train loss: 1.115, Test loss: 1.879, Test accuracy: 75.97
Round  62, Train loss: 1.113, Test loss: 1.879, Test accuracy: 75.88
Round  63, Train loss: 1.111, Test loss: 1.880, Test accuracy: 75.83
Round  64, Train loss: 1.113, Test loss: 1.881, Test accuracy: 75.69
Round  65, Train loss: 1.111, Test loss: 1.881, Test accuracy: 75.49
Round  66, Train loss: 1.109, Test loss: 1.881, Test accuracy: 75.53
Round  67, Train loss: 1.111, Test loss: 1.881, Test accuracy: 75.23
Round  68, Train loss: 1.109, Test loss: 1.882, Test accuracy: 75.03
Round  69, Train loss: 1.110, Test loss: 1.882, Test accuracy: 74.97
Round  70, Train loss: 1.109, Test loss: 1.882, Test accuracy: 74.83
Round  71, Train loss: 1.108, Test loss: 1.882, Test accuracy: 74.72
Round  72, Train loss: 1.109, Test loss: 1.882, Test accuracy: 74.47
Round  73, Train loss: 1.109, Test loss: 1.883, Test accuracy: 74.41
Round  74, Train loss: 1.110, Test loss: 1.883, Test accuracy: 74.09
Round  75, Train loss: 1.112, Test loss: 1.884, Test accuracy: 73.94
Round  76, Train loss: 1.109, Test loss: 1.884, Test accuracy: 73.91
Round  77, Train loss: 1.109, Test loss: 1.884, Test accuracy: 73.82
Round  78, Train loss: 1.111, Test loss: 1.885, Test accuracy: 73.69
Round  79, Train loss: 1.108, Test loss: 1.884, Test accuracy: 73.62
Round  80, Train loss: 1.109, Test loss: 1.885, Test accuracy: 73.54
Round  81, Train loss: 1.111, Test loss: 1.886, Test accuracy: 73.49
Round  82, Train loss: 1.110, Test loss: 1.886, Test accuracy: 73.39
Round  83, Train loss: 1.110, Test loss: 1.886, Test accuracy: 73.30
Round  84, Train loss: 1.108, Test loss: 1.886, Test accuracy: 73.27
Round  85, Train loss: 1.108, Test loss: 1.887, Test accuracy: 73.09
Round  86, Train loss: 1.110, Test loss: 1.888, Test accuracy: 73.08
Round  87, Train loss: 1.110, Test loss: 1.888, Test accuracy: 72.92
Round  88, Train loss: 1.107, Test loss: 1.888, Test accuracy: 72.93
Round  89, Train loss: 1.109, Test loss: 1.888, Test accuracy: 72.90
Round  90, Train loss: 1.112, Test loss: 1.889, Test accuracy: 72.67
Round  91, Train loss: 1.112, Test loss: 1.889, Test accuracy: 72.46
Round  92, Train loss: 1.107, Test loss: 1.889, Test accuracy: 72.46
Round  93, Train loss: 1.108, Test loss: 1.890, Test accuracy: 72.29
Round  94, Train loss: 1.108, Test loss: 1.890, Test accuracy: 72.06
Round  95, Train loss: 1.108, Test loss: 1.891, Test accuracy: 71.94
Round  96, Train loss: 1.108, Test loss: 1.891, Test accuracy: 71.99
Round  97, Train loss: 1.108, Test loss: 1.891, Test accuracy: 71.89
Round  98, Train loss: 1.108, Test loss: 1.892, Test accuracy: 71.71
Round  99, Train loss: 1.109, Test loss: 1.891, Test accuracy: 71.72
Final Round, Train loss: 1.108, Test loss: 1.892, Test accuracy: 71.54
Average accuracy final 10 rounds: 72.11999999999999
1534.7390975952148
[]
[17.275, 39.225, 50.858333333333334, 63.708333333333336, 71.25, 74.48333333333333, 77.58333333333333, 78.28333333333333, 79.76666666666667, 82.1, 82.79166666666667, 83.14166666666667, 84.075, 84.525, 84.95833333333333, 85.28333333333333, 85.28333333333333, 85.15833333333333, 84.875, 84.51666666666667, 84.3, 83.81666666666666, 83.75, 83.59166666666667, 83.075, 83.175, 83.05, 82.925, 82.53333333333333, 82.45, 82.375, 81.96666666666667, 81.68333333333334, 81.58333333333333, 81.35, 80.93333333333334, 80.8, 80.5, 80.26666666666667, 80.075, 79.96666666666667, 79.86666666666666, 79.53333333333333, 79.31666666666666, 78.95833333333333, 78.95833333333333, 78.65833333333333, 78.3, 78.19166666666666, 77.85833333333333, 77.56666666666666, 77.475, 77.33333333333333, 77.13333333333334, 77.14166666666667, 76.95833333333333, 76.96666666666667, 76.475, 76.48333333333333, 76.325, 76.08333333333333, 75.975, 75.88333333333334, 75.825, 75.69166666666666, 75.49166666666666, 75.53333333333333, 75.23333333333333, 75.03333333333333, 74.975, 74.825, 74.725, 74.46666666666667, 74.40833333333333, 74.09166666666667, 73.94166666666666, 73.90833333333333, 73.81666666666666, 73.69166666666666, 73.625, 73.54166666666667, 73.49166666666666, 73.39166666666667, 73.3, 73.26666666666667, 73.09166666666667, 73.08333333333333, 72.925, 72.93333333333334, 72.9, 72.675, 72.45833333333333, 72.45833333333333, 72.29166666666667, 72.05833333333334, 71.94166666666666, 71.99166666666666, 71.89166666666667, 71.70833333333333, 71.725, 71.54166666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.282, Test loss: 2.265, Test accuracy: 23.13
Round   1, Train loss: 2.164, Test loss: 2.214, Test accuracy: 28.43
Round   2, Train loss: 2.127, Test loss: 2.224, Test accuracy: 22.98
Round   3, Train loss: 1.853, Test loss: 2.134, Test accuracy: 39.37
Round   4, Train loss: 1.874, Test loss: 2.095, Test accuracy: 43.64
Round   5, Train loss: 1.684, Test loss: 2.071, Test accuracy: 44.30
Round   6, Train loss: 1.325, Test loss: 2.071, Test accuracy: 45.98
Round   7, Train loss: 2.049, Test loss: 2.113, Test accuracy: 37.53
Round   8, Train loss: 1.578, Test loss: 2.113, Test accuracy: 37.59
Round   9, Train loss: 1.703, Test loss: 2.078, Test accuracy: 40.42
Round  10, Train loss: 1.680, Test loss: 2.154, Test accuracy: 32.77
Round  11, Train loss: 1.288, Test loss: 2.069, Test accuracy: 39.87
Round  12, Train loss: 0.862, Test loss: 1.993, Test accuracy: 47.49
Round  13, Train loss: 1.079, Test loss: 2.009, Test accuracy: 45.58
Round  14, Train loss: 0.991, Test loss: 2.015, Test accuracy: 46.20
Round  15, Train loss: 0.110, Test loss: 1.955, Test accuracy: 51.37
Round  16, Train loss: 1.585, Test loss: 1.951, Test accuracy: 52.55
Round  17, Train loss: 0.986, Test loss: 1.926, Test accuracy: 55.59
Round  18, Train loss: 0.713, Test loss: 1.889, Test accuracy: 59.05
Round  19, Train loss: 1.159, Test loss: 1.884, Test accuracy: 59.64
Round  20, Train loss: 0.773, Test loss: 1.868, Test accuracy: 60.64
Round  21, Train loss: 0.519, Test loss: 1.809, Test accuracy: 66.17
Round  22, Train loss: 0.973, Test loss: 1.773, Test accuracy: 69.27
Round  23, Train loss: 0.885, Test loss: 1.744, Test accuracy: 72.16
Round  24, Train loss: -0.128, Test loss: 1.734, Test accuracy: 72.98
Round  25, Train loss: 0.409, Test loss: 1.735, Test accuracy: 72.85
Round  26, Train loss: -0.362, Test loss: 1.738, Test accuracy: 72.50
Round  27, Train loss: 0.824, Test loss: 1.732, Test accuracy: 73.24
Round  28, Train loss: 0.788, Test loss: 1.722, Test accuracy: 74.24
Round  29, Train loss: 0.719, Test loss: 1.710, Test accuracy: 75.42
Round  30, Train loss: 0.673, Test loss: 1.722, Test accuracy: 74.11
Round  31, Train loss: 0.771, Test loss: 1.723, Test accuracy: 74.04
Round  32, Train loss: 0.627, Test loss: 1.705, Test accuracy: 75.71
Round  33, Train loss: 0.505, Test loss: 1.695, Test accuracy: 76.61
Round  34, Train loss: 0.478, Test loss: 1.694, Test accuracy: 76.71
Round  35, Train loss: 0.762, Test loss: 1.695, Test accuracy: 76.53
Round  36, Train loss: 0.509, Test loss: 1.689, Test accuracy: 77.15
Round  37, Train loss: 0.308, Test loss: 1.676, Test accuracy: 78.47
Round  38, Train loss: 0.660, Test loss: 1.688, Test accuracy: 77.25
Round  39, Train loss: 0.544, Test loss: 1.696, Test accuracy: 76.49
Round  40, Train loss: 0.331, Test loss: 1.696, Test accuracy: 76.47
Round  41, Train loss: 0.626, Test loss: 1.692, Test accuracy: 76.88
Round  42, Train loss: 0.299, Test loss: 1.690, Test accuracy: 77.09
Round  43, Train loss: 0.390, Test loss: 1.690, Test accuracy: 77.10
Round  44, Train loss: 0.311, Test loss: 1.686, Test accuracy: 77.43
Round  45, Train loss: 0.027, Test loss: 1.691, Test accuracy: 76.94
Round  46, Train loss: -0.049, Test loss: 1.688, Test accuracy: 77.25
Round  47, Train loss: 0.634, Test loss: 1.684, Test accuracy: 77.66
Round  48, Train loss: 0.165, Test loss: 1.687, Test accuracy: 77.42
Round  49, Train loss: -0.212, Test loss: 1.686, Test accuracy: 77.42
Round  50, Train loss: 0.596, Test loss: 1.668, Test accuracy: 79.35
Round  51, Train loss: 0.432, Test loss: 1.682, Test accuracy: 77.86
Round  52, Train loss: 0.467, Test loss: 1.676, Test accuracy: 78.42
Round  53, Train loss: 0.071, Test loss: 1.673, Test accuracy: 78.84
Round  54, Train loss: 0.914, Test loss: 1.680, Test accuracy: 78.08
Round  55, Train loss: 0.588, Test loss: 1.684, Test accuracy: 77.66
Round  56, Train loss: 0.515, Test loss: 1.676, Test accuracy: 78.47
Round  57, Train loss: 0.310, Test loss: 1.689, Test accuracy: 77.10
Round  58, Train loss: 0.354, Test loss: 1.687, Test accuracy: 77.37
Round  59, Train loss: -0.068, Test loss: 1.684, Test accuracy: 77.66
Round  60, Train loss: 0.720, Test loss: 1.676, Test accuracy: 78.52
Round  61, Train loss: 0.275, Test loss: 1.683, Test accuracy: 77.81
Round  62, Train loss: 0.560, Test loss: 1.682, Test accuracy: 77.86
Round  63, Train loss: 0.244, Test loss: 1.682, Test accuracy: 77.92
Round  64, Train loss: 0.660, Test loss: 1.676, Test accuracy: 78.47
Round  65, Train loss: 0.111, Test loss: 1.680, Test accuracy: 78.14
Round  66, Train loss: 0.640, Test loss: 1.683, Test accuracy: 77.69
Round  67, Train loss: 0.542, Test loss: 1.684, Test accuracy: 77.68
Round  68, Train loss: 0.567, Test loss: 1.673, Test accuracy: 78.82
Round  69, Train loss: 0.759, Test loss: 1.671, Test accuracy: 79.05
Round  70, Train loss: 0.493, Test loss: 1.670, Test accuracy: 79.08
Round  71, Train loss: 0.412, Test loss: 1.671, Test accuracy: 79.06
Round  72, Train loss: 0.527, Test loss: 1.666, Test accuracy: 79.53
Round  73, Train loss: 0.248, Test loss: 1.669, Test accuracy: 79.15
Round  74, Train loss: -0.113, Test loss: 1.668, Test accuracy: 79.27
Round  75, Train loss: 0.729, Test loss: 1.661, Test accuracy: 80.04
Round  76, Train loss: 0.532, Test loss: 1.669, Test accuracy: 79.22
Round  77, Train loss: 0.690, Test loss: 1.676, Test accuracy: 78.51
Round  78, Train loss: 0.540, Test loss: 1.671, Test accuracy: 78.97
Round  79, Train loss: 0.609, Test loss: 1.665, Test accuracy: 79.63
Round  80, Train loss: 0.343, Test loss: 1.662, Test accuracy: 80.00
Round  81, Train loss: 0.512, Test loss: 1.654, Test accuracy: 80.71
Round  82, Train loss: 0.527, Test loss: 1.661, Test accuracy: 79.99
Round  83, Train loss: 0.648, Test loss: 1.674, Test accuracy: 78.64
Round  84, Train loss: 0.480, Test loss: 1.658, Test accuracy: 80.20
Round  85, Train loss: 0.733, Test loss: 1.666, Test accuracy: 79.46
Round  86, Train loss: 0.731, Test loss: 1.680, Test accuracy: 77.97
Round  87, Train loss: 0.761, Test loss: 1.673, Test accuracy: 78.75
Round  88, Train loss: 0.439, Test loss: 1.681, Test accuracy: 77.97
Round  89, Train loss: 0.581, Test loss: 1.683, Test accuracy: 77.81
Round  90, Train loss: 0.872, Test loss: 1.682, Test accuracy: 77.89
Round  91, Train loss: 0.747, Test loss: 1.667, Test accuracy: 79.36
Round  92, Train loss: 0.585, Test loss: 1.668, Test accuracy: 79.26
Round  93, Train loss: 0.632, Test loss: 1.668, Test accuracy: 79.30
Round  94, Train loss: 0.599, Test loss: 1.658, Test accuracy: 80.28
Round  95, Train loss: 0.670, Test loss: 1.659, Test accuracy: 80.19
Round  96, Train loss: 0.635, Test loss: 1.660, Test accuracy: 80.08
Round  97, Train loss: 0.542, Test loss: 1.651, Test accuracy: 80.98
Round  98, Train loss: 0.809, Test loss: 1.660, Test accuracy: 80.08
Round  99, Train loss: 0.643, Test loss: 1.658, Test accuracy: 80.29
Final Round, Train loss: 1.626, Test loss: 1.621, Test accuracy: 84.39
Average accuracy final 10 rounds: 79.76875000000001
Average global accuracy final 10 rounds: 79.76875000000001
3520.310885667801
[]
[23.13, 28.4275, 22.98, 39.3675, 43.64, 44.305, 45.985, 37.53, 37.595, 40.4175, 32.7675, 39.8675, 47.4875, 45.58, 46.2, 51.365, 52.5525, 55.5925, 59.0475, 59.64, 60.64, 66.175, 69.2675, 72.16, 72.98, 72.85, 72.5025, 73.2425, 74.2425, 75.425, 74.11, 74.0425, 75.7075, 76.615, 76.71, 76.53, 77.1525, 78.465, 77.25, 76.4925, 76.475, 76.88, 77.095, 77.1025, 77.4325, 76.94, 77.245, 77.6575, 77.42, 77.425, 79.3475, 77.8575, 78.425, 78.84, 78.0825, 77.66, 78.4675, 77.1, 77.3675, 77.66, 78.515, 77.805, 77.865, 77.9225, 78.475, 78.14, 77.6925, 77.6825, 78.8175, 79.05, 79.075, 79.055, 79.5325, 79.1525, 79.27, 80.04, 79.2175, 78.5125, 78.9725, 79.63, 79.995, 80.7125, 79.9925, 78.64, 80.205, 79.46, 77.97, 78.7525, 77.9675, 77.815, 77.885, 79.3625, 79.26, 79.295, 80.275, 80.1925, 80.075, 80.98, 80.075, 80.2875, 84.395]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.68
Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.67
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.67
Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.68
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.71
Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.73
Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.69
Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.70
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.70
Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.73
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.71
Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.74
Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.73
Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.74
Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78
Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.73
Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.74
Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.79
Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.81
Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.80
Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.79
Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.76
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.79
Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.81
Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.82
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.81
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.81
Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.82
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.81
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.84
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.83
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.83
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.83
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.85
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.87
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.87
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.87
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.88
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.86
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.88
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.87
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.90
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.90
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.99
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.93
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.99
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.95
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.99
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.98
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.04
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.04
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.01
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.00
Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.01
Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.00
Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.03
Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.07
Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.05
Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.12
Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.06
Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.08
Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.06
Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.05
Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.04
Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.04
Round  58, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 13.01
Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  59, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.94
Round  60, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  60, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.92
Round  61, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.96
Round  61, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.97
Round  62, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  62, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.01
Round  63, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  63, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.92
Round  64, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  64, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.96
Round  65, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.97
Round  65, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.99
Round  66, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.99
Round  66, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.02
Round  67, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.03
Round  67, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.10
Round  68, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.07
Round  68, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.13
Round  69, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.11
Round  69, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.17
Round  70, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.15
Round  70, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.18
Round  71, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.16
Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.17
Round  72, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.19
Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.23
Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.21
Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.24
Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.23
Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.21
Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.25
Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.30
Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.27
Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.29
Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.29
Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.31
Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.31
Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.37
Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.34
Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.35
Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.36
Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.38
Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.38
Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.37
Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.41
Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.48
Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.41
Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.48
Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.45
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.55
Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.47
Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.57
Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.53
Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.61
Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.59
Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.61
Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.60
Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.62
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.63
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.61
Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.62
Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.62
Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.62
Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.62
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.59
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.59
Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.58
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.60
Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.56
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.53
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.55
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.52
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.56
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.56/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.55
Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.52
Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.52
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.50
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.50
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.50
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.51
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.50
Average accuracy final 10 rounds: 13.566499999999998 

Average global accuracy final 10 rounds: 13.5555 

4456.784124851227
[3.563004970550537, 6.844911575317383, 10.113284349441528, 13.369019985198975, 16.777698755264282, 19.985750198364258, 23.090543031692505, 26.355243921279907, 29.861509084701538, 33.335506200790405, 37.133392333984375, 40.604469299316406, 44.275830030441284, 47.77281093597412, 51.54963541030884, 55.085001707077026, 58.6940541267395, 62.29011154174805, 65.71761894226074, 69.33883023262024, 72.88821411132812, 76.63048911094666, 80.85748505592346, 84.96566438674927, 89.14780497550964, 93.43360352516174, 97.6964704990387, 101.84296250343323, 105.91579961776733, 110.17804551124573, 114.43949675559998, 118.58232879638672, 122.64569807052612, 126.83494901657104, 131.11475229263306, 135.3394103050232, 139.31694984436035, 143.61375379562378, 147.8525369167328, 152.1171452999115, 156.20720553398132, 160.37719750404358, 164.4779567718506, 168.4903745651245, 172.50467443466187, 175.996084690094, 179.9003505706787, 183.76956009864807, 187.6663055419922, 191.6525375843048, 195.58306908607483, 199.1410427093506, 202.9242503643036, 206.6460633277893, 210.1560537815094, 213.83495235443115, 217.45771312713623, 220.9034388065338, 224.51352429389954, 228.77167463302612, 232.43568968772888, 236.59611701965332, 240.54107213020325, 244.53014278411865, 248.35969281196594, 252.0544831752777, 255.92817854881287, 259.5439975261688, 263.4795219898224, 267.2376878261566, 271.03114008903503, 274.8858449459076, 278.4307191371918, 282.0954644680023, 286.0071361064911, 289.9729504585266, 293.6545720100403, 297.3953070640564, 301.06560492515564, 304.71927070617676, 308.57178831100464, 312.36102771759033, 316.1681203842163, 320.3183743953705, 324.08771538734436, 327.72293996810913, 331.3751826286316, 335.2053372859955, 339.17204999923706, 342.87774896621704, 346.72114634513855, 350.8236138820648, 354.6647002696991, 358.65835332870483, 362.47550201416016, 366.2647008895874, 369.94736433029175, 373.60834312438965, 377.2339856624603, 381.00479674339294, 382.8956968784332]
[12.675, 12.6725, 12.705, 12.69, 12.695, 12.705, 12.7275, 12.74, 12.7275, 12.7475, 12.745, 12.75, 12.75, 12.785, 12.8075, 12.8, 12.795, 12.795, 12.8125, 12.82, 12.81, 12.82, 12.8375, 12.83, 12.8475, 12.8675, 12.8725, 12.8725, 12.8625, 12.8775, 12.8825, 12.88, 12.88, 12.895, 12.89, 12.895, 12.9025, 12.8925, 12.9025, 12.9275, 12.9525, 12.9825, 13.0, 13.005, 13.02, 12.9975, 12.9975, 13.0075, 13.0175, 13.015, 13.015, 13.0025, 13.015, 13.03, 13.05, 13.06, 13.0575, 13.035, 13.035, 12.9975, 12.98, 12.9625, 12.9825, 12.9775, 12.985, 12.975, 12.9875, 13.03, 13.07, 13.1075, 13.1475, 13.1575, 13.1875, 13.205, 13.2325, 13.2525, 13.2725, 13.285, 13.3125, 13.3375, 13.3575, 13.3825, 13.4075, 13.415, 13.445, 13.47, 13.525, 13.585, 13.6025, 13.6275, 13.6225, 13.62, 13.5925, 13.5775, 13.5625, 13.555, 13.565, 13.5525, 13.515, 13.5025, 13.5125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.301, Test loss: 2.297, Test accuracy: 36.38
Round   1, Train loss: 2.296, Test loss: 2.286, Test accuracy: 55.43
Round   2, Train loss: 2.283, Test loss: 2.210, Test accuracy: 40.73
Round   3, Train loss: 2.196, Test loss: 1.977, Test accuracy: 51.12
Round   4, Train loss: 1.986, Test loss: 1.827, Test accuracy: 66.53
Round   5, Train loss: 1.910, Test loss: 1.761, Test accuracy: 73.19
Round   6, Train loss: 1.796, Test loss: 1.731, Test accuracy: 74.70
Round   7, Train loss: 1.741, Test loss: 1.720, Test accuracy: 75.20
Round   8, Train loss: 1.782, Test loss: 1.711, Test accuracy: 75.80
Round   9, Train loss: 1.718, Test loss: 1.708, Test accuracy: 75.88
Round  10, Train loss: 1.719, Test loss: 1.705, Test accuracy: 76.02
Round  11, Train loss: 1.705, Test loss: 1.703, Test accuracy: 76.15
Round  12, Train loss: 1.695, Test loss: 1.701, Test accuracy: 76.33
Round  13, Train loss: 1.703, Test loss: 1.699, Test accuracy: 76.43
Round  14, Train loss: 1.702, Test loss: 1.699, Test accuracy: 76.39
Round  15, Train loss: 1.702, Test loss: 1.696, Test accuracy: 76.42
Round  16, Train loss: 1.693, Test loss: 1.657, Test accuracy: 81.42
Round  17, Train loss: 1.666, Test loss: 1.646, Test accuracy: 82.30
Round  18, Train loss: 1.648, Test loss: 1.640, Test accuracy: 82.56
Round  19, Train loss: 1.639, Test loss: 1.638, Test accuracy: 82.58
Round  20, Train loss: 1.627, Test loss: 1.634, Test accuracy: 83.15
Round  21, Train loss: 1.616, Test loss: 1.632, Test accuracy: 83.07
Round  22, Train loss: 1.608, Test loss: 1.630, Test accuracy: 83.39
Round  23, Train loss: 1.629, Test loss: 1.629, Test accuracy: 83.44
Round  24, Train loss: 1.607, Test loss: 1.627, Test accuracy: 83.56
Round  25, Train loss: 1.609, Test loss: 1.627, Test accuracy: 83.64
Round  26, Train loss: 1.604, Test loss: 1.626, Test accuracy: 83.76
Round  27, Train loss: 1.606, Test loss: 1.624, Test accuracy: 83.85
Round  28, Train loss: 1.602, Test loss: 1.624, Test accuracy: 83.84
Round  29, Train loss: 1.600, Test loss: 1.622, Test accuracy: 84.03
Round  30, Train loss: 1.601, Test loss: 1.621, Test accuracy: 84.07
Round  31, Train loss: 1.599, Test loss: 1.620, Test accuracy: 84.22
Round  32, Train loss: 1.602, Test loss: 1.620, Test accuracy: 84.12
Round  33, Train loss: 1.598, Test loss: 1.619, Test accuracy: 84.27
Round  34, Train loss: 1.604, Test loss: 1.618, Test accuracy: 84.29
Round  35, Train loss: 1.601, Test loss: 1.618, Test accuracy: 84.37
Round  36, Train loss: 1.596, Test loss: 1.618, Test accuracy: 84.39
Round  37, Train loss: 1.598, Test loss: 1.617, Test accuracy: 84.56
Round  38, Train loss: 1.594, Test loss: 1.616, Test accuracy: 84.67
Round  39, Train loss: 1.590, Test loss: 1.617, Test accuracy: 84.47
Round  40, Train loss: 1.595, Test loss: 1.617, Test accuracy: 84.38
Round  41, Train loss: 1.598, Test loss: 1.615, Test accuracy: 84.75
Round  42, Train loss: 1.585, Test loss: 1.614, Test accuracy: 84.89
Round  43, Train loss: 1.591, Test loss: 1.615, Test accuracy: 84.58
Round  44, Train loss: 1.593, Test loss: 1.614, Test accuracy: 84.78
Round  45, Train loss: 1.584, Test loss: 1.614, Test accuracy: 84.72
Round  46, Train loss: 1.583, Test loss: 1.614, Test accuracy: 84.78
Round  47, Train loss: 1.590, Test loss: 1.613, Test accuracy: 84.74
Round  48, Train loss: 1.585, Test loss: 1.613, Test accuracy: 84.83
Round  49, Train loss: 1.582, Test loss: 1.612, Test accuracy: 84.95
Round  50, Train loss: 1.587, Test loss: 1.612, Test accuracy: 84.87
Round  51, Train loss: 1.587, Test loss: 1.613, Test accuracy: 84.82
Round  52, Train loss: 1.585, Test loss: 1.612, Test accuracy: 84.90
Round  53, Train loss: 1.589, Test loss: 1.612, Test accuracy: 84.92
Round  54, Train loss: 1.581, Test loss: 1.612, Test accuracy: 84.88
Round  55, Train loss: 1.579, Test loss: 1.612, Test accuracy: 85.00
Round  56, Train loss: 1.583, Test loss: 1.611, Test accuracy: 84.90
Round  57, Train loss: 1.583, Test loss: 1.611, Test accuracy: 84.93
Round  58, Train loss: 1.589, Test loss: 1.611, Test accuracy: 84.96
Round  59, Train loss: 1.583, Test loss: 1.611, Test accuracy: 84.97
Round  60, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.04
Round  61, Train loss: 1.582, Test loss: 1.612, Test accuracy: 85.03
Round  62, Train loss: 1.581, Test loss: 1.611, Test accuracy: 85.11
Round  63, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.12
Round  64, Train loss: 1.582, Test loss: 1.610, Test accuracy: 85.12
Round  65, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.21
Round  66, Train loss: 1.581, Test loss: 1.609, Test accuracy: 85.27
Round  67, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.15
Round  68, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.31
Round  69, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.15
Round  70, Train loss: 1.574, Test loss: 1.610, Test accuracy: 85.17
Round  71, Train loss: 1.577, Test loss: 1.609, Test accuracy: 85.27
Round  72, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.25
Round  73, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.25
Round  74, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.26
Round  75, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.28
Round  76, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.26
Round  77, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.24
Round  78, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.31
Round  79, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.26
Round  80, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.37
Round  81, Train loss: 1.580, Test loss: 1.608, Test accuracy: 85.21
Round  82, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.19
Round  83, Train loss: 1.585, Test loss: 1.607, Test accuracy: 85.31
Round  84, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.29
Round  85, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.38
Round  86, Train loss: 1.580, Test loss: 1.607, Test accuracy: 85.39
Round  87, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.41
Round  88, Train loss: 1.581, Test loss: 1.607, Test accuracy: 85.46
Round  89, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.33
Round  90, Train loss: 1.570, Test loss: 1.607, Test accuracy: 85.33
Round  91, Train loss: 1.577, Test loss: 1.607, Test accuracy: 85.29
Round  92, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.45
Round  93, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.27
Round  94, Train loss: 1.568, Test loss: 1.607, Test accuracy: 85.36
Round  95, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.46
Round  96, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.44
Round  97, Train loss: 1.581, Test loss: 1.607, Test accuracy: 85.33
Round  98, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.39
Round  99, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.32
Final Round, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.45
Average accuracy final 10 rounds: 85.36416666666668
1962.2931978702545
[2.9166135787963867, 5.701465845108032, 8.430794954299927, 11.147584438323975, 13.815101861953735, 16.535646438598633, 19.146953105926514, 21.857017040252686, 24.494184255599976, 27.31550884246826, 29.9602472782135, 32.714112997055054, 35.41555666923523, 38.29101324081421, 41.06843447685242, 43.90212845802307, 46.688786029815674, 49.56585359573364, 52.409799575805664, 55.289629220962524, 58.17054510116577, 61.132683753967285, 63.785239696502686, 66.58046960830688, 69.39220476150513, 72.04444742202759, 74.8735785484314, 77.64364075660706, 80.36051297187805, 83.16076517105103, 85.93339443206787, 88.64104747772217, 91.41827964782715, 94.2054831981659, 96.93939161300659, 99.81444096565247, 102.58756995201111, 105.4806444644928, 108.41298246383667, 111.31443190574646, 114.04365396499634, 116.81951665878296, 119.46334528923035, 122.18340611457825, 124.93215417861938, 127.65069675445557, 130.40764808654785, 133.12152457237244, 135.8441183567047, 138.5625765323639, 141.28390383720398, 144.14867758750916, 146.91201281547546, 149.5974977016449, 152.37688970565796, 155.16041135787964, 157.83188128471375, 160.60110664367676, 163.34746146202087, 166.10838150978088, 168.99288630485535, 171.81706380844116, 174.5810956954956, 177.38907098770142, 180.13450980186462, 182.97094750404358, 185.7263445854187, 188.41181635856628, 191.25742840766907, 194.01888918876648, 196.70667457580566, 199.4042992591858, 202.15383791923523, 204.8887119293213, 207.66705346107483, 210.40288496017456, 213.04926800727844, 215.89037823677063, 218.66741132736206, 221.36758685112, 224.028235912323, 226.68187069892883, 229.3491997718811, 232.03138637542725, 234.53996634483337, 237.12726879119873, 239.72631216049194, 242.22225069999695, 244.77424001693726, 247.24487233161926, 249.7609624862671, 252.21740198135376, 254.74209666252136, 257.1865646839142, 259.65668749809265, 262.02766370773315, 264.3393430709839, 266.77892994880676, 269.19803524017334, 271.65176129341125, 273.67871832847595]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[36.38333333333333, 55.43333333333333, 40.725, 51.11666666666667, 66.525, 73.19166666666666, 74.7, 75.2, 75.8, 75.88333333333334, 76.01666666666667, 76.15, 76.325, 76.43333333333334, 76.39166666666667, 76.41666666666667, 81.41666666666667, 82.3, 82.55833333333334, 82.575, 83.15, 83.06666666666666, 83.39166666666667, 83.44166666666666, 83.55833333333334, 83.64166666666667, 83.75833333333334, 83.85, 83.84166666666667, 84.025, 84.06666666666666, 84.225, 84.11666666666666, 84.26666666666667, 84.29166666666667, 84.36666666666666, 84.39166666666667, 84.55833333333334, 84.66666666666667, 84.46666666666667, 84.38333333333334, 84.75, 84.89166666666667, 84.58333333333333, 84.775, 84.71666666666667, 84.78333333333333, 84.74166666666666, 84.825, 84.95, 84.86666666666666, 84.81666666666666, 84.9, 84.925, 84.875, 85.0, 84.9, 84.93333333333334, 84.95833333333333, 84.96666666666667, 85.04166666666667, 85.025, 85.10833333333333, 85.11666666666666, 85.125, 85.20833333333333, 85.26666666666667, 85.15, 85.30833333333334, 85.15, 85.175, 85.26666666666667, 85.25, 85.25, 85.25833333333334, 85.275, 85.25833333333334, 85.24166666666666, 85.30833333333334, 85.25833333333334, 85.36666666666666, 85.20833333333333, 85.19166666666666, 85.30833333333334, 85.29166666666667, 85.375, 85.39166666666667, 85.40833333333333, 85.45833333333333, 85.33333333333333, 85.33333333333333, 85.29166666666667, 85.45, 85.26666666666667, 85.35833333333333, 85.45833333333333, 85.44166666666666, 85.33333333333333, 85.39166666666667, 85.31666666666666, 85.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.312, Test loss: 2.276, Test accuracy: 29.29
Round   1, Train loss: 2.209, Test loss: 2.034, Test accuracy: 50.45
Round   2, Train loss: 1.937, Test loss: 1.876, Test accuracy: 67.10
Round   3, Train loss: 1.813, Test loss: 1.770, Test accuracy: 78.14
Round   4, Train loss: 1.717, Test loss: 1.706, Test accuracy: 82.55
Round   5, Train loss: 1.670, Test loss: 1.685, Test accuracy: 83.61
Round   6, Train loss: 1.664, Test loss: 1.656, Test accuracy: 84.75
Round   7, Train loss: 1.630, Test loss: 1.629, Test accuracy: 87.52
Round   8, Train loss: 1.601, Test loss: 1.597, Test accuracy: 90.30
Round   9, Train loss: 1.581, Test loss: 1.577, Test accuracy: 91.53
Round  10, Train loss: 1.558, Test loss: 1.568, Test accuracy: 92.36
Round  11, Train loss: 1.559, Test loss: 1.552, Test accuracy: 93.16
Round  12, Train loss: 1.548, Test loss: 1.541, Test accuracy: 94.10
Round  13, Train loss: 1.541, Test loss: 1.533, Test accuracy: 94.83
Round  14, Train loss: 1.532, Test loss: 1.530, Test accuracy: 95.07
Round  15, Train loss: 1.532, Test loss: 1.525, Test accuracy: 95.48
Round  16, Train loss: 1.523, Test loss: 1.522, Test accuracy: 95.70
Round  17, Train loss: 1.518, Test loss: 1.520, Test accuracy: 95.80
Round  18, Train loss: 1.516, Test loss: 1.518, Test accuracy: 95.95
Round  19, Train loss: 1.511, Test loss: 1.516, Test accuracy: 96.14
Round  20, Train loss: 1.515, Test loss: 1.514, Test accuracy: 96.30
Round  21, Train loss: 1.507, Test loss: 1.513, Test accuracy: 96.41
Round  22, Train loss: 1.506, Test loss: 1.511, Test accuracy: 96.50
Round  23, Train loss: 1.505, Test loss: 1.510, Test accuracy: 96.57
Round  24, Train loss: 1.503, Test loss: 1.509, Test accuracy: 96.61
Round  25, Train loss: 1.501, Test loss: 1.508, Test accuracy: 96.63
Round  26, Train loss: 1.501, Test loss: 1.506, Test accuracy: 96.68
Round  27, Train loss: 1.499, Test loss: 1.505, Test accuracy: 96.80
Round  28, Train loss: 1.495, Test loss: 1.505, Test accuracy: 96.84
Round  29, Train loss: 1.495, Test loss: 1.503, Test accuracy: 96.88
Round  30, Train loss: 1.492, Test loss: 1.503, Test accuracy: 97.00
Round  31, Train loss: 1.490, Test loss: 1.502, Test accuracy: 97.04
Round  32, Train loss: 1.493, Test loss: 1.501, Test accuracy: 97.06
Round  33, Train loss: 1.491, Test loss: 1.500, Test accuracy: 97.17
Round  34, Train loss: 1.490, Test loss: 1.500, Test accuracy: 97.17
Round  35, Train loss: 1.487, Test loss: 1.500, Test accuracy: 97.17
Round  36, Train loss: 1.485, Test loss: 1.500, Test accuracy: 97.23
Round  37, Train loss: 1.484, Test loss: 1.499, Test accuracy: 97.30
Round  38, Train loss: 1.485, Test loss: 1.499, Test accuracy: 97.33
Round  39, Train loss: 1.485, Test loss: 1.498, Test accuracy: 97.36
Round  40, Train loss: 1.486, Test loss: 1.498, Test accuracy: 97.36
Round  41, Train loss: 1.488, Test loss: 1.497, Test accuracy: 97.39
Round  42, Train loss: 1.482, Test loss: 1.497, Test accuracy: 97.43
Round  43, Train loss: 1.483, Test loss: 1.496, Test accuracy: 97.47
Round  44, Train loss: 1.479, Test loss: 1.496, Test accuracy: 97.45
Round  45, Train loss: 1.481, Test loss: 1.496, Test accuracy: 97.50
Round  46, Train loss: 1.483, Test loss: 1.495, Test accuracy: 97.55
Round  47, Train loss: 1.482, Test loss: 1.495, Test accuracy: 97.56
Round  48, Train loss: 1.480, Test loss: 1.495, Test accuracy: 97.56
Round  49, Train loss: 1.480, Test loss: 1.495, Test accuracy: 97.56
Round  50, Train loss: 1.481, Test loss: 1.494, Test accuracy: 97.61
Round  51, Train loss: 1.480, Test loss: 1.494, Test accuracy: 97.61
Round  52, Train loss: 1.479, Test loss: 1.494, Test accuracy: 97.56
Round  53, Train loss: 1.478, Test loss: 1.494, Test accuracy: 97.60
Round  54, Train loss: 1.478, Test loss: 1.494, Test accuracy: 97.56
Round  55, Train loss: 1.478, Test loss: 1.493, Test accuracy: 97.63
Round  56, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.63
Round  57, Train loss: 1.476, Test loss: 1.493, Test accuracy: 97.61
Round  58, Train loss: 1.476, Test loss: 1.493, Test accuracy: 97.64
Round  59, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.71
Round  60, Train loss: 1.476, Test loss: 1.493, Test accuracy: 97.70
Round  61, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.68
Round  62, Train loss: 1.475, Test loss: 1.492, Test accuracy: 97.69
Round  63, Train loss: 1.476, Test loss: 1.492, Test accuracy: 97.70
Round  64, Train loss: 1.474, Test loss: 1.492, Test accuracy: 97.71
Round  65, Train loss: 1.476, Test loss: 1.492, Test accuracy: 97.76
Round  66, Train loss: 1.474, Test loss: 1.492, Test accuracy: 97.72
Round  67, Train loss: 1.473, Test loss: 1.492, Test accuracy: 97.70
Round  68, Train loss: 1.474, Test loss: 1.491, Test accuracy: 97.72
Round  69, Train loss: 1.474, Test loss: 1.491, Test accuracy: 97.76
Round  70, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.78
Round  71, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.75
Round  72, Train loss: 1.473, Test loss: 1.491, Test accuracy: 97.76
Round  73, Train loss: 1.473, Test loss: 1.491, Test accuracy: 97.76
Round  74, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.75
Round  75, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.73
Round  76, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.75
Round  77, Train loss: 1.471, Test loss: 1.491, Test accuracy: 97.76
Round  78, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.78
Round  79, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.82
Round  80, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.82
Round  81, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.77
Round  82, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.82
Round  83, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.84
Round  84, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.83
Round  85, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.83
Round  86, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.81
Round  87, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.83
Round  88, Train loss: 1.469, Test loss: 1.490, Test accuracy: 97.82
Round  89, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.84
Round  90, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.85
Round  91, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.86
Round  92, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.89
Round  93, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.90
Round  94, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.88/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.85
Round  96, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.89
Round  97, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.89
Round  98, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.93
Round  99, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.93
Final Round, Train loss: 1.468, Test loss: 1.489, Test accuracy: 97.88
Average accuracy final 10 rounds: 97.88575
4065.6579229831696
[4.258806943893433, 9.135666847229004, 13.919945478439331, 18.659745454788208, 23.554295778274536, 28.088661193847656, 32.66466665267944, 37.40535044670105, 42.12899971008301, 46.885504484176636, 51.64134359359741, 56.411473512649536, 61.27231979370117, 66.07889938354492, 70.81152534484863, 75.50058603286743, 80.30264282226562, 85.15670347213745, 89.72140741348267, 94.27651929855347, 98.81743383407593, 103.32467818260193, 108.13547039031982, 112.78366947174072, 117.44611382484436, 122.2769525051117, 127.11836671829224, 132.0878872871399, 136.7537145614624, 141.57327508926392, 146.55067610740662, 151.09959292411804, 155.92909598350525, 160.50071740150452, 165.2265944480896, 170.09798574447632, 174.79245495796204, 179.88526439666748, 185.03010535240173, 189.83518958091736, 195.02642393112183, 200.17092561721802, 205.07381224632263, 210.18011593818665, 215.12967014312744, 219.97485399246216, 225.04704666137695, 229.821843624115, 235.0090367794037, 239.99924969673157, 244.88504147529602, 249.85563945770264, 254.75758528709412, 259.6139886379242, 264.46868920326233, 269.4775640964508, 274.344580411911, 279.16671681404114, 283.978812456131, 288.9727215766907, 293.8784439563751, 298.81807374954224, 303.7026436328888, 308.7263469696045, 313.5924620628357, 318.55541491508484, 323.37658286094666, 328.2840015888214, 333.20031476020813, 337.9790642261505, 342.74918031692505, 347.65023398399353, 352.562344789505, 357.4730894565582, 362.5315773487091, 367.57241654396057, 372.51822447776794, 377.3701570034027, 382.35587072372437, 387.26207542419434, 392.24706411361694, 397.06879019737244, 401.93450903892517, 406.9589014053345, 411.96694779396057, 416.70707273483276, 421.7163519859314, 426.5905590057373, 431.4868173599243, 436.6483163833618, 441.75985741615295, 446.38347363471985, 451.36948800086975, 456.3048770427704, 461.0505177974701, 465.984171628952, 470.9807913303375, 475.9226450920105, 480.887642621994, 485.8836131095886, 487.8531684875488]
[29.29, 50.4525, 67.1, 78.1375, 82.5475, 83.6125, 84.75, 87.52, 90.3025, 91.535, 92.365, 93.1625, 94.1, 94.8275, 95.0725, 95.4825, 95.705, 95.8025, 95.955, 96.1425, 96.2975, 96.405, 96.4975, 96.57, 96.615, 96.6275, 96.6825, 96.8025, 96.8425, 96.88, 97.0, 97.0425, 97.055, 97.175, 97.1725, 97.1725, 97.235, 97.295, 97.325, 97.355, 97.36, 97.3925, 97.4275, 97.465, 97.4525, 97.5025, 97.545, 97.565, 97.555, 97.5575, 97.6125, 97.6075, 97.5625, 97.5975, 97.5625, 97.6275, 97.63, 97.6125, 97.6375, 97.7075, 97.6975, 97.6825, 97.685, 97.7, 97.7125, 97.7575, 97.725, 97.7, 97.715, 97.76, 97.78, 97.7525, 97.76, 97.7575, 97.75, 97.7325, 97.7475, 97.76, 97.7825, 97.8225, 97.8225, 97.77, 97.8175, 97.8375, 97.8275, 97.8325, 97.8125, 97.83, 97.82, 97.8375, 97.8475, 97.855, 97.895, 97.9025, 97.875, 97.85, 97.885, 97.8925, 97.9275, 97.9275, 97.8825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.313, Test loss: 2.278, Test accuracy: 37.90
Round   1, Train loss: 2.218, Test loss: 2.075, Test accuracy: 43.07
Round   2, Train loss: 2.002, Test loss: 1.913, Test accuracy: 65.47
Round   3, Train loss: 1.856, Test loss: 1.816, Test accuracy: 74.91
Round   4, Train loss: 1.788, Test loss: 1.764, Test accuracy: 80.35
Round   5, Train loss: 1.737, Test loss: 1.728, Test accuracy: 82.84
Round   6, Train loss: 1.704, Test loss: 1.679, Test accuracy: 89.27
Round   7, Train loss: 1.661, Test loss: 1.630, Test accuracy: 91.64
Round   8, Train loss: 1.631, Test loss: 1.613, Test accuracy: 92.72
Round   9, Train loss: 1.607, Test loss: 1.605, Test accuracy: 93.49
Round  10, Train loss: 1.618, Test loss: 1.574, Test accuracy: 94.22
Round  11, Train loss: 1.581, Test loss: 1.576, Test accuracy: 94.69
Round  12, Train loss: 1.594, Test loss: 1.559, Test accuracy: 94.94
Round  13, Train loss: 1.580, Test loss: 1.551, Test accuracy: 95.36
Round  14, Train loss: 1.568, Test loss: 1.548, Test accuracy: 95.55
Round  15, Train loss: 1.564, Test loss: 1.544, Test accuracy: 95.83
Round  16, Train loss: 1.565, Test loss: 1.536, Test accuracy: 96.03
Round  17, Train loss: 1.552, Test loss: 1.535, Test accuracy: 96.24
Round  18, Train loss: 1.547, Test loss: 1.533, Test accuracy: 96.39
Round  19, Train loss: 1.544, Test loss: 1.531, Test accuracy: 96.53
Round  20, Train loss: 1.547, Test loss: 1.528, Test accuracy: 96.66
Round  21, Train loss: 1.537, Test loss: 1.528, Test accuracy: 96.62
Round  22, Train loss: 1.537, Test loss: 1.526, Test accuracy: 96.71
Round  23, Train loss: 1.534, Test loss: 1.524, Test accuracy: 96.69
Round  24, Train loss: 1.537, Test loss: 1.519, Test accuracy: 96.79
Round  25, Train loss: 1.527, Test loss: 1.521, Test accuracy: 96.83
Round  26, Train loss: 1.531, Test loss: 1.518, Test accuracy: 96.98
Round  27, Train loss: 1.525, Test loss: 1.518, Test accuracy: 97.11
Round  28, Train loss: 1.523, Test loss: 1.516, Test accuracy: 97.11
Round  29, Train loss: 1.521, Test loss: 1.517, Test accuracy: 97.10
Round  30, Train loss: 1.521, Test loss: 1.514, Test accuracy: 97.16
Round  31, Train loss: 1.518, Test loss: 1.513, Test accuracy: 97.17
Round  32, Train loss: 1.516, Test loss: 1.513, Test accuracy: 97.29
Round  33, Train loss: 1.516, Test loss: 1.512, Test accuracy: 97.36
Round  34, Train loss: 1.517, Test loss: 1.511, Test accuracy: 97.39
Round  35, Train loss: 1.514, Test loss: 1.510, Test accuracy: 97.41
Round  36, Train loss: 1.510, Test loss: 1.511, Test accuracy: 97.41
Round  37, Train loss: 1.511, Test loss: 1.510, Test accuracy: 97.45
Round  38, Train loss: 1.511, Test loss: 1.509, Test accuracy: 97.38
Round  39, Train loss: 1.511, Test loss: 1.508, Test accuracy: 97.45
Round  40, Train loss: 1.506, Test loss: 1.509, Test accuracy: 97.52
Round  41, Train loss: 1.508, Test loss: 1.507, Test accuracy: 97.59
Round  42, Train loss: 1.507, Test loss: 1.507, Test accuracy: 97.47
Round  43, Train loss: 1.504, Test loss: 1.507, Test accuracy: 97.59
Round  44, Train loss: 1.504, Test loss: 1.506, Test accuracy: 97.62
Round  45, Train loss: 1.504, Test loss: 1.505, Test accuracy: 97.64
Round  46, Train loss: 1.503, Test loss: 1.505, Test accuracy: 97.73
Round  47, Train loss: 1.505, Test loss: 1.504, Test accuracy: 97.62
Round  48, Train loss: 1.502, Test loss: 1.504, Test accuracy: 97.61
Round  49, Train loss: 1.501, Test loss: 1.504, Test accuracy: 97.72
Round  50, Train loss: 1.501, Test loss: 1.504, Test accuracy: 97.78
Round  51, Train loss: 1.499, Test loss: 1.504, Test accuracy: 97.82
Round  52, Train loss: 1.500, Test loss: 1.504, Test accuracy: 97.72
Round  53, Train loss: 1.502, Test loss: 1.502, Test accuracy: 97.81
Round  54, Train loss: 1.500, Test loss: 1.502, Test accuracy: 97.79
Round  55, Train loss: 1.498, Test loss: 1.502, Test accuracy: 97.81
Round  56, Train loss: 1.498, Test loss: 1.502, Test accuracy: 97.84
Round  57, Train loss: 1.497, Test loss: 1.502, Test accuracy: 97.83
Round  58, Train loss: 1.498, Test loss: 1.501, Test accuracy: 97.85
Round  59, Train loss: 1.499, Test loss: 1.501, Test accuracy: 97.86
Round  60, Train loss: 1.495, Test loss: 1.501, Test accuracy: 97.91
Round  61, Train loss: 1.494, Test loss: 1.501, Test accuracy: 97.86
Round  62, Train loss: 1.495, Test loss: 1.501, Test accuracy: 97.89
Round  63, Train loss: 1.497, Test loss: 1.500, Test accuracy: 97.93
Round  64, Train loss: 1.495, Test loss: 1.500, Test accuracy: 97.91
Round  65, Train loss: 1.496, Test loss: 1.499, Test accuracy: 97.94
Round  66, Train loss: 1.492, Test loss: 1.500, Test accuracy: 98.00
Round  67, Train loss: 1.493, Test loss: 1.500, Test accuracy: 97.94
Round  68, Train loss: 1.495, Test loss: 1.499, Test accuracy: 97.99
Round  69, Train loss: 1.496, Test loss: 1.499, Test accuracy: 97.97
Round  70, Train loss: 1.493, Test loss: 1.499, Test accuracy: 97.95
Round  71, Train loss: 1.493, Test loss: 1.499, Test accuracy: 97.98
Round  72, Train loss: 1.492, Test loss: 1.499, Test accuracy: 97.99
Round  73, Train loss: 1.490, Test loss: 1.499, Test accuracy: 98.03
Round  74, Train loss: 1.493, Test loss: 1.498, Test accuracy: 98.06
Round  75, Train loss: 1.492, Test loss: 1.498, Test accuracy: 98.07
Round  76, Train loss: 1.490, Test loss: 1.498, Test accuracy: 98.10
Round  77, Train loss: 1.490, Test loss: 1.498, Test accuracy: 98.04
Round  78, Train loss: 1.489, Test loss: 1.499, Test accuracy: 98.04
Round  79, Train loss: 1.491, Test loss: 1.498, Test accuracy: 98.02
Round  80, Train loss: 1.491, Test loss: 1.498, Test accuracy: 97.98
Round  81, Train loss: 1.490, Test loss: 1.497, Test accuracy: 98.05
Round  82, Train loss: 1.490, Test loss: 1.498, Test accuracy: 98.02
Round  83, Train loss: 1.490, Test loss: 1.497, Test accuracy: 98.01
Round  84, Train loss: 1.489, Test loss: 1.497, Test accuracy: 98.04
Round  85, Train loss: 1.489, Test loss: 1.497, Test accuracy: 98.03
Round  86, Train loss: 1.489, Test loss: 1.498, Test accuracy: 98.03
Round  87, Train loss: 1.491, Test loss: 1.496, Test accuracy: 97.98
Round  88, Train loss: 1.489, Test loss: 1.497, Test accuracy: 97.99
Round  89, Train loss: 1.488, Test loss: 1.497, Test accuracy: 98.02
Round  90, Train loss: 1.488, Test loss: 1.497, Test accuracy: 97.99
Round  91, Train loss: 1.489, Test loss: 1.497, Test accuracy: 98.03
Round  92, Train loss: 1.488, Test loss: 1.497, Test accuracy: 98.08
Round  93, Train loss: 1.488, Test loss: 1.497, Test accuracy: 98.03/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.489, Test loss: 1.496, Test accuracy: 98.02
Round  95, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.02
Round  96, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.01
Round  97, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.02
Round  98, Train loss: 1.486, Test loss: 1.496, Test accuracy: 98.05
Round  99, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.03
Final Round, Train loss: 1.471, Test loss: 1.495, Test accuracy: 98.02
Average accuracy final 10 rounds: 98.02825
5641.264214038849
[5.142660856246948, 10.285321712493896, 15.044090270996094, 19.80285882949829, 24.437625646591187, 29.072392463684082, 33.807934284210205, 38.54347610473633, 43.29310154914856, 48.04272699356079, 52.74139618873596, 57.44006538391113, 62.2307026386261, 67.02133989334106, 71.68405938148499, 76.3467788696289, 81.12433528900146, 85.90189170837402, 90.67327308654785, 95.44465446472168, 100.0849916934967, 104.72532892227173, 109.47695827484131, 114.22858762741089, 118.85130858421326, 123.47402954101562, 128.21656894683838, 132.95910835266113, 138.05643463134766, 143.15376091003418, 148.24607253074646, 153.33838415145874, 158.57029390335083, 163.80220365524292, 168.89083552360535, 173.97946739196777, 178.96956729888916, 183.95966720581055, 188.88198733329773, 193.8043074607849, 198.92466187477112, 204.04501628875732, 209.18075275421143, 214.31648921966553, 219.27853393554688, 224.24057865142822, 229.19767951965332, 234.15478038787842, 239.18778109550476, 244.2207818031311, 249.26553440093994, 254.31028699874878, 259.57146859169006, 264.83265018463135, 269.84436798095703, 274.8560857772827, 279.90522146224976, 284.9543571472168, 289.71537160873413, 294.47638607025146, 299.41870951652527, 304.3610329627991, 309.5144565105438, 314.6678800582886, 319.7549777030945, 324.8420753479004, 329.828200340271, 334.8143253326416, 339.67829608917236, 344.5422668457031, 349.46812176704407, 354.393976688385, 359.20357394218445, 364.0131711959839, 368.963862657547, 373.9145541191101, 378.67484974861145, 383.4351453781128, 388.19992089271545, 392.9646964073181, 397.82954573631287, 402.6943950653076, 407.7163333892822, 412.73827171325684, 417.6678891181946, 422.5975065231323, 427.32967925071716, 432.061851978302, 437.0623686313629, 442.0628852844238, 447.0706226825714, 452.078360080719, 457.00001764297485, 461.9216752052307, 466.6787989139557, 471.43592262268066, 476.18525314331055, 480.93458366394043, 486.021488904953, 491.1083941459656, 496.3482496738434, 501.5881052017212, 506.7235288619995, 511.85895252227783, 516.942587852478, 522.0262231826782, 527.0688395500183, 532.1114559173584, 537.1208183765411, 542.1301808357239, 547.1173183917999, 552.104455947876, 557.0703837871552, 562.0363116264343, 567.0209717750549, 572.0056319236755, 576.8366861343384, 581.6677403450012, 586.5764253139496, 591.485110282898, 596.5684745311737, 601.6518387794495, 606.7627131938934, 611.8735876083374, 616.9580178260803, 622.0424480438232, 627.0408024787903, 632.0391569137573, 637.0106530189514, 641.9821491241455, 646.8092725276947, 651.6363959312439, 656.6392018795013, 661.6420078277588, 666.613495349884, 671.5849828720093, 676.6414198875427, 681.6978569030762, 686.592217206955, 691.4865775108337, 696.3758065700531, 701.2650356292725, 706.0847380161285, 710.9044404029846, 715.9765250682831, 721.0486097335815, 726.0760095119476, 731.1034092903137, 735.9815702438354, 740.8597311973572, 746.0175940990448, 751.1754570007324, 756.248197555542, 761.3209381103516, 766.1251785755157, 770.9294190406799, 776.0238146781921, 781.1182103157043, 786.1613342761993, 791.2044582366943, 796.033243894577, 800.8620295524597, 805.8255779743195, 810.7891263961792, 815.9829812049866, 821.176836013794, 826.056788444519, 830.9367408752441, 835.8677105903625, 840.798680305481, 845.9573028087616, 851.1159253120422, 856.3972098827362, 861.6784944534302, 866.8239042758942, 871.9693140983582, 876.9410855770111, 881.9128570556641, 886.8435614109039, 891.7742657661438, 896.5229852199554, 901.2717046737671, 906.1796281337738, 911.0875515937805, 916.0722362995148, 921.056921005249, 926.1925911903381, 931.3282613754272, 936.3957207202911, 941.463180065155, 946.4913566112518, 951.5195331573486, 956.5628724098206, 961.6062116622925, 966.6823327541351, 971.7584538459778, 976.8554196357727, 981.9523854255676, 986.784656047821, 991.6169266700745, 993.6165988445282, 995.6162710189819]
[37.895, 37.895, 43.0725, 43.0725, 65.4675, 65.4675, 74.9075, 74.9075, 80.35, 80.35, 82.845, 82.845, 89.265, 89.265, 91.635, 91.635, 92.7225, 92.7225, 93.49, 93.49, 94.2225, 94.2225, 94.69, 94.69, 94.935, 94.935, 95.3575, 95.3575, 95.55, 95.55, 95.8275, 95.8275, 96.03, 96.03, 96.24, 96.24, 96.395, 96.395, 96.53, 96.53, 96.6625, 96.6625, 96.625, 96.625, 96.7125, 96.7125, 96.695, 96.695, 96.7875, 96.7875, 96.83, 96.83, 96.985, 96.985, 97.105, 97.105, 97.1075, 97.1075, 97.1, 97.1, 97.1575, 97.1575, 97.175, 97.175, 97.29, 97.29, 97.3575, 97.3575, 97.395, 97.395, 97.405, 97.405, 97.41, 97.41, 97.45, 97.45, 97.375, 97.375, 97.4475, 97.4475, 97.515, 97.515, 97.595, 97.595, 97.47, 97.47, 97.5925, 97.5925, 97.6175, 97.6175, 97.6425, 97.6425, 97.7275, 97.7275, 97.6175, 97.6175, 97.615, 97.615, 97.725, 97.725, 97.78, 97.78, 97.8175, 97.8175, 97.7175, 97.7175, 97.815, 97.815, 97.7875, 97.7875, 97.81, 97.81, 97.8425, 97.8425, 97.835, 97.835, 97.85, 97.85, 97.855, 97.855, 97.9075, 97.9075, 97.86, 97.86, 97.89, 97.89, 97.9275, 97.9275, 97.905, 97.905, 97.935, 97.935, 98.0, 98.0, 97.935, 97.935, 97.9925, 97.9925, 97.965, 97.965, 97.95, 97.95, 97.98, 97.98, 97.9875, 97.9875, 98.0275, 98.0275, 98.055, 98.055, 98.07, 98.07, 98.1, 98.1, 98.04, 98.04, 98.0425, 98.0425, 98.0175, 98.0175, 97.9775, 97.9775, 98.0525, 98.0525, 98.015, 98.015, 98.01, 98.01, 98.0425, 98.0425, 98.0325, 98.0325, 98.035, 98.035, 97.9825, 97.9825, 97.9875, 97.9875, 98.0175, 98.0175, 97.9875, 97.9875, 98.03, 98.03, 98.085, 98.085, 98.03, 98.03, 98.02, 98.02, 98.0175, 98.0175, 98.01, 98.01, 98.0175, 98.0175, 98.05, 98.05, 98.035, 98.035, 98.0225, 98.0225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.189, Test loss: 2.135, Test accuracy: 46.78
Round   0, Global train loss: 2.189, Global test loss: 2.248, Global test accuracy: 40.11
Round   1, Train loss: 1.872, Test loss: 1.905, Test accuracy: 62.91
Round   1, Global train loss: 1.872, Global test loss: 2.106, Global test accuracy: 43.58
Round   2, Train loss: 1.628, Test loss: 1.764, Test accuracy: 73.74
Round   2, Global train loss: 1.628, Global test loss: 2.055, Global test accuracy: 43.01
Round   3, Train loss: 1.507, Test loss: 1.730, Test accuracy: 75.08
Round   3, Global train loss: 1.507, Global test loss: 2.133, Global test accuracy: 32.49
Round   4, Train loss: 1.528, Test loss: 1.680, Test accuracy: 81.71
Round   4, Global train loss: 1.528, Global test loss: 2.094, Global test accuracy: 41.36
Round   5, Train loss: 1.502, Test loss: 1.723, Test accuracy: 74.17
Round   5, Global train loss: 1.502, Global test loss: 2.235, Global test accuracy: 21.03
Round   6, Train loss: 1.594, Test loss: 1.646, Test accuracy: 83.16
Round   6, Global train loss: 1.594, Global test loss: 2.016, Global test accuracy: 48.42
Round   7, Train loss: 1.557, Test loss: 1.607, Test accuracy: 86.57
Round   7, Global train loss: 1.557, Global test loss: 2.044, Global test accuracy: 41.50
Round   8, Train loss: 1.639, Test loss: 1.567, Test accuracy: 90.39
Round   8, Global train loss: 1.639, Global test loss: 2.015, Global test accuracy: 45.27
Round   9, Train loss: 1.590, Test loss: 1.561, Test accuracy: 90.48
Round   9, Global train loss: 1.590, Global test loss: 2.057, Global test accuracy: 39.63
Round  10, Train loss: 1.481, Test loss: 1.567, Test accuracy: 91.65
Round  10, Global train loss: 1.481, Global test loss: 2.094, Global test accuracy: 48.10
Round  11, Train loss: 1.491, Test loss: 1.546, Test accuracy: 91.86
Round  11, Global train loss: 1.491, Global test loss: 2.053, Global test accuracy: 44.57
Round  12, Train loss: 1.500, Test loss: 1.531, Test accuracy: 93.39
Round  12, Global train loss: 1.500, Global test loss: 2.096, Global test accuracy: 40.41
Round  13, Train loss: 1.503, Test loss: 1.521, Test accuracy: 94.53
Round  13, Global train loss: 1.503, Global test loss: 2.141, Global test accuracy: 30.52
Round  14, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.58
Round  14, Global train loss: 1.478, Global test loss: 2.076, Global test accuracy: 34.43
Round  15, Train loss: 1.479, Test loss: 1.518, Test accuracy: 94.75
Round  15, Global train loss: 1.479, Global test loss: 2.088, Global test accuracy: 40.64
Round  16, Train loss: 1.525, Test loss: 1.518, Test accuracy: 94.72
Round  16, Global train loss: 1.525, Global test loss: 2.065, Global test accuracy: 48.98
Round  17, Train loss: 1.522, Test loss: 1.518, Test accuracy: 94.71
Round  17, Global train loss: 1.522, Global test loss: 2.082, Global test accuracy: 42.44
Round  18, Train loss: 1.471, Test loss: 1.518, Test accuracy: 94.69
Round  18, Global train loss: 1.471, Global test loss: 2.045, Global test accuracy: 39.62
Round  19, Train loss: 1.525, Test loss: 1.517, Test accuracy: 94.72
Round  19, Global train loss: 1.525, Global test loss: 2.036, Global test accuracy: 42.08
Round  20, Train loss: 1.471, Test loss: 1.517, Test accuracy: 94.77
Round  20, Global train loss: 1.471, Global test loss: 2.050, Global test accuracy: 40.53
Round  21, Train loss: 1.472, Test loss: 1.517, Test accuracy: 94.74
Round  21, Global train loss: 1.472, Global test loss: 1.978, Global test accuracy: 50.65
Round  22, Train loss: 1.467, Test loss: 1.516, Test accuracy: 94.79
Round  22, Global train loss: 1.467, Global test loss: 2.068, Global test accuracy: 38.83
Round  23, Train loss: 1.469, Test loss: 1.516, Test accuracy: 94.85
Round  23, Global train loss: 1.469, Global test loss: 2.048, Global test accuracy: 49.86
Round  24, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.87
Round  24, Global train loss: 1.522, Global test loss: 2.213, Global test accuracy: 21.55
Round  25, Train loss: 1.471, Test loss: 1.516, Test accuracy: 94.83
Round  25, Global train loss: 1.471, Global test loss: 1.966, Global test accuracy: 55.07
Round  26, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.86
Round  26, Global train loss: 1.467, Global test loss: 2.137, Global test accuracy: 28.52
Round  27, Train loss: 1.522, Test loss: 1.515, Test accuracy: 94.88
Round  27, Global train loss: 1.522, Global test loss: 2.070, Global test accuracy: 42.42
Round  28, Train loss: 1.471, Test loss: 1.515, Test accuracy: 94.89
Round  28, Global train loss: 1.471, Global test loss: 2.122, Global test accuracy: 31.80
Round  29, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.86
Round  29, Global train loss: 1.467, Global test loss: 2.032, Global test accuracy: 42.52
Round  30, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.83
Round  30, Global train loss: 1.469, Global test loss: 2.042, Global test accuracy: 43.09
Round  31, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.83
Round  31, Global train loss: 1.467, Global test loss: 1.992, Global test accuracy: 49.28
Round  32, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.83
Round  32, Global train loss: 1.469, Global test loss: 2.068, Global test accuracy: 33.58
Round  33, Train loss: 1.470, Test loss: 1.515, Test accuracy: 94.84
Round  33, Global train loss: 1.470, Global test loss: 2.178, Global test accuracy: 26.13
Round  34, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.82
Round  34, Global train loss: 1.469, Global test loss: 2.077, Global test accuracy: 38.85
Round  35, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.85
Round  35, Global train loss: 1.468, Global test loss: 2.088, Global test accuracy: 33.26
Round  36, Train loss: 1.467, Test loss: 1.514, Test accuracy: 94.82
Round  36, Global train loss: 1.467, Global test loss: 2.057, Global test accuracy: 37.43
Round  37, Train loss: 1.521, Test loss: 1.514, Test accuracy: 94.81
Round  37, Global train loss: 1.521, Global test loss: 1.977, Global test accuracy: 46.81
Round  38, Train loss: 1.469, Test loss: 1.514, Test accuracy: 94.81
Round  38, Global train loss: 1.469, Global test loss: 2.078, Global test accuracy: 45.12
Round  39, Train loss: 1.520, Test loss: 1.514, Test accuracy: 94.84
Round  39, Global train loss: 1.520, Global test loss: 2.024, Global test accuracy: 42.52
Round  40, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.86
Round  40, Global train loss: 1.468, Global test loss: 2.056, Global test accuracy: 56.56
Round  41, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.84
Round  41, Global train loss: 1.468, Global test loss: 2.115, Global test accuracy: 35.08
Round  42, Train loss: 1.467, Test loss: 1.514, Test accuracy: 94.86
Round  42, Global train loss: 1.467, Global test loss: 1.985, Global test accuracy: 48.52
Round  43, Train loss: 1.467, Test loss: 1.514, Test accuracy: 94.86
Round  43, Global train loss: 1.467, Global test loss: 2.222, Global test accuracy: 21.31
Round  44, Train loss: 1.521, Test loss: 1.514, Test accuracy: 94.84
Round  44, Global train loss: 1.521, Global test loss: 2.031, Global test accuracy: 42.62
Round  45, Train loss: 1.521, Test loss: 1.514, Test accuracy: 94.85
Round  45, Global train loss: 1.521, Global test loss: 2.071, Global test accuracy: 40.25
Round  46, Train loss: 1.519, Test loss: 1.514, Test accuracy: 94.84
Round  46, Global train loss: 1.519, Global test loss: 2.091, Global test accuracy: 33.14
Round  47, Train loss: 1.519, Test loss: 1.514, Test accuracy: 94.85
Round  47, Global train loss: 1.519, Global test loss: 2.022, Global test accuracy: 42.66
Round  48, Train loss: 1.522, Test loss: 1.514, Test accuracy: 94.86
Round  48, Global train loss: 1.522, Global test loss: 2.029, Global test accuracy: 42.52
Round  49, Train loss: 1.466, Test loss: 1.514, Test accuracy: 94.83
Round  49, Global train loss: 1.466, Global test loss: 2.195, Global test accuracy: 26.44
Round  50, Train loss: 1.470, Test loss: 1.514, Test accuracy: 94.83
Round  50, Global train loss: 1.470, Global test loss: 2.140, Global test accuracy: 34.96
Round  51, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.83
Round  51, Global train loss: 1.468, Global test loss: 2.109, Global test accuracy: 31.50
Round  52, Train loss: 1.469, Test loss: 1.514, Test accuracy: 94.88
Round  52, Global train loss: 1.469, Global test loss: 2.195, Global test accuracy: 21.29
Round  53, Train loss: 1.470, Test loss: 1.514, Test accuracy: 94.90
Round  53, Global train loss: 1.470, Global test loss: 2.073, Global test accuracy: 40.35
Round  54, Train loss: 1.466, Test loss: 1.514, Test accuracy: 94.88
Round  54, Global train loss: 1.466, Global test loss: 2.015, Global test accuracy: 41.91
Round  55, Train loss: 1.470, Test loss: 1.514, Test accuracy: 94.90
Round  55, Global train loss: 1.470, Global test loss: 1.934, Global test accuracy: 53.49
Round  56, Train loss: 1.465, Test loss: 1.514, Test accuracy: 94.88
Round  56, Global train loss: 1.465, Global test loss: 2.057, Global test accuracy: 40.18
Round  57, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.93
Round  57, Global train loss: 1.468, Global test loss: 2.066, Global test accuracy: 45.77
Round  58, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  58, Global train loss: 1.467, Global test loss: 2.074, Global test accuracy: 39.14
Round  59, Train loss: 1.518, Test loss: 1.513, Test accuracy: 94.90
Round  59, Global train loss: 1.518, Global test loss: 1.997, Global test accuracy: 47.20
Round  60, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.91
Round  60, Global train loss: 1.468, Global test loss: 2.067, Global test accuracy: 43.90
Round  61, Train loss: 1.521, Test loss: 1.513, Test accuracy: 94.92
Round  61, Global train loss: 1.521, Global test loss: 2.022, Global test accuracy: 40.94
Round  62, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  62, Global train loss: 1.466, Global test loss: 2.081, Global test accuracy: 40.07
Round  63, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.93
Round  63, Global train loss: 1.468, Global test loss: 2.014, Global test accuracy: 52.92
Round  64, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  64, Global train loss: 1.467, Global test loss: 2.037, Global test accuracy: 42.60
Round  65, Train loss: 1.520, Test loss: 1.513, Test accuracy: 94.92
Round  65, Global train loss: 1.520, Global test loss: 2.058, Global test accuracy: 41.58
Round  66, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  66, Global train loss: 1.467, Global test loss: 2.126, Global test accuracy: 36.23
Round  67, Train loss: 1.518, Test loss: 1.513, Test accuracy: 94.91
Round  67, Global train loss: 1.518, Global test loss: 2.044, Global test accuracy: 41.83
Round  68, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  68, Global train loss: 1.466, Global test loss: 2.129, Global test accuracy: 35.77
Round  69, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.92
Round  69, Global train loss: 1.468, Global test loss: 2.073, Global test accuracy: 43.75
Round  70, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  70, Global train loss: 1.467, Global test loss: 2.112, Global test accuracy: 33.42
Round  71, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.90
Round  71, Global train loss: 1.466, Global test loss: 2.094, Global test accuracy: 42.43
Round  72, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.91
Round  72, Global train loss: 1.466, Global test loss: 1.980, Global test accuracy: 51.45
Round  73, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.92
Round  73, Global train loss: 1.465, Global test loss: 1.987, Global test accuracy: 50.17
Round  74, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.91
Round  74, Global train loss: 1.466, Global test loss: 2.065, Global test accuracy: 38.45
Round  75, Train loss: 1.522, Test loss: 1.513, Test accuracy: 94.91
Round  75, Global train loss: 1.522, Global test loss: 1.977, Global test accuracy: 48.99
Round  76, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  76, Global train loss: 1.467, Global test loss: 2.077, Global test accuracy: 36.02
Round  77, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  77, Global train loss: 1.467, Global test loss: 2.015, Global test accuracy: 45.07
Round  78, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  78, Global train loss: 1.467, Global test loss: 2.115, Global test accuracy: 31.48
Round  79, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  79, Global train loss: 1.467, Global test loss: 2.035, Global test accuracy: 47.30
Round  80, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  80, Global train loss: 1.466, Global test loss: 2.135, Global test accuracy: 42.07
Round  81, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.92
Round  81, Global train loss: 1.465, Global test loss: 2.038, Global test accuracy: 43.31
Round  82, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  82, Global train loss: 1.467, Global test loss: 1.994, Global test accuracy: 51.80
Round  83, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.92
Round  83, Global train loss: 1.465, Global test loss: 1.980, Global test accuracy: 48.42
Round  84, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  84, Global train loss: 1.466, Global test loss: 2.080, Global test accuracy: 37.26
Round  85, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.93
Round  85, Global train loss: 1.466, Global test loss: 2.107, Global test accuracy: 32.42
Round  86, Train loss: 1.470, Test loss: 1.513, Test accuracy: 94.93
Round  86, Global train loss: 1.470, Global test loss: 2.055, Global test accuracy: 44.11
Round  87, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  87, Global train loss: 1.467, Global test loss: 2.013, Global test accuracy: 42.76
Round  88, Train loss: 1.519, Test loss: 1.513, Test accuracy: 94.93
Round  88, Global train loss: 1.519, Global test loss: 2.162, Global test accuracy: 29.87
Round  89, Train loss: 1.469, Test loss: 1.513, Test accuracy: 94.93
Round  89, Global train loss: 1.469, Global test loss: 2.003, Global test accuracy: 42.72
Round  90, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  90, Global train loss: 1.467, Global test loss: 2.022, Global test accuracy: 42.02
Round  91, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  91, Global train loss: 1.467, Global test loss: 1.963, Global test accuracy: 52.12
Round  92, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  92, Global train loss: 1.467, Global test loss: 2.044, Global test accuracy: 42.41
Round  93, Train loss: 1.464, Test loss: 1.513, Test accuracy: 94.94
Round  93, Global train loss: 1.464, Global test loss: 2.102, Global test accuracy: 33.47
Round  94, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.95
Round  94, Global train loss: 1.465, Global test loss: 2.111, Global test accuracy: 40.60
Round  95, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.96
Round  95, Global train loss: 1.468, Global test loss: 2.023, Global test accuracy: 42.00/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.518, Test loss: 1.513, Test accuracy: 94.96
Round  96, Global train loss: 1.518, Global test loss: 1.994, Global test accuracy: 43.80
Round  97, Train loss: 1.520, Test loss: 1.513, Test accuracy: 94.96
Round  97, Global train loss: 1.520, Global test loss: 2.111, Global test accuracy: 36.60
Round  98, Train loss: 1.520, Test loss: 1.513, Test accuracy: 94.96
Round  98, Global train loss: 1.520, Global test loss: 2.103, Global test accuracy: 34.70
Round  99, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.96
Round  99, Global train loss: 1.468, Global test loss: 2.093, Global test accuracy: 35.73
Final Round, Train loss: 1.483, Test loss: 1.513, Test accuracy: 94.94
Final Round, Global train loss: 1.483, Global test loss: 2.093, Global test accuracy: 35.73
Average accuracy final 10 rounds: 94.94833333333335 

Average global accuracy final 10 rounds: 40.34499999999999 

1911.5511236190796
[1.2639381885528564, 2.527876377105713, 3.7014577388763428, 4.875039100646973, 6.083296775817871, 7.2915544509887695, 8.507723569869995, 9.72389268875122, 10.980223178863525, 12.23655366897583, 13.356053113937378, 14.475552558898926, 15.650567293167114, 16.825582027435303, 17.972347259521484, 19.119112491607666, 20.23951292037964, 21.35991334915161, 22.526528120040894, 23.693142890930176, 24.803438186645508, 25.91373348236084, 26.971678972244263, 28.029624462127686, 29.14976692199707, 30.269909381866455, 31.40818214416504, 32.54645490646362, 33.71655488014221, 34.8866548538208, 36.00002598762512, 37.11339712142944, 38.199564933776855, 39.28573274612427, 40.42039394378662, 41.555055141448975, 42.68776750564575, 43.82047986984253, 44.945833921432495, 46.07118797302246, 47.26418852806091, 48.457189083099365, 49.559171199798584, 50.6611533164978, 51.79882478713989, 52.93649625778198, 54.07388639450073, 55.21127653121948, 56.39854288101196, 57.58580923080444, 58.73570656776428, 59.88560390472412, 60.97811007499695, 62.070616245269775, 63.203935623168945, 64.33725500106812, 65.50130677223206, 66.665358543396, 67.78414916992188, 68.90293979644775, 70.11558485031128, 71.3282299041748, 72.42890524864197, 73.52958059310913, 74.67145609855652, 75.8133316040039, 76.91583728790283, 78.01834297180176, 79.06724047660828, 80.1161379814148, 81.27856802940369, 82.44099807739258, 83.59374380111694, 84.74648952484131, 85.7588620185852, 86.7712345123291, 87.91962552070618, 89.06801652908325, 90.15345358848572, 91.23889064788818, 92.26404237747192, 93.28919410705566, 94.31506824493408, 95.3409423828125, 96.38133192062378, 97.42172145843506, 98.57297158241272, 99.72422170639038, 100.86607003211975, 102.00791835784912, 103.05544829368591, 104.1029782295227, 105.33887267112732, 106.57476711273193, 107.64475607872009, 108.71474504470825, 109.96144247055054, 111.20813989639282, 112.5357825756073, 113.86342525482178, 114.98757553100586, 116.11172580718994, 117.20671939849854, 118.30171298980713, 119.50160455703735, 120.70149612426758, 121.80102682113647, 122.90055751800537, 124.04266166687012, 125.18476581573486, 126.29741835594177, 127.41007089614868, 128.61638474464417, 129.82269859313965, 130.9980490207672, 132.17339944839478, 133.25670218467712, 134.34000492095947, 135.49072980880737, 136.64145469665527, 137.68992447853088, 138.7383942604065, 139.74087953567505, 140.7433648109436, 141.8928828239441, 143.04240083694458, 144.07038259506226, 145.09836435317993, 146.14215922355652, 147.1859540939331, 148.27381920814514, 149.36168432235718, 150.34738087654114, 151.3330774307251, 152.39156770706177, 153.45005798339844, 154.52679061889648, 155.60352325439453, 156.6166009902954, 157.6296787261963, 158.7794497013092, 159.92922067642212, 161.0112602710724, 162.09329986572266, 163.2027039527893, 164.31210803985596, 165.46001410484314, 166.60792016983032, 167.73900723457336, 168.8700942993164, 169.93286275863647, 170.99563121795654, 172.0992467403412, 173.20286226272583, 174.41485357284546, 175.6268448829651, 176.75102424621582, 177.87520360946655, 178.94963431358337, 180.0240650177002, 181.11159658432007, 182.19912815093994, 183.34119153022766, 184.48325490951538, 185.5728883743286, 186.66252183914185, 187.72818326950073, 188.79384469985962, 189.90443778038025, 191.01503086090088, 192.1086564064026, 193.2022819519043, 194.2865550518036, 195.37082815170288, 196.4686086177826, 197.5663890838623, 198.6850700378418, 199.8037509918213, 200.9175295829773, 202.0313081741333, 203.1129412651062, 204.1945743560791, 205.27321600914001, 206.35185766220093, 207.4562087059021, 208.56055974960327, 209.65938353538513, 210.758207321167, 211.77587938308716, 212.79355144500732, 213.9712793827057, 215.14900732040405, 216.20695853233337, 217.2649097442627, 218.3611798286438, 219.4574499130249, 220.58982038497925, 221.7221908569336, 222.7709400653839, 223.81968927383423, 225.75887060165405, 227.69805192947388]
[46.78333333333333, 46.78333333333333, 62.90833333333333, 62.90833333333333, 73.74166666666666, 73.74166666666666, 75.08333333333333, 75.08333333333333, 81.70833333333333, 81.70833333333333, 74.16666666666667, 74.16666666666667, 83.15833333333333, 83.15833333333333, 86.56666666666666, 86.56666666666666, 90.39166666666667, 90.39166666666667, 90.48333333333333, 90.48333333333333, 91.65, 91.65, 91.85833333333333, 91.85833333333333, 93.39166666666667, 93.39166666666667, 94.525, 94.525, 94.575, 94.575, 94.75, 94.75, 94.725, 94.725, 94.70833333333333, 94.70833333333333, 94.69166666666666, 94.69166666666666, 94.71666666666667, 94.71666666666667, 94.76666666666667, 94.76666666666667, 94.74166666666666, 94.74166666666666, 94.79166666666667, 94.79166666666667, 94.85, 94.85, 94.86666666666666, 94.86666666666666, 94.83333333333333, 94.83333333333333, 94.85833333333333, 94.85833333333333, 94.875, 94.875, 94.89166666666667, 94.89166666666667, 94.85833333333333, 94.85833333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.84166666666667, 94.84166666666667, 94.81666666666666, 94.81666666666666, 94.85, 94.85, 94.81666666666666, 94.81666666666666, 94.80833333333334, 94.80833333333334, 94.80833333333334, 94.80833333333334, 94.84166666666667, 94.84166666666667, 94.85833333333333, 94.85833333333333, 94.84166666666667, 94.84166666666667, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.84166666666667, 94.84166666666667, 94.85, 94.85, 94.84166666666667, 94.84166666666667, 94.85, 94.85, 94.85833333333333, 94.85833333333333, 94.825, 94.825, 94.825, 94.825, 94.825, 94.825, 94.875, 94.875, 94.9, 94.9, 94.88333333333334, 94.88333333333334, 94.9, 94.9, 94.88333333333334, 94.88333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.9, 94.9, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.93333333333334, 94.93333333333334, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.9, 94.9, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.94166666666666, 94.94166666666666, 94.95, 94.95, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.94166666666666, 94.94166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.277, Test loss: 2.188, Test accuracy: 37.61
Round   0, Global train loss: 2.277, Global test loss: 2.189, Global test accuracy: 37.65
Round   1, Train loss: 1.905, Test loss: 1.749, Test accuracy: 75.63
Round   1, Global train loss: 1.905, Global test loss: 1.697, Global test accuracy: 80.48
Round   2, Train loss: 1.661, Test loss: 1.710, Test accuracy: 77.62
Round   2, Global train loss: 1.661, Global test loss: 1.633, Global test accuracy: 83.94
Round   3, Train loss: 1.627, Test loss: 1.643, Test accuracy: 83.07
Round   3, Global train loss: 1.627, Global test loss: 1.620, Global test accuracy: 84.62
Round   4, Train loss: 1.614, Test loss: 1.639, Test accuracy: 83.38
Round   4, Global train loss: 1.614, Global test loss: 1.613, Global test accuracy: 85.28
Round   5, Train loss: 1.603, Test loss: 1.632, Test accuracy: 83.80
Round   5, Global train loss: 1.603, Global test loss: 1.608, Global test accuracy: 85.70
Round   6, Train loss: 1.598, Test loss: 1.625, Test accuracy: 84.27
Round   6, Global train loss: 1.598, Global test loss: 1.606, Global test accuracy: 85.72
Round   7, Train loss: 1.598, Test loss: 1.618, Test accuracy: 84.73
Round   7, Global train loss: 1.598, Global test loss: 1.604, Global test accuracy: 85.96
Round   8, Train loss: 1.590, Test loss: 1.616, Test accuracy: 84.94
Round   8, Global train loss: 1.590, Global test loss: 1.600, Global test accuracy: 86.31
Round   9, Train loss: 1.592, Test loss: 1.610, Test accuracy: 85.34
Round   9, Global train loss: 1.592, Global test loss: 1.599, Global test accuracy: 86.41
Round  10, Train loss: 1.589, Test loss: 1.606, Test accuracy: 85.72
Round  10, Global train loss: 1.589, Global test loss: 1.598, Global test accuracy: 86.56
Round  11, Train loss: 1.586, Test loss: 1.604, Test accuracy: 85.87
Round  11, Global train loss: 1.586, Global test loss: 1.596, Global test accuracy: 86.83
Round  12, Train loss: 1.584, Test loss: 1.603, Test accuracy: 86.03
Round  12, Global train loss: 1.584, Global test loss: 1.595, Global test accuracy: 86.82
Round  13, Train loss: 1.579, Test loss: 1.600, Test accuracy: 86.22
Round  13, Global train loss: 1.579, Global test loss: 1.594, Global test accuracy: 86.91
Round  14, Train loss: 1.579, Test loss: 1.599, Test accuracy: 86.40
Round  14, Global train loss: 1.579, Global test loss: 1.593, Global test accuracy: 86.95
Round  15, Train loss: 1.574, Test loss: 1.598, Test accuracy: 86.53
Round  15, Global train loss: 1.574, Global test loss: 1.592, Global test accuracy: 86.99
Round  16, Train loss: 1.574, Test loss: 1.597, Test accuracy: 86.60
Round  16, Global train loss: 1.574, Global test loss: 1.591, Global test accuracy: 87.11
Round  17, Train loss: 1.576, Test loss: 1.596, Test accuracy: 86.66
Round  17, Global train loss: 1.576, Global test loss: 1.590, Global test accuracy: 87.28
Round  18, Train loss: 1.577, Test loss: 1.595, Test accuracy: 86.72
Round  18, Global train loss: 1.577, Global test loss: 1.589, Global test accuracy: 87.34
Round  19, Train loss: 1.573, Test loss: 1.594, Test accuracy: 86.83
Round  19, Global train loss: 1.573, Global test loss: 1.588, Global test accuracy: 87.26
Round  20, Train loss: 1.571, Test loss: 1.593, Test accuracy: 86.95
Round  20, Global train loss: 1.571, Global test loss: 1.587, Global test accuracy: 87.43
Round  21, Train loss: 1.571, Test loss: 1.592, Test accuracy: 87.01
Round  21, Global train loss: 1.571, Global test loss: 1.587, Global test accuracy: 87.45
Round  22, Train loss: 1.573, Test loss: 1.592, Test accuracy: 87.10
Round  22, Global train loss: 1.573, Global test loss: 1.587, Global test accuracy: 87.50
Round  23, Train loss: 1.575, Test loss: 1.591, Test accuracy: 87.13
Round  23, Global train loss: 1.575, Global test loss: 1.586, Global test accuracy: 87.50
Round  24, Train loss: 1.570, Test loss: 1.590, Test accuracy: 87.17
Round  24, Global train loss: 1.570, Global test loss: 1.586, Global test accuracy: 87.66
Round  25, Train loss: 1.572, Test loss: 1.590, Test accuracy: 87.23
Round  25, Global train loss: 1.572, Global test loss: 1.586, Global test accuracy: 87.64
Round  26, Train loss: 1.569, Test loss: 1.589, Test accuracy: 87.34
Round  26, Global train loss: 1.569, Global test loss: 1.585, Global test accuracy: 87.75
Round  27, Train loss: 1.568, Test loss: 1.588, Test accuracy: 87.41
Round  27, Global train loss: 1.568, Global test loss: 1.584, Global test accuracy: 87.83
Round  28, Train loss: 1.569, Test loss: 1.588, Test accuracy: 87.43
Round  28, Global train loss: 1.569, Global test loss: 1.584, Global test accuracy: 87.83
Round  29, Train loss: 1.566, Test loss: 1.587, Test accuracy: 87.45
Round  29, Global train loss: 1.566, Global test loss: 1.583, Global test accuracy: 87.84
Round  30, Train loss: 1.568, Test loss: 1.586, Test accuracy: 87.57
Round  30, Global train loss: 1.568, Global test loss: 1.583, Global test accuracy: 87.90
Round  31, Train loss: 1.565, Test loss: 1.586, Test accuracy: 87.64
Round  31, Global train loss: 1.565, Global test loss: 1.582, Global test accuracy: 87.90
Round  32, Train loss: 1.565, Test loss: 1.586, Test accuracy: 87.61
Round  32, Global train loss: 1.565, Global test loss: 1.582, Global test accuracy: 87.95
Round  33, Train loss: 1.571, Test loss: 1.585, Test accuracy: 87.68
Round  33, Global train loss: 1.571, Global test loss: 1.581, Global test accuracy: 87.96
Round  34, Train loss: 1.566, Test loss: 1.585, Test accuracy: 87.72
Round  34, Global train loss: 1.566, Global test loss: 1.581, Global test accuracy: 88.00
Round  35, Train loss: 1.563, Test loss: 1.584, Test accuracy: 87.78
Round  35, Global train loss: 1.563, Global test loss: 1.581, Global test accuracy: 88.10
Round  36, Train loss: 1.566, Test loss: 1.584, Test accuracy: 87.84
Round  36, Global train loss: 1.566, Global test loss: 1.580, Global test accuracy: 88.18
Round  37, Train loss: 1.564, Test loss: 1.583, Test accuracy: 87.92
Round  37, Global train loss: 1.564, Global test loss: 1.580, Global test accuracy: 88.06
Round  38, Train loss: 1.563, Test loss: 1.582, Test accuracy: 87.95
Round  38, Global train loss: 1.563, Global test loss: 1.580, Global test accuracy: 88.05
Round  39, Train loss: 1.563, Test loss: 1.582, Test accuracy: 87.99
Round  39, Global train loss: 1.563, Global test loss: 1.580, Global test accuracy: 88.10
Round  40, Train loss: 1.566, Test loss: 1.582, Test accuracy: 88.03
Round  40, Global train loss: 1.566, Global test loss: 1.579, Global test accuracy: 88.25
Round  41, Train loss: 1.561, Test loss: 1.581, Test accuracy: 88.05
Round  41, Global train loss: 1.561, Global test loss: 1.579, Global test accuracy: 88.19
Round  42, Train loss: 1.560, Test loss: 1.581, Test accuracy: 88.05
Round  42, Global train loss: 1.560, Global test loss: 1.579, Global test accuracy: 88.19
Round  43, Train loss: 1.559, Test loss: 1.581, Test accuracy: 88.06
Round  43, Global train loss: 1.559, Global test loss: 1.578, Global test accuracy: 88.28
Round  44, Train loss: 1.563, Test loss: 1.580, Test accuracy: 88.13
Round  44, Global train loss: 1.563, Global test loss: 1.578, Global test accuracy: 88.28
Round  45, Train loss: 1.560, Test loss: 1.580, Test accuracy: 88.16
Round  45, Global train loss: 1.560, Global test loss: 1.578, Global test accuracy: 88.30
Round  46, Train loss: 1.562, Test loss: 1.580, Test accuracy: 88.16
Round  46, Global train loss: 1.562, Global test loss: 1.578, Global test accuracy: 88.24
Round  47, Train loss: 1.559, Test loss: 1.580, Test accuracy: 88.16
Round  47, Global train loss: 1.559, Global test loss: 1.577, Global test accuracy: 88.36
Round  48, Train loss: 1.557, Test loss: 1.579, Test accuracy: 88.14
Round  48, Global train loss: 1.557, Global test loss: 1.577, Global test accuracy: 88.44
Round  49, Train loss: 1.561, Test loss: 1.577, Test accuracy: 88.43
Round  49, Global train loss: 1.561, Global test loss: 1.575, Global test accuracy: 88.40
Round  50, Train loss: 1.503, Test loss: 1.561, Test accuracy: 90.12
Round  50, Global train loss: 1.503, Global test loss: 1.514, Global test accuracy: 95.17
Round  51, Train loss: 1.486, Test loss: 1.545, Test accuracy: 91.88
Round  51, Global train loss: 1.486, Global test loss: 1.510, Global test accuracy: 95.57
Round  52, Train loss: 1.481, Test loss: 1.540, Test accuracy: 92.41
Round  52, Global train loss: 1.481, Global test loss: 1.507, Global test accuracy: 95.74
Round  53, Train loss: 1.483, Test loss: 1.529, Test accuracy: 93.47
Round  53, Global train loss: 1.483, Global test loss: 1.506, Global test accuracy: 95.80
Round  54, Train loss: 1.478, Test loss: 1.525, Test accuracy: 93.95
Round  54, Global train loss: 1.478, Global test loss: 1.504, Global test accuracy: 95.97
Round  55, Train loss: 1.478, Test loss: 1.513, Test accuracy: 95.15
Round  55, Global train loss: 1.478, Global test loss: 1.503, Global test accuracy: 96.03
Round  56, Train loss: 1.475, Test loss: 1.513, Test accuracy: 95.22
Round  56, Global train loss: 1.475, Global test loss: 1.502, Global test accuracy: 96.19
Round  57, Train loss: 1.475, Test loss: 1.511, Test accuracy: 95.33
Round  57, Global train loss: 1.475, Global test loss: 1.502, Global test accuracy: 96.22
Round  58, Train loss: 1.477, Test loss: 1.506, Test accuracy: 95.79
Round  58, Global train loss: 1.477, Global test loss: 1.502, Global test accuracy: 96.11
Round  59, Train loss: 1.476, Test loss: 1.506, Test accuracy: 95.82
Round  59, Global train loss: 1.476, Global test loss: 1.501, Global test accuracy: 96.21
Round  60, Train loss: 1.473, Test loss: 1.505, Test accuracy: 95.88
Round  60, Global train loss: 1.473, Global test loss: 1.499, Global test accuracy: 96.37
Round  61, Train loss: 1.474, Test loss: 1.505, Test accuracy: 95.92
Round  61, Global train loss: 1.474, Global test loss: 1.499, Global test accuracy: 96.33
Round  62, Train loss: 1.473, Test loss: 1.504, Test accuracy: 95.93
Round  62, Global train loss: 1.473, Global test loss: 1.499, Global test accuracy: 96.47
Round  63, Train loss: 1.474, Test loss: 1.504, Test accuracy: 96.02
Round  63, Global train loss: 1.474, Global test loss: 1.498, Global test accuracy: 96.44
Round  64, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.09
Round  64, Global train loss: 1.475, Global test loss: 1.498, Global test accuracy: 96.41
Round  65, Train loss: 1.473, Test loss: 1.502, Test accuracy: 96.12
Round  65, Global train loss: 1.473, Global test loss: 1.497, Global test accuracy: 96.54
Round  66, Train loss: 1.471, Test loss: 1.501, Test accuracy: 96.17
Round  66, Global train loss: 1.471, Global test loss: 1.497, Global test accuracy: 96.61
Round  67, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.21
Round  67, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.56
Round  68, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.30
Round  68, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.65
Round  69, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.36
Round  69, Global train loss: 1.472, Global test loss: 1.496, Global test accuracy: 96.79
Round  70, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.40
Round  70, Global train loss: 1.471, Global test loss: 1.496, Global test accuracy: 96.77
Round  71, Train loss: 1.473, Test loss: 1.499, Test accuracy: 96.46
Round  71, Global train loss: 1.473, Global test loss: 1.496, Global test accuracy: 96.75
Round  72, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.56
Round  72, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.85
Round  73, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.61
Round  73, Global train loss: 1.471, Global test loss: 1.495, Global test accuracy: 96.86
Round  74, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.64
Round  74, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.81
Round  75, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.66
Round  75, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.91
Round  76, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.66
Round  76, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.91
Round  77, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.64
Round  77, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.86
Round  78, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.67
Round  78, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.90
Round  79, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.70
Round  79, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.85
Round  80, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.70
Round  80, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.78
Round  81, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.71
Round  81, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.86
Round  82, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.71
Round  82, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.97
Round  83, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.72
Round  83, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.86
Round  84, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.72
Round  84, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.77
Round  85, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.75
Round  85, Global train loss: 1.471, Global test loss: 1.493, Global test accuracy: 96.91
Round  86, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.75
Round  86, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.91
Round  87, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.75
Round  87, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.88
Round  88, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.76
Round  88, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.84
Round  89, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.79
Round  89, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 96.95
Round  90, Train loss: 1.468, Test loss: 1.495, Test accuracy: 96.79
Round  90, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.89
Round  91, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.81
Round  91, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 96.93
Round  92, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.82
Round  92, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.04
Round  93, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.83
Round  93, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.05
Round  94, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.82
Round  94, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.97
Round  95, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.82
Round  95, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 97.03/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.85
Round  96, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.00
Round  97, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.86
Round  97, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.06
Round  98, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.85
Round  98, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.03
Round  99, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.85
Round  99, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 96.95
Final Round, Train loss: 1.467, Test loss: 1.493, Test accuracy: 96.90
Final Round, Global train loss: 1.467, Global test loss: 1.492, Global test accuracy: 96.95
Average accuracy final 10 rounds: 96.83024999999999 

Average global accuracy final 10 rounds: 96.993 

6195.196897745132
[4.328021049499512, 8.656042098999023, 12.233827352523804, 15.811612606048584, 19.366617918014526, 22.92162322998047, 26.320698261260986, 29.719773292541504, 33.32843208312988, 36.93709087371826, 40.582886695861816, 44.22868251800537, 47.921260833740234, 51.6138391494751, 55.38750624656677, 59.16117334365845, 62.88951230049133, 66.61785125732422, 70.36571907997131, 74.11358690261841, 77.64364671707153, 81.17370653152466, 84.72921562194824, 88.28472471237183, 92.17254137992859, 96.06035804748535, 99.82534098625183, 103.59032392501831, 107.22282338142395, 110.85532283782959, 114.34169864654541, 117.82807445526123, 121.56597518920898, 125.30387592315674, 129.03696608543396, 132.77005624771118, 136.66104936599731, 140.55204248428345, 144.13570356369019, 147.71936464309692, 151.35888171195984, 154.99839878082275, 158.7974889278412, 162.59657907485962, 166.374694108963, 170.1528091430664, 173.82034516334534, 177.48788118362427, 181.17644238471985, 184.86500358581543, 188.50431370735168, 192.14362382888794, 195.67182779312134, 199.20003175735474, 202.79309225082397, 206.3861527442932, 209.79846620559692, 213.21077966690063, 216.77644801139832, 220.342116355896, 224.0157446861267, 227.68937301635742, 231.28022813796997, 234.87108325958252, 238.28319764137268, 241.69531202316284, 245.31000757217407, 248.9247031211853, 252.5582251548767, 256.1917471885681, 259.7501058578491, 263.3084645271301, 266.85959792137146, 270.4107313156128, 273.9492793083191, 277.4878273010254, 281.15986132621765, 284.8318953514099, 288.4057080745697, 291.9795207977295, 295.51510667800903, 299.0506925582886, 302.48532152175903, 305.9199504852295, 309.3937075138092, 312.8674645423889, 316.4403007030487, 320.0131368637085, 323.6871826648712, 327.36122846603394, 330.8869786262512, 334.4127287864685, 337.85875034332275, 341.304771900177, 344.9512758255005, 348.597779750824, 352.15462923049927, 355.71147871017456, 359.13899779319763, 362.5665168762207, 366.0231547355652, 369.47979259490967, 372.8701739311218, 376.260555267334, 379.791522026062, 383.32248878479004, 386.9863214492798, 390.65015411376953, 394.46179246902466, 398.2734308242798, 401.8970127105713, 405.5205945968628, 409.1043961048126, 412.68819761276245, 416.44082474708557, 420.1934518814087, 423.99629759788513, 427.7991433143616, 431.45573830604553, 435.1123332977295, 438.51364946365356, 441.91496562957764, 445.53325510025024, 449.15154457092285, 452.71889209747314, 456.28623962402344, 459.83713555336, 463.38803148269653, 466.75388741493225, 470.11974334716797, 473.64824533462524, 477.1767473220825, 481.00931549072266, 484.8418836593628, 488.65664863586426, 492.4714136123657, 495.97749066352844, 499.48356771469116, 503.25620794296265, 507.02884817123413, 510.88106989860535, 514.7332916259766, 518.4029834270477, 522.0726752281189, 525.4894318580627, 528.9061884880066, 532.4265768527985, 535.9469652175903, 539.5195081233978, 543.0920510292053, 546.6256515979767, 550.159252166748, 553.6072797775269, 557.0553073883057, 560.5764858722687, 564.0976643562317, 567.5530970096588, 571.0085296630859, 574.5382883548737, 578.0680470466614, 581.6809959411621, 585.2939448356628, 589.0232903957367, 592.7526359558105, 596.4751915931702, 600.1977472305298, 603.8269894123077, 607.4562315940857, 610.9729309082031, 614.4896302223206, 617.9726052284241, 621.4555802345276, 625.0929992198944, 628.7304182052612, 632.3368566036224, 635.9432950019836, 639.9393119812012, 643.9353289604187, 647.8507506847382, 651.7661724090576, 655.6519756317139, 659.5377788543701, 663.5432126522064, 667.5486464500427, 671.7995293140411, 676.0504121780396, 680.2080862522125, 684.3657603263855, 688.5445492267609, 692.7233381271362, 696.8907985687256, 701.0582590103149, 705.222954750061, 709.3876504898071, 713.589518070221, 717.7913856506348, 722.0573630332947, 726.3233404159546, 730.5150344371796, 734.7067284584045, 736.8268640041351, 738.9469995498657]
[37.6125, 37.6125, 75.6275, 75.6275, 77.6175, 77.6175, 83.0725, 83.0725, 83.3775, 83.3775, 83.7975, 83.7975, 84.2675, 84.2675, 84.7275, 84.7275, 84.9425, 84.9425, 85.345, 85.345, 85.715, 85.715, 85.8725, 85.8725, 86.0275, 86.0275, 86.2225, 86.2225, 86.3975, 86.3975, 86.53, 86.53, 86.6025, 86.6025, 86.66, 86.66, 86.725, 86.725, 86.83, 86.83, 86.955, 86.955, 87.0125, 87.0125, 87.1, 87.1, 87.1325, 87.1325, 87.1725, 87.1725, 87.2275, 87.2275, 87.34, 87.34, 87.405, 87.405, 87.4325, 87.4325, 87.455, 87.455, 87.57, 87.57, 87.635, 87.635, 87.6125, 87.6125, 87.68, 87.68, 87.72, 87.72, 87.7775, 87.7775, 87.8425, 87.8425, 87.915, 87.915, 87.95, 87.95, 87.9925, 87.9925, 88.035, 88.035, 88.045, 88.045, 88.05, 88.05, 88.065, 88.065, 88.1325, 88.1325, 88.16, 88.16, 88.155, 88.155, 88.155, 88.155, 88.1375, 88.1375, 88.4275, 88.4275, 90.125, 90.125, 91.88, 91.88, 92.41, 92.41, 93.475, 93.475, 93.95, 93.95, 95.1525, 95.1525, 95.22, 95.22, 95.325, 95.325, 95.7875, 95.7875, 95.8175, 95.8175, 95.875, 95.875, 95.9175, 95.9175, 95.9325, 95.9325, 96.02, 96.02, 96.0925, 96.0925, 96.1175, 96.1175, 96.175, 96.175, 96.2075, 96.2075, 96.3025, 96.3025, 96.3625, 96.3625, 96.4, 96.4, 96.4625, 96.4625, 96.5575, 96.5575, 96.6075, 96.6075, 96.635, 96.635, 96.655, 96.655, 96.655, 96.655, 96.645, 96.645, 96.6675, 96.6675, 96.7025, 96.7025, 96.7025, 96.7025, 96.7075, 96.7075, 96.71, 96.71, 96.72, 96.72, 96.715, 96.715, 96.7475, 96.7475, 96.745, 96.745, 96.75, 96.75, 96.7575, 96.7575, 96.79, 96.79, 96.79, 96.79, 96.815, 96.815, 96.82, 96.82, 96.8275, 96.8275, 96.8225, 96.8225, 96.82, 96.82, 96.85, 96.85, 96.8575, 96.8575, 96.8475, 96.8475, 96.8525, 96.8525, 96.9025, 96.9025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.290, Test accuracy: 32.42
Round   1, Train loss: 2.249, Test loss: 2.133, Test accuracy: 37.03
Round   2, Train loss: 2.007, Test loss: 1.893, Test accuracy: 63.08
Round   3, Train loss: 1.759, Test loss: 1.754, Test accuracy: 75.12
Round   4, Train loss: 1.652, Test loss: 1.667, Test accuracy: 82.14
Round   5, Train loss: 1.597, Test loss: 1.622, Test accuracy: 85.56
Round   6, Train loss: 1.578, Test loss: 1.603, Test accuracy: 87.14
Round   7, Train loss: 1.573, Test loss: 1.583, Test accuracy: 88.94
Round   8, Train loss: 1.552, Test loss: 1.579, Test accuracy: 89.14
Round   9, Train loss: 1.549, Test loss: 1.567, Test accuracy: 90.30
Round  10, Train loss: 1.544, Test loss: 1.563, Test accuracy: 90.49
Round  11, Train loss: 1.545, Test loss: 1.559, Test accuracy: 90.96
Round  12, Train loss: 1.544, Test loss: 1.548, Test accuracy: 91.99
Round  13, Train loss: 1.535, Test loss: 1.547, Test accuracy: 92.03
Round  14, Train loss: 1.531, Test loss: 1.544, Test accuracy: 92.30
Round  15, Train loss: 1.524, Test loss: 1.541, Test accuracy: 92.54
Round  16, Train loss: 1.527, Test loss: 1.539, Test accuracy: 92.65
Round  17, Train loss: 1.525, Test loss: 1.538, Test accuracy: 92.74
Round  18, Train loss: 1.517, Test loss: 1.539, Test accuracy: 92.70
Round  19, Train loss: 1.517, Test loss: 1.538, Test accuracy: 92.75
Round  20, Train loss: 1.518, Test loss: 1.536, Test accuracy: 92.94
Round  21, Train loss: 1.518, Test loss: 1.533, Test accuracy: 93.18
Round  22, Train loss: 1.513, Test loss: 1.531, Test accuracy: 93.39
Round  23, Train loss: 1.512, Test loss: 1.529, Test accuracy: 93.55
Round  24, Train loss: 1.511, Test loss: 1.528, Test accuracy: 93.66
Round  25, Train loss: 1.508, Test loss: 1.527, Test accuracy: 93.85
Round  26, Train loss: 1.505, Test loss: 1.526, Test accuracy: 93.84
Round  27, Train loss: 1.507, Test loss: 1.525, Test accuracy: 93.89
Round  28, Train loss: 1.504, Test loss: 1.524, Test accuracy: 94.00
Round  29, Train loss: 1.501, Test loss: 1.524, Test accuracy: 94.08
Round  30, Train loss: 1.502, Test loss: 1.523, Test accuracy: 94.10
Round  31, Train loss: 1.504, Test loss: 1.522, Test accuracy: 94.19
Round  32, Train loss: 1.500, Test loss: 1.521, Test accuracy: 94.31
Round  33, Train loss: 1.499, Test loss: 1.520, Test accuracy: 94.35
Round  34, Train loss: 1.498, Test loss: 1.520, Test accuracy: 94.40
Round  35, Train loss: 1.495, Test loss: 1.519, Test accuracy: 94.51
Round  36, Train loss: 1.497, Test loss: 1.519, Test accuracy: 94.48
Round  37, Train loss: 1.493, Test loss: 1.519, Test accuracy: 94.50
Round  38, Train loss: 1.493, Test loss: 1.518, Test accuracy: 94.58
Round  39, Train loss: 1.493, Test loss: 1.517, Test accuracy: 94.61
Round  40, Train loss: 1.492, Test loss: 1.516, Test accuracy: 94.72
Round  41, Train loss: 1.491, Test loss: 1.515, Test accuracy: 94.85
Round  42, Train loss: 1.492, Test loss: 1.515, Test accuracy: 94.87
Round  43, Train loss: 1.490, Test loss: 1.515, Test accuracy: 94.91
Round  44, Train loss: 1.489, Test loss: 1.515, Test accuracy: 94.78
Round  45, Train loss: 1.493, Test loss: 1.514, Test accuracy: 94.89
Round  46, Train loss: 1.488, Test loss: 1.514, Test accuracy: 94.98
Round  47, Train loss: 1.491, Test loss: 1.513, Test accuracy: 95.00
Round  48, Train loss: 1.488, Test loss: 1.513, Test accuracy: 94.94
Round  49, Train loss: 1.488, Test loss: 1.513, Test accuracy: 94.99
Round  50, Train loss: 1.486, Test loss: 1.513, Test accuracy: 94.99
Round  51, Train loss: 1.488, Test loss: 1.511, Test accuracy: 95.16
Round  52, Train loss: 1.486, Test loss: 1.512, Test accuracy: 95.15
Round  53, Train loss: 1.486, Test loss: 1.511, Test accuracy: 95.19
Round  54, Train loss: 1.485, Test loss: 1.511, Test accuracy: 95.23
Round  55, Train loss: 1.486, Test loss: 1.510, Test accuracy: 95.28
Round  56, Train loss: 1.484, Test loss: 1.510, Test accuracy: 95.28
Round  57, Train loss: 1.484, Test loss: 1.510, Test accuracy: 95.32
Round  58, Train loss: 1.485, Test loss: 1.510, Test accuracy: 95.34
Round  59, Train loss: 1.482, Test loss: 1.509, Test accuracy: 95.44
Round  60, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.51
Round  61, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.54
Round  62, Train loss: 1.483, Test loss: 1.508, Test accuracy: 95.48
Round  63, Train loss: 1.482, Test loss: 1.508, Test accuracy: 95.55
Round  64, Train loss: 1.481, Test loss: 1.507, Test accuracy: 95.56
Round  65, Train loss: 1.482, Test loss: 1.507, Test accuracy: 95.57
Round  66, Train loss: 1.481, Test loss: 1.507, Test accuracy: 95.64
Round  67, Train loss: 1.482, Test loss: 1.507, Test accuracy: 95.58
Round  68, Train loss: 1.481, Test loss: 1.507, Test accuracy: 95.60
Round  69, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.68
Round  70, Train loss: 1.481, Test loss: 1.506, Test accuracy: 95.71
Round  71, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.70
Round  72, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.73
Round  73, Train loss: 1.479, Test loss: 1.506, Test accuracy: 95.76
Round  74, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.73
Round  75, Train loss: 1.479, Test loss: 1.506, Test accuracy: 95.70
Round  76, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.73
Round  77, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.79
Round  78, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.81
Round  79, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.82
Round  80, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.82
Round  81, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.81
Round  82, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.81
Round  83, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.78
Round  84, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.84
Round  85, Train loss: 1.477, Test loss: 1.504, Test accuracy: 95.86
Round  86, Train loss: 1.477, Test loss: 1.504, Test accuracy: 95.84
Round  87, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.90
Round  88, Train loss: 1.478, Test loss: 1.503, Test accuracy: 95.90
Round  89, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.88
Round  90, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.89
Round  91, Train loss: 1.476, Test loss: 1.504, Test accuracy: 95.86
Round  92, Train loss: 1.477, Test loss: 1.504, Test accuracy: 95.89
Round  93, Train loss: 1.476, Test loss: 1.503, Test accuracy: 95.92
Round  94, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.94
Round  95, Train loss: 1.476, Test loss: 1.503, Test accuracy: 95.92
Round  96, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.94
Round  97, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.94
Round  98, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.90/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.00
Final Round, Train loss: 1.473, Test loss: 1.503, Test accuracy: 96.00
Average accuracy final 10 rounds: 95.92125000000001 

5055.900146245956
[3.770472764968872, 7.540945529937744, 11.193896532058716, 14.846847534179688, 18.486830711364746, 22.126813888549805, 25.674368381500244, 29.221922874450684, 32.83134412765503, 36.440765380859375, 40.156097412109375, 43.871429443359375, 47.611443758010864, 51.35145807266235, 54.92490315437317, 58.498348236083984, 62.04292845726013, 65.58750867843628, 69.25226593017578, 72.91702318191528, 76.54789447784424, 80.1787657737732, 83.7350206375122, 87.29127550125122, 90.76462841033936, 94.23798131942749, 97.92938256263733, 101.62078380584717, 105.41699314117432, 109.21320247650146, 112.72685217857361, 116.24050188064575, 119.75422143936157, 123.26794099807739, 126.89236187934875, 130.51678276062012, 134.15819668769836, 137.7996106147766, 141.31707215309143, 144.83453369140625, 148.33265948295593, 151.83078527450562, 155.44263887405396, 159.0544924736023, 162.6395845413208, 166.2246766090393, 169.5831413269043, 172.9416060447693, 176.4235224723816, 179.9054388999939, 183.51801800727844, 187.130597114563, 190.6697859764099, 194.20897483825684, 197.6245243549347, 201.04007387161255, 204.59679675102234, 208.15351963043213, 211.70966863632202, 215.2658176422119, 218.81061029434204, 222.35540294647217, 225.88000321388245, 229.40460348129272, 232.98751640319824, 236.57042932510376, 240.2518970966339, 243.93336486816406, 247.4856719970703, 251.03797912597656, 254.62200665473938, 258.2060341835022, 261.8925211429596, 265.579008102417, 269.2625324726105, 272.94605684280396, 276.6918921470642, 280.43772745132446, 284.20812606811523, 287.978524684906, 291.66976261138916, 295.3610005378723, 299.1417453289032, 302.9224901199341, 306.6382477283478, 310.3540053367615, 314.00088810920715, 317.64777088165283, 321.3874809741974, 325.12719106674194, 328.9436225891113, 332.7600541114807, 336.36236810684204, 339.96468210220337, 343.721759557724, 347.47883701324463, 351.28741931915283, 355.09600162506104, 358.7161228656769, 362.3362441062927, 366.0266463756561, 369.71704864501953, 373.47442746162415, 377.23180627822876, 380.86772775650024, 384.50364923477173, 388.34601068496704, 392.18837213516235, 395.82367181777954, 399.45897150039673, 403.1073637008667, 406.75575590133667, 410.55601263046265, 414.3562693595886, 418.1413447856903, 421.926420211792, 425.6101698875427, 429.29391956329346, 432.89132952690125, 436.48873949050903, 440.3516263961792, 444.21451330184937, 447.9824733734131, 451.7504334449768, 455.3999516963959, 459.04946994781494, 462.8023202419281, 466.55517053604126, 470.31137108802795, 474.06757164001465, 477.8596887588501, 481.65180587768555, 485.34379291534424, 489.03577995300293, 492.7288017272949, 496.4218235015869, 500.11673378944397, 503.811644077301, 507.55518913269043, 511.29873418807983, 515.0056052207947, 518.7124762535095, 522.3926792144775, 526.0728821754456, 529.6594295501709, 533.2459769248962, 536.8447754383087, 540.4435739517212, 544.1395304203033, 547.8354868888855, 551.4792845249176, 555.1230821609497, 558.7689762115479, 562.414870262146, 566.0052773952484, 569.5956845283508, 573.3489322662354, 577.1021800041199, 580.7584772109985, 584.4147744178772, 588.0723028182983, 591.7298312187195, 595.3480713367462, 598.966311454773, 602.6972298622131, 606.4281482696533, 610.1093754768372, 613.790602684021, 617.5474054813385, 621.304208278656, 624.9252066612244, 628.5462050437927, 632.1355810165405, 635.7249569892883, 639.4190785884857, 643.1132001876831, 646.8580379486084, 650.6028757095337, 654.2744085788727, 657.9459414482117, 661.6201884746552, 665.2944355010986, 668.9910151958466, 672.6875948905945, 676.3632171154022, 680.03883934021, 683.8591341972351, 687.6794290542603, 691.3564584255219, 695.0334877967834, 698.7902374267578, 702.5469870567322, 706.3003823757172, 710.0537776947021, 713.8349816799164, 717.6161856651306, 721.305447101593, 724.9947085380554, 728.6983458995819, 732.4019832611084, 734.2377140522003, 736.0734448432922]
[32.4175, 32.4175, 37.03, 37.03, 63.0775, 63.0775, 75.1175, 75.1175, 82.1425, 82.1425, 85.56, 85.56, 87.14, 87.14, 88.94, 88.94, 89.14, 89.14, 90.3, 90.3, 90.4875, 90.4875, 90.96, 90.96, 91.99, 91.99, 92.0325, 92.0325, 92.295, 92.295, 92.5425, 92.5425, 92.6475, 92.6475, 92.74, 92.74, 92.6975, 92.6975, 92.745, 92.745, 92.94, 92.94, 93.1825, 93.1825, 93.395, 93.395, 93.5525, 93.5525, 93.6575, 93.6575, 93.8475, 93.8475, 93.84, 93.84, 93.895, 93.895, 94.0, 94.0, 94.085, 94.085, 94.1025, 94.1025, 94.19, 94.19, 94.3125, 94.3125, 94.3525, 94.3525, 94.4025, 94.4025, 94.5125, 94.5125, 94.4825, 94.4825, 94.505, 94.505, 94.585, 94.585, 94.605, 94.605, 94.715, 94.715, 94.8475, 94.8475, 94.8725, 94.8725, 94.905, 94.905, 94.775, 94.775, 94.8875, 94.8875, 94.985, 94.985, 95.0025, 95.0025, 94.9375, 94.9375, 94.99, 94.99, 94.99, 94.99, 95.1625, 95.1625, 95.15, 95.15, 95.195, 95.195, 95.23, 95.23, 95.2775, 95.2775, 95.2775, 95.2775, 95.32, 95.32, 95.345, 95.345, 95.445, 95.445, 95.5075, 95.5075, 95.54, 95.54, 95.4775, 95.4775, 95.5525, 95.5525, 95.5625, 95.5625, 95.57, 95.57, 95.6375, 95.6375, 95.5775, 95.5775, 95.5975, 95.5975, 95.6775, 95.6775, 95.7075, 95.7075, 95.705, 95.705, 95.73, 95.73, 95.7575, 95.7575, 95.735, 95.735, 95.7, 95.7, 95.7325, 95.7325, 95.7925, 95.7925, 95.8125, 95.8125, 95.8175, 95.8175, 95.8175, 95.8175, 95.805, 95.805, 95.805, 95.805, 95.775, 95.775, 95.8375, 95.8375, 95.855, 95.855, 95.845, 95.845, 95.9, 95.9, 95.9, 95.9, 95.875, 95.875, 95.89, 95.89, 95.865, 95.865, 95.895, 95.895, 95.92, 95.92, 95.935, 95.935, 95.925, 95.925, 95.945, 95.945, 95.935, 95.935, 95.9, 95.9, 96.0025, 96.0025, 96.005, 96.005]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.286, Test loss: 2.213, Test accuracy: 27.58
Round   1, Train loss: 1.974, Test loss: 1.823, Test accuracy: 68.16
Round   2, Train loss: 1.702, Test loss: 1.704, Test accuracy: 77.57
Round   3, Train loss: 1.653, Test loss: 1.652, Test accuracy: 81.87
Round   4, Train loss: 1.629, Test loss: 1.635, Test accuracy: 83.12
Round   5, Train loss: 1.622, Test loss: 1.629, Test accuracy: 83.62
Round   6, Train loss: 1.606, Test loss: 1.620, Test accuracy: 84.39
Round   7, Train loss: 1.600, Test loss: 1.611, Test accuracy: 85.39
Round   8, Train loss: 1.586, Test loss: 1.606, Test accuracy: 85.88
Round   9, Train loss: 1.585, Test loss: 1.598, Test accuracy: 86.63
Round  10, Train loss: 1.586, Test loss: 1.595, Test accuracy: 86.98
Round  11, Train loss: 1.576, Test loss: 1.592, Test accuracy: 87.16
Round  12, Train loss: 1.595, Test loss: 1.589, Test accuracy: 87.42
Round  13, Train loss: 1.550, Test loss: 1.585, Test accuracy: 87.89
Round  14, Train loss: 1.578, Test loss: 1.567, Test accuracy: 89.82
Round  15, Train loss: 1.530, Test loss: 1.556, Test accuracy: 91.00
Round  16, Train loss: 1.512, Test loss: 1.549, Test accuracy: 91.56
Round  17, Train loss: 1.504, Test loss: 1.541, Test accuracy: 92.40
Round  18, Train loss: 1.505, Test loss: 1.531, Test accuracy: 93.40
Round  19, Train loss: 1.496, Test loss: 1.530, Test accuracy: 93.44
Round  20, Train loss: 1.498, Test loss: 1.523, Test accuracy: 94.12
Round  21, Train loss: 1.493, Test loss: 1.519, Test accuracy: 94.55
Round  22, Train loss: 1.491, Test loss: 1.518, Test accuracy: 94.62
Round  23, Train loss: 1.493, Test loss: 1.512, Test accuracy: 95.26
Round  24, Train loss: 1.492, Test loss: 1.511, Test accuracy: 95.42
Round  25, Train loss: 1.487, Test loss: 1.511, Test accuracy: 95.37
Round  26, Train loss: 1.487, Test loss: 1.509, Test accuracy: 95.57
Round  27, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.62
Round  28, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.59
Round  29, Train loss: 1.484, Test loss: 1.507, Test accuracy: 95.75
Round  30, Train loss: 1.482, Test loss: 1.506, Test accuracy: 95.79
Round  31, Train loss: 1.482, Test loss: 1.506, Test accuracy: 95.81
Round  32, Train loss: 1.481, Test loss: 1.506, Test accuracy: 95.81
Round  33, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.82
Round  34, Train loss: 1.483, Test loss: 1.505, Test accuracy: 95.89
Round  35, Train loss: 1.480, Test loss: 1.505, Test accuracy: 95.92
Round  36, Train loss: 1.478, Test loss: 1.504, Test accuracy: 96.01
Round  37, Train loss: 1.480, Test loss: 1.504, Test accuracy: 96.07
Round  38, Train loss: 1.479, Test loss: 1.503, Test accuracy: 96.11
Round  39, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.02
Round  40, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.11
Round  41, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.21
Round  42, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.20
Round  43, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.17
Round  44, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.21
Round  45, Train loss: 1.476, Test loss: 1.501, Test accuracy: 96.22
Round  46, Train loss: 1.474, Test loss: 1.502, Test accuracy: 96.19
Round  47, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.23
Round  48, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19
Round  49, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19
Round  50, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19
Round  51, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.25
Round  52, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.26
Round  53, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.27
Round  54, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.31
Round  55, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.30
Round  56, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.23
Round  57, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.35
Round  58, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.33
Round  59, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.37
Round  60, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.33
Round  61, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.35
Round  62, Train loss: 1.473, Test loss: 1.499, Test accuracy: 96.43
Round  63, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.44
Round  64, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.43
Round  65, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.38
Round  66, Train loss: 1.474, Test loss: 1.499, Test accuracy: 96.39
Round  67, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.43
Round  68, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.42
Round  69, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.45
Round  70, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.45
Round  71, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.45
Round  72, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.44
Round  73, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.41
Round  74, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.43
Round  75, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.44
Round  76, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.44
Round  77, Train loss: 1.470, Test loss: 1.499, Test accuracy: 96.45
Round  78, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.50
Round  79, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.50
Round  80, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.43
Round  81, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.48
Round  82, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.49
Round  83, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.50
Round  84, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.55
Round  85, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.56
Round  86, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.59
Round  87, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.56
Round  88, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.56
Round  89, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.51
Round  90, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.52
Round  91, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.56
Round  92, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  93, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.62
Round  94, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.62
Round  95, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  96, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.62
Round  97, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.61
Round  98, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.65/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.60
Final Round, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.60
Average accuracy final 10 rounds: 96.60125000000002 

5146.573312520981
[4.458303451538086, 8.916606903076172, 13.355732202529907, 17.794857501983643, 22.42243790626526, 27.050018310546875, 31.63066267967224, 36.21130704879761, 40.61610412597656, 45.02090120315552, 49.58607888221741, 54.1512565612793, 58.54452300071716, 62.93778944015503, 67.3659598827362, 71.79413032531738, 76.29548740386963, 80.79684448242188, 85.52058601379395, 90.24432754516602, 94.93474817276001, 99.625168800354, 104.23228716850281, 108.83940553665161, 113.29285192489624, 117.74629831314087, 122.31684374809265, 126.88738918304443, 131.63823294639587, 136.38907670974731, 141.054851770401, 145.7206268310547, 150.15317559242249, 154.58572435379028, 158.95872449874878, 163.33172464370728, 167.39677906036377, 171.46183347702026, 175.36926674842834, 179.27670001983643, 183.14829516410828, 187.01989030838013, 190.8140013217926, 194.60811233520508, 198.45685386657715, 202.30559539794922, 206.09298419952393, 209.88037300109863, 213.7561218738556, 217.63187074661255, 221.49764347076416, 225.36341619491577, 229.16742849349976, 232.97144079208374, 236.78398513793945, 240.59652948379517, 244.43052005767822, 248.26451063156128, 252.0473186969757, 255.83012676239014, 259.6512014865875, 263.4722762107849, 267.336056470871, 271.19983673095703, 274.9384672641754, 278.6770977973938, 282.4383313655853, 286.19956493377686, 289.98763632774353, 293.7757077217102, 297.7463104724884, 301.7169132232666, 305.5000057220459, 309.2830982208252, 313.0031840801239, 316.7232699394226, 320.43761229515076, 324.1519546508789, 328.0272352695465, 331.9025158882141, 335.8390974998474, 339.7756791114807, 343.5042781829834, 347.2328772544861, 350.97466254234314, 354.7164478302002, 358.5481812953949, 362.3799147605896, 366.31887316703796, 370.2578315734863, 374.16518235206604, 378.07253313064575, 381.81149077415466, 385.5504484176636, 389.39490509033203, 393.2393617630005, 397.14451265335083, 401.0496635437012, 404.9487590789795, 408.8478546142578, 412.57490849494934, 416.30196237564087, 420.1469078063965, 423.9918532371521, 427.9079885482788, 431.8241238594055, 435.70342445373535, 439.5827250480652, 443.32956886291504, 447.0764126777649, 450.82083082199097, 454.56524896621704, 458.3528187274933, 462.14038848876953, 465.90378046035767, 469.6671724319458, 473.46956419944763, 477.27195596694946, 481.0035047531128, 484.7350535392761, 488.53151774406433, 492.32798194885254, 496.12220883369446, 499.9164357185364, 503.6652183532715, 507.4140009880066, 511.16329646110535, 514.9125919342041, 518.7468817234039, 522.5811715126038, 526.3970584869385, 530.2129454612732, 533.9878680706024, 537.7627906799316, 541.5231380462646, 545.2834854125977, 549.0527865886688, 552.82208776474, 556.4022777080536, 559.9824676513672, 563.4493193626404, 566.9161710739136, 570.5050768852234, 574.0939826965332, 577.6790244579315, 581.2640662193298, 584.8568322658539, 588.4495983123779, 591.9793770313263, 595.5091557502747, 599.1367921829224, 602.7644286155701, 606.3385846614838, 609.9127407073975, 613.4508438110352, 616.9889469146729, 620.4735898971558, 623.9582328796387, 627.5827493667603, 631.2072658538818, 634.8789203166962, 638.5505747795105, 642.0905072689056, 645.6304397583008, 649.1559278964996, 652.6814160346985, 656.2595338821411, 659.8376517295837, 663.4306991100311, 667.0237464904785, 670.4487106800079, 673.8736748695374, 677.4661660194397, 681.058657169342, 684.6235370635986, 688.1884169578552, 691.6563813686371, 695.124345779419, 698.6297724246979, 702.1351990699768, 705.7089025974274, 709.2826061248779, 712.875422000885, 716.4682378768921, 719.9323990345001, 723.3965601921082, 727.0278613567352, 730.6591625213623, 734.1449775695801, 737.6307926177979, 741.1550853252411, 744.6793780326843, 748.2851660251617, 751.8909540176392, 755.4013369083405, 758.9117197990417, 762.4566621780396, 766.0016045570374, 769.555112361908, 773.1086201667786, 774.7474224567413, 776.3862247467041]
[27.58, 27.58, 68.1575, 68.1575, 77.5725, 77.5725, 81.8675, 81.8675, 83.12, 83.12, 83.6175, 83.6175, 84.3875, 84.3875, 85.3925, 85.3925, 85.8825, 85.8825, 86.63, 86.63, 86.985, 86.985, 87.1625, 87.1625, 87.425, 87.425, 87.8875, 87.8875, 89.8175, 89.8175, 91.0, 91.0, 91.565, 91.565, 92.4025, 92.4025, 93.4025, 93.4025, 93.44, 93.44, 94.1175, 94.1175, 94.545, 94.545, 94.62, 94.62, 95.2575, 95.2575, 95.415, 95.415, 95.37, 95.37, 95.5675, 95.5675, 95.6175, 95.6175, 95.595, 95.595, 95.7525, 95.7525, 95.7925, 95.7925, 95.81, 95.81, 95.8075, 95.8075, 95.8225, 95.8225, 95.89, 95.89, 95.925, 95.925, 96.0125, 96.0125, 96.0725, 96.0725, 96.115, 96.115, 96.0175, 96.0175, 96.1125, 96.1125, 96.21, 96.21, 96.1975, 96.1975, 96.1725, 96.1725, 96.2075, 96.2075, 96.2225, 96.2225, 96.185, 96.185, 96.2275, 96.2275, 96.1875, 96.1875, 96.19, 96.19, 96.195, 96.195, 96.2525, 96.2525, 96.26, 96.26, 96.27, 96.27, 96.3125, 96.3125, 96.295, 96.295, 96.235, 96.235, 96.35, 96.35, 96.33, 96.33, 96.3725, 96.3725, 96.3275, 96.3275, 96.35, 96.35, 96.4325, 96.4325, 96.435, 96.435, 96.43, 96.43, 96.3775, 96.3775, 96.395, 96.395, 96.43, 96.43, 96.42, 96.42, 96.4475, 96.4475, 96.455, 96.455, 96.4525, 96.4525, 96.4425, 96.4425, 96.405, 96.405, 96.4275, 96.4275, 96.44, 96.44, 96.4425, 96.4425, 96.455, 96.455, 96.4975, 96.4975, 96.505, 96.505, 96.43, 96.43, 96.4775, 96.4775, 96.49, 96.49, 96.505, 96.505, 96.5475, 96.5475, 96.5625, 96.5625, 96.59, 96.59, 96.5575, 96.5575, 96.555, 96.555, 96.5125, 96.5125, 96.5225, 96.5225, 96.565, 96.565, 96.585, 96.585, 96.625, 96.625, 96.6225, 96.6225, 96.6125, 96.6125, 96.62, 96.62, 96.6075, 96.6075, 96.65, 96.65, 96.6025, 96.6025, 96.6025, 96.6025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 24.78
Round   1, Train loss: 2.297, Test loss: 2.297, Test accuracy: 30.50
Round   2, Train loss: 2.295, Test loss: 2.294, Test accuracy: 30.50
Round   3, Train loss: 2.288, Test loss: 2.289, Test accuracy: 29.83
Round   4, Train loss: 2.280, Test loss: 2.280, Test accuracy: 29.65
Round   5, Train loss: 2.253, Test loss: 2.254, Test accuracy: 28.85
Round   6, Train loss: 2.212, Test loss: 2.212, Test accuracy: 28.62
Round   7, Train loss: 2.146, Test loss: 2.166, Test accuracy: 34.28
Round   8, Train loss: 2.126, Test loss: 2.125, Test accuracy: 38.58
Round   9, Train loss: 2.066, Test loss: 2.073, Test accuracy: 44.05
Round  10, Train loss: 1.988, Test loss: 2.018, Test accuracy: 51.72
Round  11, Train loss: 1.959, Test loss: 1.970, Test accuracy: 53.98
Round  12, Train loss: 1.911, Test loss: 1.931, Test accuracy: 56.67
Round  13, Train loss: 1.865, Test loss: 1.901, Test accuracy: 60.88
Round  14, Train loss: 1.808, Test loss: 1.882, Test accuracy: 62.22
Round  15, Train loss: 1.800, Test loss: 1.858, Test accuracy: 64.00
Round  16, Train loss: 1.779, Test loss: 1.836, Test accuracy: 66.75
Round  17, Train loss: 1.726, Test loss: 1.827, Test accuracy: 67.03
Round  18, Train loss: 1.710, Test loss: 1.817, Test accuracy: 67.65
Round  19, Train loss: 1.733, Test loss: 1.806, Test accuracy: 68.28
Round  20, Train loss: 1.712, Test loss: 1.799, Test accuracy: 68.53
Round  21, Train loss: 1.729, Test loss: 1.778, Test accuracy: 70.07
Round  22, Train loss: 1.701, Test loss: 1.773, Test accuracy: 70.30
Round  23, Train loss: 1.682, Test loss: 1.766, Test accuracy: 70.52
Round  24, Train loss: 1.666, Test loss: 1.756, Test accuracy: 73.23
Round  25, Train loss: 1.636, Test loss: 1.748, Test accuracy: 73.82
Round  26, Train loss: 1.613, Test loss: 1.742, Test accuracy: 74.23
Round  27, Train loss: 1.621, Test loss: 1.735, Test accuracy: 74.57
Round  28, Train loss: 1.608, Test loss: 1.730, Test accuracy: 75.00
Round  29, Train loss: 1.605, Test loss: 1.724, Test accuracy: 75.27
Round  30, Train loss: 1.609, Test loss: 1.720, Test accuracy: 75.65
Round  31, Train loss: 1.594, Test loss: 1.716, Test accuracy: 76.03
Round  32, Train loss: 1.575, Test loss: 1.714, Test accuracy: 76.03
Round  33, Train loss: 1.576, Test loss: 1.712, Test accuracy: 76.13
Round  34, Train loss: 1.582, Test loss: 1.711, Test accuracy: 76.22
Round  35, Train loss: 1.581, Test loss: 1.709, Test accuracy: 76.32
Round  36, Train loss: 1.569, Test loss: 1.708, Test accuracy: 76.37
Round  37, Train loss: 1.586, Test loss: 1.707, Test accuracy: 76.42
Round  38, Train loss: 1.575, Test loss: 1.704, Test accuracy: 76.53
Round  39, Train loss: 1.535, Test loss: 1.676, Test accuracy: 80.77
Round  40, Train loss: 1.527, Test loss: 1.667, Test accuracy: 81.27
Round  41, Train loss: 1.506, Test loss: 1.661, Test accuracy: 81.93
Round  42, Train loss: 1.493, Test loss: 1.657, Test accuracy: 82.12
Round  43, Train loss: 1.491, Test loss: 1.654, Test accuracy: 82.27
Round  44, Train loss: 1.493, Test loss: 1.652, Test accuracy: 82.20
Round  45, Train loss: 1.496, Test loss: 1.648, Test accuracy: 82.63
Round  46, Train loss: 1.481, Test loss: 1.647, Test accuracy: 82.73
Round  47, Train loss: 1.486, Test loss: 1.645, Test accuracy: 82.85
Round  48, Train loss: 1.488, Test loss: 1.643, Test accuracy: 83.02
Round  49, Train loss: 1.486, Test loss: 1.643, Test accuracy: 82.97
Round  50, Train loss: 1.475, Test loss: 1.642, Test accuracy: 83.00
Round  51, Train loss: 1.480, Test loss: 1.640, Test accuracy: 83.02
Round  52, Train loss: 1.481, Test loss: 1.641, Test accuracy: 83.12
Round  53, Train loss: 1.482, Test loss: 1.640, Test accuracy: 83.15
Round  54, Train loss: 1.473, Test loss: 1.640, Test accuracy: 83.13
Round  55, Train loss: 1.478, Test loss: 1.640, Test accuracy: 83.15
Round  56, Train loss: 1.472, Test loss: 1.639, Test accuracy: 83.10
Round  57, Train loss: 1.476, Test loss: 1.638, Test accuracy: 83.22
Round  58, Train loss: 1.478, Test loss: 1.638, Test accuracy: 83.08
Round  59, Train loss: 1.477, Test loss: 1.638, Test accuracy: 83.10
Round  60, Train loss: 1.475, Test loss: 1.637, Test accuracy: 83.22
Round  61, Train loss: 1.475, Test loss: 1.637, Test accuracy: 83.17
Round  62, Train loss: 1.475, Test loss: 1.637, Test accuracy: 83.22
Round  63, Train loss: 1.476, Test loss: 1.637, Test accuracy: 83.27
Round  64, Train loss: 1.467, Test loss: 1.637, Test accuracy: 83.18
Round  65, Train loss: 1.476, Test loss: 1.637, Test accuracy: 83.17
Round  66, Train loss: 1.476, Test loss: 1.638, Test accuracy: 83.18
Round  67, Train loss: 1.471, Test loss: 1.637, Test accuracy: 83.28
Round  68, Train loss: 1.471, Test loss: 1.637, Test accuracy: 83.18
Round  69, Train loss: 1.474, Test loss: 1.637, Test accuracy: 83.08
Round  70, Train loss: 1.476, Test loss: 1.636, Test accuracy: 83.22
Round  71, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.15
Round  72, Train loss: 1.474, Test loss: 1.636, Test accuracy: 83.13
Round  73, Train loss: 1.478, Test loss: 1.636, Test accuracy: 83.15
Round  74, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.07
Round  75, Train loss: 1.473, Test loss: 1.636, Test accuracy: 83.13
Round  76, Train loss: 1.473, Test loss: 1.636, Test accuracy: 83.10
Round  77, Train loss: 1.474, Test loss: 1.636, Test accuracy: 83.08
Round  78, Train loss: 1.473, Test loss: 1.636, Test accuracy: 83.10
Round  79, Train loss: 1.477, Test loss: 1.636, Test accuracy: 83.07
Round  80, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.07
Round  81, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.07
Round  82, Train loss: 1.472, Test loss: 1.635, Test accuracy: 83.12
Round  83, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.13
Round  84, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.12
Round  85, Train loss: 1.471, Test loss: 1.635, Test accuracy: 83.15
Round  86, Train loss: 1.470, Test loss: 1.635, Test accuracy: 83.15
Round  87, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.05
Round  88, Train loss: 1.470, Test loss: 1.635, Test accuracy: 83.10
Round  89, Train loss: 1.470, Test loss: 1.635, Test accuracy: 83.12
Round  90, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.08
Round  91, Train loss: 1.472, Test loss: 1.635, Test accuracy: 83.12
Round  92, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.10
Round  93, Train loss: 1.475, Test loss: 1.635, Test accuracy: 83.12
Round  94, Train loss: 1.472, Test loss: 1.634, Test accuracy: 83.05
Round  95, Train loss: 1.476, Test loss: 1.634, Test accuracy: 83.08
Round  96, Train loss: 1.473, Test loss: 1.634, Test accuracy: 83.08
Round  97, Train loss: 1.475, Test loss: 1.634, Test accuracy: 83.10
Round  98, Train loss: 1.475, Test loss: 1.634, Test accuracy: 83.12
Round  99, Train loss: 1.478, Test loss: 1.635, Test accuracy: 83.08/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.472, Test loss: 1.634, Test accuracy: 83.22
Average accuracy final 10 rounds: 83.09333333333333 

731.9901342391968
[0.6391983032226562, 1.2783966064453125, 1.8654911518096924, 2.4525856971740723, 3.034757375717163, 3.616929054260254, 4.136734485626221, 4.6565399169921875, 5.198365688323975, 5.740191459655762, 6.258649110794067, 6.777106761932373, 7.340189218521118, 7.903271675109863, 8.433603525161743, 8.963935375213623, 9.46847152709961, 9.973007678985596, 10.512972831726074, 11.052937984466553, 11.580363273620605, 12.107788562774658, 12.647832155227661, 13.187875747680664, 13.709259271621704, 14.230642795562744, 14.748307943344116, 15.265973091125488, 15.810805320739746, 16.355637550354004, 16.894309282302856, 17.43298101425171, 17.951533794403076, 18.470086574554443, 18.99966025352478, 19.529233932495117, 20.0368754863739, 20.544517040252686, 21.09084725379944, 21.63717746734619, 22.142024040222168, 22.646870613098145, 23.169878482818604, 23.692886352539062, 24.212071657180786, 24.73125696182251, 25.236923456192017, 25.742589950561523, 26.25899624824524, 26.775402545928955, 27.31085205078125, 27.846301555633545, 28.372254371643066, 28.898207187652588, 29.435012340545654, 29.97181749343872, 30.48974061012268, 31.00766372680664, 31.539568662643433, 32.071473598480225, 32.58342146873474, 33.09536933898926, 33.623335123062134, 34.15130090713501, 34.69672513008118, 35.242149353027344, 35.75456929206848, 36.26698923110962, 36.795308351516724, 37.32362747192383, 37.878312826156616, 38.432998180389404, 38.96994376182556, 39.50688934326172, 40.04806876182556, 40.589248180389404, 41.11412334442139, 41.63899850845337, 42.16348671913147, 42.68797492980957, 43.22293758392334, 43.75790023803711, 44.274624824523926, 44.79134941101074, 45.32703924179077, 45.8627290725708, 46.389689207077026, 46.91664934158325, 47.44992733001709, 47.98320531845093, 48.48983955383301, 48.99647378921509, 49.51246404647827, 50.028454303741455, 50.570643186569214, 51.11283206939697, 51.64701175689697, 52.18119144439697, 52.70453667640686, 53.22788190841675, 53.783621311187744, 54.33936071395874, 54.875335693359375, 55.41131067276001, 55.954819202423096, 56.49832773208618, 57.02813649177551, 57.557945251464844, 58.11192083358765, 58.66589641571045, 59.220948934555054, 59.77600145339966, 60.297343015670776, 60.818684577941895, 61.31932735443115, 61.81997013092041, 62.37077522277832, 62.92158031463623, 63.46300029754639, 64.00442028045654, 64.54496955871582, 65.0855188369751, 65.62561964988708, 66.16572046279907, 66.72922325134277, 67.29272603988647, 67.82836842536926, 68.36401081085205, 68.88929390907288, 69.4145770072937, 69.96890830993652, 70.52323961257935, 71.07453632354736, 71.62583303451538, 72.16946148872375, 72.71308994293213, 73.24405455589294, 73.77501916885376, 74.28044104576111, 74.78586292266846, 75.3494644165039, 75.91306591033936, 76.44299960136414, 76.97293329238892, 77.50799584388733, 78.04305839538574, 78.59666061401367, 79.1502628326416, 79.70123291015625, 80.2522029876709, 80.778315782547, 81.3044285774231, 81.83393335342407, 82.36343812942505, 82.9020848274231, 83.44073152542114, 83.9949460029602, 84.54916048049927, 85.06814360618591, 85.58712673187256, 86.1210150718689, 86.65490341186523, 87.17307829856873, 87.69125318527222, 88.22336053848267, 88.75546789169312, 89.30192518234253, 89.84838247299194, 90.3848946094513, 90.92140674591064, 91.47529458999634, 92.02918243408203, 92.55845594406128, 93.08772945404053, 93.59572052955627, 94.10371160507202, 94.6441445350647, 95.18457746505737, 95.7295618057251, 96.27454614639282, 96.83733439445496, 97.40012264251709, 97.93120884895325, 98.4622950553894, 98.98944997787476, 99.51660490036011, 100.05062103271484, 100.58463716506958, 101.11128258705139, 101.6379280090332, 102.16739416122437, 102.69686031341553, 103.25537419319153, 103.81388807296753, 104.35599398612976, 104.89809989929199, 105.45996069908142, 106.02182149887085, 106.5453188419342, 107.06881618499756, 108.15942358970642, 109.25003099441528]
[24.783333333333335, 24.783333333333335, 30.5, 30.5, 30.5, 30.5, 29.833333333333332, 29.833333333333332, 29.65, 29.65, 28.85, 28.85, 28.616666666666667, 28.616666666666667, 34.28333333333333, 34.28333333333333, 38.583333333333336, 38.583333333333336, 44.05, 44.05, 51.71666666666667, 51.71666666666667, 53.983333333333334, 53.983333333333334, 56.666666666666664, 56.666666666666664, 60.88333333333333, 60.88333333333333, 62.21666666666667, 62.21666666666667, 64.0, 64.0, 66.75, 66.75, 67.03333333333333, 67.03333333333333, 67.65, 67.65, 68.28333333333333, 68.28333333333333, 68.53333333333333, 68.53333333333333, 70.06666666666666, 70.06666666666666, 70.3, 70.3, 70.51666666666667, 70.51666666666667, 73.23333333333333, 73.23333333333333, 73.81666666666666, 73.81666666666666, 74.23333333333333, 74.23333333333333, 74.56666666666666, 74.56666666666666, 75.0, 75.0, 75.26666666666667, 75.26666666666667, 75.65, 75.65, 76.03333333333333, 76.03333333333333, 76.03333333333333, 76.03333333333333, 76.13333333333334, 76.13333333333334, 76.21666666666667, 76.21666666666667, 76.31666666666666, 76.31666666666666, 76.36666666666666, 76.36666666666666, 76.41666666666667, 76.41666666666667, 76.53333333333333, 76.53333333333333, 80.76666666666667, 80.76666666666667, 81.26666666666667, 81.26666666666667, 81.93333333333334, 81.93333333333334, 82.11666666666666, 82.11666666666666, 82.26666666666667, 82.26666666666667, 82.2, 82.2, 82.63333333333334, 82.63333333333334, 82.73333333333333, 82.73333333333333, 82.85, 82.85, 83.01666666666667, 83.01666666666667, 82.96666666666667, 82.96666666666667, 83.0, 83.0, 83.01666666666667, 83.01666666666667, 83.11666666666666, 83.11666666666666, 83.15, 83.15, 83.13333333333334, 83.13333333333334, 83.15, 83.15, 83.1, 83.1, 83.21666666666667, 83.21666666666667, 83.08333333333333, 83.08333333333333, 83.1, 83.1, 83.21666666666667, 83.21666666666667, 83.16666666666667, 83.16666666666667, 83.21666666666667, 83.21666666666667, 83.26666666666667, 83.26666666666667, 83.18333333333334, 83.18333333333334, 83.16666666666667, 83.16666666666667, 83.18333333333334, 83.18333333333334, 83.28333333333333, 83.28333333333333, 83.18333333333334, 83.18333333333334, 83.08333333333333, 83.08333333333333, 83.21666666666667, 83.21666666666667, 83.15, 83.15, 83.13333333333334, 83.13333333333334, 83.15, 83.15, 83.06666666666666, 83.06666666666666, 83.13333333333334, 83.13333333333334, 83.1, 83.1, 83.08333333333333, 83.08333333333333, 83.1, 83.1, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.11666666666666, 83.11666666666666, 83.13333333333334, 83.13333333333334, 83.11666666666666, 83.11666666666666, 83.15, 83.15, 83.15, 83.15, 83.05, 83.05, 83.1, 83.1, 83.11666666666666, 83.11666666666666, 83.08333333333333, 83.08333333333333, 83.11666666666666, 83.11666666666666, 83.1, 83.1, 83.11666666666666, 83.11666666666666, 83.05, 83.05, 83.08333333333333, 83.08333333333333, 83.08333333333333, 83.08333333333333, 83.1, 83.1, 83.11666666666666, 83.11666666666666, 83.08333333333333, 83.08333333333333, 83.21666666666667, 83.21666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.719, Test loss: 2.299, Test accuracy: 18.68
Round   1, Train loss: 1.699, Test loss: 2.294, Test accuracy: 22.30
Round   2, Train loss: 1.655, Test loss: 2.285, Test accuracy: 30.58
Round   3, Train loss: 1.554, Test loss: 2.267, Test accuracy: 42.13
Round   4, Train loss: 1.449, Test loss: 2.233, Test accuracy: 52.02
Round   5, Train loss: 1.372, Test loss: 2.183, Test accuracy: 60.17
Round   6, Train loss: 1.340, Test loss: 2.149, Test accuracy: 64.22
Round   7, Train loss: 1.287, Test loss: 2.123, Test accuracy: 66.57
Round   8, Train loss: 1.256, Test loss: 2.105, Test accuracy: 68.43
Round   9, Train loss: 1.237, Test loss: 2.085, Test accuracy: 70.62
Round  10, Train loss: 1.223, Test loss: 2.071, Test accuracy: 74.23
Round  11, Train loss: 1.221, Test loss: 2.062, Test accuracy: 76.15
Round  12, Train loss: 1.201, Test loss: 2.051, Test accuracy: 76.50
Round  13, Train loss: 1.207, Test loss: 2.044, Test accuracy: 76.58
Round  14, Train loss: 1.194, Test loss: 2.037, Test accuracy: 78.17
Round  15, Train loss: 1.184, Test loss: 2.032, Test accuracy: 77.93
Round  16, Train loss: 1.184, Test loss: 2.027, Test accuracy: 78.05
Round  17, Train loss: 1.182, Test loss: 2.021, Test accuracy: 78.28
Round  18, Train loss: 1.185, Test loss: 2.018, Test accuracy: 78.12
Round  19, Train loss: 1.189, Test loss: 2.015, Test accuracy: 78.33
Round  20, Train loss: 1.180, Test loss: 2.011, Test accuracy: 78.07
Round  21, Train loss: 1.165, Test loss: 2.008, Test accuracy: 77.73
Round  22, Train loss: 1.172, Test loss: 2.006, Test accuracy: 77.65
Round  23, Train loss: 1.173, Test loss: 2.003, Test accuracy: 77.53
Round  24, Train loss: 1.176, Test loss: 2.001, Test accuracy: 77.32
Round  25, Train loss: 1.170, Test loss: 2.000, Test accuracy: 77.02
Round  26, Train loss: 1.173, Test loss: 1.999, Test accuracy: 77.12
Round  27, Train loss: 1.178, Test loss: 1.997, Test accuracy: 76.87
Round  28, Train loss: 1.179, Test loss: 1.997, Test accuracy: 76.48
Round  29, Train loss: 1.174, Test loss: 1.995, Test accuracy: 76.07
Round  30, Train loss: 1.177, Test loss: 1.994, Test accuracy: 75.78
Round  31, Train loss: 1.181, Test loss: 1.993, Test accuracy: 75.80
Round  32, Train loss: 1.175, Test loss: 1.992, Test accuracy: 75.68
Round  33, Train loss: 1.175, Test loss: 1.992, Test accuracy: 75.02
Round  34, Train loss: 1.174, Test loss: 1.992, Test accuracy: 74.72
Round  35, Train loss: 1.174, Test loss: 1.991, Test accuracy: 74.53
Round  36, Train loss: 1.171, Test loss: 1.990, Test accuracy: 74.25
Round  37, Train loss: 1.174, Test loss: 1.990, Test accuracy: 74.12
Round  38, Train loss: 1.174, Test loss: 1.989, Test accuracy: 74.27
Round  39, Train loss: 1.172, Test loss: 1.989, Test accuracy: 73.92
Round  40, Train loss: 1.175, Test loss: 1.989, Test accuracy: 73.65
Round  41, Train loss: 1.177, Test loss: 1.989, Test accuracy: 73.40
Round  42, Train loss: 1.176, Test loss: 1.990, Test accuracy: 73.20
Round  43, Train loss: 1.164, Test loss: 1.989, Test accuracy: 72.88
Round  44, Train loss: 1.172, Test loss: 1.987, Test accuracy: 72.48
Round  45, Train loss: 1.168, Test loss: 1.988, Test accuracy: 72.25
Round  46, Train loss: 1.176, Test loss: 1.987, Test accuracy: 72.18
Round  47, Train loss: 1.168, Test loss: 1.988, Test accuracy: 71.77
Round  48, Train loss: 1.165, Test loss: 1.988, Test accuracy: 71.62
Round  49, Train loss: 1.169, Test loss: 1.988, Test accuracy: 71.18
Round  50, Train loss: 1.169, Test loss: 1.988, Test accuracy: 70.92
Round  51, Train loss: 1.172, Test loss: 1.987, Test accuracy: 70.87
Round  52, Train loss: 1.170, Test loss: 1.987, Test accuracy: 70.83
Round  53, Train loss: 1.176, Test loss: 1.987, Test accuracy: 70.50
Round  54, Train loss: 1.167, Test loss: 1.988, Test accuracy: 70.27
Round  55, Train loss: 1.169, Test loss: 1.987, Test accuracy: 70.28
Round  56, Train loss: 1.172, Test loss: 1.986, Test accuracy: 70.23
Round  57, Train loss: 1.166, Test loss: 1.986, Test accuracy: 69.92
Round  58, Train loss: 1.176, Test loss: 1.987, Test accuracy: 69.83
Round  59, Train loss: 1.178, Test loss: 1.987, Test accuracy: 69.47
Round  60, Train loss: 1.177, Test loss: 1.987, Test accuracy: 69.47
Round  61, Train loss: 1.177, Test loss: 1.987, Test accuracy: 69.52
Round  62, Train loss: 1.175, Test loss: 1.987, Test accuracy: 69.32
Round  63, Train loss: 1.169, Test loss: 1.987, Test accuracy: 69.05
Round  64, Train loss: 1.166, Test loss: 1.987, Test accuracy: 69.00
Round  65, Train loss: 1.169, Test loss: 1.988, Test accuracy: 68.85
Round  66, Train loss: 1.179, Test loss: 1.987, Test accuracy: 68.75
Round  67, Train loss: 1.164, Test loss: 1.987, Test accuracy: 68.22
Round  68, Train loss: 1.174, Test loss: 1.987, Test accuracy: 68.10
Round  69, Train loss: 1.171, Test loss: 1.988, Test accuracy: 67.85
Round  70, Train loss: 1.168, Test loss: 1.988, Test accuracy: 67.72
Round  71, Train loss: 1.173, Test loss: 1.989, Test accuracy: 67.42
Round  72, Train loss: 1.175, Test loss: 1.989, Test accuracy: 67.35
Round  73, Train loss: 1.168, Test loss: 1.988, Test accuracy: 67.10
Round  74, Train loss: 1.170, Test loss: 1.989, Test accuracy: 67.07
Round  75, Train loss: 1.173, Test loss: 1.989, Test accuracy: 66.83
Round  76, Train loss: 1.170, Test loss: 1.989, Test accuracy: 66.52
Round  77, Train loss: 1.160, Test loss: 1.989, Test accuracy: 66.28
Round  78, Train loss: 1.172, Test loss: 1.989, Test accuracy: 66.45
Round  79, Train loss: 1.174, Test loss: 1.990, Test accuracy: 66.07
Round  80, Train loss: 1.173, Test loss: 1.989, Test accuracy: 66.07
Round  81, Train loss: 1.162, Test loss: 1.989, Test accuracy: 65.88
Round  82, Train loss: 1.175, Test loss: 1.989, Test accuracy: 66.07
Round  83, Train loss: 1.170, Test loss: 1.990, Test accuracy: 65.92
Round  84, Train loss: 1.174, Test loss: 1.990, Test accuracy: 65.68
Round  85, Train loss: 1.170, Test loss: 1.990, Test accuracy: 65.72
Round  86, Train loss: 1.177, Test loss: 1.990, Test accuracy: 65.63
Round  87, Train loss: 1.181, Test loss: 1.990, Test accuracy: 65.48
Round  88, Train loss: 1.169, Test loss: 1.991, Test accuracy: 65.27
Round  89, Train loss: 1.169, Test loss: 1.990, Test accuracy: 65.15
Round  90, Train loss: 1.161, Test loss: 1.990, Test accuracy: 65.18
Round  91, Train loss: 1.169, Test loss: 1.991, Test accuracy: 64.93
Round  92, Train loss: 1.164, Test loss: 1.991, Test accuracy: 64.97
Round  93, Train loss: 1.165, Test loss: 1.991, Test accuracy: 64.73
Round  94, Train loss: 1.173, Test loss: 1.991, Test accuracy: 64.73
Round  95, Train loss: 1.178, Test loss: 1.992, Test accuracy: 64.62
Round  96, Train loss: 1.171, Test loss: 1.991, Test accuracy: 64.67
Round  97, Train loss: 1.167, Test loss: 1.992, Test accuracy: 64.50
Round  98, Train loss: 1.172, Test loss: 1.992, Test accuracy: 64.25
Round  99, Train loss: 1.180, Test loss: 1.992, Test accuracy: 64.25
Final Round, Train loss: 1.170, Test loss: 1.993, Test accuracy: 63.75
Average accuracy final 10 rounds: 64.68333333333334
831.1946139335632
[]
[18.683333333333334, 22.3, 30.583333333333332, 42.13333333333333, 52.016666666666666, 60.166666666666664, 64.21666666666667, 66.56666666666666, 68.43333333333334, 70.61666666666666, 74.23333333333333, 76.15, 76.5, 76.58333333333333, 78.16666666666667, 77.93333333333334, 78.05, 78.28333333333333, 78.11666666666666, 78.33333333333333, 78.06666666666666, 77.73333333333333, 77.65, 77.53333333333333, 77.31666666666666, 77.01666666666667, 77.11666666666666, 76.86666666666666, 76.48333333333333, 76.06666666666666, 75.78333333333333, 75.8, 75.68333333333334, 75.01666666666667, 74.71666666666667, 74.53333333333333, 74.25, 74.11666666666666, 74.26666666666667, 73.91666666666667, 73.65, 73.4, 73.2, 72.88333333333334, 72.48333333333333, 72.25, 72.18333333333334, 71.76666666666667, 71.61666666666666, 71.18333333333334, 70.91666666666667, 70.86666666666666, 70.83333333333333, 70.5, 70.26666666666667, 70.28333333333333, 70.23333333333333, 69.91666666666667, 69.83333333333333, 69.46666666666667, 69.46666666666667, 69.51666666666667, 69.31666666666666, 69.05, 69.0, 68.85, 68.75, 68.21666666666667, 68.1, 67.85, 67.71666666666667, 67.41666666666667, 67.35, 67.1, 67.06666666666666, 66.83333333333333, 66.51666666666667, 66.28333333333333, 66.45, 66.06666666666666, 66.06666666666666, 65.88333333333334, 66.06666666666666, 65.91666666666667, 65.68333333333334, 65.71666666666667, 65.63333333333334, 65.48333333333333, 65.26666666666667, 65.15, 65.18333333333334, 64.93333333333334, 64.96666666666667, 64.73333333333333, 64.73333333333333, 64.61666666666666, 64.66666666666667, 64.5, 64.25, 64.25, 63.75]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.96
Round   1, Train loss: 2.311, Test loss: 2.299, Test accuracy: 16.91
Round   2, Train loss: 2.301, Test loss: 2.297, Test accuracy: 18.95
Round   3, Train loss: 2.294, Test loss: 2.295, Test accuracy: 21.01
Round   4, Train loss: 2.291, Test loss: 2.293, Test accuracy: 21.01
Round   5, Train loss: 2.289, Test loss: 2.291, Test accuracy: 22.27
Round   6, Train loss: 2.291, Test loss: 2.292, Test accuracy: 21.47
Round   7, Train loss: 2.275, Test loss: 2.285, Test accuracy: 21.71
Round   8, Train loss: 2.225, Test loss: 2.270, Test accuracy: 26.27
Round   9, Train loss: 2.227, Test loss: 2.263, Test accuracy: 26.17
Round  10, Train loss: 2.197, Test loss: 2.257, Test accuracy: 25.22
Round  11, Train loss: 1.999, Test loss: 2.219, Test accuracy: 31.28
Round  12, Train loss: 2.002, Test loss: 2.191, Test accuracy: 34.77
Round  13, Train loss: 1.962, Test loss: 2.179, Test accuracy: 35.17
Round  14, Train loss: 1.437, Test loss: 2.103, Test accuracy: 44.08
Round  15, Train loss: 1.970, Test loss: 2.107, Test accuracy: 43.86
Round  16, Train loss: 2.212, Test loss: 2.145, Test accuracy: 38.16
Round  17, Train loss: 1.340, Test loss: 2.065, Test accuracy: 45.31
Round  18, Train loss: 1.671, Test loss: 2.060, Test accuracy: 45.45
Round  19, Train loss: 2.182, Test loss: 2.069, Test accuracy: 42.58
Round  20, Train loss: 1.270, Test loss: 2.029, Test accuracy: 45.02
Round  21, Train loss: 1.400, Test loss: 1.998, Test accuracy: 48.05
Round  22, Train loss: 0.947, Test loss: 1.946, Test accuracy: 52.28
Round  23, Train loss: 1.398, Test loss: 1.968, Test accuracy: 50.72
Round  24, Train loss: 2.440, Test loss: 2.052, Test accuracy: 42.01
Round  25, Train loss: 2.412, Test loss: 2.104, Test accuracy: 35.15
Round  26, Train loss: 1.420, Test loss: 2.034, Test accuracy: 42.52
Round  27, Train loss: 1.307, Test loss: 2.004, Test accuracy: 45.84
Round  28, Train loss: 0.929, Test loss: 1.912, Test accuracy: 55.67
Round  29, Train loss: 1.346, Test loss: 1.901, Test accuracy: 57.63
Round  30, Train loss: 1.644, Test loss: 1.902, Test accuracy: 58.23
Round  31, Train loss: 2.161, Test loss: 1.965, Test accuracy: 52.66
Round  32, Train loss: 1.481, Test loss: 1.948, Test accuracy: 54.91
Round  33, Train loss: 1.574, Test loss: 1.937, Test accuracy: 55.01
Round  34, Train loss: 0.806, Test loss: 1.895, Test accuracy: 59.31
Round  35, Train loss: 1.244, Test loss: 1.906, Test accuracy: 58.02
Round  36, Train loss: 0.900, Test loss: 1.860, Test accuracy: 62.09
Round  37, Train loss: 1.206, Test loss: 1.881, Test accuracy: 60.08
Round  38, Train loss: 0.956, Test loss: 1.846, Test accuracy: 62.92
Round  39, Train loss: 0.926, Test loss: 1.839, Test accuracy: 63.12
Round  40, Train loss: 0.807, Test loss: 1.818, Test accuracy: 65.47
Round  41, Train loss: 0.612, Test loss: 1.799, Test accuracy: 67.25
Round  42, Train loss: 0.234, Test loss: 1.788, Test accuracy: 68.16
Round  43, Train loss: 0.470, Test loss: 1.765, Test accuracy: 70.78
Round  44, Train loss: 0.504, Test loss: 1.781, Test accuracy: 69.03
Round  45, Train loss: 0.381, Test loss: 1.783, Test accuracy: 68.61
Round  46, Train loss: 0.773, Test loss: 1.806, Test accuracy: 66.27
Round  47, Train loss: 0.447, Test loss: 1.780, Test accuracy: 68.96
Round  48, Train loss: 0.486, Test loss: 1.780, Test accuracy: 69.03
Round  49, Train loss: 0.503, Test loss: 1.779, Test accuracy: 68.98
Round  50, Train loss: 0.471, Test loss: 1.758, Test accuracy: 71.20
Round  51, Train loss: 0.213, Test loss: 1.767, Test accuracy: 70.06
Round  52, Train loss: 0.098, Test loss: 1.745, Test accuracy: 72.32
Round  53, Train loss: 0.447, Test loss: 1.749, Test accuracy: 71.87
Round  54, Train loss: 0.150, Test loss: 1.723, Test accuracy: 74.50
Round  55, Train loss: 0.544, Test loss: 1.754, Test accuracy: 71.43
Round  56, Train loss: 0.400, Test loss: 1.766, Test accuracy: 70.14
Round  57, Train loss: 0.113, Test loss: 1.728, Test accuracy: 73.97
Round  58, Train loss: 0.046, Test loss: 1.733, Test accuracy: 73.42
Round  59, Train loss: 0.300, Test loss: 1.752, Test accuracy: 71.53
Round  60, Train loss: -0.239, Test loss: 1.722, Test accuracy: 74.41
Round  61, Train loss: -0.013, Test loss: 1.728, Test accuracy: 73.84
Round  62, Train loss: -0.022, Test loss: 1.729, Test accuracy: 73.61
Round  63, Train loss: -0.350, Test loss: 1.730, Test accuracy: 73.34
Round  64, Train loss: -0.616, Test loss: 1.697, Test accuracy: 76.68
Round  65, Train loss: 0.308, Test loss: 1.723, Test accuracy: 74.23
Round  66, Train loss: 0.290, Test loss: 1.735, Test accuracy: 72.94
Round  67, Train loss: -0.469, Test loss: 1.692, Test accuracy: 77.27
Round  68, Train loss: 0.415, Test loss: 1.716, Test accuracy: 74.92
Round  69, Train loss: 0.210, Test loss: 1.718, Test accuracy: 74.67
Round  70, Train loss: 0.275, Test loss: 1.725, Test accuracy: 74.12
Round  71, Train loss: 0.015, Test loss: 1.712, Test accuracy: 75.54
Round  72, Train loss: -0.007, Test loss: 1.699, Test accuracy: 76.85
Round  73, Train loss: 0.273, Test loss: 1.710, Test accuracy: 75.83
Round  74, Train loss: -0.341, Test loss: 1.695, Test accuracy: 77.37
Round  75, Train loss: -0.147, Test loss: 1.703, Test accuracy: 76.71
Round  76, Train loss: -0.070, Test loss: 1.716, Test accuracy: 75.48
Round  77, Train loss: 0.061, Test loss: 1.696, Test accuracy: 77.49
Round  78, Train loss: -0.013, Test loss: 1.697, Test accuracy: 77.05
Round  79, Train loss: 0.106, Test loss: 1.691, Test accuracy: 77.65
Round  80, Train loss: 0.006, Test loss: 1.711, Test accuracy: 75.58
Round  81, Train loss: -0.066, Test loss: 1.701, Test accuracy: 76.46
Round  82, Train loss: -0.198, Test loss: 1.685, Test accuracy: 78.28
Round  83, Train loss: -0.247, Test loss: 1.688, Test accuracy: 77.85
Round  84, Train loss: -0.468, Test loss: 1.691, Test accuracy: 77.44
Round  85, Train loss: -0.323, Test loss: 1.686, Test accuracy: 77.97
Round  86, Train loss: -0.320, Test loss: 1.685, Test accuracy: 77.96
Round  87, Train loss: 0.004, Test loss: 1.691, Test accuracy: 77.33
Round  88, Train loss: -0.307, Test loss: 1.680, Test accuracy: 78.43
Round  89, Train loss: -0.243, Test loss: 1.690, Test accuracy: 77.47
Round  90, Train loss: -0.594, Test loss: 1.677, Test accuracy: 78.72
Round  91, Train loss: -0.170, Test loss: 1.686, Test accuracy: 77.88
Round  92, Train loss: -0.276, Test loss: 1.690, Test accuracy: 77.55
Round  93, Train loss: -0.245, Test loss: 1.679, Test accuracy: 78.69
Round  94, Train loss: -0.401, Test loss: 1.677, Test accuracy: 78.96
Round  95, Train loss: -0.342, Test loss: 1.682, Test accuracy: 78.34
Round  96, Train loss: -0.518, Test loss: 1.678, Test accuracy: 78.67
Round  97, Train loss: -0.259, Test loss: 1.682, Test accuracy: 78.27
Round  98, Train loss: -0.515, Test loss: 1.686, Test accuracy: 77.90
Round  99, Train loss: -0.287, Test loss: 1.675, Test accuracy: 78.99
Final Round, Train loss: 1.710, Test loss: 1.707, Test accuracy: 76.31
Average accuracy final 10 rounds: 78.39750000000001
Average global accuracy final 10 rounds: 78.39750000000001
1292.2029466629028
[]
[13.958333333333334, 16.908333333333335, 18.95, 21.008333333333333, 21.008333333333333, 22.275, 21.466666666666665, 21.708333333333332, 26.266666666666666, 26.166666666666668, 25.216666666666665, 31.283333333333335, 34.766666666666666, 35.175, 44.075, 43.858333333333334, 38.15833333333333, 45.30833333333333, 45.45, 42.575, 45.025, 48.05, 52.28333333333333, 50.71666666666667, 42.00833333333333, 35.15, 42.516666666666666, 45.84166666666667, 55.675, 57.63333333333333, 58.225, 52.65833333333333, 54.90833333333333, 55.00833333333333, 59.30833333333333, 58.016666666666666, 62.09166666666667, 60.083333333333336, 62.925, 63.11666666666667, 65.46666666666667, 67.25, 68.15833333333333, 70.775, 69.025, 68.60833333333333, 66.26666666666667, 68.95833333333333, 69.03333333333333, 68.98333333333333, 71.2, 70.05833333333334, 72.31666666666666, 71.86666666666666, 74.5, 71.43333333333334, 70.14166666666667, 73.96666666666667, 73.425, 71.53333333333333, 74.40833333333333, 73.84166666666667, 73.60833333333333, 73.34166666666667, 76.68333333333334, 74.23333333333333, 72.94166666666666, 77.26666666666667, 74.91666666666667, 74.675, 74.125, 75.54166666666667, 76.85, 75.83333333333333, 77.36666666666666, 76.70833333333333, 75.48333333333333, 77.49166666666666, 77.05, 77.65, 75.575, 76.45833333333333, 78.28333333333333, 77.85, 77.44166666666666, 77.975, 77.95833333333333, 77.33333333333333, 78.43333333333334, 77.475, 78.71666666666667, 77.88333333333334, 77.55, 78.69166666666666, 78.95833333333333, 78.34166666666667, 78.675, 78.26666666666667, 77.9, 78.99166666666666, 76.30833333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.37
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.38
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.38
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.39
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.37
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.39
Round   3, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.38
Round   3, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.39
Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.38
Round   4, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.39
Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.39
Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.38
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.44
Round   7, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.39
Round   7, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.43
Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round   9, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round   9, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  10, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  10, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  11, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  11, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  12, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  12, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  13, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  13, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  14, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  15, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  15, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  16, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.45
Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  17, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.44
Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.44
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.45
Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.47
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.46
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.46
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  59, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  61, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  62, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  63, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  63, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  64, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  65, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  65, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  66, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  66, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  67, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  67, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  68, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  68, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  69, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  69, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  70, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  71, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  71, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  72, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  72, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  73, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  73, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  74, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  74, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  75, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.54
Round  75, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  76, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.55
Round  76, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  77, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  77, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.54
Round  78, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.57
Round  78, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.54
Round  79, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.57
Round  79, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.54
Round  80, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  80, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  81, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  81, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.57
Round  82, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  83, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.58
Round  83, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  84, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.58
Round  84, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.59
Round  85, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.58
Round  85, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  86, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.60
Round  86, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  87, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.60
Round  87, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  88, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.61
Round  88, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  89, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.60
Round  89, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.62
Round  90, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.63
Round  90, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.63
Round  91, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.65
Round  91, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.68
Round  92, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.65
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.64
Round  93, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.64
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.64
Round  94, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.65
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.66
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.66
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.70
Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.65
Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.66
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.68
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.69
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Average accuracy final 10 rounds: 9.6525 

Average global accuracy final 10 rounds: 9.658333333333331 

1555.272881269455
[1.296515703201294, 2.5437402725219727, 3.7805838584899902, 4.935051679611206, 6.125579833984375, 7.404958724975586, 8.690626382827759, 9.933756351470947, 11.12629508972168, 12.365580797195435, 13.67121434211731, 14.960506916046143, 16.202524662017822, 17.448606252670288, 18.705224990844727, 19.984943866729736, 21.24941635131836, 22.47550654411316, 23.71184277534485, 24.989012241363525, 26.285255670547485, 27.545510053634644, 28.801198720932007, 30.02093744277954, 31.277238130569458, 32.579107999801636, 33.88041305541992, 35.10579586029053, 36.319775104522705, 37.636680126190186, 38.90259289741516, 40.17058873176575, 41.40043020248413, 42.67914843559265, 43.96069669723511, 45.212814807891846, 46.415985107421875, 47.65088963508606, 48.980292558670044, 50.27839469909668, 51.536787033081055, 52.80252003669739, 54.04987573623657, 55.31317138671875, 56.61258268356323, 57.90086269378662, 59.188443660736084, 60.452582597732544, 61.74173545837402, 63.04247498512268, 64.25906538963318, 65.46394491195679, 66.64602088928223, 67.85388779640198, 69.05333685874939, 70.168527841568, 71.41051816940308, 72.77644538879395, 74.09136056900024, 75.38304018974304, 76.63316583633423, 77.90718221664429, 79.1943199634552, 80.4692370891571, 81.70615458488464, 82.91433215141296, 84.18845891952515, 85.43416833877563, 86.73744893074036, 88.0253746509552, 89.25355410575867, 90.54275107383728, 91.87713551521301, 93.14519500732422, 94.42100024223328, 95.6778872013092, 97.0012788772583, 98.29925680160522, 99.61009955406189, 100.83241248130798, 102.0585560798645, 103.28940320014954, 104.44377207756042, 105.59585881233215, 106.82708978652954, 108.09629893302917, 109.30764961242676, 110.48341083526611, 111.64731931686401, 112.92252564430237, 114.19318270683289, 115.52127242088318, 116.84069418907166, 118.07437992095947, 119.32302522659302, 120.62693881988525, 121.91274690628052, 123.21844553947449, 124.52076411247253, 125.7873899936676, 127.94734025001526]
[9.366666666666667, 9.375, 9.366666666666667, 9.375, 9.375, 9.391666666666667, 9.383333333333333, 9.391666666666667, 9.425, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.45, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.45, 9.458333333333334, 9.458333333333334, 9.475, 9.475, 9.475, 9.483333333333333, 9.483333333333333, 9.491666666666667, 9.491666666666667, 9.483333333333333, 9.483333333333333, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.483333333333333, 9.483333333333333, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.5, 9.508333333333333, 9.508333333333333, 9.508333333333333, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.508333333333333, 9.516666666666667, 9.516666666666667, 9.516666666666667, 9.516666666666667, 9.533333333333333, 9.533333333333333, 9.533333333333333, 9.525, 9.525, 9.541666666666666, 9.55, 9.558333333333334, 9.566666666666666, 9.566666666666666, 9.558333333333334, 9.558333333333334, 9.566666666666666, 9.583333333333334, 9.583333333333334, 9.583333333333334, 9.6, 9.6, 9.608333333333333, 9.6, 9.633333333333333, 9.65, 9.65, 9.641666666666667, 9.65, 9.658333333333333, 9.658333333333333, 9.65, 9.658333333333333, 9.675, 9.691666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.298, Test accuracy: 18.69
Round   1, Train loss: 2.297, Test loss: 2.287, Test accuracy: 21.94
Round   2, Train loss: 2.282, Test loss: 2.209, Test accuracy: 33.19
Round   3, Train loss: 2.195, Test loss: 2.051, Test accuracy: 45.93
Round   4, Train loss: 2.064, Test loss: 1.913, Test accuracy: 56.42
Round   5, Train loss: 1.957, Test loss: 1.824, Test accuracy: 66.60
Round   6, Train loss: 1.846, Test loss: 1.766, Test accuracy: 71.67
Round   7, Train loss: 1.809, Test loss: 1.746, Test accuracy: 72.81
Round   8, Train loss: 1.795, Test loss: 1.734, Test accuracy: 73.57
Round   9, Train loss: 1.805, Test loss: 1.730, Test accuracy: 73.77
Round  10, Train loss: 1.719, Test loss: 1.725, Test accuracy: 74.13
Round  11, Train loss: 1.723, Test loss: 1.722, Test accuracy: 74.28
Round  12, Train loss: 1.717, Test loss: 1.719, Test accuracy: 74.49
Round  13, Train loss: 1.702, Test loss: 1.715, Test accuracy: 74.53
Round  14, Train loss: 1.693, Test loss: 1.658, Test accuracy: 81.48
Round  15, Train loss: 1.658, Test loss: 1.647, Test accuracy: 82.08
Round  16, Train loss: 1.649, Test loss: 1.641, Test accuracy: 82.65
Round  17, Train loss: 1.637, Test loss: 1.636, Test accuracy: 82.93
Round  18, Train loss: 1.639, Test loss: 1.634, Test accuracy: 83.08
Round  19, Train loss: 1.615, Test loss: 1.631, Test accuracy: 83.33
Round  20, Train loss: 1.613, Test loss: 1.629, Test accuracy: 83.49
Round  21, Train loss: 1.613, Test loss: 1.628, Test accuracy: 83.62
Round  22, Train loss: 1.609, Test loss: 1.626, Test accuracy: 83.93
Round  23, Train loss: 1.612, Test loss: 1.625, Test accuracy: 83.87
Round  24, Train loss: 1.605, Test loss: 1.625, Test accuracy: 83.79
Round  25, Train loss: 1.596, Test loss: 1.624, Test accuracy: 83.97
Round  26, Train loss: 1.600, Test loss: 1.622, Test accuracy: 84.08
Round  27, Train loss: 1.595, Test loss: 1.622, Test accuracy: 84.03
Round  28, Train loss: 1.604, Test loss: 1.621, Test accuracy: 84.25
Round  29, Train loss: 1.585, Test loss: 1.620, Test accuracy: 84.23
Round  30, Train loss: 1.591, Test loss: 1.620, Test accuracy: 84.26
Round  31, Train loss: 1.595, Test loss: 1.619, Test accuracy: 84.34
Round  32, Train loss: 1.595, Test loss: 1.619, Test accuracy: 84.47
Round  33, Train loss: 1.592, Test loss: 1.617, Test accuracy: 84.53
Round  34, Train loss: 1.601, Test loss: 1.617, Test accuracy: 84.61
Round  35, Train loss: 1.585, Test loss: 1.618, Test accuracy: 84.46
Round  36, Train loss: 1.585, Test loss: 1.617, Test accuracy: 84.59
Round  37, Train loss: 1.590, Test loss: 1.616, Test accuracy: 84.69
Round  38, Train loss: 1.595, Test loss: 1.615, Test accuracy: 84.71
Round  39, Train loss: 1.592, Test loss: 1.615, Test accuracy: 84.64
Round  40, Train loss: 1.581, Test loss: 1.615, Test accuracy: 84.69
Round  41, Train loss: 1.581, Test loss: 1.613, Test accuracy: 84.97
Round  42, Train loss: 1.587, Test loss: 1.613, Test accuracy: 84.88
Round  43, Train loss: 1.590, Test loss: 1.613, Test accuracy: 84.77
Round  44, Train loss: 1.593, Test loss: 1.613, Test accuracy: 84.75
Round  45, Train loss: 1.583, Test loss: 1.613, Test accuracy: 84.87
Round  46, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.06
Round  47, Train loss: 1.583, Test loss: 1.611, Test accuracy: 85.01
Round  48, Train loss: 1.585, Test loss: 1.611, Test accuracy: 85.03
Round  49, Train loss: 1.571, Test loss: 1.611, Test accuracy: 85.08
Round  50, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.05
Round  51, Train loss: 1.590, Test loss: 1.610, Test accuracy: 85.27
Round  52, Train loss: 1.573, Test loss: 1.611, Test accuracy: 85.08
Round  53, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.16
Round  54, Train loss: 1.574, Test loss: 1.610, Test accuracy: 85.12
Round  55, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.30
Round  56, Train loss: 1.587, Test loss: 1.609, Test accuracy: 85.18
Round  57, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.33
Round  58, Train loss: 1.584, Test loss: 1.608, Test accuracy: 85.38
Round  59, Train loss: 1.577, Test loss: 1.609, Test accuracy: 85.24
Round  60, Train loss: 1.586, Test loss: 1.608, Test accuracy: 85.43
Round  61, Train loss: 1.577, Test loss: 1.608, Test accuracy: 85.43
Round  62, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.39
Round  63, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.45
Round  64, Train loss: 1.575, Test loss: 1.607, Test accuracy: 85.44
Round  65, Train loss: 1.582, Test loss: 1.606, Test accuracy: 85.47
Round  66, Train loss: 1.575, Test loss: 1.606, Test accuracy: 85.43
Round  67, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.47
Round  68, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.39
Round  69, Train loss: 1.567, Test loss: 1.606, Test accuracy: 85.45
Round  70, Train loss: 1.575, Test loss: 1.607, Test accuracy: 85.53
Round  71, Train loss: 1.570, Test loss: 1.606, Test accuracy: 85.55
Round  72, Train loss: 1.573, Test loss: 1.605, Test accuracy: 85.54
Round  73, Train loss: 1.568, Test loss: 1.605, Test accuracy: 85.58
Round  74, Train loss: 1.565, Test loss: 1.605, Test accuracy: 85.61
Round  75, Train loss: 1.580, Test loss: 1.605, Test accuracy: 85.59
Round  76, Train loss: 1.569, Test loss: 1.604, Test accuracy: 85.60
Round  77, Train loss: 1.573, Test loss: 1.605, Test accuracy: 85.64
Round  78, Train loss: 1.571, Test loss: 1.605, Test accuracy: 85.65
Round  79, Train loss: 1.563, Test loss: 1.605, Test accuracy: 85.67
Round  80, Train loss: 1.571, Test loss: 1.604, Test accuracy: 85.83
Round  81, Train loss: 1.579, Test loss: 1.603, Test accuracy: 85.75
Round  82, Train loss: 1.577, Test loss: 1.603, Test accuracy: 85.83
Round  83, Train loss: 1.569, Test loss: 1.603, Test accuracy: 85.78
Round  84, Train loss: 1.564, Test loss: 1.602, Test accuracy: 85.69
Round  85, Train loss: 1.554, Test loss: 1.545, Test accuracy: 92.22
Round  86, Train loss: 1.519, Test loss: 1.537, Test accuracy: 93.00
Round  87, Train loss: 1.519, Test loss: 1.532, Test accuracy: 93.51
Round  88, Train loss: 1.502, Test loss: 1.531, Test accuracy: 93.71
Round  89, Train loss: 1.497, Test loss: 1.529, Test accuracy: 93.74
Round  90, Train loss: 1.498, Test loss: 1.527, Test accuracy: 93.78
Round  91, Train loss: 1.493, Test loss: 1.526, Test accuracy: 94.02
Round  92, Train loss: 1.490, Test loss: 1.525, Test accuracy: 94.09
Round  93, Train loss: 1.491, Test loss: 1.524, Test accuracy: 94.03
Round  94, Train loss: 1.487, Test loss: 1.523, Test accuracy: 94.23
Round  95, Train loss: 1.491, Test loss: 1.522, Test accuracy: 94.26
Round  96, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.23
Round  97, Train loss: 1.487, Test loss: 1.521, Test accuracy: 94.32
Round  98, Train loss: 1.484, Test loss: 1.520, Test accuracy: 94.34
Round  99, Train loss: 1.481, Test loss: 1.520, Test accuracy: 94.42
Final Round, Train loss: 1.483, Test loss: 1.519, Test accuracy: 94.47
Average accuracy final 10 rounds: 94.1725
2069.144362926483
[2.9927899837493896, 5.8816680908203125, 8.890999794006348, 11.886844396591187, 14.979207515716553, 18.066613912582397, 21.00369954109192, 23.759321689605713, 26.658794403076172, 29.56064224243164, 32.42268490791321, 35.43816828727722, 38.36240339279175, 41.31727361679077, 44.30153131484985, 47.12334656715393, 49.949159383773804, 52.84457087516785, 55.537757873535156, 58.20240354537964, 60.91028451919556, 63.60237741470337, 66.31991314888, 69.06357622146606, 71.79670572280884, 74.49636387825012, 77.15881776809692, 79.96046328544617, 82.74874377250671, 85.39342784881592, 88.01861619949341, 90.75813794136047, 93.4405460357666, 96.08859753608704, 98.82972478866577, 101.47641253471375, 104.14930725097656, 106.87806558609009, 109.57522463798523, 112.23128390312195, 114.99866223335266, 117.74171805381775, 120.45593929290771, 123.22562217712402, 126.0602331161499, 128.8398745059967, 131.5860140323639, 134.33413577079773, 137.05174136161804, 139.7586898803711, 142.50806140899658, 145.25220561027527, 147.9472005367279, 150.70571303367615, 153.42732739448547, 156.15795993804932, 158.83659839630127, 161.617534160614, 164.33725953102112, 167.01012563705444, 169.84504175186157, 172.6495943069458, 175.44039011001587, 178.28120803833008, 181.01665091514587, 183.7176558971405, 186.40964460372925, 189.25217533111572, 191.98696064949036, 194.70819234848022, 197.45889616012573, 200.16790509223938, 202.9610493183136, 205.74654078483582, 208.54315209388733, 211.24657011032104, 213.9597508907318, 216.7381591796875, 219.42955255508423, 222.2246551513672, 224.95598030090332, 227.69688391685486, 230.3969111442566, 233.05483865737915, 235.75652956962585, 238.41553044319153, 241.41834998130798, 244.2991805076599, 247.10794138908386, 249.96021223068237, 252.83194732666016, 255.6760811805725, 258.53437757492065, 261.4297516345978, 264.33861541748047, 267.2691822052002, 270.231502532959, 273.1639609336853, 276.0233166217804, 278.9738256931305, 281.49768567085266]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[18.691666666666666, 21.941666666666666, 33.19166666666667, 45.93333333333333, 56.425, 66.6, 71.66666666666667, 72.80833333333334, 73.56666666666666, 73.76666666666667, 74.13333333333334, 74.275, 74.49166666666666, 74.525, 81.48333333333333, 82.075, 82.65, 82.93333333333334, 83.08333333333333, 83.33333333333333, 83.49166666666666, 83.625, 83.93333333333334, 83.86666666666666, 83.79166666666667, 83.975, 84.075, 84.025, 84.25, 84.23333333333333, 84.25833333333334, 84.34166666666667, 84.475, 84.53333333333333, 84.60833333333333, 84.45833333333333, 84.59166666666667, 84.69166666666666, 84.70833333333333, 84.64166666666667, 84.69166666666666, 84.975, 84.88333333333334, 84.76666666666667, 84.75, 84.86666666666666, 85.05833333333334, 85.00833333333334, 85.025, 85.075, 85.05, 85.26666666666667, 85.08333333333333, 85.15833333333333, 85.125, 85.3, 85.18333333333334, 85.325, 85.375, 85.24166666666666, 85.43333333333334, 85.43333333333334, 85.39166666666667, 85.45, 85.44166666666666, 85.46666666666667, 85.43333333333334, 85.46666666666667, 85.39166666666667, 85.45, 85.525, 85.55, 85.54166666666667, 85.575, 85.60833333333333, 85.59166666666667, 85.6, 85.64166666666667, 85.65, 85.66666666666667, 85.825, 85.75, 85.825, 85.78333333333333, 85.69166666666666, 92.21666666666667, 93.0, 93.50833333333334, 93.70833333333333, 93.74166666666666, 93.78333333333333, 94.01666666666667, 94.09166666666667, 94.03333333333333, 94.23333333333333, 94.25833333333334, 94.23333333333333, 94.31666666666666, 94.34166666666667, 94.41666666666667, 94.475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.300, Test accuracy: 22.34
Round   1, Train loss: 2.299, Test loss: 2.297, Test accuracy: 27.68
Round   2, Train loss: 2.295, Test loss: 2.292, Test accuracy: 30.08
Round   3, Train loss: 2.289, Test loss: 2.281, Test accuracy: 31.11
Round   4, Train loss: 2.275, Test loss: 2.252, Test accuracy: 31.01
Round   5, Train loss: 2.222, Test loss: 2.192, Test accuracy: 35.16
Round   6, Train loss: 2.166, Test loss: 2.129, Test accuracy: 41.21
Round   7, Train loss: 2.108, Test loss: 2.058, Test accuracy: 53.32
Round   8, Train loss: 2.008, Test loss: 1.987, Test accuracy: 56.12
Round   9, Train loss: 1.952, Test loss: 1.942, Test accuracy: 57.00
Round  10, Train loss: 1.915, Test loss: 1.917, Test accuracy: 58.02
Round  11, Train loss: 1.897, Test loss: 1.883, Test accuracy: 61.45
Round  12, Train loss: 1.860, Test loss: 1.852, Test accuracy: 64.92
Round  13, Train loss: 1.836, Test loss: 1.822, Test accuracy: 68.61
Round  14, Train loss: 1.798, Test loss: 1.800, Test accuracy: 70.56
Round  15, Train loss: 1.786, Test loss: 1.774, Test accuracy: 72.78
Round  16, Train loss: 1.756, Test loss: 1.763, Test accuracy: 73.56
Round  17, Train loss: 1.751, Test loss: 1.754, Test accuracy: 74.28
Round  18, Train loss: 1.744, Test loss: 1.738, Test accuracy: 75.88
Round  19, Train loss: 1.727, Test loss: 1.723, Test accuracy: 77.72
Round  20, Train loss: 1.733, Test loss: 1.704, Test accuracy: 79.67
Round  21, Train loss: 1.703, Test loss: 1.695, Test accuracy: 80.35
Round  22, Train loss: 1.680, Test loss: 1.686, Test accuracy: 81.09
Round  23, Train loss: 1.685, Test loss: 1.675, Test accuracy: 82.15
Round  24, Train loss: 1.663, Test loss: 1.671, Test accuracy: 82.42
Round  25, Train loss: 1.672, Test loss: 1.666, Test accuracy: 82.70
Round  26, Train loss: 1.660, Test loss: 1.662, Test accuracy: 82.89
Round  27, Train loss: 1.657, Test loss: 1.658, Test accuracy: 83.19
Round  28, Train loss: 1.647, Test loss: 1.653, Test accuracy: 83.21
Round  29, Train loss: 1.631, Test loss: 1.651, Test accuracy: 83.47
Round  30, Train loss: 1.637, Test loss: 1.646, Test accuracy: 83.79
Round  31, Train loss: 1.634, Test loss: 1.640, Test accuracy: 84.27
Round  32, Train loss: 1.610, Test loss: 1.621, Test accuracy: 87.23
Round  33, Train loss: 1.586, Test loss: 1.603, Test accuracy: 89.03
Round  34, Train loss: 1.572, Test loss: 1.590, Test accuracy: 90.55
Round  35, Train loss: 1.571, Test loss: 1.578, Test accuracy: 91.62
Round  36, Train loss: 1.566, Test loss: 1.570, Test accuracy: 92.45
Round  37, Train loss: 1.555, Test loss: 1.564, Test accuracy: 92.95
Round  38, Train loss: 1.552, Test loss: 1.558, Test accuracy: 93.34
Round  39, Train loss: 1.541, Test loss: 1.556, Test accuracy: 93.46
Round  40, Train loss: 1.547, Test loss: 1.553, Test accuracy: 93.67
Round  41, Train loss: 1.535, Test loss: 1.551, Test accuracy: 93.84
Round  42, Train loss: 1.542, Test loss: 1.549, Test accuracy: 93.97
Round  43, Train loss: 1.538, Test loss: 1.547, Test accuracy: 94.06
Round  44, Train loss: 1.537, Test loss: 1.545, Test accuracy: 94.12
Round  45, Train loss: 1.525, Test loss: 1.544, Test accuracy: 94.22
Round  46, Train loss: 1.523, Test loss: 1.543, Test accuracy: 94.27
Round  47, Train loss: 1.534, Test loss: 1.541, Test accuracy: 94.42
Round  48, Train loss: 1.529, Test loss: 1.539, Test accuracy: 94.48
Round  49, Train loss: 1.525, Test loss: 1.538, Test accuracy: 94.60
Round  50, Train loss: 1.525, Test loss: 1.536, Test accuracy: 94.67
Round  51, Train loss: 1.523, Test loss: 1.535, Test accuracy: 94.70
Round  52, Train loss: 1.516, Test loss: 1.535, Test accuracy: 94.76
Round  53, Train loss: 1.513, Test loss: 1.534, Test accuracy: 94.86
Round  54, Train loss: 1.518, Test loss: 1.533, Test accuracy: 94.91
Round  55, Train loss: 1.513, Test loss: 1.532, Test accuracy: 95.01
Round  56, Train loss: 1.505, Test loss: 1.532, Test accuracy: 95.02
Round  57, Train loss: 1.512, Test loss: 1.530, Test accuracy: 95.15
Round  58, Train loss: 1.511, Test loss: 1.529, Test accuracy: 95.19
Round  59, Train loss: 1.512, Test loss: 1.528, Test accuracy: 95.23
Round  60, Train loss: 1.506, Test loss: 1.527, Test accuracy: 95.24
Round  61, Train loss: 1.502, Test loss: 1.527, Test accuracy: 95.39
Round  62, Train loss: 1.504, Test loss: 1.527, Test accuracy: 95.37
Round  63, Train loss: 1.508, Test loss: 1.526, Test accuracy: 95.44
Round  64, Train loss: 1.505, Test loss: 1.525, Test accuracy: 95.46
Round  65, Train loss: 1.497, Test loss: 1.525, Test accuracy: 95.47
Round  66, Train loss: 1.497, Test loss: 1.525, Test accuracy: 95.48
Round  67, Train loss: 1.511, Test loss: 1.524, Test accuracy: 95.49
Round  68, Train loss: 1.504, Test loss: 1.523, Test accuracy: 95.55
Round  69, Train loss: 1.505, Test loss: 1.522, Test accuracy: 95.55
Round  70, Train loss: 1.500, Test loss: 1.522, Test accuracy: 95.63
Round  71, Train loss: 1.501, Test loss: 1.521, Test accuracy: 95.65
Round  72, Train loss: 1.494, Test loss: 1.521, Test accuracy: 95.65
Round  73, Train loss: 1.497, Test loss: 1.521, Test accuracy: 95.72
Round  74, Train loss: 1.492, Test loss: 1.520, Test accuracy: 95.72
Round  75, Train loss: 1.494, Test loss: 1.520, Test accuracy: 95.68
Round  76, Train loss: 1.500, Test loss: 1.520, Test accuracy: 95.73
Round  77, Train loss: 1.490, Test loss: 1.519, Test accuracy: 95.76
Round  78, Train loss: 1.496, Test loss: 1.519, Test accuracy: 95.83
Round  79, Train loss: 1.496, Test loss: 1.518, Test accuracy: 95.91
Round  80, Train loss: 1.490, Test loss: 1.518, Test accuracy: 95.86
Round  81, Train loss: 1.496, Test loss: 1.517, Test accuracy: 95.93
Round  82, Train loss: 1.490, Test loss: 1.517, Test accuracy: 95.89
Round  83, Train loss: 1.492, Test loss: 1.517, Test accuracy: 95.95
Round  84, Train loss: 1.488, Test loss: 1.517, Test accuracy: 95.96
Round  85, Train loss: 1.492, Test loss: 1.517, Test accuracy: 95.92
Round  86, Train loss: 1.485, Test loss: 1.517, Test accuracy: 95.96
Round  87, Train loss: 1.494, Test loss: 1.515, Test accuracy: 95.92
Round  88, Train loss: 1.489, Test loss: 1.515, Test accuracy: 95.99
Round  89, Train loss: 1.490, Test loss: 1.515, Test accuracy: 96.05
Round  90, Train loss: 1.487, Test loss: 1.515, Test accuracy: 96.00
Round  91, Train loss: 1.486, Test loss: 1.514, Test accuracy: 95.99
Round  92, Train loss: 1.488, Test loss: 1.515, Test accuracy: 95.94
Round  93, Train loss: 1.487, Test loss: 1.514, Test accuracy: 96.07
Round  94, Train loss: 1.489, Test loss: 1.514, Test accuracy: 96.08/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.488, Test loss: 1.514, Test accuracy: 96.08
Round  96, Train loss: 1.483, Test loss: 1.514, Test accuracy: 96.12
Round  97, Train loss: 1.482, Test loss: 1.513, Test accuracy: 96.10
Round  98, Train loss: 1.485, Test loss: 1.513, Test accuracy: 96.12
Round  99, Train loss: 1.481, Test loss: 1.513, Test accuracy: 96.17
Final Round, Train loss: 1.479, Test loss: 1.512, Test accuracy: 96.14
Average accuracy final 10 rounds: 96.0675
1327.185010433197
[1.7159051895141602, 3.1605288982391357, 4.722837924957275, 6.281842470169067, 7.838411569595337, 9.37998914718628, 10.948380947113037, 12.452411413192749, 14.007824420928955, 15.596550941467285, 17.21559190750122, 18.771520614624023, 20.265828609466553, 21.81656813621521, 23.400384187698364, 24.95111918449402, 26.440386533737183, 28.00104308128357, 29.564133405685425, 31.075271129608154, 32.60119271278381, 34.19217586517334, 35.777597427368164, 37.26559853553772, 38.81113576889038, 40.3956983089447, 41.94485259056091, 43.48512125015259, 45.024569511413574, 46.55535912513733, 48.13686013221741, 49.710166692733765, 51.26903820037842, 52.793166637420654, 54.31903553009033, 55.90640616416931, 57.49973464012146, 59.0353889465332, 60.566038370132446, 62.18936848640442, 63.746201515197754, 65.2408516407013, 66.79448580741882, 68.35920262336731, 69.89752697944641, 71.42917561531067, 72.96751642227173, 74.51490044593811, 76.01963829994202, 77.50177955627441, 79.05284976959229, 80.60615539550781, 82.10699391365051, 83.59517502784729, 85.10196018218994, 86.6337022781372, 88.14510989189148, 89.64647650718689, 91.15241527557373, 92.65123271942139, 94.1782591342926, 95.72388458251953, 97.25774598121643, 98.79560017585754, 100.31614708900452, 101.83112597465515, 103.33936595916748, 104.87051939964294, 106.34578919410706, 107.82695841789246, 109.38693594932556, 110.95604991912842, 112.50047779083252, 114.00989007949829, 115.54317235946655, 117.07850909233093, 118.60197114944458, 120.12841296195984, 121.69947743415833, 123.26164054870605, 124.82790541648865, 126.36808395385742, 127.89842963218689, 129.45013809204102, 130.97098112106323, 132.5174117088318, 134.0576343536377, 135.55792808532715, 137.09208512306213, 138.6006965637207, 140.14606070518494, 141.7685353755951, 143.34727811813354, 144.915607213974, 146.46660661697388, 148.04602789878845, 149.65621995925903, 151.19128370285034, 152.75182628631592, 154.30840301513672, 156.3378667831421]
[22.341666666666665, 27.683333333333334, 30.083333333333332, 31.108333333333334, 31.008333333333333, 35.15833333333333, 41.208333333333336, 53.31666666666667, 56.11666666666667, 57.0, 58.016666666666666, 61.45, 64.925, 68.60833333333333, 70.55833333333334, 72.78333333333333, 73.55833333333334, 74.275, 75.88333333333334, 77.71666666666667, 79.66666666666667, 80.35, 81.09166666666667, 82.15, 82.41666666666667, 82.7, 82.89166666666667, 83.19166666666666, 83.20833333333333, 83.46666666666667, 83.79166666666667, 84.26666666666667, 87.23333333333333, 89.025, 90.55, 91.625, 92.45, 92.95, 93.34166666666667, 93.45833333333333, 93.675, 93.84166666666667, 93.975, 94.05833333333334, 94.11666666666666, 94.225, 94.26666666666667, 94.41666666666667, 94.48333333333333, 94.6, 94.675, 94.7, 94.75833333333334, 94.85833333333333, 94.90833333333333, 95.00833333333334, 95.01666666666667, 95.15, 95.19166666666666, 95.23333333333333, 95.24166666666666, 95.39166666666667, 95.36666666666666, 95.44166666666666, 95.45833333333333, 95.475, 95.48333333333333, 95.49166666666666, 95.55, 95.55, 95.63333333333334, 95.65, 95.65, 95.71666666666667, 95.725, 95.68333333333334, 95.73333333333333, 95.75833333333334, 95.83333333333333, 95.90833333333333, 95.85833333333333, 95.93333333333334, 95.89166666666667, 95.95, 95.95833333333333, 95.925, 95.95833333333333, 95.925, 95.99166666666666, 96.05, 96.0, 95.99166666666666, 95.94166666666666, 96.06666666666666, 96.08333333333333, 96.075, 96.125, 96.1, 96.11666666666666, 96.175, 96.14166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.319, Test loss: 2.299, Test accuracy: 23.36
Round   1, Train loss: 2.310, Test loss: 2.293, Test accuracy: 21.61
Round   2, Train loss: 2.299, Test loss: 2.282, Test accuracy: 22.11
Round   3, Train loss: 2.281, Test loss: 2.253, Test accuracy: 32.76
Round   4, Train loss: 2.220, Test loss: 2.159, Test accuracy: 40.36
Round   5, Train loss: 2.141, Test loss: 2.096, Test accuracy: 52.17
Round   6, Train loss: 2.073, Test loss: 2.022, Test accuracy: 56.08
Round   7, Train loss: 2.004, Test loss: 1.972, Test accuracy: 57.25
Round   8, Train loss: 1.957, Test loss: 1.943, Test accuracy: 58.43
Round   9, Train loss: 1.951, Test loss: 1.916, Test accuracy: 59.89
Round  10, Train loss: 1.923, Test loss: 1.892, Test accuracy: 63.64
Round  11, Train loss: 1.878, Test loss: 1.866, Test accuracy: 67.99
Round  12, Train loss: 1.866, Test loss: 1.834, Test accuracy: 70.67
Round  13, Train loss: 1.835, Test loss: 1.811, Test accuracy: 73.18
Round  14, Train loss: 1.824, Test loss: 1.785, Test accuracy: 75.31
Round  15, Train loss: 1.800, Test loss: 1.763, Test accuracy: 78.51
Round  16, Train loss: 1.800, Test loss: 1.727, Test accuracy: 83.77
Round  17, Train loss: 1.749, Test loss: 1.706, Test accuracy: 86.10
Round  18, Train loss: 1.727, Test loss: 1.689, Test accuracy: 87.57
Round  19, Train loss: 1.710, Test loss: 1.669, Test accuracy: 88.78
Round  20, Train loss: 1.699, Test loss: 1.658, Test accuracy: 89.47
Round  21, Train loss: 1.686, Test loss: 1.644, Test accuracy: 90.22
Round  22, Train loss: 1.675, Test loss: 1.632, Test accuracy: 90.92
Round  23, Train loss: 1.662, Test loss: 1.624, Test accuracy: 91.26
Round  24, Train loss: 1.653, Test loss: 1.619, Test accuracy: 91.75
Round  25, Train loss: 1.644, Test loss: 1.612, Test accuracy: 92.16
Round  26, Train loss: 1.635, Test loss: 1.609, Test accuracy: 92.32
Round  27, Train loss: 1.627, Test loss: 1.606, Test accuracy: 92.53
Round  28, Train loss: 1.632, Test loss: 1.595, Test accuracy: 92.93
Round  29, Train loss: 1.618, Test loss: 1.593, Test accuracy: 93.08
Round  30, Train loss: 1.612, Test loss: 1.587, Test accuracy: 93.29
Round  31, Train loss: 1.600, Test loss: 1.587, Test accuracy: 93.45
Round  32, Train loss: 1.599, Test loss: 1.584, Test accuracy: 93.64
Round  33, Train loss: 1.609, Test loss: 1.578, Test accuracy: 93.68
Round  34, Train loss: 1.603, Test loss: 1.574, Test accuracy: 93.97
Round  35, Train loss: 1.592, Test loss: 1.573, Test accuracy: 94.06
Round  36, Train loss: 1.589, Test loss: 1.571, Test accuracy: 93.97
Round  37, Train loss: 1.588, Test loss: 1.568, Test accuracy: 94.10
Round  38, Train loss: 1.579, Test loss: 1.567, Test accuracy: 94.30
Round  39, Train loss: 1.580, Test loss: 1.566, Test accuracy: 94.37
Round  40, Train loss: 1.573, Test loss: 1.564, Test accuracy: 94.51
Round  41, Train loss: 1.577, Test loss: 1.561, Test accuracy: 94.72
Round  42, Train loss: 1.580, Test loss: 1.557, Test accuracy: 94.78
Round  43, Train loss: 1.562, Test loss: 1.558, Test accuracy: 94.92
Round  44, Train loss: 1.559, Test loss: 1.557, Test accuracy: 94.98
Round  45, Train loss: 1.570, Test loss: 1.555, Test accuracy: 94.97
Round  46, Train loss: 1.561, Test loss: 1.554, Test accuracy: 95.09
Round  47, Train loss: 1.557, Test loss: 1.553, Test accuracy: 95.18
Round  48, Train loss: 1.552, Test loss: 1.553, Test accuracy: 95.26
Round  49, Train loss: 1.559, Test loss: 1.551, Test accuracy: 95.28
Round  50, Train loss: 1.549, Test loss: 1.551, Test accuracy: 95.37
Round  51, Train loss: 1.562, Test loss: 1.548, Test accuracy: 95.47
Round  52, Train loss: 1.547, Test loss: 1.549, Test accuracy: 95.51
Round  53, Train loss: 1.549, Test loss: 1.547, Test accuracy: 95.61
Round  54, Train loss: 1.545, Test loss: 1.547, Test accuracy: 95.64
Round  55, Train loss: 1.545, Test loss: 1.545, Test accuracy: 95.72
Round  56, Train loss: 1.546, Test loss: 1.544, Test accuracy: 95.75
Round  57, Train loss: 1.536, Test loss: 1.545, Test accuracy: 95.82
Round  58, Train loss: 1.540, Test loss: 1.544, Test accuracy: 95.82
Round  59, Train loss: 1.535, Test loss: 1.543, Test accuracy: 95.83
Round  60, Train loss: 1.537, Test loss: 1.544, Test accuracy: 95.83
Round  61, Train loss: 1.537, Test loss: 1.542, Test accuracy: 95.86
Round  62, Train loss: 1.531, Test loss: 1.543, Test accuracy: 95.81
Round  63, Train loss: 1.530, Test loss: 1.542, Test accuracy: 95.95
Round  64, Train loss: 1.537, Test loss: 1.540, Test accuracy: 95.92
Round  65, Train loss: 1.538, Test loss: 1.538, Test accuracy: 96.00
Round  66, Train loss: 1.539, Test loss: 1.537, Test accuracy: 95.98
Round  67, Train loss: 1.532, Test loss: 1.537, Test accuracy: 96.08
Round  68, Train loss: 1.532, Test loss: 1.537, Test accuracy: 96.12
Round  69, Train loss: 1.526, Test loss: 1.537, Test accuracy: 96.15
Round  70, Train loss: 1.526, Test loss: 1.537, Test accuracy: 96.28
Round  71, Train loss: 1.529, Test loss: 1.535, Test accuracy: 96.23
Round  72, Train loss: 1.529, Test loss: 1.534, Test accuracy: 96.22
Round  73, Train loss: 1.531, Test loss: 1.534, Test accuracy: 96.22
Round  74, Train loss: 1.528, Test loss: 1.533, Test accuracy: 96.28
Round  75, Train loss: 1.527, Test loss: 1.534, Test accuracy: 96.36
Round  76, Train loss: 1.532, Test loss: 1.531, Test accuracy: 96.31
Round  77, Train loss: 1.522, Test loss: 1.532, Test accuracy: 96.34
Round  78, Train loss: 1.521, Test loss: 1.532, Test accuracy: 96.29
Round  79, Train loss: 1.523, Test loss: 1.531, Test accuracy: 96.36
Round  80, Train loss: 1.516, Test loss: 1.532, Test accuracy: 96.44
Round  81, Train loss: 1.526, Test loss: 1.529, Test accuracy: 96.42
Round  82, Train loss: 1.525, Test loss: 1.528, Test accuracy: 96.40
Round  83, Train loss: 1.521, Test loss: 1.529, Test accuracy: 96.44
Round  84, Train loss: 1.517, Test loss: 1.529, Test accuracy: 96.50
Round  85, Train loss: 1.517, Test loss: 1.528, Test accuracy: 96.45
Round  86, Train loss: 1.518, Test loss: 1.528, Test accuracy: 96.49
Round  87, Train loss: 1.515, Test loss: 1.529, Test accuracy: 96.57
Round  88, Train loss: 1.518, Test loss: 1.528, Test accuracy: 96.58
Round  89, Train loss: 1.516, Test loss: 1.528, Test accuracy: 96.51
Round  90, Train loss: 1.512, Test loss: 1.529, Test accuracy: 96.53
Round  91, Train loss: 1.524, Test loss: 1.524, Test accuracy: 96.54
Round  92, Train loss: 1.508, Test loss: 1.528, Test accuracy: 96.59
Round  93, Train loss: 1.513, Test loss: 1.527, Test accuracy: 96.62/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.514, Test loss: 1.526, Test accuracy: 96.62
Round  95, Train loss: 1.516, Test loss: 1.525, Test accuracy: 96.54
Round  96, Train loss: 1.512, Test loss: 1.526, Test accuracy: 96.65
Round  97, Train loss: 1.512, Test loss: 1.525, Test accuracy: 96.61
Round  98, Train loss: 1.514, Test loss: 1.524, Test accuracy: 96.62
Round  99, Train loss: 1.513, Test loss: 1.524, Test accuracy: 96.63
Final Round, Train loss: 1.488, Test loss: 1.521, Test accuracy: 96.65
Average accuracy final 10 rounds: 96.59583333333333
1693.2628610134125
[1.6007492542266846, 3.201498508453369, 4.668906927108765, 6.13631534576416, 7.627040147781372, 9.117764949798584, 10.58167552947998, 12.045586109161377, 13.51627516746521, 14.986964225769043, 16.425663471221924, 17.864362716674805, 19.340399980545044, 20.816437244415283, 22.2784264087677, 23.740415573120117, 25.28264093399048, 26.82486629486084, 28.330231428146362, 29.835596561431885, 31.4157874584198, 32.995978355407715, 34.49530506134033, 35.99463176727295, 37.439531087875366, 38.88443040847778, 40.37499284744263, 41.86555528640747, 43.49058651924133, 45.115617752075195, 46.71575951576233, 48.31590127944946, 49.7827730178833, 51.24964475631714, 52.75566244125366, 54.261680126190186, 55.72817850112915, 57.194676876068115, 58.68742847442627, 60.180180072784424, 61.63012886047363, 63.08007764816284, 64.57895994186401, 66.07784223556519, 67.5604248046875, 69.04300737380981, 70.49345922470093, 71.94391107559204, 73.40006041526794, 74.85620975494385, 76.27078533172607, 77.6853609085083, 79.1080870628357, 80.53081321716309, 81.97188115119934, 83.4129490852356, 84.79431056976318, 86.17567205429077, 87.59480714797974, 89.0139422416687, 90.4741849899292, 91.9344277381897, 93.40758204460144, 94.88073635101318, 96.3544909954071, 97.82824563980103, 99.26066064834595, 100.69307565689087, 102.1187527179718, 103.54442977905273, 105.07919955253601, 106.61396932601929, 108.1098062992096, 109.6056432723999, 111.10025906562805, 112.5948748588562, 114.0429949760437, 115.4911150932312, 117.04266548156738, 118.59421586990356, 120.0807580947876, 121.56730031967163, 123.00121402740479, 124.43512773513794, 125.9273145198822, 127.41950130462646, 128.96745800971985, 130.51541471481323, 132.0442657470703, 133.5731167793274, 135.04398798942566, 136.51485919952393, 138.0479211807251, 139.58098316192627, 141.15656089782715, 142.73213863372803, 144.24044370651245, 145.74874877929688, 147.2050976753235, 148.6614465713501, 150.18115258216858, 151.70085859298706, 153.20267844200134, 154.70449829101562, 156.17203950881958, 157.63958072662354, 159.0923569202423, 160.54513311386108, 162.02555012702942, 163.50596714019775, 164.95637583732605, 166.40678453445435, 167.90065550804138, 169.39452648162842, 170.87303972244263, 172.35155296325684, 173.8224458694458, 175.29333877563477, 176.6989827156067, 178.1046266555786, 179.54962801933289, 180.99462938308716, 182.48202872276306, 183.96942806243896, 185.40248489379883, 186.8355417251587, 188.2669780254364, 189.6984143257141, 191.08818554878235, 192.4779567718506, 193.89773225784302, 195.31750774383545, 196.6954026222229, 198.07329750061035, 199.49537444114685, 200.91745138168335, 202.3173053264618, 203.71715927124023, 205.07503581047058, 206.43291234970093, 207.87988591194153, 209.32685947418213, 210.74513483047485, 212.16341018676758, 213.57349562644958, 214.9835810661316, 216.40282607078552, 217.82207107543945, 219.22742128372192, 220.6327714920044, 222.0240683555603, 223.4153652191162, 224.8778760433197, 226.3403868675232, 227.77745938301086, 229.21453189849854, 230.62326860427856, 232.0320053100586, 233.47248554229736, 234.91296577453613, 236.37745022773743, 237.84193468093872, 239.25577330589294, 240.66961193084717, 242.0808289051056, 243.492045879364, 245.01292037963867, 246.53379487991333, 247.9948925971985, 249.45599031448364, 250.95548629760742, 252.4549822807312, 253.8008270263672, 255.14667177200317, 256.5209274291992, 257.89518308639526, 259.22068071365356, 260.54617834091187, 261.86401748657227, 263.18185663223267, 264.50573563575745, 265.8296146392822, 267.1439983844757, 268.4583821296692, 269.7589452266693, 271.05950832366943, 272.4323093891144, 273.8051104545593, 275.15602588653564, 276.50694131851196, 277.7917494773865, 279.076557636261, 280.446044921875, 281.815532207489, 283.1621677875519, 284.50880336761475, 285.8687906265259, 287.228777885437, 288.5408537387848, 289.85292959213257, 291.6215317249298, 293.39013385772705]
[23.358333333333334, 23.358333333333334, 21.608333333333334, 21.608333333333334, 22.108333333333334, 22.108333333333334, 32.75833333333333, 32.75833333333333, 40.358333333333334, 40.358333333333334, 52.166666666666664, 52.166666666666664, 56.075, 56.075, 57.25, 57.25, 58.43333333333333, 58.43333333333333, 59.891666666666666, 59.891666666666666, 63.641666666666666, 63.641666666666666, 67.99166666666666, 67.99166666666666, 70.675, 70.675, 73.18333333333334, 73.18333333333334, 75.30833333333334, 75.30833333333334, 78.50833333333334, 78.50833333333334, 83.76666666666667, 83.76666666666667, 86.1, 86.1, 87.56666666666666, 87.56666666666666, 88.78333333333333, 88.78333333333333, 89.475, 89.475, 90.21666666666667, 90.21666666666667, 90.925, 90.925, 91.25833333333334, 91.25833333333334, 91.75, 91.75, 92.15833333333333, 92.15833333333333, 92.31666666666666, 92.31666666666666, 92.53333333333333, 92.53333333333333, 92.93333333333334, 92.93333333333334, 93.075, 93.075, 93.29166666666667, 93.29166666666667, 93.45, 93.45, 93.64166666666667, 93.64166666666667, 93.68333333333334, 93.68333333333334, 93.975, 93.975, 94.05833333333334, 94.05833333333334, 93.96666666666667, 93.96666666666667, 94.1, 94.1, 94.3, 94.3, 94.36666666666666, 94.36666666666666, 94.50833333333334, 94.50833333333334, 94.71666666666667, 94.71666666666667, 94.78333333333333, 94.78333333333333, 94.91666666666667, 94.91666666666667, 94.98333333333333, 94.98333333333333, 94.96666666666667, 94.96666666666667, 95.09166666666667, 95.09166666666667, 95.18333333333334, 95.18333333333334, 95.25833333333334, 95.25833333333334, 95.275, 95.275, 95.36666666666666, 95.36666666666666, 95.475, 95.475, 95.50833333333334, 95.50833333333334, 95.60833333333333, 95.60833333333333, 95.64166666666667, 95.64166666666667, 95.71666666666667, 95.71666666666667, 95.75, 95.75, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.83333333333333, 95.83333333333333, 95.825, 95.825, 95.85833333333333, 95.85833333333333, 95.80833333333334, 95.80833333333334, 95.95, 95.95, 95.925, 95.925, 96.0, 96.0, 95.98333333333333, 95.98333333333333, 96.075, 96.075, 96.125, 96.125, 96.15, 96.15, 96.28333333333333, 96.28333333333333, 96.23333333333333, 96.23333333333333, 96.225, 96.225, 96.21666666666667, 96.21666666666667, 96.275, 96.275, 96.35833333333333, 96.35833333333333, 96.30833333333334, 96.30833333333334, 96.34166666666667, 96.34166666666667, 96.29166666666667, 96.29166666666667, 96.35833333333333, 96.35833333333333, 96.44166666666666, 96.44166666666666, 96.41666666666667, 96.41666666666667, 96.4, 96.4, 96.44166666666666, 96.44166666666666, 96.5, 96.5, 96.45, 96.45, 96.49166666666666, 96.49166666666666, 96.56666666666666, 96.56666666666666, 96.58333333333333, 96.58333333333333, 96.50833333333334, 96.50833333333334, 96.53333333333333, 96.53333333333333, 96.54166666666667, 96.54166666666667, 96.59166666666667, 96.59166666666667, 96.61666666666666, 96.61666666666666, 96.625, 96.625, 96.54166666666667, 96.54166666666667, 96.65, 96.65, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.63333333333334, 96.63333333333334, 96.65, 96.65]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.126, Test loss: 2.151, Test accuracy: 29.76
Round   0, Global train loss: 2.126, Global test loss: 2.302, Global test accuracy: 10.00
Round   1, Train loss: 1.847, Test loss: 1.999, Test accuracy: 49.08
Round   1, Global train loss: 1.847, Global test loss: 2.283, Global test accuracy: 14.70
Round   2, Train loss: 1.696, Test loss: 1.925, Test accuracy: 56.77
Round   2, Global train loss: 1.696, Global test loss: 2.267, Global test accuracy: 16.22
Round   3, Train loss: 1.651, Test loss: 1.879, Test accuracy: 60.56
Round   3, Global train loss: 1.651, Global test loss: 2.280, Global test accuracy: 14.67
Round   4, Train loss: 1.630, Test loss: 1.845, Test accuracy: 61.90
Round   4, Global train loss: 1.630, Global test loss: 2.260, Global test accuracy: 18.23
Round   5, Train loss: 1.624, Test loss: 1.827, Test accuracy: 63.01
Round   5, Global train loss: 1.624, Global test loss: 2.278, Global test accuracy: 15.72
Round   6, Train loss: 1.720, Test loss: 1.762, Test accuracy: 70.00
Round   6, Global train loss: 1.720, Global test loss: 2.299, Global test accuracy: 10.96
Round   7, Train loss: 1.531, Test loss: 1.757, Test accuracy: 70.89
Round   7, Global train loss: 1.531, Global test loss: 2.279, Global test accuracy: 17.37
Round   8, Train loss: 1.681, Test loss: 1.722, Test accuracy: 75.15
Round   8, Global train loss: 1.681, Global test loss: 2.264, Global test accuracy: 16.76
Round   9, Train loss: 1.699, Test loss: 1.690, Test accuracy: 77.21
Round   9, Global train loss: 1.699, Global test loss: 2.271, Global test accuracy: 14.02
Round  10, Train loss: 1.728, Test loss: 1.662, Test accuracy: 80.35
Round  10, Global train loss: 1.728, Global test loss: 2.250, Global test accuracy: 18.71
Round  11, Train loss: 1.696, Test loss: 1.660, Test accuracy: 80.42
Round  11, Global train loss: 1.696, Global test loss: 2.262, Global test accuracy: 17.42
Round  12, Train loss: 1.745, Test loss: 1.620, Test accuracy: 84.80
Round  12, Global train loss: 1.745, Global test loss: 2.256, Global test accuracy: 19.93
Round  13, Train loss: 1.586, Test loss: 1.618, Test accuracy: 84.91
Round  13, Global train loss: 1.586, Global test loss: 2.276, Global test accuracy: 16.03
Round  14, Train loss: 1.638, Test loss: 1.617, Test accuracy: 84.92
Round  14, Global train loss: 1.638, Global test loss: 2.269, Global test accuracy: 17.98
Round  15, Train loss: 1.678, Test loss: 1.605, Test accuracy: 86.27
Round  15, Global train loss: 1.678, Global test loss: 2.267, Global test accuracy: 18.33
Round  16, Train loss: 1.527, Test loss: 1.604, Test accuracy: 86.34
Round  16, Global train loss: 1.527, Global test loss: 2.253, Global test accuracy: 18.27
Round  17, Train loss: 1.687, Test loss: 1.604, Test accuracy: 86.37
Round  17, Global train loss: 1.687, Global test loss: 2.262, Global test accuracy: 18.24
Round  18, Train loss: 1.477, Test loss: 1.601, Test accuracy: 86.47
Round  18, Global train loss: 1.477, Global test loss: 2.272, Global test accuracy: 15.85
Round  19, Train loss: 1.632, Test loss: 1.601, Test accuracy: 86.49
Round  19, Global train loss: 1.632, Global test loss: 2.263, Global test accuracy: 18.25
Round  20, Train loss: 1.692, Test loss: 1.599, Test accuracy: 86.62
Round  20, Global train loss: 1.692, Global test loss: 2.254, Global test accuracy: 18.68
Round  21, Train loss: 1.576, Test loss: 1.599, Test accuracy: 86.64
Round  21, Global train loss: 1.576, Global test loss: 2.342, Global test accuracy: 10.68
Round  22, Train loss: 1.525, Test loss: 1.599, Test accuracy: 86.65
Round  22, Global train loss: 1.525, Global test loss: 2.257, Global test accuracy: 18.33
Round  23, Train loss: 1.686, Test loss: 1.599, Test accuracy: 86.64
Round  23, Global train loss: 1.686, Global test loss: 2.266, Global test accuracy: 17.77
Round  24, Train loss: 1.576, Test loss: 1.598, Test accuracy: 86.66
Round  24, Global train loss: 1.576, Global test loss: 2.292, Global test accuracy: 13.32
Round  25, Train loss: 1.522, Test loss: 1.598, Test accuracy: 86.65
Round  25, Global train loss: 1.522, Global test loss: 2.300, Global test accuracy: 12.94
Round  26, Train loss: 1.582, Test loss: 1.598, Test accuracy: 86.65
Round  26, Global train loss: 1.582, Global test loss: 2.252, Global test accuracy: 19.36
Round  27, Train loss: 1.630, Test loss: 1.598, Test accuracy: 86.63
Round  27, Global train loss: 1.630, Global test loss: 2.299, Global test accuracy: 13.66
Round  28, Train loss: 1.521, Test loss: 1.598, Test accuracy: 86.63
Round  28, Global train loss: 1.521, Global test loss: 2.296, Global test accuracy: 14.43
Round  29, Train loss: 1.579, Test loss: 1.598, Test accuracy: 86.62
Round  29, Global train loss: 1.579, Global test loss: 2.247, Global test accuracy: 20.47
Round  30, Train loss: 1.580, Test loss: 1.598, Test accuracy: 86.62
Round  30, Global train loss: 1.580, Global test loss: 2.261, Global test accuracy: 18.46
Round  31, Train loss: 1.522, Test loss: 1.598, Test accuracy: 86.59
Round  31, Global train loss: 1.522, Global test loss: 2.272, Global test accuracy: 16.93
Round  32, Train loss: 1.523, Test loss: 1.597, Test accuracy: 86.62
Round  32, Global train loss: 1.523, Global test loss: 2.262, Global test accuracy: 19.32
Round  33, Train loss: 1.632, Test loss: 1.594, Test accuracy: 86.92
Round  33, Global train loss: 1.632, Global test loss: 2.267, Global test accuracy: 15.46
Round  34, Train loss: 1.579, Test loss: 1.594, Test accuracy: 86.92
Round  34, Global train loss: 1.579, Global test loss: 2.252, Global test accuracy: 18.93
Round  35, Train loss: 1.568, Test loss: 1.584, Test accuracy: 87.97
Round  35, Global train loss: 1.568, Global test loss: 2.265, Global test accuracy: 16.12
Round  36, Train loss: 1.578, Test loss: 1.584, Test accuracy: 87.95
Round  36, Global train loss: 1.578, Global test loss: 2.278, Global test accuracy: 15.25
Round  37, Train loss: 1.523, Test loss: 1.584, Test accuracy: 87.96
Round  37, Global train loss: 1.523, Global test loss: 2.265, Global test accuracy: 17.39
Round  38, Train loss: 1.530, Test loss: 1.581, Test accuracy: 88.20
Round  38, Global train loss: 1.530, Global test loss: 2.269, Global test accuracy: 16.70
Round  39, Train loss: 1.550, Test loss: 1.568, Test accuracy: 89.68
Round  39, Global train loss: 1.550, Global test loss: 2.272, Global test accuracy: 18.33
Round  40, Train loss: 1.590, Test loss: 1.554, Test accuracy: 90.98
Round  40, Global train loss: 1.590, Global test loss: 2.273, Global test accuracy: 16.75
Round  41, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.97
Round  41, Global train loss: 1.520, Global test loss: 2.306, Global test accuracy: 12.16
Round  42, Train loss: 1.526, Test loss: 1.554, Test accuracy: 90.97
Round  42, Global train loss: 1.526, Global test loss: 2.297, Global test accuracy: 14.34
Round  43, Train loss: 1.527, Test loss: 1.553, Test accuracy: 90.97
Round  43, Global train loss: 1.527, Global test loss: 2.280, Global test accuracy: 14.66
Round  44, Train loss: 1.468, Test loss: 1.553, Test accuracy: 90.97
Round  44, Global train loss: 1.468, Global test loss: 2.253, Global test accuracy: 19.60
Round  45, Train loss: 1.524, Test loss: 1.553, Test accuracy: 90.99
Round  45, Global train loss: 1.524, Global test loss: 2.270, Global test accuracy: 17.71
Round  46, Train loss: 1.575, Test loss: 1.553, Test accuracy: 91.00
Round  46, Global train loss: 1.575, Global test loss: 2.299, Global test accuracy: 12.93
Round  47, Train loss: 1.524, Test loss: 1.553, Test accuracy: 91.00
Round  47, Global train loss: 1.524, Global test loss: 2.273, Global test accuracy: 18.33
Round  48, Train loss: 1.466, Test loss: 1.553, Test accuracy: 91.01
Round  48, Global train loss: 1.466, Global test loss: 2.267, Global test accuracy: 15.80
Round  49, Train loss: 1.469, Test loss: 1.552, Test accuracy: 91.06
Round  49, Global train loss: 1.469, Global test loss: 2.294, Global test accuracy: 12.97
Round  50, Train loss: 1.469, Test loss: 1.552, Test accuracy: 91.07
Round  50, Global train loss: 1.469, Global test loss: 2.274, Global test accuracy: 15.83
Round  51, Train loss: 1.468, Test loss: 1.552, Test accuracy: 91.08
Round  51, Global train loss: 1.468, Global test loss: 2.274, Global test accuracy: 16.68
Round  52, Train loss: 1.522, Test loss: 1.552, Test accuracy: 91.08
Round  52, Global train loss: 1.522, Global test loss: 2.306, Global test accuracy: 12.05
Round  53, Train loss: 1.575, Test loss: 1.552, Test accuracy: 91.10
Round  53, Global train loss: 1.575, Global test loss: 2.261, Global test accuracy: 18.22
Round  54, Train loss: 1.575, Test loss: 1.552, Test accuracy: 91.12
Round  54, Global train loss: 1.575, Global test loss: 2.245, Global test accuracy: 19.62
Round  55, Train loss: 1.468, Test loss: 1.552, Test accuracy: 91.12
Round  55, Global train loss: 1.468, Global test loss: 2.306, Global test accuracy: 13.11
Round  56, Train loss: 1.523, Test loss: 1.552, Test accuracy: 91.08
Round  56, Global train loss: 1.523, Global test loss: 2.249, Global test accuracy: 20.26
Round  57, Train loss: 1.522, Test loss: 1.552, Test accuracy: 91.08
Round  57, Global train loss: 1.522, Global test loss: 2.291, Global test accuracy: 15.69
Round  58, Train loss: 1.470, Test loss: 1.552, Test accuracy: 91.07
Round  58, Global train loss: 1.470, Global test loss: 2.249, Global test accuracy: 19.38
Round  59, Train loss: 1.560, Test loss: 1.537, Test accuracy: 92.62
Round  59, Global train loss: 1.560, Global test loss: 2.250, Global test accuracy: 18.77
Round  60, Train loss: 1.523, Test loss: 1.537, Test accuracy: 92.63
Round  60, Global train loss: 1.523, Global test loss: 2.256, Global test accuracy: 19.07
Round  61, Train loss: 1.469, Test loss: 1.536, Test accuracy: 92.65
Round  61, Global train loss: 1.469, Global test loss: 2.244, Global test accuracy: 21.09
Round  62, Train loss: 1.520, Test loss: 1.536, Test accuracy: 92.65
Round  62, Global train loss: 1.520, Global test loss: 2.293, Global test accuracy: 12.70
Round  63, Train loss: 1.471, Test loss: 1.536, Test accuracy: 92.62
Round  63, Global train loss: 1.471, Global test loss: 2.284, Global test accuracy: 16.11
Round  64, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.67
Round  64, Global train loss: 1.521, Global test loss: 2.272, Global test accuracy: 16.35
Round  65, Train loss: 1.467, Test loss: 1.536, Test accuracy: 92.67
Round  65, Global train loss: 1.467, Global test loss: 2.267, Global test accuracy: 18.02
Round  66, Train loss: 1.574, Test loss: 1.536, Test accuracy: 92.65
Round  66, Global train loss: 1.574, Global test loss: 2.256, Global test accuracy: 18.19
Round  67, Train loss: 1.466, Test loss: 1.536, Test accuracy: 92.67
Round  67, Global train loss: 1.466, Global test loss: 2.302, Global test accuracy: 13.32
Round  68, Train loss: 1.519, Test loss: 1.536, Test accuracy: 92.67
Round  68, Global train loss: 1.519, Global test loss: 2.256, Global test accuracy: 17.89
Round  69, Train loss: 1.471, Test loss: 1.536, Test accuracy: 92.62
Round  69, Global train loss: 1.471, Global test loss: 2.274, Global test accuracy: 15.32
Round  70, Train loss: 1.519, Test loss: 1.536, Test accuracy: 92.62
Round  70, Global train loss: 1.519, Global test loss: 2.259, Global test accuracy: 18.02
Round  71, Train loss: 1.519, Test loss: 1.536, Test accuracy: 92.62
Round  71, Global train loss: 1.519, Global test loss: 2.288, Global test accuracy: 11.99
Round  72, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.62
Round  72, Global train loss: 1.521, Global test loss: 2.256, Global test accuracy: 18.80
Round  73, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.62
Round  73, Global train loss: 1.468, Global test loss: 2.285, Global test accuracy: 15.28
Round  74, Train loss: 1.467, Test loss: 1.536, Test accuracy: 92.65
Round  74, Global train loss: 1.467, Global test loss: 2.262, Global test accuracy: 17.24
Round  75, Train loss: 1.520, Test loss: 1.536, Test accuracy: 92.63
Round  75, Global train loss: 1.520, Global test loss: 2.276, Global test accuracy: 16.94
Round  76, Train loss: 1.463, Test loss: 1.536, Test accuracy: 92.65
Round  76, Global train loss: 1.463, Global test loss: 2.271, Global test accuracy: 15.87
Round  77, Train loss: 1.465, Test loss: 1.536, Test accuracy: 92.64
Round  77, Global train loss: 1.465, Global test loss: 2.263, Global test accuracy: 16.65
Round  78, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.63
Round  78, Global train loss: 1.521, Global test loss: 2.266, Global test accuracy: 18.27
Round  79, Train loss: 1.520, Test loss: 1.536, Test accuracy: 92.65
Round  79, Global train loss: 1.520, Global test loss: 2.272, Global test accuracy: 16.33
Round  80, Train loss: 1.469, Test loss: 1.536, Test accuracy: 92.65
Round  80, Global train loss: 1.469, Global test loss: 2.291, Global test accuracy: 14.07
Round  81, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.65
Round  81, Global train loss: 1.468, Global test loss: 2.292, Global test accuracy: 12.82
Round  82, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.66
Round  82, Global train loss: 1.468, Global test loss: 2.299, Global test accuracy: 13.32
Round  83, Train loss: 1.471, Test loss: 1.536, Test accuracy: 92.67
Round  83, Global train loss: 1.471, Global test loss: 2.290, Global test accuracy: 12.68
Round  84, Train loss: 1.574, Test loss: 1.536, Test accuracy: 92.65
Round  84, Global train loss: 1.574, Global test loss: 2.260, Global test accuracy: 18.32
Round  85, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.66
Round  85, Global train loss: 1.468, Global test loss: 2.271, Global test accuracy: 16.30
Round  86, Train loss: 1.524, Test loss: 1.536, Test accuracy: 92.67
Round  86, Global train loss: 1.524, Global test loss: 2.272, Global test accuracy: 18.33
Round  87, Train loss: 1.574, Test loss: 1.536, Test accuracy: 92.66
Round  87, Global train loss: 1.574, Global test loss: 2.257, Global test accuracy: 18.48
Round  88, Train loss: 1.522, Test loss: 1.536, Test accuracy: 92.67
Round  88, Global train loss: 1.522, Global test loss: 2.270, Global test accuracy: 18.27
Round  89, Train loss: 1.466, Test loss: 1.536, Test accuracy: 92.67
Round  89, Global train loss: 1.466, Global test loss: 2.257, Global test accuracy: 18.12
Round  90, Train loss: 1.470, Test loss: 1.536, Test accuracy: 92.67
Round  90, Global train loss: 1.470, Global test loss: 2.277, Global test accuracy: 14.73
Round  91, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.66
Round  91, Global train loss: 1.521, Global test loss: 2.259, Global test accuracy: 18.12
Round  92, Train loss: 1.470, Test loss: 1.535, Test accuracy: 92.66
Round  92, Global train loss: 1.470, Global test loss: 2.261, Global test accuracy: 17.77
Round  93, Train loss: 1.573, Test loss: 1.535, Test accuracy: 92.67
Round  93, Global train loss: 1.573, Global test loss: 2.255, Global test accuracy: 18.41
Round  94, Train loss: 1.467, Test loss: 1.535, Test accuracy: 92.66
Round  94, Global train loss: 1.467, Global test loss: 2.272, Global test accuracy: 15.89
Round  95, Train loss: 1.518, Test loss: 1.535, Test accuracy: 92.66
Round  95, Global train loss: 1.518, Global test loss: 2.270, Global test accuracy: 16.51/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.471, Test loss: 1.535, Test accuracy: 92.67
Round  96, Global train loss: 1.471, Global test loss: 2.250, Global test accuracy: 19.71
Round  97, Train loss: 1.466, Test loss: 1.535, Test accuracy: 92.69
Round  97, Global train loss: 1.466, Global test loss: 2.276, Global test accuracy: 17.48
Round  98, Train loss: 1.466, Test loss: 1.535, Test accuracy: 92.71
Round  98, Global train loss: 1.466, Global test loss: 2.282, Global test accuracy: 14.94
Round  99, Train loss: 1.521, Test loss: 1.535, Test accuracy: 92.70
Round  99, Global train loss: 1.521, Global test loss: 2.267, Global test accuracy: 18.24
Final Round, Train loss: 1.499, Test loss: 1.535, Test accuracy: 92.67
Final Round, Global train loss: 1.499, Global test loss: 2.267, Global test accuracy: 18.24
Average accuracy final 10 rounds: 92.67416666666665 

Average global accuracy final 10 rounds: 17.181666666666665 

1822.9176616668701
[1.2023556232452393, 2.4047112464904785, 3.5385971069335938, 4.672482967376709, 5.7117815017700195, 6.75108003616333, 7.879024028778076, 9.006968021392822, 10.108105182647705, 11.209242343902588, 12.391313791275024, 13.573385238647461, 14.729222774505615, 15.88506031036377, 16.966779947280884, 18.048499584197998, 19.18014645576477, 20.311793327331543, 21.430498361587524, 22.549203395843506, 23.633347988128662, 24.71749258041382, 25.804816961288452, 26.892141342163086, 28.045157194137573, 29.19817304611206, 30.33489489555359, 31.471616744995117, 32.59690070152283, 33.72218465805054, 34.8810601234436, 36.03993558883667, 37.08724927902222, 38.134562969207764, 39.26082444190979, 40.387085914611816, 41.51132345199585, 42.63556098937988, 43.79666018486023, 44.957759380340576, 46.10708498954773, 47.25641059875488, 48.379685163497925, 49.50295972824097, 50.65385842323303, 51.8047571182251, 52.960922956466675, 54.11708879470825, 55.253185510635376, 56.3892822265625, 57.453086853027344, 58.51689147949219, 59.74452877044678, 60.97216606140137, 62.13459396362305, 63.29702186584473, 64.4407148361206, 65.58440780639648, 66.77611970901489, 67.9678316116333, 69.10297966003418, 70.23812770843506, 71.37557888031006, 72.51303005218506, 73.67464256286621, 74.83625507354736, 76.03168869018555, 77.22712230682373, 78.343679189682, 79.46023607254028, 80.63022994995117, 81.80022382736206, 82.93287658691406, 84.06552934646606, 85.25621104240417, 86.44689273834229, 87.61020350456238, 88.77351427078247, 89.93762159347534, 91.10172891616821, 92.33379578590393, 93.56586265563965, 94.72280979156494, 95.87975692749023, 97.00329756736755, 98.12683820724487, 99.28851509094238, 100.45019197463989, 101.6339111328125, 102.81763029098511, 103.93028163909912, 105.04293298721313, 106.2461290359497, 107.44932508468628, 108.64392828941345, 109.83853149414062, 110.93943500518799, 112.04033851623535, 113.17028498649597, 114.30023145675659, 115.4282066822052, 116.55618190765381, 117.75453996658325, 118.9528980255127, 120.11414456367493, 121.27539110183716, 122.42499017715454, 123.57458925247192, 124.781907081604, 125.98922491073608, 127.14071559906006, 128.29220628738403, 129.41173672676086, 130.5312671661377, 131.65902590751648, 132.78678464889526, 133.9834394454956, 135.18009424209595, 136.30688285827637, 137.4336714744568, 138.63273239135742, 139.83179330825806, 141.0597424507141, 142.28769159317017, 143.42555046081543, 144.5634093284607, 145.72198295593262, 146.88055658340454, 148.0146906375885, 149.14882469177246, 150.31812644004822, 151.48742818832397, 152.6364300251007, 153.78543186187744, 154.96235990524292, 156.1392879486084, 157.3272898197174, 158.51529169082642, 159.68573808670044, 160.85618448257446, 161.98214554786682, 163.10810661315918, 164.2264587879181, 165.344810962677, 166.53973054885864, 167.73465013504028, 168.87084245681763, 170.00703477859497, 171.16222834587097, 172.31742191314697, 173.50186562538147, 174.68630933761597, 175.83835554122925, 176.99040174484253, 178.14206981658936, 179.29373788833618, 180.43966698646545, 181.58559608459473, 182.75863003730774, 183.93166399002075, 185.0176763534546, 186.10368871688843, 187.2703413963318, 188.43699407577515, 189.5921013355255, 190.74720859527588, 191.91555547714233, 193.0839023590088, 194.25844287872314, 195.4329833984375, 196.59115743637085, 197.7493314743042, 198.94427490234375, 200.1392183303833, 201.26100540161133, 202.38279247283936, 203.51870441436768, 204.654616355896, 205.77916383743286, 206.90371131896973, 208.06525707244873, 209.22680282592773, 210.37768054008484, 211.52855825424194, 212.75907850265503, 213.98959875106812, 215.1810405254364, 216.3724822998047, 217.5012447834015, 218.6300072669983, 219.66290426254272, 220.69580125808716, 221.69253993034363, 222.6892786026001, 223.67192482948303, 224.65457105636597, 225.63051080703735, 226.60645055770874, 227.59084010124207, 228.5752296447754, 230.2439935207367, 231.912757396698]
[29.758333333333333, 29.758333333333333, 49.083333333333336, 49.083333333333336, 56.775, 56.775, 60.55833333333333, 60.55833333333333, 61.9, 61.9, 63.00833333333333, 63.00833333333333, 70.0, 70.0, 70.89166666666667, 70.89166666666667, 75.15, 75.15, 77.20833333333333, 77.20833333333333, 80.35, 80.35, 80.41666666666667, 80.41666666666667, 84.8, 84.8, 84.90833333333333, 84.90833333333333, 84.91666666666667, 84.91666666666667, 86.26666666666667, 86.26666666666667, 86.34166666666667, 86.34166666666667, 86.36666666666666, 86.36666666666666, 86.475, 86.475, 86.49166666666666, 86.49166666666666, 86.61666666666666, 86.61666666666666, 86.64166666666667, 86.64166666666667, 86.65, 86.65, 86.64166666666667, 86.64166666666667, 86.65833333333333, 86.65833333333333, 86.65, 86.65, 86.65, 86.65, 86.63333333333334, 86.63333333333334, 86.63333333333334, 86.63333333333334, 86.625, 86.625, 86.61666666666666, 86.61666666666666, 86.59166666666667, 86.59166666666667, 86.61666666666666, 86.61666666666666, 86.925, 86.925, 86.925, 86.925, 87.96666666666667, 87.96666666666667, 87.95, 87.95, 87.95833333333333, 87.95833333333333, 88.2, 88.2, 89.68333333333334, 89.68333333333334, 90.98333333333333, 90.98333333333333, 90.975, 90.975, 90.975, 90.975, 90.975, 90.975, 90.96666666666667, 90.96666666666667, 90.99166666666666, 90.99166666666666, 91.0, 91.0, 91.0, 91.0, 91.00833333333334, 91.00833333333334, 91.05833333333334, 91.05833333333334, 91.06666666666666, 91.06666666666666, 91.075, 91.075, 91.08333333333333, 91.08333333333333, 91.1, 91.1, 91.11666666666666, 91.11666666666666, 91.11666666666666, 91.11666666666666, 91.08333333333333, 91.08333333333333, 91.075, 91.075, 91.06666666666666, 91.06666666666666, 92.625, 92.625, 92.63333333333334, 92.63333333333334, 92.65, 92.65, 92.65, 92.65, 92.625, 92.625, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.65, 92.65, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.61666666666666, 92.61666666666666, 92.625, 92.625, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.625, 92.625, 92.65, 92.65, 92.63333333333334, 92.63333333333334, 92.65, 92.65, 92.64166666666667, 92.64166666666667, 92.63333333333334, 92.63333333333334, 92.65, 92.65, 92.65, 92.65, 92.65, 92.65, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.65, 92.65, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.675, 92.675, 92.66666666666667, 92.66666666666667, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.675, 92.675, 92.69166666666666, 92.69166666666666, 92.70833333333333, 92.70833333333333, 92.7, 92.7, 92.675, 92.675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.85
Round   0, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 10.16
Round   1, Train loss: 2.300, Test loss: 2.300, Test accuracy: 19.43
Round   1, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.66
Round   2, Train loss: 2.299, Test loss: 2.299, Test accuracy: 22.77
Round   2, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 12.34
Round   3, Train loss: 2.298, Test loss: 2.297, Test accuracy: 26.84
Round   3, Global train loss: 2.298, Global test loss: 2.301, Global test accuracy: 12.57
Round   4, Train loss: 2.295, Test loss: 2.295, Test accuracy: 28.52
Round   4, Global train loss: 2.295, Global test loss: 2.300, Global test accuracy: 14.28
Round   5, Train loss: 2.292, Test loss: 2.289, Test accuracy: 29.78
Round   5, Global train loss: 2.292, Global test loss: 2.299, Global test accuracy: 13.59
Round   6, Train loss: 2.282, Test loss: 2.278, Test accuracy: 27.83
Round   6, Global train loss: 2.282, Global test loss: 2.296, Global test accuracy: 13.60
Round   7, Train loss: 2.246, Test loss: 2.250, Test accuracy: 30.41
Round   7, Global train loss: 2.246, Global test loss: 2.287, Global test accuracy: 14.40
Round   8, Train loss: 2.176, Test loss: 2.197, Test accuracy: 34.02
Round   8, Global train loss: 2.176, Global test loss: 2.278, Global test accuracy: 15.25
Round   9, Train loss: 2.138, Test loss: 2.159, Test accuracy: 37.22
Round   9, Global train loss: 2.138, Global test loss: 2.274, Global test accuracy: 16.35
Round  10, Train loss: 2.118, Test loss: 2.110, Test accuracy: 40.42
Round  10, Global train loss: 2.118, Global test loss: 2.266, Global test accuracy: 17.02
Round  11, Train loss: 2.084, Test loss: 2.092, Test accuracy: 42.42
Round  11, Global train loss: 2.084, Global test loss: 2.275, Global test accuracy: 15.39
Round  12, Train loss: 2.063, Test loss: 2.077, Test accuracy: 43.14
Round  12, Global train loss: 2.063, Global test loss: 2.282, Global test accuracy: 14.99
Round  13, Train loss: 2.064, Test loss: 2.063, Test accuracy: 43.35
Round  13, Global train loss: 2.064, Global test loss: 2.268, Global test accuracy: 17.56
Round  14, Train loss: 2.035, Test loss: 2.036, Test accuracy: 44.50
Round  14, Global train loss: 2.035, Global test loss: 2.274, Global test accuracy: 15.93
Round  15, Train loss: 2.019, Test loss: 2.007, Test accuracy: 46.36
Round  15, Global train loss: 2.019, Global test loss: 2.260, Global test accuracy: 18.32
Round  16, Train loss: 2.037, Test loss: 2.001, Test accuracy: 46.60
Round  16, Global train loss: 2.037, Global test loss: 2.271, Global test accuracy: 16.67
Round  17, Train loss: 2.065, Test loss: 2.004, Test accuracy: 46.15
Round  17, Global train loss: 2.065, Global test loss: 2.269, Global test accuracy: 16.92
Round  18, Train loss: 2.042, Test loss: 2.001, Test accuracy: 46.52
Round  18, Global train loss: 2.042, Global test loss: 2.269, Global test accuracy: 17.32
Round  19, Train loss: 2.024, Test loss: 1.990, Test accuracy: 47.64
Round  19, Global train loss: 2.024, Global test loss: 2.285, Global test accuracy: 15.12
Round  20, Train loss: 2.030, Test loss: 1.985, Test accuracy: 48.12
Round  20, Global train loss: 2.030, Global test loss: 2.267, Global test accuracy: 17.47
Round  21, Train loss: 2.057, Test loss: 1.991, Test accuracy: 47.39
Round  21, Global train loss: 2.057, Global test loss: 2.274, Global test accuracy: 17.02
Round  22, Train loss: 2.015, Test loss: 1.986, Test accuracy: 47.70
Round  22, Global train loss: 2.015, Global test loss: 2.257, Global test accuracy: 18.96
Round  23, Train loss: 1.971, Test loss: 1.982, Test accuracy: 48.15
Round  23, Global train loss: 1.971, Global test loss: 2.268, Global test accuracy: 17.32
Round  24, Train loss: 2.046, Test loss: 1.996, Test accuracy: 46.65
Round  24, Global train loss: 2.046, Global test loss: 2.262, Global test accuracy: 18.40
Round  25, Train loss: 2.047, Test loss: 1.988, Test accuracy: 47.52
Round  25, Global train loss: 2.047, Global test loss: 2.270, Global test accuracy: 17.16
Round  26, Train loss: 1.984, Test loss: 1.986, Test accuracy: 47.70
Round  26, Global train loss: 1.984, Global test loss: 2.263, Global test accuracy: 18.06
Round  27, Train loss: 1.988, Test loss: 1.974, Test accuracy: 48.87
Round  27, Global train loss: 1.988, Global test loss: 2.267, Global test accuracy: 17.62
Round  28, Train loss: 1.993, Test loss: 1.984, Test accuracy: 47.76
Round  28, Global train loss: 1.993, Global test loss: 2.261, Global test accuracy: 18.20
Round  29, Train loss: 2.029, Test loss: 1.982, Test accuracy: 47.91
Round  29, Global train loss: 2.029, Global test loss: 2.264, Global test accuracy: 17.79
Round  30, Train loss: 1.994, Test loss: 1.976, Test accuracy: 48.51
Round  30, Global train loss: 1.994, Global test loss: 2.277, Global test accuracy: 16.48
Round  31, Train loss: 2.019, Test loss: 1.980, Test accuracy: 48.09
Round  31, Global train loss: 2.019, Global test loss: 2.258, Global test accuracy: 19.20
Round  32, Train loss: 2.018, Test loss: 1.979, Test accuracy: 48.08
Round  32, Global train loss: 2.018, Global test loss: 2.266, Global test accuracy: 17.76
Round  33, Train loss: 1.953, Test loss: 1.979, Test accuracy: 48.22
Round  33, Global train loss: 1.953, Global test loss: 2.262, Global test accuracy: 18.18
Round  34, Train loss: 2.019, Test loss: 1.992, Test accuracy: 46.64
Round  34, Global train loss: 2.019, Global test loss: 2.266, Global test accuracy: 17.71
Round  35, Train loss: 2.020, Test loss: 1.985, Test accuracy: 47.36
Round  35, Global train loss: 2.020, Global test loss: 2.271, Global test accuracy: 17.23
Round  36, Train loss: 2.003, Test loss: 1.983, Test accuracy: 47.49
Round  36, Global train loss: 2.003, Global test loss: 2.262, Global test accuracy: 18.12
Round  37, Train loss: 2.002, Test loss: 1.980, Test accuracy: 47.88
Round  37, Global train loss: 2.002, Global test loss: 2.265, Global test accuracy: 17.68
Round  38, Train loss: 1.882, Test loss: 1.975, Test accuracy: 48.41
Round  38, Global train loss: 1.882, Global test loss: 2.260, Global test accuracy: 18.37
Round  39, Train loss: 1.971, Test loss: 1.976, Test accuracy: 48.38
Round  39, Global train loss: 1.971, Global test loss: 2.266, Global test accuracy: 17.57
Round  40, Train loss: 1.952, Test loss: 1.978, Test accuracy: 48.08
Round  40, Global train loss: 1.952, Global test loss: 2.262, Global test accuracy: 18.33
Round  41, Train loss: 1.972, Test loss: 1.977, Test accuracy: 48.07
Round  41, Global train loss: 1.972, Global test loss: 2.262, Global test accuracy: 18.50
Round  42, Train loss: 2.015, Test loss: 1.969, Test accuracy: 48.96
Round  42, Global train loss: 2.015, Global test loss: 2.283, Global test accuracy: 15.47
Round  43, Train loss: 1.959, Test loss: 1.963, Test accuracy: 49.56
Round  43, Global train loss: 1.959, Global test loss: 2.279, Global test accuracy: 15.90
Round  44, Train loss: 1.951, Test loss: 1.963, Test accuracy: 49.60
Round  44, Global train loss: 1.951, Global test loss: 2.267, Global test accuracy: 17.33
Round  45, Train loss: 1.961, Test loss: 1.972, Test accuracy: 48.49
Round  45, Global train loss: 1.961, Global test loss: 2.263, Global test accuracy: 18.19
Round  46, Train loss: 2.022, Test loss: 1.972, Test accuracy: 48.48
Round  46, Global train loss: 2.022, Global test loss: 2.277, Global test accuracy: 16.34
Round  47, Train loss: 1.943, Test loss: 1.973, Test accuracy: 48.48
Round  47, Global train loss: 1.943, Global test loss: 2.259, Global test accuracy: 18.73
Round  48, Train loss: 1.993, Test loss: 1.971, Test accuracy: 48.64
Round  48, Global train loss: 1.993, Global test loss: 2.257, Global test accuracy: 18.74
Round  49, Train loss: 1.863, Test loss: 1.961, Test accuracy: 49.68
Round  49, Global train loss: 1.863, Global test loss: 2.255, Global test accuracy: 19.23
Round  50, Train loss: 1.929, Test loss: 1.964, Test accuracy: 49.35
Round  50, Global train loss: 1.929, Global test loss: 2.274, Global test accuracy: 16.22
Round  51, Train loss: 1.974, Test loss: 1.959, Test accuracy: 49.90
Round  51, Global train loss: 1.974, Global test loss: 2.273, Global test accuracy: 17.03
Round  52, Train loss: 1.968, Test loss: 1.958, Test accuracy: 49.92
Round  52, Global train loss: 1.968, Global test loss: 2.261, Global test accuracy: 18.31
Round  53, Train loss: 1.981, Test loss: 1.963, Test accuracy: 49.49
Round  53, Global train loss: 1.981, Global test loss: 2.262, Global test accuracy: 18.23
Round  54, Train loss: 1.968, Test loss: 1.964, Test accuracy: 49.40
Round  54, Global train loss: 1.968, Global test loss: 2.276, Global test accuracy: 16.55
Round  55, Train loss: 1.972, Test loss: 1.964, Test accuracy: 49.40
Round  55, Global train loss: 1.972, Global test loss: 2.265, Global test accuracy: 17.52
Round  56, Train loss: 1.943, Test loss: 1.961, Test accuracy: 49.66
Round  56, Global train loss: 1.943, Global test loss: 2.271, Global test accuracy: 17.25
Round  57, Train loss: 1.964, Test loss: 1.956, Test accuracy: 50.18
Round  57, Global train loss: 1.964, Global test loss: 2.268, Global test accuracy: 17.12
Round  58, Train loss: 2.008, Test loss: 1.951, Test accuracy: 50.74
Round  58, Global train loss: 2.008, Global test loss: 2.272, Global test accuracy: 15.95
Round  59, Train loss: 1.966, Test loss: 1.941, Test accuracy: 51.76
Round  59, Global train loss: 1.966, Global test loss: 2.262, Global test accuracy: 18.20
Round  60, Train loss: 1.917, Test loss: 1.938, Test accuracy: 52.09
Round  60, Global train loss: 1.917, Global test loss: 2.277, Global test accuracy: 16.57
Round  61, Train loss: 1.906, Test loss: 1.936, Test accuracy: 52.28
Round  61, Global train loss: 1.906, Global test loss: 2.277, Global test accuracy: 15.99
Round  62, Train loss: 1.955, Test loss: 1.936, Test accuracy: 52.29
Round  62, Global train loss: 1.955, Global test loss: 2.264, Global test accuracy: 17.67
Round  63, Train loss: 1.884, Test loss: 1.932, Test accuracy: 52.62
Round  63, Global train loss: 1.884, Global test loss: 2.270, Global test accuracy: 17.14
Round  64, Train loss: 1.910, Test loss: 1.925, Test accuracy: 53.45
Round  64, Global train loss: 1.910, Global test loss: 2.252, Global test accuracy: 19.48
Round  65, Train loss: 1.939, Test loss: 1.926, Test accuracy: 53.33
Round  65, Global train loss: 1.939, Global test loss: 2.281, Global test accuracy: 15.77
Round  66, Train loss: 1.874, Test loss: 1.920, Test accuracy: 53.94
Round  66, Global train loss: 1.874, Global test loss: 2.271, Global test accuracy: 17.27
Round  67, Train loss: 1.904, Test loss: 1.923, Test accuracy: 53.70
Round  67, Global train loss: 1.904, Global test loss: 2.265, Global test accuracy: 17.39
Round  68, Train loss: 1.935, Test loss: 1.924, Test accuracy: 53.52
Round  68, Global train loss: 1.935, Global test loss: 2.285, Global test accuracy: 15.21
Round  69, Train loss: 1.928, Test loss: 1.921, Test accuracy: 53.86
Round  69, Global train loss: 1.928, Global test loss: 2.269, Global test accuracy: 17.30
Round  70, Train loss: 1.958, Test loss: 1.916, Test accuracy: 54.24
Round  70, Global train loss: 1.958, Global test loss: 2.271, Global test accuracy: 17.28
Round  71, Train loss: 1.893, Test loss: 1.915, Test accuracy: 54.30
Round  71, Global train loss: 1.893, Global test loss: 2.268, Global test accuracy: 17.57
Round  72, Train loss: 1.938, Test loss: 1.916, Test accuracy: 54.28
Round  72, Global train loss: 1.938, Global test loss: 2.269, Global test accuracy: 17.23
Round  73, Train loss: 1.900, Test loss: 1.913, Test accuracy: 54.63
Round  73, Global train loss: 1.900, Global test loss: 2.270, Global test accuracy: 16.88
Round  74, Train loss: 1.898, Test loss: 1.912, Test accuracy: 54.76
Round  74, Global train loss: 1.898, Global test loss: 2.281, Global test accuracy: 15.90
Round  75, Train loss: 1.930, Test loss: 1.908, Test accuracy: 55.09
Round  75, Global train loss: 1.930, Global test loss: 2.261, Global test accuracy: 18.18
Round  76, Train loss: 1.912, Test loss: 1.914, Test accuracy: 54.43
Round  76, Global train loss: 1.912, Global test loss: 2.256, Global test accuracy: 18.77
Round  77, Train loss: 1.927, Test loss: 1.911, Test accuracy: 54.73
Round  77, Global train loss: 1.927, Global test loss: 2.262, Global test accuracy: 18.12
Round  78, Train loss: 1.911, Test loss: 1.905, Test accuracy: 55.31
Round  78, Global train loss: 1.911, Global test loss: 2.262, Global test accuracy: 17.68
Round  79, Train loss: 1.913, Test loss: 1.907, Test accuracy: 55.25
Round  79, Global train loss: 1.913, Global test loss: 2.265, Global test accuracy: 17.60
Round  80, Train loss: 1.869, Test loss: 1.899, Test accuracy: 56.04
Round  80, Global train loss: 1.869, Global test loss: 2.268, Global test accuracy: 17.02
Round  81, Train loss: 1.846, Test loss: 1.900, Test accuracy: 56.08
Round  81, Global train loss: 1.846, Global test loss: 2.265, Global test accuracy: 17.84
Round  82, Train loss: 1.889, Test loss: 1.900, Test accuracy: 56.09
Round  82, Global train loss: 1.889, Global test loss: 2.271, Global test accuracy: 16.49
Round  83, Train loss: 1.887, Test loss: 1.904, Test accuracy: 55.53
Round  83, Global train loss: 1.887, Global test loss: 2.258, Global test accuracy: 18.32
Round  84, Train loss: 1.887, Test loss: 1.903, Test accuracy: 55.60
Round  84, Global train loss: 1.887, Global test loss: 2.258, Global test accuracy: 18.77
Round  85, Train loss: 1.872, Test loss: 1.903, Test accuracy: 55.72
Round  85, Global train loss: 1.872, Global test loss: 2.257, Global test accuracy: 18.70
Round  86, Train loss: 1.847, Test loss: 1.903, Test accuracy: 55.73
Round  86, Global train loss: 1.847, Global test loss: 2.262, Global test accuracy: 18.09
Round  87, Train loss: 1.829, Test loss: 1.902, Test accuracy: 55.66
Round  87, Global train loss: 1.829, Global test loss: 2.258, Global test accuracy: 18.79
Round  88, Train loss: 1.887, Test loss: 1.904, Test accuracy: 55.45
Round  88, Global train loss: 1.887, Global test loss: 2.264, Global test accuracy: 17.43
Round  89, Train loss: 1.887, Test loss: 1.903, Test accuracy: 55.57
Round  89, Global train loss: 1.887, Global test loss: 2.265, Global test accuracy: 17.60
Round  90, Train loss: 1.823, Test loss: 1.899, Test accuracy: 56.04
Round  90, Global train loss: 1.823, Global test loss: 2.264, Global test accuracy: 17.82
Round  91, Train loss: 1.862, Test loss: 1.898, Test accuracy: 56.19
Round  91, Global train loss: 1.862, Global test loss: 2.283, Global test accuracy: 15.33
Round  92, Train loss: 1.863, Test loss: 1.898, Test accuracy: 56.16
Round  92, Global train loss: 1.863, Global test loss: 2.261, Global test accuracy: 18.47
Round  93, Train loss: 1.858, Test loss: 1.909, Test accuracy: 55.08
Round  93, Global train loss: 1.858, Global test loss: 2.257, Global test accuracy: 18.93
Round  94, Train loss: 1.851, Test loss: 1.908, Test accuracy: 55.11
Round  94, Global train loss: 1.851, Global test loss: 2.259, Global test accuracy: 18.61
Round  95, Train loss: 1.870, Test loss: 1.902, Test accuracy: 55.73
Round  95, Global train loss: 1.870, Global test loss: 2.277, Global test accuracy: 16.24/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.928, Test loss: 1.891, Test accuracy: 56.85
Round  96, Global train loss: 1.928, Global test loss: 2.266, Global test accuracy: 17.61
Round  97, Train loss: 1.880, Test loss: 1.887, Test accuracy: 57.28
Round  97, Global train loss: 1.880, Global test loss: 2.274, Global test accuracy: 16.18
Round  98, Train loss: 1.879, Test loss: 1.891, Test accuracy: 56.86
Round  98, Global train loss: 1.879, Global test loss: 2.270, Global test accuracy: 17.02
Round  99, Train loss: 1.895, Test loss: 1.895, Test accuracy: 56.40
Round  99, Global train loss: 1.895, Global test loss: 2.261, Global test accuracy: 18.00
Final Round, Train loss: 1.845, Test loss: 1.885, Test accuracy: 57.27
Final Round, Global train loss: 1.845, Global test loss: 2.261, Global test accuracy: 18.00
Average accuracy final 10 rounds: 56.17 

Average global accuracy final 10 rounds: 17.42 

1799.8145911693573
[1.3159425258636475, 2.631885051727295, 3.831759452819824, 5.0316338539123535, 6.171195983886719, 7.310758113861084, 8.463684320449829, 9.616610527038574, 10.786265850067139, 11.955921173095703, 13.061208009719849, 14.166494846343994, 15.2741379737854, 16.381781101226807, 17.53905725479126, 18.696333408355713, 19.795631408691406, 20.8949294090271, 21.951335430145264, 23.007741451263428, 24.150272846221924, 25.29280424118042, 26.44931197166443, 27.605819702148438, 28.68204116821289, 29.758262634277344, 30.88025736808777, 32.00225210189819, 33.11403751373291, 34.22582292556763, 35.35825490951538, 36.490686893463135, 37.61275792121887, 38.73482894897461, 39.84144568443298, 40.94806241989136, 42.13667345046997, 43.325284481048584, 44.4098002910614, 45.49431610107422, 46.576963901519775, 47.65961170196533, 48.7870237827301, 49.91443586349487, 51.013325452804565, 52.11221504211426, 53.19741892814636, 54.28262281417847, 55.38697147369385, 56.49132013320923, 57.64729118347168, 58.80326223373413, 59.897380113601685, 60.99149799346924, 62.09899044036865, 63.206482887268066, 64.35217356681824, 65.49786424636841, 66.66073179244995, 67.8235993385315, 68.9341492652893, 70.04469919204712, 71.19604659080505, 72.34739398956299, 73.56592178344727, 74.78444957733154, 75.85504102706909, 76.92563247680664, 78.08150506019592, 79.2373776435852, 80.42908382415771, 81.62079000473022, 82.7816092967987, 83.94242858886719, 85.06629586219788, 86.19016313552856, 87.34170937538147, 88.49325561523438, 89.65933895111084, 90.8254222869873, 91.89752531051636, 92.96962833404541, 94.09196376800537, 95.21429920196533, 96.29400897026062, 97.37371873855591, 98.51908087730408, 99.66444301605225, 100.73501801490784, 101.80559301376343, 102.90532994270325, 104.00506687164307, 105.15611839294434, 106.3071699142456, 107.35923147201538, 108.41129302978516, 109.48007440567017, 110.54885578155518, 111.65725564956665, 112.76565551757812, 113.88474655151367, 115.00383758544922, 116.1015100479126, 117.19918251037598, 118.29058074951172, 119.38197898864746, 120.56247353553772, 121.74296808242798, 122.8605432510376, 123.97811841964722, 125.04835176467896, 126.1185851097107, 127.26333165168762, 128.40807819366455, 129.54012727737427, 130.67217636108398, 131.75904893875122, 132.84592151641846, 133.98553681373596, 135.12515211105347, 136.31965827941895, 137.51416444778442, 138.53855967521667, 139.56295490264893, 140.64682698249817, 141.7306990623474, 142.85450530052185, 143.9783115386963, 145.07406949996948, 146.16982746124268, 147.26026940345764, 148.3507113456726, 149.44165873527527, 150.53260612487793, 151.65622401237488, 152.77984189987183, 153.86505556106567, 154.95026922225952, 156.05805492401123, 157.16584062576294, 158.21313905715942, 159.2604374885559, 160.41804599761963, 161.57565450668335, 162.6578528881073, 163.74005126953125, 164.8143322467804, 165.88861322402954, 167.01967358589172, 168.1507339477539, 169.26363229751587, 170.37653064727783, 171.4637303352356, 172.55093002319336, 173.66663360595703, 174.7823371887207, 175.9279980659485, 177.07365894317627, 178.17564749717712, 179.27763605117798, 180.35859322547913, 181.43955039978027, 182.57999086380005, 183.72043132781982, 184.84283876419067, 185.96524620056152, 187.0258014202118, 188.08635663986206, 189.19652318954468, 190.3066897392273, 191.47163105010986, 192.63657236099243, 193.72435450553894, 194.81213665008545, 195.90543341636658, 196.9987301826477, 198.16257405281067, 199.32641792297363, 200.38005828857422, 201.4336986541748, 202.55785536766052, 203.68201208114624, 204.82567524909973, 205.96933841705322, 207.13588571548462, 208.30243301391602, 209.40849018096924, 210.51454734802246, 211.64920949935913, 212.7838716506958, 213.91191792488098, 215.03996419906616, 216.1642608642578, 217.28855752944946, 218.3996560573578, 219.5107545852661, 220.5837414264679, 221.65672826766968, 222.7964973449707, 223.93626642227173, 225.78283667564392, 227.6294069290161]
[13.85, 13.85, 19.425, 19.425, 22.775, 22.775, 26.841666666666665, 26.841666666666665, 28.525, 28.525, 29.783333333333335, 29.783333333333335, 27.833333333333332, 27.833333333333332, 30.408333333333335, 30.408333333333335, 34.016666666666666, 34.016666666666666, 37.21666666666667, 37.21666666666667, 40.425, 40.425, 42.416666666666664, 42.416666666666664, 43.141666666666666, 43.141666666666666, 43.35, 43.35, 44.5, 44.5, 46.358333333333334, 46.358333333333334, 46.6, 46.6, 46.15, 46.15, 46.525, 46.525, 47.641666666666666, 47.641666666666666, 48.11666666666667, 48.11666666666667, 47.391666666666666, 47.391666666666666, 47.7, 47.7, 48.15, 48.15, 46.65, 46.65, 47.516666666666666, 47.516666666666666, 47.7, 47.7, 48.86666666666667, 48.86666666666667, 47.75833333333333, 47.75833333333333, 47.90833333333333, 47.90833333333333, 48.50833333333333, 48.50833333333333, 48.09166666666667, 48.09166666666667, 48.083333333333336, 48.083333333333336, 48.21666666666667, 48.21666666666667, 46.641666666666666, 46.641666666666666, 47.358333333333334, 47.358333333333334, 47.49166666666667, 47.49166666666667, 47.88333333333333, 47.88333333333333, 48.40833333333333, 48.40833333333333, 48.375, 48.375, 48.075, 48.075, 48.06666666666667, 48.06666666666667, 48.958333333333336, 48.958333333333336, 49.55833333333333, 49.55833333333333, 49.6, 49.6, 48.49166666666667, 48.49166666666667, 48.475, 48.475, 48.475, 48.475, 48.641666666666666, 48.641666666666666, 49.68333333333333, 49.68333333333333, 49.35, 49.35, 49.9, 49.9, 49.916666666666664, 49.916666666666664, 49.49166666666667, 49.49166666666667, 49.4, 49.4, 49.4, 49.4, 49.65833333333333, 49.65833333333333, 50.18333333333333, 50.18333333333333, 50.74166666666667, 50.74166666666667, 51.75833333333333, 51.75833333333333, 52.09166666666667, 52.09166666666667, 52.28333333333333, 52.28333333333333, 52.291666666666664, 52.291666666666664, 52.625, 52.625, 53.45, 53.45, 53.333333333333336, 53.333333333333336, 53.94166666666667, 53.94166666666667, 53.7, 53.7, 53.516666666666666, 53.516666666666666, 53.858333333333334, 53.858333333333334, 54.24166666666667, 54.24166666666667, 54.3, 54.3, 54.28333333333333, 54.28333333333333, 54.63333333333333, 54.63333333333333, 54.75833333333333, 54.75833333333333, 55.09166666666667, 55.09166666666667, 54.43333333333333, 54.43333333333333, 54.733333333333334, 54.733333333333334, 55.30833333333333, 55.30833333333333, 55.25, 55.25, 56.041666666666664, 56.041666666666664, 56.075, 56.075, 56.09166666666667, 56.09166666666667, 55.53333333333333, 55.53333333333333, 55.6, 55.6, 55.71666666666667, 55.71666666666667, 55.733333333333334, 55.733333333333334, 55.65833333333333, 55.65833333333333, 55.45, 55.45, 55.56666666666667, 55.56666666666667, 56.041666666666664, 56.041666666666664, 56.19166666666667, 56.19166666666667, 56.15833333333333, 56.15833333333333, 55.075, 55.075, 55.108333333333334, 55.108333333333334, 55.733333333333334, 55.733333333333334, 56.85, 56.85, 57.28333333333333, 57.28333333333333, 56.858333333333334, 56.858333333333334, 56.4, 56.4, 57.266666666666666, 57.266666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.02
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.28
Round   2, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.26
Round   3, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.16
Round   4, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.60
Round   5, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.48
Round   6, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.51
Round   7, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.56
Round   8, Train loss: 2.300, Test loss: 2.301, Test accuracy: 14.25
Round   9, Train loss: 2.298, Test loss: 2.300, Test accuracy: 14.94
Round  10, Train loss: 2.298, Test loss: 2.299, Test accuracy: 16.04
Round  11, Train loss: 2.297, Test loss: 2.299, Test accuracy: 17.35
Round  12, Train loss: 2.297, Test loss: 2.297, Test accuracy: 17.71
Round  13, Train loss: 2.291, Test loss: 2.295, Test accuracy: 18.14
Round  14, Train loss: 2.283, Test loss: 2.291, Test accuracy: 16.70
Round  15, Train loss: 2.272, Test loss: 2.282, Test accuracy: 16.98
Round  16, Train loss: 2.251, Test loss: 2.271, Test accuracy: 19.87
Round  17, Train loss: 2.251, Test loss: 2.255, Test accuracy: 22.48
Round  18, Train loss: 2.200, Test loss: 2.231, Test accuracy: 23.42
Round  19, Train loss: 2.142, Test loss: 2.195, Test accuracy: 25.82
Round  20, Train loss: 2.091, Test loss: 2.152, Test accuracy: 30.57
Round  21, Train loss: 2.058, Test loss: 2.117, Test accuracy: 34.36
Round  22, Train loss: 2.001, Test loss: 2.096, Test accuracy: 36.09
Round  23, Train loss: 2.023, Test loss: 2.060, Test accuracy: 39.95
Round  24, Train loss: 1.974, Test loss: 2.035, Test accuracy: 42.66
Round  25, Train loss: 1.965, Test loss: 2.010, Test accuracy: 45.38
Round  26, Train loss: 1.953, Test loss: 1.996, Test accuracy: 46.67
Round  27, Train loss: 1.890, Test loss: 1.983, Test accuracy: 48.27
Round  28, Train loss: 1.990, Test loss: 1.964, Test accuracy: 50.34
Round  29, Train loss: 1.944, Test loss: 1.941, Test accuracy: 52.94
Round  30, Train loss: 1.914, Test loss: 1.937, Test accuracy: 53.41
Round  31, Train loss: 1.925, Test loss: 1.914, Test accuracy: 55.41
Round  32, Train loss: 1.907, Test loss: 1.910, Test accuracy: 55.71
Round  33, Train loss: 1.871, Test loss: 1.906, Test accuracy: 56.07
Round  34, Train loss: 1.872, Test loss: 1.903, Test accuracy: 56.23
Round  35, Train loss: 1.899, Test loss: 1.901, Test accuracy: 56.41
Round  36, Train loss: 1.873, Test loss: 1.899, Test accuracy: 56.58
Round  37, Train loss: 1.868, Test loss: 1.893, Test accuracy: 57.14
Round  38, Train loss: 1.872, Test loss: 1.888, Test accuracy: 57.77
Round  39, Train loss: 1.848, Test loss: 1.883, Test accuracy: 58.10
Round  40, Train loss: 1.840, Test loss: 1.880, Test accuracy: 58.45
Round  41, Train loss: 1.895, Test loss: 1.875, Test accuracy: 58.91
Round  42, Train loss: 1.885, Test loss: 1.874, Test accuracy: 59.03
Round  43, Train loss: 1.828, Test loss: 1.872, Test accuracy: 59.20
Round  44, Train loss: 1.831, Test loss: 1.870, Test accuracy: 59.27
Round  45, Train loss: 1.822, Test loss: 1.866, Test accuracy: 59.77
Round  46, Train loss: 1.825, Test loss: 1.864, Test accuracy: 59.88
Round  47, Train loss: 1.804, Test loss: 1.864, Test accuracy: 59.95
Round  48, Train loss: 1.838, Test loss: 1.863, Test accuracy: 59.92
Round  49, Train loss: 1.784, Test loss: 1.863, Test accuracy: 59.84
Round  50, Train loss: 1.854, Test loss: 1.862, Test accuracy: 59.91
Round  51, Train loss: 1.869, Test loss: 1.860, Test accuracy: 60.14
Round  52, Train loss: 1.850, Test loss: 1.858, Test accuracy: 60.31
Round  53, Train loss: 1.825, Test loss: 1.858, Test accuracy: 60.42
Round  54, Train loss: 1.802, Test loss: 1.858, Test accuracy: 60.43
Round  55, Train loss: 1.868, Test loss: 1.857, Test accuracy: 60.38
Round  56, Train loss: 1.809, Test loss: 1.857, Test accuracy: 60.43
Round  57, Train loss: 1.818, Test loss: 1.851, Test accuracy: 61.13
Round  58, Train loss: 1.801, Test loss: 1.850, Test accuracy: 61.24
Round  59, Train loss: 1.794, Test loss: 1.849, Test accuracy: 61.13
Round  60, Train loss: 1.761, Test loss: 1.849, Test accuracy: 61.19
Round  61, Train loss: 1.832, Test loss: 1.849, Test accuracy: 61.23
Round  62, Train loss: 1.829, Test loss: 1.849, Test accuracy: 61.22
Round  63, Train loss: 1.823, Test loss: 1.848, Test accuracy: 61.27
Round  64, Train loss: 1.824, Test loss: 1.849, Test accuracy: 61.29
Round  65, Train loss: 1.826, Test loss: 1.848, Test accuracy: 61.31
Round  66, Train loss: 1.770, Test loss: 1.846, Test accuracy: 61.58
Round  67, Train loss: 1.877, Test loss: 1.846, Test accuracy: 61.42
Round  68, Train loss: 1.858, Test loss: 1.845, Test accuracy: 61.58
Round  69, Train loss: 1.797, Test loss: 1.845, Test accuracy: 61.58
Round  70, Train loss: 1.761, Test loss: 1.844, Test accuracy: 61.66
Round  71, Train loss: 1.776, Test loss: 1.845, Test accuracy: 61.62
Round  72, Train loss: 1.829, Test loss: 1.844, Test accuracy: 61.78
Round  73, Train loss: 1.846, Test loss: 1.843, Test accuracy: 61.75
Round  74, Train loss: 1.819, Test loss: 1.843, Test accuracy: 61.76
Round  75, Train loss: 1.862, Test loss: 1.843, Test accuracy: 61.73
Round  76, Train loss: 1.862, Test loss: 1.843, Test accuracy: 61.84
Round  77, Train loss: 1.812, Test loss: 1.842, Test accuracy: 61.92
Round  78, Train loss: 1.820, Test loss: 1.842, Test accuracy: 61.88
Round  79, Train loss: 1.822, Test loss: 1.841, Test accuracy: 61.92
Round  80, Train loss: 1.815, Test loss: 1.841, Test accuracy: 61.94
Round  81, Train loss: 1.817, Test loss: 1.838, Test accuracy: 62.23
Round  82, Train loss: 1.784, Test loss: 1.838, Test accuracy: 62.29
Round  83, Train loss: 1.813, Test loss: 1.838, Test accuracy: 62.23
Round  84, Train loss: 1.801, Test loss: 1.837, Test accuracy: 62.36
Round  85, Train loss: 1.784, Test loss: 1.837, Test accuracy: 62.38
Round  86, Train loss: 1.816, Test loss: 1.837, Test accuracy: 62.38
Round  87, Train loss: 1.816, Test loss: 1.836, Test accuracy: 62.45
Round  88, Train loss: 1.848, Test loss: 1.836, Test accuracy: 62.53
Round  89, Train loss: 1.751, Test loss: 1.835, Test accuracy: 62.45
Round  90, Train loss: 1.825, Test loss: 1.835, Test accuracy: 62.46
Round  91, Train loss: 1.799, Test loss: 1.835, Test accuracy: 62.52
Round  92, Train loss: 1.837, Test loss: 1.834, Test accuracy: 62.52
Round  93, Train loss: 1.819, Test loss: 1.833, Test accuracy: 62.71
Round  94, Train loss: 1.806, Test loss: 1.833, Test accuracy: 62.74
Round  95, Train loss: 1.809, Test loss: 1.833, Test accuracy: 62.66
Round  96, Train loss: 1.797, Test loss: 1.830, Test accuracy: 63.02
Round  97, Train loss: 1.847, Test loss: 1.830, Test accuracy: 63.00
Round  98, Train loss: 1.800, Test loss: 1.829, Test accuracy: 63.02/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.817, Test loss: 1.829, Test accuracy: 63.02
Final Round, Train loss: 1.797, Test loss: 1.828, Test accuracy: 63.17
Average accuracy final 10 rounds: 62.767500000000005 

1410.395344734192
[1.1953210830688477, 2.3906421661376953, 3.4845526218414307, 4.578463077545166, 5.642160892486572, 6.7058587074279785, 7.8509345054626465, 8.996010303497314, 10.138628005981445, 11.281245708465576, 12.40322756767273, 13.525209426879883, 14.581189155578613, 15.637168884277344, 16.724204301834106, 17.81123971939087, 18.9644296169281, 20.117619514465332, 21.213669776916504, 22.309720039367676, 23.3784601688385, 24.447200298309326, 25.531481504440308, 26.61576271057129, 27.744181871414185, 28.87260103225708, 30.036638736724854, 31.200676441192627, 32.24859070777893, 33.296504974365234, 34.37670063972473, 35.45689630508423, 36.58472752571106, 37.71255874633789, 38.845258474349976, 39.97795820236206, 41.02566313743591, 42.073368072509766, 43.12613773345947, 44.17890739440918, 45.22411918640137, 46.269330978393555, 47.410780906677246, 48.55223083496094, 49.65454697608948, 50.75686311721802, 51.846312046051025, 52.93576097488403, 53.99504780769348, 55.05433464050293, 56.143678426742554, 57.23302221298218, 58.35278034210205, 59.472538471221924, 60.54545211791992, 61.61836576461792, 62.68355965614319, 63.74875354766846, 64.80830812454224, 65.86786270141602, 66.89372515678406, 67.9195876121521, 69.03536581993103, 70.15114402770996, 71.20132946968079, 72.25151491165161, 73.31281447410583, 74.37411403656006, 75.48823285102844, 76.60235166549683, 77.78713130950928, 78.97191095352173, 80.03572988510132, 81.09954881668091, 82.16669511795044, 83.23384141921997, 84.2934000492096, 85.35295867919922, 86.44344019889832, 87.53392171859741, 88.63122153282166, 89.7285213470459, 90.74749660491943, 91.76647186279297, 92.84387183189392, 93.92127180099487, 95.01589488983154, 96.11051797866821, 97.22450637817383, 98.33849477767944, 99.50386142730713, 100.66922807693481, 101.70821642875671, 102.74720478057861, 103.8283178806305, 104.90943098068237, 106.1405999660492, 107.37176895141602, 108.48522448539734, 109.59868001937866, 110.57725358009338, 111.5558271408081, 112.58451986312866, 113.61321258544922, 114.6735475063324, 115.73388242721558, 116.82829546928406, 117.92270851135254, 119.01426792144775, 120.10582733154297, 121.16088724136353, 122.21594715118408, 123.24824404716492, 124.28054094314575, 125.37849545478821, 126.47644996643066, 127.58934497833252, 128.70223999023438, 129.80991768836975, 130.91759538650513, 131.9768307209015, 133.03606605529785, 134.13354301452637, 135.23101997375488, 136.3462061882019, 137.46139240264893, 138.52216386795044, 139.58293533325195, 140.64947271347046, 141.71601009368896, 142.7547607421875, 143.79351139068604, 144.8611671924591, 145.92882299423218, 147.00883603096008, 148.088849067688, 149.12974572181702, 150.17064237594604, 151.21062660217285, 152.25061082839966, 153.31525301933289, 154.3798952102661, 155.46372771263123, 156.54756021499634, 157.65326690673828, 158.75897359848022, 159.86094784736633, 160.96292209625244, 162.04820728302002, 163.1334924697876, 164.2283341884613, 165.323175907135, 166.44380974769592, 167.56444358825684, 168.66448545455933, 169.76452732086182, 170.84324049949646, 171.9219536781311, 172.9674048423767, 174.01285600662231, 175.05019879341125, 176.0875415802002, 177.08019185066223, 178.07284212112427, 179.05434370040894, 180.0358452796936, 181.01662254333496, 181.99739980697632, 182.9389100074768, 183.8804202079773, 184.85938596725464, 185.83835172653198, 186.86545658111572, 187.89256143569946, 188.90847826004028, 189.9243950843811, 190.85285925865173, 191.78132343292236, 192.78739190101624, 193.7934603691101, 194.83647680282593, 195.87949323654175, 196.8626925945282, 197.84589195251465, 198.84387755393982, 199.841863155365, 200.78817677497864, 201.73449039459229, 202.67895364761353, 203.62341690063477, 204.6202142238617, 205.61701154708862, 206.65031099319458, 207.68361043930054, 208.70673894882202, 209.7298674583435, 210.68770098686218, 211.64553451538086, 212.62820482254028, 213.6108751296997, 215.24676728248596, 216.88265943527222]
[10.016666666666667, 10.016666666666667, 10.275, 10.275, 10.258333333333333, 10.258333333333333, 10.158333333333333, 10.158333333333333, 10.6, 10.6, 11.483333333333333, 11.483333333333333, 12.508333333333333, 12.508333333333333, 13.558333333333334, 13.558333333333334, 14.25, 14.25, 14.941666666666666, 14.941666666666666, 16.041666666666668, 16.041666666666668, 17.35, 17.35, 17.708333333333332, 17.708333333333332, 18.141666666666666, 18.141666666666666, 16.7, 16.7, 16.975, 16.975, 19.866666666666667, 19.866666666666667, 22.475, 22.475, 23.416666666666668, 23.416666666666668, 25.816666666666666, 25.816666666666666, 30.566666666666666, 30.566666666666666, 34.358333333333334, 34.358333333333334, 36.09166666666667, 36.09166666666667, 39.95, 39.95, 42.65833333333333, 42.65833333333333, 45.38333333333333, 45.38333333333333, 46.666666666666664, 46.666666666666664, 48.266666666666666, 48.266666666666666, 50.34166666666667, 50.34166666666667, 52.94166666666667, 52.94166666666667, 53.40833333333333, 53.40833333333333, 55.40833333333333, 55.40833333333333, 55.708333333333336, 55.708333333333336, 56.06666666666667, 56.06666666666667, 56.225, 56.225, 56.40833333333333, 56.40833333333333, 56.583333333333336, 56.583333333333336, 57.141666666666666, 57.141666666666666, 57.766666666666666, 57.766666666666666, 58.1, 58.1, 58.45, 58.45, 58.90833333333333, 58.90833333333333, 59.03333333333333, 59.03333333333333, 59.2, 59.2, 59.275, 59.275, 59.766666666666666, 59.766666666666666, 59.88333333333333, 59.88333333333333, 59.95, 59.95, 59.925, 59.925, 59.84166666666667, 59.84166666666667, 59.90833333333333, 59.90833333333333, 60.141666666666666, 60.141666666666666, 60.30833333333333, 60.30833333333333, 60.425, 60.425, 60.43333333333333, 60.43333333333333, 60.38333333333333, 60.38333333333333, 60.43333333333333, 60.43333333333333, 61.13333333333333, 61.13333333333333, 61.24166666666667, 61.24166666666667, 61.13333333333333, 61.13333333333333, 61.19166666666667, 61.19166666666667, 61.233333333333334, 61.233333333333334, 61.21666666666667, 61.21666666666667, 61.266666666666666, 61.266666666666666, 61.291666666666664, 61.291666666666664, 61.30833333333333, 61.30833333333333, 61.575, 61.575, 61.416666666666664, 61.416666666666664, 61.575, 61.575, 61.575, 61.575, 61.65833333333333, 61.65833333333333, 61.625, 61.625, 61.78333333333333, 61.78333333333333, 61.75, 61.75, 61.75833333333333, 61.75833333333333, 61.733333333333334, 61.733333333333334, 61.84166666666667, 61.84166666666667, 61.916666666666664, 61.916666666666664, 61.88333333333333, 61.88333333333333, 61.916666666666664, 61.916666666666664, 61.94166666666667, 61.94166666666667, 62.225, 62.225, 62.291666666666664, 62.291666666666664, 62.233333333333334, 62.233333333333334, 62.358333333333334, 62.358333333333334, 62.375, 62.375, 62.375, 62.375, 62.45, 62.45, 62.53333333333333, 62.53333333333333, 62.45, 62.45, 62.458333333333336, 62.458333333333336, 62.525, 62.525, 62.516666666666666, 62.516666666666666, 62.708333333333336, 62.708333333333336, 62.74166666666667, 62.74166666666667, 62.65833333333333, 62.65833333333333, 63.025, 63.025, 63.0, 63.0, 63.016666666666666, 63.016666666666666, 63.025, 63.025, 63.175, 63.175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.303, Test accuracy: 10.62
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 12.01
Round   2, Train loss: 2.299, Test loss: 2.302, Test accuracy: 12.03
Round   3, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.33
Round   4, Train loss: 2.295, Test loss: 2.301, Test accuracy: 11.93
Round   5, Train loss: 2.293, Test loss: 2.299, Test accuracy: 13.73
Round   6, Train loss: 2.284, Test loss: 2.295, Test accuracy: 13.34
Round   7, Train loss: 2.267, Test loss: 2.288, Test accuracy: 16.11
Round   8, Train loss: 2.237, Test loss: 2.269, Test accuracy: 18.26
Round   9, Train loss: 2.165, Test loss: 2.230, Test accuracy: 23.12
Round  10, Train loss: 2.066, Test loss: 2.184, Test accuracy: 28.17
Round  11, Train loss: 2.008, Test loss: 2.132, Test accuracy: 33.22
Round  12, Train loss: 1.957, Test loss: 2.077, Test accuracy: 38.81
Round  13, Train loss: 1.902, Test loss: 2.033, Test accuracy: 43.13
Round  14, Train loss: 1.918, Test loss: 1.986, Test accuracy: 48.44
Round  15, Train loss: 1.930, Test loss: 1.962, Test accuracy: 50.73
Round  16, Train loss: 1.897, Test loss: 1.934, Test accuracy: 53.43
Round  17, Train loss: 1.883, Test loss: 1.911, Test accuracy: 55.44
Round  18, Train loss: 1.841, Test loss: 1.902, Test accuracy: 56.45
Round  19, Train loss: 1.856, Test loss: 1.894, Test accuracy: 57.34
Round  20, Train loss: 1.857, Test loss: 1.883, Test accuracy: 58.40
Round  21, Train loss: 1.814, Test loss: 1.871, Test accuracy: 59.58
Round  22, Train loss: 1.831, Test loss: 1.868, Test accuracy: 59.77
Round  23, Train loss: 1.804, Test loss: 1.857, Test accuracy: 60.79
Round  24, Train loss: 1.793, Test loss: 1.845, Test accuracy: 61.83
Round  25, Train loss: 1.831, Test loss: 1.841, Test accuracy: 62.48
Round  26, Train loss: 1.811, Test loss: 1.834, Test accuracy: 63.09
Round  27, Train loss: 1.774, Test loss: 1.831, Test accuracy: 63.38
Round  28, Train loss: 1.801, Test loss: 1.830, Test accuracy: 63.29
Round  29, Train loss: 1.770, Test loss: 1.824, Test accuracy: 64.01
Round  30, Train loss: 1.825, Test loss: 1.822, Test accuracy: 64.27
Round  31, Train loss: 1.733, Test loss: 1.819, Test accuracy: 64.34
Round  32, Train loss: 1.764, Test loss: 1.818, Test accuracy: 64.50
Round  33, Train loss: 1.775, Test loss: 1.817, Test accuracy: 64.58
Round  34, Train loss: 1.791, Test loss: 1.816, Test accuracy: 64.57
Round  35, Train loss: 1.768, Test loss: 1.811, Test accuracy: 65.17
Round  36, Train loss: 1.778, Test loss: 1.806, Test accuracy: 65.48
Round  37, Train loss: 1.791, Test loss: 1.804, Test accuracy: 65.71
Round  38, Train loss: 1.738, Test loss: 1.804, Test accuracy: 65.76
Round  39, Train loss: 1.724, Test loss: 1.802, Test accuracy: 65.93
Round  40, Train loss: 1.769, Test loss: 1.801, Test accuracy: 66.03
Round  41, Train loss: 1.726, Test loss: 1.800, Test accuracy: 66.09
Round  42, Train loss: 1.733, Test loss: 1.800, Test accuracy: 66.12
Round  43, Train loss: 1.740, Test loss: 1.799, Test accuracy: 66.08
Round  44, Train loss: 1.733, Test loss: 1.798, Test accuracy: 66.23
Round  45, Train loss: 1.749, Test loss: 1.798, Test accuracy: 66.31
Round  46, Train loss: 1.753, Test loss: 1.793, Test accuracy: 66.89
Round  47, Train loss: 1.746, Test loss: 1.792, Test accuracy: 67.03
Round  48, Train loss: 1.767, Test loss: 1.791, Test accuracy: 67.05
Round  49, Train loss: 1.777, Test loss: 1.791, Test accuracy: 67.01
Round  50, Train loss: 1.744, Test loss: 1.790, Test accuracy: 67.17
Round  51, Train loss: 1.782, Test loss: 1.790, Test accuracy: 67.22
Round  52, Train loss: 1.755, Test loss: 1.790, Test accuracy: 67.17
Round  53, Train loss: 1.750, Test loss: 1.790, Test accuracy: 67.17
Round  54, Train loss: 1.734, Test loss: 1.789, Test accuracy: 67.19
Round  55, Train loss: 1.790, Test loss: 1.787, Test accuracy: 67.47
Round  56, Train loss: 1.725, Test loss: 1.785, Test accuracy: 67.66
Round  57, Train loss: 1.760, Test loss: 1.784, Test accuracy: 67.74
Round  58, Train loss: 1.741, Test loss: 1.784, Test accuracy: 67.78
Round  59, Train loss: 1.752, Test loss: 1.784, Test accuracy: 67.78
Round  60, Train loss: 1.758, Test loss: 1.784, Test accuracy: 67.77
Round  61, Train loss: 1.734, Test loss: 1.783, Test accuracy: 67.92
Round  62, Train loss: 1.763, Test loss: 1.782, Test accuracy: 67.88
Round  63, Train loss: 1.751, Test loss: 1.782, Test accuracy: 67.84
Round  64, Train loss: 1.696, Test loss: 1.782, Test accuracy: 67.97
Round  65, Train loss: 1.735, Test loss: 1.782, Test accuracy: 67.97
Round  66, Train loss: 1.760, Test loss: 1.779, Test accuracy: 68.28
Round  67, Train loss: 1.725, Test loss: 1.778, Test accuracy: 68.37
Round  68, Train loss: 1.798, Test loss: 1.779, Test accuracy: 68.28
Round  69, Train loss: 1.758, Test loss: 1.779, Test accuracy: 68.17
Round  70, Train loss: 1.741, Test loss: 1.779, Test accuracy: 68.25
Round  71, Train loss: 1.733, Test loss: 1.779, Test accuracy: 68.21
Round  72, Train loss: 1.734, Test loss: 1.777, Test accuracy: 68.34
Round  73, Train loss: 1.765, Test loss: 1.776, Test accuracy: 68.52
Round  74, Train loss: 1.668, Test loss: 1.774, Test accuracy: 68.71
Round  75, Train loss: 1.709, Test loss: 1.773, Test accuracy: 68.80
Round  76, Train loss: 1.756, Test loss: 1.772, Test accuracy: 68.79
Round  77, Train loss: 1.701, Test loss: 1.772, Test accuracy: 68.88
Round  78, Train loss: 1.725, Test loss: 1.772, Test accuracy: 68.90
Round  79, Train loss: 1.698, Test loss: 1.772, Test accuracy: 68.88
Round  80, Train loss: 1.723, Test loss: 1.768, Test accuracy: 69.30
Round  81, Train loss: 1.696, Test loss: 1.767, Test accuracy: 69.39
Round  82, Train loss: 1.736, Test loss: 1.767, Test accuracy: 69.45
Round  83, Train loss: 1.720, Test loss: 1.766, Test accuracy: 69.59
Round  84, Train loss: 1.720, Test loss: 1.762, Test accuracy: 69.86
Round  85, Train loss: 1.738, Test loss: 1.762, Test accuracy: 69.83
Round  86, Train loss: 1.673, Test loss: 1.762, Test accuracy: 69.85
Round  87, Train loss: 1.703, Test loss: 1.762, Test accuracy: 69.86
Round  88, Train loss: 1.744, Test loss: 1.761, Test accuracy: 69.92
Round  89, Train loss: 1.689, Test loss: 1.761, Test accuracy: 69.92
Round  90, Train loss: 1.743, Test loss: 1.761, Test accuracy: 69.97
Round  91, Train loss: 1.694, Test loss: 1.761, Test accuracy: 70.00
Round  92, Train loss: 1.704, Test loss: 1.761, Test accuracy: 70.03
Round  93, Train loss: 1.746, Test loss: 1.760, Test accuracy: 70.04
Round  94, Train loss: 1.718, Test loss: 1.760, Test accuracy: 70.03
Round  95, Train loss: 1.702, Test loss: 1.760, Test accuracy: 70.12
Round  96, Train loss: 1.710, Test loss: 1.760, Test accuracy: 70.09
Round  97, Train loss: 1.745, Test loss: 1.760, Test accuracy: 70.08
Round  98, Train loss: 1.687, Test loss: 1.759, Test accuracy: 70.15/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.729, Test loss: 1.760, Test accuracy: 70.14
Final Round, Train loss: 1.712, Test loss: 1.753, Test accuracy: 70.76
Average accuracy final 10 rounds: 70.065 

1411.3310551643372
[1.2434194087982178, 2.4868388175964355, 3.6330347061157227, 4.77923059463501, 5.94815731048584, 7.11708402633667, 8.28020715713501, 9.44333028793335, 10.59785008430481, 11.75236988067627, 13.01192593574524, 14.271481990814209, 15.427062034606934, 16.582642078399658, 17.76407480239868, 18.945507526397705, 20.011295318603516, 21.077083110809326, 22.039695978164673, 23.00230884552002, 24.03781032562256, 25.073311805725098, 26.132901906967163, 27.19249200820923, 28.22310972213745, 29.253727436065674, 30.26509714126587, 31.276466846466064, 32.275551557540894, 33.27463626861572, 34.28298997879028, 35.291343688964844, 36.35448122024536, 37.41761875152588, 38.477121114730835, 39.53662347793579, 40.56234622001648, 41.58806896209717, 42.62438893318176, 43.66070890426636, 44.72260046005249, 45.78449201583862, 46.78482460975647, 47.785157203674316, 48.837958335876465, 49.89075946807861, 50.895864725112915, 51.90096998214722, 52.89484643936157, 53.88872289657593, 54.955222606658936, 56.02172231674194, 57.02206373214722, 58.02240514755249, 59.0744104385376, 60.126415729522705, 61.183475971221924, 62.24053621292114, 63.25527310371399, 64.27000999450684, 65.3289155960083, 66.38782119750977, 67.43613481521606, 68.48444843292236, 69.52557468414307, 70.56670093536377, 71.59034967422485, 72.61399841308594, 73.61457228660583, 74.61514616012573, 75.68397569656372, 76.75280523300171, 77.79640769958496, 78.84001016616821, 79.83878588676453, 80.83756160736084, 81.87647700309753, 82.91539239883423, 83.95816159248352, 85.00093078613281, 86.04325461387634, 87.08557844161987, 88.14171481132507, 89.19785118103027, 90.20716667175293, 91.21648216247559, 92.30107355117798, 93.38566493988037, 94.45029211044312, 95.51491928100586, 96.52743530273438, 97.53995132446289, 98.57233119010925, 99.60471105575562, 100.67427158355713, 101.74383211135864, 102.7764527797699, 103.80907344818115, 104.88765621185303, 105.9662389755249, 107.03574442863464, 108.10524988174438, 109.13321137428284, 110.16117286682129, 111.15287446975708, 112.14457607269287, 113.19066596031189, 114.23675584793091, 115.33257007598877, 116.42838430404663, 117.4433662891388, 118.45834827423096, 119.54473757743835, 120.63112688064575, 121.7090117931366, 122.78689670562744, 123.83207988739014, 124.87726306915283, 125.88171815872192, 126.88617324829102, 127.95266127586365, 129.01914930343628, 130.07614254951477, 131.13313579559326, 132.17159223556519, 133.2100486755371, 134.2867259979248, 135.3634033203125, 136.4076225757599, 137.45184183120728, 138.49465990066528, 139.5374779701233, 140.58067727088928, 141.62387657165527, 142.71184611320496, 143.79981565475464, 144.8951804637909, 145.99054527282715, 147.01340293884277, 148.0362606048584, 149.0990834236145, 150.1619062423706, 151.23397183418274, 152.30603742599487, 153.34991478919983, 154.39379215240479, 155.44854307174683, 156.50329399108887, 157.61710262298584, 158.7309112548828, 159.84610056877136, 160.9612898826599, 162.0606415271759, 163.1599931716919, 164.25751757621765, 165.3550419807434, 166.4311420917511, 167.5072422027588, 168.57439827919006, 169.64155435562134, 170.65431475639343, 171.66707515716553, 172.69915914535522, 173.73124313354492, 174.75571274757385, 175.78018236160278, 176.77702379226685, 177.7738652229309, 178.813072681427, 179.8522801399231, 180.9103889465332, 181.9684977531433, 183.01368021965027, 184.05886268615723, 185.0916690826416, 186.12447547912598, 187.16099071502686, 188.19750595092773, 189.20050024986267, 190.2034945487976, 191.22012901306152, 192.23676347732544, 193.28760647773743, 194.3384494781494, 195.33851766586304, 196.33858585357666, 197.38899421691895, 198.43940258026123, 199.47675704956055, 200.51411151885986, 201.52437949180603, 202.5346474647522, 203.5060260295868, 204.4774045944214, 205.48436641693115, 206.49132823944092, 207.5139262676239, 208.53652429580688, 209.57773971557617, 210.61895513534546, 212.25915622711182, 213.89935731887817]
[10.616666666666667, 10.616666666666667, 12.008333333333333, 12.008333333333333, 12.033333333333333, 12.033333333333333, 12.333333333333334, 12.333333333333334, 11.933333333333334, 11.933333333333334, 13.733333333333333, 13.733333333333333, 13.341666666666667, 13.341666666666667, 16.108333333333334, 16.108333333333334, 18.258333333333333, 18.258333333333333, 23.125, 23.125, 28.166666666666668, 28.166666666666668, 33.21666666666667, 33.21666666666667, 38.80833333333333, 38.80833333333333, 43.13333333333333, 43.13333333333333, 48.44166666666667, 48.44166666666667, 50.733333333333334, 50.733333333333334, 53.43333333333333, 53.43333333333333, 55.44166666666667, 55.44166666666667, 56.45, 56.45, 57.34166666666667, 57.34166666666667, 58.4, 58.4, 59.575, 59.575, 59.766666666666666, 59.766666666666666, 60.791666666666664, 60.791666666666664, 61.833333333333336, 61.833333333333336, 62.483333333333334, 62.483333333333334, 63.09166666666667, 63.09166666666667, 63.38333333333333, 63.38333333333333, 63.291666666666664, 63.291666666666664, 64.00833333333334, 64.00833333333334, 64.26666666666667, 64.26666666666667, 64.34166666666667, 64.34166666666667, 64.5, 64.5, 64.58333333333333, 64.58333333333333, 64.56666666666666, 64.56666666666666, 65.175, 65.175, 65.48333333333333, 65.48333333333333, 65.70833333333333, 65.70833333333333, 65.75833333333334, 65.75833333333334, 65.93333333333334, 65.93333333333334, 66.025, 66.025, 66.09166666666667, 66.09166666666667, 66.125, 66.125, 66.075, 66.075, 66.23333333333333, 66.23333333333333, 66.30833333333334, 66.30833333333334, 66.89166666666667, 66.89166666666667, 67.03333333333333, 67.03333333333333, 67.05, 67.05, 67.00833333333334, 67.00833333333334, 67.175, 67.175, 67.225, 67.225, 67.175, 67.175, 67.16666666666667, 67.16666666666667, 67.19166666666666, 67.19166666666666, 67.46666666666667, 67.46666666666667, 67.65833333333333, 67.65833333333333, 67.74166666666666, 67.74166666666666, 67.775, 67.775, 67.78333333333333, 67.78333333333333, 67.76666666666667, 67.76666666666667, 67.925, 67.925, 67.88333333333334, 67.88333333333334, 67.84166666666667, 67.84166666666667, 67.96666666666667, 67.96666666666667, 67.96666666666667, 67.96666666666667, 68.275, 68.275, 68.36666666666666, 68.36666666666666, 68.275, 68.275, 68.175, 68.175, 68.25, 68.25, 68.20833333333333, 68.20833333333333, 68.34166666666667, 68.34166666666667, 68.51666666666667, 68.51666666666667, 68.70833333333333, 68.70833333333333, 68.8, 68.8, 68.79166666666667, 68.79166666666667, 68.88333333333334, 68.88333333333334, 68.9, 68.9, 68.875, 68.875, 69.3, 69.3, 69.39166666666667, 69.39166666666667, 69.45, 69.45, 69.59166666666667, 69.59166666666667, 69.85833333333333, 69.85833333333333, 69.83333333333333, 69.83333333333333, 69.85, 69.85, 69.85833333333333, 69.85833333333333, 69.925, 69.925, 69.925, 69.925, 69.975, 69.975, 70.0, 70.0, 70.025, 70.025, 70.04166666666667, 70.04166666666667, 70.025, 70.025, 70.11666666666666, 70.11666666666666, 70.09166666666667, 70.09166666666667, 70.08333333333333, 70.08333333333333, 70.15, 70.15, 70.14166666666667, 70.14166666666667, 70.75833333333334, 70.75833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.72
Round   1, Train loss: 2.300, Test loss: 2.301, Test accuracy: 14.75
Round   2, Train loss: 2.296, Test loss: 2.299, Test accuracy: 20.36
Round   3, Train loss: 2.298, Test loss: 2.296, Test accuracy: 23.12
Round   4, Train loss: 2.285, Test loss: 2.290, Test accuracy: 23.07
Round   5, Train loss: 2.270, Test loss: 2.277, Test accuracy: 19.75
Round   6, Train loss: 2.249, Test loss: 2.254, Test accuracy: 20.85
Round   7, Train loss: 2.215, Test loss: 2.211, Test accuracy: 27.61
Round   8, Train loss: 2.109, Test loss: 2.146, Test accuracy: 35.69
Round   9, Train loss: 1.973, Test loss: 2.079, Test accuracy: 45.23
Round  10, Train loss: 1.916, Test loss: 2.006, Test accuracy: 51.13
Round  11, Train loss: 1.842, Test loss: 1.943, Test accuracy: 57.56
Round  12, Train loss: 1.715, Test loss: 1.895, Test accuracy: 61.35
Round  13, Train loss: 1.765, Test loss: 1.832, Test accuracy: 67.48
Round  14, Train loss: 1.656, Test loss: 1.782, Test accuracy: 72.02
Round  15, Train loss: 1.640, Test loss: 1.747, Test accuracy: 75.03
Round  16, Train loss: 1.564, Test loss: 1.731, Test accuracy: 76.21
Round  17, Train loss: 1.573, Test loss: 1.718, Test accuracy: 77.27
Round  18, Train loss: 1.609, Test loss: 1.676, Test accuracy: 80.65
Round  19, Train loss: 1.532, Test loss: 1.668, Test accuracy: 81.35
Round  20, Train loss: 1.535, Test loss: 1.662, Test accuracy: 81.74
Round  21, Train loss: 1.526, Test loss: 1.658, Test accuracy: 81.89
Round  22, Train loss: 1.540, Test loss: 1.655, Test accuracy: 82.17
Round  23, Train loss: 1.512, Test loss: 1.648, Test accuracy: 82.69
Round  24, Train loss: 1.538, Test loss: 1.641, Test accuracy: 83.25
Round  25, Train loss: 1.540, Test loss: 1.633, Test accuracy: 84.32
Round  26, Train loss: 1.525, Test loss: 1.628, Test accuracy: 84.65
Round  27, Train loss: 1.495, Test loss: 1.626, Test accuracy: 84.76
Round  28, Train loss: 1.504, Test loss: 1.623, Test accuracy: 85.03
Round  29, Train loss: 1.489, Test loss: 1.622, Test accuracy: 85.10
Round  30, Train loss: 1.495, Test loss: 1.619, Test accuracy: 85.26
Round  31, Train loss: 1.483, Test loss: 1.618, Test accuracy: 85.34
Round  32, Train loss: 1.492, Test loss: 1.615, Test accuracy: 85.73
Round  33, Train loss: 1.486, Test loss: 1.614, Test accuracy: 85.85
Round  34, Train loss: 1.503, Test loss: 1.613, Test accuracy: 85.97
Round  35, Train loss: 1.486, Test loss: 1.611, Test accuracy: 86.10
Round  36, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.97
Round  37, Train loss: 1.498, Test loss: 1.610, Test accuracy: 86.12
Round  38, Train loss: 1.498, Test loss: 1.610, Test accuracy: 86.16
Round  39, Train loss: 1.486, Test loss: 1.609, Test accuracy: 86.15
Round  40, Train loss: 1.486, Test loss: 1.609, Test accuracy: 86.18
Round  41, Train loss: 1.481, Test loss: 1.609, Test accuracy: 86.21
Round  42, Train loss: 1.484, Test loss: 1.609, Test accuracy: 86.29
Round  43, Train loss: 1.481, Test loss: 1.608, Test accuracy: 86.22
Round  44, Train loss: 1.484, Test loss: 1.608, Test accuracy: 86.25
Round  45, Train loss: 1.482, Test loss: 1.608, Test accuracy: 86.19
Round  46, Train loss: 1.479, Test loss: 1.608, Test accuracy: 86.17
Round  47, Train loss: 1.480, Test loss: 1.608, Test accuracy: 86.20
Round  48, Train loss: 1.498, Test loss: 1.608, Test accuracy: 86.13
Round  49, Train loss: 1.483, Test loss: 1.608, Test accuracy: 86.11
Round  50, Train loss: 1.479, Test loss: 1.607, Test accuracy: 86.17
Round  51, Train loss: 1.483, Test loss: 1.607, Test accuracy: 86.12
Round  52, Train loss: 1.480, Test loss: 1.607, Test accuracy: 86.19
Round  53, Train loss: 1.496, Test loss: 1.607, Test accuracy: 86.17
Round  54, Train loss: 1.498, Test loss: 1.607, Test accuracy: 86.17
Round  55, Train loss: 1.479, Test loss: 1.607, Test accuracy: 86.13
Round  56, Train loss: 1.481, Test loss: 1.607, Test accuracy: 86.14
Round  57, Train loss: 1.482, Test loss: 1.607, Test accuracy: 86.12
Round  58, Train loss: 1.479, Test loss: 1.607, Test accuracy: 86.13
Round  59, Train loss: 1.482, Test loss: 1.607, Test accuracy: 86.06
Round  60, Train loss: 1.480, Test loss: 1.606, Test accuracy: 86.08
Round  61, Train loss: 1.477, Test loss: 1.606, Test accuracy: 86.07
Round  62, Train loss: 1.477, Test loss: 1.606, Test accuracy: 86.11
Round  63, Train loss: 1.480, Test loss: 1.606, Test accuracy: 86.11
Round  64, Train loss: 1.498, Test loss: 1.606, Test accuracy: 86.16
Round  65, Train loss: 1.496, Test loss: 1.606, Test accuracy: 86.17
Round  66, Train loss: 1.474, Test loss: 1.606, Test accuracy: 86.11
Round  67, Train loss: 1.476, Test loss: 1.606, Test accuracy: 86.12
Round  68, Train loss: 1.499, Test loss: 1.606, Test accuracy: 86.05
Round  69, Train loss: 1.480, Test loss: 1.606, Test accuracy: 86.09
Round  70, Train loss: 1.481, Test loss: 1.606, Test accuracy: 86.07
Round  71, Train loss: 1.475, Test loss: 1.606, Test accuracy: 86.09
Round  72, Train loss: 1.476, Test loss: 1.606, Test accuracy: 86.12
Round  73, Train loss: 1.479, Test loss: 1.606, Test accuracy: 86.15
Round  74, Train loss: 1.474, Test loss: 1.606, Test accuracy: 86.14
Round  75, Train loss: 1.482, Test loss: 1.605, Test accuracy: 86.17
Round  76, Train loss: 1.499, Test loss: 1.605, Test accuracy: 86.20
Round  77, Train loss: 1.475, Test loss: 1.605, Test accuracy: 86.18
Round  78, Train loss: 1.497, Test loss: 1.605, Test accuracy: 86.18
Round  79, Train loss: 1.495, Test loss: 1.605, Test accuracy: 86.17
Round  80, Train loss: 1.481, Test loss: 1.605, Test accuracy: 86.15
Round  81, Train loss: 1.478, Test loss: 1.605, Test accuracy: 86.22
Round  82, Train loss: 1.475, Test loss: 1.605, Test accuracy: 86.22
Round  83, Train loss: 1.499, Test loss: 1.605, Test accuracy: 86.22
Round  84, Train loss: 1.497, Test loss: 1.605, Test accuracy: 86.22
Round  85, Train loss: 1.483, Test loss: 1.605, Test accuracy: 86.27
Round  86, Train loss: 1.490, Test loss: 1.605, Test accuracy: 86.25
Round  87, Train loss: 1.493, Test loss: 1.605, Test accuracy: 86.28
Round  88, Train loss: 1.479, Test loss: 1.605, Test accuracy: 86.28
Round  89, Train loss: 1.495, Test loss: 1.605, Test accuracy: 86.30
Round  90, Train loss: 1.473, Test loss: 1.605, Test accuracy: 86.30
Round  91, Train loss: 1.476, Test loss: 1.605, Test accuracy: 86.27
Round  92, Train loss: 1.479, Test loss: 1.605, Test accuracy: 86.27
Round  93, Train loss: 1.478, Test loss: 1.605, Test accuracy: 86.25
Round  94, Train loss: 1.475, Test loss: 1.605, Test accuracy: 86.31
Round  95, Train loss: 1.495, Test loss: 1.604, Test accuracy: 86.31
Round  96, Train loss: 1.475, Test loss: 1.604, Test accuracy: 86.31
Round  97, Train loss: 1.476, Test loss: 1.604, Test accuracy: 86.28
Round  98, Train loss: 1.491, Test loss: 1.604, Test accuracy: 86.29
Round  99, Train loss: 1.480, Test loss: 1.604, Test accuracy: 86.29/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.483, Test loss: 1.604, Test accuracy: 86.27
Average accuracy final 10 rounds: 86.2875 

1519.20134806633
[1.228949785232544, 2.457899570465088, 3.6858813762664795, 4.913863182067871, 6.1386024951934814, 7.363341808319092, 8.577703952789307, 9.792066097259521, 10.982769250869751, 12.17347240447998, 13.38670301437378, 14.599933624267578, 15.839403867721558, 17.078874111175537, 18.346723079681396, 19.614572048187256, 20.827849864959717, 22.041127681732178, 23.23894953727722, 24.436771392822266, 25.700887203216553, 26.96500301361084, 28.208917140960693, 29.452831268310547, 30.670962810516357, 31.889094352722168, 33.1030387878418, 34.316983222961426, 35.588298082351685, 36.85961294174194, 38.08071732521057, 39.3018217086792, 40.4174587726593, 41.533095836639404, 42.66562366485596, 43.79815149307251, 45.01783895492554, 46.237526416778564, 47.42383408546448, 48.61014175415039, 49.83614540100098, 51.06214904785156, 52.32585835456848, 53.5895676612854, 54.84317684173584, 56.09678602218628, 57.31841206550598, 58.540038108825684, 59.752925872802734, 60.965813636779785, 62.18910264968872, 63.412391662597656, 64.67216897010803, 65.93194627761841, 67.14805436134338, 68.36416244506836, 69.53237342834473, 70.7005844116211, 71.88545203208923, 73.07031965255737, 74.27493238449097, 75.47954511642456, 76.67761540412903, 77.8756856918335, 79.06944704055786, 80.26320838928223, 81.48451972007751, 82.7058310508728, 83.98406600952148, 85.26230096817017, 86.49910187721252, 87.73590278625488, 88.94040179252625, 90.14490079879761, 91.36187028884888, 92.57883977890015, 93.82311081886292, 95.06738185882568, 96.32632946968079, 97.58527708053589, 98.79288506507874, 100.00049304962158, 101.25661134719849, 102.51272964477539, 103.78309774398804, 105.05346584320068, 106.35785961151123, 107.66225337982178, 108.86090755462646, 110.05956172943115, 111.241872549057, 112.42418336868286, 113.64359068870544, 114.86299800872803, 116.09699606895447, 117.33099412918091, 118.55250263214111, 119.77401113510132, 121.01903438568115, 122.26405763626099, 123.53332233428955, 124.80258703231812, 126.04976320266724, 127.29693937301636, 128.5220239162445, 129.74710845947266, 130.96543407440186, 132.18375968933105, 133.3896713256836, 134.59558296203613, 135.87615275382996, 137.15672254562378, 138.41773438453674, 139.6787462234497, 140.9012405872345, 142.1237349510193, 143.33013582229614, 144.536536693573, 145.7555000782013, 146.9744634628296, 148.33191871643066, 149.68937397003174, 151.01310443878174, 152.33683490753174, 153.64590501785278, 154.95497512817383, 156.16961121559143, 157.38424730300903, 158.63375854492188, 159.88326978683472, 161.13818788528442, 162.39310598373413, 163.58611416816711, 164.7791223526001, 165.9704031944275, 167.16168403625488, 168.38344955444336, 169.60521507263184, 170.83307194709778, 172.06092882156372, 173.28875160217285, 174.51657438278198, 175.72827577590942, 176.93997716903687, 178.16180443763733, 179.3836317062378, 180.6149172782898, 181.8462028503418, 183.0815098285675, 184.3168168067932, 185.51855969429016, 186.7203025817871, 187.9799201488495, 189.23953771591187, 190.63514375686646, 192.03074979782104, 193.36900353431702, 194.707257270813, 196.07885646820068, 197.45045566558838, 198.7171437740326, 199.9838318824768, 201.2842674255371, 202.5847029685974, 203.87075567245483, 205.15680837631226, 206.33253645896912, 207.50826454162598, 208.65892481803894, 209.8095850944519, 210.83885312080383, 211.86812114715576, 212.9787769317627, 214.08943271636963, 215.12330746650696, 216.1571822166443, 217.29195380210876, 218.42672538757324, 219.57131910324097, 220.7159128189087, 221.92386078834534, 223.13180875778198, 224.32290768623352, 225.51400661468506, 226.7128038406372, 227.91160106658936, 229.06770396232605, 230.22380685806274, 231.3664402961731, 232.50907373428345, 233.67693519592285, 234.84479665756226, 236.00303769111633, 237.1612787246704, 238.3420135974884, 239.5227484703064, 240.69009590148926, 241.85744333267212, 243.00533175468445, 244.15322017669678, 246.00859832763672, 247.86397647857666]
[10.725, 10.725, 14.75, 14.75, 20.358333333333334, 20.358333333333334, 23.125, 23.125, 23.075, 23.075, 19.75, 19.75, 20.85, 20.85, 27.608333333333334, 27.608333333333334, 35.69166666666667, 35.69166666666667, 45.233333333333334, 45.233333333333334, 51.13333333333333, 51.13333333333333, 57.55833333333333, 57.55833333333333, 61.35, 61.35, 67.48333333333333, 67.48333333333333, 72.01666666666667, 72.01666666666667, 75.03333333333333, 75.03333333333333, 76.20833333333333, 76.20833333333333, 77.26666666666667, 77.26666666666667, 80.65, 80.65, 81.35, 81.35, 81.74166666666666, 81.74166666666666, 81.89166666666667, 81.89166666666667, 82.175, 82.175, 82.69166666666666, 82.69166666666666, 83.25, 83.25, 84.31666666666666, 84.31666666666666, 84.65, 84.65, 84.75833333333334, 84.75833333333334, 85.03333333333333, 85.03333333333333, 85.1, 85.1, 85.25833333333334, 85.25833333333334, 85.34166666666667, 85.34166666666667, 85.73333333333333, 85.73333333333333, 85.85, 85.85, 85.975, 85.975, 86.1, 86.1, 85.975, 85.975, 86.11666666666666, 86.11666666666666, 86.15833333333333, 86.15833333333333, 86.15, 86.15, 86.18333333333334, 86.18333333333334, 86.20833333333333, 86.20833333333333, 86.29166666666667, 86.29166666666667, 86.21666666666667, 86.21666666666667, 86.25, 86.25, 86.19166666666666, 86.19166666666666, 86.16666666666667, 86.16666666666667, 86.2, 86.2, 86.13333333333334, 86.13333333333334, 86.10833333333333, 86.10833333333333, 86.16666666666667, 86.16666666666667, 86.125, 86.125, 86.19166666666666, 86.19166666666666, 86.16666666666667, 86.16666666666667, 86.16666666666667, 86.16666666666667, 86.13333333333334, 86.13333333333334, 86.14166666666667, 86.14166666666667, 86.125, 86.125, 86.13333333333334, 86.13333333333334, 86.05833333333334, 86.05833333333334, 86.08333333333333, 86.08333333333333, 86.06666666666666, 86.06666666666666, 86.10833333333333, 86.10833333333333, 86.10833333333333, 86.10833333333333, 86.15833333333333, 86.15833333333333, 86.16666666666667, 86.16666666666667, 86.10833333333333, 86.10833333333333, 86.11666666666666, 86.11666666666666, 86.05, 86.05, 86.09166666666667, 86.09166666666667, 86.06666666666666, 86.06666666666666, 86.09166666666667, 86.09166666666667, 86.125, 86.125, 86.15, 86.15, 86.14166666666667, 86.14166666666667, 86.175, 86.175, 86.2, 86.2, 86.18333333333334, 86.18333333333334, 86.18333333333334, 86.18333333333334, 86.16666666666667, 86.16666666666667, 86.15, 86.15, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.26666666666667, 86.26666666666667, 86.25, 86.25, 86.28333333333333, 86.28333333333333, 86.275, 86.275, 86.3, 86.3, 86.3, 86.3, 86.26666666666667, 86.26666666666667, 86.26666666666667, 86.26666666666667, 86.25, 86.25, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.28333333333333, 86.28333333333333, 86.29166666666667, 86.29166666666667, 86.29166666666667, 86.29166666666667, 86.26666666666667, 86.26666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.703, Test loss: 2.299, Test accuracy: 16.58
Round   1, Train loss: 1.668, Test loss: 2.290, Test accuracy: 25.09
Round   2, Train loss: 1.547, Test loss: 2.270, Test accuracy: 31.14
Round   3, Train loss: 1.483, Test loss: 2.236, Test accuracy: 39.77
Round   4, Train loss: 1.506, Test loss: 2.199, Test accuracy: 45.83
Round   5, Train loss: 1.445, Test loss: 2.153, Test accuracy: 49.41
Round   6, Train loss: 1.377, Test loss: 2.118, Test accuracy: 50.36
Round   7, Train loss: 1.413, Test loss: 2.094, Test accuracy: 51.16
Round   8, Train loss: 1.337, Test loss: 2.076, Test accuracy: 50.98
Round   9, Train loss: 1.370, Test loss: 2.058, Test accuracy: 51.63
Round  10, Train loss: 1.394, Test loss: 2.046, Test accuracy: 52.41
Round  11, Train loss: 1.366, Test loss: 2.037, Test accuracy: 52.89
Round  12, Train loss: 1.378, Test loss: 2.037, Test accuracy: 51.93
Round  13, Train loss: 1.329, Test loss: 2.026, Test accuracy: 52.29
Round  14, Train loss: 1.313, Test loss: 2.020, Test accuracy: 52.97
Round  15, Train loss: 1.437, Test loss: 2.009, Test accuracy: 53.19
Round  16, Train loss: 1.303, Test loss: 2.009, Test accuracy: 52.55
Round  17, Train loss: 1.312, Test loss: 2.002, Test accuracy: 52.87
Round  18, Train loss: 1.320, Test loss: 1.990, Test accuracy: 53.74
Round  19, Train loss: 1.302, Test loss: 1.984, Test accuracy: 54.42
Round  20, Train loss: 1.312, Test loss: 1.980, Test accuracy: 54.27
Round  21, Train loss: 1.363, Test loss: 1.970, Test accuracy: 55.86
Round  22, Train loss: 1.333, Test loss: 1.965, Test accuracy: 56.39
Round  23, Train loss: 1.296, Test loss: 1.958, Test accuracy: 56.50
Round  24, Train loss: 1.287, Test loss: 1.958, Test accuracy: 56.46
Round  25, Train loss: 1.327, Test loss: 1.958, Test accuracy: 55.38
Round  26, Train loss: 1.347, Test loss: 1.958, Test accuracy: 55.34
Round  27, Train loss: 1.338, Test loss: 1.958, Test accuracy: 55.02
Round  28, Train loss: 1.300, Test loss: 1.957, Test accuracy: 55.06
Round  29, Train loss: 1.281, Test loss: 1.959, Test accuracy: 54.58
Round  30, Train loss: 1.284, Test loss: 1.955, Test accuracy: 54.93
Round  31, Train loss: 1.352, Test loss: 1.950, Test accuracy: 55.86
Round  32, Train loss: 1.312, Test loss: 1.951, Test accuracy: 55.29
Round  33, Train loss: 1.288, Test loss: 1.951, Test accuracy: 55.23
Round  34, Train loss: 1.316, Test loss: 1.946, Test accuracy: 55.73
Round  35, Train loss: 1.335, Test loss: 1.946, Test accuracy: 55.58
Round  36, Train loss: 1.337, Test loss: 1.948, Test accuracy: 55.08
Round  37, Train loss: 1.272, Test loss: 1.947, Test accuracy: 55.07
Round  38, Train loss: 1.331, Test loss: 1.947, Test accuracy: 54.92
Round  39, Train loss: 1.302, Test loss: 1.943, Test accuracy: 55.61
Round  40, Train loss: 1.299, Test loss: 1.942, Test accuracy: 55.48
Round  41, Train loss: 1.352, Test loss: 1.940, Test accuracy: 55.59
Round  42, Train loss: 1.229, Test loss: 1.937, Test accuracy: 55.80
Round  43, Train loss: 1.292, Test loss: 1.937, Test accuracy: 55.99
Round  44, Train loss: 1.275, Test loss: 1.935, Test accuracy: 55.78
Round  45, Train loss: 1.280, Test loss: 1.936, Test accuracy: 55.50
Round  46, Train loss: 1.329, Test loss: 1.936, Test accuracy: 55.38
Round  47, Train loss: 1.304, Test loss: 1.939, Test accuracy: 54.95
Round  48, Train loss: 1.332, Test loss: 1.941, Test accuracy: 54.63
Round  49, Train loss: 1.360, Test loss: 1.944, Test accuracy: 54.42
Round  50, Train loss: 1.304, Test loss: 1.941, Test accuracy: 54.61
Round  51, Train loss: 1.318, Test loss: 1.940, Test accuracy: 54.64
Round  52, Train loss: 1.287, Test loss: 1.940, Test accuracy: 54.62
Round  53, Train loss: 1.277, Test loss: 1.941, Test accuracy: 54.36
Round  54, Train loss: 1.300, Test loss: 1.942, Test accuracy: 54.26
Round  55, Train loss: 1.370, Test loss: 1.944, Test accuracy: 54.13
Round  56, Train loss: 1.278, Test loss: 1.947, Test accuracy: 53.72
Round  57, Train loss: 1.334, Test loss: 1.947, Test accuracy: 53.63
Round  58, Train loss: 1.318, Test loss: 1.949, Test accuracy: 53.23
Round  59, Train loss: 1.298, Test loss: 1.946, Test accuracy: 53.52
Round  60, Train loss: 1.240, Test loss: 1.946, Test accuracy: 53.55
Round  61, Train loss: 1.330, Test loss: 1.949, Test accuracy: 53.27
Round  62, Train loss: 1.312, Test loss: 1.952, Test accuracy: 52.73
Round  63, Train loss: 1.308, Test loss: 1.953, Test accuracy: 52.58
Round  64, Train loss: 1.276, Test loss: 1.955, Test accuracy: 52.11
Round  65, Train loss: 1.318, Test loss: 1.956, Test accuracy: 51.98
Round  66, Train loss: 1.297, Test loss: 1.957, Test accuracy: 51.61
Round  67, Train loss: 1.302, Test loss: 1.958, Test accuracy: 51.60
Round  68, Train loss: 1.255, Test loss: 1.958, Test accuracy: 51.42
Round  69, Train loss: 1.270, Test loss: 1.959, Test accuracy: 51.33
Round  70, Train loss: 1.313, Test loss: 1.960, Test accuracy: 51.18
Round  71, Train loss: 1.273, Test loss: 1.960, Test accuracy: 51.25
Round  72, Train loss: 1.256, Test loss: 1.961, Test accuracy: 50.90
Round  73, Train loss: 1.285, Test loss: 1.963, Test accuracy: 50.64
Round  74, Train loss: 1.279, Test loss: 1.963, Test accuracy: 50.66
Round  75, Train loss: 1.307, Test loss: 1.964, Test accuracy: 50.51
Round  76, Train loss: 1.278, Test loss: 1.964, Test accuracy: 50.25
Round  77, Train loss: 1.277, Test loss: 1.968, Test accuracy: 49.90
Round  78, Train loss: 1.297, Test loss: 1.969, Test accuracy: 49.75
Round  79, Train loss: 1.296, Test loss: 1.972, Test accuracy: 49.33
Round  80, Train loss: 1.265, Test loss: 1.968, Test accuracy: 49.74
Round  81, Train loss: 1.261, Test loss: 1.968, Test accuracy: 49.83
Round  82, Train loss: 1.321, Test loss: 1.967, Test accuracy: 49.82
Round  83, Train loss: 1.297, Test loss: 1.967, Test accuracy: 49.93
Round  84, Train loss: 1.253, Test loss: 1.966, Test accuracy: 50.02
Round  85, Train loss: 1.238, Test loss: 1.972, Test accuracy: 49.30
Round  86, Train loss: 1.254, Test loss: 1.970, Test accuracy: 49.45
Round  87, Train loss: 1.346, Test loss: 1.972, Test accuracy: 49.26
Round  88, Train loss: 1.271, Test loss: 1.972, Test accuracy: 49.31
Round  89, Train loss: 1.252, Test loss: 1.972, Test accuracy: 49.16
Round  90, Train loss: 1.237, Test loss: 1.971, Test accuracy: 49.25
Round  91, Train loss: 1.311, Test loss: 1.974, Test accuracy: 48.75
Round  92, Train loss: 1.275, Test loss: 1.975, Test accuracy: 48.57
Round  93, Train loss: 1.297, Test loss: 1.978, Test accuracy: 48.20
Round  94, Train loss: 1.275, Test loss: 1.977, Test accuracy: 48.53
Round  95, Train loss: 1.274, Test loss: 1.977, Test accuracy: 48.42
Round  96, Train loss: 1.344, Test loss: 1.978, Test accuracy: 48.17
Round  97, Train loss: 1.353, Test loss: 1.981, Test accuracy: 47.83
Round  98, Train loss: 1.296, Test loss: 1.981, Test accuracy: 47.85
Round  99, Train loss: 1.305, Test loss: 1.983, Test accuracy: 47.55
Final Round, Train loss: 1.277, Test loss: 1.985, Test accuracy: 47.46
Average accuracy final 10 rounds: 48.31083333333333
1727.8235869407654
[]
[16.583333333333332, 25.091666666666665, 31.141666666666666, 39.775, 45.825, 49.40833333333333, 50.358333333333334, 51.15833333333333, 50.983333333333334, 51.63333333333333, 52.40833333333333, 52.891666666666666, 51.93333333333333, 52.291666666666664, 52.96666666666667, 53.19166666666667, 52.55, 52.86666666666667, 53.74166666666667, 54.425, 54.275, 55.858333333333334, 56.391666666666666, 56.5, 56.458333333333336, 55.375, 55.34166666666667, 55.025, 55.05833333333333, 54.583333333333336, 54.93333333333333, 55.858333333333334, 55.291666666666664, 55.233333333333334, 55.733333333333334, 55.583333333333336, 55.075, 55.06666666666667, 54.916666666666664, 55.608333333333334, 55.475, 55.59166666666667, 55.8, 55.99166666666667, 55.78333333333333, 55.5, 55.375, 54.95, 54.63333333333333, 54.425, 54.608333333333334, 54.641666666666666, 54.625, 54.358333333333334, 54.25833333333333, 54.13333333333333, 53.71666666666667, 53.63333333333333, 53.233333333333334, 53.525, 53.55, 53.275, 52.725, 52.575, 52.108333333333334, 51.975, 51.608333333333334, 51.6, 51.416666666666664, 51.325, 51.18333333333333, 51.25, 50.9, 50.641666666666666, 50.65833333333333, 50.50833333333333, 50.25, 49.9, 49.75, 49.325, 49.74166666666667, 49.833333333333336, 49.81666666666667, 49.93333333333333, 50.016666666666666, 49.3, 49.45, 49.25833333333333, 49.30833333333333, 49.15833333333333, 49.25, 48.75, 48.56666666666667, 48.2, 48.53333333333333, 48.416666666666664, 48.166666666666664, 47.825, 47.85, 47.55, 47.458333333333336]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.45
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 14.29
Round   2, Train loss: 2.289, Test loss: 2.302, Test accuracy: 12.72
Round   3, Train loss: 2.290, Test loss: 2.302, Test accuracy: 13.22
Round   4, Train loss: 2.294, Test loss: 2.301, Test accuracy: 14.57
Round   5, Train loss: 2.306, Test loss: 2.301, Test accuracy: 16.90
Round   6, Train loss: 2.311, Test loss: 2.299, Test accuracy: 19.22
Round   7, Train loss: 2.298, Test loss: 2.300, Test accuracy: 19.46
Round   8, Train loss: 2.294, Test loss: 2.300, Test accuracy: 17.65
Round   9, Train loss: 2.291, Test loss: 2.299, Test accuracy: 17.59
Round  10, Train loss: 2.303, Test loss: 2.299, Test accuracy: 17.13
Round  11, Train loss: 2.255, Test loss: 2.296, Test accuracy: 15.96
Round  12, Train loss: 2.281, Test loss: 2.293, Test accuracy: 19.19
Round  13, Train loss: 2.255, Test loss: 2.289, Test accuracy: 17.93
Round  14, Train loss: 2.278, Test loss: 2.294, Test accuracy: 13.62
Round  15, Train loss: 2.253, Test loss: 2.285, Test accuracy: 15.69
Round  16, Train loss: 2.241, Test loss: 2.283, Test accuracy: 15.86
Round  17, Train loss: 2.223, Test loss: 2.275, Test accuracy: 19.22
Round  18, Train loss: 2.244, Test loss: 2.273, Test accuracy: 18.98
Round  19, Train loss: 2.157, Test loss: 2.260, Test accuracy: 20.23
Round  20, Train loss: 2.264, Test loss: 2.268, Test accuracy: 17.54
Round  21, Train loss: 2.179, Test loss: 2.264, Test accuracy: 19.19
Round  22, Train loss: 2.141, Test loss: 2.254, Test accuracy: 20.13
Round  23, Train loss: 2.016, Test loss: 2.230, Test accuracy: 23.50
Round  24, Train loss: 1.989, Test loss: 2.208, Test accuracy: 26.27
Round  25, Train loss: 2.119, Test loss: 2.225, Test accuracy: 24.73
Round  26, Train loss: 1.843, Test loss: 2.177, Test accuracy: 30.41
Round  27, Train loss: 1.961, Test loss: 2.196, Test accuracy: 28.66
Round  28, Train loss: 1.829, Test loss: 2.168, Test accuracy: 30.54
Round  29, Train loss: 1.776, Test loss: 2.162, Test accuracy: 31.81
Round  30, Train loss: 1.739, Test loss: 2.145, Test accuracy: 33.94
Round  31, Train loss: 1.914, Test loss: 2.164, Test accuracy: 29.74
Round  32, Train loss: 1.789, Test loss: 2.164, Test accuracy: 30.24
Round  33, Train loss: 1.248, Test loss: 2.122, Test accuracy: 34.65
Round  34, Train loss: 0.665, Test loss: 2.041, Test accuracy: 43.08
Round  35, Train loss: 1.575, Test loss: 2.120, Test accuracy: 36.76
Round  36, Train loss: 1.680, Test loss: 2.162, Test accuracy: 32.48
Round  37, Train loss: 1.065, Test loss: 2.107, Test accuracy: 36.17
Round  38, Train loss: 0.877, Test loss: 2.062, Test accuracy: 40.62
Round  39, Train loss: 1.130, Test loss: 2.091, Test accuracy: 37.57
Round  40, Train loss: 0.763, Test loss: 2.099, Test accuracy: 37.22
Round  41, Train loss: 0.647, Test loss: 2.049, Test accuracy: 40.84
Round  42, Train loss: 1.165, Test loss: 2.083, Test accuracy: 38.61
Round  43, Train loss: -0.194, Test loss: 2.027, Test accuracy: 45.23
Round  44, Train loss: 0.895, Test loss: 2.054, Test accuracy: 43.12
Round  45, Train loss: 0.520, Test loss: 2.067, Test accuracy: 41.02
Round  46, Train loss: 0.030, Test loss: 1.969, Test accuracy: 49.43
Round  47, Train loss: 1.561, Test loss: 2.083, Test accuracy: 39.05
Round  48, Train loss: 0.749, Test loss: 2.077, Test accuracy: 39.51
Round  49, Train loss: 0.082, Test loss: 2.022, Test accuracy: 44.52
Round  50, Train loss: -0.299, Test loss: 1.965, Test accuracy: 50.42
Round  51, Train loss: -0.682, Test loss: 1.934, Test accuracy: 54.13
Round  52, Train loss: 1.181, Test loss: 2.016, Test accuracy: 46.72
Round  53, Train loss: -0.403, Test loss: 1.958, Test accuracy: 51.88
Round  54, Train loss: -0.696, Test loss: 1.927, Test accuracy: 54.51
Round  55, Train loss: -0.461, Test loss: 1.943, Test accuracy: 53.34
Round  56, Train loss: 0.555, Test loss: 2.022, Test accuracy: 45.77
Round  57, Train loss: -0.259, Test loss: 1.984, Test accuracy: 48.95
Round  58, Train loss: -0.527, Test loss: 1.953, Test accuracy: 51.52
Round  59, Train loss: -0.360, Test loss: 1.928, Test accuracy: 54.44
Round  60, Train loss: -0.101, Test loss: 1.937, Test accuracy: 53.67
Round  61, Train loss: -0.695, Test loss: 1.936, Test accuracy: 54.23
Round  62, Train loss: -0.639, Test loss: 1.921, Test accuracy: 55.52
Round  63, Train loss: -0.720, Test loss: 1.906, Test accuracy: 57.22
Round  64, Train loss: -0.788, Test loss: 1.914, Test accuracy: 55.92
Round  65, Train loss: -0.467, Test loss: 1.908, Test accuracy: 56.72
Round  66, Train loss: -1.409, Test loss: 1.897, Test accuracy: 57.53
Round  67, Train loss: -1.117, Test loss: 1.895, Test accuracy: 57.71
Round  68, Train loss: -1.760, Test loss: 1.887, Test accuracy: 58.43
Round  69, Train loss: -0.966, Test loss: 1.884, Test accuracy: 58.57
Round  70, Train loss: -1.860, Test loss: 1.859, Test accuracy: 61.01
Round  71, Train loss: -1.200, Test loss: 1.834, Test accuracy: 63.05
Round  72, Train loss: -0.301, Test loss: 1.878, Test accuracy: 59.17
Round  73, Train loss: -2.127, Test loss: 1.825, Test accuracy: 63.92
Round  74, Train loss: -1.362, Test loss: 1.830, Test accuracy: 63.43
Round  75, Train loss: -1.199, Test loss: 1.838, Test accuracy: 62.68
Round  76, Train loss: -3.078, Test loss: 1.820, Test accuracy: 64.11
Round  77, Train loss: -1.528, Test loss: 1.824, Test accuracy: 63.87
Round  78, Train loss: -1.307, Test loss: 1.810, Test accuracy: 65.35
Round  79, Train loss: -1.819, Test loss: 1.823, Test accuracy: 63.89
Round  80, Train loss: -2.469, Test loss: 1.820, Test accuracy: 63.99
Round  81, Train loss: -2.723, Test loss: 1.825, Test accuracy: 63.46
Round  82, Train loss: -1.940, Test loss: 1.818, Test accuracy: 64.19
Round  83, Train loss: -1.968, Test loss: 1.821, Test accuracy: 63.90
Round  84, Train loss: -2.486, Test loss: 1.798, Test accuracy: 66.22
Round  85, Train loss: -2.585, Test loss: 1.810, Test accuracy: 65.08
Round  86, Train loss: -1.087, Test loss: 1.821, Test accuracy: 64.35
Round  87, Train loss: -1.490, Test loss: 1.836, Test accuracy: 62.86
Round  88, Train loss: -1.902, Test loss: 1.816, Test accuracy: 64.77
Round  89, Train loss: -2.133, Test loss: 1.808, Test accuracy: 65.28
Round  90, Train loss: -1.837, Test loss: 1.794, Test accuracy: 66.87
Round  91, Train loss: -2.347, Test loss: 1.775, Test accuracy: 68.72
Round  92, Train loss: -2.339, Test loss: 1.802, Test accuracy: 66.07
Round  93, Train loss: -1.760, Test loss: 1.812, Test accuracy: 64.95
Round  94, Train loss: -1.679, Test loss: 1.806, Test accuracy: 65.59
Round  95, Train loss: -2.802, Test loss: 1.797, Test accuracy: 66.40
Round  96, Train loss: -2.631, Test loss: 1.798, Test accuracy: 66.20
Round  97, Train loss: -3.066, Test loss: 1.801, Test accuracy: 65.87
Round  98, Train loss: -3.015, Test loss: 1.810, Test accuracy: 64.94
Round  99, Train loss: -1.744, Test loss: 1.813, Test accuracy: 64.62
Final Round, Train loss: 1.926, Test loss: 1.872, Test accuracy: 60.41
Average accuracy final 10 rounds: 66.0225
Average global accuracy final 10 rounds: 66.0225
1272.344036102295
[]
[12.45, 14.291666666666666, 12.725, 13.216666666666667, 14.566666666666666, 16.9, 19.216666666666665, 19.458333333333332, 17.65, 17.591666666666665, 17.133333333333333, 15.958333333333334, 19.191666666666666, 17.925, 13.625, 15.691666666666666, 15.858333333333333, 19.216666666666665, 18.983333333333334, 20.233333333333334, 17.541666666666668, 19.191666666666666, 20.133333333333333, 23.5, 26.266666666666666, 24.733333333333334, 30.408333333333335, 28.658333333333335, 30.541666666666668, 31.808333333333334, 33.94166666666667, 29.741666666666667, 30.241666666666667, 34.65, 43.075, 36.75833333333333, 32.475, 36.175, 40.61666666666667, 37.56666666666667, 37.21666666666667, 40.84166666666667, 38.608333333333334, 45.233333333333334, 43.125, 41.025, 49.43333333333333, 39.05, 39.50833333333333, 44.525, 50.425, 54.13333333333333, 46.71666666666667, 51.88333333333333, 54.50833333333333, 53.34166666666667, 45.775, 48.95, 51.516666666666666, 54.44166666666667, 53.666666666666664, 54.233333333333334, 55.525, 57.21666666666667, 55.925, 56.71666666666667, 57.53333333333333, 57.708333333333336, 58.43333333333333, 58.56666666666667, 61.00833333333333, 63.05, 59.166666666666664, 63.916666666666664, 63.43333333333333, 62.68333333333333, 64.10833333333333, 63.86666666666667, 65.35, 63.891666666666666, 63.99166666666667, 63.458333333333336, 64.19166666666666, 63.9, 66.225, 65.075, 64.35, 62.858333333333334, 64.76666666666667, 65.28333333333333, 66.86666666666666, 68.71666666666667, 66.06666666666666, 64.95, 65.59166666666667, 66.4, 66.2, 65.86666666666666, 64.94166666666666, 64.625, 60.40833333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.59
Round   0, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round   3, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round   3, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.58
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  10, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  10, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.61
Round  11, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  11, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  13, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  14, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  14, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  16, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  16, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  19, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  20, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  21, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  21, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  22, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  22, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  23, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  23, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.64
Round  24, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  24, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  25, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  25, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.64
Round  26, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  26, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  27, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  27, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  28, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  28, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  29, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  29, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  30, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  30, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  31, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  31, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  32, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  32, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  33, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  33, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  34, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  34, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  35, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  35, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  36, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  36, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  37, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  37, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  38, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.69
Round  38, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  39, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.69
Round  39, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  40, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.69
Round  40, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  41, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  41, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  42, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  42, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  43, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  43, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  44, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  44, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  45, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  45, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.69
Round  46, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  46, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  47, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  47, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  48, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  48, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.69
Round  49, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  49, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  50, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  50, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  51, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  51, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  52, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  52, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  53, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  53, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  54, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  54, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  55, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  55, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.66
Round  56, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  56, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  57, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  57, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  58, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  58, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  59, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  59, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  60, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  60, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  61, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  61, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  62, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  62, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  63, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  63, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.68
Round  64, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  64, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  65, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  65, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.66
Round  66, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  66, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  67, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  67, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  68, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  68, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.68
Round  69, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  69, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.69
Round  70, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  70, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  71, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  71, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  72, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  72, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  73, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  73, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  74, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  74, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  75, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  75, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  76, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  76, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  77, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  77, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.68
Round  78, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  78, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  79, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  79, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Round  80, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  80, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  81, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  81, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  82, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  82, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  83, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  83, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  84, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  84, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  85, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  85, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.57
Round  86, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  86, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.56
Round  87, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  87, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  88, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  88, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.58
Round  89, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  89, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  90, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  90, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  91, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  91, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.56
Round  92, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  92, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  93, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  93, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Round  94, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  94, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  95, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  95, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.61
Round  96, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.70
Round  96, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  97, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  98, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  98, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.61
Round  99, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  99, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Final Round, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Final Round, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Average accuracy final 10 rounds: 10.665249999999999 

Average global accuracy final 10 rounds: 10.597 

4541.732900381088
[4.233440399169922, 8.628674745559692, 12.872212886810303, 16.971180200576782, 21.02176523208618, 25.344435691833496, 29.564435720443726, 33.82705545425415, 37.93192744255066, 42.17180562019348, 46.48314023017883, 50.66766905784607, 54.782055616378784, 59.00771880149841, 63.29819083213806, 67.46133327484131, 71.19016122817993, 74.85294198989868, 78.62465357780457, 82.38174104690552, 86.03063035011292, 89.7167558670044, 93.45078372955322, 97.25076580047607, 100.91772532463074, 104.46229243278503, 108.16703271865845, 111.90305948257446, 115.54484677314758, 119.21344995498657, 122.8233687877655, 126.56663751602173, 130.26028156280518, 133.9266061782837, 137.53187441825867, 141.23988151550293, 144.89210391044617, 148.5216405391693, 152.2807698249817, 155.91435050964355, 159.6288981437683, 163.33736062049866, 166.9344620704651, 170.4853355884552, 174.23309540748596, 177.97084999084473, 181.67170548439026, 185.4084701538086, 189.24585223197937, 192.86841773986816, 196.58643007278442, 200.26547265052795, 203.9519636631012, 207.6613781452179, 211.38427066802979, 214.94473123550415, 218.5338237285614, 222.26783561706543, 225.9862744808197, 229.53127932548523, 233.18420028686523, 236.8303027153015, 240.56882190704346, 244.33360695838928, 248.01930737495422, 251.70212721824646, 255.3956253528595, 259.06131505966187, 262.67195320129395, 266.24862146377563, 270.03430247306824, 273.7917938232422, 277.4953553676605, 281.2104799747467, 284.9575412273407, 288.56370782852173, 292.2417998313904, 296.0700452327728, 299.6733593940735, 303.2353367805481, 306.8538432121277, 310.5597529411316, 313.99596071243286, 317.54749059677124, 321.26671862602234, 324.9285924434662, 328.4833791255951, 332.069721698761, 335.9496033191681, 339.64915561676025, 343.12381863594055, 346.8443057537079, 350.51737785339355, 354.15037989616394, 357.62620520591736, 361.27969884872437, 364.96374559402466, 368.61221742630005, 372.1472487449646, 375.73175859451294, 377.5743238925934]
[10.5875, 10.635, 10.645, 10.64, 10.645, 10.6375, 10.6575, 10.64, 10.6225, 10.625, 10.63, 10.64, 10.61, 10.6075, 10.62, 10.61, 10.6275, 10.63, 10.64, 10.675, 10.6375, 10.6325, 10.615, 10.63, 10.64, 10.64, 10.6325, 10.65, 10.6375, 10.64, 10.655, 10.6525, 10.67, 10.6775, 10.6725, 10.655, 10.6625, 10.68, 10.685, 10.69, 10.69, 10.675, 10.6725, 10.6675, 10.6725, 10.6675, 10.6775, 10.6775, 10.665, 10.6725, 10.6675, 10.6675, 10.6725, 10.6575, 10.65, 10.6575, 10.66, 10.665, 10.675, 10.67, 10.6725, 10.665, 10.66, 10.6725, 10.6725, 10.6675, 10.675, 10.6675, 10.635, 10.67, 10.6775, 10.675, 10.6725, 10.6725, 10.655, 10.6625, 10.65, 10.6575, 10.66, 10.67, 10.6775, 10.625, 10.6425, 10.625, 10.62, 10.64, 10.65, 10.66, 10.6375, 10.6275, 10.6675, 10.675, 10.66, 10.6475, 10.6525, 10.6775, 10.7, 10.6825, 10.655, 10.635, 10.62]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.66
Round   1, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.26
Round   2, Train loss: 2.299, Test loss: 2.299, Test accuracy: 12.08
Round   3, Train loss: 2.290, Test loss: 2.291, Test accuracy: 14.36
Round   4, Train loss: 2.246, Test loss: 2.278, Test accuracy: 16.69
Round   5, Train loss: 2.162, Test loss: 2.281, Test accuracy: 15.98
Round   6, Train loss: 2.130, Test loss: 2.271, Test accuracy: 16.42
Round   7, Train loss: 2.044, Test loss: 2.261, Test accuracy: 18.48
Round   8, Train loss: 2.101, Test loss: 2.265, Test accuracy: 17.67
Round   9, Train loss: 2.098, Test loss: 2.270, Test accuracy: 17.37
Round  10, Train loss: 2.052, Test loss: 2.268, Test accuracy: 17.74
Round  11, Train loss: 2.041, Test loss: 2.271, Test accuracy: 16.83
Round  12, Train loss: 2.003, Test loss: 2.267, Test accuracy: 17.31
Round  13, Train loss: 1.998, Test loss: 2.262, Test accuracy: 17.77
Round  14, Train loss: 1.994, Test loss: 2.267, Test accuracy: 17.74
Round  15, Train loss: 1.996, Test loss: 2.264, Test accuracy: 17.82
Round  16, Train loss: 1.921, Test loss: 2.265, Test accuracy: 18.26
Round  17, Train loss: 1.972, Test loss: 2.265, Test accuracy: 17.92
Round  18, Train loss: 1.980, Test loss: 2.268, Test accuracy: 17.41
Round  19, Train loss: 1.929, Test loss: 2.255, Test accuracy: 19.19
Round  20, Train loss: 1.942, Test loss: 2.260, Test accuracy: 19.15
Round  21, Train loss: 1.933, Test loss: 2.257, Test accuracy: 18.39
Round  22, Train loss: 1.982, Test loss: 2.264, Test accuracy: 17.92
Round  23, Train loss: 1.920, Test loss: 2.263, Test accuracy: 18.17
Round  24, Train loss: 1.913, Test loss: 2.256, Test accuracy: 18.96
Round  25, Train loss: 1.923, Test loss: 2.263, Test accuracy: 18.07
Round  26, Train loss: 1.920, Test loss: 2.268, Test accuracy: 17.35
Round  27, Train loss: 1.920, Test loss: 2.268, Test accuracy: 17.46
Round  28, Train loss: 1.896, Test loss: 2.259, Test accuracy: 18.19
Round  29, Train loss: 1.878, Test loss: 2.258, Test accuracy: 18.69
Round  30, Train loss: 1.877, Test loss: 2.263, Test accuracy: 18.06
Round  31, Train loss: 1.896, Test loss: 2.266, Test accuracy: 17.88
Round  32, Train loss: 1.876, Test loss: 2.251, Test accuracy: 19.49
Round  33, Train loss: 1.880, Test loss: 2.254, Test accuracy: 19.08
Round  34, Train loss: 1.925, Test loss: 2.260, Test accuracy: 18.47
Round  35, Train loss: 1.890, Test loss: 2.261, Test accuracy: 18.16
Round  36, Train loss: 1.900, Test loss: 2.259, Test accuracy: 18.71
Round  37, Train loss: 1.908, Test loss: 2.262, Test accuracy: 18.01
Round  38, Train loss: 1.963, Test loss: 2.259, Test accuracy: 18.03
Round  39, Train loss: 1.876, Test loss: 2.259, Test accuracy: 17.82
Round  40, Train loss: 1.866, Test loss: 2.260, Test accuracy: 18.31
Round  41, Train loss: 1.823, Test loss: 2.263, Test accuracy: 17.97
Round  42, Train loss: 1.821, Test loss: 2.269, Test accuracy: 17.54
Round  43, Train loss: 1.889, Test loss: 2.259, Test accuracy: 18.99
Round  44, Train loss: 1.909, Test loss: 2.254, Test accuracy: 19.00
Round  45, Train loss: 1.806, Test loss: 2.251, Test accuracy: 19.14
Round  46, Train loss: 1.850, Test loss: 2.263, Test accuracy: 18.33
Round  47, Train loss: 1.848, Test loss: 2.257, Test accuracy: 19.21
Round  48, Train loss: 1.897, Test loss: 2.258, Test accuracy: 18.67
Round  49, Train loss: 1.874, Test loss: 2.259, Test accuracy: 18.56
Round  50, Train loss: 1.839, Test loss: 2.260, Test accuracy: 18.33
Round  51, Train loss: 1.865, Test loss: 2.264, Test accuracy: 17.50
Round  52, Train loss: 1.834, Test loss: 2.264, Test accuracy: 17.57
Round  53, Train loss: 1.834, Test loss: 2.267, Test accuracy: 17.91
Round  54, Train loss: 1.834, Test loss: 2.254, Test accuracy: 18.71
Round  55, Train loss: 1.855, Test loss: 2.257, Test accuracy: 18.30
Round  56, Train loss: 1.865, Test loss: 2.270, Test accuracy: 16.74
Round  57, Train loss: 1.833, Test loss: 2.269, Test accuracy: 16.79
Round  58, Train loss: 1.852, Test loss: 2.272, Test accuracy: 16.52
Round  59, Train loss: 1.823, Test loss: 2.260, Test accuracy: 18.24
Round  60, Train loss: 1.871, Test loss: 2.264, Test accuracy: 17.62
Round  61, Train loss: 1.860, Test loss: 2.264, Test accuracy: 17.78
Round  62, Train loss: 1.814, Test loss: 2.275, Test accuracy: 16.51
Round  63, Train loss: 1.818, Test loss: 2.263, Test accuracy: 18.00
Round  64, Train loss: 1.856, Test loss: 2.269, Test accuracy: 16.87
Round  65, Train loss: 1.838, Test loss: 2.261, Test accuracy: 17.70
Round  66, Train loss: 1.859, Test loss: 2.268, Test accuracy: 16.93
Round  67, Train loss: 1.787, Test loss: 2.257, Test accuracy: 18.81
Round  68, Train loss: 1.781, Test loss: 2.270, Test accuracy: 16.84
Round  69, Train loss: 1.789, Test loss: 2.274, Test accuracy: 16.02
Round  70, Train loss: 1.826, Test loss: 2.269, Test accuracy: 17.11
Round  71, Train loss: 1.838, Test loss: 2.272, Test accuracy: 16.82
Round  72, Train loss: 1.801, Test loss: 2.264, Test accuracy: 17.73
Round  73, Train loss: 1.806, Test loss: 2.271, Test accuracy: 17.01
Round  74, Train loss: 1.765, Test loss: 2.264, Test accuracy: 17.89
Round  75, Train loss: 1.813, Test loss: 2.267, Test accuracy: 17.55
Round  76, Train loss: 1.787, Test loss: 2.259, Test accuracy: 18.45
Round  77, Train loss: 1.867, Test loss: 2.272, Test accuracy: 16.80
Round  78, Train loss: 1.798, Test loss: 2.266, Test accuracy: 17.23
Round  79, Train loss: 1.807, Test loss: 2.270, Test accuracy: 17.14
Round  80, Train loss: 1.756, Test loss: 2.277, Test accuracy: 16.31
Round  81, Train loss: 1.810, Test loss: 2.272, Test accuracy: 16.87
Round  82, Train loss: 1.739, Test loss: 2.258, Test accuracy: 18.52
Round  83, Train loss: 1.776, Test loss: 2.269, Test accuracy: 17.41
Round  84, Train loss: 1.796, Test loss: 2.280, Test accuracy: 15.44
Round  85, Train loss: 1.809, Test loss: 2.259, Test accuracy: 18.68
Round  86, Train loss: 1.789, Test loss: 2.281, Test accuracy: 15.28
Round  87, Train loss: 1.768, Test loss: 2.261, Test accuracy: 18.03
Round  88, Train loss: 1.786, Test loss: 2.270, Test accuracy: 17.09
Round  89, Train loss: 1.788, Test loss: 2.266, Test accuracy: 16.98
Round  90, Train loss: 1.795, Test loss: 2.261, Test accuracy: 17.67
Round  91, Train loss: 1.740, Test loss: 2.267, Test accuracy: 17.41
Round  92, Train loss: 1.728, Test loss: 2.257, Test accuracy: 18.73
Round  93, Train loss: 1.741, Test loss: 2.270, Test accuracy: 16.89
Round  94, Train loss: 1.735, Test loss: 2.282, Test accuracy: 15.61
Round  95, Train loss: 1.781, Test loss: 2.281, Test accuracy: 15.32
Round  96, Train loss: 1.732, Test loss: 2.273, Test accuracy: 16.52
Round  97, Train loss: 1.720, Test loss: 2.288, Test accuracy: 14.79
Round  98, Train loss: 1.728, Test loss: 2.286, Test accuracy: 15.17
Round  99, Train loss: 1.725, Test loss: 2.283, Test accuracy: 15.41
Final Round, Train loss: 1.711, Test loss: 2.278, Test accuracy: 15.94
Average accuracy final 10 rounds: 16.351111111111113
2908.96786403656
[4.454253673553467, 8.496688604354858, 12.72618579864502, 17.01198172569275, 21.204593896865845, 25.22544574737549, 29.258583068847656, 33.21639537811279, 37.212037563323975, 41.12995004653931, 45.167765617370605, 49.04067349433899, 53.049028635025024, 56.950806856155396, 60.94308924674988, 64.8909330368042, 68.8615152835846, 72.78733110427856, 76.74017095565796, 80.70743179321289, 84.6599109172821, 88.66151523590088, 92.6311342716217, 96.62000060081482, 100.53038144111633, 104.48222732543945, 108.3799500465393, 112.40574097633362, 116.30369234085083, 120.27775502204895, 124.21446752548218, 128.301367521286, 132.27770805358887, 136.24923276901245, 140.25135850906372, 144.2638189792633, 148.1376371383667, 152.13156843185425, 156.05326557159424, 160.11280584335327, 164.04106426239014, 168.0609769821167, 171.977290391922, 176.06095242500305, 180.0383493900299, 184.07629346847534, 188.0093810558319, 192.06090784072876, 196.03522324562073, 200.00201535224915, 203.9544792175293, 207.93817019462585, 211.8839554786682, 215.8278923034668, 219.8157057762146, 223.80366969108582, 227.79058980941772, 231.7617223262787, 235.6683931350708, 239.63783240318298, 243.5391490459442, 247.48851132392883, 251.4739580154419, 255.48739314079285, 259.43885254859924, 263.47995829582214, 267.51110768318176, 271.43953108787537, 275.4486405849457, 279.347722530365, 283.3081443309784, 287.23785519599915, 291.16975021362305, 295.07959032058716, 298.98710894584656, 302.9223051071167, 306.8779447078705, 310.8519341945648, 314.8463091850281, 318.89371490478516, 322.90503764152527, 326.9030804634094, 330.85570645332336, 334.88677501678467, 338.86546635627747, 342.8467905521393, 346.87114334106445, 350.8158357143402, 354.73274660110474, 358.6884047985077, 362.63134241104126, 366.66346073150635, 370.6761157512665, 374.6265993118286, 378.6267902851105, 382.53706550598145, 386.5937180519104, 390.5758926868439, 394.56249380111694, 398.56261682510376, 400.76931953430176]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[11.655555555555555, 13.255555555555556, 12.077777777777778, 14.355555555555556, 16.694444444444443, 15.977777777777778, 16.42222222222222, 18.483333333333334, 17.67222222222222, 17.372222222222224, 17.744444444444444, 16.833333333333332, 17.305555555555557, 17.772222222222222, 17.738888888888887, 17.822222222222223, 18.261111111111113, 17.916666666666668, 17.405555555555555, 19.194444444444443, 19.15, 18.38888888888889, 17.916666666666668, 18.166666666666668, 18.961111111111112, 18.066666666666666, 17.35, 17.455555555555556, 18.194444444444443, 18.694444444444443, 18.06111111111111, 17.883333333333333, 19.494444444444444, 19.083333333333332, 18.47222222222222, 18.155555555555555, 18.705555555555556, 18.005555555555556, 18.02777777777778, 17.816666666666666, 18.305555555555557, 17.97222222222222, 17.538888888888888, 18.994444444444444, 19.0, 19.144444444444446, 18.32777777777778, 19.211111111111112, 18.666666666666668, 18.555555555555557, 18.333333333333332, 17.5, 17.566666666666666, 17.91111111111111, 18.711111111111112, 18.3, 16.744444444444444, 16.794444444444444, 16.516666666666666, 18.238888888888887, 17.616666666666667, 17.77777777777778, 16.505555555555556, 18.0, 16.866666666666667, 17.7, 16.933333333333334, 18.81111111111111, 16.83888888888889, 16.022222222222222, 17.105555555555554, 16.816666666666666, 17.727777777777778, 17.011111111111113, 17.88888888888889, 17.55, 18.45, 16.8, 17.227777777777778, 17.144444444444446, 16.305555555555557, 16.872222222222224, 18.522222222222222, 17.41111111111111, 15.438888888888888, 18.683333333333334, 15.283333333333333, 18.033333333333335, 17.08888888888889, 16.983333333333334, 17.67222222222222, 17.41111111111111, 18.733333333333334, 16.88888888888889, 15.605555555555556, 15.322222222222223, 16.516666666666666, 14.78888888888889, 15.166666666666666, 15.405555555555555, 15.938888888888888]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.303, Test accuracy: 10.08
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.17
Round   2, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.58
Round   3, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.75
Round   4, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.23
Round   5, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.46
Round   6, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.49
Round   7, Train loss: 2.299, Test loss: 2.300, Test accuracy: 11.78
Round   8, Train loss: 2.296, Test loss: 2.299, Test accuracy: 12.10
Round   9, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.05
Round  10, Train loss: 2.293, Test loss: 2.298, Test accuracy: 12.68
Round  11, Train loss: 2.285, Test loss: 2.294, Test accuracy: 12.68
Round  12, Train loss: 2.296, Test loss: 2.293, Test accuracy: 12.03
Round  13, Train loss: 2.279, Test loss: 2.287, Test accuracy: 13.41
Round  14, Train loss: 2.271, Test loss: 2.280, Test accuracy: 14.72
Round  15, Train loss: 2.264, Test loss: 2.270, Test accuracy: 17.25
Round  16, Train loss: 2.234, Test loss: 2.259, Test accuracy: 18.91
Round  17, Train loss: 2.234, Test loss: 2.249, Test accuracy: 20.32
Round  18, Train loss: 2.196, Test loss: 2.235, Test accuracy: 21.67
Round  19, Train loss: 2.199, Test loss: 2.226, Test accuracy: 23.74
Round  20, Train loss: 2.176, Test loss: 2.206, Test accuracy: 25.77
Round  21, Train loss: 2.172, Test loss: 2.194, Test accuracy: 26.77
Round  22, Train loss: 2.121, Test loss: 2.178, Test accuracy: 28.41
Round  23, Train loss: 2.149, Test loss: 2.159, Test accuracy: 30.20
Round  24, Train loss: 2.090, Test loss: 2.143, Test accuracy: 32.09
Round  25, Train loss: 2.092, Test loss: 2.133, Test accuracy: 34.01
Round  26, Train loss: 2.121, Test loss: 2.109, Test accuracy: 38.61
Round  27, Train loss: 2.065, Test loss: 2.090, Test accuracy: 40.62
Round  28, Train loss: 2.060, Test loss: 2.074, Test accuracy: 42.57
Round  29, Train loss: 2.041, Test loss: 2.054, Test accuracy: 45.52
Round  30, Train loss: 2.026, Test loss: 2.031, Test accuracy: 47.88
Round  31, Train loss: 2.005, Test loss: 2.015, Test accuracy: 49.43
Round  32, Train loss: 1.967, Test loss: 1.988, Test accuracy: 51.96
Round  33, Train loss: 1.956, Test loss: 1.969, Test accuracy: 53.84
Round  34, Train loss: 1.922, Test loss: 1.955, Test accuracy: 54.38
Round  35, Train loss: 1.928, Test loss: 1.944, Test accuracy: 55.62
Round  36, Train loss: 1.924, Test loss: 1.939, Test accuracy: 56.71
Round  37, Train loss: 1.919, Test loss: 1.926, Test accuracy: 57.54
Round  38, Train loss: 1.896, Test loss: 1.924, Test accuracy: 58.17
Round  39, Train loss: 1.881, Test loss: 1.918, Test accuracy: 58.66
Round  40, Train loss: 1.894, Test loss: 1.909, Test accuracy: 60.04
Round  41, Train loss: 1.865, Test loss: 1.897, Test accuracy: 60.90
Round  42, Train loss: 1.918, Test loss: 1.895, Test accuracy: 61.32
Round  43, Train loss: 1.942, Test loss: 1.888, Test accuracy: 62.23
Round  44, Train loss: 1.878, Test loss: 1.881, Test accuracy: 62.92
Round  45, Train loss: 1.887, Test loss: 1.879, Test accuracy: 63.13
Round  46, Train loss: 1.830, Test loss: 1.863, Test accuracy: 63.71
Round  47, Train loss: 1.845, Test loss: 1.860, Test accuracy: 64.31
Round  48, Train loss: 1.862, Test loss: 1.859, Test accuracy: 64.72
Round  49, Train loss: 1.905, Test loss: 1.857, Test accuracy: 65.74
Round  50, Train loss: 1.849, Test loss: 1.859, Test accuracy: 65.80
Round  51, Train loss: 1.850, Test loss: 1.850, Test accuracy: 66.37
Round  52, Train loss: 1.856, Test loss: 1.840, Test accuracy: 67.05
Round  53, Train loss: 1.828, Test loss: 1.841, Test accuracy: 67.12
Round  54, Train loss: 1.820, Test loss: 1.834, Test accuracy: 67.26
Round  55, Train loss: 1.814, Test loss: 1.837, Test accuracy: 67.41
Round  56, Train loss: 1.782, Test loss: 1.828, Test accuracy: 67.23
Round  57, Train loss: 1.821, Test loss: 1.827, Test accuracy: 67.51
Round  58, Train loss: 1.808, Test loss: 1.818, Test accuracy: 68.62
Round  59, Train loss: 1.780, Test loss: 1.812, Test accuracy: 68.69
Round  60, Train loss: 1.803, Test loss: 1.814, Test accuracy: 68.74
Round  61, Train loss: 1.767, Test loss: 1.809, Test accuracy: 68.71
Round  62, Train loss: 1.845, Test loss: 1.812, Test accuracy: 69.07
Round  63, Train loss: 1.781, Test loss: 1.806, Test accuracy: 69.33
Round  64, Train loss: 1.771, Test loss: 1.800, Test accuracy: 70.28
Round  65, Train loss: 1.785, Test loss: 1.797, Test accuracy: 70.24
Round  66, Train loss: 1.766, Test loss: 1.795, Test accuracy: 70.41
Round  67, Train loss: 1.780, Test loss: 1.797, Test accuracy: 70.25
Round  68, Train loss: 1.711, Test loss: 1.788, Test accuracy: 70.47
Round  69, Train loss: 1.755, Test loss: 1.787, Test accuracy: 70.44
Round  70, Train loss: 1.794, Test loss: 1.791, Test accuracy: 70.61
Round  71, Train loss: 1.754, Test loss: 1.787, Test accuracy: 70.71
Round  72, Train loss: 1.798, Test loss: 1.794, Test accuracy: 70.78
Round  73, Train loss: 1.758, Test loss: 1.782, Test accuracy: 71.17
Round  74, Train loss: 1.780, Test loss: 1.786, Test accuracy: 71.24
Round  75, Train loss: 1.733, Test loss: 1.781, Test accuracy: 71.30
Round  76, Train loss: 1.722, Test loss: 1.778, Test accuracy: 71.30
Round  77, Train loss: 1.774, Test loss: 1.777, Test accuracy: 71.70
Round  78, Train loss: 1.752, Test loss: 1.777, Test accuracy: 71.97
Round  79, Train loss: 1.760, Test loss: 1.778, Test accuracy: 72.04
Round  80, Train loss: 1.740, Test loss: 1.771, Test accuracy: 72.34
Round  81, Train loss: 1.726, Test loss: 1.771, Test accuracy: 72.28
Round  82, Train loss: 1.768, Test loss: 1.766, Test accuracy: 72.92
Round  83, Train loss: 1.745, Test loss: 1.766, Test accuracy: 73.10
Round  84, Train loss: 1.755, Test loss: 1.762, Test accuracy: 73.62
Round  85, Train loss: 1.699, Test loss: 1.756, Test accuracy: 73.97
Round  86, Train loss: 1.705, Test loss: 1.756, Test accuracy: 73.92
Round  87, Train loss: 1.754, Test loss: 1.750, Test accuracy: 74.58
Round  88, Train loss: 1.697, Test loss: 1.750, Test accuracy: 74.81
Round  89, Train loss: 1.708, Test loss: 1.747, Test accuracy: 74.87
Round  90, Train loss: 1.734, Test loss: 1.751, Test accuracy: 74.87
Round  91, Train loss: 1.679, Test loss: 1.739, Test accuracy: 74.89
Round  92, Train loss: 1.685, Test loss: 1.745, Test accuracy: 74.95
Round  93, Train loss: 1.714, Test loss: 1.747, Test accuracy: 74.85
Round  94, Train loss: 1.722, Test loss: 1.745, Test accuracy: 75.10/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.701, Test loss: 1.737, Test accuracy: 75.08
Round  96, Train loss: 1.666, Test loss: 1.739, Test accuracy: 75.15
Round  97, Train loss: 1.705, Test loss: 1.735, Test accuracy: 75.17
Round  98, Train loss: 1.697, Test loss: 1.735, Test accuracy: 75.58
Round  99, Train loss: 1.699, Test loss: 1.737, Test accuracy: 75.73
Final Round, Train loss: 1.681, Test loss: 1.717, Test accuracy: 77.19
Average accuracy final 10 rounds: 75.13583333333334
1162.1134142875671
[1.455955982208252, 2.8837199211120605, 4.195188045501709, 5.574090957641602, 6.928565740585327, 8.237805128097534, 9.60829496383667, 10.943595170974731, 12.326197862625122, 13.65784740447998, 14.98026442527771, 16.307101488113403, 17.64836025238037, 18.992987394332886, 20.362298488616943, 21.710030555725098, 23.01760244369507, 24.40182900428772, 25.71857523918152, 27.072836875915527, 28.421063899993896, 29.758962631225586, 31.128390789031982, 32.406797885894775, 33.80563926696777, 35.12088441848755, 36.44829559326172, 37.81863331794739, 39.10480260848999, 40.46075630187988, 41.760828495025635, 43.0645432472229, 44.427125215530396, 45.69123148918152, 47.06895112991333, 48.379348278045654, 49.70473670959473, 51.046119689941406, 52.31316614151001, 53.72250819206238, 55.03148341178894, 56.40004348754883, 57.78754806518555, 59.070483684539795, 60.41689157485962, 61.697121143341064, 63.05089449882507, 64.4066092967987, 65.8112633228302, 67.27587270736694, 68.60357737541199, 69.98849296569824, 71.32703852653503, 72.68657875061035, 74.01083397865295, 75.35262656211853, 76.70721507072449, 78.00981187820435, 79.35609865188599, 80.6710467338562, 81.97749066352844, 83.35441517829895, 84.6508641242981, 86.11858820915222, 87.44244837760925, 88.87381362915039, 90.22169423103333, 91.56659984588623, 93.01800274848938, 94.38633346557617, 95.759357213974, 97.19325542449951, 98.58259844779968, 100.00190734863281, 101.36133098602295, 102.7586121559143, 104.07745361328125, 105.34601020812988, 106.62376856803894, 107.89460945129395, 109.20811176300049, 110.50448799133301, 111.79920768737793, 113.10476183891296, 114.39125537872314, 115.67057633399963, 116.96786236763, 118.29608368873596, 119.5689492225647, 120.82469987869263, 122.14071273803711, 123.36653017997742, 124.66726088523865, 125.91245555877686, 127.17548298835754, 128.45949411392212, 129.73386812210083, 131.04814863204956, 132.30098366737366, 133.58597612380981, 135.27929139137268]
[10.083333333333334, 10.166666666666666, 10.583333333333334, 10.75, 11.233333333333333, 11.458333333333334, 11.491666666666667, 11.783333333333333, 12.1, 12.05, 12.683333333333334, 12.683333333333334, 12.033333333333333, 13.408333333333333, 14.716666666666667, 17.25, 18.908333333333335, 20.325, 21.666666666666668, 23.741666666666667, 25.766666666666666, 26.766666666666666, 28.408333333333335, 30.2, 32.09166666666667, 34.00833333333333, 38.608333333333334, 40.61666666666667, 42.56666666666667, 45.516666666666666, 47.875, 49.43333333333333, 51.958333333333336, 53.84166666666667, 54.38333333333333, 55.625, 56.708333333333336, 57.541666666666664, 58.166666666666664, 58.65833333333333, 60.041666666666664, 60.9, 61.31666666666667, 62.225, 62.925, 63.13333333333333, 63.708333333333336, 64.30833333333334, 64.725, 65.74166666666666, 65.8, 66.36666666666666, 67.05, 67.125, 67.25833333333334, 67.40833333333333, 67.23333333333333, 67.50833333333334, 68.625, 68.69166666666666, 68.74166666666666, 68.70833333333333, 69.06666666666666, 69.325, 70.275, 70.24166666666666, 70.40833333333333, 70.25, 70.46666666666667, 70.44166666666666, 70.60833333333333, 70.70833333333333, 70.78333333333333, 71.175, 71.24166666666666, 71.3, 71.3, 71.7, 71.975, 72.04166666666667, 72.34166666666667, 72.275, 72.91666666666667, 73.1, 73.625, 73.96666666666667, 73.925, 74.58333333333333, 74.80833333333334, 74.86666666666666, 74.86666666666666, 74.89166666666667, 74.95, 74.85, 75.1, 75.075, 75.15, 75.16666666666667, 75.575, 75.73333333333333, 77.19166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.302, Test accuracy: 9.97
Round   1, Train loss: 2.315, Test loss: 2.302, Test accuracy: 9.88
Round   2, Train loss: 2.310, Test loss: 2.302, Test accuracy: 10.07
Round   3, Train loss: 2.306, Test loss: 2.301, Test accuracy: 10.04
Round   4, Train loss: 2.305, Test loss: 2.301, Test accuracy: 11.08
Round   5, Train loss: 2.302, Test loss: 2.300, Test accuracy: 11.49
Round   6, Train loss: 2.299, Test loss: 2.298, Test accuracy: 12.42
Round   7, Train loss: 2.297, Test loss: 2.296, Test accuracy: 13.12
Round   8, Train loss: 2.291, Test loss: 2.293, Test accuracy: 13.85
Round   9, Train loss: 2.282, Test loss: 2.287, Test accuracy: 15.88
Round  10, Train loss: 2.257, Test loss: 2.276, Test accuracy: 18.17
Round  11, Train loss: 2.237, Test loss: 2.265, Test accuracy: 18.22
Round  12, Train loss: 2.224, Test loss: 2.255, Test accuracy: 19.61
Round  13, Train loss: 2.213, Test loss: 2.243, Test accuracy: 20.98
Round  14, Train loss: 2.194, Test loss: 2.233, Test accuracy: 21.75
Round  15, Train loss: 2.159, Test loss: 2.217, Test accuracy: 23.51
Round  16, Train loss: 2.203, Test loss: 2.205, Test accuracy: 25.97
Round  17, Train loss: 2.160, Test loss: 2.188, Test accuracy: 27.37
Round  18, Train loss: 2.133, Test loss: 2.175, Test accuracy: 28.68
Round  19, Train loss: 2.122, Test loss: 2.158, Test accuracy: 30.69
Round  20, Train loss: 2.101, Test loss: 2.143, Test accuracy: 33.08
Round  21, Train loss: 2.082, Test loss: 2.123, Test accuracy: 35.08
Round  22, Train loss: 2.116, Test loss: 2.114, Test accuracy: 35.28
Round  23, Train loss: 2.054, Test loss: 2.099, Test accuracy: 37.10
Round  24, Train loss: 2.039, Test loss: 2.085, Test accuracy: 39.16
Round  25, Train loss: 2.038, Test loss: 2.070, Test accuracy: 41.36
Round  26, Train loss: 2.018, Test loss: 2.058, Test accuracy: 43.21
Round  27, Train loss: 2.065, Test loss: 2.049, Test accuracy: 45.52
Round  28, Train loss: 2.037, Test loss: 2.037, Test accuracy: 46.92
Round  29, Train loss: 2.035, Test loss: 2.031, Test accuracy: 47.80
Round  30, Train loss: 2.015, Test loss: 2.006, Test accuracy: 49.73
Round  31, Train loss: 1.997, Test loss: 2.009, Test accuracy: 49.80
Round  32, Train loss: 1.988, Test loss: 2.008, Test accuracy: 50.00
Round  33, Train loss: 1.942, Test loss: 1.990, Test accuracy: 51.05
Round  34, Train loss: 1.922, Test loss: 1.980, Test accuracy: 51.35
Round  35, Train loss: 1.966, Test loss: 1.960, Test accuracy: 53.97
Round  36, Train loss: 1.933, Test loss: 1.953, Test accuracy: 55.09
Round  37, Train loss: 1.991, Test loss: 1.948, Test accuracy: 55.68
Round  38, Train loss: 1.907, Test loss: 1.935, Test accuracy: 56.67
Round  39, Train loss: 1.979, Test loss: 1.938, Test accuracy: 57.14
Round  40, Train loss: 1.908, Test loss: 1.920, Test accuracy: 57.73
Round  41, Train loss: 1.910, Test loss: 1.918, Test accuracy: 57.98
Round  42, Train loss: 1.929, Test loss: 1.901, Test accuracy: 59.80
Round  43, Train loss: 1.883, Test loss: 1.901, Test accuracy: 60.20
Round  44, Train loss: 1.846, Test loss: 1.888, Test accuracy: 61.37
Round  45, Train loss: 1.851, Test loss: 1.873, Test accuracy: 63.11
Round  46, Train loss: 1.840, Test loss: 1.863, Test accuracy: 63.73
Round  47, Train loss: 1.815, Test loss: 1.857, Test accuracy: 64.86
Round  48, Train loss: 1.864, Test loss: 1.857, Test accuracy: 65.47
Round  49, Train loss: 1.827, Test loss: 1.842, Test accuracy: 66.27
Round  50, Train loss: 1.838, Test loss: 1.840, Test accuracy: 67.63
Round  51, Train loss: 1.882, Test loss: 1.842, Test accuracy: 67.56
Round  52, Train loss: 1.827, Test loss: 1.825, Test accuracy: 68.40
Round  53, Train loss: 1.814, Test loss: 1.820, Test accuracy: 68.72
Round  54, Train loss: 1.824, Test loss: 1.809, Test accuracy: 69.53
Round  55, Train loss: 1.778, Test loss: 1.799, Test accuracy: 70.33
Round  56, Train loss: 1.780, Test loss: 1.792, Test accuracy: 71.36
Round  57, Train loss: 1.737, Test loss: 1.787, Test accuracy: 71.67
Round  58, Train loss: 1.792, Test loss: 1.784, Test accuracy: 72.02
Round  59, Train loss: 1.788, Test loss: 1.778, Test accuracy: 73.10
Round  60, Train loss: 1.775, Test loss: 1.772, Test accuracy: 73.58
Round  61, Train loss: 1.726, Test loss: 1.763, Test accuracy: 74.06
Round  62, Train loss: 1.756, Test loss: 1.759, Test accuracy: 74.41
Round  63, Train loss: 1.731, Test loss: 1.761, Test accuracy: 74.55
Round  64, Train loss: 1.731, Test loss: 1.755, Test accuracy: 74.97
Round  65, Train loss: 1.745, Test loss: 1.749, Test accuracy: 75.49
Round  66, Train loss: 1.702, Test loss: 1.741, Test accuracy: 76.03
Round  67, Train loss: 1.742, Test loss: 1.739, Test accuracy: 76.38
Round  68, Train loss: 1.749, Test loss: 1.733, Test accuracy: 76.64
Round  69, Train loss: 1.723, Test loss: 1.733, Test accuracy: 76.67
Round  70, Train loss: 1.718, Test loss: 1.731, Test accuracy: 76.80
Round  71, Train loss: 1.731, Test loss: 1.732, Test accuracy: 76.58
Round  72, Train loss: 1.741, Test loss: 1.735, Test accuracy: 76.82
Round  73, Train loss: 1.703, Test loss: 1.731, Test accuracy: 76.88
Round  74, Train loss: 1.735, Test loss: 1.724, Test accuracy: 76.86
Round  75, Train loss: 1.711, Test loss: 1.719, Test accuracy: 77.63
Round  76, Train loss: 1.689, Test loss: 1.716, Test accuracy: 77.78
Round  77, Train loss: 1.687, Test loss: 1.713, Test accuracy: 78.06
Round  78, Train loss: 1.735, Test loss: 1.715, Test accuracy: 78.01
Round  79, Train loss: 1.706, Test loss: 1.713, Test accuracy: 78.24
Round  80, Train loss: 1.688, Test loss: 1.710, Test accuracy: 78.37
Round  81, Train loss: 1.692, Test loss: 1.708, Test accuracy: 78.38
Round  82, Train loss: 1.671, Test loss: 1.705, Test accuracy: 78.67
Round  83, Train loss: 1.688, Test loss: 1.703, Test accuracy: 79.06
Round  84, Train loss: 1.688, Test loss: 1.702, Test accuracy: 79.07
Round  85, Train loss: 1.677, Test loss: 1.704, Test accuracy: 79.03
Round  86, Train loss: 1.694, Test loss: 1.704, Test accuracy: 79.36
Round  87, Train loss: 1.684, Test loss: 1.694, Test accuracy: 79.74
Round  88, Train loss: 1.692, Test loss: 1.695, Test accuracy: 79.73
Round  89, Train loss: 1.645, Test loss: 1.689, Test accuracy: 80.25
Round  90, Train loss: 1.643, Test loss: 1.689, Test accuracy: 80.21
Round  91, Train loss: 1.682, Test loss: 1.691, Test accuracy: 80.27
Round  92, Train loss: 1.695, Test loss: 1.690, Test accuracy: 80.30
Round  93, Train loss: 1.657, Test loss: 1.687, Test accuracy: 80.39/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.655, Test loss: 1.684, Test accuracy: 80.53
Round  95, Train loss: 1.654, Test loss: 1.685, Test accuracy: 80.55
Round  96, Train loss: 1.672, Test loss: 1.686, Test accuracy: 80.53
Round  97, Train loss: 1.687, Test loss: 1.681, Test accuracy: 81.03
Round  98, Train loss: 1.654, Test loss: 1.680, Test accuracy: 81.28
Round  99, Train loss: 1.629, Test loss: 1.672, Test accuracy: 81.74
Final Round, Train loss: 1.624, Test loss: 1.653, Test accuracy: 83.28
Average accuracy final 10 rounds: 80.68166666666667
1582.318754196167
[1.5072615146636963, 3.0145230293273926, 4.438404560089111, 5.86228609085083, 7.234187126159668, 8.606088161468506, 9.975659132003784, 11.345230102539062, 12.765076398849487, 14.184922695159912, 15.606828689575195, 17.02873468399048, 18.437591314315796, 19.846447944641113, 21.184406280517578, 22.522364616394043, 23.882890701293945, 25.243416786193848, 26.677436351776123, 28.1114559173584, 29.522774696350098, 30.934093475341797, 32.272321701049805, 33.61054992675781, 34.98045349121094, 36.35035705566406, 37.73804497718811, 39.12573289871216, 40.567471504211426, 42.00921010971069, 43.42875552177429, 44.84830093383789, 46.260533571243286, 47.67276620864868, 49.059569358825684, 50.446372509002686, 51.82663893699646, 53.206905364990234, 54.57327365875244, 55.93964195251465, 57.35684776306152, 58.7740535736084, 60.16901969909668, 61.56398582458496, 62.93287920951843, 64.3017725944519, 65.55464601516724, 66.80751943588257, 68.12787652015686, 69.44823360443115, 70.761150598526, 72.07406759262085, 73.32701325416565, 74.57995891571045, 75.87844896316528, 77.17693901062012, 78.457839012146, 79.73873901367188, 81.03782343864441, 82.33690786361694, 83.6111843585968, 84.88546085357666, 86.13305735588074, 87.38065385818481, 88.70247197151184, 90.02429008483887, 91.28879737854004, 92.55330467224121, 93.818434715271, 95.08356475830078, 96.35593605041504, 97.6283073425293, 98.93401646614075, 100.2397255897522, 101.50909996032715, 102.7784743309021, 104.04320812225342, 105.30794191360474, 106.55478739738464, 107.80163288116455, 109.09637665748596, 110.39112043380737, 111.72683620452881, 113.06255197525024, 114.36643505096436, 115.67031812667847, 116.93368577957153, 118.1970534324646, 119.53577637672424, 120.87449932098389, 122.23167490959167, 123.58885049819946, 124.89304208755493, 126.1972336769104, 127.49387168884277, 128.79050970077515, 130.0530755519867, 131.31564140319824, 132.69798588752747, 134.0803303718567, 135.4960150718689, 136.9116997718811, 138.31999349594116, 139.72828722000122, 141.0180184841156, 142.30774974822998, 143.6274266242981, 144.9471035003662, 146.4133939743042, 147.8796844482422, 149.29017686843872, 150.70066928863525, 152.10499334335327, 153.5093173980713, 154.83470559120178, 156.16009378433228, 157.55036544799805, 158.94063711166382, 160.4129867553711, 161.88533639907837, 163.39233422279358, 164.8993320465088, 166.41007328033447, 167.92081451416016, 169.35422778129578, 170.7876410484314, 172.23001503944397, 173.67238903045654, 175.1641936302185, 176.65599822998047, 178.1500084400177, 179.64401865005493, 181.09921073913574, 182.55440282821655, 183.96309971809387, 185.3717966079712, 186.80437016487122, 188.23694372177124, 189.6866478919983, 191.13635206222534, 192.60568761825562, 194.0750231742859, 195.56131219863892, 197.04760122299194, 198.5231113433838, 199.99862146377563, 201.49141573905945, 202.98421001434326, 204.4290108680725, 205.87381172180176, 207.30454444885254, 208.73527717590332, 210.20408844947815, 211.67289972305298, 213.1223862171173, 214.57187271118164, 216.04847621917725, 217.52507972717285, 219.02175545692444, 220.51843118667603, 221.96552968025208, 223.41262817382812, 224.865975856781, 226.3193235397339, 227.77508759498596, 229.23085165023804, 230.70973563194275, 232.18861961364746, 233.67779231071472, 235.16696500778198, 236.6235113143921, 238.0800576210022, 239.5350947380066, 240.990131855011, 242.38494229316711, 243.77975273132324, 245.19216060638428, 246.6045684814453, 248.09261322021484, 249.58065795898438, 251.06062984466553, 252.54060173034668, 253.9716019630432, 255.40260219573975, 256.82532954216003, 258.2480568885803, 259.73044753074646, 261.2128381729126, 262.70222902297974, 264.1916198730469, 265.70211911201477, 267.21261835098267, 268.6412081718445, 270.0697979927063, 271.5267586708069, 272.98371934890747, 274.45185112953186, 275.91998291015625, 277.3332664966583, 278.7465500831604, 280.54414200782776, 282.3417339324951]
[9.975, 9.975, 9.883333333333333, 9.883333333333333, 10.066666666666666, 10.066666666666666, 10.041666666666666, 10.041666666666666, 11.083333333333334, 11.083333333333334, 11.491666666666667, 11.491666666666667, 12.416666666666666, 12.416666666666666, 13.116666666666667, 13.116666666666667, 13.85, 13.85, 15.875, 15.875, 18.166666666666668, 18.166666666666668, 18.216666666666665, 18.216666666666665, 19.608333333333334, 19.608333333333334, 20.983333333333334, 20.983333333333334, 21.75, 21.75, 23.508333333333333, 23.508333333333333, 25.966666666666665, 25.966666666666665, 27.366666666666667, 27.366666666666667, 28.683333333333334, 28.683333333333334, 30.691666666666666, 30.691666666666666, 33.075, 33.075, 35.075, 35.075, 35.28333333333333, 35.28333333333333, 37.1, 37.1, 39.15833333333333, 39.15833333333333, 41.358333333333334, 41.358333333333334, 43.208333333333336, 43.208333333333336, 45.516666666666666, 45.516666666666666, 46.925, 46.925, 47.8, 47.8, 49.733333333333334, 49.733333333333334, 49.8, 49.8, 50.0, 50.0, 51.05, 51.05, 51.35, 51.35, 53.96666666666667, 53.96666666666667, 55.09166666666667, 55.09166666666667, 55.68333333333333, 55.68333333333333, 56.675, 56.675, 57.141666666666666, 57.141666666666666, 57.725, 57.725, 57.975, 57.975, 59.8, 59.8, 60.2, 60.2, 61.36666666666667, 61.36666666666667, 63.108333333333334, 63.108333333333334, 63.733333333333334, 63.733333333333334, 64.85833333333333, 64.85833333333333, 65.475, 65.475, 66.26666666666667, 66.26666666666667, 67.63333333333334, 67.63333333333334, 67.55833333333334, 67.55833333333334, 68.4, 68.4, 68.71666666666667, 68.71666666666667, 69.53333333333333, 69.53333333333333, 70.33333333333333, 70.33333333333333, 71.35833333333333, 71.35833333333333, 71.66666666666667, 71.66666666666667, 72.01666666666667, 72.01666666666667, 73.1, 73.1, 73.58333333333333, 73.58333333333333, 74.05833333333334, 74.05833333333334, 74.40833333333333, 74.40833333333333, 74.55, 74.55, 74.96666666666667, 74.96666666666667, 75.49166666666666, 75.49166666666666, 76.03333333333333, 76.03333333333333, 76.375, 76.375, 76.64166666666667, 76.64166666666667, 76.66666666666667, 76.66666666666667, 76.8, 76.8, 76.58333333333333, 76.58333333333333, 76.81666666666666, 76.81666666666666, 76.88333333333334, 76.88333333333334, 76.85833333333333, 76.85833333333333, 77.63333333333334, 77.63333333333334, 77.775, 77.775, 78.05833333333334, 78.05833333333334, 78.00833333333334, 78.00833333333334, 78.24166666666666, 78.24166666666666, 78.36666666666666, 78.36666666666666, 78.38333333333334, 78.38333333333334, 78.675, 78.675, 79.05833333333334, 79.05833333333334, 79.06666666666666, 79.06666666666666, 79.03333333333333, 79.03333333333333, 79.35833333333333, 79.35833333333333, 79.74166666666666, 79.74166666666666, 79.73333333333333, 79.73333333333333, 80.25, 80.25, 80.20833333333333, 80.20833333333333, 80.26666666666667, 80.26666666666667, 80.3, 80.3, 80.39166666666667, 80.39166666666667, 80.525, 80.525, 80.55, 80.55, 80.525, 80.525, 81.03333333333333, 81.03333333333333, 81.275, 81.275, 81.74166666666666, 81.74166666666666, 83.28333333333333, 83.28333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.202, Test loss: 2.186, Test accuracy: 29.86
Round   0, Global train loss: 2.202, Global test loss: 2.285, Global test accuracy: 18.96
Round   1, Train loss: 1.829, Test loss: 2.050, Test accuracy: 44.35
Round   1, Global train loss: 1.829, Global test loss: 2.277, Global test accuracy: 21.15
Round   2, Train loss: 1.725, Test loss: 1.929, Test accuracy: 52.98
Round   2, Global train loss: 1.725, Global test loss: 2.298, Global test accuracy: 12.60
Round   3, Train loss: 1.690, Test loss: 1.856, Test accuracy: 62.10
Round   3, Global train loss: 1.690, Global test loss: 2.255, Global test accuracy: 20.22
Round   4, Train loss: 1.595, Test loss: 1.826, Test accuracy: 64.35
Round   4, Global train loss: 1.595, Global test loss: 2.279, Global test accuracy: 17.36
Round   5, Train loss: 1.760, Test loss: 1.752, Test accuracy: 73.63
Round   5, Global train loss: 1.760, Global test loss: 2.258, Global test accuracy: 18.63
Round   6, Train loss: 1.515, Test loss: 1.738, Test accuracy: 72.66
Round   6, Global train loss: 1.515, Global test loss: 2.267, Global test accuracy: 18.08
Round   7, Train loss: 1.600, Test loss: 1.680, Test accuracy: 77.68
Round   7, Global train loss: 1.600, Global test loss: 2.302, Global test accuracy: 11.45
Round   8, Train loss: 1.620, Test loss: 1.650, Test accuracy: 81.83
Round   8, Global train loss: 1.620, Global test loss: 2.268, Global test accuracy: 18.33
Round   9, Train loss: 1.659, Test loss: 1.587, Test accuracy: 88.22
Round   9, Global train loss: 1.659, Global test loss: 2.252, Global test accuracy: 19.20
Round  10, Train loss: 1.535, Test loss: 1.585, Test accuracy: 88.28
Round  10, Global train loss: 1.535, Global test loss: 2.296, Global test accuracy: 11.72
Round  11, Train loss: 1.588, Test loss: 1.583, Test accuracy: 88.38
Round  11, Global train loss: 1.588, Global test loss: 2.268, Global test accuracy: 16.95
Round  12, Train loss: 1.503, Test loss: 1.570, Test accuracy: 89.67
Round  12, Global train loss: 1.503, Global test loss: 2.297, Global test accuracy: 12.28
Round  13, Train loss: 1.530, Test loss: 1.570, Test accuracy: 89.64
Round  13, Global train loss: 1.530, Global test loss: 2.253, Global test accuracy: 20.90
Round  14, Train loss: 1.475, Test loss: 1.570, Test accuracy: 89.57
Round  14, Global train loss: 1.475, Global test loss: 2.272, Global test accuracy: 16.30
Round  15, Train loss: 1.527, Test loss: 1.569, Test accuracy: 89.58
Round  15, Global train loss: 1.527, Global test loss: 2.295, Global test accuracy: 15.00
Round  16, Train loss: 1.581, Test loss: 1.569, Test accuracy: 89.67
Round  16, Global train loss: 1.581, Global test loss: 2.244, Global test accuracy: 20.43
Round  17, Train loss: 1.524, Test loss: 1.568, Test accuracy: 89.70
Round  17, Global train loss: 1.524, Global test loss: 2.256, Global test accuracy: 20.07
Round  18, Train loss: 1.529, Test loss: 1.566, Test accuracy: 89.88
Round  18, Global train loss: 1.529, Global test loss: 2.256, Global test accuracy: 21.18
Round  19, Train loss: 1.471, Test loss: 1.566, Test accuracy: 89.82
Round  19, Global train loss: 1.471, Global test loss: 2.285, Global test accuracy: 15.74
Round  20, Train loss: 1.525, Test loss: 1.566, Test accuracy: 89.83
Round  20, Global train loss: 1.525, Global test loss: 2.282, Global test accuracy: 13.78
Round  21, Train loss: 1.523, Test loss: 1.566, Test accuracy: 89.83
Round  21, Global train loss: 1.523, Global test loss: 2.287, Global test accuracy: 15.00
Round  22, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.85
Round  22, Global train loss: 1.522, Global test loss: 2.246, Global test accuracy: 20.78
Round  23, Train loss: 1.575, Test loss: 1.565, Test accuracy: 89.85
Round  23, Global train loss: 1.575, Global test loss: 2.238, Global test accuracy: 18.28
Round  24, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.92
Round  24, Global train loss: 1.522, Global test loss: 2.257, Global test accuracy: 18.03
Round  25, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.92
Round  25, Global train loss: 1.522, Global test loss: 2.266, Global test accuracy: 18.14
Round  26, Train loss: 1.466, Test loss: 1.565, Test accuracy: 89.91
Round  26, Global train loss: 1.466, Global test loss: 2.293, Global test accuracy: 10.86
Round  27, Train loss: 1.523, Test loss: 1.564, Test accuracy: 89.94
Round  27, Global train loss: 1.523, Global test loss: 2.276, Global test accuracy: 17.33
Round  28, Train loss: 1.466, Test loss: 1.564, Test accuracy: 89.93
Round  28, Global train loss: 1.466, Global test loss: 2.255, Global test accuracy: 18.24
Round  29, Train loss: 1.576, Test loss: 1.564, Test accuracy: 89.92
Round  29, Global train loss: 1.576, Global test loss: 2.251, Global test accuracy: 19.61
Round  30, Train loss: 1.578, Test loss: 1.564, Test accuracy: 89.91
Round  30, Global train loss: 1.578, Global test loss: 2.239, Global test accuracy: 21.60
Round  31, Train loss: 1.467, Test loss: 1.564, Test accuracy: 89.91
Round  31, Global train loss: 1.467, Global test loss: 2.278, Global test accuracy: 12.69
Round  32, Train loss: 1.575, Test loss: 1.564, Test accuracy: 89.90
Round  32, Global train loss: 1.575, Global test loss: 2.257, Global test accuracy: 19.77
Round  33, Train loss: 1.518, Test loss: 1.564, Test accuracy: 89.90
Round  33, Global train loss: 1.518, Global test loss: 2.272, Global test accuracy: 17.21
Round  34, Train loss: 1.576, Test loss: 1.564, Test accuracy: 89.92
Round  34, Global train loss: 1.576, Global test loss: 2.253, Global test accuracy: 18.16
Round  35, Train loss: 1.575, Test loss: 1.564, Test accuracy: 89.91
Round  35, Global train loss: 1.575, Global test loss: 2.236, Global test accuracy: 21.46
Round  36, Train loss: 1.521, Test loss: 1.564, Test accuracy: 89.91
Round  36, Global train loss: 1.521, Global test loss: 2.286, Global test accuracy: 14.16
Round  37, Train loss: 1.573, Test loss: 1.564, Test accuracy: 89.89
Round  37, Global train loss: 1.573, Global test loss: 2.235, Global test accuracy: 20.63
Round  38, Train loss: 1.628, Test loss: 1.564, Test accuracy: 89.88
Round  38, Global train loss: 1.628, Global test loss: 2.284, Global test accuracy: 15.32
Round  39, Train loss: 1.574, Test loss: 1.564, Test accuracy: 89.90
Round  39, Global train loss: 1.574, Global test loss: 2.245, Global test accuracy: 21.67
Round  40, Train loss: 1.522, Test loss: 1.564, Test accuracy: 89.88
Round  40, Global train loss: 1.522, Global test loss: 2.277, Global test accuracy: 14.54
Round  41, Train loss: 1.466, Test loss: 1.564, Test accuracy: 89.91
Round  41, Global train loss: 1.466, Global test loss: 2.264, Global test accuracy: 18.34
Round  42, Train loss: 1.520, Test loss: 1.564, Test accuracy: 89.90
Round  42, Global train loss: 1.520, Global test loss: 2.258, Global test accuracy: 19.98
Round  43, Train loss: 1.522, Test loss: 1.564, Test accuracy: 89.88
Round  43, Global train loss: 1.522, Global test loss: 2.274, Global test accuracy: 14.21
Round  44, Train loss: 1.497, Test loss: 1.550, Test accuracy: 91.38
Round  44, Global train loss: 1.497, Global test loss: 2.254, Global test accuracy: 20.28
Round  45, Train loss: 1.577, Test loss: 1.549, Test accuracy: 91.42
Round  45, Global train loss: 1.577, Global test loss: 2.230, Global test accuracy: 21.69
Round  46, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.39
Round  46, Global train loss: 1.519, Global test loss: 2.288, Global test accuracy: 14.17
Round  47, Train loss: 1.574, Test loss: 1.549, Test accuracy: 91.38
Round  47, Global train loss: 1.574, Global test loss: 2.300, Global test accuracy: 15.02
Round  48, Train loss: 1.574, Test loss: 1.549, Test accuracy: 91.38
Round  48, Global train loss: 1.574, Global test loss: 2.259, Global test accuracy: 18.48
Round  49, Train loss: 1.574, Test loss: 1.549, Test accuracy: 91.38
Round  49, Global train loss: 1.574, Global test loss: 2.269, Global test accuracy: 14.96
Round  50, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.31
Round  50, Global train loss: 1.520, Global test loss: 2.264, Global test accuracy: 18.07
Round  51, Train loss: 1.466, Test loss: 1.549, Test accuracy: 91.32
Round  51, Global train loss: 1.466, Global test loss: 2.260, Global test accuracy: 17.98
Round  52, Train loss: 1.465, Test loss: 1.549, Test accuracy: 91.34
Round  52, Global train loss: 1.465, Global test loss: 2.272, Global test accuracy: 18.24
Round  53, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.34
Round  53, Global train loss: 1.518, Global test loss: 2.247, Global test accuracy: 20.16
Round  54, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.35
Round  54, Global train loss: 1.520, Global test loss: 2.269, Global test accuracy: 18.33
Round  55, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.31
Round  55, Global train loss: 1.518, Global test loss: 2.253, Global test accuracy: 20.01
Round  56, Train loss: 1.465, Test loss: 1.549, Test accuracy: 91.31
Round  56, Global train loss: 1.465, Global test loss: 2.247, Global test accuracy: 20.11
Round  57, Train loss: 1.575, Test loss: 1.549, Test accuracy: 91.31
Round  57, Global train loss: 1.575, Global test loss: 2.288, Global test accuracy: 11.92
Round  58, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.31
Round  58, Global train loss: 1.520, Global test loss: 2.263, Global test accuracy: 16.80
Round  59, Train loss: 1.467, Test loss: 1.549, Test accuracy: 91.31
Round  59, Global train loss: 1.467, Global test loss: 2.336, Global test accuracy: 9.62
Round  60, Train loss: 1.572, Test loss: 1.549, Test accuracy: 91.29
Round  60, Global train loss: 1.572, Global test loss: 2.265, Global test accuracy: 18.33
Round  61, Train loss: 1.464, Test loss: 1.549, Test accuracy: 91.30
Round  61, Global train loss: 1.464, Global test loss: 2.254, Global test accuracy: 18.50
Round  62, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.32
Round  62, Global train loss: 1.519, Global test loss: 2.268, Global test accuracy: 17.03
Round  63, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  63, Global train loss: 1.519, Global test loss: 2.251, Global test accuracy: 20.95
Round  64, Train loss: 1.464, Test loss: 1.549, Test accuracy: 91.33
Round  64, Global train loss: 1.464, Global test loss: 2.238, Global test accuracy: 20.61
Round  65, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.33
Round  65, Global train loss: 1.520, Global test loss: 2.257, Global test accuracy: 16.43
Round  66, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.34
Round  66, Global train loss: 1.520, Global test loss: 2.275, Global test accuracy: 18.33
Round  67, Train loss: 1.465, Test loss: 1.549, Test accuracy: 91.33
Round  67, Global train loss: 1.465, Global test loss: 2.275, Global test accuracy: 15.95
Round  68, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.34
Round  68, Global train loss: 1.520, Global test loss: 2.226, Global test accuracy: 22.01
Round  69, Train loss: 1.572, Test loss: 1.549, Test accuracy: 91.32
Round  69, Global train loss: 1.572, Global test loss: 2.269, Global test accuracy: 16.62
Round  70, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  70, Global train loss: 1.519, Global test loss: 2.245, Global test accuracy: 19.98
Round  71, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  71, Global train loss: 1.519, Global test loss: 2.278, Global test accuracy: 13.63
Round  72, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  72, Global train loss: 1.519, Global test loss: 2.272, Global test accuracy: 14.30
Round  73, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.35
Round  73, Global train loss: 1.519, Global test loss: 2.317, Global test accuracy: 11.68
Round  74, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.37
Round  74, Global train loss: 1.518, Global test loss: 2.266, Global test accuracy: 18.40
Round  75, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.37
Round  75, Global train loss: 1.519, Global test loss: 2.256, Global test accuracy: 21.62
Round  76, Train loss: 1.466, Test loss: 1.549, Test accuracy: 91.38
Round  76, Global train loss: 1.466, Global test loss: 2.225, Global test accuracy: 23.02
Round  77, Train loss: 1.464, Test loss: 1.549, Test accuracy: 91.39
Round  77, Global train loss: 1.464, Global test loss: 2.264, Global test accuracy: 17.50
Round  78, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.38
Round  78, Global train loss: 1.519, Global test loss: 2.240, Global test accuracy: 21.10
Round  79, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.38
Round  79, Global train loss: 1.518, Global test loss: 2.300, Global test accuracy: 12.75
Round  80, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.39
Round  80, Global train loss: 1.520, Global test loss: 2.249, Global test accuracy: 18.10
Round  81, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.40
Round  81, Global train loss: 1.518, Global test loss: 2.298, Global test accuracy: 11.81
Round  82, Train loss: 1.465, Test loss: 1.548, Test accuracy: 91.41
Round  82, Global train loss: 1.465, Global test loss: 2.255, Global test accuracy: 18.09
Round  83, Train loss: 1.517, Test loss: 1.548, Test accuracy: 91.41
Round  83, Global train loss: 1.517, Global test loss: 2.236, Global test accuracy: 22.18
Round  84, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.40
Round  84, Global train loss: 1.464, Global test loss: 2.258, Global test accuracy: 17.77
Round  85, Train loss: 1.572, Test loss: 1.548, Test accuracy: 91.40
Round  85, Global train loss: 1.572, Global test loss: 2.244, Global test accuracy: 19.50
Round  86, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.39
Round  86, Global train loss: 1.519, Global test loss: 2.285, Global test accuracy: 12.95
Round  87, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.39
Round  87, Global train loss: 1.464, Global test loss: 2.243, Global test accuracy: 19.88
Round  88, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.39
Round  88, Global train loss: 1.464, Global test loss: 2.294, Global test accuracy: 12.58
Round  89, Train loss: 1.572, Test loss: 1.548, Test accuracy: 91.39
Round  89, Global train loss: 1.572, Global test loss: 2.299, Global test accuracy: 12.08
Round  90, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.39
Round  90, Global train loss: 1.519, Global test loss: 2.262, Global test accuracy: 17.61
Round  91, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.39
Round  91, Global train loss: 1.519, Global test loss: 2.280, Global test accuracy: 15.40
Round  92, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.38
Round  92, Global train loss: 1.519, Global test loss: 2.285, Global test accuracy: 12.82
Round  93, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.38
Round  93, Global train loss: 1.464, Global test loss: 2.260, Global test accuracy: 19.06
Round  94, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.38
Round  94, Global train loss: 1.464, Global test loss: 2.245, Global test accuracy: 20.23
Round  95, Train loss: 1.465, Test loss: 1.548, Test accuracy: 91.37
Round  95, Global train loss: 1.465, Global test loss: 2.269, Global test accuracy: 16.53/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.466, Test loss: 1.548, Test accuracy: 91.38
Round  96, Global train loss: 1.466, Global test loss: 2.220, Global test accuracy: 23.64
Round  97, Train loss: 1.465, Test loss: 1.548, Test accuracy: 91.38
Round  97, Global train loss: 1.465, Global test loss: 2.269, Global test accuracy: 16.41
Round  98, Train loss: 1.466, Test loss: 1.548, Test accuracy: 91.38
Round  98, Global train loss: 1.466, Global test loss: 2.298, Global test accuracy: 12.98
Round  99, Train loss: 1.573, Test loss: 1.548, Test accuracy: 91.38
Round  99, Global train loss: 1.573, Global test loss: 2.285, Global test accuracy: 13.40
Final Round, Train loss: 1.512, Test loss: 1.548, Test accuracy: 91.38
Final Round, Global train loss: 1.512, Global test loss: 2.285, Global test accuracy: 13.40
Average accuracy final 10 rounds: 91.38000000000001 

Average global accuracy final 10 rounds: 16.807500000000005 

1631.7130942344666
[1.2457003593444824, 2.491400718688965, 3.6576008796691895, 4.823801040649414, 6.010705471038818, 7.197609901428223, 8.359447956085205, 9.521286010742188, 10.663824319839478, 11.806362628936768, 12.956706762313843, 14.107050895690918, 15.31734848022461, 16.5276460647583, 17.75052046775818, 18.973394870758057, 20.012800216674805, 21.052205562591553, 22.042078256607056, 23.03195095062256, 24.01418972015381, 24.99642848968506, 25.93848967552185, 26.880550861358643, 27.835181713104248, 28.789812564849854, 29.81527280807495, 30.84073305130005, 31.818371295928955, 32.79600954055786, 33.80137228965759, 34.806735038757324, 35.82373833656311, 36.8407416343689, 37.841289043426514, 38.84183645248413, 39.84019422531128, 40.83855199813843, 41.83521127700806, 42.831870555877686, 43.749155044555664, 44.66643953323364, 45.660829305648804, 46.655219078063965, 47.626707315444946, 48.59819555282593, 49.57623052597046, 50.55426549911499, 51.538228034973145, 52.5221905708313, 53.47016382217407, 54.418137073516846, 55.366520404815674, 56.3149037361145, 57.31104397773743, 58.30718421936035, 59.29261636734009, 60.278048515319824, 61.21817660331726, 62.1583046913147, 63.11991548538208, 64.08152627944946, 65.00761485099792, 65.93370342254639, 66.87641525268555, 67.8191270828247, 68.72983026504517, 69.64053344726562, 70.58184599876404, 71.52315855026245, 72.51624608039856, 73.50933361053467, 74.502925157547, 75.49651670455933, 76.48390555381775, 77.47129440307617, 78.41578769683838, 79.36028099060059, 80.32955718040466, 81.29883337020874, 82.22613549232483, 83.15343761444092, 84.08962845802307, 85.02581930160522, 86.0851263999939, 87.14443349838257, 88.21786069869995, 89.29128789901733, 90.38836932182312, 91.4854507446289, 92.52872490882874, 93.57199907302856, 94.69011998176575, 95.80824089050293, 96.97237038612366, 98.13649988174438, 99.12418842315674, 100.11187696456909, 101.07344889640808, 102.03502082824707, 102.97938704490662, 103.92375326156616, 104.90758800506592, 105.89142274856567, 106.8117105960846, 107.73199844360352, 108.7142903804779, 109.6965823173523, 110.77203464508057, 111.84748697280884, 112.78571367263794, 113.72394037246704, 114.70144319534302, 115.678946018219, 116.63282799720764, 117.58670997619629, 118.55898404121399, 119.53125810623169, 120.52501726150513, 121.51877641677856, 122.45678472518921, 123.39479303359985, 124.28757667541504, 125.18036031723022, 126.17883086204529, 127.17730140686035, 128.1195478439331, 129.06179428100586, 129.9677517414093, 130.87370920181274, 131.80162167549133, 132.72953414916992, 133.71186542510986, 134.6941967010498, 135.61942672729492, 136.54465675354004, 137.4474229812622, 138.35018920898438, 139.33553791046143, 140.32088661193848, 141.27248978614807, 142.22409296035767, 143.2006540298462, 144.17721509933472, 145.13422274589539, 146.09123039245605, 147.0154104232788, 147.93959045410156, 148.8870701789856, 149.83454990386963, 150.80927968025208, 151.78400945663452, 152.74101400375366, 153.6980185508728, 154.67532587051392, 155.65263319015503, 156.5928156375885, 157.53299808502197, 158.50857424736023, 159.4841504096985, 160.46735000610352, 161.45054960250854, 162.36272072792053, 163.27489185333252, 164.25260210037231, 165.2303123474121, 166.1806640625, 167.1310157775879, 168.04899215698242, 168.96696853637695, 169.85783624649048, 170.748703956604, 171.7701563835144, 172.7916088104248, 173.77931189537048, 174.76701498031616, 175.6892068386078, 176.6113986968994, 177.6022083759308, 178.59301805496216, 179.57729506492615, 180.56157207489014, 181.5102505683899, 182.45892906188965, 183.4091100692749, 184.35929107666016, 185.32145762443542, 186.2836241722107, 187.24010682106018, 188.19658946990967, 189.18967413902283, 190.182758808136, 191.10171222686768, 192.02066564559937, 192.9783968925476, 193.93612813949585, 194.92531633377075, 195.91450452804565, 196.880211353302, 197.84591817855835, 199.50356101989746, 201.16120386123657]
[29.858333333333334, 29.858333333333334, 44.35, 44.35, 52.975, 52.975, 62.1, 62.1, 64.35, 64.35, 73.63333333333334, 73.63333333333334, 72.65833333333333, 72.65833333333333, 77.68333333333334, 77.68333333333334, 81.83333333333333, 81.83333333333333, 88.21666666666667, 88.21666666666667, 88.28333333333333, 88.28333333333333, 88.375, 88.375, 89.66666666666667, 89.66666666666667, 89.64166666666667, 89.64166666666667, 89.56666666666666, 89.56666666666666, 89.575, 89.575, 89.66666666666667, 89.66666666666667, 89.7, 89.7, 89.88333333333334, 89.88333333333334, 89.81666666666666, 89.81666666666666, 89.83333333333333, 89.83333333333333, 89.825, 89.825, 89.85, 89.85, 89.85, 89.85, 89.91666666666667, 89.91666666666667, 89.91666666666667, 89.91666666666667, 89.90833333333333, 89.90833333333333, 89.94166666666666, 89.94166666666666, 89.93333333333334, 89.93333333333334, 89.925, 89.925, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.9, 89.9, 89.9, 89.9, 89.91666666666667, 89.91666666666667, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.89166666666667, 89.89166666666667, 89.88333333333334, 89.88333333333334, 89.9, 89.9, 89.88333333333334, 89.88333333333334, 89.90833333333333, 89.90833333333333, 89.9, 89.9, 89.875, 89.875, 91.375, 91.375, 91.41666666666667, 91.41666666666667, 91.39166666666667, 91.39166666666667, 91.375, 91.375, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.30833333333334, 91.30833333333334, 91.31666666666666, 91.31666666666666, 91.34166666666667, 91.34166666666667, 91.34166666666667, 91.34166666666667, 91.35, 91.35, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.29166666666667, 91.29166666666667, 91.3, 91.3, 91.31666666666666, 91.31666666666666, 91.325, 91.325, 91.325, 91.325, 91.325, 91.325, 91.34166666666667, 91.34166666666667, 91.325, 91.325, 91.34166666666667, 91.34166666666667, 91.31666666666666, 91.31666666666666, 91.325, 91.325, 91.325, 91.325, 91.33333333333333, 91.33333333333333, 91.35, 91.35, 91.36666666666666, 91.36666666666666, 91.36666666666666, 91.36666666666666, 91.38333333333334, 91.38333333333334, 91.39166666666667, 91.39166666666667, 91.375, 91.375, 91.38333333333334, 91.38333333333334, 91.39166666666667, 91.39166666666667, 91.4, 91.4, 91.40833333333333, 91.40833333333333, 91.40833333333333, 91.40833333333333, 91.4, 91.4, 91.4, 91.4, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.38333333333334, 91.38333333333334, 91.375, 91.375, 91.375, 91.375, 91.36666666666666, 91.36666666666666, 91.375, 91.375, 91.375, 91.375, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.273, Test loss: 2.263, Test accuracy: 18.37
Round   0, Global train loss: 2.273, Global test loss: 2.301, Global test accuracy: 11.79
Round   1, Train loss: 2.039, Test loss: 2.146, Test accuracy: 31.84
Round   1, Global train loss: 2.039, Global test loss: 2.300, Global test accuracy: 12.26
Round   2, Train loss: 1.952, Test loss: 2.069, Test accuracy: 39.28
Round   2, Global train loss: 1.952, Global test loss: 2.308, Global test accuracy: 11.91
Round   3, Train loss: 2.002, Test loss: 2.025, Test accuracy: 43.76
Round   3, Global train loss: 2.002, Global test loss: 2.313, Global test accuracy: 12.67
Round   4, Train loss: 1.955, Test loss: 1.980, Test accuracy: 48.29
Round   4, Global train loss: 1.955, Global test loss: 2.305, Global test accuracy: 12.95
Round   5, Train loss: 1.952, Test loss: 1.939, Test accuracy: 52.74
Round   5, Global train loss: 1.952, Global test loss: 2.300, Global test accuracy: 13.81
Round   6, Train loss: 2.024, Test loss: 1.944, Test accuracy: 52.08
Round   6, Global train loss: 2.024, Global test loss: 2.302, Global test accuracy: 13.26
Round   7, Train loss: 1.984, Test loss: 1.936, Test accuracy: 52.82
Round   7, Global train loss: 1.984, Global test loss: 2.307, Global test accuracy: 12.65
Round   8, Train loss: 1.951, Test loss: 1.932, Test accuracy: 53.16
Round   8, Global train loss: 1.951, Global test loss: 2.307, Global test accuracy: 13.00
Round   9, Train loss: 1.900, Test loss: 1.935, Test accuracy: 52.83
Round   9, Global train loss: 1.900, Global test loss: 2.315, Global test accuracy: 11.78
Round  10, Train loss: 1.961, Test loss: 1.928, Test accuracy: 53.02
Round  10, Global train loss: 1.961, Global test loss: 2.301, Global test accuracy: 12.91
Round  11, Train loss: 1.893, Test loss: 1.916, Test accuracy: 54.25
Round  11, Global train loss: 1.893, Global test loss: 2.292, Global test accuracy: 14.25
Round  12, Train loss: 1.895, Test loss: 1.899, Test accuracy: 55.94
Round  12, Global train loss: 1.895, Global test loss: 2.297, Global test accuracy: 13.94
Round  13, Train loss: 1.883, Test loss: 1.873, Test accuracy: 58.62
Round  13, Global train loss: 1.883, Global test loss: 2.295, Global test accuracy: 14.14
Round  14, Train loss: 1.855, Test loss: 1.879, Test accuracy: 57.91
Round  14, Global train loss: 1.855, Global test loss: 2.289, Global test accuracy: 14.71
Round  15, Train loss: 1.928, Test loss: 1.880, Test accuracy: 57.71
Round  15, Global train loss: 1.928, Global test loss: 2.292, Global test accuracy: 14.22
Round  16, Train loss: 1.852, Test loss: 1.854, Test accuracy: 60.55
Round  16, Global train loss: 1.852, Global test loss: 2.278, Global test accuracy: 16.20
Round  17, Train loss: 1.868, Test loss: 1.857, Test accuracy: 60.16
Round  17, Global train loss: 1.868, Global test loss: 2.291, Global test accuracy: 14.51
Round  18, Train loss: 1.832, Test loss: 1.851, Test accuracy: 60.80
Round  18, Global train loss: 1.832, Global test loss: 2.287, Global test accuracy: 15.43
Round  19, Train loss: 1.838, Test loss: 1.856, Test accuracy: 60.24
Round  19, Global train loss: 1.838, Global test loss: 2.285, Global test accuracy: 15.32
Round  20, Train loss: 1.909, Test loss: 1.845, Test accuracy: 61.30
Round  20, Global train loss: 1.909, Global test loss: 2.282, Global test accuracy: 15.44
Round  21, Train loss: 1.851, Test loss: 1.844, Test accuracy: 61.42
Round  21, Global train loss: 1.851, Global test loss: 2.285, Global test accuracy: 15.37
Round  22, Train loss: 1.832, Test loss: 1.848, Test accuracy: 60.93
Round  22, Global train loss: 1.832, Global test loss: 2.282, Global test accuracy: 15.68
Round  23, Train loss: 1.881, Test loss: 1.841, Test accuracy: 61.69
Round  23, Global train loss: 1.881, Global test loss: 2.285, Global test accuracy: 15.30
Round  24, Train loss: 1.907, Test loss: 1.847, Test accuracy: 60.99
Round  24, Global train loss: 1.907, Global test loss: 2.284, Global test accuracy: 15.22
Round  25, Train loss: 1.834, Test loss: 1.846, Test accuracy: 60.99
Round  25, Global train loss: 1.834, Global test loss: 2.284, Global test accuracy: 15.53
Round  26, Train loss: 1.846, Test loss: 1.842, Test accuracy: 61.41
Round  26, Global train loss: 1.846, Global test loss: 2.293, Global test accuracy: 13.88
Round  27, Train loss: 1.767, Test loss: 1.826, Test accuracy: 63.12
Round  27, Global train loss: 1.767, Global test loss: 2.278, Global test accuracy: 16.29
Round  28, Train loss: 1.836, Test loss: 1.822, Test accuracy: 63.53
Round  28, Global train loss: 1.836, Global test loss: 2.284, Global test accuracy: 15.61
Round  29, Train loss: 1.809, Test loss: 1.802, Test accuracy: 65.62
Round  29, Global train loss: 1.809, Global test loss: 2.281, Global test accuracy: 15.18
Round  30, Train loss: 1.800, Test loss: 1.796, Test accuracy: 66.27
Round  30, Global train loss: 1.800, Global test loss: 2.286, Global test accuracy: 15.29
Round  31, Train loss: 1.812, Test loss: 1.783, Test accuracy: 67.54
Round  31, Global train loss: 1.812, Global test loss: 2.283, Global test accuracy: 15.33
Round  32, Train loss: 1.873, Test loss: 1.794, Test accuracy: 66.47
Round  32, Global train loss: 1.873, Global test loss: 2.282, Global test accuracy: 15.62
Round  33, Train loss: 1.763, Test loss: 1.789, Test accuracy: 67.02
Round  33, Global train loss: 1.763, Global test loss: 2.286, Global test accuracy: 15.17
Round  34, Train loss: 1.722, Test loss: 1.776, Test accuracy: 68.29
Round  34, Global train loss: 1.722, Global test loss: 2.295, Global test accuracy: 14.73
Round  35, Train loss: 1.755, Test loss: 1.772, Test accuracy: 68.69
Round  35, Global train loss: 1.755, Global test loss: 2.294, Global test accuracy: 14.12
Round  36, Train loss: 1.739, Test loss: 1.769, Test accuracy: 69.02
Round  36, Global train loss: 1.739, Global test loss: 2.288, Global test accuracy: 14.57
Round  37, Train loss: 1.840, Test loss: 1.769, Test accuracy: 68.99
Round  37, Global train loss: 1.840, Global test loss: 2.294, Global test accuracy: 13.62
Round  38, Train loss: 1.752, Test loss: 1.768, Test accuracy: 69.13
Round  38, Global train loss: 1.752, Global test loss: 2.283, Global test accuracy: 14.92
Round  39, Train loss: 1.811, Test loss: 1.753, Test accuracy: 70.69
Round  39, Global train loss: 1.811, Global test loss: 2.284, Global test accuracy: 15.14
Round  40, Train loss: 1.777, Test loss: 1.753, Test accuracy: 70.72
Round  40, Global train loss: 1.777, Global test loss: 2.295, Global test accuracy: 14.31
Round  41, Train loss: 1.729, Test loss: 1.747, Test accuracy: 71.30
Round  41, Global train loss: 1.729, Global test loss: 2.285, Global test accuracy: 15.25
Round  42, Train loss: 1.751, Test loss: 1.748, Test accuracy: 71.20
Round  42, Global train loss: 1.751, Global test loss: 2.281, Global test accuracy: 15.64
Round  43, Train loss: 1.761, Test loss: 1.747, Test accuracy: 71.20
Round  43, Global train loss: 1.761, Global test loss: 2.278, Global test accuracy: 15.81
Round  44, Train loss: 1.678, Test loss: 1.747, Test accuracy: 71.17
Round  44, Global train loss: 1.678, Global test loss: 2.279, Global test accuracy: 15.82
Round  45, Train loss: 1.707, Test loss: 1.743, Test accuracy: 71.65
Round  45, Global train loss: 1.707, Global test loss: 2.290, Global test accuracy: 14.55
Round  46, Train loss: 1.715, Test loss: 1.743, Test accuracy: 71.58
Round  46, Global train loss: 1.715, Global test loss: 2.288, Global test accuracy: 15.15
Round  47, Train loss: 1.811, Test loss: 1.747, Test accuracy: 71.12
Round  47, Global train loss: 1.811, Global test loss: 2.277, Global test accuracy: 15.95
Round  48, Train loss: 1.812, Test loss: 1.751, Test accuracy: 70.69
Round  48, Global train loss: 1.812, Global test loss: 2.297, Global test accuracy: 13.28
Round  49, Train loss: 1.741, Test loss: 1.742, Test accuracy: 71.69
Round  49, Global train loss: 1.741, Global test loss: 2.282, Global test accuracy: 15.02
Round  50, Train loss: 1.708, Test loss: 1.746, Test accuracy: 71.22
Round  50, Global train loss: 1.708, Global test loss: 2.277, Global test accuracy: 16.09
Round  51, Train loss: 1.743, Test loss: 1.734, Test accuracy: 72.45
Round  51, Global train loss: 1.743, Global test loss: 2.290, Global test accuracy: 14.14
Round  52, Train loss: 1.727, Test loss: 1.727, Test accuracy: 73.23
Round  52, Global train loss: 1.727, Global test loss: 2.272, Global test accuracy: 16.62
Round  53, Train loss: 1.672, Test loss: 1.726, Test accuracy: 73.35
Round  53, Global train loss: 1.672, Global test loss: 2.283, Global test accuracy: 15.02
Round  54, Train loss: 1.721, Test loss: 1.725, Test accuracy: 73.39
Round  54, Global train loss: 1.721, Global test loss: 2.289, Global test accuracy: 14.46
Round  55, Train loss: 1.753, Test loss: 1.721, Test accuracy: 73.78
Round  55, Global train loss: 1.753, Global test loss: 2.289, Global test accuracy: 14.97
Round  56, Train loss: 1.714, Test loss: 1.719, Test accuracy: 73.96
Round  56, Global train loss: 1.714, Global test loss: 2.301, Global test accuracy: 13.82
Round  57, Train loss: 1.774, Test loss: 1.720, Test accuracy: 73.92
Round  57, Global train loss: 1.774, Global test loss: 2.280, Global test accuracy: 15.90
Round  58, Train loss: 1.773, Test loss: 1.718, Test accuracy: 74.04
Round  58, Global train loss: 1.773, Global test loss: 2.302, Global test accuracy: 12.33
Round  59, Train loss: 1.663, Test loss: 1.718, Test accuracy: 74.07
Round  59, Global train loss: 1.663, Global test loss: 2.290, Global test accuracy: 14.90
Round  60, Train loss: 1.755, Test loss: 1.719, Test accuracy: 74.02
Round  60, Global train loss: 1.755, Global test loss: 2.290, Global test accuracy: 14.37
Round  61, Train loss: 1.651, Test loss: 1.713, Test accuracy: 74.58
Round  61, Global train loss: 1.651, Global test loss: 2.277, Global test accuracy: 16.25
Round  62, Train loss: 1.710, Test loss: 1.715, Test accuracy: 74.39
Round  62, Global train loss: 1.710, Global test loss: 2.286, Global test accuracy: 15.72
Round  63, Train loss: 1.714, Test loss: 1.719, Test accuracy: 73.92
Round  63, Global train loss: 1.714, Global test loss: 2.288, Global test accuracy: 14.61
Round  64, Train loss: 1.698, Test loss: 1.716, Test accuracy: 74.30
Round  64, Global train loss: 1.698, Global test loss: 2.295, Global test accuracy: 14.24
Round  65, Train loss: 1.724, Test loss: 1.714, Test accuracy: 74.53
Round  65, Global train loss: 1.724, Global test loss: 2.284, Global test accuracy: 15.32
Round  66, Train loss: 1.769, Test loss: 1.710, Test accuracy: 74.97
Round  66, Global train loss: 1.769, Global test loss: 2.287, Global test accuracy: 15.00
Round  67, Train loss: 1.735, Test loss: 1.712, Test accuracy: 74.70
Round  67, Global train loss: 1.735, Global test loss: 2.286, Global test accuracy: 14.94
Round  68, Train loss: 1.741, Test loss: 1.709, Test accuracy: 75.11
Round  68, Global train loss: 1.741, Global test loss: 2.279, Global test accuracy: 15.63
Round  69, Train loss: 1.691, Test loss: 1.709, Test accuracy: 75.09
Round  69, Global train loss: 1.691, Global test loss: 2.291, Global test accuracy: 15.02
Round  70, Train loss: 1.665, Test loss: 1.713, Test accuracy: 74.69
Round  70, Global train loss: 1.665, Global test loss: 2.282, Global test accuracy: 15.81
Round  71, Train loss: 1.683, Test loss: 1.712, Test accuracy: 74.72
Round  71, Global train loss: 1.683, Global test loss: 2.296, Global test accuracy: 14.12
Round  72, Train loss: 1.728, Test loss: 1.720, Test accuracy: 73.88
Round  72, Global train loss: 1.728, Global test loss: 2.295, Global test accuracy: 13.94
Round  73, Train loss: 1.754, Test loss: 1.712, Test accuracy: 74.73
Round  73, Global train loss: 1.754, Global test loss: 2.293, Global test accuracy: 14.77
Round  74, Train loss: 1.687, Test loss: 1.712, Test accuracy: 74.73
Round  74, Global train loss: 1.687, Global test loss: 2.284, Global test accuracy: 14.99
Round  75, Train loss: 1.708, Test loss: 1.707, Test accuracy: 75.19
Round  75, Global train loss: 1.708, Global test loss: 2.276, Global test accuracy: 16.35
Round  76, Train loss: 1.690, Test loss: 1.707, Test accuracy: 75.22
Round  76, Global train loss: 1.690, Global test loss: 2.276, Global test accuracy: 16.52
Round  77, Train loss: 1.680, Test loss: 1.708, Test accuracy: 75.13
Round  77, Global train loss: 1.680, Global test loss: 2.274, Global test accuracy: 16.26
Round  78, Train loss: 1.658, Test loss: 1.703, Test accuracy: 75.64
Round  78, Global train loss: 1.658, Global test loss: 2.281, Global test accuracy: 15.74
Round  79, Train loss: 1.701, Test loss: 1.699, Test accuracy: 76.03
Round  79, Global train loss: 1.701, Global test loss: 2.284, Global test accuracy: 15.14
Round  80, Train loss: 1.642, Test loss: 1.704, Test accuracy: 75.55
Round  80, Global train loss: 1.642, Global test loss: 2.282, Global test accuracy: 15.79
Round  81, Train loss: 1.739, Test loss: 1.705, Test accuracy: 75.40
Round  81, Global train loss: 1.739, Global test loss: 2.287, Global test accuracy: 14.61
Round  82, Train loss: 1.692, Test loss: 1.697, Test accuracy: 76.33
Round  82, Global train loss: 1.692, Global test loss: 2.284, Global test accuracy: 15.53
Round  83, Train loss: 1.657, Test loss: 1.696, Test accuracy: 76.33
Round  83, Global train loss: 1.657, Global test loss: 2.288, Global test accuracy: 15.28
Round  84, Train loss: 1.693, Test loss: 1.696, Test accuracy: 76.34
Round  84, Global train loss: 1.693, Global test loss: 2.289, Global test accuracy: 15.16
Round  85, Train loss: 1.656, Test loss: 1.692, Test accuracy: 76.77
Round  85, Global train loss: 1.656, Global test loss: 2.297, Global test accuracy: 13.78
Round  86, Train loss: 1.673, Test loss: 1.687, Test accuracy: 77.28
Round  86, Global train loss: 1.673, Global test loss: 2.284, Global test accuracy: 15.03
Round  87, Train loss: 1.700, Test loss: 1.682, Test accuracy: 77.74
Round  87, Global train loss: 1.700, Global test loss: 2.285, Global test accuracy: 15.10
Round  88, Train loss: 1.675, Test loss: 1.683, Test accuracy: 77.67
Round  88, Global train loss: 1.675, Global test loss: 2.292, Global test accuracy: 14.81
Round  89, Train loss: 1.661, Test loss: 1.678, Test accuracy: 78.18
Round  89, Global train loss: 1.661, Global test loss: 2.295, Global test accuracy: 13.79
Round  90, Train loss: 1.736, Test loss: 1.683, Test accuracy: 77.70
Round  90, Global train loss: 1.736, Global test loss: 2.288, Global test accuracy: 14.53
Round  91, Train loss: 1.715, Test loss: 1.690, Test accuracy: 76.96
Round  91, Global train loss: 1.715, Global test loss: 2.275, Global test accuracy: 16.19
Round  92, Train loss: 1.675, Test loss: 1.687, Test accuracy: 77.28
Round  92, Global train loss: 1.675, Global test loss: 2.285, Global test accuracy: 15.48
Round  93, Train loss: 1.672, Test loss: 1.682, Test accuracy: 77.81
Round  93, Global train loss: 1.672, Global test loss: 2.298, Global test accuracy: 13.95
Round  94, Train loss: 1.658, Test loss: 1.680, Test accuracy: 77.95
Round  94, Global train loss: 1.658, Global test loss: 2.272, Global test accuracy: 17.29
Round  95, Train loss: 1.652, Test loss: 1.680, Test accuracy: 77.99
Round  95, Global train loss: 1.652, Global test loss: 2.288, Global test accuracy: 14.27/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.606, Test loss: 1.677, Test accuracy: 78.31
Round  96, Global train loss: 1.606, Global test loss: 2.278, Global test accuracy: 16.61
Round  97, Train loss: 1.681, Test loss: 1.672, Test accuracy: 78.83
Round  97, Global train loss: 1.681, Global test loss: 2.284, Global test accuracy: 15.18
Round  98, Train loss: 1.614, Test loss: 1.672, Test accuracy: 78.88
Round  98, Global train loss: 1.614, Global test loss: 2.278, Global test accuracy: 16.45
Round  99, Train loss: 1.643, Test loss: 1.671, Test accuracy: 78.89
Round  99, Global train loss: 1.643, Global test loss: 2.294, Global test accuracy: 14.40
Final Round, Train loss: 1.692, Test loss: 1.708, Test accuracy: 75.41
Final Round, Global train loss: 1.692, Global test loss: 2.294, Global test accuracy: 14.40
Average accuracy final 10 rounds: 78.0595 

Average global accuracy final 10 rounds: 15.434 

5280.736265897751
[3.379554033279419, 6.759108066558838, 9.956866025924683, 13.154623985290527, 16.46398949623108, 19.77335500717163, 23.042120456695557, 26.310885906219482, 29.52164125442505, 32.732396602630615, 35.88644242286682, 39.04048824310303, 42.04981732368469, 45.05914640426636, 48.19964098930359, 51.34013557434082, 54.52819085121155, 57.716246128082275, 60.921857595443726, 64.12746906280518, 67.41546273231506, 70.70345640182495, 73.99449014663696, 77.28552389144897, 80.46410322189331, 83.64268255233765, 86.86051368713379, 90.07834482192993, 93.25364708900452, 96.4289493560791, 99.58893752098083, 102.74892568588257, 105.93178582191467, 109.11464595794678, 112.36733102798462, 115.62001609802246, 118.82160973548889, 122.02320337295532, 125.2349865436554, 128.44676971435547, 131.59895753860474, 134.751145362854, 137.84916019439697, 140.94717502593994, 144.0362639427185, 147.12535285949707, 150.27386713027954, 153.422381401062, 156.63662123680115, 159.85086107254028, 162.93786692619324, 166.0248727798462, 169.1184365749359, 172.21200037002563, 175.38770651817322, 178.5634126663208, 182.03954601287842, 185.51567935943604, 188.53805661201477, 191.5604338645935, 194.56055784225464, 197.56068181991577, 200.78868508338928, 204.0166883468628, 207.21944642066956, 210.42220449447632, 213.63429617881775, 216.84638786315918, 219.965478181839, 223.0845685005188, 226.2642879486084, 229.444007396698, 232.66234135627747, 235.88067531585693, 239.03323006629944, 242.18578481674194, 245.4659788608551, 248.74617290496826, 251.9858365058899, 255.22550010681152, 258.53192591667175, 261.838351726532, 265.00220799446106, 268.16606426239014, 271.3700518608093, 274.5740394592285, 277.7466170787811, 280.91919469833374, 284.03585839271545, 287.15252208709717, 290.3977196216583, 293.6429171562195, 297.0166337490082, 300.3903503417969, 303.66904640197754, 306.9477424621582, 310.1475839614868, 313.34742546081543, 316.5855176448822, 319.823609828949, 322.99528551101685, 326.1669611930847, 329.4642038345337, 332.76144647598267, 335.94918394088745, 339.13692140579224, 342.3887619972229, 345.64060258865356, 348.8603444099426, 352.0800862312317, 355.4062077999115, 358.7323293685913, 362.07355546951294, 365.41478157043457, 368.6214406490326, 371.8280997276306, 375.14209032058716, 378.4560809135437, 381.700407743454, 384.94473457336426, 388.28464794158936, 391.62456130981445, 395.00930881500244, 398.39405632019043, 401.6150074005127, 404.83595848083496, 408.1430010795593, 411.4500436782837, 414.7085151672363, 417.96698665618896, 421.1640794277191, 424.36117219924927, 427.36729168891907, 430.37341117858887, 433.51077246665955, 436.6481337547302, 439.8861961364746, 443.124258518219, 446.4544954299927, 449.78473234176636, 453.0790047645569, 456.3732771873474, 459.6770405769348, 462.9808039665222, 466.37621688842773, 469.77162981033325, 473.1636281013489, 476.5556263923645, 479.88360619544983, 483.21158599853516, 486.40211939811707, 489.592652797699, 492.87881350517273, 496.1649742126465, 499.55057883262634, 502.9361834526062, 506.2622697353363, 509.5883560180664, 512.9905822277069, 516.3928084373474, 519.6438028812408, 522.8947973251343, 526.2298278808594, 529.5648584365845, 532.819057226181, 536.0732560157776, 539.253082036972, 542.4329080581665, 545.5707609653473, 548.7086138725281, 551.9216110706329, 555.1346082687378, 558.5173630714417, 561.9001178741455, 565.282187461853, 568.6642570495605, 572.0575437545776, 575.4508304595947, 578.6194536685944, 581.788076877594, 584.8449003696442, 587.9017238616943, 591.0693218708038, 594.2369198799133, 597.560384273529, 600.8838486671448, 604.1513695716858, 607.4188904762268, 610.6115589141846, 613.8042273521423, 616.9068574905396, 620.0094876289368, 623.1126611232758, 626.2158346176147, 629.3728709220886, 632.5299072265625, 635.8024344444275, 639.0749616622925, 642.3590395450592, 645.6431174278259, 647.3036625385284, 648.964207649231]
[18.3675, 18.3675, 31.845, 31.845, 39.2775, 39.2775, 43.76, 43.76, 48.29, 48.29, 52.74, 52.74, 52.0775, 52.0775, 52.82, 52.82, 53.1625, 53.1625, 52.83, 52.83, 53.025, 53.025, 54.2525, 54.2525, 55.94, 55.94, 58.615, 58.615, 57.915, 57.915, 57.71, 57.71, 60.5475, 60.5475, 60.165, 60.165, 60.7975, 60.7975, 60.2375, 60.2375, 61.3, 61.3, 61.4225, 61.4225, 60.9275, 60.9275, 61.69, 61.69, 60.9875, 60.9875, 60.9875, 60.9875, 61.415, 61.415, 63.115, 63.115, 63.5325, 63.5325, 65.625, 65.625, 66.27, 66.27, 67.5425, 67.5425, 66.47, 66.47, 67.015, 67.015, 68.2925, 68.2925, 68.6925, 68.6925, 69.02, 69.02, 68.9925, 68.9925, 69.1275, 69.1275, 70.6875, 70.6875, 70.715, 70.715, 71.3, 71.3, 71.205, 71.205, 71.2025, 71.2025, 71.1725, 71.1725, 71.6475, 71.6475, 71.585, 71.585, 71.1175, 71.1175, 70.6875, 70.6875, 71.6875, 71.6875, 71.215, 71.215, 72.4525, 72.4525, 73.2325, 73.2325, 73.3475, 73.3475, 73.39, 73.39, 73.7825, 73.7825, 73.96, 73.96, 73.92, 73.92, 74.0425, 74.0425, 74.0675, 74.0675, 74.0175, 74.0175, 74.5775, 74.5775, 74.39, 74.39, 73.9175, 73.9175, 74.295, 74.295, 74.5275, 74.5275, 74.9725, 74.9725, 74.705, 74.705, 75.1125, 75.1125, 75.09, 75.09, 74.6875, 74.6875, 74.72, 74.72, 73.875, 73.875, 74.7275, 74.7275, 74.735, 74.735, 75.1925, 75.1925, 75.2175, 75.2175, 75.1325, 75.1325, 75.6425, 75.6425, 76.025, 76.025, 75.5525, 75.5525, 75.4, 75.4, 76.3275, 76.3275, 76.3275, 76.3275, 76.3375, 76.3375, 76.765, 76.765, 77.28, 77.28, 77.7425, 77.7425, 77.675, 77.675, 78.1825, 78.1825, 77.6975, 77.6975, 76.9625, 76.9625, 77.285, 77.285, 77.81, 77.81, 77.9525, 77.9525, 77.9875, 77.9875, 78.3075, 78.3075, 78.8275, 78.8275, 78.875, 78.875, 78.89, 78.89, 75.405, 75.405]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.302, Test accuracy: 10.61
Round   1, Train loss: 2.289, Test loss: 2.300, Test accuracy: 11.08
Round   2, Train loss: 2.249, Test loss: 2.282, Test accuracy: 15.77
Round   3, Train loss: 2.135, Test loss: 2.226, Test accuracy: 22.23
Round   4, Train loss: 2.016, Test loss: 2.151, Test accuracy: 30.04
Round   5, Train loss: 1.990, Test loss: 2.108, Test accuracy: 34.85
Round   6, Train loss: 2.016, Test loss: 2.064, Test accuracy: 39.72
Round   7, Train loss: 1.978, Test loss: 2.008, Test accuracy: 45.26
Round   8, Train loss: 1.978, Test loss: 1.999, Test accuracy: 46.23
Round   9, Train loss: 1.969, Test loss: 1.966, Test accuracy: 49.81
Round  10, Train loss: 1.942, Test loss: 1.945, Test accuracy: 51.95
Round  11, Train loss: 1.953, Test loss: 1.937, Test accuracy: 52.61
Round  12, Train loss: 1.910, Test loss: 1.931, Test accuracy: 53.20
Round  13, Train loss: 1.879, Test loss: 1.919, Test accuracy: 54.34
Round  14, Train loss: 1.928, Test loss: 1.915, Test accuracy: 54.61
Round  15, Train loss: 1.894, Test loss: 1.910, Test accuracy: 55.19
Round  16, Train loss: 1.851, Test loss: 1.910, Test accuracy: 55.22
Round  17, Train loss: 1.868, Test loss: 1.900, Test accuracy: 56.21
Round  18, Train loss: 1.827, Test loss: 1.887, Test accuracy: 57.39
Round  19, Train loss: 1.818, Test loss: 1.884, Test accuracy: 57.63
Round  20, Train loss: 1.804, Test loss: 1.883, Test accuracy: 57.79
Round  21, Train loss: 1.866, Test loss: 1.881, Test accuracy: 57.83
Round  22, Train loss: 1.827, Test loss: 1.878, Test accuracy: 58.17
Round  23, Train loss: 1.834, Test loss: 1.876, Test accuracy: 58.35
Round  24, Train loss: 1.880, Test loss: 1.874, Test accuracy: 58.53
Round  25, Train loss: 1.838, Test loss: 1.869, Test accuracy: 59.03
Round  26, Train loss: 1.856, Test loss: 1.868, Test accuracy: 59.10
Round  27, Train loss: 1.819, Test loss: 1.863, Test accuracy: 59.60
Round  28, Train loss: 1.842, Test loss: 1.863, Test accuracy: 59.67
Round  29, Train loss: 1.856, Test loss: 1.862, Test accuracy: 59.69
Round  30, Train loss: 1.860, Test loss: 1.862, Test accuracy: 59.67
Round  31, Train loss: 1.894, Test loss: 1.852, Test accuracy: 60.64
Round  32, Train loss: 1.831, Test loss: 1.851, Test accuracy: 60.73
Round  33, Train loss: 1.779, Test loss: 1.850, Test accuracy: 60.88
Round  34, Train loss: 1.808, Test loss: 1.850, Test accuracy: 60.86
Round  35, Train loss: 1.827, Test loss: 1.844, Test accuracy: 61.46
Round  36, Train loss: 1.887, Test loss: 1.839, Test accuracy: 61.97
Round  37, Train loss: 1.789, Test loss: 1.832, Test accuracy: 62.68
Round  38, Train loss: 1.810, Test loss: 1.830, Test accuracy: 62.83
Round  39, Train loss: 1.787, Test loss: 1.826, Test accuracy: 63.35
Round  40, Train loss: 1.721, Test loss: 1.820, Test accuracy: 63.91
Round  41, Train loss: 1.800, Test loss: 1.819, Test accuracy: 63.89
Round  42, Train loss: 1.813, Test loss: 1.818, Test accuracy: 64.02
Round  43, Train loss: 1.810, Test loss: 1.817, Test accuracy: 64.12
Round  44, Train loss: 1.790, Test loss: 1.817, Test accuracy: 64.12
Round  45, Train loss: 1.855, Test loss: 1.813, Test accuracy: 64.51
Round  46, Train loss: 1.794, Test loss: 1.813, Test accuracy: 64.56
Round  47, Train loss: 1.821, Test loss: 1.812, Test accuracy: 64.60
Round  48, Train loss: 1.852, Test loss: 1.812, Test accuracy: 64.59
Round  49, Train loss: 1.802, Test loss: 1.807, Test accuracy: 65.08
Round  50, Train loss: 1.818, Test loss: 1.803, Test accuracy: 65.63
Round  51, Train loss: 1.836, Test loss: 1.796, Test accuracy: 66.36
Round  52, Train loss: 1.768, Test loss: 1.794, Test accuracy: 66.48
Round  53, Train loss: 1.742, Test loss: 1.793, Test accuracy: 66.57
Round  54, Train loss: 1.723, Test loss: 1.788, Test accuracy: 67.10
Round  55, Train loss: 1.767, Test loss: 1.784, Test accuracy: 67.47
Round  56, Train loss: 1.752, Test loss: 1.783, Test accuracy: 67.55
Round  57, Train loss: 1.735, Test loss: 1.782, Test accuracy: 67.58
Round  58, Train loss: 1.702, Test loss: 1.782, Test accuracy: 67.61
Round  59, Train loss: 1.813, Test loss: 1.781, Test accuracy: 67.67
Round  60, Train loss: 1.720, Test loss: 1.781, Test accuracy: 67.70
Round  61, Train loss: 1.712, Test loss: 1.778, Test accuracy: 68.11
Round  62, Train loss: 1.705, Test loss: 1.777, Test accuracy: 68.14
Round  63, Train loss: 1.779, Test loss: 1.776, Test accuracy: 68.17
Round  64, Train loss: 1.815, Test loss: 1.772, Test accuracy: 68.67
Round  65, Train loss: 1.687, Test loss: 1.770, Test accuracy: 68.73
Round  66, Train loss: 1.707, Test loss: 1.772, Test accuracy: 68.56
Round  67, Train loss: 1.697, Test loss: 1.762, Test accuracy: 69.66
Round  68, Train loss: 1.668, Test loss: 1.762, Test accuracy: 69.63
Round  69, Train loss: 1.676, Test loss: 1.757, Test accuracy: 70.11
Round  70, Train loss: 1.692, Test loss: 1.756, Test accuracy: 70.19
Round  71, Train loss: 1.724, Test loss: 1.747, Test accuracy: 71.15
Round  72, Train loss: 1.781, Test loss: 1.747, Test accuracy: 71.17
Round  73, Train loss: 1.646, Test loss: 1.746, Test accuracy: 71.25
Round  74, Train loss: 1.696, Test loss: 1.743, Test accuracy: 71.59
Round  75, Train loss: 1.724, Test loss: 1.742, Test accuracy: 71.59
Round  76, Train loss: 1.649, Test loss: 1.742, Test accuracy: 71.67
Round  77, Train loss: 1.684, Test loss: 1.742, Test accuracy: 71.65
Round  78, Train loss: 1.648, Test loss: 1.741, Test accuracy: 71.72
Round  79, Train loss: 1.713, Test loss: 1.737, Test accuracy: 72.09
Round  80, Train loss: 1.728, Test loss: 1.737, Test accuracy: 72.13
Round  81, Train loss: 1.716, Test loss: 1.732, Test accuracy: 72.72
Round  82, Train loss: 1.671, Test loss: 1.728, Test accuracy: 73.12
Round  83, Train loss: 1.680, Test loss: 1.727, Test accuracy: 73.06
Round  84, Train loss: 1.683, Test loss: 1.727, Test accuracy: 73.15
Round  85, Train loss: 1.730, Test loss: 1.726, Test accuracy: 73.19
Round  86, Train loss: 1.728, Test loss: 1.726, Test accuracy: 73.18
Round  87, Train loss: 1.719, Test loss: 1.726, Test accuracy: 73.24
Round  88, Train loss: 1.713, Test loss: 1.725, Test accuracy: 73.30
Round  89, Train loss: 1.763, Test loss: 1.725, Test accuracy: 73.25
Round  90, Train loss: 1.732, Test loss: 1.725, Test accuracy: 73.30
Round  91, Train loss: 1.680, Test loss: 1.724, Test accuracy: 73.33
Round  92, Train loss: 1.695, Test loss: 1.724, Test accuracy: 73.33
Round  93, Train loss: 1.733, Test loss: 1.720, Test accuracy: 73.81
Round  94, Train loss: 1.695, Test loss: 1.720, Test accuracy: 73.77
Round  95, Train loss: 1.665, Test loss: 1.720, Test accuracy: 73.81
Round  96, Train loss: 1.671, Test loss: 1.716, Test accuracy: 74.29
Round  97, Train loss: 1.729, Test loss: 1.715, Test accuracy: 74.35
Round  98, Train loss: 1.712, Test loss: 1.715, Test accuracy: 74.34/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.711, Test loss: 1.715, Test accuracy: 74.31
Final Round, Train loss: 1.694, Test loss: 1.711, Test accuracy: 74.72
Average accuracy final 10 rounds: 73.86375000000001 

4241.142031908035
[3.4783267974853516, 6.956653594970703, 10.38325548171997, 13.809857368469238, 16.97911834716797, 20.1483793258667, 23.116767406463623, 26.085155487060547, 29.178598880767822, 32.2720422744751, 35.50261831283569, 38.73319435119629, 41.89841055870056, 45.063626766204834, 48.13518142700195, 51.20673608779907, 54.38483500480652, 57.562933921813965, 60.66414451599121, 63.76535511016846, 66.9830482006073, 70.20074129104614, 73.34228467941284, 76.48382806777954, 79.78796339035034, 83.09209871292114, 86.37263917922974, 89.65317964553833, 92.94619274139404, 96.23920583724976, 99.41079640388489, 102.58238697052002, 105.85317134857178, 109.12395572662354, 112.45030450820923, 115.77665328979492, 119.25457620620728, 122.73249912261963, 126.15067481994629, 129.56885051727295, 132.9328100681305, 136.29676961898804, 139.41432094573975, 142.53187227249146, 145.6378104686737, 148.74374866485596, 151.80436158180237, 154.86497449874878, 157.99952840805054, 161.1340823173523, 164.22515153884888, 167.31622076034546, 170.38684582710266, 173.45747089385986, 176.50676846504211, 179.55606603622437, 182.72560238838196, 185.89513874053955, 189.17514657974243, 192.4551544189453, 195.63159227371216, 198.808030128479, 201.99014520645142, 205.17226028442383, 208.3833429813385, 211.59442567825317, 214.77937602996826, 217.96432638168335, 221.05976915359497, 224.1552119255066, 227.25049328804016, 230.34577465057373, 233.52513670921326, 236.70449876785278, 239.93928718566895, 243.1740756034851, 246.12455368041992, 249.07503175735474, 252.14086270332336, 255.206693649292, 258.35057163238525, 261.4944496154785, 264.5733473300934, 267.65224504470825, 271.09347224235535, 274.53469944000244, 278.08748483657837, 281.6402702331543, 285.0481524467468, 288.45603466033936, 291.86871671676636, 295.28139877319336, 298.73891949653625, 302.19644021987915, 305.4072468280792, 308.6180534362793, 312.2212646007538, 315.82447576522827, 319.37556886672974, 322.9266619682312, 326.5043451786041, 330.08202838897705, 333.55278182029724, 337.02353525161743, 340.4238886833191, 343.82424211502075, 347.1901800632477, 350.5561180114746, 353.82205605506897, 357.08799409866333, 360.30395770072937, 363.5199213027954, 366.8322842121124, 370.14464712142944, 373.39781618118286, 376.6509852409363, 379.74156045913696, 382.83213567733765, 385.9933843612671, 389.15463304519653, 392.6917223930359, 396.22881174087524, 399.51992774009705, 402.81104373931885, 405.9358766078949, 409.06070947647095, 412.3002245426178, 415.53973960876465, 418.6060583591461, 421.6723771095276, 424.9025242328644, 428.1326713562012, 431.4734981060028, 434.81432485580444, 438.14471411705017, 441.4751033782959, 444.6558873653412, 447.8366713523865, 450.88417053222656, 453.93166971206665, 457.1114921569824, 460.2913146018982, 463.52860856056213, 466.7659025192261, 470.0461642742157, 473.3264260292053, 476.57613158226013, 479.82583713531494, 482.8854637145996, 485.9450902938843, 489.1623876094818, 492.37968492507935, 495.60479378700256, 498.8299026489258, 502.12877225875854, 505.4276418685913, 508.68649220466614, 511.94534254074097, 515.0563521385193, 518.1673617362976, 521.3635098934174, 524.5596580505371, 527.7842571735382, 531.0088562965393, 534.23930311203, 537.4697499275208, 540.656455039978, 543.8431601524353, 547.0365645885468, 550.2299690246582, 553.2622611522675, 556.2945532798767, 559.5414319038391, 562.7883105278015, 566.3082029819489, 569.8280954360962, 573.3091866970062, 576.7902779579163, 579.9461710453033, 583.1020641326904, 586.2415370941162, 589.381010055542, 592.4696137905121, 595.5582175254822, 598.7808141708374, 602.0034108161926, 605.2346060276031, 608.4658012390137, 611.7381687164307, 615.0105361938477, 617.9641227722168, 620.9177093505859, 624.0322153568268, 627.1467213630676, 630.4302296638489, 633.7137379646301, 636.8748199939728, 640.0359020233154, 643.1742873191833, 646.3126726150513, 647.8163197040558, 649.3199667930603]
[10.61, 10.61, 11.08, 11.08, 15.765, 15.765, 22.2325, 22.2325, 30.0375, 30.0375, 34.85, 34.85, 39.7175, 39.7175, 45.2625, 45.2625, 46.23, 46.23, 49.8125, 49.8125, 51.955, 51.955, 52.6125, 52.6125, 53.2025, 53.2025, 54.345, 54.345, 54.6075, 54.6075, 55.1875, 55.1875, 55.2175, 55.2175, 56.2075, 56.2075, 57.3925, 57.3925, 57.63, 57.63, 57.7875, 57.7875, 57.8325, 57.8325, 58.175, 58.175, 58.35, 58.35, 58.5325, 58.5325, 59.0275, 59.0275, 59.1, 59.1, 59.6, 59.6, 59.6675, 59.6675, 59.685, 59.685, 59.67, 59.67, 60.6375, 60.6375, 60.7325, 60.7325, 60.88, 60.88, 60.86, 60.86, 61.4575, 61.4575, 61.9725, 61.9725, 62.6825, 62.6825, 62.825, 62.825, 63.35, 63.35, 63.91, 63.91, 63.8925, 63.8925, 64.02, 64.02, 64.1175, 64.1175, 64.1175, 64.1175, 64.51, 64.51, 64.555, 64.555, 64.6, 64.6, 64.5925, 64.5925, 65.075, 65.075, 65.63, 65.63, 66.365, 66.365, 66.485, 66.485, 66.57, 66.57, 67.1025, 67.1025, 67.475, 67.475, 67.55, 67.55, 67.575, 67.575, 67.615, 67.615, 67.6725, 67.6725, 67.7025, 67.7025, 68.105, 68.105, 68.1425, 68.1425, 68.17, 68.17, 68.6675, 68.6675, 68.7325, 68.7325, 68.5625, 68.5625, 69.655, 69.655, 69.6275, 69.6275, 70.1125, 70.1125, 70.195, 70.195, 71.1475, 71.1475, 71.1725, 71.1725, 71.2525, 71.2525, 71.5925, 71.5925, 71.5925, 71.5925, 71.6675, 71.6675, 71.65, 71.65, 71.7225, 71.7225, 72.09, 72.09, 72.1275, 72.1275, 72.715, 72.715, 73.12, 73.12, 73.0625, 73.0625, 73.1525, 73.1525, 73.185, 73.185, 73.18, 73.18, 73.2425, 73.2425, 73.2975, 73.2975, 73.2525, 73.2525, 73.2975, 73.2975, 73.3275, 73.3275, 73.33, 73.33, 73.81, 73.81, 73.765, 73.765, 73.815, 73.815, 74.2925, 74.2925, 74.3525, 74.3525, 74.34, 74.34, 74.3075, 74.3075, 74.715, 74.715]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.277, Test loss: 2.295, Test accuracy: 13.39
Round   1, Train loss: 2.078, Test loss: 2.269, Test accuracy: 17.32
Round   2, Train loss: 1.948, Test loss: 2.228, Test accuracy: 21.97
Round   3, Train loss: 1.965, Test loss: 2.191, Test accuracy: 26.55
Round   4, Train loss: 1.921, Test loss: 2.147, Test accuracy: 30.88
Round   5, Train loss: 1.830, Test loss: 2.118, Test accuracy: 34.19
Round   6, Train loss: 1.919, Test loss: 2.077, Test accuracy: 38.56
Round   7, Train loss: 1.878, Test loss: 2.056, Test accuracy: 40.93
Round   8, Train loss: 1.925, Test loss: 2.043, Test accuracy: 41.86
Round   9, Train loss: 1.936, Test loss: 2.000, Test accuracy: 46.86
Round  10, Train loss: 1.916, Test loss: 1.969, Test accuracy: 49.77
Round  11, Train loss: 1.905, Test loss: 1.942, Test accuracy: 52.75
Round  12, Train loss: 1.817, Test loss: 1.916, Test accuracy: 55.12
Round  13, Train loss: 1.754, Test loss: 1.908, Test accuracy: 55.61
Round  14, Train loss: 1.796, Test loss: 1.899, Test accuracy: 56.50
Round  15, Train loss: 1.852, Test loss: 1.886, Test accuracy: 58.15
Round  16, Train loss: 1.800, Test loss: 1.871, Test accuracy: 59.47
Round  17, Train loss: 1.775, Test loss: 1.864, Test accuracy: 60.01
Round  18, Train loss: 1.892, Test loss: 1.866, Test accuracy: 59.58
Round  19, Train loss: 1.755, Test loss: 1.851, Test accuracy: 61.15
Round  20, Train loss: 1.830, Test loss: 1.846, Test accuracy: 61.63
Round  21, Train loss: 1.831, Test loss: 1.850, Test accuracy: 61.20
Round  22, Train loss: 1.847, Test loss: 1.844, Test accuracy: 61.78
Round  23, Train loss: 1.766, Test loss: 1.836, Test accuracy: 62.54
Round  24, Train loss: 1.752, Test loss: 1.835, Test accuracy: 62.64
Round  25, Train loss: 1.775, Test loss: 1.833, Test accuracy: 62.66
Round  26, Train loss: 1.871, Test loss: 1.828, Test accuracy: 63.17
Round  27, Train loss: 1.829, Test loss: 1.826, Test accuracy: 63.38
Round  28, Train loss: 1.762, Test loss: 1.822, Test accuracy: 63.74
Round  29, Train loss: 1.774, Test loss: 1.825, Test accuracy: 63.47
Round  30, Train loss: 1.835, Test loss: 1.821, Test accuracy: 63.81
Round  31, Train loss: 1.807, Test loss: 1.817, Test accuracy: 64.18
Round  32, Train loss: 1.776, Test loss: 1.815, Test accuracy: 64.39
Round  33, Train loss: 1.802, Test loss: 1.810, Test accuracy: 64.89
Round  34, Train loss: 1.765, Test loss: 1.811, Test accuracy: 64.86
Round  35, Train loss: 1.716, Test loss: 1.809, Test accuracy: 65.06
Round  36, Train loss: 1.811, Test loss: 1.809, Test accuracy: 64.99
Round  37, Train loss: 1.856, Test loss: 1.805, Test accuracy: 65.34
Round  38, Train loss: 1.822, Test loss: 1.802, Test accuracy: 65.62
Round  39, Train loss: 1.807, Test loss: 1.803, Test accuracy: 65.58
Round  40, Train loss: 1.709, Test loss: 1.802, Test accuracy: 65.46
Round  41, Train loss: 1.800, Test loss: 1.788, Test accuracy: 67.16
Round  42, Train loss: 1.762, Test loss: 1.784, Test accuracy: 67.49
Round  43, Train loss: 1.757, Test loss: 1.781, Test accuracy: 67.82
Round  44, Train loss: 1.772, Test loss: 1.779, Test accuracy: 67.93
Round  45, Train loss: 1.730, Test loss: 1.774, Test accuracy: 68.54
Round  46, Train loss: 1.783, Test loss: 1.773, Test accuracy: 68.61
Round  47, Train loss: 1.666, Test loss: 1.772, Test accuracy: 68.65
Round  48, Train loss: 1.731, Test loss: 1.771, Test accuracy: 68.72
Round  49, Train loss: 1.695, Test loss: 1.770, Test accuracy: 68.76
Round  50, Train loss: 1.758, Test loss: 1.768, Test accuracy: 69.00
Round  51, Train loss: 1.757, Test loss: 1.768, Test accuracy: 68.94
Round  52, Train loss: 1.744, Test loss: 1.766, Test accuracy: 69.23
Round  53, Train loss: 1.782, Test loss: 1.765, Test accuracy: 69.26
Round  54, Train loss: 1.735, Test loss: 1.763, Test accuracy: 69.46
Round  55, Train loss: 1.789, Test loss: 1.764, Test accuracy: 69.33
Round  56, Train loss: 1.730, Test loss: 1.762, Test accuracy: 69.58
Round  57, Train loss: 1.711, Test loss: 1.761, Test accuracy: 69.59
Round  58, Train loss: 1.687, Test loss: 1.762, Test accuracy: 69.60
Round  59, Train loss: 1.719, Test loss: 1.760, Test accuracy: 69.71
Round  60, Train loss: 1.739, Test loss: 1.760, Test accuracy: 69.73
Round  61, Train loss: 1.783, Test loss: 1.760, Test accuracy: 69.70
Round  62, Train loss: 1.731, Test loss: 1.759, Test accuracy: 69.73
Round  63, Train loss: 1.660, Test loss: 1.759, Test accuracy: 69.78
Round  64, Train loss: 1.778, Test loss: 1.756, Test accuracy: 70.12
Round  65, Train loss: 1.685, Test loss: 1.755, Test accuracy: 70.18
Round  66, Train loss: 1.674, Test loss: 1.755, Test accuracy: 70.13
Round  67, Train loss: 1.704, Test loss: 1.754, Test accuracy: 70.30
Round  68, Train loss: 1.724, Test loss: 1.750, Test accuracy: 70.79
Round  69, Train loss: 1.821, Test loss: 1.750, Test accuracy: 70.71
Round  70, Train loss: 1.682, Test loss: 1.749, Test accuracy: 70.80
Round  71, Train loss: 1.649, Test loss: 1.749, Test accuracy: 70.75
Round  72, Train loss: 1.722, Test loss: 1.749, Test accuracy: 70.80
Round  73, Train loss: 1.750, Test loss: 1.744, Test accuracy: 71.23
Round  74, Train loss: 1.707, Test loss: 1.743, Test accuracy: 71.36
Round  75, Train loss: 1.735, Test loss: 1.743, Test accuracy: 71.34
Round  76, Train loss: 1.744, Test loss: 1.743, Test accuracy: 71.36
Round  77, Train loss: 1.668, Test loss: 1.743, Test accuracy: 71.39
Round  78, Train loss: 1.708, Test loss: 1.743, Test accuracy: 71.42
Round  79, Train loss: 1.734, Test loss: 1.742, Test accuracy: 71.40
Round  80, Train loss: 1.695, Test loss: 1.742, Test accuracy: 71.42
Round  81, Train loss: 1.720, Test loss: 1.742, Test accuracy: 71.39
Round  82, Train loss: 1.671, Test loss: 1.743, Test accuracy: 71.39
Round  83, Train loss: 1.753, Test loss: 1.742, Test accuracy: 71.42
Round  84, Train loss: 1.750, Test loss: 1.742, Test accuracy: 71.48
Round  85, Train loss: 1.736, Test loss: 1.742, Test accuracy: 71.47
Round  86, Train loss: 1.659, Test loss: 1.742, Test accuracy: 71.42
Round  87, Train loss: 1.763, Test loss: 1.738, Test accuracy: 71.86
Round  88, Train loss: 1.736, Test loss: 1.736, Test accuracy: 72.06
Round  89, Train loss: 1.717, Test loss: 1.736, Test accuracy: 72.00
Round  90, Train loss: 1.716, Test loss: 1.731, Test accuracy: 72.59
Round  91, Train loss: 1.799, Test loss: 1.734, Test accuracy: 72.38
Round  92, Train loss: 1.676, Test loss: 1.730, Test accuracy: 72.63
Round  93, Train loss: 1.668, Test loss: 1.730, Test accuracy: 72.66
Round  94, Train loss: 1.670, Test loss: 1.730, Test accuracy: 72.68
Round  95, Train loss: 1.741, Test loss: 1.727, Test accuracy: 73.03
Round  96, Train loss: 1.733, Test loss: 1.727, Test accuracy: 73.02
Round  97, Train loss: 1.762, Test loss: 1.726, Test accuracy: 73.11
Round  98, Train loss: 1.738, Test loss: 1.726, Test accuracy: 73.13/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.684, Test loss: 1.726, Test accuracy: 73.13
Final Round, Train loss: 1.701, Test loss: 1.724, Test accuracy: 73.20
Average accuracy final 10 rounds: 72.83574999999999 

4263.910555839539
[3.4053165912628174, 6.810633182525635, 10.4231698513031, 14.035706520080566, 17.617029666900635, 21.198352813720703, 24.785764932632446, 28.37317705154419, 31.97880458831787, 35.58443212509155, 39.06820583343506, 42.551979541778564, 45.881192445755005, 49.210405349731445, 52.71472406387329, 56.21904277801514, 59.52699375152588, 62.83494472503662, 66.09774780273438, 69.36055088043213, 72.70782446861267, 76.05509805679321, 79.4307861328125, 82.80647420883179, 85.9683940410614, 89.13031387329102, 92.3263828754425, 95.522451877594, 98.79712414741516, 102.07179641723633, 105.37967801094055, 108.68755960464478, 112.03205442428589, 115.376549243927, 118.77547526359558, 122.17440128326416, 125.29285168647766, 128.41130208969116, 131.56527018547058, 134.71923828125, 138.07073283195496, 141.4222273826599, 144.84248971939087, 148.26275205612183, 151.61851143836975, 154.97427082061768, 158.1843605041504, 161.3944501876831, 164.5803861618042, 167.7663221359253, 171.1707420349121, 174.57516193389893, 177.92669010162354, 181.27821826934814, 184.72045135498047, 188.1626844406128, 191.56489038467407, 194.96709632873535, 198.00620365142822, 201.0453109741211, 204.24989557266235, 207.4544801712036, 210.70536422729492, 213.95624828338623, 217.329993724823, 220.70373916625977, 224.15655875205994, 227.6093783378601, 230.83393335342407, 234.05848836898804, 237.48016214370728, 240.9018359184265, 244.2218918800354, 247.5419478416443, 250.79447078704834, 254.0469937324524, 257.25568556785583, 260.4643774032593, 263.79052925109863, 267.116681098938, 270.50968527793884, 273.9026894569397, 277.2531888484955, 280.60368824005127, 283.9700560569763, 287.33642387390137, 290.9007816314697, 294.4651393890381, 298.14688777923584, 301.8286361694336, 305.5324013233185, 309.23616647720337, 312.9788520336151, 316.72153759002686, 320.4525442123413, 324.18355083465576, 327.86824202537537, 331.55293321609497, 334.7643401622772, 337.9757471084595, 340.9775285720825, 343.97931003570557, 347.38666224479675, 350.79401445388794, 354.12714767456055, 357.46028089523315, 360.8140194416046, 364.1677579879761, 367.576287984848, 370.98481798171997, 374.17233967781067, 377.35986137390137, 380.51649379730225, 383.6731262207031, 387.01353931427, 390.3539524078369, 393.60073590278625, 396.8475193977356, 400.0856828689575, 403.32384634017944, 406.6004436016083, 409.8770408630371, 413.1321487426758, 416.38725662231445, 419.6182134151459, 422.8491702079773, 426.23126006126404, 429.6133499145508, 432.9435513019562, 436.2737526893616, 439.419310092926, 442.5648674964905, 445.9222147464752, 449.27956199645996, 452.4935870170593, 455.7076120376587, 458.99852871894836, 462.28944540023804, 465.5730073451996, 468.85656929016113, 471.98740911483765, 475.11824893951416, 478.4661741256714, 481.8140993118286, 485.2404463291168, 488.66679334640503, 492.047794342041, 495.428795337677, 498.72792530059814, 502.0270552635193, 505.1883637905121, 508.3496723175049, 511.56229758262634, 514.7749228477478, 518.0925378799438, 521.4101529121399, 524.8342335224152, 528.2583141326904, 531.6697480678558, 535.0811820030212, 538.2927598953247, 541.5043377876282, 544.575189113617, 547.6460404396057, 550.866132736206, 554.0862250328064, 557.3840279579163, 560.6818308830261, 564.0106964111328, 567.3395619392395, 570.6468577384949, 573.9541535377502, 577.1318399906158, 580.3095264434814, 583.4417169094086, 586.5739073753357, 589.8838362693787, 593.1937651634216, 596.447751045227, 599.7017369270325, 603.0310497283936, 606.3603625297546, 609.6819219589233, 613.003481388092, 616.0870716571808, 619.1706619262695, 622.2969408035278, 625.4232196807861, 628.7958540916443, 632.1684885025024, 635.4334006309509, 638.6983127593994, 641.848494052887, 644.9986753463745, 648.3480732440948, 651.6974711418152, 654.8878481388092, 658.0782251358032, 661.3030188083649, 664.5278124809265, 666.070981502533, 667.6141505241394]
[13.3925, 13.3925, 17.32, 17.32, 21.9725, 21.9725, 26.55, 26.55, 30.8825, 30.8825, 34.1875, 34.1875, 38.56, 38.56, 40.9275, 40.9275, 41.8575, 41.8575, 46.8625, 46.8625, 49.775, 49.775, 52.75, 52.75, 55.12, 55.12, 55.6075, 55.6075, 56.5025, 56.5025, 58.1525, 58.1525, 59.47, 59.47, 60.01, 60.01, 59.575, 59.575, 61.1525, 61.1525, 61.63, 61.63, 61.2, 61.2, 61.785, 61.785, 62.54, 62.54, 62.6375, 62.6375, 62.6625, 62.6625, 63.175, 63.175, 63.3775, 63.3775, 63.74, 63.74, 63.4675, 63.4675, 63.815, 63.815, 64.18, 64.18, 64.385, 64.385, 64.8875, 64.8875, 64.865, 64.865, 65.0625, 65.0625, 64.9875, 64.9875, 65.3375, 65.3375, 65.6175, 65.6175, 65.5775, 65.5775, 65.46, 65.46, 67.155, 67.155, 67.4875, 67.4875, 67.82, 67.82, 67.9325, 67.9325, 68.54, 68.54, 68.6075, 68.6075, 68.6475, 68.6475, 68.72, 68.72, 68.7575, 68.7575, 69.0, 69.0, 68.945, 68.945, 69.2275, 69.2275, 69.2575, 69.2575, 69.46, 69.46, 69.3275, 69.3275, 69.575, 69.575, 69.595, 69.595, 69.5975, 69.5975, 69.7125, 69.7125, 69.7325, 69.7325, 69.6975, 69.6975, 69.735, 69.735, 69.775, 69.775, 70.125, 70.125, 70.18, 70.18, 70.1325, 70.1325, 70.3, 70.3, 70.7875, 70.7875, 70.7125, 70.7125, 70.8025, 70.8025, 70.745, 70.745, 70.8, 70.8, 71.2275, 71.2275, 71.36, 71.36, 71.3425, 71.3425, 71.3575, 71.3575, 71.3925, 71.3925, 71.425, 71.425, 71.4025, 71.4025, 71.415, 71.415, 71.3875, 71.3875, 71.3875, 71.3875, 71.415, 71.415, 71.48, 71.48, 71.47, 71.47, 71.415, 71.415, 71.855, 71.855, 72.0575, 72.0575, 71.995, 71.995, 72.59, 72.59, 72.3825, 72.3825, 72.6275, 72.6275, 72.6575, 72.6575, 72.6775, 72.6775, 73.03, 73.03, 73.0175, 73.0175, 73.1125, 73.1125, 73.1325, 73.1325, 73.13, 73.13, 73.2, 73.2]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.268, Test loss: 2.269, Test accuracy: 19.70
Round   1, Train loss: 2.020, Test loss: 2.159, Test accuracy: 31.81
Round   2, Train loss: 1.905, Test loss: 2.097, Test accuracy: 38.20
Round   3, Train loss: 1.763, Test loss: 2.006, Test accuracy: 46.26
Round   4, Train loss: 1.838, Test loss: 1.960, Test accuracy: 51.47
Round   5, Train loss: 1.810, Test loss: 1.902, Test accuracy: 56.49
Round   6, Train loss: 1.743, Test loss: 1.890, Test accuracy: 57.40
Round   7, Train loss: 1.790, Test loss: 1.839, Test accuracy: 62.65
Round   8, Train loss: 1.673, Test loss: 1.823, Test accuracy: 64.17
Round   9, Train loss: 1.838, Test loss: 1.777, Test accuracy: 68.89
Round  10, Train loss: 1.686, Test loss: 1.759, Test accuracy: 70.56
Round  11, Train loss: 1.735, Test loss: 1.754, Test accuracy: 70.99
Round  12, Train loss: 1.691, Test loss: 1.750, Test accuracy: 71.27
Round  13, Train loss: 1.695, Test loss: 1.741, Test accuracy: 72.14
Round  14, Train loss: 1.702, Test loss: 1.735, Test accuracy: 72.84
Round  15, Train loss: 1.624, Test loss: 1.717, Test accuracy: 74.69
Round  16, Train loss: 1.622, Test loss: 1.704, Test accuracy: 76.06
Round  17, Train loss: 1.701, Test loss: 1.693, Test accuracy: 77.22
Round  18, Train loss: 1.629, Test loss: 1.685, Test accuracy: 78.08
Round  19, Train loss: 1.598, Test loss: 1.676, Test accuracy: 78.81
Round  20, Train loss: 1.642, Test loss: 1.668, Test accuracy: 79.62
Round  21, Train loss: 1.548, Test loss: 1.663, Test accuracy: 80.09
Round  22, Train loss: 1.526, Test loss: 1.659, Test accuracy: 80.44
Round  23, Train loss: 1.568, Test loss: 1.657, Test accuracy: 80.60
Round  24, Train loss: 1.675, Test loss: 1.652, Test accuracy: 81.12
Round  25, Train loss: 1.530, Test loss: 1.651, Test accuracy: 81.22
Round  26, Train loss: 1.538, Test loss: 1.649, Test accuracy: 81.41
Round  27, Train loss: 1.577, Test loss: 1.645, Test accuracy: 81.82
Round  28, Train loss: 1.496, Test loss: 1.645, Test accuracy: 81.85
Round  29, Train loss: 1.611, Test loss: 1.644, Test accuracy: 81.83
Round  30, Train loss: 1.572, Test loss: 1.643, Test accuracy: 81.86
Round  31, Train loss: 1.527, Test loss: 1.643, Test accuracy: 81.91
Round  32, Train loss: 1.609, Test loss: 1.643, Test accuracy: 81.92
Round  33, Train loss: 1.607, Test loss: 1.643, Test accuracy: 81.89
Round  34, Train loss: 1.570, Test loss: 1.643, Test accuracy: 81.89
Round  35, Train loss: 1.527, Test loss: 1.643, Test accuracy: 81.89
Round  36, Train loss: 1.591, Test loss: 1.642, Test accuracy: 81.91
Round  37, Train loss: 1.588, Test loss: 1.642, Test accuracy: 82.00
Round  38, Train loss: 1.548, Test loss: 1.642, Test accuracy: 82.03
Round  39, Train loss: 1.615, Test loss: 1.636, Test accuracy: 82.56
Round  40, Train loss: 1.617, Test loss: 1.634, Test accuracy: 82.79
Round  41, Train loss: 1.590, Test loss: 1.634, Test accuracy: 82.81
Round  42, Train loss: 1.597, Test loss: 1.631, Test accuracy: 83.03
Round  43, Train loss: 1.549, Test loss: 1.631, Test accuracy: 83.08
Round  44, Train loss: 1.541, Test loss: 1.631, Test accuracy: 83.09
Round  45, Train loss: 1.528, Test loss: 1.624, Test accuracy: 83.81
Round  46, Train loss: 1.588, Test loss: 1.623, Test accuracy: 83.86
Round  47, Train loss: 1.631, Test loss: 1.623, Test accuracy: 83.83
Round  48, Train loss: 1.508, Test loss: 1.623, Test accuracy: 83.86
Round  49, Train loss: 1.606, Test loss: 1.622, Test accuracy: 84.00
Round  50, Train loss: 1.516, Test loss: 1.622, Test accuracy: 84.03
Round  51, Train loss: 1.564, Test loss: 1.621, Test accuracy: 84.04
Round  52, Train loss: 1.538, Test loss: 1.621, Test accuracy: 84.05
Round  53, Train loss: 1.587, Test loss: 1.621, Test accuracy: 84.08
Round  54, Train loss: 1.556, Test loss: 1.621, Test accuracy: 84.11
Round  55, Train loss: 1.553, Test loss: 1.621, Test accuracy: 84.13
Round  56, Train loss: 1.569, Test loss: 1.620, Test accuracy: 84.13
Round  57, Train loss: 1.544, Test loss: 1.620, Test accuracy: 84.14
Round  58, Train loss: 1.561, Test loss: 1.620, Test accuracy: 84.14
Round  59, Train loss: 1.546, Test loss: 1.620, Test accuracy: 84.15
Round  60, Train loss: 1.527, Test loss: 1.620, Test accuracy: 84.17
Round  61, Train loss: 1.570, Test loss: 1.620, Test accuracy: 84.20
Round  62, Train loss: 1.573, Test loss: 1.620, Test accuracy: 84.22
Round  63, Train loss: 1.586, Test loss: 1.620, Test accuracy: 84.19
Round  64, Train loss: 1.515, Test loss: 1.620, Test accuracy: 84.19
Round  65, Train loss: 1.570, Test loss: 1.620, Test accuracy: 84.20
Round  66, Train loss: 1.578, Test loss: 1.620, Test accuracy: 84.19
Round  67, Train loss: 1.495, Test loss: 1.620, Test accuracy: 84.20
Round  68, Train loss: 1.589, Test loss: 1.620, Test accuracy: 84.20
Round  69, Train loss: 1.568, Test loss: 1.620, Test accuracy: 84.20
Round  70, Train loss: 1.514, Test loss: 1.620, Test accuracy: 84.19
Round  71, Train loss: 1.559, Test loss: 1.620, Test accuracy: 84.20
Round  72, Train loss: 1.530, Test loss: 1.616, Test accuracy: 84.62
Round  73, Train loss: 1.571, Test loss: 1.615, Test accuracy: 84.63
Round  74, Train loss: 1.562, Test loss: 1.615, Test accuracy: 84.63
Round  75, Train loss: 1.554, Test loss: 1.615, Test accuracy: 84.59
Round  76, Train loss: 1.608, Test loss: 1.615, Test accuracy: 84.61
Round  77, Train loss: 1.569, Test loss: 1.615, Test accuracy: 84.63
Round  78, Train loss: 1.556, Test loss: 1.615, Test accuracy: 84.62
Round  79, Train loss: 1.551, Test loss: 1.615, Test accuracy: 84.62
Round  80, Train loss: 1.555, Test loss: 1.615, Test accuracy: 84.63
Round  81, Train loss: 1.586, Test loss: 1.615, Test accuracy: 84.64
Round  82, Train loss: 1.527, Test loss: 1.615, Test accuracy: 84.66
Round  83, Train loss: 1.571, Test loss: 1.615, Test accuracy: 84.64
Round  84, Train loss: 1.523, Test loss: 1.615, Test accuracy: 84.64
Round  85, Train loss: 1.572, Test loss: 1.615, Test accuracy: 84.66
Round  86, Train loss: 1.547, Test loss: 1.615, Test accuracy: 84.67
Round  87, Train loss: 1.525, Test loss: 1.615, Test accuracy: 84.66
Round  88, Train loss: 1.554, Test loss: 1.615, Test accuracy: 84.67
Round  89, Train loss: 1.616, Test loss: 1.615, Test accuracy: 84.68
Round  90, Train loss: 1.576, Test loss: 1.615, Test accuracy: 84.69
Round  91, Train loss: 1.616, Test loss: 1.615, Test accuracy: 84.69
Round  92, Train loss: 1.540, Test loss: 1.615, Test accuracy: 84.70
Round  93, Train loss: 1.578, Test loss: 1.612, Test accuracy: 85.00
Round  94, Train loss: 1.526, Test loss: 1.612, Test accuracy: 85.04
Round  95, Train loss: 1.572, Test loss: 1.611, Test accuracy: 85.11
Round  96, Train loss: 1.561, Test loss: 1.611, Test accuracy: 85.11
Round  97, Train loss: 1.559, Test loss: 1.602, Test accuracy: 86.07
Round  98, Train loss: 1.496, Test loss: 1.602, Test accuracy: 86.06
Round  99, Train loss: 1.517, Test loss: 1.601, Test accuracy: 86.19/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.533, Test loss: 1.599, Test accuracy: 86.31
Average accuracy final 10 rounds: 85.26600000000002 

4232.94845867157
[3.744511604309082, 7.489023208618164, 11.149403810501099, 14.809784412384033, 18.037853479385376, 21.26592254638672, 24.662309646606445, 28.058696746826172, 31.519974946975708, 34.981253147125244, 38.57899451255798, 42.17673587799072, 45.705811500549316, 49.23488712310791, 52.74646234512329, 56.25803756713867, 59.86065220832825, 63.46326684951782, 67.13348960876465, 70.80371236801147, 74.32060384750366, 77.83749532699585, 81.34930944442749, 84.86112356185913, 88.44699573516846, 92.03286790847778, 95.5578715801239, 99.08287525177002, 102.61857414245605, 106.15427303314209, 109.90474104881287, 113.65520906448364, 117.37400722503662, 121.0928053855896, 124.81071925163269, 128.52863311767578, 132.07035398483276, 135.61207485198975, 139.28745412826538, 142.96283340454102, 146.61481952667236, 150.2668056488037, 153.95152187347412, 157.63623809814453, 161.32954621315002, 165.02285432815552, 168.6856997013092, 172.3485450744629, 175.75885152816772, 179.16915798187256, 182.66768884658813, 186.1662197113037, 189.60161089897156, 193.0370020866394, 196.61292004585266, 200.18883800506592, 203.95675659179688, 207.72467517852783, 211.13938236236572, 214.5540895462036, 217.8243658542633, 221.094642162323, 224.3014440536499, 227.5082459449768, 230.80651831626892, 234.10479068756104, 237.58202958106995, 241.05926847457886, 244.49200296401978, 247.9247374534607, 251.12237453460693, 254.32001161575317, 257.5905911922455, 260.8611707687378, 264.18378138542175, 267.5063920021057, 270.8956048488617, 274.2848176956177, 277.56016087532043, 280.8355040550232, 284.06288385391235, 287.2902636528015, 290.7574155330658, 294.2245674133301, 297.510267496109, 300.79596757888794, 304.01401114463806, 307.2320547103882, 310.56946063041687, 313.90686655044556, 317.1002285480499, 320.2935905456543, 323.4529116153717, 326.6122326850891, 329.93595933914185, 333.2596859931946, 336.5660345554352, 339.8723831176758, 343.1767406463623, 346.4810981750488, 349.87917590141296, 353.2772536277771, 356.57314109802246, 359.8690285682678, 363.0780816078186, 366.2871346473694, 369.6511507034302, 373.01516675949097, 376.28677582740784, 379.5583848953247, 382.97444438934326, 386.3905038833618, 389.98774003982544, 393.58497619628906, 396.97468614578247, 400.3643960952759, 403.6349124908447, 406.9054288864136, 410.3416078090668, 413.77778673171997, 416.9951593875885, 420.21253204345703, 423.69229912757874, 427.17206621170044, 430.3630402088165, 433.5540142059326, 436.74503207206726, 439.9360499382019, 443.1913106441498, 446.44657135009766, 449.6181616783142, 452.78975200653076, 455.9120545387268, 459.03435707092285, 462.307758808136, 465.5811605453491, 468.77542185783386, 471.9696831703186, 475.0563871860504, 478.1430912017822, 481.31664204597473, 484.49019289016724, 487.52692699432373, 490.5636610984802, 493.63469099998474, 496.70572090148926, 500.16035079956055, 503.61498069763184, 507.0523991584778, 510.48981761932373, 513.8629896640778, 517.2361617088318, 520.7559328079224, 524.2757039070129, 527.8681647777557, 531.4606256484985, 534.8729100227356, 538.2851943969727, 541.8264319896698, 545.367669582367, 548.9020402431488, 552.4364109039307, 555.6832184791565, 558.9300260543823, 562.2550284862518, 565.5800309181213, 568.8321294784546, 572.0842280387878, 575.3691129684448, 578.6539978981018, 581.9809651374817, 585.3079323768616, 588.4583673477173, 591.608802318573, 594.8242197036743, 598.0396370887756, 601.6024723052979, 605.1653075218201, 608.6877732276917, 612.2102389335632, 615.7444295883179, 619.2786202430725, 622.7232444286346, 626.1678686141968, 629.3984870910645, 632.6291055679321, 635.7941267490387, 638.9591479301453, 642.1386551856995, 645.3181624412537, 648.4881575107574, 651.6581525802612, 654.7376158237457, 657.8170790672302, 660.9851379394531, 664.153196811676, 667.1610188484192, 670.1688408851624, 673.3590874671936, 676.5493340492249, 678.1438488960266, 679.7383637428284]
[19.7025, 19.7025, 31.8075, 31.8075, 38.195, 38.195, 46.255, 46.255, 51.4725, 51.4725, 56.495, 56.495, 57.4025, 57.4025, 62.6475, 62.6475, 64.1725, 64.1725, 68.885, 68.885, 70.5575, 70.5575, 70.9925, 70.9925, 71.265, 71.265, 72.1425, 72.1425, 72.84, 72.84, 74.695, 74.695, 76.065, 76.065, 77.2175, 77.2175, 78.0775, 78.0775, 78.81, 78.81, 79.6175, 79.6175, 80.095, 80.095, 80.445, 80.445, 80.6025, 80.6025, 81.1225, 81.1225, 81.225, 81.225, 81.4125, 81.4125, 81.8225, 81.8225, 81.8475, 81.8475, 81.8325, 81.8325, 81.865, 81.865, 81.9125, 81.9125, 81.92, 81.92, 81.89, 81.89, 81.885, 81.885, 81.89, 81.89, 81.9125, 81.9125, 82.005, 82.005, 82.0325, 82.0325, 82.56, 82.56, 82.79, 82.79, 82.815, 82.815, 83.03, 83.03, 83.08, 83.08, 83.0925, 83.0925, 83.8125, 83.8125, 83.865, 83.865, 83.8325, 83.8325, 83.865, 83.865, 84.0, 84.0, 84.035, 84.035, 84.0375, 84.0375, 84.0525, 84.0525, 84.0825, 84.0825, 84.1125, 84.1125, 84.1325, 84.1325, 84.1275, 84.1275, 84.14, 84.14, 84.14, 84.14, 84.15, 84.15, 84.175, 84.175, 84.2025, 84.2025, 84.2225, 84.2225, 84.195, 84.195, 84.1925, 84.1925, 84.205, 84.205, 84.195, 84.195, 84.2, 84.2, 84.205, 84.205, 84.1975, 84.1975, 84.185, 84.185, 84.2025, 84.2025, 84.625, 84.625, 84.63, 84.63, 84.63, 84.63, 84.595, 84.595, 84.6075, 84.6075, 84.6275, 84.6275, 84.625, 84.625, 84.6225, 84.6225, 84.6275, 84.6275, 84.645, 84.645, 84.6575, 84.6575, 84.6375, 84.6375, 84.6375, 84.6375, 84.6575, 84.6575, 84.665, 84.665, 84.66, 84.66, 84.6675, 84.6675, 84.6825, 84.6825, 84.6925, 84.6925, 84.695, 84.695, 84.6975, 84.6975, 84.9975, 84.9975, 85.0375, 85.0375, 85.1125, 85.1125, 85.1075, 85.1075, 86.0725, 86.0725, 86.06, 86.06, 86.1875, 86.1875, 86.3125, 86.3125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.626, Test loss: 2.280, Test accuracy: 27.06
Round   1, Train loss: 1.508, Test loss: 2.224, Test accuracy: 34.21
Round   2, Train loss: 1.407, Test loss: 2.168, Test accuracy: 37.46
Round   3, Train loss: 1.409, Test loss: 2.140, Test accuracy: 39.02
Round   4, Train loss: 1.429, Test loss: 2.097, Test accuracy: 43.04
Round   5, Train loss: 1.446, Test loss: 2.053, Test accuracy: 47.12
Round   6, Train loss: 1.439, Test loss: 2.021, Test accuracy: 49.69
Round   7, Train loss: 1.414, Test loss: 1.997, Test accuracy: 52.17
Round   8, Train loss: 1.416, Test loss: 1.981, Test accuracy: 52.99
Round   9, Train loss: 1.354, Test loss: 1.973, Test accuracy: 53.41
Round  10, Train loss: 1.353, Test loss: 1.968, Test accuracy: 53.35
Round  11, Train loss: 1.392, Test loss: 1.963, Test accuracy: 53.80
Round  12, Train loss: 1.340, Test loss: 1.946, Test accuracy: 55.43
Round  13, Train loss: 1.402, Test loss: 1.937, Test accuracy: 56.48
Round  14, Train loss: 1.292, Test loss: 1.935, Test accuracy: 56.44
Round  15, Train loss: 1.318, Test loss: 1.930, Test accuracy: 56.58
Round  16, Train loss: 1.367, Test loss: 1.925, Test accuracy: 57.10
Round  17, Train loss: 1.317, Test loss: 1.911, Test accuracy: 58.73
Round  18, Train loss: 1.354, Test loss: 1.906, Test accuracy: 59.02
Round  19, Train loss: 1.322, Test loss: 1.906, Test accuracy: 58.98
Round  20, Train loss: 1.332, Test loss: 1.901, Test accuracy: 59.47
Round  21, Train loss: 1.363, Test loss: 1.898, Test accuracy: 59.59
Round  22, Train loss: 1.348, Test loss: 1.894, Test accuracy: 60.11
Round  23, Train loss: 1.356, Test loss: 1.891, Test accuracy: 60.21
Round  24, Train loss: 1.330, Test loss: 1.886, Test accuracy: 60.42
Round  25, Train loss: 1.293, Test loss: 1.883, Test accuracy: 60.60
Round  26, Train loss: 1.271, Test loss: 1.880, Test accuracy: 60.99
Round  27, Train loss: 1.272, Test loss: 1.878, Test accuracy: 61.21
Round  28, Train loss: 1.305, Test loss: 1.876, Test accuracy: 61.48
Round  29, Train loss: 1.380, Test loss: 1.878, Test accuracy: 61.19
Round  30, Train loss: 1.305, Test loss: 1.878, Test accuracy: 61.04
Round  31, Train loss: 1.347, Test loss: 1.879, Test accuracy: 60.75
Round  32, Train loss: 1.283, Test loss: 1.878, Test accuracy: 60.70
Round  33, Train loss: 1.311, Test loss: 1.879, Test accuracy: 60.49
Round  34, Train loss: 1.307, Test loss: 1.881, Test accuracy: 60.32
Round  35, Train loss: 1.282, Test loss: 1.882, Test accuracy: 60.11
Round  36, Train loss: 1.300, Test loss: 1.879, Test accuracy: 60.38
Round  37, Train loss: 1.328, Test loss: 1.878, Test accuracy: 60.32
Round  38, Train loss: 1.295, Test loss: 1.870, Test accuracy: 61.50
Round  39, Train loss: 1.258, Test loss: 1.868, Test accuracy: 61.62
Round  40, Train loss: 1.282, Test loss: 1.871, Test accuracy: 61.14
Round  41, Train loss: 1.338, Test loss: 1.869, Test accuracy: 61.34
Round  42, Train loss: 1.295, Test loss: 1.866, Test accuracy: 61.70
Round  43, Train loss: 1.288, Test loss: 1.867, Test accuracy: 61.46
Round  44, Train loss: 1.281, Test loss: 1.868, Test accuracy: 61.10
Round  45, Train loss: 1.312, Test loss: 1.869, Test accuracy: 61.02
Round  46, Train loss: 1.324, Test loss: 1.872, Test accuracy: 60.61
Round  47, Train loss: 1.274, Test loss: 1.872, Test accuracy: 60.51
Round  48, Train loss: 1.306, Test loss: 1.873, Test accuracy: 60.30
Round  49, Train loss: 1.315, Test loss: 1.873, Test accuracy: 60.27
Round  50, Train loss: 1.317, Test loss: 1.874, Test accuracy: 60.09
Round  51, Train loss: 1.286, Test loss: 1.874, Test accuracy: 60.16
Round  52, Train loss: 1.229, Test loss: 1.871, Test accuracy: 60.32
Round  53, Train loss: 1.315, Test loss: 1.872, Test accuracy: 60.25
Round  54, Train loss: 1.256, Test loss: 1.874, Test accuracy: 60.02
Round  55, Train loss: 1.284, Test loss: 1.872, Test accuracy: 60.23
Round  56, Train loss: 1.280, Test loss: 1.876, Test accuracy: 59.82
Round  57, Train loss: 1.290, Test loss: 1.876, Test accuracy: 59.75
Round  58, Train loss: 1.318, Test loss: 1.878, Test accuracy: 59.48
Round  59, Train loss: 1.286, Test loss: 1.876, Test accuracy: 59.69
Round  60, Train loss: 1.270, Test loss: 1.875, Test accuracy: 59.81
Round  61, Train loss: 1.309, Test loss: 1.875, Test accuracy: 59.72
Round  62, Train loss: 1.275, Test loss: 1.878, Test accuracy: 59.41
Round  63, Train loss: 1.277, Test loss: 1.877, Test accuracy: 59.30
Round  64, Train loss: 1.246, Test loss: 1.876, Test accuracy: 59.36
Round  65, Train loss: 1.295, Test loss: 1.876, Test accuracy: 59.38
Round  66, Train loss: 1.282, Test loss: 1.879, Test accuracy: 59.04
Round  67, Train loss: 1.256, Test loss: 1.881, Test accuracy: 58.80
Round  68, Train loss: 1.278, Test loss: 1.878, Test accuracy: 59.08
Round  69, Train loss: 1.254, Test loss: 1.880, Test accuracy: 58.87
Round  70, Train loss: 1.283, Test loss: 1.881, Test accuracy: 58.73
Round  71, Train loss: 1.303, Test loss: 1.880, Test accuracy: 58.85
Round  72, Train loss: 1.292, Test loss: 1.881, Test accuracy: 58.83
Round  73, Train loss: 1.247, Test loss: 1.883, Test accuracy: 58.59
Round  74, Train loss: 1.287, Test loss: 1.881, Test accuracy: 58.68
Round  75, Train loss: 1.249, Test loss: 1.881, Test accuracy: 58.55
Round  76, Train loss: 1.303, Test loss: 1.876, Test accuracy: 59.23
Round  77, Train loss: 1.249, Test loss: 1.877, Test accuracy: 59.25
Round  78, Train loss: 1.268, Test loss: 1.878, Test accuracy: 59.01
Round  79, Train loss: 1.319, Test loss: 1.882, Test accuracy: 58.42
Round  80, Train loss: 1.260, Test loss: 1.878, Test accuracy: 59.02
Round  81, Train loss: 1.236, Test loss: 1.882, Test accuracy: 58.46
Round  82, Train loss: 1.265, Test loss: 1.882, Test accuracy: 58.48
Round  83, Train loss: 1.292, Test loss: 1.885, Test accuracy: 58.16
Round  84, Train loss: 1.282, Test loss: 1.882, Test accuracy: 58.35
Round  85, Train loss: 1.272, Test loss: 1.884, Test accuracy: 58.20
Round  86, Train loss: 1.301, Test loss: 1.884, Test accuracy: 58.11
Round  87, Train loss: 1.239, Test loss: 1.882, Test accuracy: 58.27
Round  88, Train loss: 1.247, Test loss: 1.886, Test accuracy: 57.89
Round  89, Train loss: 1.262, Test loss: 1.882, Test accuracy: 58.29
Round  90, Train loss: 1.226, Test loss: 1.886, Test accuracy: 57.81
Round  91, Train loss: 1.294, Test loss: 1.888, Test accuracy: 57.56
Round  92, Train loss: 1.235, Test loss: 1.888, Test accuracy: 57.55
Round  93, Train loss: 1.297, Test loss: 1.886, Test accuracy: 57.86
Round  94, Train loss: 1.248, Test loss: 1.885, Test accuracy: 57.85
Round  95, Train loss: 1.284, Test loss: 1.884, Test accuracy: 58.05
Round  96, Train loss: 1.288, Test loss: 1.886, Test accuracy: 57.77
Round  97, Train loss: 1.295, Test loss: 1.889, Test accuracy: 57.46
Round  98, Train loss: 1.278, Test loss: 1.891, Test accuracy: 57.33
Round  99, Train loss: 1.303, Test loss: 1.893, Test accuracy: 57.06
Final Round, Train loss: 1.276, Test loss: 1.896, Test accuracy: 56.58
Average accuracy final 10 rounds: 57.63125000000001
3013.0789222717285
[]
[27.058333333333334, 34.2125, 37.4625, 39.020833333333336, 43.041666666666664, 47.12083333333333, 49.69166666666667, 52.170833333333334, 52.9875, 53.4125, 53.354166666666664, 53.795833333333334, 55.42916666666667, 56.475, 56.4375, 56.575, 57.09583333333333, 58.725, 59.025, 58.983333333333334, 59.46666666666667, 59.59166666666667, 60.1125, 60.2125, 60.416666666666664, 60.59583333333333, 60.9875, 61.2125, 61.479166666666664, 61.19166666666667, 61.041666666666664, 60.74583333333333, 60.69583333333333, 60.4875, 60.31666666666667, 60.1125, 60.37916666666667, 60.31666666666667, 61.49583333333333, 61.61666666666667, 61.1375, 61.3375, 61.704166666666666, 61.458333333333336, 61.1, 61.025, 60.6125, 60.50833333333333, 60.3, 60.270833333333336, 60.0875, 60.15833333333333, 60.32083333333333, 60.25416666666667, 60.020833333333336, 60.225, 59.81666666666667, 59.75416666666667, 59.475, 59.6875, 59.80833333333333, 59.71666666666667, 59.4125, 59.295833333333334, 59.3625, 59.375, 59.0375, 58.8, 59.079166666666666, 58.87083333333333, 58.729166666666664, 58.854166666666664, 58.825, 58.5875, 58.67916666666667, 58.55, 59.225, 59.25, 59.00833333333333, 58.420833333333334, 59.020833333333336, 58.458333333333336, 58.479166666666664, 58.15833333333333, 58.35, 58.2, 58.108333333333334, 58.266666666666666, 57.891666666666666, 58.2875, 57.8125, 57.5625, 57.545833333333334, 57.8625, 57.854166666666664, 58.05, 57.775, 57.458333333333336, 57.333333333333336, 57.05833333333333, 56.579166666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.136, Test loss: 2.126, Test accuracy: 28.26
Round   1, Train loss: 1.904, Test loss: 1.985, Test accuracy: 43.26
Round   2, Train loss: 1.807, Test loss: 1.921, Test accuracy: 51.96
Round   3, Train loss: 2.007, Test loss: 2.061, Test accuracy: 35.98
Round   4, Train loss: 1.534, Test loss: 2.053, Test accuracy: 37.28
Round   5, Train loss: 1.394, Test loss: 2.116, Test accuracy: 30.82
Round   6, Train loss: 1.441, Test loss: 2.028, Test accuracy: 42.84
Round   7, Train loss: 0.869, Test loss: 1.932, Test accuracy: 54.86
Round   8, Train loss: -0.117, Test loss: 1.713, Test accuracy: 74.35
Round   9, Train loss: 1.171, Test loss: 1.822, Test accuracy: 66.30
Round  10, Train loss: 0.034, Test loss: 1.663, Test accuracy: 80.92
Round  11, Train loss: 0.142, Test loss: 1.711, Test accuracy: 76.58
Round  12, Train loss: -0.083, Test loss: 1.633, Test accuracy: 85.02
Round  13, Train loss: 0.076, Test loss: 1.639, Test accuracy: 83.79
Round  14, Train loss: -0.668, Test loss: 1.578, Test accuracy: 89.76
Round  15, Train loss: -0.490, Test loss: 1.565, Test accuracy: 89.78
Round  16, Train loss: -0.319, Test loss: 1.579, Test accuracy: 88.54
Round  17, Train loss: 0.706, Test loss: 1.606, Test accuracy: 86.05
Round  18, Train loss: -0.263, Test loss: 1.577, Test accuracy: 88.63
Round  19, Train loss: -0.973, Test loss: 1.577, Test accuracy: 88.73
Round  20, Train loss: -0.513, Test loss: 1.598, Test accuracy: 86.59
Round  21, Train loss: -0.726, Test loss: 1.573, Test accuracy: 89.22
Round  22, Train loss: -0.773, Test loss: 1.586, Test accuracy: 87.69
Round  23, Train loss: -0.831, Test loss: 1.597, Test accuracy: 86.49
Round  24, Train loss: -1.639, Test loss: 1.582, Test accuracy: 87.97
Round  25, Train loss: -0.946, Test loss: 1.567, Test accuracy: 89.41
Round  26, Train loss: -0.947, Test loss: 1.581, Test accuracy: 88.16
Round  27, Train loss: -0.813, Test loss: 1.555, Test accuracy: 90.71
Round  28, Train loss: -1.258, Test loss: 1.525, Test accuracy: 93.67
Round  29, Train loss: -1.313, Test loss: 1.555, Test accuracy: 90.72
Round  30, Train loss: -1.650, Test loss: 1.568, Test accuracy: 89.38
Round  31, Train loss: -1.948, Test loss: 1.567, Test accuracy: 89.36
Round  32, Train loss: -2.400, Test loss: 1.564, Test accuracy: 89.67
Round  33, Train loss: -1.301, Test loss: 1.557, Test accuracy: 90.66
Round  34, Train loss: -1.840, Test loss: 1.557, Test accuracy: 90.66
Round  35, Train loss: -1.045, Test loss: 1.536, Test accuracy: 92.53
Round  36, Train loss: -2.268, Test loss: 1.549, Test accuracy: 91.19
Round  37, Train loss: -1.696, Test loss: 1.552, Test accuracy: 91.03
Round  38, Train loss: -2.020, Test loss: 1.540, Test accuracy: 92.28
Round  39, Train loss: -1.934, Test loss: 1.538, Test accuracy: 92.42
Round  40, Train loss: -2.250, Test loss: 1.535, Test accuracy: 92.70
Round  41, Train loss: -2.152, Test loss: 1.524, Test accuracy: 93.79
Round  42, Train loss: -1.905, Test loss: 1.522, Test accuracy: 93.92
Round  43, Train loss: -1.242, Test loss: 1.549, Test accuracy: 91.23
Round  44, Train loss: -2.010, Test loss: 1.550, Test accuracy: 91.20
Round  45, Train loss: -1.945, Test loss: 1.548, Test accuracy: 91.34
Round  46, Train loss: -2.372, Test loss: 1.535, Test accuracy: 92.65
Round  47, Train loss: -1.415, Test loss: 1.535, Test accuracy: 92.67
Round  48, Train loss: -1.375, Test loss: 1.549, Test accuracy: 91.23
Round  49, Train loss: -1.253, Test loss: 1.522, Test accuracy: 93.91
Round  50, Train loss: -1.717, Test loss: 1.562, Test accuracy: 89.88
Round  51, Train loss: -2.515, Test loss: 1.534, Test accuracy: 92.70
Round  52, Train loss: -2.004, Test loss: 1.533, Test accuracy: 92.78
Round  53, Train loss: -1.275, Test loss: 1.548, Test accuracy: 91.28
Round  54, Train loss: -2.272, Test loss: 1.537, Test accuracy: 92.42
Round  55, Train loss: -1.762, Test loss: 1.536, Test accuracy: 92.45
Round  56, Train loss: -1.698, Test loss: 1.521, Test accuracy: 93.98
Round  57, Train loss: -2.004, Test loss: 1.508, Test accuracy: 95.28
Round  58, Train loss: -2.143, Test loss: 1.526, Test accuracy: 93.48
Round  59, Train loss: -2.087, Test loss: 1.507, Test accuracy: 95.40
Round  60, Train loss: -2.004, Test loss: 1.548, Test accuracy: 91.35
Round  61, Train loss: -2.151, Test loss: 1.547, Test accuracy: 91.38
Round  62, Train loss: -2.284, Test loss: 1.549, Test accuracy: 91.25
Round  63, Train loss: -2.117, Test loss: 1.548, Test accuracy: 91.33
Round  64, Train loss: -2.178, Test loss: 1.550, Test accuracy: 91.19
Round  65, Train loss: -2.017, Test loss: 1.537, Test accuracy: 92.39
Round  66, Train loss: -1.828, Test loss: 1.536, Test accuracy: 92.52
Round  67, Train loss: -2.064, Test loss: 1.535, Test accuracy: 92.67
Round  68, Train loss: -2.130, Test loss: 1.535, Test accuracy: 92.59
Round  69, Train loss: -1.360, Test loss: 1.521, Test accuracy: 94.01
Round  70, Train loss: -2.427, Test loss: 1.506, Test accuracy: 95.51
Round  71, Train loss: -1.420, Test loss: 1.534, Test accuracy: 92.74
Round  72, Train loss: -2.051, Test loss: 1.506, Test accuracy: 95.49
Round  73, Train loss: -1.571, Test loss: 1.509, Test accuracy: 95.32
Round  74, Train loss: -1.645, Test loss: 1.522, Test accuracy: 93.97
Round  75, Train loss: -2.285, Test loss: 1.519, Test accuracy: 94.17
Round  76, Train loss: -1.668, Test loss: 1.520, Test accuracy: 94.09
Round  77, Train loss: -1.814, Test loss: 1.522, Test accuracy: 93.92
Round  78, Train loss: -1.562, Test loss: 1.522, Test accuracy: 93.94
Round  79, Train loss: -1.740, Test loss: 1.536, Test accuracy: 92.53
Round  80, Train loss: -2.399, Test loss: 1.521, Test accuracy: 94.06
Round  81, Train loss: -2.091, Test loss: 1.521, Test accuracy: 94.03
Round  82, Train loss: -2.103, Test loss: 1.535, Test accuracy: 92.65
Round  83, Train loss: -1.770, Test loss: 1.520, Test accuracy: 94.13
Round  84, Train loss: -2.199, Test loss: 1.509, Test accuracy: 95.27
Round  85, Train loss: -1.881, Test loss: 1.522, Test accuracy: 93.97
Round  86, Train loss: -2.054, Test loss: 1.522, Test accuracy: 94.01
Round  87, Train loss: -1.499, Test loss: 1.522, Test accuracy: 94.03
Round  88, Train loss: -1.199, Test loss: 1.522, Test accuracy: 93.94
Round  89, Train loss: -2.175, Test loss: 1.523, Test accuracy: 93.78
Round  90, Train loss: -1.857, Test loss: 1.521, Test accuracy: 94.01
Round  91, Train loss: -1.999, Test loss: 1.520, Test accuracy: 94.07
Round  92, Train loss: -1.788, Test loss: 1.521, Test accuracy: 93.99
Round  93, Train loss: -2.246, Test loss: 1.519, Test accuracy: 94.16
Round  94, Train loss: -2.485, Test loss: 1.521, Test accuracy: 93.97
Round  95, Train loss: -1.842, Test loss: 1.506, Test accuracy: 95.48
Round  96, Train loss: -2.579, Test loss: 1.505, Test accuracy: 95.63
Round  97, Train loss: -1.274, Test loss: 1.519, Test accuracy: 94.23
Round  98, Train loss: -2.387, Test loss: 1.507, Test accuracy: 95.47
Round  99, Train loss: -1.674, Test loss: 1.507, Test accuracy: 95.51
Final Round, Train loss: 1.616, Test loss: 1.557, Test accuracy: 91.24
Average accuracy final 10 rounds: 94.65333333333334
Average global accuracy final 10 rounds: 94.65333333333334
1147.1323730945587
[]
[28.258333333333333, 43.25833333333333, 51.958333333333336, 35.983333333333334, 37.28333333333333, 30.816666666666666, 42.84166666666667, 54.858333333333334, 74.35, 66.3, 80.91666666666667, 76.58333333333333, 85.01666666666667, 83.79166666666667, 89.75833333333334, 89.775, 88.54166666666667, 86.05, 88.63333333333334, 88.73333333333333, 86.59166666666667, 89.21666666666667, 87.69166666666666, 86.49166666666666, 87.96666666666667, 89.40833333333333, 88.15833333333333, 90.70833333333333, 93.66666666666667, 90.71666666666667, 89.38333333333334, 89.35833333333333, 89.675, 90.65833333333333, 90.65833333333333, 92.525, 91.19166666666666, 91.025, 92.275, 92.425, 92.7, 93.79166666666667, 93.91666666666667, 91.23333333333333, 91.2, 91.34166666666667, 92.65, 92.675, 91.23333333333333, 93.90833333333333, 89.875, 92.7, 92.775, 91.28333333333333, 92.425, 92.45, 93.98333333333333, 95.28333333333333, 93.48333333333333, 95.4, 91.35, 91.38333333333334, 91.25, 91.325, 91.19166666666666, 92.39166666666667, 92.51666666666667, 92.66666666666667, 92.59166666666667, 94.00833333333334, 95.50833333333334, 92.74166666666666, 95.49166666666666, 95.31666666666666, 93.96666666666667, 94.175, 94.09166666666667, 93.91666666666667, 93.94166666666666, 92.53333333333333, 94.05833333333334, 94.025, 92.65, 94.13333333333334, 95.26666666666667, 93.975, 94.00833333333334, 94.025, 93.94166666666666, 93.78333333333333, 94.00833333333334, 94.06666666666666, 93.99166666666666, 94.15833333333333, 93.975, 95.48333333333333, 95.63333333333334, 94.23333333333333, 95.475, 95.50833333333334, 91.24166666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.51
Round   0, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.33
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.69
Round   1, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.34
Round   2, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.76
Round   2, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.33
Round   3, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.79
Round   3, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.37
Round   4, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.79
Round   4, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.40
Round   5, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.83
Round   5, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.46
Round   6, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.83
Round   6, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.47
Round   7, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.78
Round   7, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.47
Round   8, Train loss: 2.303, Test loss: 2.302, Test accuracy: 11.76
Round   8, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.46
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.90
Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.40
Round  10, Train loss: 2.303, Test loss: 2.302, Test accuracy: 11.88
Round  10, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.41
Round  11, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.88
Round  11, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.47
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.83
Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.49
Round  13, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.82
Round  13, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.48
Round  14, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.84
Round  14, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.48
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.79
Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.47
Round  16, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.78
Round  16, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.53
Round  17, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.81
Round  17, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.49
Round  18, Train loss: 2.299, Test loss: 2.302, Test accuracy: 11.88
Round  18, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.50
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.92
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.53
Round  20, Train loss: 2.303, Test loss: 2.302, Test accuracy: 11.93
Round  20, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.53
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.93
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.47
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.91
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.53
Round  23, Train loss: 2.304, Test loss: 2.302, Test accuracy: 11.96
Round  23, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 11.49
Round  24, Train loss: 2.300, Test loss: 2.302, Test accuracy: 11.93
Round  24, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.55
Round  25, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.93
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.60
Round  26, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.90
Round  26, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.56
Round  27, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.97
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.57
Round  28, Train loss: 2.300, Test loss: 2.301, Test accuracy: 11.97
Round  28, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.60
Round  29, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.12
Round  29, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.62
Round  30, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.12
Round  30, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.60
Round  31, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.07
Round  31, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.65
Round  32, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.07
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.53
Round  33, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.95
Round  33, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.68
Round  34, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.98
Round  34, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.69
Round  35, Train loss: 2.298, Test loss: 2.301, Test accuracy: 12.11
Round  35, Global train loss: 2.298, Global test loss: 2.302, Global test accuracy: 11.72
Round  36, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.11
Round  36, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.72
Round  37, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.08
Round  37, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.80
Round  38, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.12
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.78
Round  39, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.17
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.78
Round  40, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.18
Round  40, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 11.72
Round  41, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.13
Round  41, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.71
Round  42, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.13
Round  42, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.71
Round  43, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.03
Round  43, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 11.72
Round  44, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.99
Round  44, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.71
Round  45, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.00
Round  45, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.72
Round  46, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.01
Round  46, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 11.72
Round  47, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.07
Round  47, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.71
Round  48, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.09
Round  48, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 11.73
Round  49, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.07
Round  49, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 11.72
Round  50, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.08
Round  50, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.80
Round  51, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.12
Round  51, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 11.83
Round  52, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.12
Round  52, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 11.88
Round  53, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.22
Round  53, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 11.88
Round  54, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.23
Round  54, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.96
Round  55, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.33
Round  55, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 11.95
Round  56, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.32
Round  56, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 11.90
Round  57, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.32
Round  57, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.94
Round  58, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.34
Round  58, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.88
Round  59, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.32
Round  59, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.96
Round  60, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.32
Round  60, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.97
Round  61, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.38
Round  61, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.01
Round  62, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.40
Round  62, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.09
Round  63, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.48
Round  63, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.03
Round  64, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.53
Round  64, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.26
Round  65, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.56
Round  65, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 12.26
Round  66, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.65
Round  66, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.28
Round  67, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.65
Round  67, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.18
Round  68, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.68
Round  68, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.28
Round  69, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.73
Round  69, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.08
Round  70, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.72
Round  70, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.15
Round  71, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.70
Round  71, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.20
Round  72, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.71
Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.26
Round  73, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.72
Round  73, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 12.34
Round  74, Train loss: 2.303, Test loss: 2.301, Test accuracy: 12.80
Round  74, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 12.32
Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.78
Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.38
Round  76, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.86
Round  76, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.42
Round  77, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.86
Round  77, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.36
Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.83
Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.42
Round  79, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.92
Round  79, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.39
Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.92
Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.44
Round  81, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.88
Round  81, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.43
Round  82, Train loss: 2.298, Test loss: 2.301, Test accuracy: 12.92
Round  82, Global train loss: 2.298, Global test loss: 2.301, Global test accuracy: 12.40
Round  83, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.95
Round  83, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.43
Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.02
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.41
Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.93
Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.38
Round  86, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.92
Round  86, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.19
Round  87, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.77
Round  87, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.42
Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.80
Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.40
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.82
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.32
Round  90, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.74
Round  90, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.28
Round  91, Train loss: 2.298, Test loss: 2.301, Test accuracy: 12.62
Round  91, Global train loss: 2.298, Global test loss: 2.301, Global test accuracy: 12.41
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.65
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.32
Round  93, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.60
Round  93, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.43
Round  94, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.70
Round  94, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.47
Round  95, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.83
Round  95, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.48
Round  96, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.85
Round  96, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.49/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.84
Round  97, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.51
Round  98, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.85
Round  98, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.51
Round  99, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.85
Round  99, Global train loss: 2.299, Global test loss: 2.301, Global test accuracy: 12.53
Final Round, Train loss: 2.300, Test loss: 2.301, Test accuracy: 13.42
Final Round, Global train loss: 2.300, Global test loss: 2.301, Global test accuracy: 12.53
Average accuracy final 10 rounds: 12.754166666666666 

Average global accuracy final 10 rounds: 12.440833333333332 

1331.813714504242
[1.256119728088379, 2.3196702003479004, 3.359321117401123, 4.4152586460113525, 5.504735708236694, 6.568307399749756, 7.626981258392334, 8.69020414352417, 9.758729696273804, 10.827319145202637, 11.897047519683838, 12.915048360824585, 13.955288648605347, 15.164649248123169, 16.359923362731934, 17.578219175338745, 18.810173988342285, 19.961345434188843, 21.123857498168945, 22.337446928024292, 23.523117542266846, 24.700761318206787, 25.85915493965149, 27.040737628936768, 28.184634923934937, 29.364157676696777, 30.573156356811523, 31.75728416442871, 32.928369998931885, 34.100157260894775, 35.27035307884216, 36.48866057395935, 37.6624972820282, 38.88694167137146, 40.046769857406616, 41.23615503311157, 42.402488708496094, 43.45510458946228, 44.524620056152344, 45.595006465911865, 46.64832615852356, 47.646655321121216, 48.71100831031799, 49.770859241485596, 50.821361780166626, 51.99463725090027, 53.05886101722717, 54.12503719329834, 55.24787378311157, 56.326822996139526, 57.37005829811096, 58.41682052612305, 59.46667504310608, 60.60232138633728, 61.617562770843506, 62.655890226364136, 63.701422452926636, 64.75920104980469, 65.80711555480957, 66.82812714576721, 67.86387729644775, 68.94423198699951, 70.01471328735352, 71.0105493068695, 72.055171251297, 73.13261771202087, 74.19803476333618, 75.20095419883728, 76.22502756118774, 77.29931783676147, 78.35379242897034, 79.42006587982178, 80.44068551063538, 81.51478242874146, 82.5856146812439, 83.66395950317383, 84.67130851745605, 85.71442151069641, 86.77101063728333, 87.83570170402527, 88.86781287193298, 89.89251899719238, 90.93542122840881, 92.00069212913513, 93.06065320968628, 94.05941009521484, 95.17895174026489, 96.23514032363892, 97.28428864479065, 98.29405689239502, 99.31416249275208, 100.3593578338623, 101.40248894691467, 102.43679451942444, 103.45501518249512, 104.49239659309387, 105.500643491745, 106.50024223327637, 107.48333263397217, 108.47868323326111, 110.20585417747498]
[11.508333333333333, 11.691666666666666, 11.758333333333333, 11.791666666666666, 11.791666666666666, 11.833333333333334, 11.833333333333334, 11.783333333333333, 11.758333333333333, 11.9, 11.883333333333333, 11.883333333333333, 11.833333333333334, 11.825, 11.841666666666667, 11.791666666666666, 11.783333333333333, 11.808333333333334, 11.883333333333333, 11.916666666666666, 11.933333333333334, 11.933333333333334, 11.908333333333333, 11.958333333333334, 11.933333333333334, 11.925, 11.9, 11.966666666666667, 11.966666666666667, 12.116666666666667, 12.125, 12.075, 12.075, 11.95, 11.983333333333333, 12.108333333333333, 12.108333333333333, 12.083333333333334, 12.116666666666667, 12.166666666666666, 12.175, 12.133333333333333, 12.133333333333333, 12.033333333333333, 11.991666666666667, 12.0, 12.008333333333333, 12.066666666666666, 12.091666666666667, 12.075, 12.083333333333334, 12.125, 12.125, 12.216666666666667, 12.233333333333333, 12.333333333333334, 12.325, 12.325, 12.341666666666667, 12.325, 12.316666666666666, 12.375, 12.4, 12.483333333333333, 12.525, 12.558333333333334, 12.65, 12.65, 12.683333333333334, 12.733333333333333, 12.716666666666667, 12.7, 12.708333333333334, 12.716666666666667, 12.8, 12.775, 12.858333333333333, 12.858333333333333, 12.833333333333334, 12.916666666666666, 12.916666666666666, 12.883333333333333, 12.916666666666666, 12.95, 13.016666666666667, 12.925, 12.916666666666666, 12.766666666666667, 12.8, 12.816666666666666, 12.741666666666667, 12.625, 12.65, 12.6, 12.7, 12.833333333333334, 12.85, 12.841666666666667, 12.85, 12.85, 13.416666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.85
Round   1, Train loss: 2.298, Test loss: 2.296, Test accuracy: 12.35
Round   2, Train loss: 2.200, Test loss: 2.295, Test accuracy: 13.63
Round   3, Train loss: 2.015, Test loss: 2.285, Test accuracy: 14.92
Round   4, Train loss: 2.025, Test loss: 2.287, Test accuracy: 15.16
Round   5, Train loss: 2.011, Test loss: 2.283, Test accuracy: 15.61
Round   6, Train loss: 2.048, Test loss: 2.279, Test accuracy: 16.14
Round   7, Train loss: 1.988, Test loss: 2.280, Test accuracy: 15.20
Round   8, Train loss: 1.980, Test loss: 2.280, Test accuracy: 16.16
Round   9, Train loss: 1.930, Test loss: 2.286, Test accuracy: 15.30
Round  10, Train loss: 2.000, Test loss: 2.278, Test accuracy: 16.63
Round  11, Train loss: 1.993, Test loss: 2.276, Test accuracy: 16.83
Round  12, Train loss: 1.889, Test loss: 2.278, Test accuracy: 16.92
Round  13, Train loss: 1.899, Test loss: 2.289, Test accuracy: 15.09
Round  14, Train loss: 1.882, Test loss: 2.281, Test accuracy: 15.61
Round  15, Train loss: 1.851, Test loss: 2.276, Test accuracy: 16.11
Round  16, Train loss: 1.899, Test loss: 2.276, Test accuracy: 16.58
Round  17, Train loss: 1.872, Test loss: 2.270, Test accuracy: 16.96
Round  18, Train loss: 1.872, Test loss: 2.276, Test accuracy: 16.63
Round  19, Train loss: 1.903, Test loss: 2.276, Test accuracy: 15.89
Round  20, Train loss: 1.888, Test loss: 2.268, Test accuracy: 17.35
Round  21, Train loss: 1.835, Test loss: 2.266, Test accuracy: 17.43
Round  22, Train loss: 1.811, Test loss: 2.278, Test accuracy: 16.17
Round  23, Train loss: 1.858, Test loss: 2.284, Test accuracy: 15.21
Round  24, Train loss: 1.856, Test loss: 2.280, Test accuracy: 16.02
Round  25, Train loss: 1.891, Test loss: 2.274, Test accuracy: 16.86
Round  26, Train loss: 1.778, Test loss: 2.265, Test accuracy: 18.17
Round  27, Train loss: 1.796, Test loss: 2.275, Test accuracy: 16.99
Round  28, Train loss: 1.808, Test loss: 2.275, Test accuracy: 16.79
Round  29, Train loss: 1.840, Test loss: 2.274, Test accuracy: 16.17
Round  30, Train loss: 1.805, Test loss: 2.271, Test accuracy: 16.79
Round  31, Train loss: 1.752, Test loss: 2.272, Test accuracy: 16.65
Round  32, Train loss: 1.809, Test loss: 2.274, Test accuracy: 16.72
Round  33, Train loss: 1.795, Test loss: 2.268, Test accuracy: 17.21
Round  34, Train loss: 1.799, Test loss: 2.276, Test accuracy: 16.42
Round  35, Train loss: 1.795, Test loss: 2.279, Test accuracy: 15.44
Round  36, Train loss: 1.737, Test loss: 2.277, Test accuracy: 15.92
Round  37, Train loss: 1.735, Test loss: 2.277, Test accuracy: 15.51
Round  38, Train loss: 1.789, Test loss: 2.279, Test accuracy: 15.70
Round  39, Train loss: 1.768, Test loss: 2.268, Test accuracy: 17.60
Round  40, Train loss: 1.767, Test loss: 2.280, Test accuracy: 15.82
Round  41, Train loss: 1.737, Test loss: 2.277, Test accuracy: 15.98
Round  42, Train loss: 1.730, Test loss: 2.268, Test accuracy: 17.05
Round  43, Train loss: 1.728, Test loss: 2.278, Test accuracy: 16.39
Round  44, Train loss: 1.790, Test loss: 2.268, Test accuracy: 17.23
Round  45, Train loss: 1.808, Test loss: 2.276, Test accuracy: 16.21
Round  46, Train loss: 1.785, Test loss: 2.272, Test accuracy: 16.85
Round  47, Train loss: 1.711, Test loss: 2.283, Test accuracy: 15.99
Round  48, Train loss: 1.730, Test loss: 2.266, Test accuracy: 17.26
Round  49, Train loss: 1.699, Test loss: 2.278, Test accuracy: 16.17
Round  50, Train loss: 1.668, Test loss: 2.272, Test accuracy: 16.66
Round  51, Train loss: 1.677, Test loss: 2.271, Test accuracy: 16.74
Round  52, Train loss: 1.667, Test loss: 2.281, Test accuracy: 15.75
Round  53, Train loss: 1.752, Test loss: 2.273, Test accuracy: 16.67
Round  54, Train loss: 1.652, Test loss: 2.273, Test accuracy: 16.55
Round  55, Train loss: 1.688, Test loss: 2.266, Test accuracy: 17.36
Round  56, Train loss: 1.708, Test loss: 2.277, Test accuracy: 15.85
Round  57, Train loss: 1.661, Test loss: 2.274, Test accuracy: 16.57
Round  58, Train loss: 1.639, Test loss: 2.279, Test accuracy: 15.78
Round  59, Train loss: 1.689, Test loss: 2.277, Test accuracy: 15.56
Round  60, Train loss: 1.706, Test loss: 2.270, Test accuracy: 16.65
Round  61, Train loss: 1.657, Test loss: 2.279, Test accuracy: 15.51
Round  62, Train loss: 1.694, Test loss: 2.287, Test accuracy: 14.96
Round  63, Train loss: 1.690, Test loss: 2.278, Test accuracy: 15.89
Round  64, Train loss: 1.647, Test loss: 2.275, Test accuracy: 16.47
Round  65, Train loss: 1.733, Test loss: 2.284, Test accuracy: 15.34
Round  66, Train loss: 1.676, Test loss: 2.283, Test accuracy: 15.24
Round  67, Train loss: 1.659, Test loss: 2.272, Test accuracy: 16.68
Round  68, Train loss: 1.692, Test loss: 2.280, Test accuracy: 15.38
Round  69, Train loss: 1.684, Test loss: 2.282, Test accuracy: 15.41
Round  70, Train loss: 1.687, Test loss: 2.281, Test accuracy: 15.55
Round  71, Train loss: 1.678, Test loss: 2.269, Test accuracy: 17.05
Round  72, Train loss: 1.647, Test loss: 2.275, Test accuracy: 16.08
Round  73, Train loss: 1.651, Test loss: 2.279, Test accuracy: 16.09
Round  74, Train loss: 1.688, Test loss: 2.283, Test accuracy: 15.22
Round  75, Train loss: 1.620, Test loss: 2.272, Test accuracy: 16.82
Round  76, Train loss: 1.684, Test loss: 2.273, Test accuracy: 16.44
Round  77, Train loss: 1.627, Test loss: 2.270, Test accuracy: 16.88
Round  78, Train loss: 1.649, Test loss: 2.277, Test accuracy: 16.12
Round  79, Train loss: 1.615, Test loss: 2.267, Test accuracy: 17.24
Round  80, Train loss: 1.580, Test loss: 2.275, Test accuracy: 16.67
Round  81, Train loss: 1.639, Test loss: 2.274, Test accuracy: 16.59
Round  82, Train loss: 1.590, Test loss: 2.273, Test accuracy: 16.43
Round  83, Train loss: 1.668, Test loss: 2.275, Test accuracy: 16.42
Round  84, Train loss: 1.687, Test loss: 2.272, Test accuracy: 16.39
Round  85, Train loss: 1.637, Test loss: 2.270, Test accuracy: 16.66
Round  86, Train loss: 1.623, Test loss: 2.267, Test accuracy: 17.34
Round  87, Train loss: 1.650, Test loss: 2.280, Test accuracy: 15.51
Round  88, Train loss: 1.649, Test loss: 2.271, Test accuracy: 16.17
Round  89, Train loss: 1.656, Test loss: 2.274, Test accuracy: 15.93
Round  90, Train loss: 1.593, Test loss: 2.271, Test accuracy: 16.43
Round  91, Train loss: 1.641, Test loss: 2.281, Test accuracy: 15.61
Round  92, Train loss: 1.584, Test loss: 2.282, Test accuracy: 16.07
Round  93, Train loss: 1.648, Test loss: 2.269, Test accuracy: 17.18
Round  94, Train loss: 1.603, Test loss: 2.284, Test accuracy: 15.63
Round  95, Train loss: 1.623, Test loss: 2.274, Test accuracy: 16.49
Round  96, Train loss: 1.656, Test loss: 2.272, Test accuracy: 16.30
Round  97, Train loss: 1.632, Test loss: 2.275, Test accuracy: 15.81
Round  98, Train loss: 1.579, Test loss: 2.282, Test accuracy: 15.72
Round  99, Train loss: 1.638, Test loss: 2.272, Test accuracy: 16.57
Final Round, Train loss: 1.610, Test loss: 2.262, Test accuracy: 17.00
Average accuracy final 10 rounds: 16.180999999999997
4741.445002317429
[7.1032469272613525, 18.06089973449707, 25.02825355529785, 32.03003263473511, 38.9345908164978, 45.89552402496338, 52.88737678527832, 59.83852458000183, 66.81047487258911, 73.76053261756897, 80.74902033805847, 87.73365712165833, 94.76146864891052, 101.8319399356842, 108.8791253566742, 116.00773787498474, 123.1655170917511, 130.30385851860046, 137.37323474884033, 144.48886513710022, 151.55477476119995, 158.64085483551025, 165.7478609085083, 172.8846893310547, 179.96992945671082, 187.0577552318573, 194.17577266693115, 201.25631642341614, 208.41015481948853, 215.56461262702942, 222.63138246536255, 228.9627866744995, 235.2418611049652, 241.5568675994873, 247.84217882156372, 254.05563807487488, 260.29391050338745, 266.59857511520386, 272.8931415081024, 279.1095805168152, 285.33464312553406, 291.5816879272461, 297.83504986763, 304.06873083114624, 310.29531955718994, 316.611328125, 323.1455581188202, 329.5832562446594, 335.840336561203, 342.3531882762909, 348.7075114250183, 354.9867732524872, 361.28168272972107, 367.5525143146515, 373.8647711277008, 380.1064438819885, 386.59741711616516, 392.8193907737732, 399.03730368614197, 405.5631160736084, 411.868940114975, 418.1190700531006, 424.39435744285583, 430.64280247688293, 436.96191143989563, 443.3341166973114, 449.5457978248596, 455.7944357395172, 462.0411853790283, 468.30961441993713, 474.5809335708618, 480.82246017456055, 487.1138732433319, 493.5518937110901, 499.93078804016113, 506.22192549705505, 512.5315020084381, 518.8250749111176, 525.2007274627686, 531.46874833107, 537.7875375747681, 544.2758433818817, 550.637552022934, 556.9941291809082, 563.3061938285828, 569.7052075862885, 576.0899896621704, 582.3418657779694, 589.0819885730743, 595.8485388755798, 602.6017661094666, 609.3265368938446, 616.0596935749054, 622.7444727420807, 629.063992023468, 635.3470921516418, 641.6261985301971, 647.9595220088959, 654.2728946208954, 660.5171754360199, 662.6658091545105]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[10.846666666666666, 12.353333333333333, 13.63, 14.923333333333334, 15.16, 15.613333333333333, 16.136666666666667, 15.196666666666667, 16.16, 15.3, 16.626666666666665, 16.826666666666668, 16.916666666666668, 15.093333333333334, 15.606666666666667, 16.106666666666666, 16.583333333333332, 16.96, 16.63, 15.89, 17.35333333333333, 17.433333333333334, 16.173333333333332, 15.206666666666667, 16.016666666666666, 16.86, 18.166666666666668, 16.99, 16.793333333333333, 16.166666666666668, 16.793333333333333, 16.64666666666667, 16.716666666666665, 17.213333333333335, 16.42, 15.436666666666667, 15.92, 15.513333333333334, 15.703333333333333, 17.596666666666668, 15.823333333333334, 15.976666666666667, 17.046666666666667, 16.393333333333334, 17.23, 16.206666666666667, 16.846666666666668, 15.99, 17.256666666666668, 16.166666666666668, 16.656666666666666, 16.74, 15.753333333333334, 16.666666666666668, 16.55, 17.363333333333333, 15.846666666666666, 16.566666666666666, 15.78, 15.563333333333333, 16.653333333333332, 15.513333333333334, 14.956666666666667, 15.886666666666667, 16.466666666666665, 15.336666666666666, 15.24, 16.68, 15.383333333333333, 15.406666666666666, 15.546666666666667, 17.05, 16.083333333333332, 16.09, 15.216666666666667, 16.82, 16.44333333333333, 16.88, 16.12, 17.236666666666668, 16.666666666666668, 16.593333333333334, 16.426666666666666, 16.416666666666668, 16.386666666666667, 16.663333333333334, 17.336666666666666, 15.513333333333334, 16.166666666666668, 15.926666666666666, 16.433333333333334, 15.606666666666667, 16.073333333333334, 17.18, 15.633333333333333, 16.49, 16.296666666666667, 15.806666666666667, 15.723333333333333, 16.566666666666666, 17.003333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.312, Test loss: 2.302, Test accuracy: 10.75
Round   1, Train loss: 2.286, Test loss: 2.298, Test accuracy: 13.65
Round   2, Train loss: 2.231, Test loss: 2.282, Test accuracy: 16.08
Round   3, Train loss: 2.177, Test loss: 2.250, Test accuracy: 20.76
Round   4, Train loss: 2.131, Test loss: 2.202, Test accuracy: 25.84
Round   5, Train loss: 2.056, Test loss: 2.147, Test accuracy: 32.16
Round   6, Train loss: 2.011, Test loss: 2.104, Test accuracy: 37.45
Round   7, Train loss: 1.964, Test loss: 2.058, Test accuracy: 44.26
Round   8, Train loss: 1.912, Test loss: 2.037, Test accuracy: 45.06
Round   9, Train loss: 1.928, Test loss: 2.001, Test accuracy: 49.59
Round  10, Train loss: 1.940, Test loss: 1.982, Test accuracy: 51.72
Round  11, Train loss: 1.942, Test loss: 1.929, Test accuracy: 58.28
Round  12, Train loss: 1.851, Test loss: 1.906, Test accuracy: 59.37
Round  13, Train loss: 1.846, Test loss: 1.893, Test accuracy: 61.44
Round  14, Train loss: 1.857, Test loss: 1.873, Test accuracy: 63.22
Round  15, Train loss: 1.786, Test loss: 1.864, Test accuracy: 64.49
Round  16, Train loss: 1.786, Test loss: 1.848, Test accuracy: 65.19
Round  17, Train loss: 1.776, Test loss: 1.837, Test accuracy: 66.33
Round  18, Train loss: 1.820, Test loss: 1.818, Test accuracy: 68.86
Round  19, Train loss: 1.870, Test loss: 1.813, Test accuracy: 70.22
Round  20, Train loss: 1.778, Test loss: 1.783, Test accuracy: 71.64
Round  21, Train loss: 1.744, Test loss: 1.780, Test accuracy: 71.94
Round  22, Train loss: 1.727, Test loss: 1.762, Test accuracy: 73.07
Round  23, Train loss: 1.711, Test loss: 1.762, Test accuracy: 73.14
Round  24, Train loss: 1.790, Test loss: 1.765, Test accuracy: 73.95
Round  25, Train loss: 1.729, Test loss: 1.755, Test accuracy: 74.25
Round  26, Train loss: 1.719, Test loss: 1.750, Test accuracy: 74.19
Round  27, Train loss: 1.741, Test loss: 1.740, Test accuracy: 74.99
Round  28, Train loss: 1.708, Test loss: 1.737, Test accuracy: 75.53
Round  29, Train loss: 1.691, Test loss: 1.727, Test accuracy: 76.28
Round  30, Train loss: 1.703, Test loss: 1.723, Test accuracy: 76.58
Round  31, Train loss: 1.729, Test loss: 1.717, Test accuracy: 78.38
Round  32, Train loss: 1.695, Test loss: 1.703, Test accuracy: 79.32
Round  33, Train loss: 1.650, Test loss: 1.688, Test accuracy: 80.17
Round  34, Train loss: 1.636, Test loss: 1.690, Test accuracy: 80.24
Round  35, Train loss: 1.612, Test loss: 1.683, Test accuracy: 80.19
Round  36, Train loss: 1.672, Test loss: 1.672, Test accuracy: 81.06
Round  37, Train loss: 1.644, Test loss: 1.668, Test accuracy: 81.58
Round  38, Train loss: 1.670, Test loss: 1.680, Test accuracy: 81.54
Round  39, Train loss: 1.676, Test loss: 1.670, Test accuracy: 82.37
Round  40, Train loss: 1.644, Test loss: 1.657, Test accuracy: 83.21
Round  41, Train loss: 1.618, Test loss: 1.654, Test accuracy: 83.20
Round  42, Train loss: 1.620, Test loss: 1.650, Test accuracy: 83.36
Round  43, Train loss: 1.629, Test loss: 1.648, Test accuracy: 83.79
Round  44, Train loss: 1.591, Test loss: 1.647, Test accuracy: 83.94
Round  45, Train loss: 1.623, Test loss: 1.649, Test accuracy: 83.91
Round  46, Train loss: 1.606, Test loss: 1.639, Test accuracy: 84.32
Round  47, Train loss: 1.645, Test loss: 1.641, Test accuracy: 84.56
Round  48, Train loss: 1.567, Test loss: 1.627, Test accuracy: 85.15
Round  49, Train loss: 1.619, Test loss: 1.627, Test accuracy: 85.90
Round  50, Train loss: 1.641, Test loss: 1.636, Test accuracy: 85.94
Round  51, Train loss: 1.669, Test loss: 1.634, Test accuracy: 86.45
Round  52, Train loss: 1.590, Test loss: 1.617, Test accuracy: 86.94
Round  53, Train loss: 1.633, Test loss: 1.614, Test accuracy: 87.91
Round  54, Train loss: 1.551, Test loss: 1.606, Test accuracy: 88.03
Round  55, Train loss: 1.552, Test loss: 1.600, Test accuracy: 88.34
Round  56, Train loss: 1.544, Test loss: 1.599, Test accuracy: 88.79
Round  57, Train loss: 1.605, Test loss: 1.599, Test accuracy: 88.80
Round  58, Train loss: 1.573, Test loss: 1.594, Test accuracy: 88.84
Round  59, Train loss: 1.586, Test loss: 1.593, Test accuracy: 89.32
Round  60, Train loss: 1.559, Test loss: 1.587, Test accuracy: 89.47
Round  61, Train loss: 1.590, Test loss: 1.588, Test accuracy: 90.03
Round  62, Train loss: 1.518, Test loss: 1.578, Test accuracy: 90.05
Round  63, Train loss: 1.521, Test loss: 1.578, Test accuracy: 90.03
Round  64, Train loss: 1.564, Test loss: 1.584, Test accuracy: 90.02
Round  65, Train loss: 1.570, Test loss: 1.584, Test accuracy: 90.08
Round  66, Train loss: 1.544, Test loss: 1.576, Test accuracy: 90.61
Round  67, Train loss: 1.529, Test loss: 1.572, Test accuracy: 90.97
Round  68, Train loss: 1.553, Test loss: 1.575, Test accuracy: 91.05
Round  69, Train loss: 1.592, Test loss: 1.583, Test accuracy: 91.02
Round  70, Train loss: 1.582, Test loss: 1.589, Test accuracy: 90.59
Round  71, Train loss: 1.602, Test loss: 1.574, Test accuracy: 91.36
Round  72, Train loss: 1.524, Test loss: 1.569, Test accuracy: 91.40
Round  73, Train loss: 1.619, Test loss: 1.576, Test accuracy: 91.44
Round  74, Train loss: 1.500, Test loss: 1.560, Test accuracy: 91.94
Round  75, Train loss: 1.511, Test loss: 1.559, Test accuracy: 92.01
Round  76, Train loss: 1.557, Test loss: 1.559, Test accuracy: 92.21
Round  77, Train loss: 1.527, Test loss: 1.552, Test accuracy: 92.90
Round  78, Train loss: 1.519, Test loss: 1.551, Test accuracy: 92.97
Round  79, Train loss: 1.506, Test loss: 1.552, Test accuracy: 92.96
Round  80, Train loss: 1.578, Test loss: 1.554, Test accuracy: 93.30
Round  81, Train loss: 1.493, Test loss: 1.544, Test accuracy: 93.57
Round  82, Train loss: 1.503, Test loss: 1.545, Test accuracy: 93.55
Round  83, Train loss: 1.561, Test loss: 1.549, Test accuracy: 93.54
Round  84, Train loss: 1.561, Test loss: 1.550, Test accuracy: 93.53
Round  85, Train loss: 1.550, Test loss: 1.546, Test accuracy: 93.92
Round  86, Train loss: 1.520, Test loss: 1.541, Test accuracy: 94.03
Round  87, Train loss: 1.529, Test loss: 1.543, Test accuracy: 94.10
Round  88, Train loss: 1.534, Test loss: 1.538, Test accuracy: 94.55
Round  89, Train loss: 1.515, Test loss: 1.539, Test accuracy: 94.58
Round  90, Train loss: 1.502, Test loss: 1.536, Test accuracy: 94.51
Round  91, Train loss: 1.499, Test loss: 1.535, Test accuracy: 94.56
Round  92, Train loss: 1.515, Test loss: 1.536, Test accuracy: 94.61
Round  93, Train loss: 1.485, Test loss: 1.534, Test accuracy: 94.63
Round  94, Train loss: 1.555, Test loss: 1.541, Test accuracy: 94.54/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.517, Test loss: 1.537, Test accuracy: 94.59
Round  96, Train loss: 1.512, Test loss: 1.537, Test accuracy: 94.61
Round  97, Train loss: 1.501, Test loss: 1.535, Test accuracy: 94.62
Round  98, Train loss: 1.497, Test loss: 1.534, Test accuracy: 94.57
Round  99, Train loss: 1.512, Test loss: 1.535, Test accuracy: 94.67
Final Round, Train loss: 1.505, Test loss: 1.532, Test accuracy: 94.70
Average accuracy final 10 rounds: 94.59150000000001
3706.263632774353
[4.5357770919799805, 8.9412362575531, 13.329565048217773, 17.75167965888977, 22.089495420455933, 26.535099029541016, 30.97965097427368, 35.509761095047, 39.94230818748474, 44.3333375453949, 48.797481060028076, 53.07037830352783, 57.43320608139038, 61.80323576927185, 66.31170201301575, 70.77086639404297, 75.04496812820435, 79.58012199401855, 83.98979353904724, 88.35139632225037, 92.85611701011658, 97.13640832901001, 101.6079454421997, 106.05598711967468, 110.49756407737732, 114.81889009475708, 119.16787552833557, 123.60817432403564, 127.78237104415894, 132.21677207946777, 136.60392332077026, 140.97949934005737, 145.5085802078247, 149.92046213150024, 154.38030815124512, 159.07625150680542, 163.66225934028625, 168.12995505332947, 172.50990986824036, 177.01491355895996, 181.48224020004272, 185.79854035377502, 190.26768469810486, 194.569274187088, 198.96075105667114, 203.37438416481018, 207.7080979347229, 212.19601607322693, 216.58045983314514, 221.0747468471527, 225.61193132400513, 229.87461638450623, 234.35888242721558, 238.7277352809906, 243.23374199867249, 247.7194004058838, 252.03609943389893, 256.49741554260254, 260.80658507347107, 265.3336856365204, 269.89802622795105, 274.2286207675934, 278.76736879348755, 283.15839862823486, 287.50807905197144, 292.035927772522, 296.4134383201599, 300.8641049861908, 305.0941638946533, 309.3415746688843, 313.8320279121399, 318.2501618862152, 322.63544273376465, 326.97320556640625, 331.41122674942017, 335.7934672832489, 340.2456338405609, 344.76691031455994, 349.0903158187866, 353.55206632614136, 358.28527545928955, 362.86040925979614, 367.4086277484894, 371.7610013484955, 376.31724548339844, 380.83109736442566, 385.2673509120941, 389.68771862983704, 394.0156955718994, 398.4565510749817, 402.8698310852051, 407.33841466903687, 411.8122777938843, 416.2867171764374, 420.8881461620331, 425.3330931663513, 429.80011892318726, 434.33812403678894, 438.7699394226074, 443.22621846199036, 444.8616282939911]
[10.75, 13.6475, 16.0825, 20.76, 25.835, 32.155, 37.4525, 44.255, 45.065, 49.595, 51.72, 58.2775, 59.3725, 61.4375, 63.215, 64.4875, 65.1875, 66.325, 68.855, 70.215, 71.6425, 71.945, 73.0675, 73.1425, 73.9525, 74.25, 74.1925, 74.99, 75.525, 76.2775, 76.5775, 78.375, 79.32, 80.1725, 80.2375, 80.195, 81.0625, 81.5825, 81.54, 82.3675, 83.2075, 83.205, 83.365, 83.79, 83.9375, 83.9125, 84.3225, 84.565, 85.1475, 85.9025, 85.94, 86.4525, 86.945, 87.905, 88.025, 88.345, 88.7875, 88.795, 88.84, 89.3175, 89.4675, 90.0325, 90.0525, 90.03, 90.0175, 90.08, 90.605, 90.975, 91.05, 91.015, 90.59, 91.3625, 91.4025, 91.44, 91.94, 92.0075, 92.2125, 92.9, 92.97, 92.96, 93.2975, 93.5725, 93.55, 93.54, 93.53, 93.925, 94.035, 94.1025, 94.545, 94.5825, 94.5075, 94.565, 94.61, 94.6275, 94.54, 94.59, 94.6075, 94.62, 94.5725, 94.675, 94.6975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.310, Test loss: 2.302, Test accuracy: 9.62
Round   1, Train loss: 2.284, Test loss: 2.296, Test accuracy: 13.11
Round   2, Train loss: 2.237, Test loss: 2.279, Test accuracy: 13.88
Round   3, Train loss: 2.175, Test loss: 2.241, Test accuracy: 21.19
Round   4, Train loss: 2.126, Test loss: 2.210, Test accuracy: 25.90
Round   5, Train loss: 2.069, Test loss: 2.164, Test accuracy: 31.27
Round   6, Train loss: 2.010, Test loss: 2.117, Test accuracy: 37.08
Round   7, Train loss: 2.052, Test loss: 2.076, Test accuracy: 43.16
Round   8, Train loss: 2.024, Test loss: 2.045, Test accuracy: 47.22
Round   9, Train loss: 1.989, Test loss: 1.982, Test accuracy: 53.56
Round  10, Train loss: 1.941, Test loss: 1.961, Test accuracy: 55.42
Round  11, Train loss: 1.885, Test loss: 1.914, Test accuracy: 58.17
Round  12, Train loss: 1.832, Test loss: 1.857, Test accuracy: 63.44
Round  13, Train loss: 1.824, Test loss: 1.846, Test accuracy: 65.46
Round  14, Train loss: 1.781, Test loss: 1.834, Test accuracy: 66.14
Round  15, Train loss: 1.831, Test loss: 1.818, Test accuracy: 68.27
Round  16, Train loss: 1.760, Test loss: 1.793, Test accuracy: 70.42
Round  17, Train loss: 1.738, Test loss: 1.774, Test accuracy: 72.47
Round  18, Train loss: 1.720, Test loss: 1.765, Test accuracy: 73.93
Round  19, Train loss: 1.727, Test loss: 1.743, Test accuracy: 75.60
Round  20, Train loss: 1.690, Test loss: 1.730, Test accuracy: 76.97
Round  21, Train loss: 1.671, Test loss: 1.725, Test accuracy: 78.01
Round  22, Train loss: 1.713, Test loss: 1.705, Test accuracy: 80.32
Round  23, Train loss: 1.678, Test loss: 1.687, Test accuracy: 81.30
Round  24, Train loss: 1.633, Test loss: 1.674, Test accuracy: 82.31
Round  25, Train loss: 1.624, Test loss: 1.664, Test accuracy: 83.08
Round  26, Train loss: 1.674, Test loss: 1.655, Test accuracy: 83.99
Round  27, Train loss: 1.676, Test loss: 1.639, Test accuracy: 86.10
Round  28, Train loss: 1.599, Test loss: 1.631, Test accuracy: 86.69
Round  29, Train loss: 1.561, Test loss: 1.618, Test accuracy: 87.32
Round  30, Train loss: 1.598, Test loss: 1.617, Test accuracy: 87.63
Round  31, Train loss: 1.642, Test loss: 1.623, Test accuracy: 87.71
Round  32, Train loss: 1.638, Test loss: 1.615, Test accuracy: 88.12
Round  33, Train loss: 1.628, Test loss: 1.617, Test accuracy: 88.27
Round  34, Train loss: 1.590, Test loss: 1.610, Test accuracy: 88.31
Round  35, Train loss: 1.600, Test loss: 1.605, Test accuracy: 88.76
Round  36, Train loss: 1.595, Test loss: 1.606, Test accuracy: 88.87
Round  37, Train loss: 1.570, Test loss: 1.597, Test accuracy: 88.94
Round  38, Train loss: 1.592, Test loss: 1.596, Test accuracy: 88.97
Round  39, Train loss: 1.600, Test loss: 1.587, Test accuracy: 90.18
Round  40, Train loss: 1.559, Test loss: 1.579, Test accuracy: 90.59
Round  41, Train loss: 1.561, Test loss: 1.577, Test accuracy: 90.94
Round  42, Train loss: 1.533, Test loss: 1.571, Test accuracy: 91.49
Round  43, Train loss: 1.541, Test loss: 1.570, Test accuracy: 91.83
Round  44, Train loss: 1.581, Test loss: 1.571, Test accuracy: 92.07
Round  45, Train loss: 1.576, Test loss: 1.563, Test accuracy: 92.24
Round  46, Train loss: 1.532, Test loss: 1.554, Test accuracy: 92.89
Round  47, Train loss: 1.527, Test loss: 1.552, Test accuracy: 93.42
Round  48, Train loss: 1.559, Test loss: 1.552, Test accuracy: 93.46
Round  49, Train loss: 1.525, Test loss: 1.548, Test accuracy: 93.55
Round  50, Train loss: 1.548, Test loss: 1.551, Test accuracy: 93.58
Round  51, Train loss: 1.575, Test loss: 1.550, Test accuracy: 93.57
Round  52, Train loss: 1.524, Test loss: 1.543, Test accuracy: 94.11
Round  53, Train loss: 1.571, Test loss: 1.538, Test accuracy: 94.65
Round  54, Train loss: 1.516, Test loss: 1.533, Test accuracy: 95.06
Round  55, Train loss: 1.546, Test loss: 1.536, Test accuracy: 95.13
Round  56, Train loss: 1.513, Test loss: 1.535, Test accuracy: 95.12
Round  57, Train loss: 1.530, Test loss: 1.531, Test accuracy: 95.22
Round  58, Train loss: 1.533, Test loss: 1.532, Test accuracy: 95.25
Round  59, Train loss: 1.512, Test loss: 1.525, Test accuracy: 95.87
Round  60, Train loss: 1.528, Test loss: 1.528, Test accuracy: 95.72
Round  61, Train loss: 1.515, Test loss: 1.527, Test accuracy: 95.77
Round  62, Train loss: 1.535, Test loss: 1.523, Test accuracy: 96.12
Round  63, Train loss: 1.511, Test loss: 1.524, Test accuracy: 96.14
Round  64, Train loss: 1.515, Test loss: 1.521, Test accuracy: 96.12
Round  65, Train loss: 1.511, Test loss: 1.522, Test accuracy: 96.25
Round  66, Train loss: 1.502, Test loss: 1.517, Test accuracy: 96.31
Round  67, Train loss: 1.495, Test loss: 1.519, Test accuracy: 96.38
Round  68, Train loss: 1.510, Test loss: 1.518, Test accuracy: 96.40
Round  69, Train loss: 1.518, Test loss: 1.514, Test accuracy: 96.77
Round  70, Train loss: 1.508, Test loss: 1.516, Test accuracy: 96.86
Round  71, Train loss: 1.507, Test loss: 1.515, Test accuracy: 96.87
Round  72, Train loss: 1.510, Test loss: 1.513, Test accuracy: 96.92
Round  73, Train loss: 1.494, Test loss: 1.513, Test accuracy: 96.97
Round  74, Train loss: 1.511, Test loss: 1.512, Test accuracy: 96.87
Round  75, Train loss: 1.510, Test loss: 1.511, Test accuracy: 97.00
Round  76, Train loss: 1.505, Test loss: 1.513, Test accuracy: 96.97
Round  77, Train loss: 1.491, Test loss: 1.513, Test accuracy: 96.97
Round  78, Train loss: 1.508, Test loss: 1.511, Test accuracy: 97.00
Round  79, Train loss: 1.507, Test loss: 1.511, Test accuracy: 96.94
Round  80, Train loss: 1.492, Test loss: 1.511, Test accuracy: 96.95
Round  81, Train loss: 1.516, Test loss: 1.512, Test accuracy: 96.97
Round  82, Train loss: 1.508, Test loss: 1.510, Test accuracy: 97.00
Round  83, Train loss: 1.490, Test loss: 1.510, Test accuracy: 96.94
Round  84, Train loss: 1.491, Test loss: 1.510, Test accuracy: 96.94
Round  85, Train loss: 1.495, Test loss: 1.509, Test accuracy: 96.96
Round  86, Train loss: 1.491, Test loss: 1.509, Test accuracy: 96.99
Round  87, Train loss: 1.503, Test loss: 1.508, Test accuracy: 97.03
Round  88, Train loss: 1.506, Test loss: 1.506, Test accuracy: 97.45
Round  89, Train loss: 1.492, Test loss: 1.505, Test accuracy: 97.45
Round  90, Train loss: 1.493, Test loss: 1.504, Test accuracy: 97.46
Round  91, Train loss: 1.496, Test loss: 1.502, Test accuracy: 97.83
Round  92, Train loss: 1.489, Test loss: 1.503, Test accuracy: 97.89
Round  93, Train loss: 1.492, Test loss: 1.501, Test accuracy: 97.91/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.490, Test loss: 1.502, Test accuracy: 97.94
Round  95, Train loss: 1.487, Test loss: 1.503, Test accuracy: 97.92
Round  96, Train loss: 1.491, Test loss: 1.500, Test accuracy: 97.94
Round  97, Train loss: 1.488, Test loss: 1.501, Test accuracy: 97.94
Round  98, Train loss: 1.489, Test loss: 1.500, Test accuracy: 97.96
Round  99, Train loss: 1.490, Test loss: 1.500, Test accuracy: 97.94
Final Round, Train loss: 1.474, Test loss: 1.499, Test accuracy: 97.94
Average accuracy final 10 rounds: 97.87274999999998
4729.609477043152
[4.764034748077393, 9.528069496154785, 14.066325902938843, 18.6045823097229, 23.13263463973999, 27.66068696975708, 32.02359962463379, 36.3865122795105, 40.86607074737549, 45.34562921524048, 49.78568935394287, 54.225749492645264, 58.58135485649109, 62.936960220336914, 67.090402841568, 71.24384546279907, 75.54174399375916, 79.83964252471924, 84.29893708229065, 88.75823163986206, 93.09445714950562, 97.43068265914917, 101.67734289169312, 105.92400312423706, 110.31312608718872, 114.70224905014038, 118.9699375629425, 123.23762607574463, 127.72526478767395, 132.21290349960327, 136.53060340881348, 140.84830331802368, 145.08290028572083, 149.31749725341797, 153.57018971443176, 157.82288217544556, 161.95696020126343, 166.0910382270813, 170.25258493423462, 174.41413164138794, 178.75613379478455, 183.09813594818115, 187.64899158477783, 192.1998472213745, 196.74090552330017, 201.28196382522583, 205.52607536315918, 209.77018690109253, 214.29368782043457, 218.8171887397766, 223.41286730766296, 228.00854587554932, 232.58113956451416, 237.153733253479, 241.68132281303406, 246.2089123725891, 250.612619638443, 255.01632690429688, 259.5622637271881, 264.10820055007935, 268.7709300518036, 273.43365955352783, 278.0317964553833, 282.62993335723877, 287.0801818370819, 291.53043031692505, 296.0821304321289, 300.63383054733276, 305.3223946094513, 310.0109586715698, 314.7094933986664, 319.40802812576294, 324.12226152420044, 328.83649492263794, 333.2779748439789, 337.7194547653198, 342.32141733169556, 346.9233798980713, 351.58606791496277, 356.24875593185425, 360.805091381073, 365.36142683029175, 369.88279271125793, 374.4041585922241, 378.7599003314972, 383.11564207077026, 387.6683609485626, 392.221079826355, 396.77797961235046, 401.33487939834595, 405.94365072250366, 410.5524220466614, 415.1109495162964, 419.6694769859314, 423.8914635181427, 428.113450050354, 432.17552876472473, 436.23760747909546, 440.3217387199402, 444.4058699607849, 448.49532198905945, 452.584774017334, 456.57456636428833, 460.5643587112427, 464.46660470962524, 468.3688507080078, 472.4369294643402, 476.5050082206726, 480.6847469806671, 484.8644857406616, 488.87990069389343, 492.89531564712524, 496.9558620452881, 501.0164084434509, 505.08287286758423, 509.14933729171753, 513.1792674064636, 517.2091975212097, 521.171854019165, 525.1345105171204, 529.0624725818634, 532.9904346466064, 536.9359471797943, 540.8814597129822, 545.0183651447296, 549.155270576477, 553.1253900527954, 557.0955095291138, 561.2358205318451, 565.3761315345764, 569.5288822650909, 573.6816329956055, 577.8519949913025, 582.0223569869995, 586.07617020607, 590.1299834251404, 594.2294766902924, 598.3289699554443, 602.5529301166534, 606.7768902778625, 611.0175330638885, 615.2581758499146, 619.345587015152, 623.4329981803894, 627.5770514011383, 631.7211046218872, 635.8807125091553, 640.0403203964233, 644.2198724746704, 648.3994245529175, 652.4284930229187, 656.4575614929199, 660.6105332374573, 664.7635049819946, 668.9435427188873, 673.12358045578, 677.3583402633667, 681.5931000709534, 685.713730096817, 689.8343601226807, 693.9055840969086, 697.9768080711365, 702.1231994628906, 706.2695908546448, 710.4946193695068, 714.7196478843689, 718.9294190406799, 723.139190196991, 727.1327619552612, 731.1263337135315, 735.1792898178101, 739.2322459220886, 743.5842304229736, 747.9362149238586, 752.0909366607666, 756.2456583976746, 760.3142483234406, 764.3828382492065, 768.5578207969666, 772.7328033447266, 777.0031778812408, 781.2735524177551, 785.5286202430725, 789.7836880683899, 793.9719197750092, 798.1601514816284, 802.3417303562164, 806.5233092308044, 810.7899234294891, 815.0565376281738, 819.2716960906982, 823.4868545532227, 827.6281468868256, 831.7694392204285, 836.2211711406708, 840.6729030609131, 844.9148604869843, 849.1568179130554, 853.344804763794, 857.5327916145325, 859.1535301208496, 860.7742686271667]
[9.6225, 9.6225, 13.1125, 13.1125, 13.8775, 13.8775, 21.19, 21.19, 25.9, 25.9, 31.275, 31.275, 37.08, 37.08, 43.16, 43.16, 47.2175, 47.2175, 53.5575, 53.5575, 55.425, 55.425, 58.1675, 58.1675, 63.44, 63.44, 65.46, 65.46, 66.1375, 66.1375, 68.265, 68.265, 70.425, 70.425, 72.47, 72.47, 73.93, 73.93, 75.5975, 75.5975, 76.9725, 76.9725, 78.0125, 78.0125, 80.3225, 80.3225, 81.2975, 81.2975, 82.315, 82.315, 83.08, 83.08, 83.9875, 83.9875, 86.1025, 86.1025, 86.685, 86.685, 87.3225, 87.3225, 87.63, 87.63, 87.7125, 87.7125, 88.1175, 88.1175, 88.265, 88.265, 88.31, 88.31, 88.7575, 88.7575, 88.8675, 88.8675, 88.9425, 88.9425, 88.9675, 88.9675, 90.1825, 90.1825, 90.59, 90.59, 90.935, 90.935, 91.4925, 91.4925, 91.83, 91.83, 92.0675, 92.0675, 92.2375, 92.2375, 92.885, 92.885, 93.4225, 93.4225, 93.4575, 93.4575, 93.55, 93.55, 93.5825, 93.5825, 93.5725, 93.5725, 94.115, 94.115, 94.6475, 94.6475, 95.0575, 95.0575, 95.1275, 95.1275, 95.1225, 95.1225, 95.215, 95.215, 95.25, 95.25, 95.8675, 95.8675, 95.715, 95.715, 95.765, 95.765, 96.1175, 96.1175, 96.14, 96.14, 96.125, 96.125, 96.2475, 96.2475, 96.305, 96.305, 96.3825, 96.3825, 96.4025, 96.4025, 96.7675, 96.7675, 96.86, 96.86, 96.8675, 96.8675, 96.925, 96.925, 96.97, 96.97, 96.8725, 96.8725, 97.005, 97.005, 96.9725, 96.9725, 96.9675, 96.9675, 96.995, 96.995, 96.9375, 96.9375, 96.95, 96.95, 96.9675, 96.9675, 97.0, 97.0, 96.945, 96.945, 96.94, 96.94, 96.9625, 96.9625, 96.99, 96.99, 97.025, 97.025, 97.4475, 97.4475, 97.45, 97.45, 97.4625, 97.4625, 97.835, 97.835, 97.885, 97.885, 97.9075, 97.9075, 97.9425, 97.9425, 97.915, 97.915, 97.9375, 97.9375, 97.94, 97.94, 97.9625, 97.9625, 97.94, 97.94, 97.9375, 97.9375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 45, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 60, in <module>
    dataset_train, dataset_test, _, _, _,_ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 174, in get_data_v3
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_apfl.py", line 49, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_scaffold.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_pfedme.py", line 54, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_ditto.py", line 48, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 58, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar100  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 119, in <module>
    dataset_train, dataset_test, _, _, _ = get_data_v2(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 110, in get_data_v2
    dict_users_test, rand_set_all = noniid_v2(dataset_test, args.num_users, args.shard_per_user, args.num_classes,
  File "/home/ChenSM/code/FL_HLS/utils/sampling.py", line 134, in noniid_v2
    idx = np.random.choice(len(idxs_dict[label]), replace=False)
  File "mtrand.pyx", line 909, in numpy.random.mtrand.RandomState.choice
ValueError: a must be greater than 0 unless no samples are taken
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.003, Test loss: 1.712, Test accuracy: 39.18
Round   0, Global train loss: 2.003, Global test loss: 1.713, Global test accuracy: 40.11
Round   1, Train loss: 1.673, Test loss: 1.530, Test accuracy: 44.42
Round   1, Global train loss: 1.673, Global test loss: 1.449, Global test accuracy: 47.83
Round   2, Train loss: 1.535, Test loss: 1.422, Test accuracy: 48.27
Round   2, Global train loss: 1.535, Global test loss: 1.322, Global test accuracy: 51.98
Round   3, Train loss: 1.433, Test loss: 1.382, Test accuracy: 50.24
Round   3, Global train loss: 1.433, Global test loss: 1.238, Global test accuracy: 56.27
Round   4, Train loss: 1.354, Test loss: 1.374, Test accuracy: 50.51
Round   4, Global train loss: 1.354, Global test loss: 1.147, Global test accuracy: 59.37
Round   5, Train loss: 1.269, Test loss: 1.344, Test accuracy: 51.69
Round   5, Global train loss: 1.269, Global test loss: 1.088, Global test accuracy: 61.89
Round   6, Train loss: 1.201, Test loss: 1.283, Test accuracy: 54.33
Round   6, Global train loss: 1.201, Global test loss: 1.044, Global test accuracy: 63.43
Round   7, Train loss: 1.135, Test loss: 1.264, Test accuracy: 55.24
Round   7, Global train loss: 1.135, Global test loss: 1.011, Global test accuracy: 64.79
Round   8, Train loss: 1.082, Test loss: 1.236, Test accuracy: 56.26
Round   8, Global train loss: 1.082, Global test loss: 0.967, Global test accuracy: 66.39
Round   9, Train loss: 1.059, Test loss: 1.193, Test accuracy: 58.18
Round   9, Global train loss: 1.059, Global test loss: 0.934, Global test accuracy: 67.70
Round  10, Train loss: 1.003, Test loss: 1.175, Test accuracy: 59.04
Round  10, Global train loss: 1.003, Global test loss: 0.925, Global test accuracy: 68.07
Round  11, Train loss: 0.967, Test loss: 1.142, Test accuracy: 60.40
Round  11, Global train loss: 0.967, Global test loss: 0.890, Global test accuracy: 69.88
Round  12, Train loss: 0.961, Test loss: 1.109, Test accuracy: 61.91
Round  12, Global train loss: 0.961, Global test loss: 0.877, Global test accuracy: 69.52
Round  13, Train loss: 0.930, Test loss: 1.085, Test accuracy: 63.03
Round  13, Global train loss: 0.930, Global test loss: 0.855, Global test accuracy: 71.03
Round  14, Train loss: 0.875, Test loss: 1.080, Test accuracy: 63.39
Round  14, Global train loss: 0.875, Global test loss: 0.849, Global test accuracy: 71.16
Round  15, Train loss: 0.888, Test loss: 1.069, Test accuracy: 63.83
Round  15, Global train loss: 0.888, Global test loss: 0.832, Global test accuracy: 71.59
Round  16, Train loss: 0.856, Test loss: 1.050, Test accuracy: 64.38
Round  16, Global train loss: 0.856, Global test loss: 0.818, Global test accuracy: 71.74
Round  17, Train loss: 0.814, Test loss: 1.051, Test accuracy: 64.66
Round  17, Global train loss: 0.814, Global test loss: 0.804, Global test accuracy: 72.42
Round  18, Train loss: 0.790, Test loss: 1.036, Test accuracy: 65.47
Round  18, Global train loss: 0.790, Global test loss: 0.810, Global test accuracy: 72.65
Round  19, Train loss: 0.754, Test loss: 1.043, Test accuracy: 65.22
Round  19, Global train loss: 0.754, Global test loss: 0.799, Global test accuracy: 73.17
Round  20, Train loss: 0.785, Test loss: 1.013, Test accuracy: 66.20
Round  20, Global train loss: 0.785, Global test loss: 0.787, Global test accuracy: 73.21
Round  21, Train loss: 0.770, Test loss: 1.022, Test accuracy: 66.13
Round  21, Global train loss: 0.770, Global test loss: 0.778, Global test accuracy: 73.62
Round  22, Train loss: 0.755, Test loss: 1.012, Test accuracy: 66.47
Round  22, Global train loss: 0.755, Global test loss: 0.779, Global test accuracy: 73.32
Round  23, Train loss: 0.711, Test loss: 0.989, Test accuracy: 67.31
Round  23, Global train loss: 0.711, Global test loss: 0.766, Global test accuracy: 74.25
Round  24, Train loss: 0.712, Test loss: 0.991, Test accuracy: 67.36
Round  24, Global train loss: 0.712, Global test loss: 0.760, Global test accuracy: 74.15
Round  25, Train loss: 0.705, Test loss: 0.988, Test accuracy: 67.48
Round  25, Global train loss: 0.705, Global test loss: 0.760, Global test accuracy: 74.16
Round  26, Train loss: 0.699, Test loss: 1.004, Test accuracy: 67.41
Round  26, Global train loss: 0.699, Global test loss: 0.764, Global test accuracy: 74.17
Round  27, Train loss: 0.680, Test loss: 0.995, Test accuracy: 67.76
Round  27, Global train loss: 0.680, Global test loss: 0.749, Global test accuracy: 74.70
Round  28, Train loss: 0.670, Test loss: 0.984, Test accuracy: 68.06
Round  28, Global train loss: 0.670, Global test loss: 0.746, Global test accuracy: 74.89
Round  29, Train loss: 0.646, Test loss: 0.975, Test accuracy: 68.61
Round  29, Global train loss: 0.646, Global test loss: 0.746, Global test accuracy: 75.52
Round  30, Train loss: 0.645, Test loss: 0.956, Test accuracy: 68.99
Round  30, Global train loss: 0.645, Global test loss: 0.739, Global test accuracy: 75.63
Round  31, Train loss: 0.626, Test loss: 0.954, Test accuracy: 69.19
Round  31, Global train loss: 0.626, Global test loss: 0.754, Global test accuracy: 75.09
Round  32, Train loss: 0.607, Test loss: 0.954, Test accuracy: 69.43
Round  32, Global train loss: 0.607, Global test loss: 0.749, Global test accuracy: 74.63
Round  33, Train loss: 0.575, Test loss: 0.964, Test accuracy: 69.39
Round  33, Global train loss: 0.575, Global test loss: 0.759, Global test accuracy: 74.90
Round  34, Train loss: 0.631, Test loss: 0.965, Test accuracy: 69.50
Round  34, Global train loss: 0.631, Global test loss: 0.738, Global test accuracy: 75.41
Round  35, Train loss: 0.609, Test loss: 0.961, Test accuracy: 69.55
Round  35, Global train loss: 0.609, Global test loss: 0.746, Global test accuracy: 75.14
Round  36, Train loss: 0.567, Test loss: 0.954, Test accuracy: 69.57
Round  36, Global train loss: 0.567, Global test loss: 0.748, Global test accuracy: 75.38
Round  37, Train loss: 0.585, Test loss: 0.950, Test accuracy: 69.78
Round  37, Global train loss: 0.585, Global test loss: 0.743, Global test accuracy: 75.82
Round  38, Train loss: 0.590, Test loss: 0.954, Test accuracy: 69.76
Round  38, Global train loss: 0.590, Global test loss: 0.738, Global test accuracy: 75.75
Round  39, Train loss: 0.544, Test loss: 0.943, Test accuracy: 70.14
Round  39, Global train loss: 0.544, Global test loss: 0.745, Global test accuracy: 75.69
Round  40, Train loss: 0.592, Test loss: 0.936, Test accuracy: 70.27
Round  40, Global train loss: 0.592, Global test loss: 0.729, Global test accuracy: 75.64
Round  41, Train loss: 0.583, Test loss: 0.937, Test accuracy: 70.67
Round  41, Global train loss: 0.583, Global test loss: 0.732, Global test accuracy: 76.23
Round  42, Train loss: 0.574, Test loss: 0.941, Test accuracy: 70.69
Round  42, Global train loss: 0.574, Global test loss: 0.724, Global test accuracy: 76.30
Round  43, Train loss: 0.512, Test loss: 0.937, Test accuracy: 70.83
Round  43, Global train loss: 0.512, Global test loss: 0.729, Global test accuracy: 76.56
Round  44, Train loss: 0.552, Test loss: 0.944, Test accuracy: 70.83
Round  44, Global train loss: 0.552, Global test loss: 0.750, Global test accuracy: 75.71
Round  45, Train loss: 0.539, Test loss: 0.949, Test accuracy: 70.70
Round  45, Global train loss: 0.539, Global test loss: 0.730, Global test accuracy: 76.12
Round  46, Train loss: 0.547, Test loss: 0.952, Test accuracy: 70.77
Round  46, Global train loss: 0.547, Global test loss: 0.724, Global test accuracy: 76.51
Round  47, Train loss: 0.560, Test loss: 0.952, Test accuracy: 70.71
Round  47, Global train loss: 0.560, Global test loss: 0.711, Global test accuracy: 76.98
Round  48, Train loss: 0.533, Test loss: 0.954, Test accuracy: 70.89
Round  48, Global train loss: 0.533, Global test loss: 0.725, Global test accuracy: 76.51
Round  49, Train loss: 0.490, Test loss: 0.952, Test accuracy: 71.12
Round  49, Global train loss: 0.490, Global test loss: 0.744, Global test accuracy: 76.23
Round  50, Train loss: 0.511, Test loss: 0.953, Test accuracy: 71.23
Round  50, Global train loss: 0.511, Global test loss: 0.730, Global test accuracy: 76.89
Round  51, Train loss: 0.507, Test loss: 0.949, Test accuracy: 71.29
Round  51, Global train loss: 0.507, Global test loss: 0.727, Global test accuracy: 76.69
Round  52, Train loss: 0.528, Test loss: 0.953, Test accuracy: 71.17
Round  52, Global train loss: 0.528, Global test loss: 0.725, Global test accuracy: 76.37
Round  53, Train loss: 0.481, Test loss: 0.947, Test accuracy: 71.47
Round  53, Global train loss: 0.481, Global test loss: 0.730, Global test accuracy: 76.38
Round  54, Train loss: 0.536, Test loss: 0.956, Test accuracy: 71.31
Round  54, Global train loss: 0.536, Global test loss: 0.709, Global test accuracy: 76.80
Round  55, Train loss: 0.539, Test loss: 0.944, Test accuracy: 71.58
Round  55, Global train loss: 0.539, Global test loss: 0.714, Global test accuracy: 76.81
Round  56, Train loss: 0.495, Test loss: 0.955, Test accuracy: 71.41
Round  56, Global train loss: 0.495, Global test loss: 0.736, Global test accuracy: 76.34
Round  57, Train loss: 0.501, Test loss: 0.952, Test accuracy: 71.63
Round  57, Global train loss: 0.501, Global test loss: 0.736, Global test accuracy: 76.43
Round  58, Train loss: 0.472, Test loss: 0.942, Test accuracy: 71.64
Round  58, Global train loss: 0.472, Global test loss: 0.732, Global test accuracy: 76.51
Round  59, Train loss: 0.492, Test loss: 0.951, Test accuracy: 71.59
Round  59, Global train loss: 0.492, Global test loss: 0.728, Global test accuracy: 76.79
Round  60, Train loss: 0.505, Test loss: 0.960, Test accuracy: 71.44
Round  60, Global train loss: 0.505, Global test loss: 0.725, Global test accuracy: 77.08
Round  61, Train loss: 0.455, Test loss: 0.969, Test accuracy: 71.35
Round  61, Global train loss: 0.455, Global test loss: 0.725, Global test accuracy: 77.09
Round  62, Train loss: 0.463, Test loss: 0.968, Test accuracy: 71.48
Round  62, Global train loss: 0.463, Global test loss: 0.733, Global test accuracy: 76.83
Round  63, Train loss: 0.477, Test loss: 0.970, Test accuracy: 71.60
Round  63, Global train loss: 0.477, Global test loss: 0.739, Global test accuracy: 76.75
Round  64, Train loss: 0.469, Test loss: 0.959, Test accuracy: 71.93
Round  64, Global train loss: 0.469, Global test loss: 0.743, Global test accuracy: 77.00
Round  65, Train loss: 0.492, Test loss: 0.950, Test accuracy: 72.22
Round  65, Global train loss: 0.492, Global test loss: 0.731, Global test accuracy: 76.78
Round  66, Train loss: 0.439, Test loss: 0.959, Test accuracy: 72.07
Round  66, Global train loss: 0.439, Global test loss: 0.749, Global test accuracy: 76.58
Round  67, Train loss: 0.447, Test loss: 0.966, Test accuracy: 72.17
Round  67, Global train loss: 0.447, Global test loss: 0.751, Global test accuracy: 76.39
Round  68, Train loss: 0.470, Test loss: 0.964, Test accuracy: 72.01
Round  68, Global train loss: 0.470, Global test loss: 0.740, Global test accuracy: 76.17
Round  69, Train loss: 0.456, Test loss: 0.974, Test accuracy: 71.90
Round  69, Global train loss: 0.456, Global test loss: 0.727, Global test accuracy: 76.78
Round  70, Train loss: 0.472, Test loss: 0.964, Test accuracy: 71.84
Round  70, Global train loss: 0.472, Global test loss: 0.725, Global test accuracy: 76.65
Round  71, Train loss: 0.438, Test loss: 0.956, Test accuracy: 72.27
Round  71, Global train loss: 0.438, Global test loss: 0.739, Global test accuracy: 76.88
Round  72, Train loss: 0.438, Test loss: 0.949, Test accuracy: 72.54
Round  72, Global train loss: 0.438, Global test loss: 0.747, Global test accuracy: 77.07
Round  73, Train loss: 0.464, Test loss: 0.963, Test accuracy: 72.32
Round  73, Global train loss: 0.464, Global test loss: 0.739, Global test accuracy: 76.93
Round  74, Train loss: 0.421, Test loss: 0.962, Test accuracy: 72.43
Round  74, Global train loss: 0.421, Global test loss: 0.751, Global test accuracy: 76.78
Round  75, Train loss: 0.444, Test loss: 0.957, Test accuracy: 72.61
Round  75, Global train loss: 0.444, Global test loss: 0.736, Global test accuracy: 77.07
Round  76, Train loss: 0.389, Test loss: 0.954, Test accuracy: 72.79
Round  76, Global train loss: 0.389, Global test loss: 0.762, Global test accuracy: 76.95
Round  77, Train loss: 0.470, Test loss: 0.986, Test accuracy: 72.42
Round  77, Global train loss: 0.470, Global test loss: 0.755, Global test accuracy: 77.11
Round  78, Train loss: 0.442, Test loss: 0.984, Test accuracy: 72.64
Round  78, Global train loss: 0.442, Global test loss: 0.728, Global test accuracy: 77.58
Round  79, Train loss: 0.416, Test loss: 0.990, Test accuracy: 72.56
Round  79, Global train loss: 0.416, Global test loss: 0.738, Global test accuracy: 77.61
Round  80, Train loss: 0.434, Test loss: 0.997, Test accuracy: 72.39
Round  80, Global train loss: 0.434, Global test loss: 0.740, Global test accuracy: 77.32
Round  81, Train loss: 0.448, Test loss: 0.984, Test accuracy: 72.46
Round  81, Global train loss: 0.448, Global test loss: 0.739, Global test accuracy: 77.48
Round  82, Train loss: 0.414, Test loss: 0.972, Test accuracy: 72.47
Round  82, Global train loss: 0.414, Global test loss: 0.739, Global test accuracy: 76.94
Round  83, Train loss: 0.399, Test loss: 0.973, Test accuracy: 72.55
Round  83, Global train loss: 0.399, Global test loss: 0.759, Global test accuracy: 77.06
Round  84, Train loss: 0.422, Test loss: 0.977, Test accuracy: 72.58
Round  84, Global train loss: 0.422, Global test loss: 0.748, Global test accuracy: 77.48
Round  85, Train loss: 0.422, Test loss: 0.975, Test accuracy: 72.58
Round  85, Global train loss: 0.422, Global test loss: 0.734, Global test accuracy: 77.78
Round  86, Train loss: 0.400, Test loss: 0.979, Test accuracy: 72.64
Round  86, Global train loss: 0.400, Global test loss: 0.751, Global test accuracy: 77.62
Round  87, Train loss: 0.406, Test loss: 0.976, Test accuracy: 72.66
Round  87, Global train loss: 0.406, Global test loss: 0.736, Global test accuracy: 77.54
Round  88, Train loss: 0.434, Test loss: 0.986, Test accuracy: 72.47
Round  88, Global train loss: 0.434, Global test loss: 0.737, Global test accuracy: 77.23
Round  89, Train loss: 0.403, Test loss: 0.983, Test accuracy: 72.45
Round  89, Global train loss: 0.403, Global test loss: 0.742, Global test accuracy: 77.36
Round  90, Train loss: 0.401, Test loss: 0.986, Test accuracy: 72.78
Round  90, Global train loss: 0.401, Global test loss: 0.760, Global test accuracy: 77.61
Round  91, Train loss: 0.378, Test loss: 0.990, Test accuracy: 72.67
Round  91, Global train loss: 0.378, Global test loss: 0.762, Global test accuracy: 77.35
Round  92, Train loss: 0.405, Test loss: 0.988, Test accuracy: 72.50
Round  92, Global train loss: 0.405, Global test loss: 0.749, Global test accuracy: 77.14
Round  93, Train loss: 0.428, Test loss: 0.979, Test accuracy: 72.60
Round  93, Global train loss: 0.428, Global test loss: 0.747, Global test accuracy: 77.22
Round  94, Train loss: 0.403, Test loss: 0.970, Test accuracy: 72.92
Round  94, Global train loss: 0.403, Global test loss: 0.765, Global test accuracy: 77.52
Round  95, Train loss: 0.393, Test loss: 0.978, Test accuracy: 72.84
Round  95, Global train loss: 0.393, Global test loss: 0.760, Global test accuracy: 77.27
Round  96, Train loss: 0.394, Test loss: 0.974, Test accuracy: 72.95
Round  96, Global train loss: 0.394, Global test loss: 0.756, Global test accuracy: 77.72
Round  97, Train loss: 0.354, Test loss: 0.982, Test accuracy: 72.78
Round  97, Global train loss: 0.354, Global test loss: 0.769, Global test accuracy: 77.56
Round  98, Train loss: 0.440, Test loss: 0.981, Test accuracy: 72.86
Round  98, Global train loss: 0.440, Global test loss: 0.726, Global test accuracy: 77.95
Round  99, Train loss: 0.412, Test loss: 0.974, Test accuracy: 72.93
Round  99, Global train loss: 0.412, Global test loss: 0.726, Global test accuracy: 78.12
Final Round, Train loss: 0.236, Test loss: 1.067, Test accuracy: 74.28
Final Round, Global train loss: 0.236, Global test loss: 0.726, Global test accuracy: 78.12
Average accuracy final 10 rounds: 72.78225 

Average global accuracy final 10 rounds: 77.54525000000001 

6316.721673965454
[5.120383977890015, 10.24076795578003, 15.199033498764038, 20.157299041748047, 25.10030722618103, 30.043315410614014, 35.013702154159546, 39.98408889770508, 44.98337173461914, 49.9826545715332, 54.96880602836609, 59.954957485198975, 64.93818736076355, 69.92141723632812, 74.9078619480133, 79.89430665969849, 84.88392305374146, 89.87353944778442, 94.86454558372498, 99.85555171966553, 104.85102272033691, 109.8464937210083, 114.81706285476685, 119.78763198852539, 124.79625344276428, 129.80487489700317, 134.79147791862488, 139.77808094024658, 144.71823048591614, 149.6583800315857, 154.6047248840332, 159.5510697364807, 164.44669914245605, 169.3423285484314, 174.28590488433838, 179.22948122024536, 184.20549988746643, 189.1815185546875, 194.14840722084045, 199.1152958869934, 204.06435370445251, 209.01341152191162, 213.91985058784485, 218.82628965377808, 223.74711799621582, 228.66794633865356, 233.60336065292358, 238.5387749671936, 243.46634340286255, 248.3939118385315, 253.32424449920654, 258.2545771598816, 263.1530132293701, 268.05144929885864, 272.9460880756378, 277.840726852417, 282.7364830970764, 287.63223934173584, 292.5013282299042, 297.3704171180725, 302.29928064346313, 307.22814416885376, 312.1928427219391, 317.1575412750244, 322.13259768486023, 327.10765409469604, 332.04171419143677, 336.9757742881775, 341.84771275520325, 346.719651222229, 351.6226646900177, 356.5256781578064, 361.42115664482117, 366.31663513183594, 371.19487404823303, 376.0731129646301, 380.9775445461273, 385.8819761276245, 390.78923201560974, 395.69648790359497, 400.6213049888611, 405.5461220741272, 410.46081376075745, 415.3755054473877, 420.30216789245605, 425.2288303375244, 430.17493987083435, 435.1210494041443, 440.0647373199463, 445.0084252357483, 449.938045501709, 454.8676657676697, 459.8013048171997, 464.73494386672974, 469.6795256137848, 474.62410736083984, 479.51932549476624, 484.4145436286926, 489.33706974983215, 494.2595958709717, 499.18287801742554, 504.1061601638794, 508.8223605155945, 513.5385608673096, 518.4662532806396, 523.3939456939697, 528.3514053821564, 533.308865070343, 538.2220959663391, 543.1353268623352, 547.4703347682953, 551.8053426742554, 556.1585988998413, 560.5118551254272, 564.8492066860199, 569.1865582466125, 573.5331897735596, 577.8798213005066, 582.2268750667572, 586.5739288330078, 590.9165389537811, 595.2591490745544, 599.5732271671295, 603.8873052597046, 608.249175786972, 612.6110463142395, 616.9290323257446, 621.2470183372498, 625.6105184555054, 629.974018573761, 634.3032758235931, 638.6325330734253, 643.0176541805267, 647.4027752876282, 651.7346844673157, 656.0665936470032, 660.4064786434174, 664.7463636398315, 669.0768835544586, 673.4074034690857, 677.7925851345062, 682.1777667999268, 686.5591986179352, 690.9406304359436, 695.3199677467346, 699.6993050575256, 704.0154421329498, 708.331579208374, 712.6553444862366, 716.9791097640991, 721.3588306903839, 725.7385516166687, 730.0677795410156, 734.3970074653625, 738.722902059555, 743.0487966537476, 747.3567504882812, 751.6647043228149, 755.9607300758362, 760.2567558288574, 764.5924656391144, 768.9281754493713, 773.2433362007141, 777.5584969520569, 781.8693723678589, 786.1802477836609, 790.4619114398956, 794.7435750961304, 799.0471584796906, 803.3507418632507, 807.6679990291595, 811.9852561950684, 816.3146867752075, 820.6441173553467, 824.9842345714569, 829.3243517875671, 833.6175615787506, 837.9107713699341, 842.2014303207397, 846.4920892715454, 850.8375062942505, 855.1829233169556, 859.529890537262, 863.8768577575684, 868.2260029315948, 872.5751481056213, 876.9256162643433, 881.2760844230652, 885.6009631156921, 889.9258418083191, 894.2863664627075, 898.646891117096, 902.9911160469055, 907.3353409767151, 911.6646296977997, 915.9939184188843, 920.2677817344666, 924.5416450500488, 928.878981590271, 933.2163181304932, 935.3815114498138, 937.5467047691345]
[39.1775, 39.1775, 44.42, 44.42, 48.2675, 48.2675, 50.245, 50.245, 50.5075, 50.5075, 51.685, 51.685, 54.325, 54.325, 55.2425, 55.2425, 56.2625, 56.2625, 58.1825, 58.1825, 59.04, 59.04, 60.3975, 60.3975, 61.9075, 61.9075, 63.035, 63.035, 63.3925, 63.3925, 63.8275, 63.8275, 64.3825, 64.3825, 64.6575, 64.6575, 65.4725, 65.4725, 65.2175, 65.2175, 66.2025, 66.2025, 66.1275, 66.1275, 66.47, 66.47, 67.31, 67.31, 67.3625, 67.3625, 67.4775, 67.4775, 67.41, 67.41, 67.76, 67.76, 68.065, 68.065, 68.615, 68.615, 68.9925, 68.9925, 69.195, 69.195, 69.4275, 69.4275, 69.3875, 69.3875, 69.4975, 69.4975, 69.55, 69.55, 69.57, 69.57, 69.785, 69.785, 69.7625, 69.7625, 70.14, 70.14, 70.265, 70.265, 70.6675, 70.6675, 70.69, 70.69, 70.8275, 70.8275, 70.83, 70.83, 70.6975, 70.6975, 70.7675, 70.7675, 70.7075, 70.7075, 70.885, 70.885, 71.125, 71.125, 71.235, 71.235, 71.29, 71.29, 71.165, 71.165, 71.4675, 71.4675, 71.31, 71.31, 71.5775, 71.5775, 71.405, 71.405, 71.6325, 71.6325, 71.6425, 71.6425, 71.5925, 71.5925, 71.435, 71.435, 71.3525, 71.3525, 71.48, 71.48, 71.6, 71.6, 71.9325, 71.9325, 72.2175, 72.2175, 72.0675, 72.0675, 72.1675, 72.1675, 72.0075, 72.0075, 71.8975, 71.8975, 71.8425, 71.8425, 72.27, 72.27, 72.5425, 72.5425, 72.3175, 72.3175, 72.4325, 72.4325, 72.6075, 72.6075, 72.7925, 72.7925, 72.4225, 72.4225, 72.64, 72.64, 72.555, 72.555, 72.39, 72.39, 72.46, 72.46, 72.47, 72.47, 72.545, 72.545, 72.58, 72.58, 72.5775, 72.5775, 72.6375, 72.6375, 72.655, 72.655, 72.4725, 72.4725, 72.455, 72.455, 72.7825, 72.7825, 72.665, 72.665, 72.5025, 72.5025, 72.5975, 72.5975, 72.9225, 72.9225, 72.8375, 72.8375, 72.9475, 72.9475, 72.78, 72.78, 72.8575, 72.8575, 72.93, 72.93, 74.275, 74.275]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.218, Test loss: 1.932, Test accuracy: 30.84
Round   1, Train loss: 1.900, Test loss: 1.676, Test accuracy: 39.14
Round   2, Train loss: 1.701, Test loss: 1.547, Test accuracy: 45.08
Round   3, Train loss: 1.582, Test loss: 1.458, Test accuracy: 47.41
Round   4, Train loss: 1.495, Test loss: 1.375, Test accuracy: 51.29
Round   5, Train loss: 1.426, Test loss: 1.321, Test accuracy: 53.24
Round   6, Train loss: 1.369, Test loss: 1.272, Test accuracy: 55.29
Round   7, Train loss: 1.315, Test loss: 1.223, Test accuracy: 57.54
Round   8, Train loss: 1.263, Test loss: 1.182, Test accuracy: 59.04
Round   9, Train loss: 1.227, Test loss: 1.142, Test accuracy: 60.26
Round  10, Train loss: 1.174, Test loss: 1.131, Test accuracy: 60.67
Round  11, Train loss: 1.137, Test loss: 1.107, Test accuracy: 61.71
Round  12, Train loss: 1.118, Test loss: 1.074, Test accuracy: 63.18
Round  13, Train loss: 1.094, Test loss: 1.049, Test accuracy: 64.00
Round  14, Train loss: 1.083, Test loss: 0.998, Test accuracy: 65.23
Round  15, Train loss: 1.008, Test loss: 1.005, Test accuracy: 65.10
Round  16, Train loss: 1.029, Test loss: 0.980, Test accuracy: 66.43
Round  17, Train loss: 0.991, Test loss: 0.978, Test accuracy: 66.31
Round  18, Train loss: 0.967, Test loss: 0.962, Test accuracy: 66.94
Round  19, Train loss: 0.945, Test loss: 0.942, Test accuracy: 67.55
Round  20, Train loss: 0.966, Test loss: 0.916, Test accuracy: 68.42
Round  21, Train loss: 0.923, Test loss: 0.918, Test accuracy: 68.03
Round  22, Train loss: 0.908, Test loss: 0.918, Test accuracy: 68.42
Round  23, Train loss: 0.889, Test loss: 0.909, Test accuracy: 69.06
Round  24, Train loss: 0.890, Test loss: 0.886, Test accuracy: 69.67
Round  25, Train loss: 0.877, Test loss: 0.870, Test accuracy: 70.26
Round  26, Train loss: 0.844, Test loss: 0.869, Test accuracy: 70.45
Round  27, Train loss: 0.848, Test loss: 0.858, Test accuracy: 70.80
Round  28, Train loss: 0.836, Test loss: 0.855, Test accuracy: 70.98
Round  29, Train loss: 0.810, Test loss: 0.858, Test accuracy: 70.85
Round  30, Train loss: 0.825, Test loss: 0.841, Test accuracy: 71.18
Round  31, Train loss: 0.798, Test loss: 0.825, Test accuracy: 72.06
Round  32, Train loss: 0.778, Test loss: 0.826, Test accuracy: 71.84
Round  33, Train loss: 0.767, Test loss: 0.826, Test accuracy: 71.98
Round  34, Train loss: 0.760, Test loss: 0.820, Test accuracy: 72.23
Round  35, Train loss: 0.751, Test loss: 0.798, Test accuracy: 72.78
Round  36, Train loss: 0.752, Test loss: 0.789, Test accuracy: 73.15
Round  37, Train loss: 0.725, Test loss: 0.790, Test accuracy: 73.25
Round  38, Train loss: 0.734, Test loss: 0.782, Test accuracy: 73.56
Round  39, Train loss: 0.722, Test loss: 0.798, Test accuracy: 73.03
Round  40, Train loss: 0.703, Test loss: 0.785, Test accuracy: 73.53
Round  41, Train loss: 0.686, Test loss: 0.797, Test accuracy: 73.32
Round  42, Train loss: 0.692, Test loss: 0.804, Test accuracy: 72.95
Round  43, Train loss: 0.670, Test loss: 0.806, Test accuracy: 72.98
Round  44, Train loss: 0.644, Test loss: 0.821, Test accuracy: 72.39
Round  45, Train loss: 0.682, Test loss: 0.805, Test accuracy: 72.74
Round  46, Train loss: 0.695, Test loss: 0.795, Test accuracy: 73.62
Round  47, Train loss: 0.650, Test loss: 0.790, Test accuracy: 73.61
Round  48, Train loss: 0.651, Test loss: 0.779, Test accuracy: 74.19
Round  49, Train loss: 0.642, Test loss: 0.794, Test accuracy: 73.65
Round  50, Train loss: 0.631, Test loss: 0.788, Test accuracy: 73.93
Round  51, Train loss: 0.655, Test loss: 0.765, Test accuracy: 74.37
Round  52, Train loss: 0.632, Test loss: 0.767, Test accuracy: 74.64
Round  53, Train loss: 0.624, Test loss: 0.770, Test accuracy: 74.55
Round  54, Train loss: 0.627, Test loss: 0.764, Test accuracy: 74.85
Round  55, Train loss: 0.589, Test loss: 0.781, Test accuracy: 74.39
Round  56, Train loss: 0.593, Test loss: 0.766, Test accuracy: 74.61
Round  57, Train loss: 0.610, Test loss: 0.771, Test accuracy: 74.24
Round  58, Train loss: 0.582, Test loss: 0.772, Test accuracy: 74.59
Round  59, Train loss: 0.595, Test loss: 0.780, Test accuracy: 74.12
Round  60, Train loss: 0.618, Test loss: 0.775, Test accuracy: 74.43
Round  61, Train loss: 0.595, Test loss: 0.758, Test accuracy: 75.00
Round  62, Train loss: 0.595, Test loss: 0.758, Test accuracy: 75.41
Round  63, Train loss: 0.595, Test loss: 0.752, Test accuracy: 75.47
Round  64, Train loss: 0.611, Test loss: 0.758, Test accuracy: 75.22
Round  65, Train loss: 0.562, Test loss: 0.761, Test accuracy: 75.00
Round  66, Train loss: 0.569, Test loss: 0.764, Test accuracy: 74.86
Round  67, Train loss: 0.538, Test loss: 0.781, Test accuracy: 74.45
Round  68, Train loss: 0.566, Test loss: 0.779, Test accuracy: 74.81
Round  69, Train loss: 0.581, Test loss: 0.771, Test accuracy: 74.88
Round  70, Train loss: 0.559, Test loss: 0.768, Test accuracy: 75.31
Round  71, Train loss: 0.527, Test loss: 0.765, Test accuracy: 75.37
Round  72, Train loss: 0.576, Test loss: 0.752, Test accuracy: 75.80
Round  73, Train loss: 0.533, Test loss: 0.755, Test accuracy: 75.86
Round  74, Train loss: 0.531, Test loss: 0.760, Test accuracy: 75.64
Round  75, Train loss: 0.524, Test loss: 0.755, Test accuracy: 75.57
Round  76, Train loss: 0.576, Test loss: 0.757, Test accuracy: 75.74
Round  77, Train loss: 0.514, Test loss: 0.759, Test accuracy: 75.51
Round  78, Train loss: 0.533, Test loss: 0.763, Test accuracy: 75.50
Round  79, Train loss: 0.523, Test loss: 0.769, Test accuracy: 75.35
Round  80, Train loss: 0.519, Test loss: 0.767, Test accuracy: 75.82
Round  81, Train loss: 0.541, Test loss: 0.764, Test accuracy: 75.59
Round  82, Train loss: 0.559, Test loss: 0.770, Test accuracy: 75.69
Round  83, Train loss: 0.518, Test loss: 0.763, Test accuracy: 76.00
Round  84, Train loss: 0.568, Test loss: 0.757, Test accuracy: 75.90
Round  85, Train loss: 0.503, Test loss: 0.769, Test accuracy: 75.94
Round  86, Train loss: 0.522, Test loss: 0.761, Test accuracy: 75.96
Round  87, Train loss: 0.544, Test loss: 0.772, Test accuracy: 75.66
Round  88, Train loss: 0.512, Test loss: 0.764, Test accuracy: 75.98
Round  89, Train loss: 0.519, Test loss: 0.761, Test accuracy: 75.75
Round  90, Train loss: 0.458, Test loss: 0.752, Test accuracy: 76.11
Round  91, Train loss: 0.490, Test loss: 0.757, Test accuracy: 76.05
Round  92, Train loss: 0.523, Test loss: 0.758, Test accuracy: 76.28
Round  93, Train loss: 0.478, Test loss: 0.763, Test accuracy: 76.26
Round  94, Train loss: 0.499, Test loss: 0.772, Test accuracy: 75.81
Round  95, Train loss: 0.528, Test loss: 0.765, Test accuracy: 75.79
Round  96, Train loss: 0.500, Test loss: 0.759, Test accuracy: 76.44
Round  97, Train loss: 0.529, Test loss: 0.750, Test accuracy: 76.40
Round  98, Train loss: 0.500, Test loss: 0.759, Test accuracy: 76.43
Round  99, Train loss: 0.473, Test loss: 0.762, Test accuracy: 76.51
Final Round, Train loss: 0.415, Test loss: 0.758, Test accuracy: 76.48
Average accuracy final 10 rounds: 76.20825000000002 

4833.3989198207855
[4.539937973022461, 9.079875946044922, 13.519213199615479, 17.958550453186035, 22.370185613632202, 26.78182077407837, 31.224332571029663, 35.66684436798096, 40.12547779083252, 44.58411121368408, 48.969234228134155, 53.35435724258423, 57.7514853477478, 62.14861345291138, 66.53533363342285, 70.92205381393433, 75.31969952583313, 79.71734523773193, 84.14941453933716, 88.58148384094238, 92.97648978233337, 97.37149572372437, 101.79147911071777, 106.21146249771118, 110.64354395866394, 115.0756254196167, 119.51880168914795, 123.9619779586792, 128.36931657791138, 132.77665519714355, 137.14807963371277, 141.51950407028198, 145.8965814113617, 150.2736587524414, 154.63193655014038, 158.99021434783936, 163.37530088424683, 167.7603874206543, 172.15741682052612, 176.55444622039795, 180.88120436668396, 185.20796251296997, 189.57681465148926, 193.94566679000854, 197.9886453151703, 202.03162384033203, 206.05925250053406, 210.08688116073608, 214.0586130619049, 218.03034496307373, 222.03809714317322, 226.0458493232727, 230.0287184715271, 234.0115876197815, 237.97530221939087, 241.93901681900024, 245.8807511329651, 249.82248544692993, 253.8410348892212, 257.85958433151245, 261.861163854599, 265.86274337768555, 269.8831145763397, 273.9034857749939, 277.9012842178345, 281.89908266067505, 285.92484641075134, 289.95061016082764, 293.9928357601166, 298.0350613594055, 302.004732131958, 305.9744029045105, 309.9699399471283, 313.9654769897461, 317.9615526199341, 321.95762825012207, 325.9356827735901, 329.9137372970581, 333.8680922985077, 337.8224472999573, 341.794469833374, 345.76649236679077, 349.73713302612305, 353.7077736854553, 357.68362855911255, 361.6594834327698, 365.6329755783081, 369.60646772384644, 373.6331467628479, 377.65982580184937, 381.696004152298, 385.7321825027466, 389.75891876220703, 393.7856550216675, 397.7914755344391, 401.7972960472107, 406.08587288856506, 410.37444972991943, 414.8408088684082, 419.307168006897, 423.4390540122986, 427.5709400177002, 431.67373728752136, 435.77653455734253, 439.90000200271606, 444.0234694480896, 448.1218602657318, 452.220251083374, 456.4888024330139, 460.7573537826538, 465.1824860572815, 469.6076183319092, 474.92251539230347, 480.23741245269775, 484.7240195274353, 489.21062660217285, 493.473757982254, 497.7368893623352, 501.85873317718506, 505.9805769920349, 510.0809555053711, 514.1813340187073, 518.2964947223663, 522.4116554260254, 526.7333524227142, 531.0550494194031, 535.4389855861664, 539.8229217529297, 544.7454950809479, 549.6680684089661, 555.955445766449, 562.2428231239319, 566.3521819114685, 570.4615406990051, 574.579799413681, 578.6980581283569, 582.8474462032318, 586.9968342781067, 591.1988091468811, 595.4007840156555, 599.5520529747009, 603.7033219337463, 607.8619713783264, 612.0206208229065, 616.2394075393677, 620.4581942558289, 624.624324798584, 628.7904553413391, 632.963650226593, 637.1368451118469, 641.3649027347565, 645.592960357666, 649.7181031703949, 653.8432459831238, 657.9848172664642, 662.1263885498047, 666.290712594986, 670.4550366401672, 674.5745763778687, 678.6941161155701, 682.8184883594513, 686.9428606033325, 691.1019053459167, 695.260950088501, 699.3961038589478, 703.5312576293945, 707.6519544124603, 711.7726511955261, 715.9090790748596, 720.0455069541931, 724.1957912445068, 728.3460755348206, 732.4870522022247, 736.6280288696289, 740.7160887718201, 744.8041486740112, 748.9155833721161, 753.027018070221, 757.1527080535889, 761.2783980369568, 765.4629044532776, 769.6474108695984, 773.8002526760101, 777.9530944824219, 782.0857872962952, 786.2184801101685, 790.3176782131195, 794.4168763160706, 798.5087375640869, 802.6005988121033, 806.6916024684906, 810.7826061248779, 814.895544052124, 819.0084819793701, 823.1687610149384, 827.3290400505066, 831.4981489181519, 835.6672577857971, 839.8234615325928, 843.9796652793884, 845.9448812007904, 847.9100971221924]
[30.8375, 30.8375, 39.14, 39.14, 45.0775, 45.0775, 47.4125, 47.4125, 51.2925, 51.2925, 53.2375, 53.2375, 55.2925, 55.2925, 57.54, 57.54, 59.04, 59.04, 60.255, 60.255, 60.6725, 60.6725, 61.71, 61.71, 63.1825, 63.1825, 64.005, 64.005, 65.23, 65.23, 65.0975, 65.0975, 66.4275, 66.4275, 66.3075, 66.3075, 66.945, 66.945, 67.55, 67.55, 68.415, 68.415, 68.03, 68.03, 68.415, 68.415, 69.065, 69.065, 69.6675, 69.6675, 70.26, 70.26, 70.45, 70.45, 70.8, 70.8, 70.9775, 70.9775, 70.8475, 70.8475, 71.18, 71.18, 72.0575, 72.0575, 71.8375, 71.8375, 71.9825, 71.9825, 72.235, 72.235, 72.7825, 72.7825, 73.1525, 73.1525, 73.255, 73.255, 73.565, 73.565, 73.0325, 73.0325, 73.5275, 73.5275, 73.3175, 73.3175, 72.95, 72.95, 72.9825, 72.9825, 72.385, 72.385, 72.7375, 72.7375, 73.62, 73.62, 73.6075, 73.6075, 74.1875, 74.1875, 73.6525, 73.6525, 73.9325, 73.9325, 74.3725, 74.3725, 74.64, 74.64, 74.5475, 74.5475, 74.8525, 74.8525, 74.385, 74.385, 74.6125, 74.6125, 74.24, 74.24, 74.595, 74.595, 74.1225, 74.1225, 74.43, 74.43, 74.995, 74.995, 75.41, 75.41, 75.4675, 75.4675, 75.2175, 75.2175, 75.005, 75.005, 74.865, 74.865, 74.455, 74.455, 74.8125, 74.8125, 74.8775, 74.8775, 75.3075, 75.3075, 75.3675, 75.3675, 75.8025, 75.8025, 75.8625, 75.8625, 75.64, 75.64, 75.5725, 75.5725, 75.7425, 75.7425, 75.5125, 75.5125, 75.5025, 75.5025, 75.35, 75.35, 75.82, 75.82, 75.59, 75.59, 75.695, 75.695, 76.0, 76.0, 75.8975, 75.8975, 75.94, 75.94, 75.96, 75.96, 75.6575, 75.6575, 75.9825, 75.9825, 75.745, 75.745, 76.1125, 76.1125, 76.0475, 76.0475, 76.275, 76.275, 76.2625, 76.2625, 75.8075, 75.8075, 75.7925, 75.7925, 76.4425, 76.4425, 76.4025, 76.4025, 76.4275, 76.4275, 76.5125, 76.5125, 76.485, 76.485]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.222, Test loss: 1.936, Test accuracy: 33.17
Round   1, Train loss: 1.881, Test loss: 1.651, Test accuracy: 42.52
Round   2, Train loss: 1.696, Test loss: 1.549, Test accuracy: 46.56
Round   3, Train loss: 1.599, Test loss: 1.451, Test accuracy: 49.66
Round   4, Train loss: 1.497, Test loss: 1.416, Test accuracy: 52.10
Round   5, Train loss: 1.457, Test loss: 1.303, Test accuracy: 55.92
Round   6, Train loss: 1.367, Test loss: 1.250, Test accuracy: 57.61
Round   7, Train loss: 1.312, Test loss: 1.199, Test accuracy: 59.55
Round   8, Train loss: 1.265, Test loss: 1.166, Test accuracy: 59.97
Round   9, Train loss: 1.229, Test loss: 1.138, Test accuracy: 60.64
Round  10, Train loss: 1.173, Test loss: 1.106, Test accuracy: 62.93
Round  11, Train loss: 1.164, Test loss: 1.054, Test accuracy: 63.84
Round  12, Train loss: 1.128, Test loss: 1.022, Test accuracy: 65.27
Round  13, Train loss: 1.095, Test loss: 0.987, Test accuracy: 66.83
Round  14, Train loss: 1.065, Test loss: 0.972, Test accuracy: 67.36
Round  15, Train loss: 1.034, Test loss: 0.951, Test accuracy: 67.89
Round  16, Train loss: 0.999, Test loss: 0.926, Test accuracy: 69.00
Round  17, Train loss: 0.985, Test loss: 0.920, Test accuracy: 69.56
Round  18, Train loss: 0.979, Test loss: 0.899, Test accuracy: 69.80
Round  19, Train loss: 0.953, Test loss: 0.883, Test accuracy: 70.42
Round  20, Train loss: 0.942, Test loss: 0.860, Test accuracy: 71.01
Round  21, Train loss: 0.916, Test loss: 0.866, Test accuracy: 71.16
Round  22, Train loss: 0.898, Test loss: 0.845, Test accuracy: 71.44
Round  23, Train loss: 0.860, Test loss: 0.848, Test accuracy: 71.94
Round  24, Train loss: 0.863, Test loss: 0.833, Test accuracy: 72.19
Round  25, Train loss: 0.850, Test loss: 0.823, Test accuracy: 72.28
Round  26, Train loss: 0.835, Test loss: 0.815, Test accuracy: 72.83
Round  27, Train loss: 0.839, Test loss: 0.818, Test accuracy: 72.74
Round  28, Train loss: 0.846, Test loss: 0.801, Test accuracy: 73.25
Round  29, Train loss: 0.788, Test loss: 0.804, Test accuracy: 72.91
Round  30, Train loss: 0.788, Test loss: 0.796, Test accuracy: 73.35
Round  31, Train loss: 0.788, Test loss: 0.792, Test accuracy: 73.35
Round  32, Train loss: 0.759, Test loss: 0.791, Test accuracy: 73.22
Round  33, Train loss: 0.764, Test loss: 0.775, Test accuracy: 73.80
Round  34, Train loss: 0.745, Test loss: 0.778, Test accuracy: 74.00
Round  35, Train loss: 0.770, Test loss: 0.777, Test accuracy: 73.90
Round  36, Train loss: 0.755, Test loss: 0.772, Test accuracy: 74.10
Round  37, Train loss: 0.744, Test loss: 0.764, Test accuracy: 74.20
Round  38, Train loss: 0.730, Test loss: 0.759, Test accuracy: 74.40
Round  39, Train loss: 0.745, Test loss: 0.753, Test accuracy: 74.67
Round  40, Train loss: 0.722, Test loss: 0.751, Test accuracy: 74.75
Round  41, Train loss: 0.705, Test loss: 0.746, Test accuracy: 74.84
Round  42, Train loss: 0.685, Test loss: 0.755, Test accuracy: 74.55
Round  43, Train loss: 0.721, Test loss: 0.738, Test accuracy: 75.06
Round  44, Train loss: 0.695, Test loss: 0.738, Test accuracy: 75.31
Round  45, Train loss: 0.672, Test loss: 0.745, Test accuracy: 74.93
Round  46, Train loss: 0.696, Test loss: 0.741, Test accuracy: 75.19
Round  47, Train loss: 0.663, Test loss: 0.732, Test accuracy: 75.33
Round  48, Train loss: 0.671, Test loss: 0.726, Test accuracy: 75.31
Round  49, Train loss: 0.656, Test loss: 0.735, Test accuracy: 75.43
Round  50, Train loss: 0.669, Test loss: 0.726, Test accuracy: 75.71
Round  51, Train loss: 0.666, Test loss: 0.715, Test accuracy: 75.87
Round  52, Train loss: 0.635, Test loss: 0.719, Test accuracy: 75.95
Round  53, Train loss: 0.621, Test loss: 0.714, Test accuracy: 76.19
Round  54, Train loss: 0.621, Test loss: 0.726, Test accuracy: 75.67
Round  55, Train loss: 0.627, Test loss: 0.722, Test accuracy: 75.99
Round  56, Train loss: 0.612, Test loss: 0.723, Test accuracy: 76.02
Round  57, Train loss: 0.609, Test loss: 0.711, Test accuracy: 76.30
Round  58, Train loss: 0.679, Test loss: 0.700, Test accuracy: 76.72
Round  59, Train loss: 0.591, Test loss: 0.717, Test accuracy: 76.14
Round  60, Train loss: 0.583, Test loss: 0.711, Test accuracy: 76.45
Round  61, Train loss: 0.628, Test loss: 0.715, Test accuracy: 76.03
Round  62, Train loss: 0.641, Test loss: 0.722, Test accuracy: 75.85
Round  63, Train loss: 0.626, Test loss: 0.710, Test accuracy: 76.56
Round  64, Train loss: 0.573, Test loss: 0.717, Test accuracy: 76.29
Round  65, Train loss: 0.634, Test loss: 0.713, Test accuracy: 76.15
Round  66, Train loss: 0.567, Test loss: 0.712, Test accuracy: 76.57
Round  67, Train loss: 0.593, Test loss: 0.698, Test accuracy: 76.89
Round  68, Train loss: 0.554, Test loss: 0.714, Test accuracy: 76.28
Round  69, Train loss: 0.579, Test loss: 0.703, Test accuracy: 76.74
Round  70, Train loss: 0.612, Test loss: 0.690, Test accuracy: 77.13
Round  71, Train loss: 0.577, Test loss: 0.697, Test accuracy: 76.78
Round  72, Train loss: 0.584, Test loss: 0.699, Test accuracy: 76.68
Round  73, Train loss: 0.569, Test loss: 0.702, Test accuracy: 77.06
Round  74, Train loss: 0.561, Test loss: 0.700, Test accuracy: 76.60
Round  75, Train loss: 0.582, Test loss: 0.712, Test accuracy: 76.58
Round  76, Train loss: 0.544, Test loss: 0.703, Test accuracy: 76.54
Round  77, Train loss: 0.539, Test loss: 0.704, Test accuracy: 76.89
Round  78, Train loss: 0.531, Test loss: 0.708, Test accuracy: 76.55
Round  79, Train loss: 0.551, Test loss: 0.702, Test accuracy: 76.90
Round  80, Train loss: 0.573, Test loss: 0.692, Test accuracy: 77.15
Round  81, Train loss: 0.571, Test loss: 0.697, Test accuracy: 76.89
Round  82, Train loss: 0.535, Test loss: 0.705, Test accuracy: 76.64
Round  83, Train loss: 0.515, Test loss: 0.704, Test accuracy: 76.90
Round  84, Train loss: 0.589, Test loss: 0.698, Test accuracy: 77.25
Round  85, Train loss: 0.551, Test loss: 0.705, Test accuracy: 76.86
Round  86, Train loss: 0.520, Test loss: 0.700, Test accuracy: 77.02
Round  87, Train loss: 0.529, Test loss: 0.705, Test accuracy: 76.84
Round  88, Train loss: 0.547, Test loss: 0.697, Test accuracy: 77.03
Round  89, Train loss: 0.532, Test loss: 0.692, Test accuracy: 77.20
Round  90, Train loss: 0.526, Test loss: 0.697, Test accuracy: 76.97
Round  91, Train loss: 0.511, Test loss: 0.703, Test accuracy: 76.91
Round  92, Train loss: 0.519, Test loss: 0.697, Test accuracy: 77.05
Round  93, Train loss: 0.556, Test loss: 0.697, Test accuracy: 76.99
Round  94, Train loss: 0.529, Test loss: 0.695, Test accuracy: 77.06
Round  95, Train loss: 0.527, Test loss: 0.697, Test accuracy: 76.78
Round  96, Train loss: 0.526, Test loss: 0.699, Test accuracy: 77.26
Round  97, Train loss: 0.508, Test loss: 0.708, Test accuracy: 76.78
Round  98, Train loss: 0.508, Test loss: 0.698, Test accuracy: 77.47
Round  99, Train loss: 0.515, Test loss: 0.696, Test accuracy: 77.41
Final Round, Train loss: 0.417, Test loss: 0.695, Test accuracy: 77.44
Average accuracy final 10 rounds: 77.06775
5840.019536733627
[6.412606954574585, 12.82521390914917, 18.78674602508545, 24.74827814102173, 30.63708209991455, 36.52588605880737, 42.44210720062256, 48.358328342437744, 54.328654050827026, 60.29897975921631, 66.24118685722351, 72.18339395523071, 78.26039981842041, 84.33740568161011, 90.3439507484436, 96.3504958152771, 102.36553645133972, 108.38057708740234, 114.39939498901367, 120.418212890625, 126.3905827999115, 132.362952709198, 138.3159601688385, 144.268967628479, 150.2160165309906, 156.1630654335022, 162.13562893867493, 168.10819244384766, 174.0959370136261, 180.08368158340454, 186.01336097717285, 191.94304037094116, 197.87006092071533, 203.7970814704895, 209.77479147911072, 215.75250148773193, 221.68357467651367, 227.6146478652954, 233.5706696510315, 239.52669143676758, 245.48524236679077, 251.44379329681396, 257.3552224636078, 263.2666516304016, 269.2183463573456, 275.17004108428955, 281.0999493598938, 287.02985763549805, 292.94324946403503, 298.856641292572, 304.78742504119873, 310.71820878982544, 316.6763093471527, 322.63440990448, 328.5927023887634, 334.5509948730469, 340.7539188861847, 346.9568428993225, 352.99858570098877, 359.04032850265503, 365.03613090515137, 371.0319333076477, 377.0577189922333, 383.08350467681885, 389.17994379997253, 395.2763829231262, 401.384464263916, 407.4925456047058, 413.5862581729889, 419.679970741272, 425.772545337677, 431.86511993408203, 438.17755913734436, 444.4899983406067, 450.59030771255493, 456.6906170845032, 462.7723562717438, 468.8540954589844, 474.9039258956909, 480.95375633239746, 486.98778796195984, 493.0218195915222, 499.00363969802856, 504.9854598045349, 511.3095848560333, 517.6337099075317, 523.1786696910858, 528.7236294746399, 534.2203016281128, 539.7169737815857, 545.290225982666, 550.8634781837463, 556.3976993560791, 561.9319205284119, 567.4415135383606, 572.9511065483093, 578.4307816028595, 583.9104566574097, 589.412118434906, 594.9137802124023, 600.4050989151001, 605.8964176177979, 611.4174523353577, 616.9384870529175, 622.475907087326, 628.0133271217346, 633.5065891742706, 638.9998512268066, 644.4834003448486, 649.9669494628906, 655.5515158176422, 661.1360821723938, 666.6592588424683, 672.1824355125427, 677.8235836029053, 683.4647316932678, 689.1140003204346, 694.7632689476013, 700.3848872184753, 706.0065054893494, 711.567675113678, 717.1288447380066, 722.6427550315857, 728.1566653251648, 733.6391117572784, 739.1215581893921, 744.6296861171722, 750.1378140449524, 755.6372067928314, 761.1365995407104, 766.6236200332642, 772.1106405258179, 777.6150920391083, 783.1195435523987, 788.7076752185822, 794.2958068847656, 799.8478376865387, 805.3998684883118, 810.965521812439, 816.5311751365662, 822.0684218406677, 827.6056685447693, 833.1730349063873, 838.7404012680054, 844.2850332260132, 849.829665184021, 855.3463621139526, 860.8630590438843, 866.4104413986206, 871.9578237533569, 877.5181300640106, 883.0784363746643, 888.639452457428, 894.2004685401917, 899.7640676498413, 905.327666759491, 910.8700342178345, 916.412401676178, 921.9678761959076, 927.5233507156372, 933.0911710262299, 938.6589913368225, 944.2028782367706, 949.7467651367188, 955.2153298854828, 960.6838946342468, 966.167466878891, 971.6510391235352, 977.1703813076019, 982.6897234916687, 988.1460611820221, 993.6023988723755, 999.0988006591797, 1004.5952024459839, 1010.0336217880249, 1015.4720411300659, 1020.9463629722595, 1026.4206848144531, 1031.8769237995148, 1037.3331627845764, 1042.8225808143616, 1048.3119988441467, 1053.8079648017883, 1059.30393075943, 1064.7102925777435, 1070.1166543960571, 1075.548258781433, 1080.979863166809, 1086.4630258083344, 1091.9461884498596, 1097.3642315864563, 1102.782274723053, 1108.2153344154358, 1113.6483941078186, 1119.0636992454529, 1124.4790043830872, 1129.9015383720398, 1135.3240723609924, 1140.7842557430267, 1146.244439125061, 1148.4150438308716, 1150.5856485366821]
[33.17, 33.17, 42.525, 42.525, 46.5575, 46.5575, 49.6625, 49.6625, 52.0975, 52.0975, 55.9225, 55.9225, 57.6125, 57.6125, 59.555, 59.555, 59.965, 59.965, 60.6425, 60.6425, 62.9325, 62.9325, 63.8425, 63.8425, 65.265, 65.265, 66.825, 66.825, 67.36, 67.36, 67.89, 67.89, 69.0025, 69.0025, 69.56, 69.56, 69.8, 69.8, 70.425, 70.425, 71.0125, 71.0125, 71.16, 71.16, 71.44, 71.44, 71.94, 71.94, 72.1875, 72.1875, 72.2825, 72.2825, 72.83, 72.83, 72.74, 72.74, 73.255, 73.255, 72.9075, 72.9075, 73.35, 73.35, 73.35, 73.35, 73.22, 73.22, 73.8025, 73.8025, 73.9975, 73.9975, 73.8975, 73.8975, 74.1, 74.1, 74.1975, 74.1975, 74.3975, 74.3975, 74.665, 74.665, 74.7525, 74.7525, 74.8425, 74.8425, 74.55, 74.55, 75.0625, 75.0625, 75.3125, 75.3125, 74.9275, 74.9275, 75.1875, 75.1875, 75.33, 75.33, 75.3125, 75.3125, 75.43, 75.43, 75.7125, 75.7125, 75.8725, 75.8725, 75.9525, 75.9525, 76.1925, 76.1925, 75.675, 75.675, 75.9875, 75.9875, 76.015, 76.015, 76.3, 76.3, 76.7175, 76.7175, 76.135, 76.135, 76.4475, 76.4475, 76.035, 76.035, 75.8475, 75.8475, 76.56, 76.56, 76.2875, 76.2875, 76.1475, 76.1475, 76.5675, 76.5675, 76.885, 76.885, 76.275, 76.275, 76.74, 76.74, 77.1325, 77.1325, 76.7825, 76.7825, 76.68, 76.68, 77.065, 77.065, 76.5975, 76.5975, 76.5825, 76.5825, 76.5425, 76.5425, 76.89, 76.89, 76.5475, 76.5475, 76.9, 76.9, 77.1525, 77.1525, 76.8875, 76.8875, 76.645, 76.645, 76.9, 76.9, 77.255, 77.255, 76.8625, 76.8625, 77.0175, 77.0175, 76.8425, 76.8425, 77.0275, 77.0275, 77.2025, 77.2025, 76.965, 76.965, 76.905, 76.905, 77.05, 77.05, 76.9875, 76.9875, 77.0625, 77.0625, 76.7825, 76.7825, 77.2625, 77.2625, 76.78, 76.78, 77.475, 77.475, 77.4075, 77.4075, 77.445, 77.445]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.173, Test loss: 1.693, Test accuracy: 40.20
Round   1, Train loss: 1.760, Test loss: 1.423, Test accuracy: 48.31
Round   2, Train loss: 1.581, Test loss: 1.303, Test accuracy: 53.64
Round   3, Train loss: 1.480, Test loss: 1.222, Test accuracy: 56.32
Round   4, Train loss: 1.368, Test loss: 1.140, Test accuracy: 59.83
Round   5, Train loss: 1.310, Test loss: 1.080, Test accuracy: 62.75
Round   6, Train loss: 1.249, Test loss: 1.050, Test accuracy: 63.57
Round   7, Train loss: 1.173, Test loss: 1.002, Test accuracy: 65.63
Round   8, Train loss: 1.119, Test loss: 0.979, Test accuracy: 66.21
Round   9, Train loss: 1.106, Test loss: 0.940, Test accuracy: 67.70
Round  10, Train loss: 1.024, Test loss: 0.924, Test accuracy: 68.36
Round  11, Train loss: 1.028, Test loss: 0.889, Test accuracy: 69.80
Round  12, Train loss: 0.946, Test loss: 0.873, Test accuracy: 70.71
Round  13, Train loss: 0.992, Test loss: 0.853, Test accuracy: 71.11
Round  14, Train loss: 0.954, Test loss: 0.840, Test accuracy: 71.36
Round  15, Train loss: 0.901, Test loss: 0.827, Test accuracy: 71.55
Round  16, Train loss: 0.882, Test loss: 0.817, Test accuracy: 72.27
Round  17, Train loss: 0.846, Test loss: 0.811, Test accuracy: 72.45
Round  18, Train loss: 0.820, Test loss: 0.801, Test accuracy: 73.00
Round  19, Train loss: 0.828, Test loss: 0.786, Test accuracy: 73.42
Round  20, Train loss: 0.785, Test loss: 0.783, Test accuracy: 73.67
Round  21, Train loss: 0.785, Test loss: 0.789, Test accuracy: 73.73
Round  22, Train loss: 0.758, Test loss: 0.777, Test accuracy: 73.73
Round  23, Train loss: 0.745, Test loss: 0.776, Test accuracy: 73.92
Round  24, Train loss: 0.758, Test loss: 0.753, Test accuracy: 74.83
Round  25, Train loss: 0.751, Test loss: 0.752, Test accuracy: 74.64
Round  26, Train loss: 0.723, Test loss: 0.754, Test accuracy: 74.47
Round  27, Train loss: 0.686, Test loss: 0.745, Test accuracy: 75.06
Round  28, Train loss: 0.674, Test loss: 0.760, Test accuracy: 74.73
Round  29, Train loss: 0.665, Test loss: 0.748, Test accuracy: 75.35
Round  30, Train loss: 0.671, Test loss: 0.737, Test accuracy: 75.35
Round  31, Train loss: 0.662, Test loss: 0.738, Test accuracy: 75.84
Round  32, Train loss: 0.675, Test loss: 0.727, Test accuracy: 75.62
Round  33, Train loss: 0.646, Test loss: 0.732, Test accuracy: 75.97
Round  34, Train loss: 0.654, Test loss: 0.729, Test accuracy: 76.30
Round  35, Train loss: 0.613, Test loss: 0.732, Test accuracy: 75.91
Round  36, Train loss: 0.624, Test loss: 0.717, Test accuracy: 76.30
Round  37, Train loss: 0.588, Test loss: 0.750, Test accuracy: 75.88
Round  38, Train loss: 0.611, Test loss: 0.732, Test accuracy: 76.23
Round  39, Train loss: 0.573, Test loss: 0.739, Test accuracy: 75.66
Round  40, Train loss: 0.593, Test loss: 0.728, Test accuracy: 76.53
Round  41, Train loss: 0.566, Test loss: 0.745, Test accuracy: 76.44
Round  42, Train loss: 0.522, Test loss: 0.747, Test accuracy: 76.41
Round  43, Train loss: 0.589, Test loss: 0.744, Test accuracy: 76.14
Round  44, Train loss: 0.563, Test loss: 0.739, Test accuracy: 76.22
Round  45, Train loss: 0.539, Test loss: 0.741, Test accuracy: 76.32
Round  46, Train loss: 0.578, Test loss: 0.731, Test accuracy: 76.87
Round  47, Train loss: 0.578, Test loss: 0.728, Test accuracy: 76.06
Round  48, Train loss: 0.564, Test loss: 0.720, Test accuracy: 76.95
Round  49, Train loss: 0.546, Test loss: 0.736, Test accuracy: 76.40
Round  50, Train loss: 0.521, Test loss: 0.726, Test accuracy: 76.67
Round  51, Train loss: 0.500, Test loss: 0.737, Test accuracy: 76.80
Round  52, Train loss: 0.541, Test loss: 0.722, Test accuracy: 77.16
Round  53, Train loss: 0.524, Test loss: 0.727, Test accuracy: 76.87
Round  54, Train loss: 0.531, Test loss: 0.719, Test accuracy: 77.02
Round  55, Train loss: 0.518, Test loss: 0.737, Test accuracy: 77.06
Round  56, Train loss: 0.475, Test loss: 0.747, Test accuracy: 76.89
Round  57, Train loss: 0.465, Test loss: 0.755, Test accuracy: 77.39
Round  58, Train loss: 0.495, Test loss: 0.739, Test accuracy: 77.37
Round  59, Train loss: 0.536, Test loss: 0.733, Test accuracy: 77.19
Round  60, Train loss: 0.474, Test loss: 0.742, Test accuracy: 76.91
Round  61, Train loss: 0.503, Test loss: 0.724, Test accuracy: 77.61
Round  62, Train loss: 0.480, Test loss: 0.743, Test accuracy: 77.03
Round  63, Train loss: 0.495, Test loss: 0.735, Test accuracy: 77.42
Round  64, Train loss: 0.449, Test loss: 0.729, Test accuracy: 77.68
Round  65, Train loss: 0.497, Test loss: 0.722, Test accuracy: 77.43
Round  66, Train loss: 0.488, Test loss: 0.726, Test accuracy: 77.75
Round  67, Train loss: 0.472, Test loss: 0.744, Test accuracy: 77.36
Round  68, Train loss: 0.464, Test loss: 0.731, Test accuracy: 77.60
Round  69, Train loss: 0.453, Test loss: 0.739, Test accuracy: 77.47
Round  70, Train loss: 0.466, Test loss: 0.733, Test accuracy: 77.70
Round  71, Train loss: 0.453, Test loss: 0.754, Test accuracy: 77.26
Round  72, Train loss: 0.473, Test loss: 0.734, Test accuracy: 77.59
Round  73, Train loss: 0.494, Test loss: 0.739, Test accuracy: 77.32
Round  74, Train loss: 0.440, Test loss: 0.740, Test accuracy: 77.45
Round  75, Train loss: 0.435, Test loss: 0.767, Test accuracy: 77.09
Round  76, Train loss: 0.461, Test loss: 0.758, Test accuracy: 77.24
Round  77, Train loss: 0.432, Test loss: 0.749, Test accuracy: 77.39
Round  78, Train loss: 0.439, Test loss: 0.741, Test accuracy: 77.43
Round  79, Train loss: 0.476, Test loss: 0.732, Test accuracy: 77.95
Round  80, Train loss: 0.431, Test loss: 0.744, Test accuracy: 77.39
Round  81, Train loss: 0.398, Test loss: 0.769, Test accuracy: 77.42
Round  82, Train loss: 0.423, Test loss: 0.751, Test accuracy: 77.94
Round  83, Train loss: 0.417, Test loss: 0.757, Test accuracy: 77.75
Round  84, Train loss: 0.381, Test loss: 0.781, Test accuracy: 77.64
Round  85, Train loss: 0.446, Test loss: 0.750, Test accuracy: 77.83
Round  86, Train loss: 0.416, Test loss: 0.748, Test accuracy: 77.57
Round  87, Train loss: 0.381, Test loss: 0.753, Test accuracy: 77.68
Round  88, Train loss: 0.397, Test loss: 0.761, Test accuracy: 77.72
Round  89, Train loss: 0.422, Test loss: 0.748, Test accuracy: 77.94
Round  90, Train loss: 0.437, Test loss: 0.746, Test accuracy: 77.76
Round  91, Train loss: 0.415, Test loss: 0.752, Test accuracy: 77.82
Round  92, Train loss: 0.458, Test loss: 0.736, Test accuracy: 77.85
Round  93, Train loss: 0.404, Test loss: 0.757, Test accuracy: 77.71
Round  94, Train loss: 0.385, Test loss: 0.756, Test accuracy: 77.89
Round  95, Train loss: 0.401, Test loss: 0.754, Test accuracy: 77.74
Round  96, Train loss: 0.424, Test loss: 0.743, Test accuracy: 77.94
Round  97, Train loss: 0.403, Test loss: 0.745, Test accuracy: 78.14
Round  98, Train loss: 0.390, Test loss: 0.742, Test accuracy: 78.40
Round  99, Train loss: 0.376, Test loss: 0.761, Test accuracy: 77.92
Final Round, Train loss: 0.312, Test loss: 0.758, Test accuracy: 78.89
Average accuracy final 10 rounds: 77.9165
8417.849253892899
[13.193647384643555, 25.993889331817627, 38.79551959037781, 51.599040269851685, 64.48976445198059, 77.3002438545227, 90.13999390602112, 102.92088031768799, 115.69773960113525, 128.45102715492249, 141.1801586151123, 153.94018936157227, 166.63165307044983, 179.33459043502808, 192.01630067825317, 204.75663876533508, 217.45435857772827, 230.15233898162842, 242.90707516670227, 255.56780767440796, 268.29623460769653, 281.03130412101746, 293.71619510650635, 306.4660232067108, 319.269549369812, 332.0227439403534, 344.79651141166687, 357.30464148521423, 369.8631851673126, 382.4662787914276, 395.0840976238251, 407.71398186683655, 420.3495032787323, 432.9901247024536, 445.5261607170105, 456.58188104629517, 467.8405816555023, 478.9982044696808, 490.1307203769684, 501.2949948310852, 512.5240931510925, 523.7527334690094, 534.993884563446, 547.0797116756439, 559.5412590503693, 572.2751569747925, 583.9177222251892, 595.3709056377411, 606.7878925800323, 618.2354462146759, 629.6024625301361, 640.9165434837341, 652.2432835102081, 663.5624437332153, 674.8634715080261, 686.2204520702362, 697.594377040863, 708.9486901760101, 720.347496509552, 731.617525100708, 742.994610786438, 754.3378252983093, 765.7133655548096, 777.1115624904633, 788.4572217464447, 799.7474589347839, 811.0643875598907, 822.341409444809, 833.6492857933044, 844.9113161563873, 857.397155046463, 870.5204093456268, 883.6613085269928, 896.7927317619324, 910.0044050216675, 923.2071025371552, 936.4127373695374, 949.5006444454193, 962.5716121196747, 975.5883235931396, 988.3639132976532, 1001.1219794750214, 1013.8594207763672, 1026.4143443107605, 1037.836755514145, 1049.3091146945953, 1062.1464383602142, 1074.966715335846, 1087.8220205307007, 1100.6286680698395, 1113.3892481327057, 1126.2739367485046, 1138.4396765232086, 1149.9575572013855, 1161.520230293274, 1173.1665604114532, 1184.6351544857025, 1196.1433069705963, 1207.7258684635162, 1219.3231348991394, 1222.2613134384155]
[40.2, 48.315, 53.6375, 56.32, 59.83, 62.7475, 63.5725, 65.6325, 66.21, 67.6975, 68.365, 69.8025, 70.7125, 71.1075, 71.36, 71.5525, 72.27, 72.4475, 72.995, 73.425, 73.6675, 73.735, 73.735, 73.9175, 74.8275, 74.645, 74.465, 75.0625, 74.7325, 75.3475, 75.3475, 75.845, 75.62, 75.965, 76.3025, 75.9075, 76.3, 75.88, 76.235, 75.6625, 76.535, 76.4375, 76.4125, 76.1425, 76.2175, 76.32, 76.8675, 76.065, 76.955, 76.3975, 76.665, 76.8025, 77.1575, 76.8725, 77.0225, 77.0625, 76.895, 77.3875, 77.3725, 77.195, 76.91, 77.6125, 77.0325, 77.42, 77.6775, 77.4275, 77.7475, 77.3625, 77.6, 77.465, 77.6975, 77.2625, 77.595, 77.3225, 77.4475, 77.0925, 77.2375, 77.385, 77.4325, 77.955, 77.385, 77.4175, 77.94, 77.75, 77.635, 77.8325, 77.5725, 77.68, 77.725, 77.9425, 77.76, 77.8175, 77.8475, 77.71, 77.8875, 77.74, 77.945, 78.135, 78.3975, 77.925, 78.89]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.271, Test loss: 2.297, Test accuracy: 10.62
Round   0, Global train loss: 2.271, Global test loss: 2.300, Global test accuracy: 10.02
Round   1, Train loss: 2.287, Test loss: 2.300, Test accuracy: 10.14
Round   1, Global train loss: 2.287, Global test loss: 2.302, Global test accuracy: 9.83
Round   2, Train loss: 2.277, Test loss: 2.300, Test accuracy: 9.62
Round   2, Global train loss: 2.277, Global test loss: 2.303, Global test accuracy: 9.51
Round   3, Train loss: 2.270, Test loss: 2.301, Test accuracy: 10.56
Round   3, Global train loss: 2.270, Global test loss: 2.304, Global test accuracy: 9.61
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 15.56
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 13.50
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 12.81
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 12.78
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 12.75
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Average accuracy final 10 rounds: 11.666666666666666 

Average global accuracy final 10 rounds: 11.666666666666666 

4789.072994709015
[5.341198921203613, 10.325319051742554, 15.426953077316284, 20.56210970878601, 25.661189079284668, 30.76844334602356, 35.87004899978638, 40.966050148010254, 46.08910536766052, 51.22473692893982, 56.35991144180298, 61.48358988761902, 66.59434008598328, 71.75976896286011, 76.95206499099731, 82.10905122756958, 87.23167872428894, 92.36734771728516, 97.52253127098083, 102.65296769142151, 107.80632662773132, 112.9636778831482, 118.1498327255249, 123.25888991355896, 128.32595252990723, 132.53370428085327, 136.78858375549316, 141.04307460784912, 145.31388092041016, 149.5559115409851, 153.75269198417664, 158.02906584739685, 162.28074836730957, 166.53975558280945, 170.79341292381287, 175.015704870224, 179.2861499786377, 183.54333233833313, 187.78512692451477, 191.9860382080078, 196.2021906375885, 200.49470925331116, 204.75956511497498, 209.00426578521729, 213.20853114128113, 217.46325826644897, 221.73049998283386, 225.99190855026245, 230.22027277946472, 234.43695974349976, 238.67879152297974, 242.91818261146545, 247.14890909194946, 251.36415004730225, 255.59746408462524, 259.8113467693329, 264.03938364982605, 268.25932335853577, 272.4743103981018, 276.7278435230255, 281.3697054386139, 285.62831473350525, 289.8847758769989, 294.0977084636688, 298.3455214500427, 302.58692383766174, 306.83453583717346, 311.0709867477417, 315.3177387714386, 319.5761411190033, 323.827983379364, 328.03406834602356, 332.28202056884766, 336.53126764297485, 341.3242907524109, 345.585711479187, 349.79517102241516, 354.47641229629517, 358.68962049484253, 362.94007778167725, 367.17863392829895, 371.37255024909973, 375.5671133995056, 380.0548937320709, 384.278582572937, 388.58764004707336, 392.79973244667053, 396.9978528022766, 401.1991641521454, 405.4048435688019, 409.69005966186523, 413.9167857170105, 418.1208198070526, 422.32856845855713, 426.5668075084686, 430.8121008872986, 435.036248922348, 442.3976993560791, 446.61763310432434, 450.8239815235138, 453.18667578697205]
[10.616666666666667, 10.141666666666667, 9.625, 10.561111111111112, 15.555555555555555, 13.497222222222222, 12.811111111111112, 12.780555555555557, 12.747222222222224, 11.669444444444446, 11.669444444444446, 11.669444444444446, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668, 11.666666666666668]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.008, Test loss: 1.789, Test accuracy: 34.07
Round   0, Global train loss: 2.008, Global test loss: 1.812, Global test accuracy: 33.22
Round   1, Train loss: 1.712, Test loss: 1.620, Test accuracy: 40.27
Round   1, Global train loss: 1.712, Global test loss: 1.581, Global test accuracy: 41.27
Round   2, Train loss: 1.570, Test loss: 1.564, Test accuracy: 43.03
Round   2, Global train loss: 1.570, Global test loss: 1.489, Global test accuracy: 46.15
Round   3, Train loss: 1.446, Test loss: 1.467, Test accuracy: 46.69
Round   3, Global train loss: 1.446, Global test loss: 1.376, Global test accuracy: 50.07
Round   4, Train loss: 1.358, Test loss: 1.410, Test accuracy: 49.45
Round   4, Global train loss: 1.358, Global test loss: 1.315, Global test accuracy: 53.07
Round   5, Train loss: 1.275, Test loss: 1.367, Test accuracy: 51.27
Round   5, Global train loss: 1.275, Global test loss: 1.263, Global test accuracy: 55.05
Round   6, Train loss: 1.226, Test loss: 1.330, Test accuracy: 52.88
Round   6, Global train loss: 1.226, Global test loss: 1.297, Global test accuracy: 54.38
Round   7, Train loss: 1.164, Test loss: 1.282, Test accuracy: 55.01
Round   7, Global train loss: 1.164, Global test loss: 1.164, Global test accuracy: 58.97
Round   8, Train loss: 1.114, Test loss: 1.253, Test accuracy: 56.02
Round   8, Global train loss: 1.114, Global test loss: 1.244, Global test accuracy: 56.72
Round   9, Train loss: 1.069, Test loss: 1.216, Test accuracy: 57.24
Round   9, Global train loss: 1.069, Global test loss: 1.138, Global test accuracy: 57.92
Round  10, Train loss: 1.030, Test loss: 1.182, Test accuracy: 58.54
Round  10, Global train loss: 1.030, Global test loss: 1.094, Global test accuracy: 59.09
Round  11, Train loss: 0.990, Test loss: 1.177, Test accuracy: 58.91
Round  11, Global train loss: 0.990, Global test loss: 1.094, Global test accuracy: 58.81
Round  12, Train loss: 0.938, Test loss: 1.154, Test accuracy: 59.81
Round  12, Global train loss: 0.938, Global test loss: 1.052, Global test accuracy: 60.96
Round  13, Train loss: 0.942, Test loss: 1.124, Test accuracy: 61.16
Round  13, Global train loss: 0.942, Global test loss: 1.063, Global test accuracy: 63.60
Round  14, Train loss: 0.908, Test loss: 1.128, Test accuracy: 61.63
Round  14, Global train loss: 0.908, Global test loss: 1.126, Global test accuracy: 64.50
Round  15, Train loss: 0.885, Test loss: 1.102, Test accuracy: 62.10
Round  15, Global train loss: 0.885, Global test loss: 1.045, Global test accuracy: 64.29
Round  16, Train loss: 0.866, Test loss: 1.073, Test accuracy: 63.20
Round  16, Global train loss: 0.866, Global test loss: 1.011, Global test accuracy: 64.69
Round  17, Train loss: 0.865, Test loss: 1.057, Test accuracy: 63.80
Round  17, Global train loss: 0.865, Global test loss: 0.997, Global test accuracy: 65.31
Round  18, Train loss: 0.826, Test loss: 1.045, Test accuracy: 64.52
Round  18, Global train loss: 0.826, Global test loss: 0.991, Global test accuracy: 63.59
Round  19, Train loss: 0.799, Test loss: 1.050, Test accuracy: 64.53
Round  19, Global train loss: 0.799, Global test loss: 1.001, Global test accuracy: 65.45
Round  20, Train loss: 0.798, Test loss: 1.037, Test accuracy: 65.35
Round  20, Global train loss: 0.798, Global test loss: 0.983, Global test accuracy: 66.57
Round  21, Train loss: 0.760, Test loss: 1.037, Test accuracy: 65.34
Round  21, Global train loss: 0.760, Global test loss: 0.989, Global test accuracy: 66.37
Round  22, Train loss: 0.752, Test loss: 1.026, Test accuracy: 65.99
Round  22, Global train loss: 0.752, Global test loss: 1.084, Global test accuracy: 67.78
Round  23, Train loss: 0.749, Test loss: 1.019, Test accuracy: 66.48
Round  23, Global train loss: 0.749, Global test loss: 0.973, Global test accuracy: 64.07
Round  24, Train loss: 0.722, Test loss: 0.994, Test accuracy: 67.42
Round  24, Global train loss: 0.722, Global test loss: 0.974, Global test accuracy: 64.49
Round  25, Train loss: 0.722, Test loss: 0.994, Test accuracy: 67.64
Round  25, Global train loss: 0.722, Global test loss: 0.958, Global test accuracy: 64.82
Round  26, Train loss: 0.728, Test loss: 0.992, Test accuracy: 67.78
Round  26, Global train loss: 0.728, Global test loss: 0.962, Global test accuracy: 67.60
Round  27, Train loss: 0.702, Test loss: 0.981, Test accuracy: 68.20
Round  27, Global train loss: 0.702, Global test loss: 0.956, Global test accuracy: 67.98
Round  28, Train loss: 0.692, Test loss: 0.983, Test accuracy: 68.27
Round  28, Global train loss: 0.692, Global test loss: 0.956, Global test accuracy: 64.96
Round  29, Train loss: 0.664, Test loss: 0.991, Test accuracy: 68.22
Round  29, Global train loss: 0.664, Global test loss: 0.952, Global test accuracy: 68.61
Round  30, Train loss: 0.681, Test loss: 0.991, Test accuracy: 68.32
Round  30, Global train loss: 0.681, Global test loss: 0.926, Global test accuracy: 66.19
Round  31, Train loss: 0.655, Test loss: 0.984, Test accuracy: 68.78
Round  31, Global train loss: 0.655, Global test loss: 0.960, Global test accuracy: 68.90
Round  32, Train loss: 0.644, Test loss: 0.979, Test accuracy: 69.03
Round  32, Global train loss: 0.644, Global test loss: 0.958, Global test accuracy: 65.68
Round  33, Train loss: 0.637, Test loss: 0.970, Test accuracy: 69.24
Round  33, Global train loss: 0.637, Global test loss: 0.931, Global test accuracy: 65.53
Round  34, Train loss: 0.647, Test loss: 0.968, Test accuracy: 69.28
Round  34, Global train loss: 0.647, Global test loss: 0.962, Global test accuracy: 68.78
Round  35, Train loss: 0.621, Test loss: 0.978, Test accuracy: 69.03
Round  35, Global train loss: 0.621, Global test loss: 1.101, Global test accuracy: 69.54
Round  36, Train loss: 0.624, Test loss: 0.965, Test accuracy: 69.59
Round  36, Global train loss: 0.624, Global test loss: 1.112, Global test accuracy: 69.61
Round  37, Train loss: 0.616, Test loss: 0.950, Test accuracy: 70.19
Round  37, Global train loss: 0.616, Global test loss: 1.172, Global test accuracy: 64.88
Round  38, Train loss: 0.613, Test loss: 0.953, Test accuracy: 70.17
Round  38, Global train loss: 0.613, Global test loss: 1.110, Global test accuracy: 69.86
Round  39, Train loss: 0.578, Test loss: 0.957, Test accuracy: 70.06
Round  39, Global train loss: 0.578, Global test loss: 1.123, Global test accuracy: 70.01
Round  40, Train loss: 0.585, Test loss: 0.963, Test accuracy: 70.20
Round  40, Global train loss: 0.585, Global test loss: 1.124, Global test accuracy: 70.52
Round  41, Train loss: 0.592, Test loss: 0.969, Test accuracy: 70.17
Round  41, Global train loss: 0.592, Global test loss: 1.123, Global test accuracy: 69.95
Round  42, Train loss: 0.573, Test loss: 0.973, Test accuracy: 70.19
Round  42, Global train loss: 0.573, Global test loss: 1.162, Global test accuracy: 69.62
Round  43, Train loss: 0.567, Test loss: 0.974, Test accuracy: 70.30
Round  43, Global train loss: 0.567, Global test loss: 1.150, Global test accuracy: 70.25
Round  44, Train loss: 0.560, Test loss: 0.974, Test accuracy: 70.14
Round  44, Global train loss: 0.560, Global test loss: 1.122, Global test accuracy: 70.30
Round  45, Train loss: 0.565, Test loss: 0.976, Test accuracy: 70.42
Round  45, Global train loss: 0.565, Global test loss: 0.927, Global test accuracy: 66.89
Round  46, Train loss: 0.544, Test loss: 0.959, Test accuracy: 70.76
Round  46, Global train loss: 0.544, Global test loss: 0.934, Global test accuracy: 67.17
Round  47, Train loss: 0.540, Test loss: 0.972, Test accuracy: 70.66
Round  47, Global train loss: 0.540, Global test loss: 0.968, Global test accuracy: 70.15
Round  48, Train loss: 0.543, Test loss: 0.966, Test accuracy: 70.99
Round  48, Global train loss: 0.543, Global test loss: 0.973, Global test accuracy: 70.11
Round  49, Train loss: 0.512, Test loss: 0.956, Test accuracy: 71.22
Round  49, Global train loss: 0.512, Global test loss: 0.916, Global test accuracy: 67.66
Round  50, Train loss: 0.531, Test loss: 0.945, Test accuracy: 71.48
Round  50, Global train loss: 0.531, Global test loss: 1.332, Global test accuracy: 70.52
Round  51, Train loss: 0.543, Test loss: 0.947, Test accuracy: 71.44
Round  51, Global train loss: 0.543, Global test loss: 0.965, Global test accuracy: 70.90
Round  52, Train loss: 0.538, Test loss: 0.948, Test accuracy: 71.61
Round  52, Global train loss: 0.538, Global test loss: 0.970, Global test accuracy: 70.24
Round  53, Train loss: 0.523, Test loss: 0.950, Test accuracy: 71.53
Round  53, Global train loss: 0.523, Global test loss: 0.961, Global test accuracy: 70.12
Round  54, Train loss: 0.496, Test loss: 0.948, Test accuracy: 71.85
Round  54, Global train loss: 0.496, Global test loss: 0.985, Global test accuracy: 70.88
Round  55, Train loss: 0.508, Test loss: 0.946, Test accuracy: 71.81
Round  55, Global train loss: 0.508, Global test loss: 1.141, Global test accuracy: 71.30
Round  56, Train loss: 0.495, Test loss: 0.952, Test accuracy: 71.68
Round  56, Global train loss: 0.495, Global test loss: 0.949, Global test accuracy: 67.35
Round  57, Train loss: 0.482, Test loss: 0.963, Test accuracy: 71.78
Round  57, Global train loss: 0.482, Global test loss: 1.169, Global test accuracy: 71.14
Round  58, Train loss: 0.490, Test loss: 0.967, Test accuracy: 71.61
Round  58, Global train loss: 0.490, Global test loss: 1.179, Global test accuracy: 71.89
Round  59, Train loss: 0.513, Test loss: 0.969, Test accuracy: 71.61
Round  59, Global train loss: 0.513, Global test loss: 0.946, Global test accuracy: 68.08
Round  60, Train loss: 0.450, Test loss: 0.964, Test accuracy: 71.86
Round  60, Global train loss: 0.450, Global test loss: 0.993, Global test accuracy: 70.92
Round  61, Train loss: 0.500, Test loss: 0.956, Test accuracy: 72.03
Round  61, Global train loss: 0.500, Global test loss: 0.961, Global test accuracy: 70.52
Round  62, Train loss: 0.514, Test loss: 0.957, Test accuracy: 72.19
Round  62, Global train loss: 0.514, Global test loss: 0.973, Global test accuracy: 71.29
Round  63, Train loss: 0.475, Test loss: 0.960, Test accuracy: 72.23
Round  63, Global train loss: 0.475, Global test loss: 0.946, Global test accuracy: 70.60
Round  64, Train loss: 0.460, Test loss: 0.971, Test accuracy: 71.94
Round  64, Global train loss: 0.460, Global test loss: 1.247, Global test accuracy: 66.48
Round  65, Train loss: 0.482, Test loss: 0.965, Test accuracy: 71.80
Round  65, Global train loss: 0.482, Global test loss: 1.189, Global test accuracy: 71.03
Round  66, Train loss: 0.463, Test loss: 0.961, Test accuracy: 71.77
Round  66, Global train loss: 0.463, Global test loss: 1.177, Global test accuracy: 66.87
Round  67, Train loss: 0.439, Test loss: 0.983, Test accuracy: 71.68
Round  67, Global train loss: 0.439, Global test loss: 0.938, Global test accuracy: 69.20
Round  68, Train loss: 0.514, Test loss: 0.974, Test accuracy: 71.98
Round  68, Global train loss: 0.514, Global test loss: 0.907, Global test accuracy: 69.58
Round  69, Train loss: 0.457, Test loss: 0.958, Test accuracy: 72.39
Round  69, Global train loss: 0.457, Global test loss: 1.134, Global test accuracy: 71.94
Round  70, Train loss: 0.457, Test loss: 0.967, Test accuracy: 72.39
Round  70, Global train loss: 0.457, Global test loss: 1.001, Global test accuracy: 71.13
Round  71, Train loss: 0.486, Test loss: 0.980, Test accuracy: 72.41
Round  71, Global train loss: 0.486, Global test loss: 0.991, Global test accuracy: 71.28
Round  72, Train loss: 0.463, Test loss: 0.974, Test accuracy: 72.60
Round  72, Global train loss: 0.463, Global test loss: 0.932, Global test accuracy: 68.49
Round  73, Train loss: 0.443, Test loss: 0.965, Test accuracy: 72.73
Round  73, Global train loss: 0.443, Global test loss: 1.223, Global test accuracy: 67.05
Round  74, Train loss: 0.437, Test loss: 0.966, Test accuracy: 72.74
Round  74, Global train loss: 0.437, Global test loss: 0.970, Global test accuracy: 71.20
Round  75, Train loss: 0.429, Test loss: 0.973, Test accuracy: 72.64
Round  75, Global train loss: 0.429, Global test loss: 1.007, Global test accuracy: 70.93
Round  76, Train loss: 0.435, Test loss: 0.972, Test accuracy: 72.98
Round  76, Global train loss: 0.435, Global test loss: 1.003, Global test accuracy: 71.26
Round  77, Train loss: 0.432, Test loss: 0.968, Test accuracy: 72.90
Round  77, Global train loss: 0.432, Global test loss: 0.998, Global test accuracy: 70.91
Round  78, Train loss: 0.457, Test loss: 0.963, Test accuracy: 73.02
Round  78, Global train loss: 0.457, Global test loss: 1.150, Global test accuracy: 72.11
Round  79, Train loss: 0.408, Test loss: 0.979, Test accuracy: 72.89
Round  79, Global train loss: 0.408, Global test loss: 0.992, Global test accuracy: 71.47
Round  80, Train loss: 0.406, Test loss: 0.979, Test accuracy: 72.77
Round  80, Global train loss: 0.406, Global test loss: 0.970, Global test accuracy: 68.27
Round  81, Train loss: 0.399, Test loss: 0.989, Test accuracy: 72.69
Round  81, Global train loss: 0.399, Global test loss: 0.967, Global test accuracy: 69.42
Round  82, Train loss: 0.446, Test loss: 0.990, Test accuracy: 72.52
Round  82, Global train loss: 0.446, Global test loss: 1.163, Global test accuracy: 71.72
Round  83, Train loss: 0.422, Test loss: 0.973, Test accuracy: 72.83
Round  83, Global train loss: 0.422, Global test loss: 1.215, Global test accuracy: 72.06
Round  84, Train loss: 0.431, Test loss: 0.966, Test accuracy: 72.95
Round  84, Global train loss: 0.431, Global test loss: 0.920, Global test accuracy: 69.47
Round  85, Train loss: 0.401, Test loss: 0.963, Test accuracy: 73.03
Round  85, Global train loss: 0.401, Global test loss: 0.975, Global test accuracy: 71.52
Round  86, Train loss: 0.428, Test loss: 0.966, Test accuracy: 73.03
Round  86, Global train loss: 0.428, Global test loss: 0.940, Global test accuracy: 68.80
Round  87, Train loss: 0.457, Test loss: 0.977, Test accuracy: 72.94
Round  87, Global train loss: 0.457, Global test loss: 1.011, Global test accuracy: 71.75
Round  88, Train loss: 0.388, Test loss: 0.982, Test accuracy: 72.80
Round  88, Global train loss: 0.388, Global test loss: 1.151, Global test accuracy: 72.17
Round  89, Train loss: 0.385, Test loss: 0.983, Test accuracy: 72.89
Round  89, Global train loss: 0.385, Global test loss: 0.997, Global test accuracy: 71.60
Round  90, Train loss: 0.423, Test loss: 0.988, Test accuracy: 72.94
Round  90, Global train loss: 0.423, Global test loss: 0.987, Global test accuracy: 71.66
Round  91, Train loss: 0.428, Test loss: 0.980, Test accuracy: 73.31
Round  91, Global train loss: 0.428, Global test loss: 0.993, Global test accuracy: 71.46
Round  92, Train loss: 0.410, Test loss: 0.977, Test accuracy: 73.27
Round  92, Global train loss: 0.410, Global test loss: 1.207, Global test accuracy: 71.81
Round  93, Train loss: 0.406, Test loss: 0.972, Test accuracy: 73.26
Round  93, Global train loss: 0.406, Global test loss: 0.972, Global test accuracy: 68.36
Round  94, Train loss: 0.409, Test loss: 0.967, Test accuracy: 73.37
Round  94, Global train loss: 0.409, Global test loss: 0.940, Global test accuracy: 69.75
Round  95, Train loss: 0.400, Test loss: 0.965, Test accuracy: 73.62
Round  95, Global train loss: 0.400, Global test loss: 0.930, Global test accuracy: 70.02
Round  96, Train loss: 0.397, Test loss: 0.970, Test accuracy: 73.76
Round  96, Global train loss: 0.397, Global test loss: 1.435, Global test accuracy: 72.17
Round  97, Train loss: 0.397, Test loss: 0.985, Test accuracy: 73.58
Round  97, Global train loss: 0.397, Global test loss: 1.329, Global test accuracy: 67.64
Round  98, Train loss: 0.414, Test loss: 0.999, Test accuracy: 73.32
Round  98, Global train loss: 0.414, Global test loss: 1.203, Global test accuracy: 72.03
Round  99, Train loss: 0.431, Test loss: 0.988, Test accuracy: 73.73
Round  99, Global train loss: 0.431, Global test loss: 1.021, Global test accuracy: 71.73
Final Round, Train loss: 0.245, Test loss: 1.054, Test accuracy: 74.45
Final Round, Global train loss: 0.245, Global test loss: 1.021, Global test accuracy: 71.73
Average accuracy final 10 rounds: 73.416 

Average global accuracy final 10 rounds: 70.66325 

6116.517750740051
[5.198864221572876, 10.397728443145752, 15.53834867477417, 20.678968906402588, 25.805344581604004, 30.93172025680542, 36.0946409702301, 41.257561683654785, 46.37188792228699, 51.48621416091919, 56.649471044540405, 61.81272792816162, 66.92744970321655, 72.04217147827148, 77.28727221488953, 82.53237295150757, 87.67077589035034, 92.80917882919312, 98.56328558921814, 104.31739234924316, 109.50026440620422, 114.68313646316528, 119.8039972782135, 124.92485809326172, 129.1395173072815, 133.35417652130127, 137.5696702003479, 141.78516387939453, 146.00383949279785, 150.22251510620117, 154.45138549804688, 158.68025588989258, 163.1193904876709, 167.55852508544922, 171.89701128005981, 176.2354974746704, 180.76963305473328, 185.30376863479614, 189.5217318534851, 193.73969507217407, 197.9603967666626, 202.18109846115112, 206.39010739326477, 210.59911632537842, 214.83312034606934, 219.06712436676025, 223.26518893241882, 227.4632534980774, 231.6691393852234, 235.87502527236938, 240.06855130195618, 244.26207733154297, 248.45679235458374, 252.6515073776245, 256.8699584007263, 261.0884094238281, 265.3297712802887, 269.57113313674927, 273.79695534706116, 278.02277755737305, 282.2525887489319, 286.4823999404907, 290.70357513427734, 294.92475032806396, 299.1509733200073, 303.3771963119507, 307.5913140773773, 311.80543184280396, 316.00241351127625, 320.19939517974854, 324.52153396606445, 328.84367275238037, 333.04697728157043, 337.2502818107605, 341.5325767993927, 345.8148717880249, 350.03306698799133, 354.25126218795776, 358.48371934890747, 362.7161765098572, 366.937228679657, 371.1582808494568, 375.3795199394226, 379.6007590293884, 383.81503438949585, 388.02930974960327, 392.2777121067047, 396.52611446380615, 400.74754643440247, 404.9689784049988, 409.15973472595215, 413.3504910469055, 417.5498445034027, 421.7491979598999, 425.9505100250244, 430.1518220901489, 434.42251801490784, 438.69321393966675, 442.89448499679565, 447.09575605392456, 451.32728385925293, 455.5588116645813, 459.786851644516, 464.0148916244507, 468.2869873046875, 472.5590829849243, 476.76228761672974, 480.96549224853516, 485.2190628051758, 489.4726333618164, 493.7444279193878, 498.01622247695923, 502.2615749835968, 506.5069274902344, 510.7665991783142, 515.026270866394, 519.2743935585022, 523.5225162506104, 527.7906908988953, 532.0588655471802, 536.3303818702698, 540.6018981933594, 544.8799405097961, 549.1579828262329, 553.4144065380096, 557.6708302497864, 561.9629981517792, 566.255166053772, 570.5243556499481, 574.7935452461243, 579.0157325267792, 583.2379198074341, 587.5126469135284, 591.7873740196228, 596.0313429832458, 600.2753119468689, 604.5363750457764, 608.7974381446838, 613.1908903121948, 617.5843424797058, 621.859379529953, 626.1344165802002, 630.3832595348358, 634.6321024894714, 638.8720152378082, 643.111927986145, 647.3890163898468, 651.6661047935486, 655.9310219287872, 660.1959390640259, 664.4763443470001, 668.7567496299744, 673.0372717380524, 677.3177938461304, 681.5700716972351, 685.8223495483398, 690.1683864593506, 694.5144233703613, 698.7925832271576, 703.0707430839539, 707.4030430316925, 711.7353429794312, 716.0269694328308, 720.3185958862305, 724.5810170173645, 728.8434381484985, 733.1177124977112, 737.3919868469238, 741.7181489467621, 746.0443110466003, 750.3227376937866, 754.6011643409729, 758.9120092391968, 763.2228541374207, 767.5549976825714, 771.8871412277222, 776.1637012958527, 780.4402613639832, 784.7179634571075, 788.9956655502319, 793.2865886688232, 797.5775117874146, 801.8660323619843, 806.154552936554, 810.4552311897278, 814.7559094429016, 819.038494348526, 823.3210792541504, 827.6127343177795, 831.9043893814087, 836.2053933143616, 840.5063972473145, 844.8024349212646, 849.0984725952148, 853.3977146148682, 857.6969566345215, 861.9936583042145, 866.2903599739075, 870.5856704711914, 874.8809809684753, 877.0498490333557, 879.2187170982361]
[34.0725, 34.0725, 40.265, 40.265, 43.0325, 43.0325, 46.6925, 46.6925, 49.445, 49.445, 51.27, 51.27, 52.88, 52.88, 55.01, 55.01, 56.0225, 56.0225, 57.245, 57.245, 58.5425, 58.5425, 58.9075, 58.9075, 59.8075, 59.8075, 61.155, 61.155, 61.6275, 61.6275, 62.105, 62.105, 63.2025, 63.2025, 63.805, 63.805, 64.52, 64.52, 64.525, 64.525, 65.35, 65.35, 65.3375, 65.3375, 65.9925, 65.9925, 66.4775, 66.4775, 67.415, 67.415, 67.64, 67.64, 67.7775, 67.7775, 68.2, 68.2, 68.2675, 68.2675, 68.2175, 68.2175, 68.3175, 68.3175, 68.78, 68.78, 69.025, 69.025, 69.2425, 69.2425, 69.28, 69.28, 69.03, 69.03, 69.5875, 69.5875, 70.19, 70.19, 70.1725, 70.1725, 70.0625, 70.0625, 70.2, 70.2, 70.165, 70.165, 70.195, 70.195, 70.3025, 70.3025, 70.1425, 70.1425, 70.4175, 70.4175, 70.76, 70.76, 70.66, 70.66, 70.99, 70.99, 71.215, 71.215, 71.48, 71.48, 71.435, 71.435, 71.605, 71.605, 71.535, 71.535, 71.85, 71.85, 71.815, 71.815, 71.6775, 71.6775, 71.775, 71.775, 71.615, 71.615, 71.615, 71.615, 71.8625, 71.8625, 72.03, 72.03, 72.185, 72.185, 72.235, 72.235, 71.935, 71.935, 71.795, 71.795, 71.765, 71.765, 71.68, 71.68, 71.9775, 71.9775, 72.3925, 72.3925, 72.3925, 72.3925, 72.41, 72.41, 72.6, 72.6, 72.73, 72.73, 72.7425, 72.7425, 72.635, 72.635, 72.9775, 72.9775, 72.9, 72.9, 73.0225, 73.0225, 72.885, 72.885, 72.765, 72.765, 72.685, 72.685, 72.5175, 72.5175, 72.835, 72.835, 72.9525, 72.9525, 73.03, 73.03, 73.0275, 73.0275, 72.9425, 72.9425, 72.8, 72.8, 72.885, 72.885, 72.9425, 72.9425, 73.305, 73.305, 73.265, 73.265, 73.2625, 73.2625, 73.3725, 73.3725, 73.6175, 73.6175, 73.7575, 73.7575, 73.58, 73.58, 73.3225, 73.3225, 73.735, 73.735, 74.4525, 74.4525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.206, Test loss: 1.965, Test accuracy: 29.09
Round   1, Train loss: 1.887, Test loss: 1.715, Test accuracy: 36.62
Round   2, Train loss: 1.691, Test loss: 1.598, Test accuracy: 41.04
Round   3, Train loss: 1.602, Test loss: 1.527, Test accuracy: 44.84
Round   4, Train loss: 1.521, Test loss: 1.450, Test accuracy: 47.74
Round   5, Train loss: 1.460, Test loss: 1.394, Test accuracy: 50.53
Round   6, Train loss: 1.420, Test loss: 1.291, Test accuracy: 53.97
Round   7, Train loss: 1.339, Test loss: 1.242, Test accuracy: 56.20
Round   8, Train loss: 1.281, Test loss: 1.203, Test accuracy: 56.75
Round   9, Train loss: 1.233, Test loss: 1.164, Test accuracy: 58.91
Round  10, Train loss: 1.213, Test loss: 1.112, Test accuracy: 60.94
Round  11, Train loss: 1.158, Test loss: 1.092, Test accuracy: 61.65
Round  12, Train loss: 1.122, Test loss: 1.081, Test accuracy: 62.07
Round  13, Train loss: 1.100, Test loss: 1.042, Test accuracy: 63.47
Round  14, Train loss: 1.074, Test loss: 1.037, Test accuracy: 63.35
Round  15, Train loss: 1.026, Test loss: 1.019, Test accuracy: 64.47
Round  16, Train loss: 1.011, Test loss: 0.991, Test accuracy: 65.50
Round  17, Train loss: 1.015, Test loss: 0.956, Test accuracy: 66.78
Round  18, Train loss: 0.972, Test loss: 0.944, Test accuracy: 67.35
Round  19, Train loss: 0.968, Test loss: 0.928, Test accuracy: 68.36
Round  20, Train loss: 0.957, Test loss: 0.907, Test accuracy: 68.82
Round  21, Train loss: 0.917, Test loss: 0.893, Test accuracy: 69.03
Round  22, Train loss: 0.876, Test loss: 0.895, Test accuracy: 69.26
Round  23, Train loss: 0.930, Test loss: 0.871, Test accuracy: 69.95
Round  24, Train loss: 0.888, Test loss: 0.881, Test accuracy: 69.74
Round  25, Train loss: 0.871, Test loss: 0.867, Test accuracy: 70.47
Round  26, Train loss: 0.855, Test loss: 0.857, Test accuracy: 70.31
Round  27, Train loss: 0.827, Test loss: 0.844, Test accuracy: 70.89
Round  28, Train loss: 0.811, Test loss: 0.840, Test accuracy: 71.12
Round  29, Train loss: 0.819, Test loss: 0.821, Test accuracy: 72.10
Round  30, Train loss: 0.805, Test loss: 0.817, Test accuracy: 72.19
Round  31, Train loss: 0.802, Test loss: 0.826, Test accuracy: 71.79
Round  32, Train loss: 0.786, Test loss: 0.816, Test accuracy: 72.44
Round  33, Train loss: 0.769, Test loss: 0.807, Test accuracy: 72.69
Round  34, Train loss: 0.781, Test loss: 0.805, Test accuracy: 72.77
Round  35, Train loss: 0.759, Test loss: 0.798, Test accuracy: 72.74
Round  36, Train loss: 0.732, Test loss: 0.808, Test accuracy: 72.36
Round  37, Train loss: 0.761, Test loss: 0.785, Test accuracy: 73.31
Round  38, Train loss: 0.725, Test loss: 0.784, Test accuracy: 73.04
Round  39, Train loss: 0.725, Test loss: 0.791, Test accuracy: 73.17
Round  40, Train loss: 0.716, Test loss: 0.778, Test accuracy: 73.58
Round  41, Train loss: 0.697, Test loss: 0.774, Test accuracy: 73.86
Round  42, Train loss: 0.706, Test loss: 0.768, Test accuracy: 74.26
Round  43, Train loss: 0.691, Test loss: 0.764, Test accuracy: 74.33
Round  44, Train loss: 0.694, Test loss: 0.761, Test accuracy: 74.44
Round  45, Train loss: 0.675, Test loss: 0.768, Test accuracy: 73.98
Round  46, Train loss: 0.660, Test loss: 0.762, Test accuracy: 74.26
Round  47, Train loss: 0.682, Test loss: 0.746, Test accuracy: 74.94
Round  48, Train loss: 0.659, Test loss: 0.739, Test accuracy: 75.27
Round  49, Train loss: 0.668, Test loss: 0.746, Test accuracy: 75.09
Round  50, Train loss: 0.649, Test loss: 0.747, Test accuracy: 74.76
Round  51, Train loss: 0.647, Test loss: 0.745, Test accuracy: 74.93
Round  52, Train loss: 0.628, Test loss: 0.748, Test accuracy: 74.75
Round  53, Train loss: 0.631, Test loss: 0.754, Test accuracy: 74.71
Round  54, Train loss: 0.621, Test loss: 0.750, Test accuracy: 74.64
Round  55, Train loss: 0.636, Test loss: 0.750, Test accuracy: 74.88
Round  56, Train loss: 0.628, Test loss: 0.749, Test accuracy: 74.97
Round  57, Train loss: 0.619, Test loss: 0.752, Test accuracy: 75.13
Round  58, Train loss: 0.616, Test loss: 0.755, Test accuracy: 74.97
Round  59, Train loss: 0.630, Test loss: 0.742, Test accuracy: 75.19
Round  60, Train loss: 0.596, Test loss: 0.729, Test accuracy: 75.76
Round  61, Train loss: 0.598, Test loss: 0.732, Test accuracy: 75.58
Round  62, Train loss: 0.585, Test loss: 0.741, Test accuracy: 75.45
Round  63, Train loss: 0.578, Test loss: 0.739, Test accuracy: 75.43
Round  64, Train loss: 0.567, Test loss: 0.748, Test accuracy: 75.25
Round  65, Train loss: 0.607, Test loss: 0.738, Test accuracy: 75.65
Round  66, Train loss: 0.591, Test loss: 0.753, Test accuracy: 75.08
Round  67, Train loss: 0.584, Test loss: 0.748, Test accuracy: 75.28
Round  68, Train loss: 0.582, Test loss: 0.748, Test accuracy: 75.33
Round  69, Train loss: 0.566, Test loss: 0.761, Test accuracy: 75.11
Round  70, Train loss: 0.574, Test loss: 0.763, Test accuracy: 75.53
Round  71, Train loss: 0.572, Test loss: 0.746, Test accuracy: 75.86
Round  72, Train loss: 0.557, Test loss: 0.748, Test accuracy: 75.65
Round  73, Train loss: 0.541, Test loss: 0.759, Test accuracy: 75.44
Round  74, Train loss: 0.523, Test loss: 0.749, Test accuracy: 75.57
Round  75, Train loss: 0.567, Test loss: 0.739, Test accuracy: 75.88
Round  76, Train loss: 0.570, Test loss: 0.750, Test accuracy: 75.69
Round  77, Train loss: 0.543, Test loss: 0.738, Test accuracy: 76.18
Round  78, Train loss: 0.527, Test loss: 0.744, Test accuracy: 75.91
Round  79, Train loss: 0.509, Test loss: 0.747, Test accuracy: 75.56
Round  80, Train loss: 0.537, Test loss: 0.734, Test accuracy: 75.92
Round  81, Train loss: 0.536, Test loss: 0.766, Test accuracy: 75.44
Round  82, Train loss: 0.539, Test loss: 0.761, Test accuracy: 75.62
Round  83, Train loss: 0.515, Test loss: 0.760, Test accuracy: 75.40
Round  84, Train loss: 0.568, Test loss: 0.759, Test accuracy: 75.76
Round  85, Train loss: 0.549, Test loss: 0.756, Test accuracy: 75.86
Round  86, Train loss: 0.524, Test loss: 0.765, Test accuracy: 75.70
Round  87, Train loss: 0.497, Test loss: 0.762, Test accuracy: 75.77
Round  88, Train loss: 0.532, Test loss: 0.757, Test accuracy: 76.08
Round  89, Train loss: 0.507, Test loss: 0.761, Test accuracy: 75.81
Round  90, Train loss: 0.489, Test loss: 0.753, Test accuracy: 76.17
Round  91, Train loss: 0.519, Test loss: 0.751, Test accuracy: 76.42
Round  92, Train loss: 0.523, Test loss: 0.742, Test accuracy: 76.64
Round  93, Train loss: 0.495, Test loss: 0.773, Test accuracy: 75.80
Round  94, Train loss: 0.500, Test loss: 0.766, Test accuracy: 76.33
Round  95, Train loss: 0.505, Test loss: 0.770, Test accuracy: 76.36
Round  96, Train loss: 0.494, Test loss: 0.761, Test accuracy: 76.25
Round  97, Train loss: 0.512, Test loss: 0.749, Test accuracy: 76.31
Round  98, Train loss: 0.484, Test loss: 0.750, Test accuracy: 76.20
Round  99, Train loss: 0.491, Test loss: 0.751, Test accuracy: 76.49
Final Round, Train loss: 0.426, Test loss: 0.744, Test accuracy: 76.81
Average accuracy final 10 rounds: 76.29825 

4779.81906414032
[4.4221155643463135, 8.844231128692627, 12.936514616012573, 17.02879810333252, 21.069042682647705, 25.10928726196289, 29.166527032852173, 33.223766803741455, 37.284831047058105, 41.345895290374756, 45.42334222793579, 49.500789165496826, 53.52516555786133, 57.54954195022583, 61.53680443763733, 65.52406692504883, 69.5200560092926, 73.51604509353638, 77.50863218307495, 81.50121927261353, 85.50078892707825, 89.50035858154297, 93.48676419258118, 97.47316980361938, 101.5159044265747, 105.55863904953003, 109.61401748657227, 113.6693959236145, 117.7589898109436, 121.8485836982727, 125.92715764045715, 130.0057315826416, 134.09323000907898, 138.18072843551636, 142.1624801158905, 146.14423179626465, 150.15058660507202, 154.1569414138794, 158.16262555122375, 162.16830968856812, 166.16929960250854, 170.17028951644897, 174.1989085674286, 178.2275276184082, 182.2359881401062, 186.2444486618042, 190.2211470603943, 194.19784545898438, 198.1980926990509, 202.19833993911743, 206.24574542045593, 210.29315090179443, 214.34806537628174, 218.40297985076904, 222.38063383102417, 226.3582878112793, 230.3342821598053, 234.3102765083313, 238.32067394256592, 242.33107137680054, 246.3801736831665, 250.42927598953247, 254.43251013755798, 258.4357442855835, 262.4187731742859, 266.4018020629883, 270.4590103626251, 274.51621866226196, 278.55918073654175, 282.60214281082153, 286.58662581443787, 290.5711088180542, 294.5676324367523, 298.56415605545044, 302.59037375450134, 306.61659145355225, 310.6688175201416, 314.72104358673096, 318.72671914100647, 322.732394695282, 326.73692989349365, 330.7414650917053, 334.7800991535187, 338.81873321533203, 342.8480181694031, 346.8773031234741, 350.8737621307373, 354.8702211380005, 358.8510718345642, 362.83192253112793, 366.865615606308, 370.89930868148804, 374.92606377601624, 378.95281887054443, 382.91005754470825, 386.86729621887207, 390.8826847076416, 394.89807319641113, 398.87663435935974, 402.85519552230835, 406.8685517311096, 410.8819079399109, 414.8324930667877, 418.78307819366455, 422.7537546157837, 426.72443103790283, 430.7336413860321, 434.7428517341614, 438.74533319473267, 442.74781465530396, 446.7438349723816, 450.73985528945923, 454.6982581615448, 458.65666103363037, 462.732549905777, 466.8084387779236, 470.8555426597595, 474.90264654159546, 478.9045202732086, 482.9063940048218, 486.8891248703003, 490.8718557357788, 494.8902208805084, 498.90858602523804, 502.94888281822205, 506.98917961120605, 511.0129723548889, 515.0367650985718, 519.0380187034607, 523.0392723083496, 527.1164164543152, 531.1935606002808, 535.2451539039612, 539.2967472076416, 543.3254327774048, 547.354118347168, 551.4105021953583, 555.4668860435486, 559.7776441574097, 564.0884022712708, 568.1868462562561, 572.2852902412415, 576.3228027820587, 580.360315322876, 584.4118330478668, 588.4633507728577, 592.5826439857483, 596.7019371986389, 600.7638776302338, 604.8258180618286, 609.4001605510712, 613.9745030403137, 618.5298428535461, 623.0851826667786, 627.5104260444641, 631.9356694221497, 636.3866882324219, 640.8377070426941, 645.4457278251648, 650.0537486076355, 654.6291966438293, 659.2046446800232, 663.6261448860168, 668.0476450920105, 672.5041356086731, 676.9606261253357, 681.4064426422119, 685.8522591590881, 690.2955012321472, 694.7387433052063, 699.1279661655426, 703.5171890258789, 707.9510395526886, 712.3848900794983, 716.8294727802277, 721.274055480957, 725.7381284236908, 730.2022013664246, 734.6359357833862, 739.0696702003479, 743.470076084137, 747.870481967926, 752.3185110092163, 756.7665400505066, 761.2083277702332, 765.6501154899597, 770.1042454242706, 774.5583753585815, 778.9649786949158, 783.37158203125, 787.7759850025177, 792.1803879737854, 796.6325497627258, 801.0847115516663, 805.5631115436554, 810.0415115356445, 814.4578251838684, 818.8741388320923, 823.2809274196625, 827.6877160072327, 829.7154507637024, 831.7431855201721]
[29.0875, 29.0875, 36.615, 36.615, 41.04, 41.04, 44.8425, 44.8425, 47.74, 47.74, 50.53, 50.53, 53.9725, 53.9725, 56.2025, 56.2025, 56.7475, 56.7475, 58.9075, 58.9075, 60.935, 60.935, 61.6525, 61.6525, 62.0675, 62.0675, 63.47, 63.47, 63.35, 63.35, 64.4725, 64.4725, 65.5, 65.5, 66.775, 66.775, 67.3475, 67.3475, 68.3625, 68.3625, 68.82, 68.82, 69.0275, 69.0275, 69.2575, 69.2575, 69.9475, 69.9475, 69.7425, 69.7425, 70.47, 70.47, 70.31, 70.31, 70.885, 70.885, 71.1175, 71.1175, 72.0975, 72.0975, 72.185, 72.185, 71.7875, 71.7875, 72.4375, 72.4375, 72.69, 72.69, 72.765, 72.765, 72.74, 72.74, 72.36, 72.36, 73.3125, 73.3125, 73.04, 73.04, 73.165, 73.165, 73.5825, 73.5825, 73.8575, 73.8575, 74.26, 74.26, 74.3275, 74.3275, 74.44, 74.44, 73.985, 73.985, 74.2625, 74.2625, 74.945, 74.945, 75.2675, 75.2675, 75.095, 75.095, 74.7625, 74.7625, 74.9275, 74.9275, 74.745, 74.745, 74.7075, 74.7075, 74.64, 74.64, 74.8775, 74.8775, 74.9725, 74.9725, 75.1275, 75.1275, 74.9725, 74.9725, 75.19, 75.19, 75.76, 75.76, 75.575, 75.575, 75.4525, 75.4525, 75.4275, 75.4275, 75.25, 75.25, 75.6525, 75.6525, 75.0775, 75.0775, 75.28, 75.28, 75.3325, 75.3325, 75.11, 75.11, 75.53, 75.53, 75.855, 75.855, 75.65, 75.65, 75.44, 75.44, 75.57, 75.57, 75.8775, 75.8775, 75.685, 75.685, 76.18, 76.18, 75.9125, 75.9125, 75.5625, 75.5625, 75.9175, 75.9175, 75.435, 75.435, 75.6175, 75.6175, 75.4, 75.4, 75.7625, 75.7625, 75.855, 75.855, 75.705, 75.705, 75.77, 75.77, 76.0775, 76.0775, 75.8125, 75.8125, 76.175, 76.175, 76.4175, 76.4175, 76.6425, 76.6425, 75.795, 75.795, 76.33, 76.33, 76.365, 76.365, 76.255, 76.255, 76.31, 76.31, 76.205, 76.205, 76.4875, 76.4875, 76.815, 76.815]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.216, Test loss: 1.963, Test accuracy: 26.36
Round   1, Train loss: 1.885, Test loss: 1.712, Test accuracy: 37.08
Round   2, Train loss: 1.692, Test loss: 1.601, Test accuracy: 42.84
Round   3, Train loss: 1.600, Test loss: 1.493, Test accuracy: 47.54
Round   4, Train loss: 1.507, Test loss: 1.383, Test accuracy: 51.29
Round   5, Train loss: 1.411, Test loss: 1.364, Test accuracy: 52.38
Round   6, Train loss: 1.375, Test loss: 1.238, Test accuracy: 56.99
Round   7, Train loss: 1.312, Test loss: 1.202, Test accuracy: 59.11
Round   8, Train loss: 1.261, Test loss: 1.133, Test accuracy: 61.06
Round   9, Train loss: 1.212, Test loss: 1.101, Test accuracy: 62.62
Round  10, Train loss: 1.185, Test loss: 1.092, Test accuracy: 63.08
Round  11, Train loss: 1.156, Test loss: 1.047, Test accuracy: 64.48
Round  12, Train loss: 1.117, Test loss: 1.027, Test accuracy: 65.06
Round  13, Train loss: 1.074, Test loss: 1.008, Test accuracy: 65.49
Round  14, Train loss: 1.058, Test loss: 0.976, Test accuracy: 66.81
Round  15, Train loss: 1.025, Test loss: 0.962, Test accuracy: 67.93
Round  16, Train loss: 1.034, Test loss: 0.939, Test accuracy: 68.47
Round  17, Train loss: 0.983, Test loss: 0.926, Test accuracy: 69.25
Round  18, Train loss: 0.973, Test loss: 0.902, Test accuracy: 69.56
Round  19, Train loss: 0.948, Test loss: 0.894, Test accuracy: 69.51
Round  20, Train loss: 0.923, Test loss: 0.887, Test accuracy: 70.01
Round  21, Train loss: 0.924, Test loss: 0.872, Test accuracy: 70.79
Round  22, Train loss: 0.900, Test loss: 0.875, Test accuracy: 70.61
Round  23, Train loss: 0.923, Test loss: 0.851, Test accuracy: 71.47
Round  24, Train loss: 0.868, Test loss: 0.834, Test accuracy: 71.80
Round  25, Train loss: 0.876, Test loss: 0.830, Test accuracy: 72.00
Round  26, Train loss: 0.857, Test loss: 0.824, Test accuracy: 71.81
Round  27, Train loss: 0.827, Test loss: 0.828, Test accuracy: 72.08
Round  28, Train loss: 0.823, Test loss: 0.814, Test accuracy: 72.59
Round  29, Train loss: 0.838, Test loss: 0.804, Test accuracy: 72.97
Round  30, Train loss: 0.792, Test loss: 0.786, Test accuracy: 73.25
Round  31, Train loss: 0.819, Test loss: 0.788, Test accuracy: 73.41
Round  32, Train loss: 0.786, Test loss: 0.780, Test accuracy: 73.48
Round  33, Train loss: 0.770, Test loss: 0.775, Test accuracy: 73.67
Round  34, Train loss: 0.775, Test loss: 0.771, Test accuracy: 73.79
Round  35, Train loss: 0.766, Test loss: 0.768, Test accuracy: 74.46
Round  36, Train loss: 0.766, Test loss: 0.765, Test accuracy: 74.48
Round  37, Train loss: 0.747, Test loss: 0.755, Test accuracy: 74.57
Round  38, Train loss: 0.729, Test loss: 0.765, Test accuracy: 74.20
Round  39, Train loss: 0.722, Test loss: 0.754, Test accuracy: 74.53
Round  40, Train loss: 0.706, Test loss: 0.768, Test accuracy: 74.38
Round  41, Train loss: 0.714, Test loss: 0.755, Test accuracy: 74.68
Round  42, Train loss: 0.716, Test loss: 0.752, Test accuracy: 75.10
Round  43, Train loss: 0.688, Test loss: 0.753, Test accuracy: 74.90
Round  44, Train loss: 0.682, Test loss: 0.759, Test accuracy: 74.76
Round  45, Train loss: 0.715, Test loss: 0.750, Test accuracy: 74.97
Round  46, Train loss: 0.690, Test loss: 0.744, Test accuracy: 75.15
Round  47, Train loss: 0.672, Test loss: 0.746, Test accuracy: 74.97
Round  48, Train loss: 0.665, Test loss: 0.745, Test accuracy: 75.09
Round  49, Train loss: 0.676, Test loss: 0.732, Test accuracy: 75.57
Round  50, Train loss: 0.662, Test loss: 0.724, Test accuracy: 75.80
Round  51, Train loss: 0.665, Test loss: 0.732, Test accuracy: 75.62
Round  52, Train loss: 0.651, Test loss: 0.729, Test accuracy: 75.70
Round  53, Train loss: 0.661, Test loss: 0.722, Test accuracy: 75.84
Round  54, Train loss: 0.645, Test loss: 0.728, Test accuracy: 75.63
Round  55, Train loss: 0.647, Test loss: 0.730, Test accuracy: 75.44
Round  56, Train loss: 0.635, Test loss: 0.725, Test accuracy: 75.84
Round  57, Train loss: 0.638, Test loss: 0.726, Test accuracy: 75.94
Round  58, Train loss: 0.616, Test loss: 0.717, Test accuracy: 76.18
Round  59, Train loss: 0.644, Test loss: 0.729, Test accuracy: 76.08
Round  60, Train loss: 0.626, Test loss: 0.718, Test accuracy: 76.19
Round  61, Train loss: 0.625, Test loss: 0.727, Test accuracy: 75.89
Round  62, Train loss: 0.605, Test loss: 0.715, Test accuracy: 76.47
Round  63, Train loss: 0.596, Test loss: 0.710, Test accuracy: 76.57
Round  64, Train loss: 0.637, Test loss: 0.723, Test accuracy: 76.09
Round  65, Train loss: 0.590, Test loss: 0.717, Test accuracy: 76.38
Round  66, Train loss: 0.617, Test loss: 0.717, Test accuracy: 76.31
Round  67, Train loss: 0.621, Test loss: 0.713, Test accuracy: 76.41
Round  68, Train loss: 0.617, Test loss: 0.705, Test accuracy: 76.70
Round  69, Train loss: 0.597, Test loss: 0.710, Test accuracy: 76.57
Round  70, Train loss: 0.595, Test loss: 0.709, Test accuracy: 76.56
Round  71, Train loss: 0.578, Test loss: 0.705, Test accuracy: 76.78
Round  72, Train loss: 0.591, Test loss: 0.713, Test accuracy: 76.54
Round  73, Train loss: 0.590, Test loss: 0.712, Test accuracy: 76.71
Round  74, Train loss: 0.575, Test loss: 0.699, Test accuracy: 77.14
Round  75, Train loss: 0.600, Test loss: 0.698, Test accuracy: 77.12
Round  76, Train loss: 0.574, Test loss: 0.691, Test accuracy: 77.08
Round  77, Train loss: 0.563, Test loss: 0.705, Test accuracy: 76.78
Round  78, Train loss: 0.552, Test loss: 0.703, Test accuracy: 76.61
Round  79, Train loss: 0.555, Test loss: 0.719, Test accuracy: 76.25
Round  80, Train loss: 0.565, Test loss: 0.699, Test accuracy: 76.91
Round  81, Train loss: 0.550, Test loss: 0.711, Test accuracy: 76.66
Round  82, Train loss: 0.558, Test loss: 0.699, Test accuracy: 76.98
Round  83, Train loss: 0.552, Test loss: 0.704, Test accuracy: 76.88
Round  84, Train loss: 0.530, Test loss: 0.697, Test accuracy: 77.09
Round  85, Train loss: 0.550, Test loss: 0.700, Test accuracy: 76.94
Round  86, Train loss: 0.543, Test loss: 0.703, Test accuracy: 76.95
Round  87, Train loss: 0.517, Test loss: 0.698, Test accuracy: 77.07
Round  88, Train loss: 0.546, Test loss: 0.690, Test accuracy: 77.34
Round  89, Train loss: 0.539, Test loss: 0.692, Test accuracy: 77.36
Round  90, Train loss: 0.522, Test loss: 0.704, Test accuracy: 76.94
Round  91, Train loss: 0.577, Test loss: 0.701, Test accuracy: 77.13
Round  92, Train loss: 0.530, Test loss: 0.697, Test accuracy: 76.92
Round  93, Train loss: 0.525, Test loss: 0.692, Test accuracy: 77.32
Round  94, Train loss: 0.495, Test loss: 0.706, Test accuracy: 77.08
Round  95, Train loss: 0.527, Test loss: 0.697, Test accuracy: 77.21
Round  96, Train loss: 0.510, Test loss: 0.713, Test accuracy: 76.85
Round  97, Train loss: 0.521, Test loss: 0.717, Test accuracy: 77.14
Round  98, Train loss: 0.538, Test loss: 0.703, Test accuracy: 76.92
Round  99, Train loss: 0.505, Test loss: 0.704, Test accuracy: 77.01
Final Round, Train loss: 0.433, Test loss: 0.703, Test accuracy: 77.06
Average accuracy final 10 rounds: 77.05100000000002
5699.693621873856
[6.226452827453613, 12.452905654907227, 18.267606019973755, 24.082306385040283, 29.931450128555298, 35.78059387207031, 41.65706825256348, 47.53354263305664, 53.41237950325012, 59.2912163734436, 65.20481204986572, 71.11840772628784, 76.85036540031433, 82.58232307434082, 88.41826295852661, 94.2542028427124, 99.98062205314636, 105.70704126358032, 111.40909814834595, 117.11115503311157, 122.44235587120056, 127.77355670928955, 133.11524295806885, 138.45692920684814, 143.7689390182495, 149.08094882965088, 154.41505527496338, 159.74916172027588, 165.03342032432556, 170.31767892837524, 175.64196038246155, 180.96624183654785, 186.27983450889587, 191.5934271812439, 196.88339638710022, 202.17336559295654, 207.50555062294006, 212.83773565292358, 218.15633630752563, 223.47493696212769, 228.84353613853455, 234.2121353149414, 239.5631811618805, 244.91422700881958, 250.20588088035583, 255.4975347518921, 260.821298122406, 266.1450614929199, 271.52836418151855, 276.9116668701172, 282.2748758792877, 287.63808488845825, 293.05346179008484, 298.4688386917114, 303.77475357055664, 309.08066844940186, 314.428936958313, 319.7772054672241, 325.1558072566986, 330.5344090461731, 335.8521635532379, 341.16991806030273, 346.53826427459717, 351.9066104888916, 357.1670298576355, 362.4274492263794, 367.7393925189972, 373.051335811615, 378.40757417678833, 383.76381254196167, 389.10653018951416, 394.44924783706665, 399.7432403564453, 405.037232875824, 410.868825674057, 416.70041847229004, 422.1559522151947, 427.61148595809937, 433.4251916408539, 439.2388973236084, 445.1098313331604, 450.9807653427124, 456.8914999961853, 462.8022346496582, 468.0611116886139, 473.3199887275696, 478.7535152435303, 484.18704175949097, 489.58771228790283, 494.9883828163147, 500.4366328716278, 505.8848829269409, 511.29210901260376, 516.6993350982666, 522.6111223697662, 528.5229096412659, 534.3674952983856, 540.2120809555054, 546.1234467029572, 552.0348124504089, 557.9370653629303, 563.8393182754517, 569.8039548397064, 575.7685914039612, 581.7212536334991, 587.6739158630371, 593.567412853241, 599.4609098434448, 605.4153802394867, 611.3698506355286, 617.2190201282501, 623.0681896209717, 629.0107491016388, 634.9533085823059, 640.8746507167816, 646.7959928512573, 652.7166223526001, 658.6372518539429, 664.5595850944519, 670.4819183349609, 676.4264287948608, 682.3709392547607, 688.2986586093903, 694.2263779640198, 700.1910400390625, 706.1557021141052, 712.0941529273987, 718.0326037406921, 723.9793000221252, 729.9259963035583, 735.8836538791656, 741.841311454773, 747.7843880653381, 753.7274646759033, 759.6922779083252, 765.6570911407471, 771.634352684021, 777.6116142272949, 783.5324878692627, 789.4533615112305, 795.3578593730927, 801.2623572349548, 807.1921575069427, 813.1219577789307, 819.0251550674438, 824.928352355957, 830.8390984535217, 836.7498445510864, 842.6584415435791, 848.5670385360718, 854.4916179180145, 860.4161972999573, 866.3672742843628, 872.3183512687683, 878.2858941555023, 884.2534370422363, 890.1835157871246, 896.1135945320129, 902.0563135147095, 907.999032497406, 913.4304802417755, 918.861927986145, 924.3118023872375, 929.7616767883301, 935.1832845211029, 940.6048922538757, 946.0439131259918, 951.4829339981079, 956.9138240814209, 962.3447141647339, 967.8214621543884, 973.298210144043, 978.7453653812408, 984.1925206184387, 989.6621885299683, 995.1318564414978, 1000.5427541732788, 1005.9536519050598, 1011.4033555984497, 1016.8530592918396, 1022.2737848758698, 1027.6945104599, 1033.161646604538, 1038.628782749176, 1044.0851635932922, 1049.5415444374084, 1055.012264251709, 1060.4829840660095, 1065.9704284667969, 1071.4578728675842, 1076.921226978302, 1082.3845810890198, 1087.755688905716, 1093.126796722412, 1098.5472314357758, 1103.9676661491394, 1109.4094457626343, 1114.8512253761292, 1120.2919459342957, 1125.7326664924622, 1127.8507840633392, 1129.9689016342163]
[26.3625, 26.3625, 37.075, 37.075, 42.8425, 42.8425, 47.54, 47.54, 51.2875, 51.2875, 52.38, 52.38, 56.995, 56.995, 59.11, 59.11, 61.0575, 61.0575, 62.6225, 62.6225, 63.075, 63.075, 64.4775, 64.4775, 65.0625, 65.0625, 65.4925, 65.4925, 66.805, 66.805, 67.9275, 67.9275, 68.4725, 68.4725, 69.245, 69.245, 69.56, 69.56, 69.5075, 69.5075, 70.01, 70.01, 70.7875, 70.7875, 70.605, 70.605, 71.4675, 71.4675, 71.8025, 71.8025, 72.0025, 72.0025, 71.8075, 71.8075, 72.0825, 72.0825, 72.595, 72.595, 72.965, 72.965, 73.2475, 73.2475, 73.4125, 73.4125, 73.4775, 73.4775, 73.6725, 73.6725, 73.7875, 73.7875, 74.4625, 74.4625, 74.48, 74.48, 74.5725, 74.5725, 74.2025, 74.2025, 74.5325, 74.5325, 74.375, 74.375, 74.6775, 74.6775, 75.1, 75.1, 74.8975, 74.8975, 74.7575, 74.7575, 74.965, 74.965, 75.1475, 75.1475, 74.975, 74.975, 75.09, 75.09, 75.5725, 75.5725, 75.8, 75.8, 75.6175, 75.6175, 75.7025, 75.7025, 75.84, 75.84, 75.63, 75.63, 75.445, 75.445, 75.8425, 75.8425, 75.935, 75.935, 76.1775, 76.1775, 76.075, 76.075, 76.195, 76.195, 75.895, 75.895, 76.4675, 76.4675, 76.57, 76.57, 76.0875, 76.0875, 76.3775, 76.3775, 76.31, 76.31, 76.4075, 76.4075, 76.705, 76.705, 76.57, 76.57, 76.5625, 76.5625, 76.78, 76.78, 76.5425, 76.5425, 76.7125, 76.7125, 77.14, 77.14, 77.125, 77.125, 77.0825, 77.0825, 76.7775, 76.7775, 76.61, 76.61, 76.245, 76.245, 76.9125, 76.9125, 76.6575, 76.6575, 76.985, 76.985, 76.875, 76.875, 77.095, 77.095, 76.935, 76.935, 76.955, 76.955, 77.0725, 77.0725, 77.34, 77.34, 77.365, 77.365, 76.94, 76.94, 77.13, 77.13, 76.9175, 76.9175, 77.3225, 77.3225, 77.08, 77.08, 77.2075, 77.2075, 76.85, 76.85, 77.135, 77.135, 76.915, 76.915, 77.0125, 77.0125, 77.055, 77.055]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.167, Test loss: 1.781, Test accuracy: 34.14
Round   1, Train loss: 1.763, Test loss: 1.551, Test accuracy: 42.72
Round   2, Train loss: 1.622, Test loss: 1.455, Test accuracy: 46.37
Round   3, Train loss: 1.531, Test loss: 1.391, Test accuracy: 48.86
Round   4, Train loss: 1.416, Test loss: 1.408, Test accuracy: 53.00
Round   5, Train loss: 1.346, Test loss: 1.261, Test accuracy: 54.74
Round   6, Train loss: 1.265, Test loss: 1.224, Test accuracy: 56.10
Round   7, Train loss: 1.214, Test loss: 1.186, Test accuracy: 57.90
Round   8, Train loss: 1.184, Test loss: 1.201, Test accuracy: 60.16
Round   9, Train loss: 1.114, Test loss: 1.116, Test accuracy: 58.61
Round  10, Train loss: 1.093, Test loss: 1.081, Test accuracy: 61.09
Round  11, Train loss: 1.053, Test loss: 1.145, Test accuracy: 62.71
Round  12, Train loss: 0.980, Test loss: 1.155, Test accuracy: 63.27
Round  13, Train loss: 0.987, Test loss: 1.027, Test accuracy: 61.16
Round  14, Train loss: 0.972, Test loss: 1.025, Test accuracy: 63.48
Round  15, Train loss: 0.924, Test loss: 1.009, Test accuracy: 61.86
Round  16, Train loss: 0.895, Test loss: 1.015, Test accuracy: 64.60
Round  17, Train loss: 0.881, Test loss: 0.978, Test accuracy: 63.16
Round  18, Train loss: 0.851, Test loss: 0.967, Test accuracy: 63.63
Round  19, Train loss: 0.864, Test loss: 0.958, Test accuracy: 63.53
Round  20, Train loss: 0.810, Test loss: 0.965, Test accuracy: 64.08
Round  21, Train loss: 0.788, Test loss: 1.247, Test accuracy: 63.80
Round  22, Train loss: 0.770, Test loss: 0.935, Test accuracy: 64.91
Round  23, Train loss: 0.786, Test loss: 1.084, Test accuracy: 67.70
Round  24, Train loss: 0.765, Test loss: 0.931, Test accuracy: 65.30
Round  25, Train loss: 0.737, Test loss: 1.059, Test accuracy: 65.33
Round  26, Train loss: 0.758, Test loss: 0.915, Test accuracy: 65.88
Round  27, Train loss: 0.706, Test loss: 0.912, Test accuracy: 66.09
Round  28, Train loss: 0.703, Test loss: 0.913, Test accuracy: 65.77
Round  29, Train loss: 0.698, Test loss: 0.908, Test accuracy: 66.40
Round  30, Train loss: 0.708, Test loss: 0.895, Test accuracy: 67.21
Round  31, Train loss: 0.695, Test loss: 1.094, Test accuracy: 69.27
Round  32, Train loss: 0.665, Test loss: 0.880, Test accuracy: 67.84
Round  33, Train loss: 0.654, Test loss: 0.939, Test accuracy: 69.26
Round  34, Train loss: 0.662, Test loss: 1.045, Test accuracy: 66.29
Round  35, Train loss: 0.622, Test loss: 0.897, Test accuracy: 67.38
Round  36, Train loss: 0.657, Test loss: 1.073, Test accuracy: 70.00
Round  37, Train loss: 0.612, Test loss: 0.882, Test accuracy: 67.88
Round  38, Train loss: 0.583, Test loss: 0.970, Test accuracy: 69.27
Round  39, Train loss: 0.618, Test loss: 1.054, Test accuracy: 66.74
Round  40, Train loss: 0.619, Test loss: 1.050, Test accuracy: 66.73
Round  41, Train loss: 0.607, Test loss: 1.086, Test accuracy: 70.06
Round  42, Train loss: 0.631, Test loss: 1.033, Test accuracy: 67.19
Round  43, Train loss: 0.570, Test loss: 1.038, Test accuracy: 67.06
Round  44, Train loss: 0.588, Test loss: 1.122, Test accuracy: 70.45
Round  45, Train loss: 0.549, Test loss: 0.985, Test accuracy: 70.22
Round  46, Train loss: 0.580, Test loss: 0.896, Test accuracy: 67.75
Round  47, Train loss: 0.574, Test loss: 1.058, Test accuracy: 67.41
Round  48, Train loss: 0.564, Test loss: 0.960, Test accuracy: 69.85
Round  49, Train loss: 0.592, Test loss: 0.949, Test accuracy: 70.11
Round  50, Train loss: 0.533, Test loss: 0.910, Test accuracy: 68.37
Round  51, Train loss: 0.556, Test loss: 0.934, Test accuracy: 70.52
Round  52, Train loss: 0.546, Test loss: 0.881, Test accuracy: 68.57
Round  53, Train loss: 0.540, Test loss: 0.936, Test accuracy: 70.52
Round  54, Train loss: 0.560, Test loss: 0.972, Test accuracy: 69.93
Round  55, Train loss: 0.571, Test loss: 0.961, Test accuracy: 70.40
Round  56, Train loss: 0.530, Test loss: 1.084, Test accuracy: 67.82
Round  57, Train loss: 0.518, Test loss: 1.136, Test accuracy: 70.90
Round  58, Train loss: 0.497, Test loss: 1.032, Test accuracy: 67.86
Round  59, Train loss: 0.506, Test loss: 1.029, Test accuracy: 71.10
Round  60, Train loss: 0.510, Test loss: 0.957, Test accuracy: 70.84
Round  61, Train loss: 0.503, Test loss: 1.088, Test accuracy: 68.02
Round  62, Train loss: 0.487, Test loss: 0.986, Test accuracy: 70.97
Round  63, Train loss: 0.528, Test loss: 0.942, Test accuracy: 70.91
Round  64, Train loss: 0.522, Test loss: 1.068, Test accuracy: 67.96
Round  65, Train loss: 0.499, Test loss: 1.102, Test accuracy: 68.38
Round  66, Train loss: 0.478, Test loss: 1.351, Test accuracy: 68.46
Round  67, Train loss: 0.456, Test loss: 1.058, Test accuracy: 68.29
Round  68, Train loss: 0.514, Test loss: 1.119, Test accuracy: 71.53
Round  69, Train loss: 0.482, Test loss: 0.877, Test accuracy: 69.65
Round  70, Train loss: 0.468, Test loss: 0.987, Test accuracy: 71.23
Round  71, Train loss: 0.455, Test loss: 0.973, Test accuracy: 71.47
Round  72, Train loss: 0.447, Test loss: 0.983, Test accuracy: 71.14
Round  73, Train loss: 0.482, Test loss: 0.950, Test accuracy: 71.13
Round  74, Train loss: 0.454, Test loss: 0.888, Test accuracy: 69.29
Round  75, Train loss: 0.495, Test loss: 0.961, Test accuracy: 71.69
Round  76, Train loss: 0.451, Test loss: 0.946, Test accuracy: 71.22
Round  77, Train loss: 0.439, Test loss: 0.979, Test accuracy: 71.55
Round  78, Train loss: 0.444, Test loss: 0.904, Test accuracy: 68.75
Round  79, Train loss: 0.448, Test loss: 1.160, Test accuracy: 71.48
Round  80, Train loss: 0.441, Test loss: 0.960, Test accuracy: 71.73
Round  81, Train loss: 0.460, Test loss: 0.971, Test accuracy: 71.15
Round  82, Train loss: 0.464, Test loss: 1.180, Test accuracy: 71.71
Round  83, Train loss: 0.472, Test loss: 0.968, Test accuracy: 71.46
Round  84, Train loss: 0.424, Test loss: 0.917, Test accuracy: 68.95
Round  85, Train loss: 0.405, Test loss: 1.104, Test accuracy: 68.91
Round  86, Train loss: 0.460, Test loss: 0.946, Test accuracy: 71.67
Round  87, Train loss: 0.428, Test loss: 0.890, Test accuracy: 69.61
Round  88, Train loss: 0.403, Test loss: 0.885, Test accuracy: 69.78
Round  89, Train loss: 0.418, Test loss: 0.885, Test accuracy: 70.44
Round  90, Train loss: 0.462, Test loss: 1.241, Test accuracy: 72.26
Round  91, Train loss: 0.420, Test loss: 1.177, Test accuracy: 72.39
Round  92, Train loss: 0.399, Test loss: 0.994, Test accuracy: 71.95
Round  93, Train loss: 0.425, Test loss: 0.955, Test accuracy: 72.05
Round  94, Train loss: 0.378, Test loss: 1.206, Test accuracy: 71.80
Round  95, Train loss: 0.392, Test loss: 0.905, Test accuracy: 69.70
Round  96, Train loss: 0.414, Test loss: 0.951, Test accuracy: 71.32
Round  97, Train loss: 0.432, Test loss: 1.131, Test accuracy: 68.58
Round  98, Train loss: 0.399, Test loss: 0.900, Test accuracy: 70.16
Round  99, Train loss: 0.396, Test loss: 1.106, Test accuracy: 69.20
Final Round, Train loss: 0.323, Test loss: 0.924, Test accuracy: 71.98
Average accuracy final 10 rounds: 70.94225
8291.956864595413
[12.625073671340942, 25.46743130683899, 38.302921295166016, 51.04456615447998, 63.79297995567322, 76.61419939994812, 89.41496109962463, 102.25477027893066, 115.02089929580688, 127.73612141609192, 140.42386960983276, 153.1140058040619, 165.91985821723938, 178.67385005950928, 191.4385929107666, 204.2276315689087, 216.996844291687, 229.82880067825317, 242.61179947853088, 255.30552744865417, 268.0651605129242, 280.77890038490295, 293.57736468315125, 305.1009826660156, 316.60621309280396, 328.061270236969, 339.5312180519104, 351.1544907093048, 362.6501934528351, 374.0657835006714, 385.4934685230255, 396.9406826496124, 408.4259476661682, 420.04043650627136, 431.5979952812195, 443.1059958934784, 454.59091901779175, 466.13255190849304, 478.1106631755829, 489.7480375766754, 501.44126558303833, 513.9936599731445, 527.0606021881104, 539.8248569965363, 552.14786195755, 564.3624441623688, 575.9196774959564, 587.5114002227783, 599.1747725009918, 610.679913520813, 622.3221521377563, 633.9343256950378, 645.461884021759, 657.6522476673126, 669.1197879314423, 681.197582244873, 693.303911447525, 704.8669431209564, 716.5163049697876, 728.0524587631226, 739.6262550354004, 751.2907819747925, 762.7108407020569, 774.4537711143494, 785.8516359329224, 796.9896035194397, 808.4479444026947, 819.6930160522461, 831.2117340564728, 842.4750518798828, 854.0055432319641, 865.5155401229858, 877.0505049228668, 888.381138086319, 899.867716550827, 911.181960105896, 922.582849740982, 934.1168758869171, 946.1505856513977, 958.0472965240479, 969.9220714569092, 982.3918616771698, 995.145316362381, 1007.760701417923, 1020.3049118518829, 1032.975916147232, 1045.6895883083344, 1058.4051876068115, 1071.0910959243774, 1083.6986727714539, 1095.0926322937012, 1106.6300110816956, 1118.2445430755615, 1129.7809007167816, 1141.3358099460602, 1152.9037907123566, 1165.202172756195, 1177.4255130290985, 1189.4519608020782, 1201.4788370132446, 1204.5747261047363]
[34.1425, 42.7225, 46.3675, 48.8575, 53.0025, 54.74, 56.105, 57.9, 60.16, 58.6075, 61.09, 62.7075, 63.27, 61.1625, 63.4775, 61.8625, 64.6025, 63.155, 63.635, 63.535, 64.075, 63.805, 64.91, 67.7025, 65.3, 65.325, 65.8825, 66.0875, 65.7725, 66.4, 67.2125, 69.2675, 67.845, 69.2625, 66.29, 67.3775, 69.995, 67.8775, 69.2725, 66.7375, 66.7325, 70.0625, 67.1875, 67.0625, 70.4525, 70.22, 67.7525, 67.4125, 69.85, 70.11, 68.3675, 70.515, 68.57, 70.515, 69.9325, 70.4025, 67.82, 70.9025, 67.86, 71.1, 70.845, 68.0225, 70.9725, 70.9075, 67.96, 68.375, 68.4625, 68.29, 71.5325, 69.6475, 71.23, 71.47, 71.1375, 71.1275, 69.2925, 71.6875, 71.2175, 71.545, 68.75, 71.4825, 71.735, 71.1525, 71.71, 71.4625, 68.95, 68.91, 71.675, 69.615, 69.7825, 70.4375, 72.2625, 72.395, 71.9475, 72.045, 71.8025, 69.705, 71.32, 68.5825, 70.1575, 69.205, 71.98]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.85
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.87
Round   2, Train loss: 2.303, Test loss: 2.302, Test accuracy: 10.96
Round   2, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 11.10
Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.24
Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.30
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.41
Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 11.53
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.45
Round   5, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.64
Round   6, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.68
Round   6, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 11.90
Round   7, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.86
Round   7, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.12
Round   8, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.86
Round   8, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 12.02
Round   9, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.08
Round   9, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 11.89
Round  10, Train loss: 2.301, Test loss: 2.300, Test accuracy: 12.08
Round  10, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 12.31
Round  11, Train loss: 2.300, Test loss: 2.300, Test accuracy: 12.16
Round  11, Global train loss: 2.300, Global test loss: 2.300, Global test accuracy: 12.58
Round  12, Train loss: 2.300, Test loss: 2.300, Test accuracy: 12.57
Round  12, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 13.25
Round  13, Train loss: 2.299, Test loss: 2.300, Test accuracy: 12.71
Round  13, Global train loss: 2.299, Global test loss: 2.299, Global test accuracy: 13.67
Round  14, Train loss: 2.299, Test loss: 2.299, Test accuracy: 13.29
Round  14, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 14.72
Round  15, Train loss: 2.298, Test loss: 2.299, Test accuracy: 13.68
Round  15, Global train loss: 2.298, Global test loss: 2.298, Global test accuracy: 15.10
Round  16, Train loss: 2.299, Test loss: 2.298, Test accuracy: 14.12
Round  16, Global train loss: 2.299, Global test loss: 2.298, Global test accuracy: 15.48
Round  17, Train loss: 2.298, Test loss: 2.298, Test accuracy: 14.40
Round  17, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 15.22
Round  18, Train loss: 2.298, Test loss: 2.298, Test accuracy: 14.64
Round  18, Global train loss: 2.298, Global test loss: 2.297, Global test accuracy: 16.33
Round  19, Train loss: 2.298, Test loss: 2.297, Test accuracy: 15.14
Round  19, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 16.66
Round  20, Train loss: 2.297, Test loss: 2.297, Test accuracy: 15.54
Round  20, Global train loss: 2.297, Global test loss: 2.296, Global test accuracy: 16.55
Round  21, Train loss: 2.297, Test loss: 2.296, Test accuracy: 15.33
Round  21, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 16.05
Round  22, Train loss: 2.296, Test loss: 2.296, Test accuracy: 15.22
Round  22, Global train loss: 2.296, Global test loss: 2.295, Global test accuracy: 16.04
Round  23, Train loss: 2.295, Test loss: 2.296, Test accuracy: 14.81
Round  23, Global train loss: 2.295, Global test loss: 2.294, Global test accuracy: 15.41
Round  24, Train loss: 2.295, Test loss: 2.295, Test accuracy: 14.79
Round  24, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 14.84
Round  25, Train loss: 2.295, Test loss: 2.294, Test accuracy: 15.00
Round  25, Global train loss: 2.295, Global test loss: 2.293, Global test accuracy: 14.96
Round  26, Train loss: 2.294, Test loss: 2.294, Test accuracy: 15.46
Round  26, Global train loss: 2.294, Global test loss: 2.293, Global test accuracy: 16.39
Round  27, Train loss: 2.294, Test loss: 2.294, Test accuracy: 15.62
Round  27, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 15.60
Round  28, Train loss: 2.294, Test loss: 2.293, Test accuracy: 15.54
Round  28, Global train loss: 2.294, Global test loss: 2.292, Global test accuracy: 15.62
Round  29, Train loss: 2.293, Test loss: 2.292, Test accuracy: 15.74
Round  29, Global train loss: 2.293, Global test loss: 2.292, Global test accuracy: 16.09
Round  30, Train loss: 2.293, Test loss: 2.292, Test accuracy: 16.02
Round  30, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 16.78
Round  31, Train loss: 2.293, Test loss: 2.291, Test accuracy: 16.18
Round  31, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 16.90
Round  32, Train loss: 2.292, Test loss: 2.291, Test accuracy: 16.17
Round  32, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 16.89
Round  33, Train loss: 2.292, Test loss: 2.291, Test accuracy: 16.26
Round  33, Global train loss: 2.292, Global test loss: 2.290, Global test accuracy: 16.97
Round  34, Train loss: 2.291, Test loss: 2.290, Test accuracy: 16.25
Round  34, Global train loss: 2.291, Global test loss: 2.289, Global test accuracy: 16.73
Round  35, Train loss: 2.290, Test loss: 2.289, Test accuracy: 16.45
Round  35, Global train loss: 2.290, Global test loss: 2.288, Global test accuracy: 16.30
Round  36, Train loss: 2.289, Test loss: 2.289, Test accuracy: 16.07
Round  36, Global train loss: 2.289, Global test loss: 2.287, Global test accuracy: 15.49
Round  37, Train loss: 2.289, Test loss: 2.288, Test accuracy: 15.39
Round  37, Global train loss: 2.289, Global test loss: 2.287, Global test accuracy: 14.29
Round  38, Train loss: 2.289, Test loss: 2.288, Test accuracy: 15.62
Round  38, Global train loss: 2.289, Global test loss: 2.286, Global test accuracy: 14.20
Round  39, Train loss: 2.289, Test loss: 2.287, Test accuracy: 15.55
Round  39, Global train loss: 2.289, Global test loss: 2.286, Global test accuracy: 15.30
Round  40, Train loss: 2.288, Test loss: 2.287, Test accuracy: 15.47
Round  40, Global train loss: 2.288, Global test loss: 2.285, Global test accuracy: 15.48
Round  41, Train loss: 2.287, Test loss: 2.286, Test accuracy: 15.35
Round  41, Global train loss: 2.287, Global test loss: 2.285, Global test accuracy: 15.34
Round  42, Train loss: 2.288, Test loss: 2.285, Test accuracy: 15.53
Round  42, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 15.02
Round  43, Train loss: 2.285, Test loss: 2.284, Test accuracy: 14.99
Round  43, Global train loss: 2.285, Global test loss: 2.283, Global test accuracy: 14.27
Round  44, Train loss: 2.285, Test loss: 2.283, Test accuracy: 14.73
Round  44, Global train loss: 2.285, Global test loss: 2.282, Global test accuracy: 13.23
Round  45, Train loss: 2.284, Test loss: 2.283, Test accuracy: 14.29
Round  45, Global train loss: 2.284, Global test loss: 2.282, Global test accuracy: 13.64
Round  46, Train loss: 2.284, Test loss: 2.282, Test accuracy: 14.23
Round  46, Global train loss: 2.284, Global test loss: 2.281, Global test accuracy: 13.95
Round  47, Train loss: 2.283, Test loss: 2.282, Test accuracy: 14.27
Round  47, Global train loss: 2.283, Global test loss: 2.280, Global test accuracy: 14.10
Round  48, Train loss: 2.283, Test loss: 2.281, Test accuracy: 14.44
Round  48, Global train loss: 2.283, Global test loss: 2.279, Global test accuracy: 14.73
Round  49, Train loss: 2.281, Test loss: 2.280, Test accuracy: 14.79
Round  49, Global train loss: 2.281, Global test loss: 2.279, Global test accuracy: 15.20
Round  50, Train loss: 2.282, Test loss: 2.279, Test accuracy: 15.24
Round  50, Global train loss: 2.282, Global test loss: 2.278, Global test accuracy: 14.96
Round  51, Train loss: 2.281, Test loss: 2.279, Test accuracy: 15.41
Round  51, Global train loss: 2.281, Global test loss: 2.277, Global test accuracy: 15.08
Round  52, Train loss: 2.279, Test loss: 2.278, Test accuracy: 15.54
Round  52, Global train loss: 2.279, Global test loss: 2.276, Global test accuracy: 14.97
Round  53, Train loss: 2.279, Test loss: 2.277, Test accuracy: 15.59
Round  53, Global train loss: 2.279, Global test loss: 2.276, Global test accuracy: 15.82
Round  54, Train loss: 2.279, Test loss: 2.277, Test accuracy: 15.91
Round  54, Global train loss: 2.279, Global test loss: 2.275, Global test accuracy: 16.47
Round  55, Train loss: 2.277, Test loss: 2.276, Test accuracy: 16.01
Round  55, Global train loss: 2.277, Global test loss: 2.274, Global test accuracy: 16.80
Round  56, Train loss: 2.277, Test loss: 2.275, Test accuracy: 16.22
Round  56, Global train loss: 2.277, Global test loss: 2.272, Global test accuracy: 17.07
Round  57, Train loss: 2.276, Test loss: 2.274, Test accuracy: 16.64
Round  57, Global train loss: 2.276, Global test loss: 2.272, Global test accuracy: 17.84
Round  58, Train loss: 2.276, Test loss: 2.273, Test accuracy: 17.00
Round  58, Global train loss: 2.276, Global test loss: 2.271, Global test accuracy: 18.62
Round  59, Train loss: 2.275, Test loss: 2.273, Test accuracy: 17.72
Round  59, Global train loss: 2.275, Global test loss: 2.270, Global test accuracy: 18.82
Round  60, Train loss: 2.274, Test loss: 2.272, Test accuracy: 17.94
Round  60, Global train loss: 2.274, Global test loss: 2.270, Global test accuracy: 19.09
Round  61, Train loss: 2.274, Test loss: 2.270, Test accuracy: 18.52
Round  61, Global train loss: 2.274, Global test loss: 2.269, Global test accuracy: 19.49
Round  62, Train loss: 2.274, Test loss: 2.270, Test accuracy: 18.87
Round  62, Global train loss: 2.274, Global test loss: 2.268, Global test accuracy: 19.17
Round  63, Train loss: 2.273, Test loss: 2.269, Test accuracy: 19.13
Round  63, Global train loss: 2.273, Global test loss: 2.267, Global test accuracy: 19.76
Round  64, Train loss: 2.272, Test loss: 2.268, Test accuracy: 19.11
Round  64, Global train loss: 2.272, Global test loss: 2.266, Global test accuracy: 19.62
Round  65, Train loss: 2.271, Test loss: 2.267, Test accuracy: 18.96
Round  65, Global train loss: 2.271, Global test loss: 2.265, Global test accuracy: 19.00
Round  66, Train loss: 2.270, Test loss: 2.266, Test accuracy: 19.00
Round  66, Global train loss: 2.270, Global test loss: 2.264, Global test accuracy: 19.09
Round  67, Train loss: 2.269, Test loss: 2.265, Test accuracy: 19.08
Round  67, Global train loss: 2.269, Global test loss: 2.263, Global test accuracy: 19.27
Round  68, Train loss: 2.268, Test loss: 2.264, Test accuracy: 19.25
Round  68, Global train loss: 2.268, Global test loss: 2.261, Global test accuracy: 19.34
Round  69, Train loss: 2.268, Test loss: 2.262, Test accuracy: 19.03
Round  69, Global train loss: 2.268, Global test loss: 2.259, Global test accuracy: 18.67
Round  70, Train loss: 2.267, Test loss: 2.261, Test accuracy: 18.83
Round  70, Global train loss: 2.267, Global test loss: 2.258, Global test accuracy: 18.96
Round  71, Train loss: 2.267, Test loss: 2.260, Test accuracy: 18.86
Round  71, Global train loss: 2.267, Global test loss: 2.257, Global test accuracy: 18.69
Round  72, Train loss: 2.264, Test loss: 2.258, Test accuracy: 18.77
Round  72, Global train loss: 2.264, Global test loss: 2.256, Global test accuracy: 18.89
Round  73, Train loss: 2.264, Test loss: 2.257, Test accuracy: 18.70
Round  73, Global train loss: 2.264, Global test loss: 2.254, Global test accuracy: 18.57
Round  74, Train loss: 2.265, Test loss: 2.256, Test accuracy: 18.68
Round  74, Global train loss: 2.265, Global test loss: 2.253, Global test accuracy: 18.61
Round  75, Train loss: 2.263, Test loss: 2.255, Test accuracy: 18.49
Round  75, Global train loss: 2.263, Global test loss: 2.252, Global test accuracy: 19.07
Round  76, Train loss: 2.261, Test loss: 2.254, Test accuracy: 18.38
Round  76, Global train loss: 2.261, Global test loss: 2.251, Global test accuracy: 18.82
Round  77, Train loss: 2.262, Test loss: 2.253, Test accuracy: 18.46
Round  77, Global train loss: 2.262, Global test loss: 2.249, Global test accuracy: 18.52
Round  78, Train loss: 2.261, Test loss: 2.251, Test accuracy: 18.32
Round  78, Global train loss: 2.261, Global test loss: 2.247, Global test accuracy: 18.41
Round  79, Train loss: 2.259, Test loss: 2.249, Test accuracy: 18.32
Round  79, Global train loss: 2.259, Global test loss: 2.245, Global test accuracy: 18.66
Round  80, Train loss: 2.259, Test loss: 2.247, Test accuracy: 18.11
Round  80, Global train loss: 2.259, Global test loss: 2.243, Global test accuracy: 18.46
Round  81, Train loss: 2.259, Test loss: 2.245, Test accuracy: 18.05
Round  81, Global train loss: 2.259, Global test loss: 2.242, Global test accuracy: 18.21
Round  82, Train loss: 2.258, Test loss: 2.244, Test accuracy: 17.99
Round  82, Global train loss: 2.258, Global test loss: 2.242, Global test accuracy: 18.43
Round  83, Train loss: 2.257, Test loss: 2.243, Test accuracy: 18.07
Round  83, Global train loss: 2.257, Global test loss: 2.239, Global test accuracy: 18.36
Round  84, Train loss: 2.255, Test loss: 2.241, Test accuracy: 17.94
Round  84, Global train loss: 2.255, Global test loss: 2.237, Global test accuracy: 18.23
Round  85, Train loss: 2.257, Test loss: 2.240, Test accuracy: 17.90
Round  85, Global train loss: 2.257, Global test loss: 2.235, Global test accuracy: 18.01
Round  86, Train loss: 2.256, Test loss: 2.238, Test accuracy: 17.98
Round  86, Global train loss: 2.256, Global test loss: 2.235, Global test accuracy: 18.50
Round  87, Train loss: 2.254, Test loss: 2.237, Test accuracy: 18.32
Round  87, Global train loss: 2.254, Global test loss: 2.234, Global test accuracy: 18.60
Round  88, Train loss: 2.251, Test loss: 2.236, Test accuracy: 18.44
Round  88, Global train loss: 2.251, Global test loss: 2.232, Global test accuracy: 19.12
Round  89, Train loss: 2.250, Test loss: 2.234, Test accuracy: 18.61
Round  89, Global train loss: 2.250, Global test loss: 2.230, Global test accuracy: 19.22
Round  90, Train loss: 2.251, Test loss: 2.232, Test accuracy: 18.37
Round  90, Global train loss: 2.251, Global test loss: 2.228, Global test accuracy: 18.50
Round  91, Train loss: 2.250, Test loss: 2.232, Test accuracy: 18.70
Round  91, Global train loss: 2.250, Global test loss: 2.228, Global test accuracy: 19.38
Round  92, Train loss: 2.249, Test loss: 2.230, Test accuracy: 18.93
Round  92, Global train loss: 2.249, Global test loss: 2.227, Global test accuracy: 19.56
Round  93, Train loss: 2.249, Test loss: 2.228, Test accuracy: 18.98
Round  93, Global train loss: 2.249, Global test loss: 2.225, Global test accuracy: 19.55
Round  94, Train loss: 2.252, Test loss: 2.228, Test accuracy: 18.89
Round  94, Global train loss: 2.252, Global test loss: 2.224, Global test accuracy: 18.91
Round  95, Train loss: 2.248, Test loss: 2.226, Test accuracy: 18.89
Round  95, Global train loss: 2.248, Global test loss: 2.224, Global test accuracy: 19.18
Round  96, Train loss: 2.245, Test loss: 2.225, Test accuracy: 19.00
Round  96, Global train loss: 2.245, Global test loss: 2.223, Global test accuracy: 19.28
Round  97, Train loss: 2.244, Test loss: 2.224, Test accuracy: 18.69
Round  97, Global train loss: 2.244, Global test loss: 2.220, Global test accuracy: 18.52
Round  98, Train loss: 2.247, Test loss: 2.223, Test accuracy: 18.83
Round  98, Global train loss: 2.247, Global test loss: 2.220, Global test accuracy: 18.91
Round  99, Train loss: 2.243, Test loss: 2.221, Test accuracy: 19.07
Round  99, Global train loss: 2.243, Global test loss: 2.219, Global test accuracy: 19.34
Final Round, Train loss: 2.242, Test loss: 2.217, Test accuracy: 19.57
Final Round, Global train loss: 2.242, Global test loss: 2.219, Global test accuracy: 19.34
Average accuracy final 10 rounds: 18.83775 

Average global accuracy final 10 rounds: 19.112000000000002 

5137.648092746735
[5.35005784034729, 10.493807077407837, 15.643902063369751, 20.822213172912598, 25.83489966392517, 30.92508816719055, 36.0855176448822, 41.24391722679138, 46.41503167152405, 51.42556929588318, 56.43747806549072, 61.63101553916931, 66.83354640007019, 72.05171704292297, 77.04675626754761, 81.69277381896973, 86.29963827133179, 90.9751648902893, 95.7108244895935, 100.43035054206848, 105.1145088672638, 109.73492741584778, 114.36784839630127, 119.04413104057312, 123.713782787323, 128.38146424293518, 133.01902294158936, 137.6658067703247, 142.31627678871155, 146.9740777015686, 151.56689405441284, 156.2292582988739, 160.88318824768066, 165.49359679222107, 170.11976790428162, 174.81262874603271, 179.49770069122314, 184.12843203544617, 188.77809023857117, 193.3936483860016, 198.05998826026917, 202.73607921600342, 207.4252142906189, 212.11132884025574, 216.72031044960022, 221.39212131500244, 226.08008241653442, 230.72500038146973, 235.32523894309998, 239.95102953910828, 244.57620453834534, 249.1348955631256, 253.72548961639404, 258.3384771347046, 262.9119827747345, 267.3857238292694, 271.90388536453247, 276.4571294784546, 280.98950028419495, 285.5051050186157, 289.9950535297394, 294.5054132938385, 299.55872988700867, 304.469083070755, 309.49559020996094, 314.096559047699, 318.68904876708984, 323.26527190208435, 327.85100269317627, 332.4776475429535, 337.0680742263794, 342.01759123802185, 346.92947268486023, 351.4469745159149, 356.4083762168884, 361.39099168777466, 366.3790555000305, 371.37864804267883, 376.3513741493225, 381.5136921405792, 386.4756407737732, 391.51910424232483, 396.51858949661255, 401.52203035354614, 406.7102167606354, 411.684228181839, 416.6352574825287, 421.71965193748474, 426.82268023490906, 431.90530586242676, 436.8142993450165, 441.4096450805664, 446.0176866054535, 450.5875144004822, 455.0458118915558, 459.5863428115845, 464.1497941017151, 468.63715505599976, 473.1431999206543, 477.66693687438965, 479.94522976875305]
[10.6825, 10.8475, 10.9625, 11.245, 11.4075, 11.445, 11.6775, 11.855, 11.855, 12.0775, 12.0825, 12.1625, 12.5675, 12.715, 13.29, 13.6775, 14.125, 14.4, 14.6375, 15.14, 15.545, 15.3275, 15.2175, 14.81, 14.7875, 15.0025, 15.465, 15.625, 15.5375, 15.74, 16.015, 16.175, 16.17, 16.2575, 16.25, 16.4475, 16.075, 15.3875, 15.6225, 15.5475, 15.47, 15.345, 15.5325, 14.995, 14.7325, 14.29, 14.2325, 14.265, 14.4425, 14.7875, 15.2425, 15.4125, 15.5425, 15.5925, 15.9125, 16.0075, 16.2175, 16.645, 17.0, 17.7175, 17.9425, 18.5225, 18.8725, 19.1275, 19.11, 18.96, 18.9975, 19.08, 19.245, 19.0325, 18.83, 18.8625, 18.7675, 18.695, 18.6775, 18.49, 18.38, 18.46, 18.3225, 18.315, 18.1075, 18.05, 17.99, 18.07, 17.94, 17.9025, 17.985, 18.315, 18.4375, 18.605, 18.3725, 18.705, 18.9325, 18.9825, 18.8925, 18.895, 19.0025, 18.69, 18.83, 19.075, 19.5675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.001, Test loss: 1.941, Test accuracy: 32.92
Round   0, Global train loss: 2.001, Global test loss: 1.977, Global test accuracy: 32.60
Round   1, Train loss: 1.713, Test loss: 1.768, Test accuracy: 37.77
Round   1, Global train loss: 1.713, Global test loss: 1.818, Global test accuracy: 38.31
Round   2, Train loss: 1.574, Test loss: 1.659, Test accuracy: 41.38
Round   2, Global train loss: 1.574, Global test loss: 1.752, Global test accuracy: 41.84
Round   3, Train loss: 1.469, Test loss: 1.550, Test accuracy: 44.26
Round   3, Global train loss: 1.469, Global test loss: 1.649, Global test accuracy: 44.00
Round   4, Train loss: 1.394, Test loss: 1.455, Test accuracy: 48.17
Round   4, Global train loss: 1.394, Global test loss: 1.651, Global test accuracy: 46.16
Round   5, Train loss: 1.321, Test loss: 1.395, Test accuracy: 50.72
Round   5, Global train loss: 1.321, Global test loss: 1.619, Global test accuracy: 47.75
Round   6, Train loss: 1.249, Test loss: 1.360, Test accuracy: 51.86
Round   6, Global train loss: 1.249, Global test loss: 1.654, Global test accuracy: 47.95
Round   7, Train loss: 1.214, Test loss: 1.279, Test accuracy: 54.53
Round   7, Global train loss: 1.214, Global test loss: 1.526, Global test accuracy: 50.73
Round   8, Train loss: 1.159, Test loss: 1.229, Test accuracy: 56.80
Round   8, Global train loss: 1.159, Global test loss: 1.516, Global test accuracy: 51.88
Round   9, Train loss: 1.098, Test loss: 1.200, Test accuracy: 57.66
Round   9, Global train loss: 1.098, Global test loss: 1.475, Global test accuracy: 51.30
Round  10, Train loss: 1.063, Test loss: 1.178, Test accuracy: 58.70
Round  10, Global train loss: 1.063, Global test loss: 1.568, Global test accuracy: 53.42
Round  11, Train loss: 1.051, Test loss: 1.138, Test accuracy: 60.27
Round  11, Global train loss: 1.051, Global test loss: 1.452, Global test accuracy: 53.48
Round  12, Train loss: 1.010, Test loss: 1.121, Test accuracy: 60.99
Round  12, Global train loss: 1.010, Global test loss: 1.414, Global test accuracy: 52.86
Round  13, Train loss: 0.972, Test loss: 1.105, Test accuracy: 61.68
Round  13, Global train loss: 0.972, Global test loss: 1.467, Global test accuracy: 55.69
Round  14, Train loss: 0.963, Test loss: 1.100, Test accuracy: 62.12
Round  14, Global train loss: 0.963, Global test loss: 1.436, Global test accuracy: 52.78
Round  15, Train loss: 0.921, Test loss: 1.090, Test accuracy: 62.58
Round  15, Global train loss: 0.921, Global test loss: 1.437, Global test accuracy: 55.38
Round  16, Train loss: 0.912, Test loss: 1.078, Test accuracy: 63.23
Round  16, Global train loss: 0.912, Global test loss: 1.465, Global test accuracy: 56.07
Round  17, Train loss: 0.871, Test loss: 1.084, Test accuracy: 63.30
Round  17, Global train loss: 0.871, Global test loss: 1.491, Global test accuracy: 56.40
Round  18, Train loss: 0.866, Test loss: 1.073, Test accuracy: 63.76
Round  18, Global train loss: 0.866, Global test loss: 1.354, Global test accuracy: 55.02
Round  19, Train loss: 0.850, Test loss: 1.050, Test accuracy: 64.90
Round  19, Global train loss: 0.850, Global test loss: 1.346, Global test accuracy: 55.97
Round  20, Train loss: 0.826, Test loss: 1.036, Test accuracy: 65.50
Round  20, Global train loss: 0.826, Global test loss: 1.408, Global test accuracy: 54.91
Round  21, Train loss: 0.810, Test loss: 1.035, Test accuracy: 65.72
Round  21, Global train loss: 0.810, Global test loss: 1.428, Global test accuracy: 54.94
Round  22, Train loss: 0.800, Test loss: 1.018, Test accuracy: 66.44
Round  22, Global train loss: 0.800, Global test loss: 1.373, Global test accuracy: 55.79
Round  23, Train loss: 0.770, Test loss: 1.011, Test accuracy: 66.76
Round  23, Global train loss: 0.770, Global test loss: 1.419, Global test accuracy: 54.95
Round  24, Train loss: 0.752, Test loss: 1.006, Test accuracy: 67.05
Round  24, Global train loss: 0.752, Global test loss: 1.379, Global test accuracy: 55.59
Round  25, Train loss: 0.776, Test loss: 0.996, Test accuracy: 67.36
Round  25, Global train loss: 0.776, Global test loss: 1.547, Global test accuracy: 58.62
Round  26, Train loss: 0.768, Test loss: 0.982, Test accuracy: 67.89
Round  26, Global train loss: 0.768, Global test loss: 1.385, Global test accuracy: 55.94
Round  27, Train loss: 0.725, Test loss: 0.977, Test accuracy: 68.25
Round  27, Global train loss: 0.725, Global test loss: 1.446, Global test accuracy: 56.94
Round  28, Train loss: 0.683, Test loss: 0.984, Test accuracy: 68.06
Round  28, Global train loss: 0.683, Global test loss: 1.489, Global test accuracy: 58.08
Round  29, Train loss: 0.708, Test loss: 0.972, Test accuracy: 68.39
Round  29, Global train loss: 0.708, Global test loss: 1.427, Global test accuracy: 58.78
Round  30, Train loss: 0.691, Test loss: 0.973, Test accuracy: 68.42
Round  30, Global train loss: 0.691, Global test loss: 1.365, Global test accuracy: 55.33
Round  31, Train loss: 0.722, Test loss: 0.973, Test accuracy: 68.56
Round  31, Global train loss: 0.722, Global test loss: 1.381, Global test accuracy: 55.45
Round  32, Train loss: 0.674, Test loss: 0.970, Test accuracy: 68.65
Round  32, Global train loss: 0.674, Global test loss: 1.522, Global test accuracy: 59.17
Round  33, Train loss: 0.662, Test loss: 0.975, Test accuracy: 68.69
Round  33, Global train loss: 0.662, Global test loss: 1.467, Global test accuracy: 59.18
Round  34, Train loss: 0.662, Test loss: 0.970, Test accuracy: 68.84
Round  34, Global train loss: 0.662, Global test loss: 1.505, Global test accuracy: 58.99
Round  35, Train loss: 0.635, Test loss: 0.969, Test accuracy: 69.12
Round  35, Global train loss: 0.635, Global test loss: 1.560, Global test accuracy: 59.08
Round  36, Train loss: 0.656, Test loss: 0.958, Test accuracy: 69.49
Round  36, Global train loss: 0.656, Global test loss: 1.352, Global test accuracy: 57.60
Round  37, Train loss: 0.655, Test loss: 0.953, Test accuracy: 69.86
Round  37, Global train loss: 0.655, Global test loss: 1.398, Global test accuracy: 58.00
Round  38, Train loss: 0.641, Test loss: 0.946, Test accuracy: 69.94
Round  38, Global train loss: 0.641, Global test loss: 1.354, Global test accuracy: 57.83
Round  39, Train loss: 0.622, Test loss: 0.942, Test accuracy: 70.20
Round  39, Global train loss: 0.622, Global test loss: 1.447, Global test accuracy: 59.08
Round  40, Train loss: 0.627, Test loss: 0.941, Test accuracy: 70.27
Round  40, Global train loss: 0.627, Global test loss: 1.515, Global test accuracy: 54.91
Round  41, Train loss: 0.620, Test loss: 0.945, Test accuracy: 70.35
Round  41, Global train loss: 0.620, Global test loss: 1.505, Global test accuracy: 60.08
Round  42, Train loss: 0.594, Test loss: 0.956, Test accuracy: 70.38
Round  42, Global train loss: 0.594, Global test loss: 1.380, Global test accuracy: 56.60
Round  43, Train loss: 0.584, Test loss: 0.951, Test accuracy: 70.51
Round  43, Global train loss: 0.584, Global test loss: 1.615, Global test accuracy: 58.95
Round  44, Train loss: 0.595, Test loss: 0.956, Test accuracy: 70.61
Round  44, Global train loss: 0.595, Global test loss: 1.563, Global test accuracy: 58.17
Round  45, Train loss: 0.598, Test loss: 0.944, Test accuracy: 70.72
Round  45, Global train loss: 0.598, Global test loss: 1.462, Global test accuracy: 59.50
Round  46, Train loss: 0.608, Test loss: 0.935, Test accuracy: 70.88
Round  46, Global train loss: 0.608, Global test loss: 1.616, Global test accuracy: 59.72
Round  47, Train loss: 0.577, Test loss: 0.930, Test accuracy: 71.00
Round  47, Global train loss: 0.577, Global test loss: 1.437, Global test accuracy: 59.23
Round  48, Train loss: 0.583, Test loss: 0.937, Test accuracy: 71.12
Round  48, Global train loss: 0.583, Global test loss: 1.352, Global test accuracy: 57.59
Round  49, Train loss: 0.531, Test loss: 0.938, Test accuracy: 71.03
Round  49, Global train loss: 0.531, Global test loss: 1.438, Global test accuracy: 59.25
Round  50, Train loss: 0.561, Test loss: 0.945, Test accuracy: 71.01
Round  50, Global train loss: 0.561, Global test loss: 1.676, Global test accuracy: 58.03
Round  51, Train loss: 0.552, Test loss: 0.945, Test accuracy: 71.26
Round  51, Global train loss: 0.552, Global test loss: 1.391, Global test accuracy: 57.54
Round  52, Train loss: 0.542, Test loss: 0.943, Test accuracy: 71.41
Round  52, Global train loss: 0.542, Global test loss: 1.415, Global test accuracy: 56.82
Round  53, Train loss: 0.565, Test loss: 0.937, Test accuracy: 71.64
Round  53, Global train loss: 0.565, Global test loss: 1.819, Global test accuracy: 56.43
Round  54, Train loss: 0.537, Test loss: 0.934, Test accuracy: 71.73
Round  54, Global train loss: 0.537, Global test loss: 1.468, Global test accuracy: 59.20
Round  55, Train loss: 0.565, Test loss: 0.937, Test accuracy: 71.60
Round  55, Global train loss: 0.565, Global test loss: 1.485, Global test accuracy: 59.71
Round  56, Train loss: 0.526, Test loss: 0.932, Test accuracy: 71.84
Round  56, Global train loss: 0.526, Global test loss: 1.463, Global test accuracy: 58.99
Round  57, Train loss: 0.529, Test loss: 0.937, Test accuracy: 71.59
Round  57, Global train loss: 0.529, Global test loss: 1.380, Global test accuracy: 59.24
Round  58, Train loss: 0.493, Test loss: 0.944, Test accuracy: 71.50
Round  58, Global train loss: 0.493, Global test loss: 1.666, Global test accuracy: 56.49
Round  59, Train loss: 0.531, Test loss: 0.940, Test accuracy: 71.69
Round  59, Global train loss: 0.531, Global test loss: 1.562, Global test accuracy: 59.85
Round  60, Train loss: 0.511, Test loss: 0.947, Test accuracy: 71.57
Round  60, Global train loss: 0.511, Global test loss: 1.586, Global test accuracy: 60.21
Round  61, Train loss: 0.537, Test loss: 0.948, Test accuracy: 71.61
Round  61, Global train loss: 0.537, Global test loss: 1.521, Global test accuracy: 58.47
Round  62, Train loss: 0.500, Test loss: 0.945, Test accuracy: 71.67
Round  62, Global train loss: 0.500, Global test loss: 1.587, Global test accuracy: 59.60
Round  63, Train loss: 0.515, Test loss: 0.941, Test accuracy: 71.72
Round  63, Global train loss: 0.515, Global test loss: 1.426, Global test accuracy: 60.12
Round  64, Train loss: 0.479, Test loss: 0.936, Test accuracy: 72.06
Round  64, Global train loss: 0.479, Global test loss: 1.571, Global test accuracy: 60.20
Round  65, Train loss: 0.539, Test loss: 0.936, Test accuracy: 71.99
Round  65, Global train loss: 0.539, Global test loss: 1.327, Global test accuracy: 58.34
Round  66, Train loss: 0.510, Test loss: 0.940, Test accuracy: 71.98
Round  66, Global train loss: 0.510, Global test loss: 1.420, Global test accuracy: 60.04
Round  67, Train loss: 0.492, Test loss: 0.934, Test accuracy: 72.19
Round  67, Global train loss: 0.492, Global test loss: 1.534, Global test accuracy: 56.78
Round  68, Train loss: 0.481, Test loss: 0.946, Test accuracy: 72.12
Round  68, Global train loss: 0.481, Global test loss: 1.506, Global test accuracy: 60.35
Round  69, Train loss: 0.499, Test loss: 0.955, Test accuracy: 72.10
Round  69, Global train loss: 0.499, Global test loss: 1.452, Global test accuracy: 60.18
Round  70, Train loss: 0.490, Test loss: 0.962, Test accuracy: 72.10
Round  70, Global train loss: 0.490, Global test loss: 1.444, Global test accuracy: 59.31
Round  71, Train loss: 0.507, Test loss: 0.960, Test accuracy: 72.27
Round  71, Global train loss: 0.507, Global test loss: 1.589, Global test accuracy: 57.82
Round  72, Train loss: 0.494, Test loss: 0.959, Test accuracy: 72.32
Round  72, Global train loss: 0.494, Global test loss: 1.561, Global test accuracy: 56.65
Round  73, Train loss: 0.482, Test loss: 0.963, Test accuracy: 72.19
Round  73, Global train loss: 0.482, Global test loss: 1.416, Global test accuracy: 59.50
Round  74, Train loss: 0.464, Test loss: 0.955, Test accuracy: 72.17
Round  74, Global train loss: 0.464, Global test loss: 1.646, Global test accuracy: 60.34
Round  75, Train loss: 0.483, Test loss: 0.949, Test accuracy: 72.50
Round  75, Global train loss: 0.483, Global test loss: 1.456, Global test accuracy: 59.75
Round  76, Train loss: 0.438, Test loss: 0.947, Test accuracy: 72.53
Round  76, Global train loss: 0.438, Global test loss: 1.877, Global test accuracy: 61.25
Round  77, Train loss: 0.485, Test loss: 0.973, Test accuracy: 72.23
Round  77, Global train loss: 0.485, Global test loss: 1.502, Global test accuracy: 60.65
Round  78, Train loss: 0.500, Test loss: 0.960, Test accuracy: 72.49
Round  78, Global train loss: 0.500, Global test loss: 1.562, Global test accuracy: 58.16
Round  79, Train loss: 0.443, Test loss: 0.952, Test accuracy: 72.65
Round  79, Global train loss: 0.443, Global test loss: 1.385, Global test accuracy: 58.17
Round  80, Train loss: 0.468, Test loss: 0.956, Test accuracy: 72.56
Round  80, Global train loss: 0.468, Global test loss: 1.359, Global test accuracy: 58.28
Round  81, Train loss: 0.446, Test loss: 0.951, Test accuracy: 72.81
Round  81, Global train loss: 0.446, Global test loss: 1.532, Global test accuracy: 60.01
Round  82, Train loss: 0.428, Test loss: 0.955, Test accuracy: 72.65
Round  82, Global train loss: 0.428, Global test loss: 1.597, Global test accuracy: 60.59
Round  83, Train loss: 0.429, Test loss: 0.958, Test accuracy: 72.67
Round  83, Global train loss: 0.429, Global test loss: 1.758, Global test accuracy: 60.77
Round  84, Train loss: 0.480, Test loss: 0.960, Test accuracy: 72.88
Round  84, Global train loss: 0.480, Global test loss: 1.562, Global test accuracy: 57.01
Round  85, Train loss: 0.438, Test loss: 0.943, Test accuracy: 73.03
Round  85, Global train loss: 0.438, Global test loss: 1.560, Global test accuracy: 61.35
Round  86, Train loss: 0.477, Test loss: 0.943, Test accuracy: 73.19
Round  86, Global train loss: 0.477, Global test loss: 1.549, Global test accuracy: 58.89
Round  87, Train loss: 0.434, Test loss: 0.953, Test accuracy: 73.00
Round  87, Global train loss: 0.434, Global test loss: 1.625, Global test accuracy: 60.39
Round  88, Train loss: 0.495, Test loss: 0.966, Test accuracy: 72.72
Round  88, Global train loss: 0.495, Global test loss: 1.680, Global test accuracy: 57.02
Round  89, Train loss: 0.433, Test loss: 0.966, Test accuracy: 72.82
Round  89, Global train loss: 0.433, Global test loss: 1.544, Global test accuracy: 59.44
Round  90, Train loss: 0.448, Test loss: 0.972, Test accuracy: 72.85
Round  90, Global train loss: 0.448, Global test loss: 1.713, Global test accuracy: 60.23
Round  91, Train loss: 0.435, Test loss: 0.971, Test accuracy: 72.91
Round  91, Global train loss: 0.435, Global test loss: 1.399, Global test accuracy: 58.57
Round  92, Train loss: 0.416, Test loss: 0.968, Test accuracy: 72.93
Round  92, Global train loss: 0.416, Global test loss: 1.689, Global test accuracy: 60.73
Round  93, Train loss: 0.461, Test loss: 0.982, Test accuracy: 72.72
Round  93, Global train loss: 0.461, Global test loss: 1.411, Global test accuracy: 59.07
Round  94, Train loss: 0.411, Test loss: 0.964, Test accuracy: 72.92
Round  94, Global train loss: 0.411, Global test loss: 1.417, Global test accuracy: 58.51
Round  95, Train loss: 0.420, Test loss: 0.972, Test accuracy: 72.86
Round  95, Global train loss: 0.420, Global test loss: 1.692, Global test accuracy: 59.93
Round  96, Train loss: 0.438, Test loss: 0.971, Test accuracy: 73.02
Round  96, Global train loss: 0.438, Global test loss: 1.681, Global test accuracy: 58.02
Round  97, Train loss: 0.433, Test loss: 0.970, Test accuracy: 73.07
Round  97, Global train loss: 0.433, Global test loss: 1.555, Global test accuracy: 58.48
Round  98, Train loss: 0.403, Test loss: 0.987, Test accuracy: 72.95
Round  98, Global train loss: 0.403, Global test loss: 1.597, Global test accuracy: 59.91
Round  99, Train loss: 0.399, Test loss: 0.973, Test accuracy: 72.96
Round  99, Global train loss: 0.399, Global test loss: 1.509, Global test accuracy: 60.30
Final Round, Train loss: 0.284, Test loss: 1.074, Test accuracy: 73.42
Final Round, Global train loss: 0.284, Global test loss: 1.509, Global test accuracy: 60.30
Average accuracy final 10 rounds: 72.9185 

Average global accuracy final 10 rounds: 59.3745 

5706.568246126175
[4.825835704803467, 9.651671409606934, 14.421779870986938, 19.191888332366943, 23.847059726715088, 28.502231121063232, 33.09623336791992, 37.69023561477661, 42.426175355911255, 47.1621150970459, 51.841944217681885, 56.52177333831787, 61.154932260513306, 65.78809118270874, 70.41317176818848, 75.03825235366821, 79.2219660282135, 83.40567970275879, 87.60306167602539, 91.80044364929199, 95.95873236656189, 100.11702108383179, 104.30085968971252, 108.48469829559326, 112.68401050567627, 116.88332271575928, 121.12751698493958, 125.37171125411987, 129.55978083610535, 133.74785041809082, 137.93293166160583, 142.11801290512085, 146.26001977920532, 150.4020266532898, 154.56121301651, 158.72039937973022, 162.88909459114075, 167.05778980255127, 171.2918336391449, 175.52587747573853, 179.72588181495667, 183.9258861541748, 188.13572192192078, 192.34555768966675, 196.50526356697083, 200.6649694442749, 204.84255743026733, 209.02014541625977, 213.17251896858215, 217.32489252090454, 221.4904842376709, 225.65607595443726, 229.8821861743927, 234.10829639434814, 238.30783247947693, 242.5073685646057, 246.72308540344238, 250.93880224227905, 255.14553093910217, 259.3522596359253, 263.5372908115387, 267.7223219871521, 271.9601933956146, 276.19806480407715, 280.46391773223877, 284.7297706604004, 288.8839213848114, 293.0380721092224, 297.18907475471497, 301.3400774002075, 305.4617528915405, 309.58342838287354, 313.7872705459595, 317.9911127090454, 322.16197967529297, 326.3328466415405, 330.51116967201233, 334.68949270248413, 338.84642481803894, 343.00335693359375, 347.13240456581116, 351.26145219802856, 355.4028079509735, 359.54416370391846, 363.67728424072266, 367.81040477752686, 371.968065738678, 376.1257266998291, 380.2779529094696, 384.4301791191101, 388.6088716983795, 392.7875642776489, 396.9717495441437, 401.1559348106384, 405.3107633590698, 409.4655919075012, 413.6051535606384, 417.74471521377563, 421.85942244529724, 425.97412967681885, 430.0603132247925, 434.1464967727661, 438.28559589385986, 442.4246950149536, 446.61694073677063, 450.80918645858765, 454.9852397441864, 459.16129302978516, 463.32005190849304, 467.4788107872009, 471.5450692176819, 475.61132764816284, 479.66684889793396, 483.7223701477051, 487.7330551147461, 491.7437400817871, 495.77437472343445, 499.8050093650818, 503.83790922164917, 507.87080907821655, 511.9058449268341, 515.9408807754517, 520.1979417800903, 524.455002784729, 528.5115144252777, 532.5680260658264, 536.624306678772, 540.6805872917175, 544.7996628284454, 548.9187383651733, 552.9526543617249, 556.9865703582764, 561.0347216129303, 565.0828728675842, 569.1036756038666, 573.1244783401489, 577.1529004573822, 581.1813225746155, 585.230500459671, 589.2796783447266, 593.3328413963318, 597.386004447937, 601.4391846656799, 605.4923648834229, 609.5303394794464, 613.56831407547, 617.6059722900391, 621.6436305046082, 625.6894769668579, 629.7353234291077, 633.7874910831451, 637.8396587371826, 641.893275976181, 645.9468932151794, 650.0101215839386, 654.0733499526978, 658.1354100704193, 662.1974701881409, 666.3012664318085, 670.4050626754761, 674.4808311462402, 678.5565996170044, 682.621719121933, 686.6868386268616, 690.7499406337738, 694.813042640686, 698.8826613426208, 702.9522800445557, 706.9902653694153, 711.0282506942749, 715.0815346240997, 719.1348185539246, 723.2322278022766, 727.3296370506287, 731.415501832962, 735.5013666152954, 739.5884974002838, 743.6756281852722, 747.7443783283234, 751.8131284713745, 755.8852095603943, 759.9572906494141, 763.9943611621857, 768.0314316749573, 772.0563139915466, 776.081196308136, 780.159371137619, 784.237545967102, 788.3291487693787, 792.4207515716553, 796.4937646389008, 800.5667777061462, 804.6851162910461, 808.803454875946, 812.9127614498138, 817.0220680236816, 821.1141545772552, 825.2062411308289, 829.29319190979, 833.3801426887512, 835.4364230632782, 837.4927034378052]
[32.9175, 32.9175, 37.77, 37.77, 41.375, 41.375, 44.2625, 44.2625, 48.1725, 48.1725, 50.7225, 50.7225, 51.8625, 51.8625, 54.5325, 54.5325, 56.8, 56.8, 57.66, 57.66, 58.6975, 58.6975, 60.2725, 60.2725, 60.99, 60.99, 61.68, 61.68, 62.125, 62.125, 62.575, 62.575, 63.235, 63.235, 63.2975, 63.2975, 63.7625, 63.7625, 64.8975, 64.8975, 65.4975, 65.4975, 65.715, 65.715, 66.445, 66.445, 66.76, 66.76, 67.05, 67.05, 67.3625, 67.3625, 67.885, 67.885, 68.2525, 68.2525, 68.0625, 68.0625, 68.395, 68.395, 68.425, 68.425, 68.565, 68.565, 68.6475, 68.6475, 68.69, 68.69, 68.845, 68.845, 69.12, 69.12, 69.49, 69.49, 69.855, 69.855, 69.935, 69.935, 70.2025, 70.2025, 70.2725, 70.2725, 70.3525, 70.3525, 70.3825, 70.3825, 70.51, 70.51, 70.61, 70.61, 70.725, 70.725, 70.875, 70.875, 71.0025, 71.0025, 71.1175, 71.1175, 71.0275, 71.0275, 71.01, 71.01, 71.2575, 71.2575, 71.41, 71.41, 71.635, 71.635, 71.735, 71.735, 71.6, 71.6, 71.84, 71.84, 71.595, 71.595, 71.5, 71.5, 71.69, 71.69, 71.5725, 71.5725, 71.61, 71.61, 71.675, 71.675, 71.725, 71.725, 72.0625, 72.0625, 71.9925, 71.9925, 71.985, 71.985, 72.185, 72.185, 72.1175, 72.1175, 72.1025, 72.1025, 72.1, 72.1, 72.2725, 72.2725, 72.3175, 72.3175, 72.185, 72.185, 72.165, 72.165, 72.5025, 72.5025, 72.5275, 72.5275, 72.2325, 72.2325, 72.4925, 72.4925, 72.6525, 72.6525, 72.56, 72.56, 72.805, 72.805, 72.65, 72.65, 72.6725, 72.6725, 72.88, 72.88, 73.035, 73.035, 73.185, 73.185, 73.005, 73.005, 72.7225, 72.7225, 72.8225, 72.8225, 72.8525, 72.8525, 72.9075, 72.9075, 72.93, 72.93, 72.72, 72.72, 72.915, 72.915, 72.8575, 72.8575, 73.0175, 73.0175, 73.0725, 73.0725, 72.955, 72.955, 72.9575, 72.9575, 73.4225, 73.4225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.194, Test loss: 2.076, Test accuracy: 27.27
Round   1, Train loss: 1.888, Test loss: 1.855, Test accuracy: 35.05
Round   2, Train loss: 1.696, Test loss: 1.718, Test accuracy: 40.13
Round   3, Train loss: 1.592, Test loss: 1.626, Test accuracy: 43.72
Round   4, Train loss: 1.526, Test loss: 1.527, Test accuracy: 46.98
Round   5, Train loss: 1.481, Test loss: 1.418, Test accuracy: 50.68
Round   6, Train loss: 1.395, Test loss: 1.381, Test accuracy: 52.02
Round   7, Train loss: 1.373, Test loss: 1.284, Test accuracy: 54.32
Round   8, Train loss: 1.315, Test loss: 1.231, Test accuracy: 56.66
Round   9, Train loss: 1.239, Test loss: 1.221, Test accuracy: 57.43
Round  10, Train loss: 1.239, Test loss: 1.181, Test accuracy: 58.40
Round  11, Train loss: 1.175, Test loss: 1.141, Test accuracy: 60.06
Round  12, Train loss: 1.161, Test loss: 1.094, Test accuracy: 61.99
Round  13, Train loss: 1.105, Test loss: 1.064, Test accuracy: 63.25
Round  14, Train loss: 1.103, Test loss: 1.047, Test accuracy: 63.80
Round  15, Train loss: 1.069, Test loss: 1.014, Test accuracy: 64.81
Round  16, Train loss: 1.037, Test loss: 0.993, Test accuracy: 65.97
Round  17, Train loss: 1.023, Test loss: 0.965, Test accuracy: 66.72
Round  18, Train loss: 0.985, Test loss: 0.950, Test accuracy: 67.39
Round  19, Train loss: 0.980, Test loss: 0.930, Test accuracy: 68.02
Round  20, Train loss: 0.960, Test loss: 0.917, Test accuracy: 68.15
Round  21, Train loss: 0.933, Test loss: 0.903, Test accuracy: 68.61
Round  22, Train loss: 0.895, Test loss: 0.899, Test accuracy: 68.67
Round  23, Train loss: 0.915, Test loss: 0.875, Test accuracy: 69.84
Round  24, Train loss: 0.880, Test loss: 0.870, Test accuracy: 70.05
Round  25, Train loss: 0.863, Test loss: 0.859, Test accuracy: 70.50
Round  26, Train loss: 0.845, Test loss: 0.850, Test accuracy: 70.62
Round  27, Train loss: 0.839, Test loss: 0.849, Test accuracy: 70.56
Round  28, Train loss: 0.843, Test loss: 0.840, Test accuracy: 70.97
Round  29, Train loss: 0.836, Test loss: 0.842, Test accuracy: 71.16
Round  30, Train loss: 0.812, Test loss: 0.837, Test accuracy: 71.52
Round  31, Train loss: 0.790, Test loss: 0.834, Test accuracy: 71.64
Round  32, Train loss: 0.788, Test loss: 0.831, Test accuracy: 71.72
Round  33, Train loss: 0.786, Test loss: 0.815, Test accuracy: 71.96
Round  34, Train loss: 0.752, Test loss: 0.810, Test accuracy: 72.52
Round  35, Train loss: 0.747, Test loss: 0.809, Test accuracy: 72.42
Round  36, Train loss: 0.776, Test loss: 0.799, Test accuracy: 72.85
Round  37, Train loss: 0.739, Test loss: 0.805, Test accuracy: 72.44
Round  38, Train loss: 0.755, Test loss: 0.791, Test accuracy: 73.06
Round  39, Train loss: 0.728, Test loss: 0.785, Test accuracy: 73.33
Round  40, Train loss: 0.698, Test loss: 0.785, Test accuracy: 73.55
Round  41, Train loss: 0.704, Test loss: 0.783, Test accuracy: 73.68
Round  42, Train loss: 0.700, Test loss: 0.781, Test accuracy: 73.62
Round  43, Train loss: 0.701, Test loss: 0.778, Test accuracy: 73.69
Round  44, Train loss: 0.722, Test loss: 0.764, Test accuracy: 74.13
Round  45, Train loss: 0.679, Test loss: 0.764, Test accuracy: 73.97
Round  46, Train loss: 0.667, Test loss: 0.767, Test accuracy: 74.16
Round  47, Train loss: 0.673, Test loss: 0.773, Test accuracy: 73.79
Round  48, Train loss: 0.674, Test loss: 0.765, Test accuracy: 74.20
Round  49, Train loss: 0.674, Test loss: 0.762, Test accuracy: 74.47
Round  50, Train loss: 0.667, Test loss: 0.767, Test accuracy: 74.24
Round  51, Train loss: 0.647, Test loss: 0.759, Test accuracy: 74.01
Round  52, Train loss: 0.649, Test loss: 0.772, Test accuracy: 74.33
Round  53, Train loss: 0.630, Test loss: 0.780, Test accuracy: 73.97
Round  54, Train loss: 0.662, Test loss: 0.768, Test accuracy: 74.20
Round  55, Train loss: 0.656, Test loss: 0.749, Test accuracy: 75.15
Round  56, Train loss: 0.630, Test loss: 0.754, Test accuracy: 75.02
Round  57, Train loss: 0.632, Test loss: 0.765, Test accuracy: 74.78
Round  58, Train loss: 0.619, Test loss: 0.760, Test accuracy: 74.96
Round  59, Train loss: 0.617, Test loss: 0.747, Test accuracy: 75.52
Round  60, Train loss: 0.609, Test loss: 0.738, Test accuracy: 75.47
Round  61, Train loss: 0.606, Test loss: 0.741, Test accuracy: 75.39
Round  62, Train loss: 0.607, Test loss: 0.748, Test accuracy: 75.16
Round  63, Train loss: 0.592, Test loss: 0.747, Test accuracy: 75.22
Round  64, Train loss: 0.591, Test loss: 0.751, Test accuracy: 75.03
Round  65, Train loss: 0.608, Test loss: 0.752, Test accuracy: 75.09
Round  66, Train loss: 0.577, Test loss: 0.747, Test accuracy: 75.44
Round  67, Train loss: 0.593, Test loss: 0.740, Test accuracy: 75.65
Round  68, Train loss: 0.555, Test loss: 0.737, Test accuracy: 75.88
Round  69, Train loss: 0.580, Test loss: 0.740, Test accuracy: 75.63
Round  70, Train loss: 0.589, Test loss: 0.749, Test accuracy: 75.82
Round  71, Train loss: 0.578, Test loss: 0.742, Test accuracy: 75.75
Round  72, Train loss: 0.575, Test loss: 0.751, Test accuracy: 75.68
Round  73, Train loss: 0.562, Test loss: 0.738, Test accuracy: 75.94
Round  74, Train loss: 0.559, Test loss: 0.739, Test accuracy: 76.12
Round  75, Train loss: 0.574, Test loss: 0.756, Test accuracy: 75.37
Round  76, Train loss: 0.540, Test loss: 0.746, Test accuracy: 75.59
Round  77, Train loss: 0.548, Test loss: 0.742, Test accuracy: 75.82
Round  78, Train loss: 0.574, Test loss: 0.750, Test accuracy: 75.87
Round  79, Train loss: 0.546, Test loss: 0.750, Test accuracy: 75.56
Round  80, Train loss: 0.558, Test loss: 0.755, Test accuracy: 75.89
Round  81, Train loss: 0.555, Test loss: 0.779, Test accuracy: 75.12
Round  82, Train loss: 0.567, Test loss: 0.767, Test accuracy: 75.51
Round  83, Train loss: 0.523, Test loss: 0.765, Test accuracy: 75.52
Round  84, Train loss: 0.553, Test loss: 0.753, Test accuracy: 76.13
Round  85, Train loss: 0.527, Test loss: 0.742, Test accuracy: 75.99
Round  86, Train loss: 0.521, Test loss: 0.752, Test accuracy: 76.00
Round  87, Train loss: 0.506, Test loss: 0.750, Test accuracy: 76.28
Round  88, Train loss: 0.520, Test loss: 0.747, Test accuracy: 76.30
Round  89, Train loss: 0.539, Test loss: 0.747, Test accuracy: 76.72
Round  90, Train loss: 0.531, Test loss: 0.764, Test accuracy: 75.89
Round  91, Train loss: 0.508, Test loss: 0.743, Test accuracy: 76.33
Round  92, Train loss: 0.500, Test loss: 0.752, Test accuracy: 76.12
Round  93, Train loss: 0.523, Test loss: 0.752, Test accuracy: 76.27
Round  94, Train loss: 0.515, Test loss: 0.757, Test accuracy: 76.24
Round  95, Train loss: 0.516, Test loss: 0.767, Test accuracy: 76.32
Round  96, Train loss: 0.487, Test loss: 0.766, Test accuracy: 76.20
Round  97, Train loss: 0.533, Test loss: 0.757, Test accuracy: 76.41
Round  98, Train loss: 0.496, Test loss: 0.770, Test accuracy: 76.11
Round  99, Train loss: 0.479, Test loss: 0.775, Test accuracy: 75.84
Final Round, Train loss: 0.425, Test loss: 0.761, Test accuracy: 76.31
Average accuracy final 10 rounds: 76.175 

4526.042634963989
[4.4180357456207275, 8.836071491241455, 13.108498573303223, 17.38092565536499, 21.642723083496094, 25.904520511627197, 30.191145181655884, 34.47776985168457, 38.75700759887695, 43.036245346069336, 47.325945138931274, 51.61564493179321, 55.934473752975464, 60.253302574157715, 64.55004000663757, 68.84677743911743, 73.15614128112793, 77.46550512313843, 81.80595421791077, 86.1464033126831, 90.59731769561768, 95.04823207855225, 99.3631649017334, 103.67809772491455, 107.96590852737427, 112.25371932983398, 116.53875160217285, 120.82378387451172, 125.10878539085388, 129.39378690719604, 133.673184633255, 137.95258235931396, 142.23175311088562, 146.51092386245728, 150.83644127845764, 155.161958694458, 159.47794127464294, 163.79392385482788, 168.10494208335876, 172.41596031188965, 176.74370431900024, 181.07144832611084, 185.40095138549805, 189.73045444488525, 194.05875611305237, 198.38705778121948, 202.70561289787292, 207.02416801452637, 211.32209253311157, 215.62001705169678, 219.94365978240967, 224.26730251312256, 228.59861183166504, 232.92992115020752, 237.24488282203674, 241.55984449386597, 245.89852809906006, 250.23721170425415, 254.56258463859558, 258.887957572937, 263.17050075531006, 267.4530439376831, 271.7371778488159, 276.02131175994873, 280.298287153244, 284.5752625465393, 288.83392572402954, 293.0925889015198, 297.3737258911133, 301.6548628807068, 305.933146238327, 310.21142959594727, 314.48107862472534, 318.7507276535034, 323.0189507007599, 327.28717374801636, 331.5747060775757, 335.862238407135, 340.14091777801514, 344.41959714889526, 348.2862801551819, 352.1529631614685, 356.023713350296, 359.89446353912354, 363.7954602241516, 367.6964569091797, 371.5526371002197, 375.40881729125977, 379.25552105903625, 383.10222482681274, 386.998473405838, 390.8947219848633, 394.8072543144226, 398.71978664398193, 402.6005346775055, 406.48128271102905, 410.3467299938202, 414.2121772766113, 418.0811777114868, 421.9501781463623, 425.8010332584381, 429.6518883705139, 433.5311863422394, 437.41048431396484, 441.31259846687317, 445.2147126197815, 449.0961627960205, 452.9776129722595, 456.8593125343323, 460.74101209640503, 464.6394352912903, 468.53785848617554, 472.5224883556366, 476.50711822509766, 480.3620648384094, 484.2170114517212, 488.075350522995, 491.9336895942688, 495.8029625415802, 499.6722354888916, 503.52892875671387, 507.38562202453613, 511.247154712677, 515.1086874008179, 518.9577922821045, 522.8068971633911, 526.7047455310822, 530.6025938987732, 534.4746704101562, 538.3467469215393, 542.2003939151764, 546.0540409088135, 549.9349331855774, 553.8158254623413, 557.6917226314545, 561.5676198005676, 565.4208693504333, 569.2741189002991, 573.1386337280273, 577.0031485557556, 580.8746650218964, 584.7461814880371, 588.6339256763458, 592.5216698646545, 596.4092202186584, 600.2967705726624, 604.183310508728, 608.0698504447937, 611.9402005672455, 615.8105506896973, 619.6970889568329, 623.5836272239685, 627.4860033988953, 631.388379573822, 635.2992877960205, 639.210196018219, 643.10910987854, 647.0080237388611, 650.8828003406525, 654.7575769424438, 658.630802154541, 662.5040273666382, 666.3749978542328, 670.2459683418274, 674.1234893798828, 678.0010104179382, 682.0102422237396, 686.019474029541, 689.9509828090668, 693.8824915885925, 697.7758162021637, 701.6691408157349, 705.5694358348846, 709.4697308540344, 713.3690295219421, 717.2683281898499, 721.1595542430878, 725.0507802963257, 728.9314727783203, 732.8121652603149, 736.73020362854, 740.6482419967651, 744.5976576805115, 748.5470733642578, 752.4933178424835, 756.4395623207092, 760.3781507015228, 764.3167390823364, 768.262366771698, 772.2079944610596, 776.1637766361237, 780.1195588111877, 784.0575704574585, 787.9955821037292, 791.9231882095337, 795.8507943153381, 799.7758474349976, 803.700900554657, 807.6452882289886, 811.5896759033203, 813.4154670238495, 815.2412581443787]
[27.2725, 27.2725, 35.055, 35.055, 40.135, 40.135, 43.7225, 43.7225, 46.9775, 46.9775, 50.6775, 50.6775, 52.0175, 52.0175, 54.3225, 54.3225, 56.6575, 56.6575, 57.4275, 57.4275, 58.395, 58.395, 60.06, 60.06, 61.99, 61.99, 63.2525, 63.2525, 63.8, 63.8, 64.8075, 64.8075, 65.9725, 65.9725, 66.7175, 66.7175, 67.385, 67.385, 68.02, 68.02, 68.15, 68.15, 68.615, 68.615, 68.6675, 68.6675, 69.8425, 69.8425, 70.0525, 70.0525, 70.495, 70.495, 70.625, 70.625, 70.56, 70.56, 70.975, 70.975, 71.16, 71.16, 71.5225, 71.5225, 71.645, 71.645, 71.7175, 71.7175, 71.9575, 71.9575, 72.52, 72.52, 72.4175, 72.4175, 72.8475, 72.8475, 72.4425, 72.4425, 73.0625, 73.0625, 73.33, 73.33, 73.5475, 73.5475, 73.6775, 73.6775, 73.6225, 73.6225, 73.69, 73.69, 74.1325, 74.1325, 73.9675, 73.9675, 74.155, 74.155, 73.7875, 73.7875, 74.2025, 74.2025, 74.4675, 74.4675, 74.2425, 74.2425, 74.0125, 74.0125, 74.3275, 74.3275, 73.9675, 73.9675, 74.205, 74.205, 75.1475, 75.1475, 75.0175, 75.0175, 74.785, 74.785, 74.9575, 74.9575, 75.5225, 75.5225, 75.465, 75.465, 75.3925, 75.3925, 75.16, 75.16, 75.215, 75.215, 75.0275, 75.0275, 75.0875, 75.0875, 75.4375, 75.4375, 75.6525, 75.6525, 75.8825, 75.8825, 75.6275, 75.6275, 75.8225, 75.8225, 75.755, 75.755, 75.6775, 75.6775, 75.935, 75.935, 76.1175, 76.1175, 75.3725, 75.3725, 75.5925, 75.5925, 75.82, 75.82, 75.8725, 75.8725, 75.555, 75.555, 75.895, 75.895, 75.12, 75.12, 75.51, 75.51, 75.5225, 75.5225, 76.1275, 76.1275, 75.9925, 75.9925, 76.0, 76.0, 76.2775, 76.2775, 76.3, 76.3, 76.7175, 76.7175, 75.895, 75.895, 76.33, 76.33, 76.12, 76.12, 76.2725, 76.2725, 76.24, 76.24, 76.3225, 76.3225, 76.2025, 76.2025, 76.4125, 76.4125, 76.1125, 76.1125, 75.8425, 75.8425, 76.31, 76.31]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.539, Test loss: 2.216, Test accuracy: 19.90
Round   1, Train loss: 1.055, Test loss: 1.929, Test accuracy: 33.28
Round   2, Train loss: 0.898, Test loss: 1.787, Test accuracy: 41.21
Round   3, Train loss: 0.853, Test loss: 1.314, Test accuracy: 53.76
Round   4, Train loss: 0.831, Test loss: 1.143, Test accuracy: 59.95
Round   5, Train loss: 0.726, Test loss: 1.122, Test accuracy: 61.87
Round   6, Train loss: 0.804, Test loss: 0.817, Test accuracy: 68.46
Round   7, Train loss: 0.682, Test loss: 0.791, Test accuracy: 69.78
Round   8, Train loss: 0.669, Test loss: 0.801, Test accuracy: 70.21
Round   9, Train loss: 0.706, Test loss: 0.790, Test accuracy: 71.38
Round  10, Train loss: 0.614, Test loss: 0.608, Test accuracy: 76.35
Round  11, Train loss: 0.602, Test loss: 0.601, Test accuracy: 76.36
Round  12, Train loss: 0.704, Test loss: 0.599, Test accuracy: 76.62
Round  13, Train loss: 0.631, Test loss: 0.552, Test accuracy: 78.58
Round  14, Train loss: 0.690, Test loss: 0.559, Test accuracy: 78.76
Round  15, Train loss: 0.655, Test loss: 0.545, Test accuracy: 79.12
Round  16, Train loss: 0.646, Test loss: 0.524, Test accuracy: 79.55
Round  17, Train loss: 0.620, Test loss: 0.536, Test accuracy: 79.65
Round  18, Train loss: 0.473, Test loss: 0.511, Test accuracy: 79.65
Round  19, Train loss: 0.521, Test loss: 0.513, Test accuracy: 79.82
Round  20, Train loss: 0.587, Test loss: 0.506, Test accuracy: 80.32
Round  21, Train loss: 0.514, Test loss: 0.489, Test accuracy: 80.62
Round  22, Train loss: 0.454, Test loss: 0.491, Test accuracy: 80.59
Round  23, Train loss: 0.494, Test loss: 0.488, Test accuracy: 80.64
Round  24, Train loss: 0.425, Test loss: 0.477, Test accuracy: 81.19
Round  25, Train loss: 0.497, Test loss: 0.467, Test accuracy: 81.82
Round  26, Train loss: 0.431, Test loss: 0.464, Test accuracy: 82.09
Round  27, Train loss: 0.376, Test loss: 0.467, Test accuracy: 81.45
Round  28, Train loss: 0.423, Test loss: 0.466, Test accuracy: 82.01
Round  29, Train loss: 0.393, Test loss: 0.450, Test accuracy: 82.38
Round  30, Train loss: 0.581, Test loss: 0.453, Test accuracy: 82.53
Round  31, Train loss: 0.450, Test loss: 0.446, Test accuracy: 82.87
Round  32, Train loss: 0.364, Test loss: 0.438, Test accuracy: 82.50
Round  33, Train loss: 0.439, Test loss: 0.445, Test accuracy: 82.83
Round  34, Train loss: 0.460, Test loss: 0.427, Test accuracy: 83.42
Round  35, Train loss: 0.374, Test loss: 0.429, Test accuracy: 83.22
Round  36, Train loss: 0.386, Test loss: 0.427, Test accuracy: 83.63
Round  37, Train loss: 0.396, Test loss: 0.440, Test accuracy: 83.02
Round  38, Train loss: 0.364, Test loss: 0.419, Test accuracy: 83.51
Round  39, Train loss: 0.462, Test loss: 0.422, Test accuracy: 83.58
Round  40, Train loss: 0.409, Test loss: 0.410, Test accuracy: 84.10
Round  41, Train loss: 0.389, Test loss: 0.415, Test accuracy: 83.99
Round  42, Train loss: 0.379, Test loss: 0.412, Test accuracy: 83.95
Round  43, Train loss: 0.342, Test loss: 0.407, Test accuracy: 84.06
Round  44, Train loss: 0.405, Test loss: 0.401, Test accuracy: 84.11
Round  45, Train loss: 0.370, Test loss: 0.399, Test accuracy: 84.17
Round  46, Train loss: 0.360, Test loss: 0.397, Test accuracy: 84.48
Round  47, Train loss: 0.442, Test loss: 0.395, Test accuracy: 84.43
Round  48, Train loss: 0.367, Test loss: 0.397, Test accuracy: 84.69
Round  49, Train loss: 0.425, Test loss: 0.401, Test accuracy: 84.42
Round  50, Train loss: 0.438, Test loss: 0.398, Test accuracy: 84.42
Round  51, Train loss: 0.295, Test loss: 0.394, Test accuracy: 84.46
Round  52, Train loss: 0.347, Test loss: 0.389, Test accuracy: 84.74
Round  53, Train loss: 0.371, Test loss: 0.381, Test accuracy: 84.88
Round  54, Train loss: 0.376, Test loss: 0.382, Test accuracy: 84.77
Round  55, Train loss: 0.410, Test loss: 0.385, Test accuracy: 84.70
Round  56, Train loss: 0.325, Test loss: 0.385, Test accuracy: 84.66
Round  57, Train loss: 0.343, Test loss: 0.381, Test accuracy: 84.96
Round  58, Train loss: 0.400, Test loss: 0.384, Test accuracy: 85.00
Round  59, Train loss: 0.364, Test loss: 0.386, Test accuracy: 84.80
Round  60, Train loss: 0.347, Test loss: 0.380, Test accuracy: 85.02
Round  61, Train loss: 0.302, Test loss: 0.377, Test accuracy: 85.37
Round  62, Train loss: 0.226, Test loss: 0.375, Test accuracy: 85.37
Round  63, Train loss: 0.251, Test loss: 0.370, Test accuracy: 85.59
Round  64, Train loss: 0.275, Test loss: 0.375, Test accuracy: 85.39
Round  65, Train loss: 0.308, Test loss: 0.375, Test accuracy: 85.55
Round  66, Train loss: 0.339, Test loss: 0.378, Test accuracy: 85.34
Round  67, Train loss: 0.363, Test loss: 0.378, Test accuracy: 85.08
Round  68, Train loss: 0.334, Test loss: 0.372, Test accuracy: 85.58
Round  69, Train loss: 0.330, Test loss: 0.369, Test accuracy: 85.37
Round  70, Train loss: 0.378, Test loss: 0.377, Test accuracy: 85.38
Round  71, Train loss: 0.308, Test loss: 0.366, Test accuracy: 85.77
Round  72, Train loss: 0.301, Test loss: 0.367, Test accuracy: 85.92
Round  73, Train loss: 0.345, Test loss: 0.359, Test accuracy: 85.96
Round  74, Train loss: 0.207, Test loss: 0.362, Test accuracy: 85.58
Round  75, Train loss: 0.198, Test loss: 0.367, Test accuracy: 85.56
Round  76, Train loss: 0.240, Test loss: 0.368, Test accuracy: 85.62
Round  77, Train loss: 0.314, Test loss: 0.362, Test accuracy: 85.83
Round  78, Train loss: 0.271, Test loss: 0.365, Test accuracy: 85.71
Round  79, Train loss: 0.264, Test loss: 0.360, Test accuracy: 85.72
Round  80, Train loss: 0.283, Test loss: 0.364, Test accuracy: 85.70
Round  81, Train loss: 0.330, Test loss: 0.366, Test accuracy: 86.02
Round  82, Train loss: 0.212, Test loss: 0.362, Test accuracy: 85.72
Round  83, Train loss: 0.274, Test loss: 0.360, Test accuracy: 86.04
Round  84, Train loss: 0.314, Test loss: 0.362, Test accuracy: 86.07
Round  85, Train loss: 0.209, Test loss: 0.356, Test accuracy: 86.12
Round  86, Train loss: 0.251, Test loss: 0.359, Test accuracy: 86.15
Round  87, Train loss: 0.279, Test loss: 0.363, Test accuracy: 85.98
Round  88, Train loss: 0.221, Test loss: 0.364, Test accuracy: 85.92
Round  89, Train loss: 0.223, Test loss: 0.353, Test accuracy: 85.91
Round  90, Train loss: 0.187, Test loss: 0.358, Test accuracy: 86.19
Round  91, Train loss: 0.179, Test loss: 0.356, Test accuracy: 86.18
Round  92, Train loss: 0.236, Test loss: 0.354, Test accuracy: 86.48
Round  93, Train loss: 0.177, Test loss: 0.351, Test accuracy: 86.35
Round  94, Train loss: 0.288, Test loss: 0.357, Test accuracy: 86.38
Round  95, Train loss: 0.236, Test loss: 0.357, Test accuracy: 85.97
Round  96, Train loss: 0.221, Test loss: 0.362, Test accuracy: 86.20
Round  97, Train loss: 0.303, Test loss: 0.360, Test accuracy: 86.29
Round  98, Train loss: 0.237, Test loss: 0.359, Test accuracy: 86.15
Round  99, Train loss: 0.277, Test loss: 0.363, Test accuracy: 86.00
Final Round, Train loss: 0.205, Test loss: 0.362, Test accuracy: 86.07
Average accuracy final 10 rounds: 86.22083333333333
1573.8905301094055
[2.060879945755005, 4.12175989151001, 5.761259078979492, 7.400758266448975, 8.967018604278564, 10.533278942108154, 12.0836341381073, 13.633989334106445, 15.257816791534424, 16.881644248962402, 18.43724536895752, 19.992846488952637, 21.54473638534546, 23.09662628173828, 24.713024139404297, 26.329421997070312, 27.9047269821167, 29.480031967163086, 31.03622055053711, 32.59240913391113, 34.158881187438965, 35.7253532409668, 37.36373043060303, 39.00210762023926, 40.63263916969299, 42.26317071914673, 43.90476369857788, 45.54635667800903, 47.173296213150024, 48.800235748291016, 50.437565326690674, 52.07489490509033, 53.71479797363281, 55.35470104217529, 56.99240732192993, 58.63011360168457, 60.25983476638794, 61.88955593109131, 63.516878604888916, 65.14420127868652, 66.76943588256836, 68.3946704864502, 70.03549575805664, 71.67632102966309, 73.30898261070251, 74.94164419174194, 76.58291172981262, 78.2241792678833, 79.87723636627197, 81.53029346466064, 83.1582293510437, 84.78616523742676, 86.40705227851868, 88.0279393196106, 89.6649558544159, 91.30197238922119, 92.93976306915283, 94.57755374908447, 96.06842374801636, 97.55929374694824, 99.05041027069092, 100.5415267944336, 102.0199556350708, 103.49838447570801, 104.97898006439209, 106.45957565307617, 107.92514181137085, 109.39070796966553, 110.87339544296265, 112.35608291625977, 113.83951997756958, 115.3229570388794, 116.8039915561676, 118.28502607345581, 119.7615020275116, 121.23797798156738, 122.72434186935425, 124.21070575714111, 125.69016146659851, 127.16961717605591, 128.65700769424438, 130.14439821243286, 131.6246931552887, 133.10498809814453, 134.5884735584259, 136.07195901870728, 137.55067992210388, 139.0294008255005, 140.5107159614563, 141.9920310974121, 143.46687984466553, 144.94172859191895, 146.41720342636108, 147.89267826080322, 149.3723452091217, 150.85201215744019, 152.33741426467896, 153.82281637191772, 155.30230569839478, 156.78179502487183, 158.26174902915955, 159.74170303344727, 161.22193026542664, 162.702157497406, 164.1775312423706, 165.6529049873352, 167.12855577468872, 168.60420656204224, 170.09058475494385, 171.57696294784546, 173.05941581726074, 174.54186868667603, 176.0259017944336, 177.50993490219116, 178.99350690841675, 180.47707891464233, 181.95384120941162, 183.4306035041809, 184.90139031410217, 186.37217712402344, 187.85410690307617, 189.3360366821289, 190.8179030418396, 192.2997694015503, 193.78981590270996, 195.27986240386963, 196.76385021209717, 198.2478380203247, 199.72623348236084, 201.20462894439697, 202.67553114891052, 204.14643335342407, 205.61745262145996, 207.08847188949585, 208.57820916175842, 210.067946434021, 211.55695581436157, 213.04596519470215, 214.53263974189758, 216.01931428909302, 217.50409078598022, 218.98886728286743, 220.46840810775757, 221.9479489326477, 223.40056824684143, 224.85318756103516, 226.34952759742737, 227.84586763381958, 229.33695435523987, 230.82804107666016, 232.3083426952362, 233.78864431381226, 235.27017426490784, 236.75170421600342, 238.2401750087738, 239.7286458015442, 241.1947693824768, 242.66089296340942, 244.14881563186646, 245.6367383003235, 247.11653566360474, 248.596333026886, 250.08765387535095, 251.57897472381592, 253.06420421600342, 254.54943370819092, 256.03513956069946, 257.520845413208, 258.9984495639801, 260.4760537147522, 261.9495575428009, 263.4230613708496, 264.9007155895233, 266.378369808197, 267.8604624271393, 269.34255504608154, 270.83699917793274, 272.33144330978394, 273.8180046081543, 275.30456590652466, 276.7873959541321, 278.2702260017395, 279.7444658279419, 281.2187056541443, 282.7032804489136, 284.18785524368286, 285.66851449012756, 287.14917373657227, 288.6291227340698, 290.1090717315674, 291.591739654541, 293.07440757751465, 294.56329345703125, 296.05217933654785, 297.5375180244446, 299.0228567123413, 300.4888563156128, 301.9548559188843, 303.4420940876007, 304.92933225631714, 306.9064221382141, 308.8835120201111]
[19.9, 19.9, 33.28333333333333, 33.28333333333333, 41.208333333333336, 41.208333333333336, 53.75833333333333, 53.75833333333333, 59.95, 59.95, 61.86666666666667, 61.86666666666667, 68.45833333333333, 68.45833333333333, 69.775, 69.775, 70.20833333333333, 70.20833333333333, 71.38333333333334, 71.38333333333334, 76.35, 76.35, 76.35833333333333, 76.35833333333333, 76.61666666666666, 76.61666666666666, 78.575, 78.575, 78.75833333333334, 78.75833333333334, 79.125, 79.125, 79.55, 79.55, 79.65, 79.65, 79.65, 79.65, 79.81666666666666, 79.81666666666666, 80.31666666666666, 80.31666666666666, 80.61666666666666, 80.61666666666666, 80.59166666666667, 80.59166666666667, 80.64166666666667, 80.64166666666667, 81.19166666666666, 81.19166666666666, 81.81666666666666, 81.81666666666666, 82.09166666666667, 82.09166666666667, 81.45, 81.45, 82.00833333333334, 82.00833333333334, 82.38333333333334, 82.38333333333334, 82.53333333333333, 82.53333333333333, 82.86666666666666, 82.86666666666666, 82.5, 82.5, 82.825, 82.825, 83.425, 83.425, 83.225, 83.225, 83.63333333333334, 83.63333333333334, 83.01666666666667, 83.01666666666667, 83.50833333333334, 83.50833333333334, 83.575, 83.575, 84.1, 84.1, 83.99166666666666, 83.99166666666666, 83.95, 83.95, 84.05833333333334, 84.05833333333334, 84.10833333333333, 84.10833333333333, 84.16666666666667, 84.16666666666667, 84.48333333333333, 84.48333333333333, 84.43333333333334, 84.43333333333334, 84.69166666666666, 84.69166666666666, 84.425, 84.425, 84.41666666666667, 84.41666666666667, 84.45833333333333, 84.45833333333333, 84.74166666666666, 84.74166666666666, 84.875, 84.875, 84.76666666666667, 84.76666666666667, 84.7, 84.7, 84.65833333333333, 84.65833333333333, 84.95833333333333, 84.95833333333333, 85.0, 85.0, 84.8, 84.8, 85.01666666666667, 85.01666666666667, 85.36666666666666, 85.36666666666666, 85.36666666666666, 85.36666666666666, 85.59166666666667, 85.59166666666667, 85.39166666666667, 85.39166666666667, 85.55, 85.55, 85.34166666666667, 85.34166666666667, 85.075, 85.075, 85.575, 85.575, 85.36666666666666, 85.36666666666666, 85.375, 85.375, 85.76666666666667, 85.76666666666667, 85.925, 85.925, 85.95833333333333, 85.95833333333333, 85.58333333333333, 85.58333333333333, 85.55833333333334, 85.55833333333334, 85.625, 85.625, 85.83333333333333, 85.83333333333333, 85.70833333333333, 85.70833333333333, 85.71666666666667, 85.71666666666667, 85.7, 85.7, 86.01666666666667, 86.01666666666667, 85.725, 85.725, 86.04166666666667, 86.04166666666667, 86.06666666666666, 86.06666666666666, 86.11666666666666, 86.11666666666666, 86.15, 86.15, 85.98333333333333, 85.98333333333333, 85.925, 85.925, 85.90833333333333, 85.90833333333333, 86.19166666666666, 86.19166666666666, 86.18333333333334, 86.18333333333334, 86.48333333333333, 86.48333333333333, 86.35, 86.35, 86.38333333333334, 86.38333333333334, 85.975, 85.975, 86.2, 86.2, 86.29166666666667, 86.29166666666667, 86.15, 86.15, 86.0, 86.0, 86.06666666666666, 86.06666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.157, Test loss: 2.228, Test accuracy: 16.14
Round   1, Train loss: 1.019, Test loss: 2.121, Test accuracy: 20.88
Round   2, Train loss: 0.950, Test loss: 2.291, Test accuracy: 26.05
Round   3, Train loss: 0.912, Test loss: 1.944, Test accuracy: 32.79
Round   4, Train loss: 0.811, Test loss: 2.038, Test accuracy: 38.08
Round   5, Train loss: 0.810, Test loss: 1.860, Test accuracy: 38.58
Round   6, Train loss: 0.735, Test loss: 1.971, Test accuracy: 38.33
Round   7, Train loss: 0.683, Test loss: 1.924, Test accuracy: 41.01
Round   8, Train loss: 0.712, Test loss: 2.032, Test accuracy: 37.73
Round   9, Train loss: 0.731, Test loss: 1.848, Test accuracy: 44.11
Round  10, Train loss: 0.697, Test loss: 1.789, Test accuracy: 43.76
Round  11, Train loss: 0.669, Test loss: 1.692, Test accuracy: 46.24
Round  12, Train loss: 0.597, Test loss: 1.838, Test accuracy: 42.77
Round  13, Train loss: 0.573, Test loss: 1.957, Test accuracy: 46.56
Round  14, Train loss: 0.567, Test loss: 1.798, Test accuracy: 45.92
Round  15, Train loss: 0.575, Test loss: 1.813, Test accuracy: 46.52
Round  16, Train loss: 0.584, Test loss: 1.579, Test accuracy: 51.04
Round  17, Train loss: 0.543, Test loss: 1.604, Test accuracy: 51.29
Round  18, Train loss: 0.525, Test loss: 1.683, Test accuracy: 49.80
Round  19, Train loss: 0.450, Test loss: 1.633, Test accuracy: 48.62
Round  20, Train loss: 0.579, Test loss: 1.826, Test accuracy: 47.02
Round  21, Train loss: 0.496, Test loss: 1.572, Test accuracy: 49.98
Round  22, Train loss: 0.525, Test loss: 1.666, Test accuracy: 49.97
Round  23, Train loss: 0.601, Test loss: 1.453, Test accuracy: 52.09
Round  24, Train loss: 0.464, Test loss: 1.765, Test accuracy: 48.38
Round  25, Train loss: 0.477, Test loss: 1.670, Test accuracy: 50.98
Round  26, Train loss: 0.490, Test loss: 1.717, Test accuracy: 45.27
Round  27, Train loss: 0.484, Test loss: 1.656, Test accuracy: 50.12
Round  28, Train loss: 0.494, Test loss: 1.450, Test accuracy: 54.08
Round  29, Train loss: 0.357, Test loss: 1.634, Test accuracy: 51.49
Round  30, Train loss: 0.493, Test loss: 1.634, Test accuracy: 48.44
Round  31, Train loss: 0.455, Test loss: 1.553, Test accuracy: 52.09
Round  32, Train loss: 0.446, Test loss: 1.627, Test accuracy: 52.23
Round  33, Train loss: 0.435, Test loss: 1.539, Test accuracy: 55.04
Round  34, Train loss: 0.390, Test loss: 1.838, Test accuracy: 48.48
Round  35, Train loss: 0.390, Test loss: 1.616, Test accuracy: 50.96
Round  36, Train loss: 0.374, Test loss: 1.604, Test accuracy: 51.58
Round  37, Train loss: 0.324, Test loss: 1.712, Test accuracy: 51.22
Round  38, Train loss: 0.396, Test loss: 1.859, Test accuracy: 45.78
Round  39, Train loss: 0.337, Test loss: 1.675, Test accuracy: 51.16
Round  40, Train loss: 0.328, Test loss: 1.629, Test accuracy: 53.12
Round  41, Train loss: 0.425, Test loss: 1.537, Test accuracy: 53.01
Round  42, Train loss: 0.267, Test loss: 1.738, Test accuracy: 53.77
Round  43, Train loss: 0.292, Test loss: 1.563, Test accuracy: 54.22
Round  44, Train loss: 0.361, Test loss: 1.556, Test accuracy: 54.71
Round  45, Train loss: 0.385, Test loss: 1.661, Test accuracy: 50.56
Round  46, Train loss: 0.318, Test loss: 1.645, Test accuracy: 52.92
Round  47, Train loss: 0.349, Test loss: 1.456, Test accuracy: 53.06
Round  48, Train loss: 0.287, Test loss: 1.616, Test accuracy: 52.60
Round  49, Train loss: 0.292, Test loss: 1.750, Test accuracy: 52.67
Round  50, Train loss: 0.393, Test loss: 1.545, Test accuracy: 53.81
Round  51, Train loss: 0.259, Test loss: 1.625, Test accuracy: 54.38
Round  52, Train loss: 0.341, Test loss: 1.397, Test accuracy: 55.63
Round  53, Train loss: 0.292, Test loss: 1.611, Test accuracy: 52.70
Round  54, Train loss: 0.260, Test loss: 1.737, Test accuracy: 52.21
Round  55, Train loss: 0.346, Test loss: 1.720, Test accuracy: 48.14
Round  56, Train loss: 0.280, Test loss: 1.688, Test accuracy: 54.70
Round  57, Train loss: 0.204, Test loss: 1.591, Test accuracy: 55.61
Round  58, Train loss: 0.289, Test loss: 1.553, Test accuracy: 55.61
Round  59, Train loss: 0.332, Test loss: 1.517, Test accuracy: 56.15
Round  60, Train loss: 0.331, Test loss: 1.477, Test accuracy: 55.83
Round  61, Train loss: 0.305, Test loss: 1.782, Test accuracy: 53.59
Round  62, Train loss: 0.269, Test loss: 1.513, Test accuracy: 56.76
Round  63, Train loss: 0.291, Test loss: 1.513, Test accuracy: 54.10
Round  64, Train loss: 0.367, Test loss: 1.556, Test accuracy: 53.71
Round  65, Train loss: 0.349, Test loss: 1.543, Test accuracy: 56.71
Round  66, Train loss: 0.299, Test loss: 1.521, Test accuracy: 56.35
Round  67, Train loss: 0.195, Test loss: 1.602, Test accuracy: 55.38
Round  68, Train loss: 0.312, Test loss: 1.579, Test accuracy: 55.51
Round  69, Train loss: 0.263, Test loss: 1.594, Test accuracy: 56.67
Round  70, Train loss: 0.253, Test loss: 1.624, Test accuracy: 55.86
Round  71, Train loss: 0.229, Test loss: 1.688, Test accuracy: 55.27
Round  72, Train loss: 0.250, Test loss: 1.610, Test accuracy: 56.79
Round  73, Train loss: 0.217, Test loss: 1.477, Test accuracy: 55.80
Round  74, Train loss: 0.328, Test loss: 1.411, Test accuracy: 55.43
Round  75, Train loss: 0.310, Test loss: 1.495, Test accuracy: 53.45
Round  76, Train loss: 0.239, Test loss: 1.375, Test accuracy: 57.60
Round  77, Train loss: 0.205, Test loss: 1.648, Test accuracy: 55.45
Round  78, Train loss: 0.196, Test loss: 2.138, Test accuracy: 49.83
Round  79, Train loss: 0.218, Test loss: 1.656, Test accuracy: 54.49
Round  80, Train loss: 0.251, Test loss: 1.581, Test accuracy: 53.13
Round  81, Train loss: 0.201, Test loss: 1.746, Test accuracy: 52.21
Round  82, Train loss: 0.230, Test loss: 1.543, Test accuracy: 54.76
Round  83, Train loss: 0.225, Test loss: 1.609, Test accuracy: 55.63
Round  84, Train loss: 0.262, Test loss: 1.569, Test accuracy: 52.19
Round  85, Train loss: 0.209, Test loss: 1.582, Test accuracy: 53.58
Round  86, Train loss: 0.163, Test loss: 1.669, Test accuracy: 55.29
Round  87, Train loss: 0.198, Test loss: 1.693, Test accuracy: 53.84
Round  88, Train loss: 0.210, Test loss: 1.869, Test accuracy: 54.69
Round  89, Train loss: 0.181, Test loss: 1.757, Test accuracy: 55.62
Round  90, Train loss: 0.195, Test loss: 1.554, Test accuracy: 56.24
Round  91, Train loss: 0.191, Test loss: 1.814, Test accuracy: 52.28
Round  92, Train loss: 0.227, Test loss: 1.526, Test accuracy: 55.21
Round  93, Train loss: 0.195, Test loss: 1.787, Test accuracy: 52.67
Round  94, Train loss: 0.185, Test loss: 1.725, Test accuracy: 54.61
Round  95, Train loss: 0.183, Test loss: 1.883, Test accuracy: 53.62
Round  96, Train loss: 0.186, Test loss: 1.616, Test accuracy: 55.79
Round  97, Train loss: 0.145, Test loss: 1.709, Test accuracy: 55.36
Round  98, Train loss: 0.175, Test loss: 2.051, Test accuracy: 54.66
Round  99, Train loss: 0.178, Test loss: 1.657, Test accuracy: 55.48
Final Round, Train loss: 0.161, Test loss: 1.498, Test accuracy: 57.85
Average accuracy final 10 rounds: 54.593333333333334
2509.228276014328
[4.0011560916900635, 7.670114755630493, 11.308424711227417, 14.960899114608765, 18.634748935699463, 22.293681621551514, 25.969006776809692, 29.61899971961975, 33.279107332229614, 36.95049023628235, 40.61091375350952, 44.29832911491394, 47.92753529548645, 51.64459466934204, 55.35795044898987, 59.03905510902405, 62.63808846473694, 66.28865098953247, 69.93231582641602, 73.58568906784058, 77.26124882698059, 80.96487593650818, 84.65621447563171, 88.30779671669006, 91.98019337654114, 95.66319108009338, 99.3260178565979, 102.99354195594788, 106.67264342308044, 110.34271192550659, 114.0104432106018, 117.6562728881836, 121.33868384361267, 125.00826573371887, 128.67052102088928, 132.3266806602478, 135.97969460487366, 139.658611536026, 143.34803175926208, 147.0112545490265, 150.68174576759338, 154.33759689331055, 158.00786185264587, 161.67813730239868, 165.35965061187744, 169.042733669281, 172.71638655662537, 176.40009236335754, 180.07420301437378, 183.75826787948608, 187.43795156478882, 191.09127163887024, 194.76626896858215, 198.44830799102783, 202.12983512878418, 205.8064181804657, 209.4998083114624, 213.1862437725067, 216.8306097984314, 220.504576921463, 224.1757984161377, 227.8613338470459, 231.54203939437866, 235.21908473968506, 238.90052390098572, 242.55034351348877, 246.233633518219, 249.90749740600586, 253.58582425117493, 257.26862931251526, 260.8840672969818, 264.54388189315796, 268.175626039505, 271.81102991104126, 275.4448347091675, 279.06637716293335, 282.6943998336792, 286.33537912368774, 289.93482851982117, 293.54740858078003, 297.2000012397766, 300.87189531326294, 304.50098180770874, 307.8042619228363, 311.13325548171997, 314.4598729610443, 317.7628929615021, 321.067978143692, 324.39247250556946, 327.7120294570923, 331.0315761566162, 334.333943605423, 337.6512734889984, 340.95452404022217, 344.2578089237213, 347.55071806907654, 350.865300655365, 354.15363121032715, 357.41639375686646, 360.732759475708, 363.48825883865356]
[16.141666666666666, 20.875, 26.05, 32.791666666666664, 38.083333333333336, 38.575, 38.333333333333336, 41.00833333333333, 37.725, 44.108333333333334, 43.75833333333333, 46.24166666666667, 42.766666666666666, 46.55833333333333, 45.916666666666664, 46.516666666666666, 51.041666666666664, 51.291666666666664, 49.8, 48.625, 47.016666666666666, 49.975, 49.96666666666667, 52.09166666666667, 48.38333333333333, 50.983333333333334, 45.266666666666666, 50.125, 54.075, 51.49166666666667, 48.44166666666667, 52.09166666666667, 52.225, 55.041666666666664, 48.475, 50.958333333333336, 51.575, 51.21666666666667, 45.78333333333333, 51.15833333333333, 53.125, 53.00833333333333, 53.766666666666666, 54.21666666666667, 54.708333333333336, 50.55833333333333, 52.925, 53.05833333333333, 52.6, 52.675, 53.80833333333333, 54.38333333333333, 55.63333333333333, 52.7, 52.208333333333336, 48.141666666666666, 54.7, 55.608333333333334, 55.608333333333334, 56.15, 55.825, 53.59166666666667, 56.75833333333333, 54.1, 53.708333333333336, 56.708333333333336, 56.35, 55.375, 55.50833333333333, 56.666666666666664, 55.858333333333334, 55.266666666666666, 56.791666666666664, 55.8, 55.43333333333333, 53.45, 57.6, 55.45, 49.833333333333336, 54.49166666666667, 53.13333333333333, 52.208333333333336, 54.75833333333333, 55.63333333333333, 52.19166666666667, 53.583333333333336, 55.291666666666664, 53.84166666666667, 54.69166666666667, 55.625, 56.24166666666667, 52.28333333333333, 55.208333333333336, 52.675, 54.608333333333334, 53.625, 55.791666666666664, 55.358333333333334, 54.65833333333333, 55.483333333333334, 57.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.298, Test loss: 2.309, Test accuracy: 10.18
Round   0, Global train loss: 2.298, Global test loss: 2.312, Global test accuracy: 10.03
Round   1, Train loss: 2.255, Test loss: 2.307, Test accuracy: 10.23
Round   1, Global train loss: 2.255, Global test loss: 2.311, Global test accuracy: 10.09
Round   2, Train loss: 2.269, Test loss: 2.306, Test accuracy: 10.42
Round   2, Global train loss: 2.269, Global test loss: 2.310, Global test accuracy: 10.05
Round   3, Train loss: 2.342, Test loss: 2.309, Test accuracy: 10.16
Round   3, Global train loss: 2.342, Global test loss: 2.310, Global test accuracy: 10.03
Round   4, Train loss: 2.287, Test loss: 2.309, Test accuracy: 9.78
Round   4, Global train loss: 2.287, Global test loss: 2.312, Global test accuracy: 10.05
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 7.42
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 8.91
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 9.12
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 10.11
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 10.13
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Average accuracy final 10 rounds: 11.666666666666663 

Average global accuracy final 10 rounds: 11.666666666666663 

1631.7066843509674
[1.7852528095245361, 3.3474044799804688, 4.911845684051514, 6.494028568267822, 8.067657709121704, 9.638411521911621, 11.209439516067505, 12.785120248794556, 14.36620283126831, 15.932541847229004, 17.522602796554565, 19.084097862243652, 20.665894985198975, 22.22229290008545, 23.77766180038452, 25.335734844207764, 26.9049551486969, 28.47723126411438, 30.047314405441284, 31.66037940979004, 33.159693241119385, 34.72682571411133, 36.28242206573486, 37.83950972557068, 39.40118741989136, 41.0009548664093, 42.56444692611694, 44.13628387451172, 45.70247936248779, 47.25393557548523, 48.815375089645386, 50.3762993812561, 51.92791533470154, 53.486608266830444, 54.9228150844574, 56.344156980514526, 57.78142762184143, 59.207093238830566, 60.71977758407593, 62.28240394592285, 63.85965085029602, 65.44440817832947, 67.01595687866211, 68.58069181442261, 70.14442896842957, 71.71567940711975, 73.28218340873718, 74.8477840423584, 76.42167019844055, 77.98858380317688, 79.56535172462463, 81.14741921424866, 82.71765470504761, 84.29659795761108, 85.86703085899353, 87.4358115196228, 89.00763988494873, 90.57566404342651, 92.14966821670532, 93.71332383155823, 95.28543853759766, 96.86050176620483, 98.43792414665222, 100.00335693359375, 101.56967997550964, 103.1404800415039, 104.75067019462585, 106.33581709861755, 107.92417240142822, 109.50989055633545, 111.09256935119629, 112.67287826538086, 114.25910091400146, 115.834153175354, 117.42299151420593, 119.0145902633667, 120.59725689888, 122.18656373023987, 123.78121089935303, 125.3795759677887, 126.95783567428589, 128.53916358947754, 130.12613940238953, 131.7227578163147, 133.31320190429688, 134.8985936641693, 136.40478253364563, 137.91062331199646, 139.47191858291626, 141.0462987422943, 142.6293535232544, 144.2188720703125, 145.80661487579346, 147.3930356502533, 148.9815855026245, 150.56125807762146, 152.15869855880737, 153.72691464424133, 155.2907223701477, 156.86301469802856, 159.4752254486084]
[10.183333333333334, 10.233333333333333, 10.416666666666666, 10.158333333333333, 9.775, 7.416666666666667, 8.908333333333333, 9.125, 10.108333333333333, 10.133333333333333, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.997, Test loss: 1.983, Test accuracy: 30.00
Round   0, Global train loss: 1.997, Global test loss: 2.068, Global test accuracy: 27.99
Round   1, Train loss: 1.740, Test loss: 1.859, Test accuracy: 34.88
Round   1, Global train loss: 1.740, Global test loss: 2.049, Global test accuracy: 31.39
Round   2, Train loss: 1.613, Test loss: 1.731, Test accuracy: 39.41
Round   2, Global train loss: 1.613, Global test loss: 1.936, Global test accuracy: 35.28
Round   3, Train loss: 1.508, Test loss: 1.633, Test accuracy: 42.25
Round   3, Global train loss: 1.508, Global test loss: 1.867, Global test accuracy: 36.84
Round   4, Train loss: 1.417, Test loss: 1.500, Test accuracy: 47.06
Round   4, Global train loss: 1.417, Global test loss: 1.803, Global test accuracy: 38.99
Round   5, Train loss: 1.354, Test loss: 1.441, Test accuracy: 48.35
Round   5, Global train loss: 1.354, Global test loss: 1.806, Global test accuracy: 39.64
Round   6, Train loss: 1.293, Test loss: 1.398, Test accuracy: 50.73
Round   6, Global train loss: 1.293, Global test loss: 1.799, Global test accuracy: 40.41
Round   7, Train loss: 1.253, Test loss: 1.315, Test accuracy: 53.15
Round   7, Global train loss: 1.253, Global test loss: 1.802, Global test accuracy: 40.22
Round   8, Train loss: 1.207, Test loss: 1.279, Test accuracy: 54.63
Round   8, Global train loss: 1.207, Global test loss: 1.777, Global test accuracy: 41.15
Round   9, Train loss: 1.157, Test loss: 1.235, Test accuracy: 56.60
Round   9, Global train loss: 1.157, Global test loss: 1.788, Global test accuracy: 41.02
Round  10, Train loss: 1.117, Test loss: 1.216, Test accuracy: 57.28
Round  10, Global train loss: 1.117, Global test loss: 1.749, Global test accuracy: 41.90
Round  11, Train loss: 1.098, Test loss: 1.184, Test accuracy: 58.67
Round  11, Global train loss: 1.098, Global test loss: 1.801, Global test accuracy: 41.00
Round  12, Train loss: 1.058, Test loss: 1.157, Test accuracy: 59.66
Round  12, Global train loss: 1.058, Global test loss: 1.820, Global test accuracy: 42.48
Round  13, Train loss: 1.025, Test loss: 1.140, Test accuracy: 60.17
Round  13, Global train loss: 1.025, Global test loss: 1.726, Global test accuracy: 43.64
Round  14, Train loss: 1.007, Test loss: 1.121, Test accuracy: 60.99
Round  14, Global train loss: 1.007, Global test loss: 1.741, Global test accuracy: 42.56
Round  15, Train loss: 0.979, Test loss: 1.118, Test accuracy: 61.19
Round  15, Global train loss: 0.979, Global test loss: 1.810, Global test accuracy: 43.17
Round  16, Train loss: 0.951, Test loss: 1.103, Test accuracy: 61.88
Round  16, Global train loss: 0.951, Global test loss: 1.785, Global test accuracy: 43.64
Round  17, Train loss: 0.941, Test loss: 1.093, Test accuracy: 62.56
Round  17, Global train loss: 0.941, Global test loss: 1.747, Global test accuracy: 42.93
Round  18, Train loss: 0.919, Test loss: 1.082, Test accuracy: 63.10
Round  18, Global train loss: 0.919, Global test loss: 1.922, Global test accuracy: 43.12
Round  19, Train loss: 0.932, Test loss: 1.067, Test accuracy: 63.89
Round  19, Global train loss: 0.932, Global test loss: 1.817, Global test accuracy: 45.65
Round  20, Train loss: 0.881, Test loss: 1.062, Test accuracy: 64.42
Round  20, Global train loss: 0.881, Global test loss: 1.815, Global test accuracy: 43.63
Round  21, Train loss: 0.861, Test loss: 1.053, Test accuracy: 64.91
Round  21, Global train loss: 0.861, Global test loss: 1.924, Global test accuracy: 43.96
Round  22, Train loss: 0.863, Test loss: 1.035, Test accuracy: 65.55
Round  22, Global train loss: 0.863, Global test loss: 1.744, Global test accuracy: 44.54
Round  23, Train loss: 0.826, Test loss: 1.036, Test accuracy: 65.80
Round  23, Global train loss: 0.826, Global test loss: 1.792, Global test accuracy: 44.05
Round  24, Train loss: 0.815, Test loss: 1.027, Test accuracy: 66.01
Round  24, Global train loss: 0.815, Global test loss: 1.831, Global test accuracy: 46.02
Round  25, Train loss: 0.803, Test loss: 1.023, Test accuracy: 66.07
Round  25, Global train loss: 0.803, Global test loss: 1.786, Global test accuracy: 43.35
Round  26, Train loss: 0.811, Test loss: 1.008, Test accuracy: 66.77
Round  26, Global train loss: 0.811, Global test loss: 1.910, Global test accuracy: 44.08
Round  27, Train loss: 0.775, Test loss: 1.004, Test accuracy: 66.96
Round  27, Global train loss: 0.775, Global test loss: 1.890, Global test accuracy: 44.97
Round  28, Train loss: 0.758, Test loss: 1.000, Test accuracy: 67.05
Round  28, Global train loss: 0.758, Global test loss: 1.794, Global test accuracy: 44.02
Round  29, Train loss: 0.776, Test loss: 0.978, Test accuracy: 67.71
Round  29, Global train loss: 0.776, Global test loss: 1.905, Global test accuracy: 44.83
Round  30, Train loss: 0.756, Test loss: 0.987, Test accuracy: 67.69
Round  30, Global train loss: 0.756, Global test loss: 1.702, Global test accuracy: 45.51
Round  31, Train loss: 0.733, Test loss: 0.987, Test accuracy: 67.75
Round  31, Global train loss: 0.733, Global test loss: 1.904, Global test accuracy: 46.84
Round  32, Train loss: 0.711, Test loss: 0.975, Test accuracy: 68.34
Round  32, Global train loss: 0.711, Global test loss: 1.820, Global test accuracy: 44.96
Round  33, Train loss: 0.704, Test loss: 0.967, Test accuracy: 68.71
Round  33, Global train loss: 0.704, Global test loss: 2.128, Global test accuracy: 45.33
Round  34, Train loss: 0.738, Test loss: 0.974, Test accuracy: 68.92
Round  34, Global train loss: 0.738, Global test loss: 1.790, Global test accuracy: 44.59
Round  35, Train loss: 0.685, Test loss: 0.968, Test accuracy: 69.13
Round  35, Global train loss: 0.685, Global test loss: 1.771, Global test accuracy: 45.57
Round  36, Train loss: 0.684, Test loss: 0.957, Test accuracy: 69.47
Round  36, Global train loss: 0.684, Global test loss: 1.942, Global test accuracy: 46.62
Round  37, Train loss: 0.671, Test loss: 0.959, Test accuracy: 69.39
Round  37, Global train loss: 0.671, Global test loss: 1.858, Global test accuracy: 45.25
Round  38, Train loss: 0.714, Test loss: 0.948, Test accuracy: 69.59
Round  38, Global train loss: 0.714, Global test loss: 1.761, Global test accuracy: 44.60
Round  39, Train loss: 0.661, Test loss: 0.939, Test accuracy: 70.10
Round  39, Global train loss: 0.661, Global test loss: 1.762, Global test accuracy: 45.57
Round  40, Train loss: 0.669, Test loss: 0.945, Test accuracy: 70.09
Round  40, Global train loss: 0.669, Global test loss: 1.759, Global test accuracy: 44.47
Round  41, Train loss: 0.633, Test loss: 0.946, Test accuracy: 70.40
Round  41, Global train loss: 0.633, Global test loss: 2.063, Global test accuracy: 47.09
Round  42, Train loss: 0.649, Test loss: 0.948, Test accuracy: 70.32
Round  42, Global train loss: 0.649, Global test loss: 1.824, Global test accuracy: 45.94
Round  43, Train loss: 0.619, Test loss: 0.956, Test accuracy: 70.25
Round  43, Global train loss: 0.619, Global test loss: 2.201, Global test accuracy: 45.19
Round  44, Train loss: 0.647, Test loss: 0.950, Test accuracy: 70.30
Round  44, Global train loss: 0.647, Global test loss: 1.977, Global test accuracy: 47.00
Round  45, Train loss: 0.620, Test loss: 0.947, Test accuracy: 70.53
Round  45, Global train loss: 0.620, Global test loss: 1.965, Global test accuracy: 46.22
Round  46, Train loss: 0.652, Test loss: 0.944, Test accuracy: 70.73
Round  46, Global train loss: 0.652, Global test loss: 1.876, Global test accuracy: 45.13
Round  47, Train loss: 0.634, Test loss: 0.956, Test accuracy: 70.67
Round  47, Global train loss: 0.634, Global test loss: 2.014, Global test accuracy: 46.38
Round  48, Train loss: 0.623, Test loss: 0.956, Test accuracy: 70.90
Round  48, Global train loss: 0.623, Global test loss: 1.851, Global test accuracy: 44.93
Round  49, Train loss: 0.602, Test loss: 0.949, Test accuracy: 71.22
Round  49, Global train loss: 0.602, Global test loss: 2.044, Global test accuracy: 46.77
Round  50, Train loss: 0.569, Test loss: 0.967, Test accuracy: 70.97
Round  50, Global train loss: 0.569, Global test loss: 2.084, Global test accuracy: 46.90
Round  51, Train loss: 0.620, Test loss: 0.967, Test accuracy: 70.92
Round  51, Global train loss: 0.620, Global test loss: 1.832, Global test accuracy: 46.35
Round  52, Train loss: 0.574, Test loss: 0.963, Test accuracy: 70.87
Round  52, Global train loss: 0.574, Global test loss: 1.973, Global test accuracy: 45.78
Round  53, Train loss: 0.581, Test loss: 0.961, Test accuracy: 70.83
Round  53, Global train loss: 0.581, Global test loss: 2.039, Global test accuracy: 46.79
Round  54, Train loss: 0.590, Test loss: 0.960, Test accuracy: 70.93
Round  54, Global train loss: 0.590, Global test loss: 1.821, Global test accuracy: 45.44
Round  55, Train loss: 0.582, Test loss: 0.960, Test accuracy: 71.04
Round  55, Global train loss: 0.582, Global test loss: 1.967, Global test accuracy: 45.82
Round  56, Train loss: 0.552, Test loss: 0.967, Test accuracy: 71.04
Round  56, Global train loss: 0.552, Global test loss: 1.901, Global test accuracy: 45.62
Round  57, Train loss: 0.542, Test loss: 0.977, Test accuracy: 71.07
Round  57, Global train loss: 0.542, Global test loss: 2.070, Global test accuracy: 45.17
Round  58, Train loss: 0.555, Test loss: 0.974, Test accuracy: 71.21
Round  58, Global train loss: 0.555, Global test loss: 1.919, Global test accuracy: 45.68
Round  59, Train loss: 0.563, Test loss: 0.972, Test accuracy: 71.30
Round  59, Global train loss: 0.563, Global test loss: 1.898, Global test accuracy: 46.72
Round  60, Train loss: 0.538, Test loss: 0.976, Test accuracy: 71.31
Round  60, Global train loss: 0.538, Global test loss: 1.973, Global test accuracy: 46.78
Round  61, Train loss: 0.558, Test loss: 0.962, Test accuracy: 71.45
Round  61, Global train loss: 0.558, Global test loss: 1.806, Global test accuracy: 47.42
Round  62, Train loss: 0.552, Test loss: 0.961, Test accuracy: 71.44
Round  62, Global train loss: 0.552, Global test loss: 2.042, Global test accuracy: 47.06
Round  63, Train loss: 0.535, Test loss: 0.963, Test accuracy: 71.74
Round  63, Global train loss: 0.535, Global test loss: 1.915, Global test accuracy: 45.41
Round  64, Train loss: 0.516, Test loss: 0.960, Test accuracy: 71.78
Round  64, Global train loss: 0.516, Global test loss: 1.874, Global test accuracy: 46.02
Round  65, Train loss: 0.555, Test loss: 0.950, Test accuracy: 72.24
Round  65, Global train loss: 0.555, Global test loss: 1.987, Global test accuracy: 47.34
Round  66, Train loss: 0.520, Test loss: 0.951, Test accuracy: 72.22
Round  66, Global train loss: 0.520, Global test loss: 1.853, Global test accuracy: 44.84
Round  67, Train loss: 0.561, Test loss: 0.964, Test accuracy: 71.91
Round  67, Global train loss: 0.561, Global test loss: 2.000, Global test accuracy: 44.26
Round  68, Train loss: 0.513, Test loss: 0.957, Test accuracy: 72.19
Round  68, Global train loss: 0.513, Global test loss: 1.944, Global test accuracy: 45.70
Round  69, Train loss: 0.508, Test loss: 0.961, Test accuracy: 72.24
Round  69, Global train loss: 0.508, Global test loss: 1.815, Global test accuracy: 46.12
Round  70, Train loss: 0.501, Test loss: 0.952, Test accuracy: 72.49
Round  70, Global train loss: 0.501, Global test loss: 2.050, Global test accuracy: 46.96
Round  71, Train loss: 0.530, Test loss: 0.946, Test accuracy: 72.49
Round  71, Global train loss: 0.530, Global test loss: 1.856, Global test accuracy: 46.03
Round  72, Train loss: 0.547, Test loss: 0.947, Test accuracy: 72.64
Round  72, Global train loss: 0.547, Global test loss: 2.046, Global test accuracy: 46.10
Round  73, Train loss: 0.503, Test loss: 0.952, Test accuracy: 72.58
Round  73, Global train loss: 0.503, Global test loss: 1.942, Global test accuracy: 45.53
Round  74, Train loss: 0.518, Test loss: 0.961, Test accuracy: 72.41
Round  74, Global train loss: 0.518, Global test loss: 1.825, Global test accuracy: 45.73
Round  75, Train loss: 0.500, Test loss: 0.963, Test accuracy: 72.46
Round  75, Global train loss: 0.500, Global test loss: 2.133, Global test accuracy: 45.01
Round  76, Train loss: 0.488, Test loss: 0.968, Test accuracy: 72.47
Round  76, Global train loss: 0.488, Global test loss: 1.966, Global test accuracy: 47.30
Round  77, Train loss: 0.520, Test loss: 0.972, Test accuracy: 72.39
Round  77, Global train loss: 0.520, Global test loss: 2.256, Global test accuracy: 47.56
Round  78, Train loss: 0.501, Test loss: 0.969, Test accuracy: 72.56
Round  78, Global train loss: 0.501, Global test loss: 1.964, Global test accuracy: 45.40
Round  79, Train loss: 0.488, Test loss: 0.966, Test accuracy: 72.69
Round  79, Global train loss: 0.488, Global test loss: 1.949, Global test accuracy: 46.66
Round  80, Train loss: 0.510, Test loss: 0.971, Test accuracy: 72.74
Round  80, Global train loss: 0.510, Global test loss: 1.843, Global test accuracy: 45.80
Round  81, Train loss: 0.513, Test loss: 0.966, Test accuracy: 72.61
Round  81, Global train loss: 0.513, Global test loss: 1.997, Global test accuracy: 45.28
Round  82, Train loss: 0.487, Test loss: 0.976, Test accuracy: 72.36
Round  82, Global train loss: 0.487, Global test loss: 2.105, Global test accuracy: 45.57
Round  83, Train loss: 0.500, Test loss: 0.968, Test accuracy: 72.39
Round  83, Global train loss: 0.500, Global test loss: 1.938, Global test accuracy: 47.62
Round  84, Train loss: 0.492, Test loss: 0.984, Test accuracy: 72.34
Round  84, Global train loss: 0.492, Global test loss: 2.027, Global test accuracy: 46.07
Round  85, Train loss: 0.501, Test loss: 0.981, Test accuracy: 72.43
Round  85, Global train loss: 0.501, Global test loss: 1.833, Global test accuracy: 46.23
Round  86, Train loss: 0.463, Test loss: 0.981, Test accuracy: 72.50
Round  86, Global train loss: 0.463, Global test loss: 2.044, Global test accuracy: 47.06
Round  87, Train loss: 0.500, Test loss: 0.980, Test accuracy: 72.60
Round  87, Global train loss: 0.500, Global test loss: 2.014, Global test accuracy: 46.47
Round  88, Train loss: 0.485, Test loss: 0.974, Test accuracy: 72.66
Round  88, Global train loss: 0.485, Global test loss: 1.892, Global test accuracy: 45.05
Round  89, Train loss: 0.474, Test loss: 0.977, Test accuracy: 72.78
Round  89, Global train loss: 0.474, Global test loss: 1.851, Global test accuracy: 44.88
Round  90, Train loss: 0.462, Test loss: 0.971, Test accuracy: 72.90
Round  90, Global train loss: 0.462, Global test loss: 1.857, Global test accuracy: 44.69
Round  91, Train loss: 0.459, Test loss: 0.970, Test accuracy: 72.81
Round  91, Global train loss: 0.459, Global test loss: 2.044, Global test accuracy: 45.56
Round  92, Train loss: 0.468, Test loss: 0.966, Test accuracy: 72.69
Round  92, Global train loss: 0.468, Global test loss: 2.191, Global test accuracy: 46.35
Round  93, Train loss: 0.457, Test loss: 0.959, Test accuracy: 73.02
Round  93, Global train loss: 0.457, Global test loss: 1.995, Global test accuracy: 48.04
Round  94, Train loss: 0.461, Test loss: 0.950, Test accuracy: 73.25
Round  94, Global train loss: 0.461, Global test loss: 1.969, Global test accuracy: 45.52
Round  95, Train loss: 0.450, Test loss: 0.964, Test accuracy: 73.09
Round  95, Global train loss: 0.450, Global test loss: 2.211, Global test accuracy: 45.72
Round  96, Train loss: 0.472, Test loss: 0.969, Test accuracy: 73.11
Round  96, Global train loss: 0.472, Global test loss: 1.804, Global test accuracy: 46.89
Round  97, Train loss: 0.447, Test loss: 0.964, Test accuracy: 73.27
Round  97, Global train loss: 0.447, Global test loss: 2.174, Global test accuracy: 45.52
Round  98, Train loss: 0.438, Test loss: 0.970, Test accuracy: 73.09
Round  98, Global train loss: 0.438, Global test loss: 2.000, Global test accuracy: 46.16
Round  99, Train loss: 0.458, Test loss: 0.969, Test accuracy: 73.13
Round  99, Global train loss: 0.458, Global test loss: 2.129, Global test accuracy: 47.08
Final Round, Train loss: 0.308, Test loss: 1.062, Test accuracy: 73.76
Final Round, Global train loss: 0.308, Global test loss: 2.129, Global test accuracy: 47.08
Average accuracy final 10 rounds: 73.03500000000001 

Average global accuracy final 10 rounds: 46.15325 

5911.66491985321
[4.7990593910217285, 9.598118782043457, 14.324026107788086, 19.049933433532715, 23.78969120979309, 28.529448986053467, 33.271244287490845, 38.01303958892822, 42.73107624053955, 47.44911289215088, 52.205077171325684, 56.96104145050049, 61.6937997341156, 66.42655801773071, 71.16073393821716, 75.89490985870361, 80.62879967689514, 85.36268949508667, 90.07808876037598, 94.79348802566528, 99.51810002326965, 104.24271202087402, 108.97316408157349, 113.70361614227295, 118.44665455818176, 123.18969297409058, 127.91675519943237, 132.64381742477417, 137.3350396156311, 142.02626180648804, 146.68548393249512, 151.3447060585022, 155.97582578659058, 160.60694551467896, 165.2290587425232, 169.85117197036743, 174.46852827072144, 179.08588457107544, 183.52642130851746, 187.96695804595947, 192.42824125289917, 196.88952445983887, 201.33643412590027, 205.78334379196167, 210.2556345462799, 214.72792530059814, 219.3880455493927, 224.04816579818726, 228.73981428146362, 233.43146276474, 238.10826897621155, 242.7850751876831, 246.86518335342407, 250.94529151916504, 255.04699635505676, 259.1487011909485, 263.24694442749023, 267.345187664032, 271.42592215538025, 275.5066566467285, 279.56870913505554, 283.63076162338257, 287.70715498924255, 291.78354835510254, 295.85418248176575, 299.92481660842896, 304.03680634498596, 308.14879608154297, 312.23859214782715, 316.3283882141113, 320.4177715778351, 324.50715494155884, 328.5985143184662, 332.68987369537354, 336.7870891094208, 340.884304523468, 345.31962609291077, 349.7549476623535, 354.22712326049805, 358.6992988586426, 363.15164017677307, 367.60398149490356, 372.23849844932556, 376.87301540374756, 381.49257016181946, 386.11212491989136, 390.7405459880829, 395.3689670562744, 400.0108964443207, 404.65282583236694, 409.29162406921387, 413.9304223060608, 418.5305230617523, 423.13062381744385, 427.75769209861755, 432.38476037979126, 436.4523329734802, 440.5199055671692, 444.58112812042236, 448.64235067367554, 452.69747281074524, 456.75259494781494, 460.80240058898926, 464.8522062301636, 468.9038965702057, 472.9555869102478, 477.02294182777405, 481.0902967453003, 485.1495270729065, 489.2087574005127, 493.2559463977814, 497.30313539505005, 501.35804438591003, 505.41295337677, 509.4600992202759, 513.5072450637817, 517.5713381767273, 521.6354312896729, 525.697943687439, 529.7604560852051, 533.8188314437866, 537.8772068023682, 541.9342865943909, 545.9913663864136, 550.0508718490601, 554.1103773117065, 558.157163143158, 562.2039489746094, 566.2404246330261, 570.2769002914429, 575.0407721996307, 579.8046441078186, 584.5494775772095, 589.2943110466003, 594.0692362785339, 598.8441615104675, 603.6303582191467, 608.4165549278259, 613.2208197116852, 618.0250844955444, 622.8272340297699, 627.6293835639954, 632.4442071914673, 637.2590308189392, 642.0363006591797, 646.8135704994202, 651.6258158683777, 656.4380612373352, 661.269540309906, 666.1010193824768, 670.9133863449097, 675.7257533073425, 680.5444777011871, 685.3632020950317, 690.2682464122772, 695.1732907295227, 700.0042824745178, 704.8352742195129, 709.6050980091095, 714.374921798706, 719.2178373336792, 724.0607528686523, 728.9005556106567, 733.7403583526611, 738.541844367981, 743.3433303833008, 748.1951763629913, 753.0470223426819, 757.8857657909393, 762.7245092391968, 767.5759162902832, 772.4273233413696, 777.2817761898041, 782.1362290382385, 786.9785017967224, 791.8207745552063, 796.6565055847168, 801.4922366142273, 806.5399074554443, 811.5875782966614, 816.647668838501, 821.7077593803406, 826.7715504169464, 831.8353414535522, 836.8861129283905, 841.9368844032288, 846.9904398918152, 852.0439953804016, 857.1108264923096, 862.1776576042175, 867.1772856712341, 872.1769137382507, 877.192754983902, 882.2085962295532, 887.2414429187775, 892.2742896080017, 897.3121898174286, 902.3500900268555, 907.4006812572479, 912.4512724876404, 914.9777345657349, 917.5041966438293]
[30.0, 30.0, 34.875, 34.875, 39.41, 39.41, 42.2475, 42.2475, 47.06, 47.06, 48.3475, 48.3475, 50.735, 50.735, 53.15, 53.15, 54.63, 54.63, 56.6025, 56.6025, 57.2775, 57.2775, 58.6675, 58.6675, 59.665, 59.665, 60.175, 60.175, 60.9925, 60.9925, 61.1925, 61.1925, 61.875, 61.875, 62.5575, 62.5575, 63.1025, 63.1025, 63.8925, 63.8925, 64.425, 64.425, 64.9075, 64.9075, 65.545, 65.545, 65.795, 65.795, 66.0075, 66.0075, 66.07, 66.07, 66.77, 66.77, 66.9575, 66.9575, 67.0525, 67.0525, 67.71, 67.71, 67.695, 67.695, 67.7475, 67.7475, 68.34, 68.34, 68.7125, 68.7125, 68.915, 68.915, 69.1325, 69.1325, 69.47, 69.47, 69.395, 69.395, 69.595, 69.595, 70.1, 70.1, 70.095, 70.095, 70.4, 70.4, 70.32, 70.32, 70.255, 70.255, 70.295, 70.295, 70.5325, 70.5325, 70.735, 70.735, 70.6725, 70.6725, 70.8975, 70.8975, 71.2175, 71.2175, 70.975, 70.975, 70.9175, 70.9175, 70.8725, 70.8725, 70.835, 70.835, 70.9325, 70.9325, 71.0375, 71.0375, 71.0375, 71.0375, 71.0675, 71.0675, 71.2125, 71.2125, 71.3025, 71.3025, 71.3125, 71.3125, 71.45, 71.45, 71.445, 71.445, 71.74, 71.74, 71.785, 71.785, 72.2375, 72.2375, 72.2175, 72.2175, 71.9075, 71.9075, 72.1875, 72.1875, 72.2375, 72.2375, 72.4875, 72.4875, 72.49, 72.49, 72.6425, 72.6425, 72.585, 72.585, 72.4125, 72.4125, 72.4575, 72.4575, 72.475, 72.475, 72.395, 72.395, 72.5625, 72.5625, 72.69, 72.69, 72.7375, 72.7375, 72.61, 72.61, 72.3575, 72.3575, 72.3875, 72.3875, 72.345, 72.345, 72.4325, 72.4325, 72.5025, 72.5025, 72.6025, 72.6025, 72.6575, 72.6575, 72.775, 72.775, 72.9025, 72.9025, 72.8075, 72.8075, 72.69, 72.69, 73.02, 73.02, 73.2475, 73.2475, 73.0875, 73.0875, 73.1075, 73.1075, 73.2675, 73.2675, 73.09, 73.09, 73.13, 73.13, 73.76, 73.76]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.466, Test loss: 2.267, Test accuracy: 24.52
Round   1, Train loss: 0.986, Test loss: 1.881, Test accuracy: 30.00
Round   2, Train loss: 0.889, Test loss: 1.654, Test accuracy: 43.70
Round   3, Train loss: 0.815, Test loss: 1.388, Test accuracy: 49.40
Round   4, Train loss: 0.822, Test loss: 1.130, Test accuracy: 53.77
Round   5, Train loss: 0.780, Test loss: 1.143, Test accuracy: 54.17
Round   6, Train loss: 0.723, Test loss: 1.091, Test accuracy: 59.26
Round   7, Train loss: 0.748, Test loss: 0.935, Test accuracy: 64.68
Round   8, Train loss: 0.740, Test loss: 0.936, Test accuracy: 63.77
Round   9, Train loss: 0.630, Test loss: 0.990, Test accuracy: 65.15
Round  10, Train loss: 0.682, Test loss: 0.836, Test accuracy: 67.60
Round  11, Train loss: 0.645, Test loss: 0.917, Test accuracy: 66.37
Round  12, Train loss: 0.517, Test loss: 0.872, Test accuracy: 69.52
Round  13, Train loss: 0.757, Test loss: 0.701, Test accuracy: 71.12
Round  14, Train loss: 0.632, Test loss: 0.600, Test accuracy: 73.44
Round  15, Train loss: 0.652, Test loss: 0.571, Test accuracy: 75.19
Round  16, Train loss: 0.568, Test loss: 0.568, Test accuracy: 75.63
Round  17, Train loss: 0.564, Test loss: 0.564, Test accuracy: 75.57
Round  18, Train loss: 0.516, Test loss: 0.542, Test accuracy: 76.53
Round  19, Train loss: 0.602, Test loss: 0.546, Test accuracy: 76.26
Round  20, Train loss: 0.561, Test loss: 0.539, Test accuracy: 77.09
Round  21, Train loss: 0.597, Test loss: 0.530, Test accuracy: 77.74
Round  22, Train loss: 0.522, Test loss: 0.524, Test accuracy: 77.16
Round  23, Train loss: 0.493, Test loss: 0.518, Test accuracy: 77.78
Round  24, Train loss: 0.583, Test loss: 0.510, Test accuracy: 78.35
Round  25, Train loss: 0.502, Test loss: 0.512, Test accuracy: 78.44
Round  26, Train loss: 0.515, Test loss: 0.500, Test accuracy: 78.78
Round  27, Train loss: 0.534, Test loss: 0.493, Test accuracy: 79.03
Round  28, Train loss: 0.493, Test loss: 0.489, Test accuracy: 79.29
Round  29, Train loss: 0.509, Test loss: 0.499, Test accuracy: 79.01
Round  30, Train loss: 0.550, Test loss: 0.492, Test accuracy: 79.47
Round  31, Train loss: 0.461, Test loss: 0.480, Test accuracy: 79.86
Round  32, Train loss: 0.537, Test loss: 0.483, Test accuracy: 80.00
Round  33, Train loss: 0.553, Test loss: 0.480, Test accuracy: 80.01
Round  34, Train loss: 0.464, Test loss: 0.475, Test accuracy: 80.49
Round  35, Train loss: 0.422, Test loss: 0.471, Test accuracy: 80.66
Round  36, Train loss: 0.480, Test loss: 0.469, Test accuracy: 80.28
Round  37, Train loss: 0.448, Test loss: 0.458, Test accuracy: 80.95
Round  38, Train loss: 0.408, Test loss: 0.450, Test accuracy: 81.56
Round  39, Train loss: 0.536, Test loss: 0.455, Test accuracy: 81.23
Round  40, Train loss: 0.443, Test loss: 0.443, Test accuracy: 81.80
Round  41, Train loss: 0.477, Test loss: 0.436, Test accuracy: 82.21
Round  42, Train loss: 0.506, Test loss: 0.440, Test accuracy: 81.88
Round  43, Train loss: 0.384, Test loss: 0.436, Test accuracy: 82.22
Round  44, Train loss: 0.452, Test loss: 0.450, Test accuracy: 81.62
Round  45, Train loss: 0.456, Test loss: 0.454, Test accuracy: 81.31
Round  46, Train loss: 0.423, Test loss: 0.446, Test accuracy: 81.75
Round  47, Train loss: 0.398, Test loss: 0.447, Test accuracy: 81.81
Round  48, Train loss: 0.466, Test loss: 0.439, Test accuracy: 82.26
Round  49, Train loss: 0.375, Test loss: 0.434, Test accuracy: 82.22
Round  50, Train loss: 0.400, Test loss: 0.436, Test accuracy: 82.30
Round  51, Train loss: 0.384, Test loss: 0.429, Test accuracy: 82.51
Round  52, Train loss: 0.393, Test loss: 0.433, Test accuracy: 82.46
Round  53, Train loss: 0.424, Test loss: 0.434, Test accuracy: 82.53
Round  54, Train loss: 0.392, Test loss: 0.419, Test accuracy: 82.94
Round  55, Train loss: 0.360, Test loss: 0.423, Test accuracy: 82.53
Round  56, Train loss: 0.364, Test loss: 0.424, Test accuracy: 82.62
Round  57, Train loss: 0.321, Test loss: 0.425, Test accuracy: 82.91
Round  58, Train loss: 0.351, Test loss: 0.419, Test accuracy: 83.01
Round  59, Train loss: 0.377, Test loss: 0.415, Test accuracy: 83.28
Round  60, Train loss: 0.340, Test loss: 0.428, Test accuracy: 82.59
Round  61, Train loss: 0.371, Test loss: 0.417, Test accuracy: 83.03
Round  62, Train loss: 0.364, Test loss: 0.420, Test accuracy: 83.12
Round  63, Train loss: 0.361, Test loss: 0.425, Test accuracy: 83.03
Round  64, Train loss: 0.404, Test loss: 0.429, Test accuracy: 83.05
Round  65, Train loss: 0.289, Test loss: 0.419, Test accuracy: 83.18
Round  66, Train loss: 0.388, Test loss: 0.422, Test accuracy: 83.14
Round  67, Train loss: 0.340, Test loss: 0.412, Test accuracy: 83.71
Round  68, Train loss: 0.305, Test loss: 0.406, Test accuracy: 84.01
Round  69, Train loss: 0.306, Test loss: 0.410, Test accuracy: 83.95
Round  70, Train loss: 0.272, Test loss: 0.415, Test accuracy: 83.71
Round  71, Train loss: 0.317, Test loss: 0.412, Test accuracy: 83.56
Round  72, Train loss: 0.336, Test loss: 0.408, Test accuracy: 83.98
Round  73, Train loss: 0.325, Test loss: 0.404, Test accuracy: 83.75
Round  74, Train loss: 0.382, Test loss: 0.410, Test accuracy: 83.92
Round  75, Train loss: 0.328, Test loss: 0.404, Test accuracy: 84.05
Round  76, Train loss: 0.342, Test loss: 0.399, Test accuracy: 84.29
Round  77, Train loss: 0.275, Test loss: 0.405, Test accuracy: 84.09
Round  78, Train loss: 0.281, Test loss: 0.409, Test accuracy: 84.01
Round  79, Train loss: 0.343, Test loss: 0.402, Test accuracy: 84.22
Round  80, Train loss: 0.290, Test loss: 0.404, Test accuracy: 84.23
Round  81, Train loss: 0.301, Test loss: 0.407, Test accuracy: 84.38
Round  82, Train loss: 0.250, Test loss: 0.405, Test accuracy: 84.41
Round  83, Train loss: 0.252, Test loss: 0.410, Test accuracy: 84.43
Round  84, Train loss: 0.279, Test loss: 0.407, Test accuracy: 84.27
Round  85, Train loss: 0.285, Test loss: 0.406, Test accuracy: 84.33
Round  86, Train loss: 0.273, Test loss: 0.403, Test accuracy: 84.42
Round  87, Train loss: 0.243, Test loss: 0.400, Test accuracy: 84.31
Round  88, Train loss: 0.297, Test loss: 0.396, Test accuracy: 84.58
Round  89, Train loss: 0.300, Test loss: 0.404, Test accuracy: 84.40
Round  90, Train loss: 0.240, Test loss: 0.410, Test accuracy: 84.25
Round  91, Train loss: 0.265, Test loss: 0.404, Test accuracy: 84.32
Round  92, Train loss: 0.259, Test loss: 0.404, Test accuracy: 84.63
Round  93, Train loss: 0.256, Test loss: 0.404, Test accuracy: 84.67
Round  94, Train loss: 0.278, Test loss: 0.409, Test accuracy: 84.28
Round  95, Train loss: 0.265, Test loss: 0.401, Test accuracy: 84.66
Round  96, Train loss: 0.199, Test loss: 0.411, Test accuracy: 84.53
Round  97, Train loss: 0.299, Test loss: 0.411, Test accuracy: 84.33
Round  98, Train loss: 0.231, Test loss: 0.407, Test accuracy: 84.58
Round  99, Train loss: 0.262, Test loss: 0.398, Test accuracy: 84.61
Final Round, Train loss: 0.222, Test loss: 0.399, Test accuracy: 84.85
Average accuracy final 10 rounds: 84.48583333333335 

1501.2477810382843
[1.6606669425964355, 3.321333885192871, 4.666755437850952, 6.012176990509033, 7.362633943557739, 8.713090896606445, 10.06833815574646, 11.423585414886475, 12.782314777374268, 14.14104413986206, 15.502716779708862, 16.864389419555664, 18.23780059814453, 19.6112117767334, 20.98229718208313, 22.35338258743286, 23.72563123703003, 25.097879886627197, 26.460022687911987, 27.822165489196777, 29.199509143829346, 30.576852798461914, 31.946157455444336, 33.31546211242676, 34.681061029434204, 36.04665994644165, 37.412060499191284, 38.77746105194092, 40.148643016815186, 41.51982498168945, 42.88757848739624, 44.25533199310303, 45.62043046951294, 46.98552894592285, 48.34339714050293, 49.70126533508301, 51.10881018638611, 52.51635503768921, 53.87635350227356, 55.23635196685791, 56.600486755371094, 57.96462154388428, 59.331236124038696, 60.697850704193115, 62.08012318611145, 63.462395668029785, 64.83477258682251, 66.20714950561523, 67.5729022026062, 68.93865489959717, 70.30210518836975, 71.66555547714233, 73.03199338912964, 74.39843130111694, 75.77561140060425, 77.15279150009155, 78.52197527885437, 79.89115905761719, 81.25154185295105, 82.61192464828491, 83.9820909500122, 85.3522572517395, 86.72116684913635, 88.0900764465332, 89.45791721343994, 90.82575798034668, 92.18936967849731, 93.55298137664795, 94.92848229408264, 96.30398321151733, 97.68340611457825, 99.06282901763916, 100.43327617645264, 101.80372333526611, 103.17051935195923, 104.53731536865234, 105.91036868095398, 107.28342199325562, 108.65978765487671, 110.0361533164978, 111.41062307357788, 112.78509283065796, 114.14597916603088, 115.50686550140381, 116.87077212333679, 118.23467874526978, 119.61049699783325, 120.98631525039673, 122.36179566383362, 123.73727607727051, 125.10218620300293, 126.46709632873535, 127.83335494995117, 129.199613571167, 130.58717608451843, 131.97473859786987, 133.34750080108643, 134.72026300430298, 136.08556699752808, 137.45087099075317, 138.82932448387146, 140.20777797698975, 141.58032894134521, 142.95287990570068, 144.34896540641785, 145.745050907135, 147.1182518005371, 148.4914526939392, 149.8610692024231, 151.23068571090698, 152.6151306629181, 153.9995756149292, 155.37244963645935, 156.7453236579895, 158.12019872665405, 159.4950737953186, 160.85974383354187, 162.22441387176514, 163.59703063964844, 164.96964740753174, 166.33384037017822, 167.6980333328247, 169.0602786540985, 170.42252397537231, 171.79970288276672, 173.17688179016113, 174.54657769203186, 175.9162735939026, 177.28598165512085, 178.6556897163391, 180.02124452590942, 181.38679933547974, 182.75588130950928, 184.12496328353882, 185.4973611831665, 186.8697590827942, 188.2390706539154, 189.60838222503662, 190.97976541519165, 192.35114860534668, 193.72717952728271, 195.10321044921875, 196.46933817863464, 197.83546590805054, 199.24091267585754, 200.64635944366455, 202.02000498771667, 203.3936505317688, 204.76796627044678, 206.14228200912476, 207.5113844871521, 208.88048696517944, 210.26806259155273, 211.65563821792603, 213.02990126609802, 214.40416431427002, 215.77342224121094, 217.14268016815186, 218.50783848762512, 219.8729968070984, 221.26681518554688, 222.66063356399536, 224.04164052009583, 225.4226474761963, 226.79201817512512, 228.16138887405396, 229.52647829055786, 230.89156770706177, 232.2632462978363, 233.63492488861084, 235.0095763206482, 236.38422775268555, 237.75003504753113, 239.1158423423767, 240.4750566482544, 241.83427095413208, 243.1916651725769, 244.54905939102173, 245.93107318878174, 247.31308698654175, 248.70396208763123, 250.0948371887207, 251.46389293670654, 252.83294868469238, 254.19542860984802, 255.55790853500366, 256.9333727359772, 258.3088369369507, 259.68874979019165, 261.0686626434326, 262.42756175994873, 263.78646087646484, 265.1377775669098, 266.48909425735474, 267.8486487865448, 269.20820331573486, 270.5588321685791, 271.90946102142334, 273.2624337673187, 274.6154065132141, 276.6968719959259, 278.7783374786377]
[24.516666666666666, 24.516666666666666, 30.0, 30.0, 43.7, 43.7, 49.4, 49.4, 53.775, 53.775, 54.166666666666664, 54.166666666666664, 59.25833333333333, 59.25833333333333, 64.68333333333334, 64.68333333333334, 63.775, 63.775, 65.15, 65.15, 67.6, 67.6, 66.36666666666666, 66.36666666666666, 69.51666666666667, 69.51666666666667, 71.11666666666666, 71.11666666666666, 73.44166666666666, 73.44166666666666, 75.19166666666666, 75.19166666666666, 75.63333333333334, 75.63333333333334, 75.56666666666666, 75.56666666666666, 76.53333333333333, 76.53333333333333, 76.25833333333334, 76.25833333333334, 77.09166666666667, 77.09166666666667, 77.74166666666666, 77.74166666666666, 77.15833333333333, 77.15833333333333, 77.775, 77.775, 78.35, 78.35, 78.44166666666666, 78.44166666666666, 78.775, 78.775, 79.025, 79.025, 79.29166666666667, 79.29166666666667, 79.00833333333334, 79.00833333333334, 79.475, 79.475, 79.85833333333333, 79.85833333333333, 80.0, 80.0, 80.00833333333334, 80.00833333333334, 80.49166666666666, 80.49166666666666, 80.65833333333333, 80.65833333333333, 80.28333333333333, 80.28333333333333, 80.95, 80.95, 81.55833333333334, 81.55833333333334, 81.23333333333333, 81.23333333333333, 81.8, 81.8, 82.20833333333333, 82.20833333333333, 81.88333333333334, 81.88333333333334, 82.225, 82.225, 81.625, 81.625, 81.30833333333334, 81.30833333333334, 81.75, 81.75, 81.80833333333334, 81.80833333333334, 82.25833333333334, 82.25833333333334, 82.21666666666667, 82.21666666666667, 82.3, 82.3, 82.50833333333334, 82.50833333333334, 82.45833333333333, 82.45833333333333, 82.525, 82.525, 82.94166666666666, 82.94166666666666, 82.53333333333333, 82.53333333333333, 82.61666666666666, 82.61666666666666, 82.90833333333333, 82.90833333333333, 83.00833333333334, 83.00833333333334, 83.28333333333333, 83.28333333333333, 82.59166666666667, 82.59166666666667, 83.025, 83.025, 83.125, 83.125, 83.025, 83.025, 83.05, 83.05, 83.18333333333334, 83.18333333333334, 83.14166666666667, 83.14166666666667, 83.70833333333333, 83.70833333333333, 84.00833333333334, 84.00833333333334, 83.95, 83.95, 83.70833333333333, 83.70833333333333, 83.55833333333334, 83.55833333333334, 83.98333333333333, 83.98333333333333, 83.75, 83.75, 83.925, 83.925, 84.05, 84.05, 84.29166666666667, 84.29166666666667, 84.09166666666667, 84.09166666666667, 84.00833333333334, 84.00833333333334, 84.21666666666667, 84.21666666666667, 84.23333333333333, 84.23333333333333, 84.375, 84.375, 84.40833333333333, 84.40833333333333, 84.43333333333334, 84.43333333333334, 84.26666666666667, 84.26666666666667, 84.33333333333333, 84.33333333333333, 84.41666666666667, 84.41666666666667, 84.30833333333334, 84.30833333333334, 84.575, 84.575, 84.4, 84.4, 84.25, 84.25, 84.31666666666666, 84.31666666666666, 84.63333333333334, 84.63333333333334, 84.675, 84.675, 84.275, 84.275, 84.65833333333333, 84.65833333333333, 84.525, 84.525, 84.33333333333333, 84.33333333333333, 84.58333333333333, 84.58333333333333, 84.60833333333333, 84.60833333333333, 84.85, 84.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.507, Test loss: 2.080, Test accuracy: 22.10
Round   1, Train loss: 1.072, Test loss: 1.903, Test accuracy: 33.48
Round   2, Train loss: 0.940, Test loss: 1.442, Test accuracy: 43.48
Round   3, Train loss: 0.864, Test loss: 1.187, Test accuracy: 54.35
Round   4, Train loss: 0.770, Test loss: 1.191, Test accuracy: 54.42
Round   5, Train loss: 0.827, Test loss: 0.990, Test accuracy: 60.45
Round   6, Train loss: 0.732, Test loss: 0.956, Test accuracy: 65.89
Round   7, Train loss: 0.698, Test loss: 0.848, Test accuracy: 67.90
Round   8, Train loss: 0.724, Test loss: 0.792, Test accuracy: 69.95
Round   9, Train loss: 0.701, Test loss: 0.634, Test accuracy: 74.16
Round  10, Train loss: 0.656, Test loss: 0.614, Test accuracy: 74.49
Round  11, Train loss: 0.635, Test loss: 0.615, Test accuracy: 74.80
Round  12, Train loss: 0.664, Test loss: 0.589, Test accuracy: 75.76
Round  13, Train loss: 0.658, Test loss: 0.578, Test accuracy: 76.65
Round  14, Train loss: 0.630, Test loss: 0.560, Test accuracy: 77.09
Round  15, Train loss: 0.564, Test loss: 0.574, Test accuracy: 77.21
Round  16, Train loss: 0.560, Test loss: 0.550, Test accuracy: 77.38
Round  17, Train loss: 0.607, Test loss: 0.543, Test accuracy: 77.58
Round  18, Train loss: 0.575, Test loss: 0.540, Test accuracy: 78.17
Round  19, Train loss: 0.580, Test loss: 0.533, Test accuracy: 78.47
Round  20, Train loss: 0.625, Test loss: 0.517, Test accuracy: 79.33
Round  21, Train loss: 0.517, Test loss: 0.515, Test accuracy: 79.01
Round  22, Train loss: 0.509, Test loss: 0.513, Test accuracy: 78.84
Round  23, Train loss: 0.601, Test loss: 0.500, Test accuracy: 79.21
Round  24, Train loss: 0.534, Test loss: 0.491, Test accuracy: 79.96
Round  25, Train loss: 0.494, Test loss: 0.486, Test accuracy: 80.08
Round  26, Train loss: 0.482, Test loss: 0.484, Test accuracy: 79.92
Round  27, Train loss: 0.521, Test loss: 0.488, Test accuracy: 80.28
Round  28, Train loss: 0.495, Test loss: 0.475, Test accuracy: 80.69
Round  29, Train loss: 0.479, Test loss: 0.469, Test accuracy: 80.71
Round  30, Train loss: 0.456, Test loss: 0.472, Test accuracy: 80.90
Round  31, Train loss: 0.531, Test loss: 0.463, Test accuracy: 81.19
Round  32, Train loss: 0.390, Test loss: 0.455, Test accuracy: 81.33
Round  33, Train loss: 0.389, Test loss: 0.459, Test accuracy: 80.87
Round  34, Train loss: 0.453, Test loss: 0.460, Test accuracy: 81.17
Round  35, Train loss: 0.445, Test loss: 0.459, Test accuracy: 81.66
Round  36, Train loss: 0.460, Test loss: 0.459, Test accuracy: 81.47
Round  37, Train loss: 0.438, Test loss: 0.453, Test accuracy: 81.28
Round  38, Train loss: 0.504, Test loss: 0.442, Test accuracy: 82.56
Round  39, Train loss: 0.494, Test loss: 0.429, Test accuracy: 82.75
Round  40, Train loss: 0.453, Test loss: 0.429, Test accuracy: 83.03
Round  41, Train loss: 0.446, Test loss: 0.432, Test accuracy: 82.80
Round  42, Train loss: 0.380, Test loss: 0.422, Test accuracy: 82.80
Round  43, Train loss: 0.470, Test loss: 0.430, Test accuracy: 82.50
Round  44, Train loss: 0.347, Test loss: 0.426, Test accuracy: 82.88
Round  45, Train loss: 0.450, Test loss: 0.417, Test accuracy: 83.13
Round  46, Train loss: 0.354, Test loss: 0.419, Test accuracy: 83.32
Round  47, Train loss: 0.409, Test loss: 0.418, Test accuracy: 83.08
Round  48, Train loss: 0.441, Test loss: 0.410, Test accuracy: 83.54
Round  49, Train loss: 0.395, Test loss: 0.416, Test accuracy: 83.37
Round  50, Train loss: 0.318, Test loss: 0.412, Test accuracy: 83.67
Round  51, Train loss: 0.390, Test loss: 0.406, Test accuracy: 84.15
Round  52, Train loss: 0.362, Test loss: 0.408, Test accuracy: 83.53
Round  53, Train loss: 0.380, Test loss: 0.407, Test accuracy: 84.21
Round  54, Train loss: 0.427, Test loss: 0.414, Test accuracy: 83.75
Round  55, Train loss: 0.401, Test loss: 0.406, Test accuracy: 84.31
Round  56, Train loss: 0.391, Test loss: 0.407, Test accuracy: 83.86
Round  57, Train loss: 0.368, Test loss: 0.400, Test accuracy: 84.02
Round  58, Train loss: 0.327, Test loss: 0.396, Test accuracy: 83.91
Round  59, Train loss: 0.322, Test loss: 0.403, Test accuracy: 83.92
Round  60, Train loss: 0.347, Test loss: 0.398, Test accuracy: 84.25
Round  61, Train loss: 0.349, Test loss: 0.393, Test accuracy: 84.44
Round  62, Train loss: 0.322, Test loss: 0.401, Test accuracy: 84.22
Round  63, Train loss: 0.351, Test loss: 0.395, Test accuracy: 84.53
Round  64, Train loss: 0.351, Test loss: 0.399, Test accuracy: 84.12
Round  65, Train loss: 0.344, Test loss: 0.399, Test accuracy: 84.47
Round  66, Train loss: 0.279, Test loss: 0.394, Test accuracy: 84.33
Round  67, Train loss: 0.322, Test loss: 0.390, Test accuracy: 84.63
Round  68, Train loss: 0.312, Test loss: 0.389, Test accuracy: 84.61
Round  69, Train loss: 0.352, Test loss: 0.385, Test accuracy: 84.80
Round  70, Train loss: 0.302, Test loss: 0.395, Test accuracy: 84.62
Round  71, Train loss: 0.328, Test loss: 0.385, Test accuracy: 84.67
Round  72, Train loss: 0.335, Test loss: 0.385, Test accuracy: 84.78
Round  73, Train loss: 0.261, Test loss: 0.381, Test accuracy: 85.03
Round  74, Train loss: 0.308, Test loss: 0.383, Test accuracy: 84.84
Round  75, Train loss: 0.252, Test loss: 0.372, Test accuracy: 85.25
Round  76, Train loss: 0.281, Test loss: 0.381, Test accuracy: 84.96
Round  77, Train loss: 0.308, Test loss: 0.376, Test accuracy: 85.05
Round  78, Train loss: 0.351, Test loss: 0.385, Test accuracy: 84.70
Round  79, Train loss: 0.285, Test loss: 0.387, Test accuracy: 84.74
Round  80, Train loss: 0.303, Test loss: 0.385, Test accuracy: 85.02
Round  81, Train loss: 0.285, Test loss: 0.381, Test accuracy: 85.20
Round  82, Train loss: 0.324, Test loss: 0.376, Test accuracy: 85.35
Round  83, Train loss: 0.297, Test loss: 0.374, Test accuracy: 85.42
Round  84, Train loss: 0.351, Test loss: 0.390, Test accuracy: 84.64
Round  85, Train loss: 0.258, Test loss: 0.387, Test accuracy: 85.28
Round  86, Train loss: 0.333, Test loss: 0.392, Test accuracy: 85.03
Round  87, Train loss: 0.303, Test loss: 0.378, Test accuracy: 85.28
Round  88, Train loss: 0.226, Test loss: 0.381, Test accuracy: 85.03
Round  89, Train loss: 0.237, Test loss: 0.387, Test accuracy: 85.00
Round  90, Train loss: 0.264, Test loss: 0.385, Test accuracy: 85.00
Round  91, Train loss: 0.255, Test loss: 0.379, Test accuracy: 85.28
Round  92, Train loss: 0.275, Test loss: 0.372, Test accuracy: 85.68
Round  93, Train loss: 0.233, Test loss: 0.375, Test accuracy: 85.41
Round  94, Train loss: 0.283, Test loss: 0.375, Test accuracy: 85.55
Round  95, Train loss: 0.244, Test loss: 0.369, Test accuracy: 85.57
Round  96, Train loss: 0.220, Test loss: 0.379, Test accuracy: 85.65
Round  97, Train loss: 0.273, Test loss: 0.374, Test accuracy: 85.72
Round  98, Train loss: 0.247, Test loss: 0.378, Test accuracy: 85.67
Round  99, Train loss: 0.231, Test loss: 0.371, Test accuracy: 86.06
Final Round, Train loss: 0.208, Test loss: 0.370, Test accuracy: 86.06
Average accuracy final 10 rounds: 85.5575
1601.365380525589
[2.0035805702209473, 4.0071611404418945, 5.703346014022827, 7.39953088760376, 8.972101211547852, 10.544671535491943, 12.070062398910522, 13.595453262329102, 15.112404584884644, 16.629355907440186, 18.183794021606445, 19.738232135772705, 21.295714855194092, 22.85319757461548, 24.37208366394043, 25.89096975326538, 27.410420417785645, 28.929871082305908, 30.456509351730347, 31.983147621154785, 33.52963924407959, 35.076130867004395, 36.586993932724, 38.0978569984436, 39.61504125595093, 41.13222551345825, 42.675273418426514, 44.218321323394775, 45.76303219795227, 47.307743072509766, 48.81750822067261, 50.32727336883545, 51.84902477264404, 53.37077617645264, 54.921785831451416, 56.472795486450195, 58.0017614364624, 59.53072738647461, 61.045562744140625, 62.56039810180664, 64.10014653205872, 65.63989496231079, 67.18383383750916, 68.72777271270752, 70.2646656036377, 71.80155849456787, 73.30939960479736, 74.81724071502686, 76.36957597732544, 77.92191123962402, 79.46609425544739, 81.01027727127075, 82.54483771324158, 84.0793981552124, 85.60477304458618, 87.13014793395996, 88.67957925796509, 90.22901058197021, 91.76892948150635, 93.30884838104248, 94.83537530899048, 96.36190223693848, 97.88572788238525, 99.40955352783203, 100.92512464523315, 102.44069576263428, 103.98502349853516, 105.52935123443604, 107.04819083213806, 108.56703042984009, 110.08728909492493, 111.60754776000977, 113.12575554847717, 114.64396333694458, 116.17599153518677, 117.70801973342896, 119.22692060470581, 120.74582147598267, 122.27331805229187, 123.80081462860107, 125.32482075691223, 126.84882688522339, 128.36747002601624, 129.88611316680908, 131.40322136878967, 132.92032957077026, 134.43948245048523, 135.9586353302002, 137.48572492599487, 139.01281452178955, 140.53648161888123, 142.0601487159729, 143.56502866744995, 145.069908618927, 146.6078405380249, 148.1457724571228, 149.67381024360657, 151.20184803009033, 152.724378824234, 154.24690961837769, 155.7739565372467, 157.30100345611572, 158.83104300498962, 160.36108255386353, 161.88208866119385, 163.40309476852417, 164.92785215377808, 166.45260953903198, 167.98378539085388, 169.51496124267578, 171.03715538978577, 172.55934953689575, 174.08589220046997, 175.6124348640442, 177.1298520565033, 178.6472692489624, 180.1753261089325, 181.7033829689026, 183.22889041900635, 184.7543978691101, 186.25814056396484, 187.76188325881958, 189.30013990402222, 190.83839654922485, 192.36809515953064, 193.89779376983643, 195.44323348999023, 196.98867321014404, 198.51152968406677, 200.0343861579895, 201.5591905117035, 203.08399486541748, 204.61032390594482, 206.13665294647217, 207.65987038612366, 209.18308782577515, 210.71009492874146, 212.23710203170776, 213.78240275382996, 215.32770347595215, 216.84955477714539, 218.37140607833862, 219.89956712722778, 221.42772817611694, 222.97068738937378, 224.51364660263062, 226.0427794456482, 227.57191228866577, 229.07713270187378, 230.5823531150818, 232.11670660972595, 233.65106010437012, 235.19582080841064, 236.74058151245117, 238.28293800354004, 239.8252944946289, 241.35429096221924, 242.88328742980957, 244.42304754257202, 245.96280765533447, 247.49718618392944, 249.0315647125244, 250.5560920238495, 252.08061933517456, 253.61900687217712, 255.1573944091797, 256.6938302516937, 258.23026609420776, 259.7705271244049, 261.31078815460205, 262.8447732925415, 264.37875843048096, 265.9349367618561, 267.4911150932312, 269.0246615409851, 270.558207988739, 272.0685362815857, 273.5788645744324, 275.10116934776306, 276.62347412109375, 278.1631419658661, 279.7028098106384, 281.2294487953186, 282.7560877799988, 284.2878167629242, 285.8195457458496, 287.35104155540466, 288.8825373649597, 290.4710636138916, 292.0595898628235, 293.59822154045105, 295.1368532180786, 296.6645972728729, 298.19234132766724, 299.69767785072327, 301.2030143737793, 302.7480297088623, 304.2930450439453, 305.829558134079, 307.36607122421265, 309.62103366851807, 311.8759961128235]
[22.1, 22.1, 33.475, 33.475, 43.475, 43.475, 54.35, 54.35, 54.416666666666664, 54.416666666666664, 60.45, 60.45, 65.89166666666667, 65.89166666666667, 67.9, 67.9, 69.95, 69.95, 74.15833333333333, 74.15833333333333, 74.49166666666666, 74.49166666666666, 74.8, 74.8, 75.75833333333334, 75.75833333333334, 76.65, 76.65, 77.09166666666667, 77.09166666666667, 77.20833333333333, 77.20833333333333, 77.375, 77.375, 77.58333333333333, 77.58333333333333, 78.175, 78.175, 78.46666666666667, 78.46666666666667, 79.33333333333333, 79.33333333333333, 79.00833333333334, 79.00833333333334, 78.84166666666667, 78.84166666666667, 79.20833333333333, 79.20833333333333, 79.95833333333333, 79.95833333333333, 80.08333333333333, 80.08333333333333, 79.925, 79.925, 80.28333333333333, 80.28333333333333, 80.69166666666666, 80.69166666666666, 80.70833333333333, 80.70833333333333, 80.9, 80.9, 81.19166666666666, 81.19166666666666, 81.33333333333333, 81.33333333333333, 80.86666666666666, 80.86666666666666, 81.175, 81.175, 81.65833333333333, 81.65833333333333, 81.475, 81.475, 81.28333333333333, 81.28333333333333, 82.55833333333334, 82.55833333333334, 82.75, 82.75, 83.025, 83.025, 82.8, 82.8, 82.8, 82.8, 82.5, 82.5, 82.875, 82.875, 83.13333333333334, 83.13333333333334, 83.31666666666666, 83.31666666666666, 83.075, 83.075, 83.54166666666667, 83.54166666666667, 83.36666666666666, 83.36666666666666, 83.66666666666667, 83.66666666666667, 84.15, 84.15, 83.525, 83.525, 84.20833333333333, 84.20833333333333, 83.75, 83.75, 84.30833333333334, 84.30833333333334, 83.85833333333333, 83.85833333333333, 84.01666666666667, 84.01666666666667, 83.90833333333333, 83.90833333333333, 83.91666666666667, 83.91666666666667, 84.25, 84.25, 84.44166666666666, 84.44166666666666, 84.225, 84.225, 84.525, 84.525, 84.125, 84.125, 84.475, 84.475, 84.33333333333333, 84.33333333333333, 84.63333333333334, 84.63333333333334, 84.60833333333333, 84.60833333333333, 84.8, 84.8, 84.625, 84.625, 84.66666666666667, 84.66666666666667, 84.78333333333333, 84.78333333333333, 85.03333333333333, 85.03333333333333, 84.84166666666667, 84.84166666666667, 85.25, 85.25, 84.95833333333333, 84.95833333333333, 85.05, 85.05, 84.7, 84.7, 84.74166666666666, 84.74166666666666, 85.01666666666667, 85.01666666666667, 85.2, 85.2, 85.35, 85.35, 85.425, 85.425, 84.64166666666667, 84.64166666666667, 85.275, 85.275, 85.03333333333333, 85.03333333333333, 85.275, 85.275, 85.03333333333333, 85.03333333333333, 85.0, 85.0, 85.0, 85.0, 85.275, 85.275, 85.68333333333334, 85.68333333333334, 85.40833333333333, 85.40833333333333, 85.55, 85.55, 85.56666666666666, 85.56666666666666, 85.65, 85.65, 85.71666666666667, 85.71666666666667, 85.66666666666667, 85.66666666666667, 86.05833333333334, 86.05833333333334, 86.05833333333334, 86.05833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.146, Test loss: 2.272, Test accuracy: 17.85
Round   1, Train loss: 0.975, Test loss: 2.186, Test accuracy: 27.00
Round   2, Train loss: 0.874, Test loss: 2.120, Test accuracy: 29.74
Round   3, Train loss: 0.811, Test loss: 2.179, Test accuracy: 30.74
Round   4, Train loss: 0.850, Test loss: 2.193, Test accuracy: 25.24
Round   5, Train loss: 0.749, Test loss: 1.995, Test accuracy: 27.76
Round   6, Train loss: 0.733, Test loss: 2.460, Test accuracy: 27.02
Round   7, Train loss: 0.701, Test loss: 1.944, Test accuracy: 35.31
Round   8, Train loss: 0.786, Test loss: 2.119, Test accuracy: 30.52
Round   9, Train loss: 0.772, Test loss: 2.135, Test accuracy: 25.10
Round  10, Train loss: 0.637, Test loss: 2.071, Test accuracy: 32.77
Round  11, Train loss: 0.607, Test loss: 1.831, Test accuracy: 37.69
Round  12, Train loss: 0.589, Test loss: 1.855, Test accuracy: 39.33
Round  13, Train loss: 0.577, Test loss: 1.971, Test accuracy: 37.48
Round  14, Train loss: 0.542, Test loss: 2.063, Test accuracy: 38.60
Round  15, Train loss: 0.545, Test loss: 1.959, Test accuracy: 38.96
Round  16, Train loss: 0.551, Test loss: 2.058, Test accuracy: 37.05
Round  17, Train loss: 0.536, Test loss: 1.805, Test accuracy: 40.72
Round  18, Train loss: 0.587, Test loss: 1.887, Test accuracy: 35.31
Round  19, Train loss: 0.602, Test loss: 1.885, Test accuracy: 37.41
Round  20, Train loss: 0.589, Test loss: 2.188, Test accuracy: 33.08
Round  21, Train loss: 0.479, Test loss: 2.114, Test accuracy: 35.51
Round  22, Train loss: 0.562, Test loss: 1.758, Test accuracy: 41.15
Round  23, Train loss: 0.478, Test loss: 1.686, Test accuracy: 40.18
Round  24, Train loss: 0.499, Test loss: 1.816, Test accuracy: 41.25
Round  25, Train loss: 0.526, Test loss: 2.147, Test accuracy: 38.16
Round  26, Train loss: 0.399, Test loss: 1.741, Test accuracy: 42.89
Round  27, Train loss: 0.489, Test loss: 1.917, Test accuracy: 41.46
Round  28, Train loss: 0.488, Test loss: 1.924, Test accuracy: 37.63
Round  29, Train loss: 0.504, Test loss: 1.932, Test accuracy: 40.39
Round  30, Train loss: 0.471, Test loss: 1.792, Test accuracy: 41.48
Round  31, Train loss: 0.488, Test loss: 1.816, Test accuracy: 42.18
Round  32, Train loss: 0.446, Test loss: 2.180, Test accuracy: 34.62
Round  33, Train loss: 0.383, Test loss: 1.958, Test accuracy: 38.38
Round  34, Train loss: 0.361, Test loss: 1.764, Test accuracy: 43.36
Round  35, Train loss: 0.442, Test loss: 1.623, Test accuracy: 45.13
Round  36, Train loss: 0.438, Test loss: 1.911, Test accuracy: 41.29
Round  37, Train loss: 0.345, Test loss: 2.037, Test accuracy: 40.72
Round  38, Train loss: 0.404, Test loss: 2.039, Test accuracy: 40.02
Round  39, Train loss: 0.395, Test loss: 1.943, Test accuracy: 43.02
Round  40, Train loss: 0.334, Test loss: 2.003, Test accuracy: 40.23
Round  41, Train loss: 0.359, Test loss: 1.747, Test accuracy: 45.45
Round  42, Train loss: 0.405, Test loss: 1.805, Test accuracy: 45.30
Round  43, Train loss: 0.347, Test loss: 2.052, Test accuracy: 41.16
Round  44, Train loss: 0.412, Test loss: 1.762, Test accuracy: 44.38
Round  45, Train loss: 0.455, Test loss: 1.818, Test accuracy: 43.25
Round  46, Train loss: 0.431, Test loss: 2.027, Test accuracy: 39.41
Round  47, Train loss: 0.391, Test loss: 1.893, Test accuracy: 44.89
Round  48, Train loss: 0.353, Test loss: 2.027, Test accuracy: 40.24
Round  49, Train loss: 0.403, Test loss: 1.661, Test accuracy: 45.56
Round  50, Train loss: 0.364, Test loss: 2.166, Test accuracy: 39.86
Round  51, Train loss: 0.312, Test loss: 1.786, Test accuracy: 44.83
Round  52, Train loss: 0.322, Test loss: 1.872, Test accuracy: 44.77
Round  53, Train loss: 0.376, Test loss: 1.843, Test accuracy: 43.07
Round  54, Train loss: 0.309, Test loss: 1.917, Test accuracy: 42.70
Round  55, Train loss: 0.332, Test loss: 1.938, Test accuracy: 42.62
Round  56, Train loss: 0.363, Test loss: 1.857, Test accuracy: 43.64
Round  57, Train loss: 0.350, Test loss: 2.184, Test accuracy: 41.26
Round  58, Train loss: 0.266, Test loss: 2.195, Test accuracy: 42.48
Round  59, Train loss: 0.279, Test loss: 2.271, Test accuracy: 39.83
Round  60, Train loss: 0.302, Test loss: 1.998, Test accuracy: 42.08
Round  61, Train loss: 0.259, Test loss: 2.005, Test accuracy: 42.82
Round  62, Train loss: 0.286, Test loss: 2.423, Test accuracy: 40.49
Round  63, Train loss: 0.232, Test loss: 2.356, Test accuracy: 42.23
Round  64, Train loss: 0.222, Test loss: 1.935, Test accuracy: 44.32
Round  65, Train loss: 0.266, Test loss: 1.884, Test accuracy: 43.91
Round  66, Train loss: 0.262, Test loss: 1.897, Test accuracy: 43.53
Round  67, Train loss: 0.235, Test loss: 1.732, Test accuracy: 46.37
Round  68, Train loss: 0.316, Test loss: 2.300, Test accuracy: 43.91
Round  69, Train loss: 0.283, Test loss: 1.911, Test accuracy: 44.88
Round  70, Train loss: 0.317, Test loss: 1.825, Test accuracy: 45.88
Round  71, Train loss: 0.238, Test loss: 2.027, Test accuracy: 45.95
Round  72, Train loss: 0.269, Test loss: 1.849, Test accuracy: 45.66
Round  73, Train loss: 0.317, Test loss: 1.772, Test accuracy: 45.96
Round  74, Train loss: 0.251, Test loss: 1.841, Test accuracy: 47.03
Round  75, Train loss: 0.280, Test loss: 1.755, Test accuracy: 44.74
Round  76, Train loss: 0.218, Test loss: 2.011, Test accuracy: 44.68
Round  77, Train loss: 0.275, Test loss: 1.791, Test accuracy: 45.44
Round  78, Train loss: 0.285, Test loss: 2.141, Test accuracy: 40.67
Round  79, Train loss: 0.244, Test loss: 2.400, Test accuracy: 40.86
Round  80, Train loss: 0.248, Test loss: 2.272, Test accuracy: 39.50
Round  81, Train loss: 0.223, Test loss: 2.006, Test accuracy: 44.46
Round  82, Train loss: 0.261, Test loss: 1.883, Test accuracy: 46.85
Round  83, Train loss: 0.240, Test loss: 1.871, Test accuracy: 46.50
Round  84, Train loss: 0.252, Test loss: 2.080, Test accuracy: 45.35
Round  85, Train loss: 0.240, Test loss: 1.931, Test accuracy: 42.12
Round  86, Train loss: 0.290, Test loss: 2.107, Test accuracy: 42.35
Round  87, Train loss: 0.220, Test loss: 2.829, Test accuracy: 37.46
Round  88, Train loss: 0.233, Test loss: 1.875, Test accuracy: 47.09
Round  89, Train loss: 0.240, Test loss: 1.999, Test accuracy: 44.49
Round  90, Train loss: 0.197, Test loss: 1.929, Test accuracy: 46.03
Round  91, Train loss: 0.193, Test loss: 2.183, Test accuracy: 42.92
Round  92, Train loss: 0.189, Test loss: 1.977, Test accuracy: 45.65
Round  93, Train loss: 0.181, Test loss: 2.108, Test accuracy: 44.45
Round  94, Train loss: 0.189, Test loss: 1.950, Test accuracy: 46.50
Round  95, Train loss: 0.187, Test loss: 2.058, Test accuracy: 46.74
Round  96, Train loss: 0.238, Test loss: 2.114, Test accuracy: 46.17
Round  97, Train loss: 0.227, Test loss: 1.940, Test accuracy: 44.83
Round  98, Train loss: 0.191, Test loss: 2.357, Test accuracy: 40.27
Round  99, Train loss: 0.233, Test loss: 1.968, Test accuracy: 44.24
Final Round, Train loss: 0.173, Test loss: 1.729, Test accuracy: 47.32
Average accuracy final 10 rounds: 44.780833333333334
2401.90797662735
[3.8980860710144043, 7.792619466781616, 11.637083053588867, 15.511058568954468, 19.323524236679077, 23.004289150238037, 26.678038358688354, 30.321978330612183, 33.866509199142456, 37.125248193740845, 40.42253565788269, 43.69741606712341, 46.98506426811218, 50.28825283050537, 53.57045388221741, 56.902916431427, 60.17964291572571, 63.46456170082092, 66.81722712516785, 70.09898161888123, 73.40593028068542, 76.69577646255493, 79.96293044090271, 83.24840116500854, 86.54393482208252, 89.82554650306702, 93.3172070980072, 96.58942008018494, 99.89841294288635, 103.60963892936707, 106.93160009384155, 110.24187350273132, 113.55396318435669, 116.83708000183105, 120.10708928108215, 123.41213941574097, 126.70777177810669, 129.97998523712158, 133.31024527549744, 136.60120749473572, 139.8902246952057, 143.17900109291077, 146.46661186218262, 149.7837209701538, 153.08095717430115, 156.4033076763153, 160.22976756095886, 163.57839107513428, 166.91462111473083, 170.2543761730194, 173.57479095458984, 176.87753701210022, 180.25808000564575, 183.59699416160583, 186.91855335235596, 190.26454329490662, 193.59556579589844, 196.8959002494812, 200.2244610786438, 203.55104684829712, 206.8961946964264, 210.21965885162354, 213.56770873069763, 216.89936351776123, 220.237309217453, 223.61174130439758, 226.92780923843384, 230.2496476173401, 233.5965120792389, 236.8942050933838, 240.50433802604675, 244.27775359153748, 247.92659425735474, 251.56589674949646, 254.86866569519043, 258.195241689682, 261.50464820861816, 264.82658433914185, 268.1576828956604, 271.48573565483093, 274.8185658454895, 278.15281867980957, 281.4684853553772, 284.77888798713684, 288.0888423919678, 291.6759855747223, 295.2906103134155, 298.92299938201904, 302.4971113204956, 306.1082286834717, 309.6347219944, 313.26642322540283, 316.92726850509644, 320.57834577560425, 324.2243800163269, 327.9004464149475, 331.5583369731903, 335.1996371746063, 338.8632745742798, 342.52377367019653, 345.6857821941376]
[17.85, 27.0, 29.741666666666667, 30.741666666666667, 25.241666666666667, 27.758333333333333, 27.016666666666666, 35.30833333333333, 30.525, 25.1, 32.766666666666666, 37.69166666666667, 39.325, 37.483333333333334, 38.6, 38.958333333333336, 37.05, 40.71666666666667, 35.30833333333333, 37.40833333333333, 33.075, 35.50833333333333, 41.15, 40.18333333333333, 41.25, 38.15833333333333, 42.891666666666666, 41.458333333333336, 37.63333333333333, 40.391666666666666, 41.475, 42.18333333333333, 34.625, 38.38333333333333, 43.358333333333334, 45.13333333333333, 41.291666666666664, 40.71666666666667, 40.025, 43.025, 40.233333333333334, 45.45, 45.3, 41.15833333333333, 44.375, 43.25, 39.40833333333333, 44.891666666666666, 40.24166666666667, 45.55833333333333, 39.858333333333334, 44.825, 44.766666666666666, 43.06666666666667, 42.7, 42.61666666666667, 43.641666666666666, 41.25833333333333, 42.483333333333334, 39.825, 42.083333333333336, 42.81666666666667, 40.49166666666667, 42.225, 44.31666666666667, 43.90833333333333, 43.53333333333333, 46.36666666666667, 43.90833333333333, 44.875, 45.88333333333333, 45.95, 45.65833333333333, 45.958333333333336, 47.03333333333333, 44.74166666666667, 44.68333333333333, 45.44166666666667, 40.675, 40.858333333333334, 39.5, 44.458333333333336, 46.85, 46.5, 45.35, 42.125, 42.35, 37.458333333333336, 47.09166666666667, 44.49166666666667, 46.03333333333333, 42.916666666666664, 45.65, 44.45, 46.5, 46.74166666666667, 46.175, 44.825, 40.275, 44.24166666666667, 47.31666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.312, Test loss: 2.307, Test accuracy: 9.98
Round   0, Global train loss: 2.312, Global test loss: 2.306, Global test accuracy: 9.98
Round   1, Train loss: 2.284, Test loss: 2.309, Test accuracy: 9.99
Round   1, Global train loss: 2.284, Global test loss: 2.307, Global test accuracy: 10.00
Round   2, Train loss: 2.274, Test loss: 2.311, Test accuracy: 10.07
Round   2, Global train loss: 2.274, Global test loss: 2.310, Global test accuracy: 9.94
Round   3, Train loss: nan, Test loss: nan, Test accuracy: 11.56
Round   3, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 15.00
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 15.00
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 15.00
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 16.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 16.67
Average accuracy final 10 rounds: 16.666666666666664 

Average global accuracy final 10 rounds: 16.666666666666664 

1569.0464372634888
[1.701704978942871, 3.1554372310638428, 4.6001200675964355, 6.052061080932617, 7.5044872760772705, 8.953423500061035, 10.405789136886597, 11.922621965408325, 13.453953981399536, 14.980275630950928, 16.520204067230225, 18.037991285324097, 19.564379692077637, 21.08420705795288, 22.614718914031982, 24.150959253311157, 25.672235012054443, 27.193843603134155, 28.71436882019043, 30.236387491226196, 31.762333869934082, 33.283888816833496, 34.79477906227112, 36.30619812011719, 37.81773090362549, 39.330541133880615, 40.89915108680725, 42.46545100212097, 44.03253221511841, 45.54248809814453, 47.058852672576904, 48.58205723762512, 50.105886697769165, 51.61852264404297, 53.13326096534729, 54.643218994140625, 56.153265953063965, 57.664630651474, 59.171868562698364, 60.68277549743652, 62.191808462142944, 63.70251274108887, 65.21755933761597, 66.72584366798401, 68.23532891273499, 69.75034499168396, 71.25891971588135, 72.76673865318298, 74.28230834007263, 75.79061651229858, 77.30127048492432, 78.80855131149292, 80.31899213790894, 81.83793497085571, 83.34734296798706, 84.86508631706238, 86.37295699119568, 87.88512563705444, 89.38397479057312, 90.88406825065613, 92.39332723617554, 93.89102745056152, 95.39896774291992, 96.9102463722229, 98.42417335510254, 99.94197821617126, 101.45318126678467, 102.96567630767822, 104.48005747795105, 105.99400615692139, 107.50618147850037, 109.02563118934631, 110.54294681549072, 112.05110788345337, 113.56011867523193, 115.06972026824951, 116.58509802818298, 118.0983567237854, 119.62023949623108, 121.14320373535156, 122.68209052085876, 124.20953154563904, 125.7436032295227, 127.2772376537323, 128.8106245994568, 130.342271566391, 131.8771686553955, 133.41373562812805, 134.9315161705017, 136.45224022865295, 137.97313618659973, 139.49626111984253, 141.0185911655426, 142.5423846244812, 144.06817960739136, 145.59764957427979, 147.12772226333618, 148.6531846523285, 150.17137622833252, 151.69563722610474, 154.2199001312256]
[9.983333333333333, 9.991666666666667, 10.075, 11.558333333333334, 13.333333333333334, 11.666666666666666, 11.666666666666666, 13.333333333333334, 13.333333333333334, 15.0, 15.0, 15.0, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668, 16.666666666666668]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.002, Test loss: 2.075, Test accuracy: 25.46
Round   0, Global train loss: 2.002, Global test loss: 2.193, Global test accuracy: 21.70
Round   1, Train loss: 1.752, Test loss: 1.885, Test accuracy: 32.00
Round   1, Global train loss: 1.752, Global test loss: 2.151, Global test accuracy: 22.92
Round   2, Train loss: 1.618, Test loss: 1.806, Test accuracy: 36.84
Round   2, Global train loss: 1.618, Global test loss: 2.222, Global test accuracy: 26.27
Round   3, Train loss: 1.528, Test loss: 1.678, Test accuracy: 40.26
Round   3, Global train loss: 1.528, Global test loss: 2.146, Global test accuracy: 25.48
Round   4, Train loss: 1.452, Test loss: 1.574, Test accuracy: 43.60
Round   4, Global train loss: 1.452, Global test loss: 2.097, Global test accuracy: 26.74
Round   5, Train loss: 1.360, Test loss: 1.556, Test accuracy: 45.23
Round   5, Global train loss: 1.360, Global test loss: 2.149, Global test accuracy: 27.55
Round   6, Train loss: 1.323, Test loss: 1.515, Test accuracy: 47.46
Round   6, Global train loss: 1.323, Global test loss: 2.207, Global test accuracy: 29.05
Round   7, Train loss: 1.252, Test loss: 1.475, Test accuracy: 48.49
Round   7, Global train loss: 1.252, Global test loss: 2.162, Global test accuracy: 28.88
Round   8, Train loss: 1.244, Test loss: 1.407, Test accuracy: 50.02
Round   8, Global train loss: 1.244, Global test loss: 2.070, Global test accuracy: 27.73
Round   9, Train loss: 1.159, Test loss: 1.421, Test accuracy: 50.47
Round   9, Global train loss: 1.159, Global test loss: 2.101, Global test accuracy: 29.89
Round  10, Train loss: 1.158, Test loss: 1.305, Test accuracy: 55.21
Round  10, Global train loss: 1.158, Global test loss: 2.154, Global test accuracy: 30.64
Round  11, Train loss: 1.116, Test loss: 1.213, Test accuracy: 57.45
Round  11, Global train loss: 1.116, Global test loss: 2.126, Global test accuracy: 28.45
Round  12, Train loss: 1.078, Test loss: 1.196, Test accuracy: 58.03
Round  12, Global train loss: 1.078, Global test loss: 2.203, Global test accuracy: 30.08
Round  13, Train loss: 1.091, Test loss: 1.163, Test accuracy: 59.55
Round  13, Global train loss: 1.091, Global test loss: 2.054, Global test accuracy: 30.19
Round  14, Train loss: 1.022, Test loss: 1.145, Test accuracy: 60.48
Round  14, Global train loss: 1.022, Global test loss: 2.099, Global test accuracy: 29.20
Round  15, Train loss: 0.998, Test loss: 1.118, Test accuracy: 61.51
Round  15, Global train loss: 0.998, Global test loss: 2.094, Global test accuracy: 31.29
Round  16, Train loss: 0.968, Test loss: 1.097, Test accuracy: 62.37
Round  16, Global train loss: 0.968, Global test loss: 2.099, Global test accuracy: 30.24
Round  17, Train loss: 0.986, Test loss: 1.057, Test accuracy: 64.03
Round  17, Global train loss: 0.986, Global test loss: 2.133, Global test accuracy: 30.00
Round  18, Train loss: 0.946, Test loss: 1.053, Test accuracy: 64.23
Round  18, Global train loss: 0.946, Global test loss: 2.243, Global test accuracy: 30.11
Round  19, Train loss: 0.921, Test loss: 1.047, Test accuracy: 64.51
Round  19, Global train loss: 0.921, Global test loss: 2.235, Global test accuracy: 31.93
Round  20, Train loss: 0.907, Test loss: 1.027, Test accuracy: 65.37
Round  20, Global train loss: 0.907, Global test loss: 2.184, Global test accuracy: 29.29
Round  21, Train loss: 0.899, Test loss: 1.029, Test accuracy: 65.39
Round  21, Global train loss: 0.899, Global test loss: 2.084, Global test accuracy: 31.68
Round  22, Train loss: 0.879, Test loss: 1.025, Test accuracy: 65.60
Round  22, Global train loss: 0.879, Global test loss: 2.068, Global test accuracy: 30.08
Round  23, Train loss: 0.854, Test loss: 1.008, Test accuracy: 66.36
Round  23, Global train loss: 0.854, Global test loss: 2.093, Global test accuracy: 31.90
Round  24, Train loss: 0.854, Test loss: 0.997, Test accuracy: 66.78
Round  24, Global train loss: 0.854, Global test loss: 2.173, Global test accuracy: 32.20
Round  25, Train loss: 0.805, Test loss: 0.994, Test accuracy: 67.01
Round  25, Global train loss: 0.805, Global test loss: 2.327, Global test accuracy: 31.46
Round  26, Train loss: 0.841, Test loss: 0.983, Test accuracy: 67.57
Round  26, Global train loss: 0.841, Global test loss: 2.272, Global test accuracy: 30.19
Round  27, Train loss: 0.828, Test loss: 0.980, Test accuracy: 67.65
Round  27, Global train loss: 0.828, Global test loss: 2.173, Global test accuracy: 30.64
Round  28, Train loss: 0.787, Test loss: 0.970, Test accuracy: 68.03
Round  28, Global train loss: 0.787, Global test loss: 2.308, Global test accuracy: 31.01
Round  29, Train loss: 0.798, Test loss: 0.976, Test accuracy: 67.80
Round  29, Global train loss: 0.798, Global test loss: 2.179, Global test accuracy: 32.54
Round  30, Train loss: 0.790, Test loss: 0.974, Test accuracy: 68.11
Round  30, Global train loss: 0.790, Global test loss: 2.524, Global test accuracy: 32.20
Round  31, Train loss: 0.765, Test loss: 0.964, Test accuracy: 68.58
Round  31, Global train loss: 0.765, Global test loss: 2.182, Global test accuracy: 31.61
Round  32, Train loss: 0.720, Test loss: 0.964, Test accuracy: 68.61
Round  32, Global train loss: 0.720, Global test loss: 2.218, Global test accuracy: 31.34
Round  33, Train loss: 0.763, Test loss: 0.962, Test accuracy: 69.06
Round  33, Global train loss: 0.763, Global test loss: 2.379, Global test accuracy: 30.79
Round  34, Train loss: 0.740, Test loss: 0.972, Test accuracy: 68.64
Round  34, Global train loss: 0.740, Global test loss: 2.174, Global test accuracy: 30.53
Round  35, Train loss: 0.739, Test loss: 0.969, Test accuracy: 68.64
Round  35, Global train loss: 0.739, Global test loss: 2.094, Global test accuracy: 31.77
Round  36, Train loss: 0.726, Test loss: 0.953, Test accuracy: 69.31
Round  36, Global train loss: 0.726, Global test loss: 2.553, Global test accuracy: 32.42
Round  37, Train loss: 0.704, Test loss: 0.950, Test accuracy: 69.42
Round  37, Global train loss: 0.704, Global test loss: 2.421, Global test accuracy: 33.60
Round  38, Train loss: 0.715, Test loss: 0.949, Test accuracy: 69.56
Round  38, Global train loss: 0.715, Global test loss: 2.395, Global test accuracy: 30.14
Round  39, Train loss: 0.745, Test loss: 0.951, Test accuracy: 69.89
Round  39, Global train loss: 0.745, Global test loss: 2.301, Global test accuracy: 29.87
Round  40, Train loss: 0.698, Test loss: 0.949, Test accuracy: 69.97
Round  40, Global train loss: 0.698, Global test loss: 2.178, Global test accuracy: 31.79
Round  41, Train loss: 0.692, Test loss: 0.950, Test accuracy: 70.06
Round  41, Global train loss: 0.692, Global test loss: 2.300, Global test accuracy: 31.32
Round  42, Train loss: 0.696, Test loss: 0.949, Test accuracy: 70.24
Round  42, Global train loss: 0.696, Global test loss: 2.321, Global test accuracy: 31.05
Round  43, Train loss: 0.654, Test loss: 0.954, Test accuracy: 70.11
Round  43, Global train loss: 0.654, Global test loss: 2.567, Global test accuracy: 30.38
Round  44, Train loss: 0.656, Test loss: 0.971, Test accuracy: 69.76
Round  44, Global train loss: 0.656, Global test loss: 2.382, Global test accuracy: 30.56
Round  45, Train loss: 0.657, Test loss: 0.963, Test accuracy: 70.13
Round  45, Global train loss: 0.657, Global test loss: 2.512, Global test accuracy: 33.12
Round  46, Train loss: 0.646, Test loss: 0.966, Test accuracy: 70.15
Round  46, Global train loss: 0.646, Global test loss: 2.243, Global test accuracy: 32.07
Round  47, Train loss: 0.641, Test loss: 0.953, Test accuracy: 70.52
Round  47, Global train loss: 0.641, Global test loss: 2.290, Global test accuracy: 30.75
Round  48, Train loss: 0.643, Test loss: 0.958, Test accuracy: 70.53
Round  48, Global train loss: 0.643, Global test loss: 2.315, Global test accuracy: 32.62
Round  49, Train loss: 0.642, Test loss: 0.966, Test accuracy: 70.58
Round  49, Global train loss: 0.642, Global test loss: 2.406, Global test accuracy: 30.96
Round  50, Train loss: 0.606, Test loss: 0.964, Test accuracy: 70.47
Round  50, Global train loss: 0.606, Global test loss: 2.368, Global test accuracy: 31.60
Round  51, Train loss: 0.621, Test loss: 0.954, Test accuracy: 70.71
Round  51, Global train loss: 0.621, Global test loss: 2.210, Global test accuracy: 31.90
Round  52, Train loss: 0.603, Test loss: 0.953, Test accuracy: 70.91
Round  52, Global train loss: 0.603, Global test loss: 2.410, Global test accuracy: 31.67
Round  53, Train loss: 0.621, Test loss: 0.960, Test accuracy: 70.78
Round  53, Global train loss: 0.621, Global test loss: 2.251, Global test accuracy: 30.97
Round  54, Train loss: 0.629, Test loss: 0.950, Test accuracy: 71.18
Round  54, Global train loss: 0.629, Global test loss: 2.261, Global test accuracy: 31.68
Round  55, Train loss: 0.582, Test loss: 0.942, Test accuracy: 71.34
Round  55, Global train loss: 0.582, Global test loss: 2.336, Global test accuracy: 32.41
Round  56, Train loss: 0.603, Test loss: 0.938, Test accuracy: 71.39
Round  56, Global train loss: 0.603, Global test loss: 2.449, Global test accuracy: 33.53
Round  57, Train loss: 0.593, Test loss: 0.934, Test accuracy: 71.50
Round  57, Global train loss: 0.593, Global test loss: 2.544, Global test accuracy: 31.42
Round  58, Train loss: 0.576, Test loss: 0.939, Test accuracy: 71.56
Round  58, Global train loss: 0.576, Global test loss: 2.519, Global test accuracy: 30.42
Round  59, Train loss: 0.563, Test loss: 0.941, Test accuracy: 71.58
Round  59, Global train loss: 0.563, Global test loss: 2.277, Global test accuracy: 31.01
Round  60, Train loss: 0.628, Test loss: 0.954, Test accuracy: 71.53
Round  60, Global train loss: 0.628, Global test loss: 2.270, Global test accuracy: 32.84
Round  61, Train loss: 0.599, Test loss: 0.952, Test accuracy: 71.59
Round  61, Global train loss: 0.599, Global test loss: 2.616, Global test accuracy: 30.34
Round  62, Train loss: 0.568, Test loss: 0.949, Test accuracy: 71.83
Round  62, Global train loss: 0.568, Global test loss: 2.660, Global test accuracy: 31.52
Round  63, Train loss: 0.567, Test loss: 0.937, Test accuracy: 72.04
Round  63, Global train loss: 0.567, Global test loss: 2.322, Global test accuracy: 29.40
Round  64, Train loss: 0.589, Test loss: 0.940, Test accuracy: 71.99
Round  64, Global train loss: 0.589, Global test loss: 2.292, Global test accuracy: 32.38
Round  65, Train loss: 0.567, Test loss: 0.939, Test accuracy: 72.33
Round  65, Global train loss: 0.567, Global test loss: 2.456, Global test accuracy: 30.58
Round  66, Train loss: 0.566, Test loss: 0.942, Test accuracy: 72.31
Round  66, Global train loss: 0.566, Global test loss: 2.344, Global test accuracy: 31.43
Round  67, Train loss: 0.535, Test loss: 0.940, Test accuracy: 72.30
Round  67, Global train loss: 0.535, Global test loss: 2.580, Global test accuracy: 30.75
Round  68, Train loss: 0.569, Test loss: 0.929, Test accuracy: 72.51
Round  68, Global train loss: 0.569, Global test loss: 2.331, Global test accuracy: 30.87
Round  69, Train loss: 0.562, Test loss: 0.926, Test accuracy: 72.72
Round  69, Global train loss: 0.562, Global test loss: 2.432, Global test accuracy: 31.28
Round  70, Train loss: 0.564, Test loss: 0.932, Test accuracy: 72.63
Round  70, Global train loss: 0.564, Global test loss: 2.366, Global test accuracy: 30.14
Round  71, Train loss: 0.542, Test loss: 0.931, Test accuracy: 72.46
Round  71, Global train loss: 0.542, Global test loss: 2.574, Global test accuracy: 33.34
Round  72, Train loss: 0.550, Test loss: 0.937, Test accuracy: 72.31
Round  72, Global train loss: 0.550, Global test loss: 2.251, Global test accuracy: 32.36
Round  73, Train loss: 0.557, Test loss: 0.921, Test accuracy: 72.65
Round  73, Global train loss: 0.557, Global test loss: 2.283, Global test accuracy: 31.61
Round  74, Train loss: 0.532, Test loss: 0.928, Test accuracy: 72.70
Round  74, Global train loss: 0.532, Global test loss: 2.596, Global test accuracy: 30.77
Round  75, Train loss: 0.526, Test loss: 0.930, Test accuracy: 72.86
Round  75, Global train loss: 0.526, Global test loss: 2.335, Global test accuracy: 31.06
Round  76, Train loss: 0.512, Test loss: 0.939, Test accuracy: 72.61
Round  76, Global train loss: 0.512, Global test loss: 2.462, Global test accuracy: 30.68
Round  77, Train loss: 0.517, Test loss: 0.944, Test accuracy: 72.46
Round  77, Global train loss: 0.517, Global test loss: 2.616, Global test accuracy: 31.02
Round  78, Train loss: 0.566, Test loss: 0.944, Test accuracy: 72.60
Round  78, Global train loss: 0.566, Global test loss: 2.459, Global test accuracy: 31.75
Round  79, Train loss: 0.525, Test loss: 0.948, Test accuracy: 72.47
Round  79, Global train loss: 0.525, Global test loss: 2.698, Global test accuracy: 31.07
Round  80, Train loss: 0.529, Test loss: 0.951, Test accuracy: 72.43
Round  80, Global train loss: 0.529, Global test loss: 2.286, Global test accuracy: 32.36
Round  81, Train loss: 0.518, Test loss: 0.947, Test accuracy: 72.51
Round  81, Global train loss: 0.518, Global test loss: 2.823, Global test accuracy: 33.23
Round  82, Train loss: 0.504, Test loss: 0.955, Test accuracy: 72.39
Round  82, Global train loss: 0.504, Global test loss: 2.479, Global test accuracy: 29.30
Round  83, Train loss: 0.506, Test loss: 0.961, Test accuracy: 72.19
Round  83, Global train loss: 0.506, Global test loss: 2.696, Global test accuracy: 33.25
Round  84, Train loss: 0.525, Test loss: 0.962, Test accuracy: 72.38
Round  84, Global train loss: 0.525, Global test loss: 2.285, Global test accuracy: 31.75
Round  85, Train loss: 0.503, Test loss: 0.961, Test accuracy: 72.36
Round  85, Global train loss: 0.503, Global test loss: 2.460, Global test accuracy: 33.00
Round  86, Train loss: 0.502, Test loss: 0.953, Test accuracy: 72.63
Round  86, Global train loss: 0.502, Global test loss: 2.373, Global test accuracy: 30.49
Round  87, Train loss: 0.501, Test loss: 0.968, Test accuracy: 72.56
Round  87, Global train loss: 0.501, Global test loss: 2.324, Global test accuracy: 31.40
Round  88, Train loss: 0.508, Test loss: 0.963, Test accuracy: 72.58
Round  88, Global train loss: 0.508, Global test loss: 2.394, Global test accuracy: 32.96
Round  89, Train loss: 0.514, Test loss: 0.974, Test accuracy: 72.36
Round  89, Global train loss: 0.514, Global test loss: 2.555, Global test accuracy: 31.50
Round  90, Train loss: 0.474, Test loss: 0.968, Test accuracy: 72.31
Round  90, Global train loss: 0.474, Global test loss: 2.174, Global test accuracy: 31.12
Round  91, Train loss: 0.515, Test loss: 0.967, Test accuracy: 72.67
Round  91, Global train loss: 0.515, Global test loss: 2.411, Global test accuracy: 33.23
Round  92, Train loss: 0.499, Test loss: 0.953, Test accuracy: 72.97
Round  92, Global train loss: 0.499, Global test loss: 2.503, Global test accuracy: 30.41
Round  93, Train loss: 0.491, Test loss: 0.933, Test accuracy: 73.33
Round  93, Global train loss: 0.491, Global test loss: 2.708, Global test accuracy: 33.80
Round  94, Train loss: 0.486, Test loss: 0.938, Test accuracy: 73.32
Round  94, Global train loss: 0.486, Global test loss: 2.578, Global test accuracy: 32.45
Round  95, Train loss: 0.465, Test loss: 0.931, Test accuracy: 73.61
Round  95, Global train loss: 0.465, Global test loss: 2.519, Global test accuracy: 31.46
Round  96, Train loss: 0.485, Test loss: 0.935, Test accuracy: 73.55
Round  96, Global train loss: 0.485, Global test loss: 2.553, Global test accuracy: 29.07
Round  97, Train loss: 0.476, Test loss: 0.926, Test accuracy: 73.72
Round  97, Global train loss: 0.476, Global test loss: 2.389, Global test accuracy: 30.95
Round  98, Train loss: 0.458, Test loss: 0.944, Test accuracy: 73.41
Round  98, Global train loss: 0.458, Global test loss: 2.170, Global test accuracy: 31.59
Round  99, Train loss: 0.478, Test loss: 0.945, Test accuracy: 73.28
Round  99, Global train loss: 0.478, Global test loss: 2.416, Global test accuracy: 31.43
Final Round, Train loss: 0.327, Test loss: 1.069, Test accuracy: 73.96
Final Round, Global train loss: 0.327, Global test loss: 2.416, Global test accuracy: 31.43
Average accuracy final 10 rounds: 73.21675 

Average global accuracy final 10 rounds: 31.55125 

5674.3628985881805
[4.716848134994507, 9.433696269989014, 14.1858491897583, 18.938002109527588, 23.673907995224, 28.40981388092041, 33.133822202682495, 37.85783052444458, 41.94330811500549, 46.028785705566406, 50.12287926673889, 54.21697282791138, 58.324379682540894, 62.43178653717041, 66.52533650398254, 70.61888647079468, 74.72513389587402, 78.83138132095337, 82.91481399536133, 86.99824666976929, 91.21697235107422, 95.43569803237915, 99.51625061035156, 103.59680318832397, 107.67967629432678, 111.76254940032959, 115.84252572059631, 119.92250204086304, 124.00121641159058, 128.07993078231812, 132.16341137886047, 136.24689197540283, 140.32170271873474, 144.39651346206665, 148.48409271240234, 152.57167196273804, 156.64435243606567, 160.7170329093933, 164.85320830345154, 168.98938369750977, 173.08206176757812, 177.17473983764648, 181.2873694896698, 185.39999914169312, 189.47678804397583, 193.55357694625854, 197.63363003730774, 201.71368312835693, 205.84056425094604, 209.96744537353516, 214.07363510131836, 218.17982482910156, 222.2903347015381, 226.4008445739746, 230.51113319396973, 234.62142181396484, 238.72785544395447, 242.8342890739441, 246.93741464614868, 251.04054021835327, 255.14863562583923, 259.2567310333252, 263.33590817451477, 267.41508531570435, 271.4973132610321, 275.57954120635986, 279.657687664032, 283.7358341217041, 287.8217353820801, 291.90763664245605, 295.99160385131836, 300.07557106018066, 304.16786885261536, 308.26016664505005, 312.3468232154846, 316.4334797859192, 320.5394923686981, 324.64550495147705, 328.75199794769287, 332.8584909439087, 336.953590631485, 341.0486903190613, 345.1447026729584, 349.24071502685547, 353.3508450984955, 357.4609751701355, 361.5738911628723, 365.68680715560913, 369.7850923538208, 373.88337755203247, 377.9783549308777, 382.0733323097229, 386.17406392097473, 390.27479553222656, 394.37758350372314, 398.4803714752197, 402.5829885005951, 406.68560552597046, 410.770384311676, 414.8551630973816, 418.9456202983856, 423.03607749938965, 427.10477924346924, 431.1734809875488, 435.27379846572876, 439.3741159439087, 443.4443414211273, 447.51456689834595, 451.6017425060272, 455.6889181137085, 459.76236963272095, 463.8358211517334, 468.24745059013367, 472.65908002853394, 476.8483622074127, 481.0376443862915, 485.2029721736908, 489.3682999610901, 493.5277376174927, 497.68717527389526, 501.8568842411041, 506.026593208313, 510.2087275981903, 514.3908619880676, 518.5614326000214, 522.7320032119751, 527.5288815498352, 532.3257598876953, 537.1281561851501, 541.930552482605, 546.7964992523193, 551.6624460220337, 556.5537014007568, 561.44495677948, 565.9955410957336, 570.5461254119873, 574.8078393936157, 579.0695533752441, 583.2887983322144, 587.5080432891846, 591.7491874694824, 595.9903316497803, 600.2270021438599, 604.4636726379395, 608.6766531467438, 612.8896336555481, 617.14865899086, 621.4076843261719, 625.6476428508759, 629.8876013755798, 634.0946650505066, 638.3017287254333, 642.5131747722626, 646.7246208190918, 650.9367480278015, 655.1488752365112, 659.3569343090057, 663.5649933815002, 667.7681295871735, 671.9712657928467, 676.2081687450409, 680.4450716972351, 684.6493263244629, 688.8535809516907, 693.035242319107, 697.2169036865234, 701.3605434894562, 705.5041832923889, 709.6227972507477, 713.7414112091064, 717.8408370018005, 721.9402627944946, 726.0745530128479, 730.2088432312012, 734.3567092418671, 738.504575252533, 742.6081700325012, 746.7117648124695, 750.8395402431488, 754.9673156738281, 759.0755870342255, 763.1838583946228, 767.293083190918, 771.4023079872131, 775.5125000476837, 779.6226921081543, 783.7222626209259, 787.8218331336975, 791.9274303913116, 796.0330276489258, 800.1533734798431, 804.2737193107605, 808.3922774791718, 812.510835647583, 816.6372587680817, 820.7636818885803, 824.8761746883392, 828.9886674880981, 833.100921869278, 837.2131762504578, 839.2805244922638, 841.3478727340698]
[25.4625, 25.4625, 32.0, 32.0, 36.8375, 36.8375, 40.255, 40.255, 43.6, 43.6, 45.2275, 45.2275, 47.4625, 47.4625, 48.4875, 48.4875, 50.025, 50.025, 50.4725, 50.4725, 55.2075, 55.2075, 57.455, 57.455, 58.035, 58.035, 59.545, 59.545, 60.48, 60.48, 61.5125, 61.5125, 62.365, 62.365, 64.025, 64.025, 64.2325, 64.2325, 64.5125, 64.5125, 65.37, 65.37, 65.3925, 65.3925, 65.5975, 65.5975, 66.355, 66.355, 66.7825, 66.7825, 67.0075, 67.0075, 67.5725, 67.5725, 67.6475, 67.6475, 68.035, 68.035, 67.795, 67.795, 68.1125, 68.1125, 68.5825, 68.5825, 68.6125, 68.6125, 69.06, 69.06, 68.635, 68.635, 68.635, 68.635, 69.31, 69.31, 69.425, 69.425, 69.565, 69.565, 69.885, 69.885, 69.975, 69.975, 70.065, 70.065, 70.2375, 70.2375, 70.115, 70.115, 69.7575, 69.7575, 70.1325, 70.1325, 70.1475, 70.1475, 70.5225, 70.5225, 70.525, 70.525, 70.58, 70.58, 70.475, 70.475, 70.7075, 70.7075, 70.91, 70.91, 70.785, 70.785, 71.18, 71.18, 71.345, 71.345, 71.3875, 71.3875, 71.4975, 71.4975, 71.5575, 71.5575, 71.5825, 71.5825, 71.535, 71.535, 71.5875, 71.5875, 71.835, 71.835, 72.0425, 72.0425, 71.9875, 71.9875, 72.325, 72.325, 72.31, 72.31, 72.2975, 72.2975, 72.5125, 72.5125, 72.715, 72.715, 72.63, 72.63, 72.4625, 72.4625, 72.3075, 72.3075, 72.65, 72.65, 72.7025, 72.7025, 72.8625, 72.8625, 72.615, 72.615, 72.4625, 72.4625, 72.6, 72.6, 72.4675, 72.4675, 72.4325, 72.4325, 72.5075, 72.5075, 72.385, 72.385, 72.185, 72.185, 72.3775, 72.3775, 72.36, 72.36, 72.6325, 72.6325, 72.5575, 72.5575, 72.58, 72.58, 72.355, 72.355, 72.31, 72.31, 72.665, 72.665, 72.975, 72.975, 73.335, 73.335, 73.3175, 73.3175, 73.61, 73.61, 73.5475, 73.5475, 73.725, 73.725, 73.405, 73.405, 73.2775, 73.2775, 73.96, 73.96]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.197, Test loss: 2.218, Test accuracy: 18.57
Round   1, Train loss: 1.930, Test loss: 2.079, Test accuracy: 24.46
Round   2, Train loss: 1.755, Test loss: 1.909, Test accuracy: 31.52
Round   3, Train loss: 1.620, Test loss: 1.767, Test accuracy: 36.52
Round   4, Train loss: 1.547, Test loss: 1.634, Test accuracy: 42.50
Round   5, Train loss: 1.446, Test loss: 1.587, Test accuracy: 43.94
Round   6, Train loss: 1.432, Test loss: 1.482, Test accuracy: 48.56
Round   7, Train loss: 1.341, Test loss: 1.449, Test accuracy: 49.47
Round   8, Train loss: 1.321, Test loss: 1.341, Test accuracy: 52.46
Round   9, Train loss: 1.270, Test loss: 1.293, Test accuracy: 54.31
Round  10, Train loss: 1.243, Test loss: 1.213, Test accuracy: 56.80
Round  11, Train loss: 1.194, Test loss: 1.180, Test accuracy: 58.27
Round  12, Train loss: 1.163, Test loss: 1.142, Test accuracy: 60.34
Round  13, Train loss: 1.126, Test loss: 1.091, Test accuracy: 62.18
Round  14, Train loss: 1.092, Test loss: 1.063, Test accuracy: 62.64
Round  15, Train loss: 1.069, Test loss: 1.005, Test accuracy: 65.20
Round  16, Train loss: 1.024, Test loss: 0.996, Test accuracy: 65.65
Round  17, Train loss: 1.013, Test loss: 0.972, Test accuracy: 66.47
Round  18, Train loss: 0.982, Test loss: 0.959, Test accuracy: 66.90
Round  19, Train loss: 0.981, Test loss: 0.945, Test accuracy: 67.36
Round  20, Train loss: 0.961, Test loss: 0.922, Test accuracy: 67.96
Round  21, Train loss: 0.924, Test loss: 0.909, Test accuracy: 68.17
Round  22, Train loss: 0.928, Test loss: 0.895, Test accuracy: 68.79
Round  23, Train loss: 0.888, Test loss: 0.882, Test accuracy: 69.61
Round  24, Train loss: 0.883, Test loss: 0.871, Test accuracy: 70.47
Round  25, Train loss: 0.873, Test loss: 0.875, Test accuracy: 70.03
Round  26, Train loss: 0.858, Test loss: 0.859, Test accuracy: 70.94
Round  27, Train loss: 0.840, Test loss: 0.853, Test accuracy: 70.91
Round  28, Train loss: 0.829, Test loss: 0.834, Test accuracy: 71.38
Round  29, Train loss: 0.826, Test loss: 0.838, Test accuracy: 71.31
Round  30, Train loss: 0.807, Test loss: 0.823, Test accuracy: 71.89
Round  31, Train loss: 0.794, Test loss: 0.825, Test accuracy: 72.13
Round  32, Train loss: 0.768, Test loss: 0.828, Test accuracy: 72.24
Round  33, Train loss: 0.801, Test loss: 0.807, Test accuracy: 72.69
Round  34, Train loss: 0.771, Test loss: 0.804, Test accuracy: 72.77
Round  35, Train loss: 0.753, Test loss: 0.809, Test accuracy: 72.61
Round  36, Train loss: 0.741, Test loss: 0.811, Test accuracy: 72.80
Round  37, Train loss: 0.748, Test loss: 0.806, Test accuracy: 72.94
Round  38, Train loss: 0.747, Test loss: 0.807, Test accuracy: 72.99
Round  39, Train loss: 0.742, Test loss: 0.796, Test accuracy: 72.83
Round  40, Train loss: 0.725, Test loss: 0.783, Test accuracy: 73.58
Round  41, Train loss: 0.709, Test loss: 0.785, Test accuracy: 73.56
Round  42, Train loss: 0.709, Test loss: 0.787, Test accuracy: 73.70
Round  43, Train loss: 0.714, Test loss: 0.780, Test accuracy: 73.97
Round  44, Train loss: 0.697, Test loss: 0.787, Test accuracy: 73.72
Round  45, Train loss: 0.696, Test loss: 0.770, Test accuracy: 74.26
Round  46, Train loss: 0.677, Test loss: 0.770, Test accuracy: 74.53
Round  47, Train loss: 0.662, Test loss: 0.765, Test accuracy: 74.43
Round  48, Train loss: 0.654, Test loss: 0.762, Test accuracy: 74.53
Round  49, Train loss: 0.658, Test loss: 0.780, Test accuracy: 74.25
Round  50, Train loss: 0.672, Test loss: 0.770, Test accuracy: 74.50
Round  51, Train loss: 0.649, Test loss: 0.753, Test accuracy: 74.93
Round  52, Train loss: 0.646, Test loss: 0.757, Test accuracy: 74.82
Round  53, Train loss: 0.659, Test loss: 0.756, Test accuracy: 74.86
Round  54, Train loss: 0.651, Test loss: 0.754, Test accuracy: 74.63
Round  55, Train loss: 0.624, Test loss: 0.745, Test accuracy: 75.16
Round  56, Train loss: 0.617, Test loss: 0.747, Test accuracy: 75.38
Round  57, Train loss: 0.632, Test loss: 0.755, Test accuracy: 74.88
Round  58, Train loss: 0.601, Test loss: 0.751, Test accuracy: 75.00
Round  59, Train loss: 0.617, Test loss: 0.746, Test accuracy: 75.00
Round  60, Train loss: 0.597, Test loss: 0.757, Test accuracy: 74.86
Round  61, Train loss: 0.607, Test loss: 0.746, Test accuracy: 75.22
Round  62, Train loss: 0.601, Test loss: 0.743, Test accuracy: 75.72
Round  63, Train loss: 0.593, Test loss: 0.748, Test accuracy: 75.32
Round  64, Train loss: 0.587, Test loss: 0.743, Test accuracy: 75.62
Round  65, Train loss: 0.576, Test loss: 0.754, Test accuracy: 75.28
Round  66, Train loss: 0.603, Test loss: 0.746, Test accuracy: 75.67
Round  67, Train loss: 0.558, Test loss: 0.755, Test accuracy: 75.36
Round  68, Train loss: 0.584, Test loss: 0.750, Test accuracy: 75.69
Round  69, Train loss: 0.575, Test loss: 0.755, Test accuracy: 75.54
Round  70, Train loss: 0.563, Test loss: 0.757, Test accuracy: 75.72
Round  71, Train loss: 0.556, Test loss: 0.759, Test accuracy: 75.16
Round  72, Train loss: 0.530, Test loss: 0.761, Test accuracy: 75.03
Round  73, Train loss: 0.587, Test loss: 0.745, Test accuracy: 75.71
Round  74, Train loss: 0.561, Test loss: 0.755, Test accuracy: 75.58
Round  75, Train loss: 0.556, Test loss: 0.751, Test accuracy: 75.68
Round  76, Train loss: 0.556, Test loss: 0.750, Test accuracy: 75.78
Round  77, Train loss: 0.564, Test loss: 0.746, Test accuracy: 76.03
Round  78, Train loss: 0.541, Test loss: 0.760, Test accuracy: 75.32
Round  79, Train loss: 0.550, Test loss: 0.751, Test accuracy: 75.88
Round  80, Train loss: 0.547, Test loss: 0.755, Test accuracy: 75.82
Round  81, Train loss: 0.526, Test loss: 0.751, Test accuracy: 75.66
Round  82, Train loss: 0.525, Test loss: 0.758, Test accuracy: 75.81
Round  83, Train loss: 0.539, Test loss: 0.746, Test accuracy: 76.28
Round  84, Train loss: 0.552, Test loss: 0.773, Test accuracy: 75.64
Round  85, Train loss: 0.534, Test loss: 0.745, Test accuracy: 76.27
Round  86, Train loss: 0.523, Test loss: 0.757, Test accuracy: 76.04
Round  87, Train loss: 0.512, Test loss: 0.753, Test accuracy: 76.03
Round  88, Train loss: 0.514, Test loss: 0.745, Test accuracy: 76.06
Round  89, Train loss: 0.512, Test loss: 0.756, Test accuracy: 75.73
Round  90, Train loss: 0.516, Test loss: 0.755, Test accuracy: 75.94
Round  91, Train loss: 0.504, Test loss: 0.750, Test accuracy: 76.33
Round  92, Train loss: 0.513, Test loss: 0.758, Test accuracy: 76.50
Round  93, Train loss: 0.497, Test loss: 0.757, Test accuracy: 76.03
Round  94, Train loss: 0.501, Test loss: 0.773, Test accuracy: 75.75
Round  95, Train loss: 0.510, Test loss: 0.751, Test accuracy: 76.55
Round  96, Train loss: 0.486, Test loss: 0.746, Test accuracy: 76.73
Round  97, Train loss: 0.466, Test loss: 0.751, Test accuracy: 76.42
Round  98, Train loss: 0.487, Test loss: 0.761, Test accuracy: 76.49
Round  99, Train loss: 0.494, Test loss: 0.761, Test accuracy: 76.08
Final Round, Train loss: 0.414, Test loss: 0.748, Test accuracy: 76.36
Average accuracy final 10 rounds: 76.2815 

4609.3691842556
[4.4559032917022705, 8.911806583404541, 13.186498641967773, 17.461190700531006, 21.7758207321167, 26.090450763702393, 30.395989179611206, 34.70152759552002, 38.95577526092529, 43.210022926330566, 47.48325181007385, 51.75648069381714, 56.02633213996887, 60.296183586120605, 64.6273775100708, 68.958571434021, 73.24385046958923, 77.52912950515747, 81.86062788963318, 86.19212627410889, 90.48973155021667, 94.78733682632446, 99.04229784011841, 103.29725885391235, 107.53593611717224, 111.77461338043213, 116.04046583175659, 120.30631828308105, 124.52974343299866, 128.75316858291626, 132.97219467163086, 137.19122076034546, 141.37235498428345, 145.55348920822144, 149.4248354434967, 153.29618167877197, 157.10789251327515, 160.91960334777832, 164.78252482414246, 168.6454463005066, 172.5056130886078, 176.36577987670898, 180.22809863090515, 184.09041738510132, 187.94623279571533, 191.80204820632935, 195.61527800559998, 199.4285078048706, 203.23933863639832, 207.05016946792603, 210.88418340682983, 214.71819734573364, 218.55681610107422, 222.3954348564148, 226.2189600467682, 230.04248523712158, 233.85688996315002, 237.67129468917847, 241.50780487060547, 245.34431505203247, 249.1588170528412, 252.9733190536499, 256.7765483856201, 260.57977771759033, 264.37472105026245, 268.16966438293457, 272.0692067146301, 275.9687490463257, 279.8596692085266, 283.75058937072754, 287.64489698410034, 291.53920459747314, 295.4058401584625, 299.2724757194519, 303.1484076976776, 307.0243396759033, 311.3091924190521, 315.5940451622009, 319.88156056404114, 324.16907596588135, 328.50150656700134, 332.83393716812134, 337.1884505748749, 341.5429639816284, 345.8691511154175, 350.19533824920654, 354.5106427669525, 358.8259472846985, 363.1814134120941, 367.53687953948975, 371.88043236732483, 376.2239851951599, 380.55650639533997, 384.88902759552, 389.2392418384552, 393.5894560813904, 397.91930866241455, 402.2491612434387, 406.58427238464355, 410.9193835258484, 415.24266171455383, 419.5659399032593, 423.91049575805664, 428.255051612854, 432.5814914703369, 436.9079313278198, 441.23538994789124, 445.56284856796265, 449.50902795791626, 453.4552073478699, 457.3914942741394, 461.32778120040894, 465.247088432312, 469.1663956642151, 473.09902000427246, 477.03164434432983, 481.00738191604614, 484.98311948776245, 488.94613790512085, 492.90915632247925, 496.87343406677246, 500.8377118110657, 504.8247356414795, 508.8117594718933, 512.7904703617096, 516.7691812515259, 520.7312936782837, 524.6934061050415, 528.6742513179779, 532.6550965309143, 536.6443040370941, 540.6335115432739, 544.6406791210175, 548.647846698761, 552.6391546726227, 556.6304626464844, 560.5916118621826, 564.5527610778809, 568.6350405216217, 572.7173199653625, 576.6762800216675, 580.6352400779724, 584.6053211688995, 588.5754022598267, 592.6149423122406, 596.6544823646545, 601.0672113895416, 605.4799404144287, 609.9297666549683, 614.3795928955078, 618.8595099449158, 623.3394269943237, 627.7159655094147, 632.0925040245056, 636.469884634018, 640.8472652435303, 644.8817160129547, 648.9161667823792, 653.0465519428253, 657.1769371032715, 661.2468900680542, 665.3168430328369, 669.319794178009, 673.3227453231812, 677.4358289241791, 681.548912525177, 685.6062355041504, 689.6635584831238, 693.7612705230713, 697.8589825630188, 701.8884770870209, 705.917971611023, 710.1014595031738, 714.2849473953247, 718.5393624305725, 722.7937774658203, 727.0176525115967, 731.241527557373, 735.4074847698212, 739.5734419822693, 743.7569723129272, 747.9405026435852, 752.0101644992828, 756.0798263549805, 760.042218208313, 764.0046100616455, 768.1792922019958, 772.3539743423462, 776.5823240280151, 780.8106737136841, 785.0211293697357, 789.2315850257874, 793.4605844020844, 797.6895837783813, 801.8602652549744, 806.0309467315674, 810.2646889686584, 814.4984312057495, 818.7052597999573, 822.912088394165, 824.9298686981201, 826.9476490020752]
[18.5675, 18.5675, 24.4625, 24.4625, 31.52, 31.52, 36.515, 36.515, 42.4975, 42.4975, 43.935, 43.935, 48.565, 48.565, 49.465, 49.465, 52.4625, 52.4625, 54.3125, 54.3125, 56.795, 56.795, 58.265, 58.265, 60.34, 60.34, 62.1825, 62.1825, 62.64, 62.64, 65.2025, 65.2025, 65.6475, 65.6475, 66.47, 66.47, 66.8975, 66.8975, 67.3575, 67.3575, 67.9575, 67.9575, 68.17, 68.17, 68.79, 68.79, 69.6125, 69.6125, 70.4725, 70.4725, 70.0275, 70.0275, 70.94, 70.94, 70.91, 70.91, 71.3775, 71.3775, 71.3075, 71.3075, 71.8875, 71.8875, 72.1275, 72.1275, 72.2425, 72.2425, 72.685, 72.685, 72.77, 72.77, 72.6075, 72.6075, 72.8025, 72.8025, 72.945, 72.945, 72.9925, 72.9925, 72.8325, 72.8325, 73.585, 73.585, 73.56, 73.56, 73.705, 73.705, 73.9675, 73.9675, 73.715, 73.715, 74.2625, 74.2625, 74.53, 74.53, 74.4325, 74.4325, 74.5275, 74.5275, 74.245, 74.245, 74.5, 74.5, 74.93, 74.93, 74.8225, 74.8225, 74.8575, 74.8575, 74.6325, 74.6325, 75.1575, 75.1575, 75.3825, 75.3825, 74.88, 74.88, 75.0025, 75.0025, 75.0025, 75.0025, 74.855, 74.855, 75.215, 75.215, 75.725, 75.725, 75.3225, 75.3225, 75.6225, 75.6225, 75.28, 75.28, 75.675, 75.675, 75.36, 75.36, 75.695, 75.695, 75.5425, 75.5425, 75.725, 75.725, 75.16, 75.16, 75.025, 75.025, 75.71, 75.71, 75.5825, 75.5825, 75.6825, 75.6825, 75.7775, 75.7775, 76.0275, 76.0275, 75.32, 75.32, 75.8825, 75.8825, 75.82, 75.82, 75.655, 75.655, 75.81, 75.81, 76.275, 76.275, 75.635, 75.635, 76.27, 76.27, 76.0375, 76.0375, 76.03, 76.03, 76.0575, 76.0575, 75.735, 75.735, 75.935, 75.935, 76.33, 76.33, 76.5, 76.5, 76.025, 76.025, 75.7525, 75.7525, 76.5475, 76.5475, 76.735, 76.735, 76.42, 76.42, 76.49, 76.49, 76.08, 76.08, 76.365, 76.365]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.228, Test loss: 2.224, Test accuracy: 19.21
Round   1, Train loss: 1.971, Test loss: 2.080, Test accuracy: 24.63
Round   2, Train loss: 1.781, Test loss: 1.932, Test accuracy: 30.91
Round   3, Train loss: 1.675, Test loss: 1.759, Test accuracy: 37.30
Round   4, Train loss: 1.569, Test loss: 1.685, Test accuracy: 40.83
Round   5, Train loss: 1.506, Test loss: 1.561, Test accuracy: 45.34
Round   6, Train loss: 1.440, Test loss: 1.461, Test accuracy: 49.08
Round   7, Train loss: 1.384, Test loss: 1.344, Test accuracy: 53.17
Round   8, Train loss: 1.332, Test loss: 1.284, Test accuracy: 55.88
Round   9, Train loss: 1.278, Test loss: 1.230, Test accuracy: 57.41
Round  10, Train loss: 1.254, Test loss: 1.151, Test accuracy: 60.54
Round  11, Train loss: 1.188, Test loss: 1.128, Test accuracy: 61.63
Round  12, Train loss: 1.176, Test loss: 1.094, Test accuracy: 62.49
Round  13, Train loss: 1.134, Test loss: 1.058, Test accuracy: 63.74
Round  14, Train loss: 1.088, Test loss: 1.084, Test accuracy: 62.45
Round  15, Train loss: 1.091, Test loss: 1.035, Test accuracy: 64.36
Round  16, Train loss: 1.066, Test loss: 0.988, Test accuracy: 66.40
Round  17, Train loss: 1.039, Test loss: 0.943, Test accuracy: 68.06
Round  18, Train loss: 1.001, Test loss: 0.957, Test accuracy: 67.80
Round  19, Train loss: 1.003, Test loss: 0.930, Test accuracy: 68.33
Round  20, Train loss: 0.969, Test loss: 0.915, Test accuracy: 69.06
Round  21, Train loss: 0.942, Test loss: 0.903, Test accuracy: 69.63
Round  22, Train loss: 0.942, Test loss: 0.872, Test accuracy: 70.33
Round  23, Train loss: 0.924, Test loss: 0.873, Test accuracy: 70.77
Round  24, Train loss: 0.895, Test loss: 0.856, Test accuracy: 71.23
Round  25, Train loss: 0.895, Test loss: 0.837, Test accuracy: 72.02
Round  26, Train loss: 0.873, Test loss: 0.834, Test accuracy: 72.31
Round  27, Train loss: 0.862, Test loss: 0.827, Test accuracy: 72.36
Round  28, Train loss: 0.854, Test loss: 0.837, Test accuracy: 71.99
Round  29, Train loss: 0.841, Test loss: 0.808, Test accuracy: 72.89
Round  30, Train loss: 0.825, Test loss: 0.811, Test accuracy: 72.72
Round  31, Train loss: 0.804, Test loss: 0.799, Test accuracy: 73.28
Round  32, Train loss: 0.805, Test loss: 0.811, Test accuracy: 72.81
Round  33, Train loss: 0.811, Test loss: 0.792, Test accuracy: 73.58
Round  34, Train loss: 0.787, Test loss: 0.784, Test accuracy: 73.89
Round  35, Train loss: 0.770, Test loss: 0.776, Test accuracy: 73.69
Round  36, Train loss: 0.774, Test loss: 0.773, Test accuracy: 74.36
Round  37, Train loss: 0.780, Test loss: 0.769, Test accuracy: 74.39
Round  38, Train loss: 0.756, Test loss: 0.756, Test accuracy: 74.69
Round  39, Train loss: 0.759, Test loss: 0.758, Test accuracy: 74.75
Round  40, Train loss: 0.743, Test loss: 0.751, Test accuracy: 74.88
Round  41, Train loss: 0.728, Test loss: 0.744, Test accuracy: 75.33
Round  42, Train loss: 0.700, Test loss: 0.746, Test accuracy: 75.27
Round  43, Train loss: 0.704, Test loss: 0.741, Test accuracy: 75.28
Round  44, Train loss: 0.725, Test loss: 0.735, Test accuracy: 75.41
Round  45, Train loss: 0.712, Test loss: 0.734, Test accuracy: 75.48
Round  46, Train loss: 0.695, Test loss: 0.731, Test accuracy: 75.69
Round  47, Train loss: 0.698, Test loss: 0.728, Test accuracy: 75.83
Round  48, Train loss: 0.677, Test loss: 0.727, Test accuracy: 76.25
Round  49, Train loss: 0.654, Test loss: 0.729, Test accuracy: 75.78
Round  50, Train loss: 0.663, Test loss: 0.737, Test accuracy: 75.55
Round  51, Train loss: 0.664, Test loss: 0.729, Test accuracy: 75.91
Round  52, Train loss: 0.648, Test loss: 0.722, Test accuracy: 76.10
Round  53, Train loss: 0.654, Test loss: 0.731, Test accuracy: 75.97
Round  54, Train loss: 0.652, Test loss: 0.726, Test accuracy: 75.97
Round  55, Train loss: 0.617, Test loss: 0.724, Test accuracy: 75.87
Round  56, Train loss: 0.632, Test loss: 0.724, Test accuracy: 76.10
Round  57, Train loss: 0.644, Test loss: 0.726, Test accuracy: 76.24
Round  58, Train loss: 0.605, Test loss: 0.723, Test accuracy: 76.11
Round  59, Train loss: 0.617, Test loss: 0.724, Test accuracy: 76.15
Round  60, Train loss: 0.588, Test loss: 0.722, Test accuracy: 76.18
Round  61, Train loss: 0.608, Test loss: 0.715, Test accuracy: 76.28
Round  62, Train loss: 0.636, Test loss: 0.719, Test accuracy: 76.56
Round  63, Train loss: 0.608, Test loss: 0.720, Test accuracy: 76.58
Round  64, Train loss: 0.608, Test loss: 0.726, Test accuracy: 76.01
Round  65, Train loss: 0.636, Test loss: 0.712, Test accuracy: 76.99
Round  66, Train loss: 0.587, Test loss: 0.707, Test accuracy: 76.98
Round  67, Train loss: 0.613, Test loss: 0.701, Test accuracy: 76.83
Round  68, Train loss: 0.613, Test loss: 0.702, Test accuracy: 77.14
Round  69, Train loss: 0.576, Test loss: 0.716, Test accuracy: 76.69
Round  70, Train loss: 0.592, Test loss: 0.712, Test accuracy: 77.00
Round  71, Train loss: 0.575, Test loss: 0.711, Test accuracy: 76.66
Round  72, Train loss: 0.572, Test loss: 0.713, Test accuracy: 77.08
Round  73, Train loss: 0.574, Test loss: 0.711, Test accuracy: 76.84
Round  74, Train loss: 0.559, Test loss: 0.715, Test accuracy: 76.75
Round  75, Train loss: 0.582, Test loss: 0.713, Test accuracy: 76.67
Round  76, Train loss: 0.582, Test loss: 0.696, Test accuracy: 77.08
Round  77, Train loss: 0.588, Test loss: 0.696, Test accuracy: 77.28
Round  78, Train loss: 0.548, Test loss: 0.694, Test accuracy: 77.45
Round  79, Train loss: 0.552, Test loss: 0.693, Test accuracy: 77.43
Round  80, Train loss: 0.567, Test loss: 0.690, Test accuracy: 77.88
Round  81, Train loss: 0.541, Test loss: 0.710, Test accuracy: 77.11
Round  82, Train loss: 0.555, Test loss: 0.696, Test accuracy: 77.67
Round  83, Train loss: 0.567, Test loss: 0.691, Test accuracy: 77.65
Round  84, Train loss: 0.584, Test loss: 0.689, Test accuracy: 77.71
Round  85, Train loss: 0.532, Test loss: 0.697, Test accuracy: 77.31
Round  86, Train loss: 0.556, Test loss: 0.691, Test accuracy: 77.74
Round  87, Train loss: 0.524, Test loss: 0.696, Test accuracy: 77.67
Round  88, Train loss: 0.521, Test loss: 0.705, Test accuracy: 77.37
Round  89, Train loss: 0.523, Test loss: 0.705, Test accuracy: 77.02
Round  90, Train loss: 0.552, Test loss: 0.705, Test accuracy: 77.35
Round  91, Train loss: 0.521, Test loss: 0.694, Test accuracy: 77.22
Round  92, Train loss: 0.547, Test loss: 0.692, Test accuracy: 77.43
Round  93, Train loss: 0.523, Test loss: 0.692, Test accuracy: 77.77
Round  94, Train loss: 0.539, Test loss: 0.701, Test accuracy: 77.36
Round  95, Train loss: 0.525, Test loss: 0.705, Test accuracy: 77.18
Round  96, Train loss: 0.528, Test loss: 0.694, Test accuracy: 77.42
Round  97, Train loss: 0.536, Test loss: 0.699, Test accuracy: 77.12
Round  98, Train loss: 0.506, Test loss: 0.696, Test accuracy: 77.50
Round  99, Train loss: 0.505, Test loss: 0.693, Test accuracy: 77.53
Final Round, Train loss: 0.423, Test loss: 0.691, Test accuracy: 77.56
Average accuracy final 10 rounds: 77.38675
5399.276287317276
[6.259947299957275, 12.51989459991455, 18.29837656021118, 24.076858520507812, 29.83973979949951, 35.60262107849121, 41.233389377593994, 46.86415767669678, 52.34848499298096, 57.83281230926514, 63.38507008552551, 68.93732786178589, 74.40858626365662, 79.87984466552734, 85.63309907913208, 91.38635349273682, 96.98941540718079, 102.59247732162476, 108.09056377410889, 113.58865022659302, 118.88585948944092, 124.18306875228882, 129.44813299179077, 134.71319723129272, 139.99590158462524, 145.27860593795776, 150.6065537929535, 155.93450164794922, 161.22997450828552, 166.52544736862183, 171.85022497177124, 177.17500257492065, 182.48966336250305, 187.80432415008545, 193.0830900669098, 198.36185598373413, 203.58638215065002, 208.81090831756592, 214.0235002040863, 219.2360920906067, 224.47093033790588, 229.70576858520508, 235.02553153038025, 240.34529447555542, 245.6410892009735, 250.9368839263916, 256.2500660419464, 261.5632481575012, 266.84686970710754, 272.13049125671387, 277.38386392593384, 282.6372365951538, 287.93896889686584, 293.2407011985779, 298.5133216381073, 303.7859420776367, 309.0600438117981, 314.3341455459595, 319.5826737880707, 324.8312020301819, 330.0924623012543, 335.35372257232666, 340.81446290016174, 346.2752032279968, 351.5399112701416, 356.8046193122864, 362.0696301460266, 367.33464097976685, 372.62475657463074, 377.91487216949463, 383.18108558654785, 388.4472990036011, 393.711074590683, 398.9748501777649, 404.26956510543823, 409.5642800331116, 414.84838676452637, 420.13249349594116, 425.43500304222107, 430.737512588501, 436.0258114337921, 441.31411027908325, 446.58051466941833, 451.8469190597534, 457.1016249656677, 462.35633087158203, 467.62249541282654, 472.88865995407104, 478.4458212852478, 484.00298261642456, 489.2650616168976, 494.5271406173706, 499.8075473308563, 505.08795404434204, 510.3599286079407, 515.6319031715393, 520.9177322387695, 526.2035613059998, 531.5102941989899, 536.81702709198, 542.1172108650208, 547.4173946380615, 552.6681761741638, 557.9189577102661, 563.1599705219269, 568.4009833335876, 573.6237943172455, 578.8466053009033, 584.199191570282, 589.5517778396606, 594.8248460292816, 600.0979142189026, 605.6649894714355, 611.2320647239685, 616.475429058075, 621.7187933921814, 627.1803479194641, 632.6419024467468, 637.8902688026428, 643.1386351585388, 648.3971784114838, 653.6557216644287, 658.9160883426666, 664.1764550209045, 669.6846587657928, 675.1928625106812, 680.4437186717987, 685.6945748329163, 690.9577238559723, 696.2208728790283, 701.8175106048584, 707.4141483306885, 712.6571707725525, 717.9001932144165, 723.1470189094543, 728.3938446044922, 733.6229453086853, 738.8520460128784, 744.1221408843994, 749.3922357559204, 754.6829855442047, 759.973735332489, 765.2270457744598, 770.4803562164307, 775.7646720409393, 781.048987865448, 786.3131630420685, 791.577338218689, 796.813894033432, 802.050449848175, 807.4113421440125, 812.7722344398499, 817.9659860134125, 823.1597375869751, 828.3472368717194, 833.5347361564636, 838.7333996295929, 843.9320631027222, 849.13978266716, 854.3475022315979, 859.5492174625397, 864.7509326934814, 869.9673461914062, 875.183759689331, 880.3892343044281, 885.5947089195251, 890.7988767623901, 896.0030446052551, 901.2127964496613, 906.4225482940674, 911.6392183303833, 916.8558883666992, 922.0667419433594, 927.2775955200195, 932.5077271461487, 937.7378587722778, 942.9452772140503, 948.1526956558228, 953.3756680488586, 958.5986404418945, 963.8144040107727, 969.0301675796509, 974.2473649978638, 979.4645624160767, 984.6798436641693, 989.895124912262, 995.0960686206818, 1000.2970123291016, 1005.5579540729523, 1010.818895816803, 1016.0167825222015, 1021.2146692276001, 1026.421134710312, 1031.6276001930237, 1036.8484964370728, 1042.0693926811218, 1047.290246963501, 1052.5111012458801, 1057.7279624938965, 1062.9448237419128, 1065.059377193451, 1067.173930644989]
[19.2125, 19.2125, 24.63, 24.63, 30.905, 30.905, 37.3, 37.3, 40.83, 40.83, 45.3425, 45.3425, 49.075, 49.075, 53.1725, 53.1725, 55.885, 55.885, 57.415, 57.415, 60.5425, 60.5425, 61.6325, 61.6325, 62.49, 62.49, 63.745, 63.745, 62.4475, 62.4475, 64.355, 64.355, 66.4, 66.4, 68.06, 68.06, 67.7975, 67.7975, 68.3275, 68.3275, 69.0575, 69.0575, 69.63, 69.63, 70.3325, 70.3325, 70.7725, 70.7725, 71.2275, 71.2275, 72.0175, 72.0175, 72.305, 72.305, 72.355, 72.355, 71.9875, 71.9875, 72.895, 72.895, 72.7175, 72.7175, 73.2825, 73.2825, 72.8125, 72.8125, 73.58, 73.58, 73.8925, 73.8925, 73.6925, 73.6925, 74.36, 74.36, 74.3875, 74.3875, 74.695, 74.695, 74.7525, 74.7525, 74.875, 74.875, 75.325, 75.325, 75.27, 75.27, 75.28, 75.28, 75.4075, 75.4075, 75.485, 75.485, 75.69, 75.69, 75.83, 75.83, 76.245, 76.245, 75.785, 75.785, 75.5525, 75.5525, 75.905, 75.905, 76.1, 76.1, 75.975, 75.975, 75.9725, 75.9725, 75.87, 75.87, 76.0975, 76.0975, 76.2425, 76.2425, 76.1075, 76.1075, 76.1475, 76.1475, 76.18, 76.18, 76.28, 76.28, 76.56, 76.56, 76.58, 76.58, 76.01, 76.01, 76.99, 76.99, 76.9775, 76.9775, 76.835, 76.835, 77.145, 77.145, 76.6875, 76.6875, 76.9975, 76.9975, 76.6575, 76.6575, 77.0775, 77.0775, 76.8375, 76.8375, 76.745, 76.745, 76.6675, 76.6675, 77.0825, 77.0825, 77.2775, 77.2775, 77.45, 77.45, 77.43, 77.43, 77.8775, 77.8775, 77.105, 77.105, 77.665, 77.665, 77.65, 77.65, 77.7125, 77.7125, 77.31, 77.31, 77.74, 77.74, 77.6675, 77.6675, 77.3725, 77.3725, 77.0225, 77.0225, 77.3475, 77.3475, 77.22, 77.22, 77.4275, 77.4275, 77.765, 77.765, 77.36, 77.36, 77.1775, 77.1775, 77.4175, 77.4175, 77.12, 77.12, 77.505, 77.505, 77.5275, 77.5275, 77.5575, 77.5575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.149, Test loss: 2.331, Test accuracy: 12.22
Round   1, Train loss: 1.036, Test loss: 2.467, Test accuracy: 17.87
Round   2, Train loss: 0.962, Test loss: 2.413, Test accuracy: 17.58
Round   3, Train loss: 0.869, Test loss: 2.252, Test accuracy: 20.97
Round   4, Train loss: 0.910, Test loss: 2.436, Test accuracy: 17.33
Round   5, Train loss: 0.767, Test loss: 2.439, Test accuracy: 20.75
Round   6, Train loss: 0.778, Test loss: 2.478, Test accuracy: 20.86
Round   7, Train loss: 0.691, Test loss: 2.437, Test accuracy: 16.76
Round   8, Train loss: 0.726, Test loss: 2.253, Test accuracy: 23.15
Round   9, Train loss: 0.720, Test loss: 2.401, Test accuracy: 18.79
Round  10, Train loss: 0.666, Test loss: 2.436, Test accuracy: 16.34
Round  11, Train loss: 0.611, Test loss: 2.283, Test accuracy: 22.70
Round  12, Train loss: 0.640, Test loss: 2.459, Test accuracy: 19.21
Round  13, Train loss: 0.598, Test loss: 2.372, Test accuracy: 24.06
Round  14, Train loss: 0.609, Test loss: 2.355, Test accuracy: 21.08
Round  15, Train loss: 0.657, Test loss: 2.554, Test accuracy: 18.63
Round  16, Train loss: 0.617, Test loss: 2.181, Test accuracy: 23.22
Round  17, Train loss: 0.557, Test loss: 2.563, Test accuracy: 22.33
Round  18, Train loss: 0.603, Test loss: 2.250, Test accuracy: 24.57
Round  19, Train loss: 0.549, Test loss: 2.194, Test accuracy: 23.89
Round  20, Train loss: 0.570, Test loss: 2.226, Test accuracy: 24.57
Round  21, Train loss: 0.546, Test loss: 2.546, Test accuracy: 19.43
Round  22, Train loss: 0.505, Test loss: 2.663, Test accuracy: 20.73
Round  23, Train loss: 0.565, Test loss: 2.323, Test accuracy: 18.57
Round  24, Train loss: 0.543, Test loss: 2.362, Test accuracy: 23.53
Round  25, Train loss: 0.477, Test loss: 2.437, Test accuracy: 21.97
Round  26, Train loss: 0.530, Test loss: 2.146, Test accuracy: 24.00
Round  27, Train loss: 0.435, Test loss: 2.264, Test accuracy: 24.54
Round  28, Train loss: 0.616, Test loss: 2.808, Test accuracy: 20.39
Round  29, Train loss: 0.543, Test loss: 2.446, Test accuracy: 20.73
Round  30, Train loss: 0.480, Test loss: 2.333, Test accuracy: 24.75
Round  31, Train loss: 0.435, Test loss: 2.293, Test accuracy: 25.12
Round  32, Train loss: 0.463, Test loss: 2.389, Test accuracy: 27.27
Round  33, Train loss: 0.443, Test loss: 2.333, Test accuracy: 26.41
Round  34, Train loss: 0.466, Test loss: 2.376, Test accuracy: 27.13
Round  35, Train loss: 0.469, Test loss: 2.893, Test accuracy: 27.18
Round  36, Train loss: 0.445, Test loss: 2.511, Test accuracy: 23.06
Round  37, Train loss: 0.441, Test loss: 2.493, Test accuracy: 20.88
Round  38, Train loss: 0.459, Test loss: 2.321, Test accuracy: 24.48
Round  39, Train loss: 0.378, Test loss: 2.399, Test accuracy: 27.09
Round  40, Train loss: 0.520, Test loss: 2.281, Test accuracy: 24.58
Round  41, Train loss: 0.382, Test loss: 2.628, Test accuracy: 26.93
Round  42, Train loss: 0.350, Test loss: 2.439, Test accuracy: 27.17
Round  43, Train loss: 0.345, Test loss: 2.767, Test accuracy: 22.53
Round  44, Train loss: 0.423, Test loss: 2.273, Test accuracy: 26.08
Round  45, Train loss: 0.450, Test loss: 2.547, Test accuracy: 20.58
Round  46, Train loss: 0.327, Test loss: 2.642, Test accuracy: 22.08
Round  47, Train loss: 0.357, Test loss: 2.754, Test accuracy: 24.28
Round  48, Train loss: 0.295, Test loss: 2.580, Test accuracy: 24.98
Round  49, Train loss: 0.281, Test loss: 2.429, Test accuracy: 26.07
Round  50, Train loss: 0.468, Test loss: 2.750, Test accuracy: 22.59
Round  51, Train loss: 0.345, Test loss: 2.508, Test accuracy: 25.59
Round  52, Train loss: 0.343, Test loss: 2.655, Test accuracy: 24.24
Round  53, Train loss: 0.379, Test loss: 2.736, Test accuracy: 25.27
Round  54, Train loss: 0.381, Test loss: 2.553, Test accuracy: 26.43
Round  55, Train loss: 0.331, Test loss: 2.572, Test accuracy: 26.30
Round  56, Train loss: 0.350, Test loss: 2.891, Test accuracy: 25.70
Round  57, Train loss: 0.325, Test loss: 2.734, Test accuracy: 26.79
Round  58, Train loss: 0.318, Test loss: 2.592, Test accuracy: 24.21
Round  59, Train loss: 0.239, Test loss: 2.613, Test accuracy: 24.34
Round  60, Train loss: 0.368, Test loss: 2.757, Test accuracy: 26.84
Round  61, Train loss: 0.364, Test loss: 2.726, Test accuracy: 24.93
Round  62, Train loss: 0.295, Test loss: 2.710, Test accuracy: 26.11
Round  63, Train loss: 0.272, Test loss: 2.646, Test accuracy: 24.49
Round  64, Train loss: 0.327, Test loss: 2.616, Test accuracy: 21.97
Round  65, Train loss: 0.315, Test loss: 2.674, Test accuracy: 23.37
Round  66, Train loss: 0.279, Test loss: 2.637, Test accuracy: 25.49
Round  67, Train loss: 0.311, Test loss: 2.507, Test accuracy: 23.88
Round  68, Train loss: 0.286, Test loss: 2.697, Test accuracy: 26.11
Round  69, Train loss: 0.300, Test loss: 2.664, Test accuracy: 26.03
Round  70, Train loss: 0.266, Test loss: 3.094, Test accuracy: 27.78
Round  71, Train loss: 0.255, Test loss: 2.907, Test accuracy: 24.99
Round  72, Train loss: 0.265, Test loss: 3.021, Test accuracy: 24.69
Round  73, Train loss: 0.311, Test loss: 2.682, Test accuracy: 23.94
Round  74, Train loss: 0.219, Test loss: 2.751, Test accuracy: 25.73
Round  75, Train loss: 0.330, Test loss: 2.403, Test accuracy: 25.17
Round  76, Train loss: 0.271, Test loss: 2.614, Test accuracy: 24.99
Round  77, Train loss: 0.326, Test loss: 2.753, Test accuracy: 24.35
Round  78, Train loss: 0.303, Test loss: 2.512, Test accuracy: 23.43
Round  79, Train loss: 0.223, Test loss: 2.615, Test accuracy: 27.20
Round  80, Train loss: 0.309, Test loss: 2.514, Test accuracy: 25.99
Round  81, Train loss: 0.264, Test loss: 2.581, Test accuracy: 26.38
Round  82, Train loss: 0.212, Test loss: 3.648, Test accuracy: 24.27
Round  83, Train loss: 0.265, Test loss: 2.932, Test accuracy: 25.93
Round  84, Train loss: 0.263, Test loss: 2.800, Test accuracy: 23.02
Round  85, Train loss: 0.228, Test loss: 2.800, Test accuracy: 24.63
Round  86, Train loss: 0.234, Test loss: 2.550, Test accuracy: 26.23
Round  87, Train loss: 0.275, Test loss: 2.554, Test accuracy: 27.25
Round  88, Train loss: 0.248, Test loss: 2.992, Test accuracy: 23.55
Round  89, Train loss: 0.206, Test loss: 2.974, Test accuracy: 22.89
Round  90, Train loss: 0.304, Test loss: 2.516, Test accuracy: 27.59
Round  91, Train loss: 0.235, Test loss: 3.185, Test accuracy: 23.30
Round  92, Train loss: 0.272, Test loss: 2.775, Test accuracy: 28.34
Round  93, Train loss: 0.184, Test loss: 2.880, Test accuracy: 22.87
Round  94, Train loss: 0.258, Test loss: 3.129, Test accuracy: 20.56
Round  95, Train loss: 0.174, Test loss: 3.013, Test accuracy: 26.25
Round  96, Train loss: 0.212, Test loss: 2.733, Test accuracy: 25.08
Round  97, Train loss: 0.226, Test loss: 2.825, Test accuracy: 25.05
Round  98, Train loss: 0.264, Test loss: 2.651, Test accuracy: 25.23
Round  99, Train loss: 0.171, Test loss: 3.100, Test accuracy: 26.76
Final Round, Train loss: 0.198, Test loss: 2.466, Test accuracy: 28.42
Average accuracy final 10 rounds: 25.1025
2464.333418369293
[3.5836002826690674, 7.182605981826782, 10.68037748336792, 14.17079782485962, 17.65501070022583, 21.139275789260864, 24.625611782073975, 28.098775386810303, 31.577597618103027, 35.06162452697754, 38.570515871047974, 42.09918403625488, 45.611324310302734, 49.098434925079346, 52.60504508018494, 56.1058988571167, 59.59307932853699, 63.09128546714783, 66.5801362991333, 70.09173679351807, 73.5970094203949, 77.07067441940308, 80.56103897094727, 84.03708338737488, 87.53539776802063, 91.0087640285492, 94.48749017715454, 97.99257278442383, 101.45320463180542, 104.9476249217987, 108.41278743743896, 111.89578938484192, 115.39675712585449, 118.85604929924011, 122.3378233909607, 125.8096694946289, 129.27657651901245, 132.768550157547, 136.22009682655334, 139.69849705696106, 143.17285895347595, 146.64065265655518, 150.1302454471588, 153.6037199497223, 157.0912001132965, 160.58164882659912, 164.0602765083313, 167.54699730873108, 171.04792189598083, 174.53505396842957, 178.02600169181824, 181.51063179969788, 185.009934425354, 188.48463225364685, 191.9758439064026, 195.6475908756256, 199.12758255004883, 202.60364437103271, 206.28727102279663, 209.97049069404602, 213.4393286705017, 216.92978596687317, 220.40284371376038, 223.87728190422058, 227.592444896698, 231.0715959072113, 234.56591844558716, 238.03993487358093, 241.52676367759705, 245.01504588127136, 248.48441290855408, 251.99292492866516, 255.46121644973755, 258.95311069488525, 262.4424443244934, 265.92317295074463, 269.4114263057709, 272.883150100708, 276.3720591068268, 279.85208439826965, 283.34405159950256, 286.82986783981323, 290.32027673721313, 293.808176279068, 297.29313611984253, 300.778769493103, 304.253586769104, 307.744069814682, 311.2443323135376, 314.75215888023376, 318.2567150592804, 321.75698924064636, 325.2687153816223, 328.76039814949036, 332.2672128677368, 335.78659176826477, 339.28377890586853, 342.7978835105896, 346.30382680892944, 349.7859561443329, 352.718861579895]
[12.216666666666667, 17.866666666666667, 17.583333333333332, 20.966666666666665, 17.333333333333332, 20.75, 20.858333333333334, 16.758333333333333, 23.15, 18.791666666666668, 16.341666666666665, 22.7, 19.208333333333332, 24.058333333333334, 21.083333333333332, 18.633333333333333, 23.216666666666665, 22.333333333333332, 24.566666666666666, 23.891666666666666, 24.566666666666666, 19.425, 20.725, 18.575, 23.533333333333335, 21.966666666666665, 24.0, 24.541666666666668, 20.391666666666666, 20.725, 24.75, 25.125, 27.266666666666666, 26.408333333333335, 27.133333333333333, 27.175, 23.058333333333334, 20.875, 24.475, 27.091666666666665, 24.583333333333332, 26.933333333333334, 27.166666666666668, 22.533333333333335, 26.083333333333332, 20.583333333333332, 22.083333333333332, 24.283333333333335, 24.975, 26.075, 22.591666666666665, 25.591666666666665, 24.241666666666667, 25.275, 26.433333333333334, 26.3, 25.7, 26.791666666666668, 24.208333333333332, 24.341666666666665, 26.841666666666665, 24.933333333333334, 26.108333333333334, 24.491666666666667, 21.966666666666665, 23.366666666666667, 25.491666666666667, 23.875, 26.108333333333334, 26.033333333333335, 27.783333333333335, 24.991666666666667, 24.691666666666666, 23.941666666666666, 25.725, 25.166666666666668, 24.991666666666667, 24.35, 23.425, 27.2, 25.991666666666667, 26.383333333333333, 24.275, 25.925, 23.016666666666666, 24.633333333333333, 26.233333333333334, 27.25, 23.55, 22.891666666666666, 27.591666666666665, 23.3, 28.341666666666665, 22.866666666666667, 20.558333333333334, 26.25, 25.083333333333332, 25.05, 25.225, 26.758333333333333, 28.416666666666668]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.318, Test loss: 2.316, Test accuracy: 4.28
Round   0, Global train loss: 2.318, Global test loss: 2.317, Global test accuracy: 4.58
Round   1, Train loss: 2.332, Test loss: 2.319, Test accuracy: 3.22
Round   1, Global train loss: 2.332, Global test loss: 2.317, Global test accuracy: 3.83
Round   2, Train loss: nan, Test loss: nan, Test accuracy: 9.45
Round   2, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   3, Train loss: nan, Test loss: nan, Test accuracy: 9.45
Round   3, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 11.96
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 13.55
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 11.69
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 11.69
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 11.67
Average accuracy final 10 rounds: 11.666666666666663 

Average global accuracy final 10 rounds: 11.666666666666663 

1567.7707641124725
[2.0960395336151123, 3.533062696456909, 5.116211891174316, 6.552274942398071, 8.000248670578003, 9.440993070602417, 10.882559299468994, 12.323697805404663, 13.774129867553711, 15.213843584060669, 16.648900508880615, 18.092535734176636, 19.5330867767334, 20.969534158706665, 22.40651273727417, 23.849693298339844, 25.282917022705078, 26.71648597717285, 28.154219150543213, 29.592254400253296, 31.03436803817749, 32.467602252960205, 33.906742811203, 35.34435772895813, 36.78134369850159, 38.22375822067261, 39.659303188323975, 41.097652196884155, 42.53238534927368, 43.96854615211487, 45.412628412246704, 46.91058802604675, 48.352415323257446, 49.794816970825195, 51.23338770866394, 52.67580580711365, 54.12053656578064, 55.564977169036865, 57.00447487831116, 58.44312381744385, 59.97849702835083, 61.55331993103027, 62.98983287811279, 64.43683958053589, 65.88036918640137, 67.31969165802002, 68.76195883750916, 70.19468116760254, 71.63471055030823, 73.07287073135376, 74.50914573669434, 75.93944191932678, 77.37846851348877, 78.81884551048279, 80.25682520866394, 81.69491291046143, 83.13277983665466, 84.57981824874878, 86.02016401290894, 87.4632682800293, 88.90182328224182, 90.33788895606995, 91.87130546569824, 93.31307649612427, 94.75182485580444, 96.1875901222229, 97.62048006057739, 99.14003777503967, 100.56892704963684, 101.99241209030151, 103.41886281967163, 104.84316873550415, 106.26763486862183, 107.70324873924255, 109.12824249267578, 110.55615615844727, 111.98363471031189, 113.41320133209229, 114.84610748291016, 116.37207174301147, 117.80864000320435, 119.23956108093262, 120.6670172214508, 122.10165405273438, 123.53554773330688, 124.99038553237915, 126.42221426963806, 127.85210514068604, 129.28811502456665, 130.74099326133728, 132.177818775177, 133.61012172698975, 135.03742003440857, 136.4711742401123, 137.91272282600403, 139.34933876991272, 140.78363966941833, 142.21978306770325, 143.6549723148346, 145.09113144874573, 147.48494744300842]
[4.283333333333333, 3.216666666666667, 9.45, 9.45, 11.958333333333334, 13.55, 11.691666666666666, 11.691666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666, 11.666666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.212, Test loss: 1.931, Test accuracy: 23.18
Round   0, Global train loss: 1.212, Global test loss: 2.290, Global test accuracy: 13.48
Round   1, Train loss: 1.014, Test loss: 1.653, Test accuracy: 32.48
Round   1, Global train loss: 1.014, Global test loss: 2.332, Global test accuracy: 12.82
Round   2, Train loss: 0.968, Test loss: 1.349, Test accuracy: 43.93
Round   2, Global train loss: 0.968, Global test loss: 2.142, Global test accuracy: 23.12
Round   3, Train loss: 0.970, Test loss: 1.273, Test accuracy: 47.82
Round   3, Global train loss: 0.970, Global test loss: 2.262, Global test accuracy: 21.77
Round   4, Train loss: 0.841, Test loss: 0.904, Test accuracy: 58.60
Round   4, Global train loss: 0.841, Global test loss: 1.931, Global test accuracy: 26.20
Round   5, Train loss: 0.891, Test loss: 0.889, Test accuracy: 61.45
Round   5, Global train loss: 0.891, Global test loss: 1.879, Global test accuracy: 31.98
Round   6, Train loss: 0.771, Test loss: 0.809, Test accuracy: 63.80
Round   6, Global train loss: 0.771, Global test loss: 1.806, Global test accuracy: 32.17
Round   7, Train loss: 0.731, Test loss: 0.750, Test accuracy: 66.28
Round   7, Global train loss: 0.731, Global test loss: 1.985, Global test accuracy: 31.13
Round   8, Train loss: 0.785, Test loss: 0.744, Test accuracy: 66.75
Round   8, Global train loss: 0.785, Global test loss: 1.993, Global test accuracy: 32.25
Round   9, Train loss: 0.820, Test loss: 0.738, Test accuracy: 67.43
Round   9, Global train loss: 0.820, Global test loss: 1.801, Global test accuracy: 34.72
Round  10, Train loss: 0.675, Test loss: 0.735, Test accuracy: 67.05
Round  10, Global train loss: 0.675, Global test loss: 1.757, Global test accuracy: 36.52
Round  11, Train loss: 0.714, Test loss: 0.704, Test accuracy: 68.83
Round  11, Global train loss: 0.714, Global test loss: 1.703, Global test accuracy: 40.23
Round  12, Train loss: 0.741, Test loss: 0.678, Test accuracy: 70.90
Round  12, Global train loss: 0.741, Global test loss: 1.684, Global test accuracy: 38.38
Round  13, Train loss: 0.691, Test loss: 0.678, Test accuracy: 70.88
Round  13, Global train loss: 0.691, Global test loss: 1.731, Global test accuracy: 40.40
Round  14, Train loss: 0.697, Test loss: 0.659, Test accuracy: 71.70
Round  14, Global train loss: 0.697, Global test loss: 1.742, Global test accuracy: 42.68
Round  15, Train loss: 0.678, Test loss: 0.677, Test accuracy: 71.72
Round  15, Global train loss: 0.678, Global test loss: 1.672, Global test accuracy: 40.20
Round  16, Train loss: 0.599, Test loss: 0.662, Test accuracy: 72.95
Round  16, Global train loss: 0.599, Global test loss: 1.926, Global test accuracy: 38.22
Round  17, Train loss: 0.605, Test loss: 0.659, Test accuracy: 73.30
Round  17, Global train loss: 0.605, Global test loss: 1.550, Global test accuracy: 43.10
Round  18, Train loss: 0.660, Test loss: 0.655, Test accuracy: 74.05
Round  18, Global train loss: 0.660, Global test loss: 1.580, Global test accuracy: 40.50
Round  19, Train loss: 0.598, Test loss: 0.645, Test accuracy: 74.17
Round  19, Global train loss: 0.598, Global test loss: 1.574, Global test accuracy: 45.98
Round  20, Train loss: 0.614, Test loss: 0.643, Test accuracy: 73.70
Round  20, Global train loss: 0.614, Global test loss: 1.417, Global test accuracy: 51.22
Round  21, Train loss: 0.614, Test loss: 0.640, Test accuracy: 74.03
Round  21, Global train loss: 0.614, Global test loss: 1.498, Global test accuracy: 47.02
Round  22, Train loss: 0.601, Test loss: 0.655, Test accuracy: 73.70
Round  22, Global train loss: 0.601, Global test loss: 1.457, Global test accuracy: 47.78
Round  23, Train loss: 0.649, Test loss: 0.657, Test accuracy: 73.87
Round  23, Global train loss: 0.649, Global test loss: 1.488, Global test accuracy: 47.62
Round  24, Train loss: 0.575, Test loss: 0.647, Test accuracy: 74.37
Round  24, Global train loss: 0.575, Global test loss: 1.432, Global test accuracy: 48.82
Round  25, Train loss: 0.646, Test loss: 0.623, Test accuracy: 75.55
Round  25, Global train loss: 0.646, Global test loss: 1.473, Global test accuracy: 47.12
Round  26, Train loss: 0.604, Test loss: 0.624, Test accuracy: 74.98
Round  26, Global train loss: 0.604, Global test loss: 1.649, Global test accuracy: 44.80
Round  27, Train loss: 0.529, Test loss: 0.633, Test accuracy: 74.92
Round  27, Global train loss: 0.529, Global test loss: 1.662, Global test accuracy: 43.82
Round  28, Train loss: 0.490, Test loss: 0.642, Test accuracy: 74.30
Round  28, Global train loss: 0.490, Global test loss: 1.443, Global test accuracy: 50.37
Round  29, Train loss: 0.519, Test loss: 0.627, Test accuracy: 75.35
Round  29, Global train loss: 0.519, Global test loss: 1.333, Global test accuracy: 53.12
Round  30, Train loss: 0.559, Test loss: 0.612, Test accuracy: 75.93
Round  30, Global train loss: 0.559, Global test loss: 1.451, Global test accuracy: 50.18
Round  31, Train loss: 0.611, Test loss: 0.600, Test accuracy: 76.33
Round  31, Global train loss: 0.611, Global test loss: 1.363, Global test accuracy: 53.83
Round  32, Train loss: 0.532, Test loss: 0.602, Test accuracy: 76.70
Round  32, Global train loss: 0.532, Global test loss: 1.421, Global test accuracy: 49.57
Round  33, Train loss: 0.527, Test loss: 0.610, Test accuracy: 76.53
Round  33, Global train loss: 0.527, Global test loss: 1.443, Global test accuracy: 49.55
Round  34, Train loss: 0.493, Test loss: 0.605, Test accuracy: 76.63
Round  34, Global train loss: 0.493, Global test loss: 1.479, Global test accuracy: 50.67
Round  35, Train loss: 0.517, Test loss: 0.606, Test accuracy: 76.85
Round  35, Global train loss: 0.517, Global test loss: 1.507, Global test accuracy: 47.25
Round  36, Train loss: 0.486, Test loss: 0.597, Test accuracy: 77.20
Round  36, Global train loss: 0.486, Global test loss: 1.499, Global test accuracy: 52.65
Round  37, Train loss: 0.531, Test loss: 0.594, Test accuracy: 76.97
Round  37, Global train loss: 0.531, Global test loss: 1.375, Global test accuracy: 51.92
Round  38, Train loss: 0.450, Test loss: 0.580, Test accuracy: 77.43
Round  38, Global train loss: 0.450, Global test loss: 1.365, Global test accuracy: 52.68
Round  39, Train loss: 0.421, Test loss: 0.593, Test accuracy: 76.68
Round  39, Global train loss: 0.421, Global test loss: 1.552, Global test accuracy: 52.07
Round  40, Train loss: 0.442, Test loss: 0.581, Test accuracy: 77.22
Round  40, Global train loss: 0.442, Global test loss: 1.505, Global test accuracy: 52.07
Round  41, Train loss: 0.439, Test loss: 0.580, Test accuracy: 77.35
Round  41, Global train loss: 0.439, Global test loss: 1.528, Global test accuracy: 51.45
Round  42, Train loss: 0.424, Test loss: 0.568, Test accuracy: 78.10
Round  42, Global train loss: 0.424, Global test loss: 1.707, Global test accuracy: 46.92
Round  43, Train loss: 0.446, Test loss: 0.568, Test accuracy: 78.20
Round  43, Global train loss: 0.446, Global test loss: 1.287, Global test accuracy: 55.63
Round  44, Train loss: 0.417, Test loss: 0.579, Test accuracy: 77.97
Round  44, Global train loss: 0.417, Global test loss: 2.019, Global test accuracy: 40.93
Round  45, Train loss: 0.436, Test loss: 0.598, Test accuracy: 77.67
Round  45, Global train loss: 0.436, Global test loss: 1.602, Global test accuracy: 49.77
Round  46, Train loss: 0.410, Test loss: 0.591, Test accuracy: 77.92
Round  46, Global train loss: 0.410, Global test loss: 1.741, Global test accuracy: 45.77
Round  47, Train loss: 0.472, Test loss: 0.584, Test accuracy: 78.37
Round  47, Global train loss: 0.472, Global test loss: 1.420, Global test accuracy: 53.53
Round  48, Train loss: 0.403, Test loss: 0.588, Test accuracy: 78.27
Round  48, Global train loss: 0.403, Global test loss: 1.435, Global test accuracy: 48.87
Round  49, Train loss: 0.428, Test loss: 0.563, Test accuracy: 79.17
Round  49, Global train loss: 0.428, Global test loss: 1.620, Global test accuracy: 50.18
Round  50, Train loss: 0.340, Test loss: 0.551, Test accuracy: 79.68
Round  50, Global train loss: 0.340, Global test loss: 1.459, Global test accuracy: 52.77
Round  51, Train loss: 0.403, Test loss: 0.544, Test accuracy: 79.85
Round  51, Global train loss: 0.403, Global test loss: 1.706, Global test accuracy: 48.60
Round  52, Train loss: 0.401, Test loss: 0.557, Test accuracy: 79.52
Round  52, Global train loss: 0.401, Global test loss: 1.402, Global test accuracy: 53.47
Round  53, Train loss: 0.378, Test loss: 0.559, Test accuracy: 79.35
Round  53, Global train loss: 0.378, Global test loss: 1.436, Global test accuracy: 51.83
Round  54, Train loss: 0.399, Test loss: 0.586, Test accuracy: 78.47
Round  54, Global train loss: 0.399, Global test loss: 1.279, Global test accuracy: 57.12
Round  55, Train loss: 0.465, Test loss: 0.584, Test accuracy: 78.57
Round  55, Global train loss: 0.465, Global test loss: 1.315, Global test accuracy: 54.33
Round  56, Train loss: 0.370, Test loss: 0.586, Test accuracy: 78.75
Round  56, Global train loss: 0.370, Global test loss: 1.315, Global test accuracy: 55.30
Round  57, Train loss: 0.455, Test loss: 0.566, Test accuracy: 79.27
Round  57, Global train loss: 0.455, Global test loss: 1.318, Global test accuracy: 55.33
Round  58, Train loss: 0.378, Test loss: 0.555, Test accuracy: 79.82
Round  58, Global train loss: 0.378, Global test loss: 1.395, Global test accuracy: 56.42
Round  59, Train loss: 0.414, Test loss: 0.556, Test accuracy: 79.52
Round  59, Global train loss: 0.414, Global test loss: 1.426, Global test accuracy: 53.58
Round  60, Train loss: 0.421, Test loss: 0.561, Test accuracy: 79.77
Round  60, Global train loss: 0.421, Global test loss: 1.301, Global test accuracy: 58.95
Round  61, Train loss: 0.364, Test loss: 0.555, Test accuracy: 79.82
Round  61, Global train loss: 0.364, Global test loss: 1.248, Global test accuracy: 58.18
Round  62, Train loss: 0.305, Test loss: 0.568, Test accuracy: 79.88
Round  62, Global train loss: 0.305, Global test loss: 1.650, Global test accuracy: 51.75
Round  63, Train loss: 0.273, Test loss: 0.572, Test accuracy: 79.82
Round  63, Global train loss: 0.273, Global test loss: 1.447, Global test accuracy: 55.78
Round  64, Train loss: 0.310, Test loss: 0.576, Test accuracy: 79.68
Round  64, Global train loss: 0.310, Global test loss: 1.429, Global test accuracy: 56.40
Round  65, Train loss: 0.329, Test loss: 0.571, Test accuracy: 79.92
Round  65, Global train loss: 0.329, Global test loss: 1.303, Global test accuracy: 58.08
Round  66, Train loss: 0.356, Test loss: 0.582, Test accuracy: 79.70
Round  66, Global train loss: 0.356, Global test loss: 1.486, Global test accuracy: 57.20
Round  67, Train loss: 0.290, Test loss: 0.588, Test accuracy: 79.75
Round  67, Global train loss: 0.290, Global test loss: 1.572, Global test accuracy: 57.20
Round  68, Train loss: 0.420, Test loss: 0.597, Test accuracy: 79.17
Round  68, Global train loss: 0.420, Global test loss: 1.300, Global test accuracy: 55.52
Round  69, Train loss: 0.319, Test loss: 0.601, Test accuracy: 79.13
Round  69, Global train loss: 0.319, Global test loss: 1.361, Global test accuracy: 58.52
Round  70, Train loss: 0.320, Test loss: 0.613, Test accuracy: 79.28
Round  70, Global train loss: 0.320, Global test loss: 1.414, Global test accuracy: 58.97
Round  71, Train loss: 0.360, Test loss: 0.625, Test accuracy: 79.43
Round  71, Global train loss: 0.360, Global test loss: 1.383, Global test accuracy: 58.10
Round  72, Train loss: 0.331, Test loss: 0.593, Test accuracy: 80.22
Round  72, Global train loss: 0.331, Global test loss: 1.269, Global test accuracy: 58.92
Round  73, Train loss: 0.290, Test loss: 0.573, Test accuracy: 80.28
Round  73, Global train loss: 0.290, Global test loss: 1.247, Global test accuracy: 59.40
Round  74, Train loss: 0.312, Test loss: 0.589, Test accuracy: 80.02
Round  74, Global train loss: 0.312, Global test loss: 1.241, Global test accuracy: 58.47
Round  75, Train loss: 0.307, Test loss: 0.584, Test accuracy: 80.30
Round  75, Global train loss: 0.307, Global test loss: 1.438, Global test accuracy: 56.93
Round  76, Train loss: 0.288, Test loss: 0.580, Test accuracy: 80.35
Round  76, Global train loss: 0.288, Global test loss: 1.391, Global test accuracy: 57.22
Round  77, Train loss: 0.349, Test loss: 0.575, Test accuracy: 80.27
Round  77, Global train loss: 0.349, Global test loss: 1.389, Global test accuracy: 56.00
Round  78, Train loss: 0.335, Test loss: 0.591, Test accuracy: 79.98
Round  78, Global train loss: 0.335, Global test loss: 1.343, Global test accuracy: 56.63
Round  79, Train loss: 0.238, Test loss: 0.595, Test accuracy: 80.08
Round  79, Global train loss: 0.238, Global test loss: 1.532, Global test accuracy: 55.90
Round  80, Train loss: 0.253, Test loss: 0.587, Test accuracy: 79.97
Round  80, Global train loss: 0.253, Global test loss: 1.515, Global test accuracy: 56.38
Round  81, Train loss: 0.289, Test loss: 0.600, Test accuracy: 80.18
Round  81, Global train loss: 0.289, Global test loss: 1.370, Global test accuracy: 59.00
Round  82, Train loss: 0.317, Test loss: 0.582, Test accuracy: 80.83
Round  82, Global train loss: 0.317, Global test loss: 1.436, Global test accuracy: 55.05
Round  83, Train loss: 0.306, Test loss: 0.596, Test accuracy: 80.43
Round  83, Global train loss: 0.306, Global test loss: 1.283, Global test accuracy: 59.00
Round  84, Train loss: 0.278, Test loss: 0.605, Test accuracy: 79.83
Round  84, Global train loss: 0.278, Global test loss: 1.435, Global test accuracy: 55.42
Round  85, Train loss: 0.273, Test loss: 0.618, Test accuracy: 79.68
Round  85, Global train loss: 0.273, Global test loss: 1.278, Global test accuracy: 57.38
Round  86, Train loss: 0.222, Test loss: 0.601, Test accuracy: 80.28
Round  86, Global train loss: 0.222, Global test loss: 1.479, Global test accuracy: 57.88
Round  87, Train loss: 0.332, Test loss: 0.597, Test accuracy: 80.18
Round  87, Global train loss: 0.332, Global test loss: 1.233, Global test accuracy: 60.37
Round  88, Train loss: 0.264, Test loss: 0.584, Test accuracy: 80.47
Round  88, Global train loss: 0.264, Global test loss: 1.207, Global test accuracy: 61.80
Round  89, Train loss: 0.283, Test loss: 0.603, Test accuracy: 80.28
Round  89, Global train loss: 0.283, Global test loss: 1.190, Global test accuracy: 62.55
Round  90, Train loss: 0.253, Test loss: 0.609, Test accuracy: 80.23
Round  90, Global train loss: 0.253, Global test loss: 1.416, Global test accuracy: 57.83
Round  91, Train loss: 0.260, Test loss: 0.621, Test accuracy: 80.18
Round  91, Global train loss: 0.260, Global test loss: 1.500, Global test accuracy: 56.38
Round  92, Train loss: 0.236, Test loss: 0.623, Test accuracy: 79.85
Round  92, Global train loss: 0.236, Global test loss: 1.274, Global test accuracy: 58.75
Round  93, Train loss: 0.245, Test loss: 0.616, Test accuracy: 80.18
Round  93, Global train loss: 0.245, Global test loss: 1.305, Global test accuracy: 59.65
Round  94, Train loss: 0.260, Test loss: 0.606, Test accuracy: 80.70
Round  94, Global train loss: 0.260, Global test loss: 1.385, Global test accuracy: 59.35
Round  95, Train loss: 0.275, Test loss: 0.604, Test accuracy: 80.65
Round  95, Global train loss: 0.275, Global test loss: 1.403, Global test accuracy: 56.23
Round  96, Train loss: 0.217, Test loss: 0.608, Test accuracy: 80.95
Round  96, Global train loss: 0.217, Global test loss: 1.473, Global test accuracy: 58.02
Round  97, Train loss: 0.219, Test loss: 0.635, Test accuracy: 80.43
Round  97, Global train loss: 0.219, Global test loss: 1.391, Global test accuracy: 60.23
Round  98, Train loss: 0.182, Test loss: 0.648, Test accuracy: 80.15
Round  98, Global train loss: 0.182, Global test loss: 1.429, Global test accuracy: 60.10
Round  99, Train loss: 0.290, Test loss: 0.640, Test accuracy: 80.35
Round  99, Global train loss: 0.290, Global test loss: 1.546, Global test accuracy: 58.00
Final Round, Train loss: 0.201, Test loss: 0.639, Test accuracy: 81.50
Final Round, Global train loss: 0.201, Global test loss: 1.546, Global test accuracy: 58.00
Average accuracy final 10 rounds: 80.36833333333334 

Average global accuracy final 10 rounds: 58.455 

905.163868188858
[0.9954988956451416, 1.9909977912902832, 2.648843765258789, 3.306689739227295, 3.958127975463867, 4.6095662117004395, 5.265370845794678, 5.921175479888916, 6.571780681610107, 7.222385883331299, 7.8771538734436035, 8.531921863555908, 9.259351968765259, 9.98678207397461, 10.645467519760132, 11.304152965545654, 11.961829662322998, 12.619506359100342, 13.276994228363037, 13.934482097625732, 14.592674255371094, 15.250866413116455, 15.909489393234253, 16.56811237335205, 17.228415727615356, 17.888719081878662, 18.542949199676514, 19.197179317474365, 19.852612495422363, 20.50804567337036, 21.15656614303589, 21.805086612701416, 22.456694841384888, 23.10830307006836, 23.763745069503784, 24.41918706893921, 25.074530601501465, 25.72987413406372, 26.385951280593872, 27.042028427124023, 27.697949171066284, 28.353869915008545, 29.004119396209717, 29.65436887741089, 30.310539722442627, 30.966710567474365, 31.61882734298706, 32.270944118499756, 32.935362100601196, 33.59978008270264, 34.25387263298035, 34.90796518325806, 35.56257510185242, 36.21718502044678, 36.87026500701904, 37.52334499359131, 38.17552375793457, 38.82770252227783, 39.482054471969604, 40.13640642166138, 40.78207015991211, 41.42773389816284, 42.082730531692505, 42.73772716522217, 43.38459062576294, 44.03145408630371, 44.68517780303955, 45.33890151977539, 45.99395823478699, 46.649014949798584, 47.301318645477295, 47.953622341156006, 48.60771989822388, 49.26181745529175, 49.915241718292236, 50.568665981292725, 51.22275447845459, 51.876842975616455, 52.53244066238403, 53.18803834915161, 53.82216286659241, 54.4562873840332, 55.10914945602417, 55.76201152801514, 56.41289138793945, 57.06377124786377, 57.707629919052124, 58.35148859024048, 59.01470899581909, 59.677929401397705, 60.328736305236816, 60.97954320907593, 61.630996227264404, 62.28244924545288, 62.93162298202515, 63.58079671859741, 64.23483920097351, 64.88888168334961, 65.53882336616516, 66.18876504898071, 66.85264348983765, 67.51652193069458, 68.17153024673462, 68.82653856277466, 69.48112607002258, 70.13571357727051, 70.79063057899475, 71.445547580719, 72.10258388519287, 72.75962018966675, 73.41931772232056, 74.07901525497437, 74.73085570335388, 75.3826961517334, 76.03498601913452, 76.68727588653564, 77.35057520866394, 78.01387453079224, 78.6729371547699, 79.33199977874756, 79.99004125595093, 80.6480827331543, 81.30470895767212, 81.96133518218994, 82.59386873245239, 83.22640228271484, 83.88294982910156, 84.53949737548828, 85.18951344490051, 85.83952951431274, 86.4932951927185, 87.14706087112427, 87.79936218261719, 88.45166349411011, 89.12465190887451, 89.79764032363892, 90.44143652915955, 91.08523273468018, 91.74465084075928, 92.40406894683838, 93.06296849250793, 93.72186803817749, 94.37707781791687, 95.03228759765625, 95.6940529346466, 96.35581827163696, 97.01264429092407, 97.66947031021118, 98.33213925361633, 98.99480819702148, 99.65689969062805, 100.31899118423462, 100.97131681442261, 101.6236424446106, 102.28275489807129, 102.94186735153198, 103.60322141647339, 104.2645754814148, 104.91879153251648, 105.57300758361816, 106.23219919204712, 106.89139080047607, 107.54079246520996, 108.19019412994385, 108.8453278541565, 109.50046157836914, 110.15034675598145, 110.80023193359375, 111.45586800575256, 112.11150407791138, 112.76019811630249, 113.4088921546936, 114.06502485275269, 114.72115755081177, 115.38152432441711, 116.04189109802246, 116.69495606422424, 117.34802103042603, 118.0002691745758, 118.65251731872559, 119.31667971611023, 119.98084211349487, 120.64089822769165, 121.30095434188843, 121.96312928199768, 122.62530422210693, 123.28352403640747, 123.94174385070801, 124.6058578491211, 125.26997184753418, 125.93295955657959, 126.595947265625, 127.2535891532898, 127.91123104095459, 128.57365083694458, 129.23607063293457, 129.89007830619812, 130.54408597946167, 131.2095079421997, 131.87492990493774, 133.2090938091278, 134.54325771331787]
[23.183333333333334, 23.183333333333334, 32.483333333333334, 32.483333333333334, 43.93333333333333, 43.93333333333333, 47.81666666666667, 47.81666666666667, 58.6, 58.6, 61.45, 61.45, 63.8, 63.8, 66.28333333333333, 66.28333333333333, 66.75, 66.75, 67.43333333333334, 67.43333333333334, 67.05, 67.05, 68.83333333333333, 68.83333333333333, 70.9, 70.9, 70.88333333333334, 70.88333333333334, 71.7, 71.7, 71.71666666666667, 71.71666666666667, 72.95, 72.95, 73.3, 73.3, 74.05, 74.05, 74.16666666666667, 74.16666666666667, 73.7, 73.7, 74.03333333333333, 74.03333333333333, 73.7, 73.7, 73.86666666666666, 73.86666666666666, 74.36666666666666, 74.36666666666666, 75.55, 75.55, 74.98333333333333, 74.98333333333333, 74.91666666666667, 74.91666666666667, 74.3, 74.3, 75.35, 75.35, 75.93333333333334, 75.93333333333334, 76.33333333333333, 76.33333333333333, 76.7, 76.7, 76.53333333333333, 76.53333333333333, 76.63333333333334, 76.63333333333334, 76.85, 76.85, 77.2, 77.2, 76.96666666666667, 76.96666666666667, 77.43333333333334, 77.43333333333334, 76.68333333333334, 76.68333333333334, 77.21666666666667, 77.21666666666667, 77.35, 77.35, 78.1, 78.1, 78.2, 78.2, 77.96666666666667, 77.96666666666667, 77.66666666666667, 77.66666666666667, 77.91666666666667, 77.91666666666667, 78.36666666666666, 78.36666666666666, 78.26666666666667, 78.26666666666667, 79.16666666666667, 79.16666666666667, 79.68333333333334, 79.68333333333334, 79.85, 79.85, 79.51666666666667, 79.51666666666667, 79.35, 79.35, 78.46666666666667, 78.46666666666667, 78.56666666666666, 78.56666666666666, 78.75, 78.75, 79.26666666666667, 79.26666666666667, 79.81666666666666, 79.81666666666666, 79.51666666666667, 79.51666666666667, 79.76666666666667, 79.76666666666667, 79.81666666666666, 79.81666666666666, 79.88333333333334, 79.88333333333334, 79.81666666666666, 79.81666666666666, 79.68333333333334, 79.68333333333334, 79.91666666666667, 79.91666666666667, 79.7, 79.7, 79.75, 79.75, 79.16666666666667, 79.16666666666667, 79.13333333333334, 79.13333333333334, 79.28333333333333, 79.28333333333333, 79.43333333333334, 79.43333333333334, 80.21666666666667, 80.21666666666667, 80.28333333333333, 80.28333333333333, 80.01666666666667, 80.01666666666667, 80.3, 80.3, 80.35, 80.35, 80.26666666666667, 80.26666666666667, 79.98333333333333, 79.98333333333333, 80.08333333333333, 80.08333333333333, 79.96666666666667, 79.96666666666667, 80.18333333333334, 80.18333333333334, 80.83333333333333, 80.83333333333333, 80.43333333333334, 80.43333333333334, 79.83333333333333, 79.83333333333333, 79.68333333333334, 79.68333333333334, 80.28333333333333, 80.28333333333333, 80.18333333333334, 80.18333333333334, 80.46666666666667, 80.46666666666667, 80.28333333333333, 80.28333333333333, 80.23333333333333, 80.23333333333333, 80.18333333333334, 80.18333333333334, 79.85, 79.85, 80.18333333333334, 80.18333333333334, 80.7, 80.7, 80.65, 80.65, 80.95, 80.95, 80.43333333333334, 80.43333333333334, 80.15, 80.15, 80.35, 80.35, 81.5, 81.5]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.592, Test loss: 2.541, Test accuracy: 20.80
Round   1, Train loss: 1.025, Test loss: 2.377, Test accuracy: 29.40
Round   2, Train loss: 0.915, Test loss: 2.284, Test accuracy: 35.20
Round   3, Train loss: 0.964, Test loss: 1.449, Test accuracy: 43.70
Round   4, Train loss: 0.938, Test loss: 1.361, Test accuracy: 48.65
Round   5, Train loss: 0.881, Test loss: 1.126, Test accuracy: 53.05
Round   6, Train loss: 0.892, Test loss: 1.042, Test accuracy: 57.12
Round   7, Train loss: 0.805, Test loss: 1.000, Test accuracy: 59.35
Round   8, Train loss: 0.819, Test loss: 0.940, Test accuracy: 59.95
Round   9, Train loss: 0.790, Test loss: 0.910, Test accuracy: 59.62
Round  10, Train loss: 0.786, Test loss: 0.859, Test accuracy: 61.25
Round  11, Train loss: 0.744, Test loss: 0.750, Test accuracy: 64.68
Round  12, Train loss: 0.768, Test loss: 0.728, Test accuracy: 66.93
Round  13, Train loss: 0.683, Test loss: 0.717, Test accuracy: 67.53
Round  14, Train loss: 0.634, Test loss: 0.700, Test accuracy: 68.42
Round  15, Train loss: 0.686, Test loss: 0.682, Test accuracy: 69.05
Round  16, Train loss: 0.656, Test loss: 0.685, Test accuracy: 68.53
Round  17, Train loss: 0.712, Test loss: 0.685, Test accuracy: 69.73
Round  18, Train loss: 0.584, Test loss: 0.669, Test accuracy: 70.72
Round  19, Train loss: 0.650, Test loss: 0.670, Test accuracy: 70.70
Round  20, Train loss: 0.702, Test loss: 0.664, Test accuracy: 70.80
Round  21, Train loss: 0.633, Test loss: 0.652, Test accuracy: 72.10
Round  22, Train loss: 0.572, Test loss: 0.652, Test accuracy: 72.15
Round  23, Train loss: 0.598, Test loss: 0.640, Test accuracy: 72.65
Round  24, Train loss: 0.644, Test loss: 0.616, Test accuracy: 73.57
Round  25, Train loss: 0.643, Test loss: 0.624, Test accuracy: 73.37
Round  26, Train loss: 0.674, Test loss: 0.599, Test accuracy: 74.15
Round  27, Train loss: 0.626, Test loss: 0.594, Test accuracy: 75.07
Round  28, Train loss: 0.531, Test loss: 0.580, Test accuracy: 75.22
Round  29, Train loss: 0.509, Test loss: 0.583, Test accuracy: 75.05
Round  30, Train loss: 0.642, Test loss: 0.589, Test accuracy: 75.10
Round  31, Train loss: 0.537, Test loss: 0.573, Test accuracy: 75.82
Round  32, Train loss: 0.636, Test loss: 0.567, Test accuracy: 76.12
Round  33, Train loss: 0.626, Test loss: 0.570, Test accuracy: 75.92
Round  34, Train loss: 0.617, Test loss: 0.565, Test accuracy: 76.27
Round  35, Train loss: 0.600, Test loss: 0.552, Test accuracy: 77.13
Round  36, Train loss: 0.474, Test loss: 0.549, Test accuracy: 77.20
Round  37, Train loss: 0.539, Test loss: 0.552, Test accuracy: 77.13
Round  38, Train loss: 0.497, Test loss: 0.549, Test accuracy: 76.95
Round  39, Train loss: 0.505, Test loss: 0.549, Test accuracy: 77.17
Round  40, Train loss: 0.558, Test loss: 0.532, Test accuracy: 78.05
Round  41, Train loss: 0.483, Test loss: 0.535, Test accuracy: 77.55
Round  42, Train loss: 0.579, Test loss: 0.544, Test accuracy: 77.42
Round  43, Train loss: 0.458, Test loss: 0.528, Test accuracy: 78.07
Round  44, Train loss: 0.548, Test loss: 0.526, Test accuracy: 78.47
Round  45, Train loss: 0.524, Test loss: 0.525, Test accuracy: 78.72
Round  46, Train loss: 0.463, Test loss: 0.516, Test accuracy: 78.85
Round  47, Train loss: 0.501, Test loss: 0.504, Test accuracy: 79.60
Round  48, Train loss: 0.529, Test loss: 0.507, Test accuracy: 79.57
Round  49, Train loss: 0.507, Test loss: 0.513, Test accuracy: 79.03
Round  50, Train loss: 0.551, Test loss: 0.515, Test accuracy: 78.98
Round  51, Train loss: 0.456, Test loss: 0.510, Test accuracy: 79.37
Round  52, Train loss: 0.460, Test loss: 0.509, Test accuracy: 79.05
Round  53, Train loss: 0.531, Test loss: 0.512, Test accuracy: 78.63
Round  54, Train loss: 0.457, Test loss: 0.498, Test accuracy: 79.27
Round  55, Train loss: 0.438, Test loss: 0.490, Test accuracy: 79.92
Round  56, Train loss: 0.410, Test loss: 0.491, Test accuracy: 79.42
Round  57, Train loss: 0.404, Test loss: 0.485, Test accuracy: 80.13
Round  58, Train loss: 0.401, Test loss: 0.489, Test accuracy: 79.93
Round  59, Train loss: 0.535, Test loss: 0.500, Test accuracy: 80.02
Round  60, Train loss: 0.393, Test loss: 0.490, Test accuracy: 80.58
Round  61, Train loss: 0.433, Test loss: 0.484, Test accuracy: 80.43
Round  62, Train loss: 0.421, Test loss: 0.487, Test accuracy: 80.02
Round  63, Train loss: 0.443, Test loss: 0.479, Test accuracy: 80.73
Round  64, Train loss: 0.365, Test loss: 0.475, Test accuracy: 81.23
Round  65, Train loss: 0.400, Test loss: 0.474, Test accuracy: 81.25
Round  66, Train loss: 0.434, Test loss: 0.482, Test accuracy: 80.53
Round  67, Train loss: 0.393, Test loss: 0.476, Test accuracy: 80.73
Round  68, Train loss: 0.335, Test loss: 0.480, Test accuracy: 80.65
Round  69, Train loss: 0.389, Test loss: 0.477, Test accuracy: 81.15
Round  70, Train loss: 0.387, Test loss: 0.477, Test accuracy: 80.82
Round  71, Train loss: 0.348, Test loss: 0.480, Test accuracy: 81.12
Round  72, Train loss: 0.355, Test loss: 0.497, Test accuracy: 80.28
Round  73, Train loss: 0.497, Test loss: 0.473, Test accuracy: 81.67
Round  74, Train loss: 0.375, Test loss: 0.484, Test accuracy: 80.93
Round  75, Train loss: 0.378, Test loss: 0.478, Test accuracy: 81.35
Round  76, Train loss: 0.335, Test loss: 0.468, Test accuracy: 81.45
Round  77, Train loss: 0.439, Test loss: 0.469, Test accuracy: 81.22
Round  78, Train loss: 0.380, Test loss: 0.468, Test accuracy: 81.27
Round  79, Train loss: 0.330, Test loss: 0.465, Test accuracy: 81.68
Round  80, Train loss: 0.372, Test loss: 0.469, Test accuracy: 82.15
Round  81, Train loss: 0.345, Test loss: 0.473, Test accuracy: 81.75
Round  82, Train loss: 0.304, Test loss: 0.472, Test accuracy: 81.83
Round  83, Train loss: 0.298, Test loss: 0.467, Test accuracy: 82.15
Round  84, Train loss: 0.388, Test loss: 0.477, Test accuracy: 81.42
Round  85, Train loss: 0.342, Test loss: 0.481, Test accuracy: 81.23
Round  86, Train loss: 0.302, Test loss: 0.476, Test accuracy: 81.58
Round  87, Train loss: 0.406, Test loss: 0.478, Test accuracy: 81.82
Round  88, Train loss: 0.327, Test loss: 0.461, Test accuracy: 82.08
Round  89, Train loss: 0.341, Test loss: 0.472, Test accuracy: 81.97
Round  90, Train loss: 0.411, Test loss: 0.470, Test accuracy: 81.93
Round  91, Train loss: 0.423, Test loss: 0.467, Test accuracy: 81.87
Round  92, Train loss: 0.347, Test loss: 0.473, Test accuracy: 81.57
Round  93, Train loss: 0.313, Test loss: 0.464, Test accuracy: 82.20
Round  94, Train loss: 0.329, Test loss: 0.466, Test accuracy: 82.15
Round  95, Train loss: 0.322, Test loss: 0.461, Test accuracy: 82.13
Round  96, Train loss: 0.363, Test loss: 0.465, Test accuracy: 82.48
Round  97, Train loss: 0.351, Test loss: 0.472, Test accuracy: 81.97
Round  98, Train loss: 0.375, Test loss: 0.467, Test accuracy: 82.42
Round  99, Train loss: 0.337, Test loss: 0.474, Test accuracy: 82.07
Final Round, Train loss: 0.267, Test loss: 0.467, Test accuracy: 82.55
Average accuracy final 10 rounds: 82.07833333333333 

724.0610873699188
[0.8934447765350342, 1.7868895530700684, 2.4080498218536377, 3.029210090637207, 3.72812557220459, 4.427041053771973, 5.048836946487427, 5.670632839202881, 6.300855398178101, 6.93107795715332, 7.557208776473999, 8.183339595794678, 8.808266639709473, 9.433193683624268, 10.0618896484375, 10.690585613250732, 11.318987607955933, 11.947389602661133, 12.578752756118774, 13.210115909576416, 13.841376304626465, 14.472636699676514, 15.091917753219604, 15.711198806762695, 16.412372589111328, 17.11354637145996, 17.74217438697815, 18.370802402496338, 18.99402117729187, 19.617239952087402, 20.23820972442627, 20.859179496765137, 21.48551845550537, 22.111857414245605, 22.74304986000061, 23.374242305755615, 23.999730348587036, 24.625218391418457, 25.246344804763794, 25.86747121810913, 26.499221563339233, 27.130971908569336, 27.761479377746582, 28.391986846923828, 29.023388624191284, 29.65479040145874, 30.288353204727173, 30.921916007995605, 31.55573558807373, 32.189555168151855, 32.82996368408203, 33.47037220001221, 34.10183835029602, 34.733304500579834, 35.36393404006958, 35.994563579559326, 36.64685320854187, 37.299142837524414, 37.97188878059387, 38.64463472366333, 39.30969285964966, 39.974750995635986, 40.603529930114746, 41.232308864593506, 41.85999631881714, 42.48768377304077, 43.0856716632843, 43.68365955352783, 44.313732624053955, 44.94380569458008, 45.57288336753845, 46.201961040496826, 46.81411671638489, 47.42627239227295, 48.04742383956909, 48.668575286865234, 49.295573472976685, 49.922571659088135, 50.548094749450684, 51.17361783981323, 51.79942607879639, 52.42523431777954, 53.05778646469116, 53.69033861160278, 54.313634395599365, 54.93693017959595, 55.56650376319885, 56.19607734680176, 56.82519865036011, 57.45431995391846, 58.079487323760986, 58.704654693603516, 59.328489542007446, 59.95232439041138, 60.57350516319275, 61.19468593597412, 61.820961236953735, 62.44723653793335, 63.074819564819336, 63.70240259170532, 64.32844424247742, 64.95448589324951, 65.5706238746643, 66.1867618560791, 66.81269598007202, 67.43863010406494, 68.06658673286438, 68.69454336166382, 69.30448865890503, 69.91443395614624, 70.53924989700317, 71.16406583786011, 71.7918655872345, 72.41966533660889, 73.02806901931763, 73.63647270202637, 74.26649832725525, 74.89652395248413, 75.52223563194275, 76.14794731140137, 76.77589535713196, 77.40384340286255, 78.02796244621277, 78.65208148956299, 79.27362632751465, 79.89517116546631, 80.51675581932068, 81.13834047317505, 81.76702404022217, 82.39570760726929, 83.02174091339111, 83.64777421951294, 84.27624106407166, 84.90470790863037, 85.53334927558899, 86.16199064254761, 86.79220795631409, 87.42242527008057, 88.04131746292114, 88.66020965576172, 89.28120851516724, 89.90220737457275, 90.52963733673096, 91.15706729888916, 91.79248261451721, 92.42789793014526, 93.05079984664917, 93.67370176315308, 94.29882526397705, 94.92394876480103, 95.54602408409119, 96.16809940338135, 96.79650950431824, 97.42491960525513, 98.05204439163208, 98.67916917800903, 99.28258514404297, 99.8860011100769, 100.51461291313171, 101.14322471618652, 101.77471113204956, 102.4061975479126, 103.01998090744019, 103.63376426696777, 104.2601306438446, 104.88649702072144, 105.51262378692627, 106.1387505531311, 106.76677560806274, 107.39480066299438, 108.02175903320312, 108.64871740341187, 109.2751989364624, 109.90168046951294, 110.53134727478027, 111.16101408004761, 111.78675103187561, 112.41248798370361, 113.03210473060608, 113.65172147750854, 114.28319048881531, 114.91465950012207, 115.5365948677063, 116.15853023529053, 116.78697633743286, 117.4154224395752, 118.04730343818665, 118.6791844367981, 119.31167316436768, 119.94416189193726, 120.5623927116394, 121.18062353134155, 121.80140995979309, 122.42219638824463, 123.04126858711243, 123.66034078598022, 124.28414916992188, 124.90795755386353, 125.52764821052551, 126.1473388671875, 127.32873892784119, 128.51013898849487]
[20.8, 20.8, 29.4, 29.4, 35.2, 35.2, 43.7, 43.7, 48.65, 48.65, 53.05, 53.05, 57.11666666666667, 57.11666666666667, 59.35, 59.35, 59.95, 59.95, 59.61666666666667, 59.61666666666667, 61.25, 61.25, 64.68333333333334, 64.68333333333334, 66.93333333333334, 66.93333333333334, 67.53333333333333, 67.53333333333333, 68.41666666666667, 68.41666666666667, 69.05, 69.05, 68.53333333333333, 68.53333333333333, 69.73333333333333, 69.73333333333333, 70.71666666666667, 70.71666666666667, 70.7, 70.7, 70.8, 70.8, 72.1, 72.1, 72.15, 72.15, 72.65, 72.65, 73.56666666666666, 73.56666666666666, 73.36666666666666, 73.36666666666666, 74.15, 74.15, 75.06666666666666, 75.06666666666666, 75.21666666666667, 75.21666666666667, 75.05, 75.05, 75.1, 75.1, 75.81666666666666, 75.81666666666666, 76.11666666666666, 76.11666666666666, 75.91666666666667, 75.91666666666667, 76.26666666666667, 76.26666666666667, 77.13333333333334, 77.13333333333334, 77.2, 77.2, 77.13333333333334, 77.13333333333334, 76.95, 76.95, 77.16666666666667, 77.16666666666667, 78.05, 78.05, 77.55, 77.55, 77.41666666666667, 77.41666666666667, 78.06666666666666, 78.06666666666666, 78.46666666666667, 78.46666666666667, 78.71666666666667, 78.71666666666667, 78.85, 78.85, 79.6, 79.6, 79.56666666666666, 79.56666666666666, 79.03333333333333, 79.03333333333333, 78.98333333333333, 78.98333333333333, 79.36666666666666, 79.36666666666666, 79.05, 79.05, 78.63333333333334, 78.63333333333334, 79.26666666666667, 79.26666666666667, 79.91666666666667, 79.91666666666667, 79.41666666666667, 79.41666666666667, 80.13333333333334, 80.13333333333334, 79.93333333333334, 79.93333333333334, 80.01666666666667, 80.01666666666667, 80.58333333333333, 80.58333333333333, 80.43333333333334, 80.43333333333334, 80.01666666666667, 80.01666666666667, 80.73333333333333, 80.73333333333333, 81.23333333333333, 81.23333333333333, 81.25, 81.25, 80.53333333333333, 80.53333333333333, 80.73333333333333, 80.73333333333333, 80.65, 80.65, 81.15, 81.15, 80.81666666666666, 80.81666666666666, 81.11666666666666, 81.11666666666666, 80.28333333333333, 80.28333333333333, 81.66666666666667, 81.66666666666667, 80.93333333333334, 80.93333333333334, 81.35, 81.35, 81.45, 81.45, 81.21666666666667, 81.21666666666667, 81.26666666666667, 81.26666666666667, 81.68333333333334, 81.68333333333334, 82.15, 82.15, 81.75, 81.75, 81.83333333333333, 81.83333333333333, 82.15, 82.15, 81.41666666666667, 81.41666666666667, 81.23333333333333, 81.23333333333333, 81.58333333333333, 81.58333333333333, 81.81666666666666, 81.81666666666666, 82.08333333333333, 82.08333333333333, 81.96666666666667, 81.96666666666667, 81.93333333333334, 81.93333333333334, 81.86666666666666, 81.86666666666666, 81.56666666666666, 81.56666666666666, 82.2, 82.2, 82.15, 82.15, 82.13333333333334, 82.13333333333334, 82.48333333333333, 82.48333333333333, 81.96666666666667, 81.96666666666667, 82.41666666666667, 82.41666666666667, 82.06666666666666, 82.06666666666666, 82.55, 82.55]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.711, Test loss: 2.162, Test accuracy: 19.80
Round   1, Train loss: 1.179, Test loss: 2.011, Test accuracy: 27.42
Round   2, Train loss: 1.056, Test loss: 1.594, Test accuracy: 40.23
Round   3, Train loss: 0.972, Test loss: 1.297, Test accuracy: 44.95
Round   4, Train loss: 0.975, Test loss: 1.139, Test accuracy: 52.47
Round   5, Train loss: 0.890, Test loss: 0.968, Test accuracy: 59.35
Round   6, Train loss: 0.846, Test loss: 0.923, Test accuracy: 62.75
Round   7, Train loss: 0.768, Test loss: 0.933, Test accuracy: 61.97
Round   8, Train loss: 0.838, Test loss: 0.892, Test accuracy: 63.20
Round   9, Train loss: 0.759, Test loss: 0.838, Test accuracy: 65.08
Round  10, Train loss: 0.727, Test loss: 0.836, Test accuracy: 65.63
Round  11, Train loss: 0.705, Test loss: 0.823, Test accuracy: 67.25
Round  12, Train loss: 0.751, Test loss: 0.812, Test accuracy: 66.77
Round  13, Train loss: 0.700, Test loss: 0.786, Test accuracy: 68.43
Round  14, Train loss: 0.688, Test loss: 0.706, Test accuracy: 71.13
Round  15, Train loss: 0.658, Test loss: 0.688, Test accuracy: 72.60
Round  16, Train loss: 0.631, Test loss: 0.660, Test accuracy: 72.77
Round  17, Train loss: 0.627, Test loss: 0.657, Test accuracy: 72.72
Round  18, Train loss: 0.671, Test loss: 0.650, Test accuracy: 73.30
Round  19, Train loss: 0.757, Test loss: 0.644, Test accuracy: 74.37
Round  20, Train loss: 0.627, Test loss: 0.648, Test accuracy: 74.27
Round  21, Train loss: 0.652, Test loss: 0.628, Test accuracy: 74.48
Round  22, Train loss: 0.595, Test loss: 0.608, Test accuracy: 75.05
Round  23, Train loss: 0.606, Test loss: 0.602, Test accuracy: 75.62
Round  24, Train loss: 0.543, Test loss: 0.598, Test accuracy: 75.73
Round  25, Train loss: 0.565, Test loss: 0.587, Test accuracy: 76.37
Round  26, Train loss: 0.610, Test loss: 0.600, Test accuracy: 76.38
Round  27, Train loss: 0.555, Test loss: 0.571, Test accuracy: 76.88
Round  28, Train loss: 0.594, Test loss: 0.566, Test accuracy: 77.92
Round  29, Train loss: 0.516, Test loss: 0.563, Test accuracy: 77.80
Round  30, Train loss: 0.483, Test loss: 0.570, Test accuracy: 78.07
Round  31, Train loss: 0.546, Test loss: 0.563, Test accuracy: 77.25
Round  32, Train loss: 0.481, Test loss: 0.561, Test accuracy: 78.08
Round  33, Train loss: 0.606, Test loss: 0.556, Test accuracy: 78.15
Round  34, Train loss: 0.563, Test loss: 0.544, Test accuracy: 78.23
Round  35, Train loss: 0.499, Test loss: 0.541, Test accuracy: 78.57
Round  36, Train loss: 0.624, Test loss: 0.537, Test accuracy: 78.45
Round  37, Train loss: 0.465, Test loss: 0.530, Test accuracy: 78.72
Round  38, Train loss: 0.545, Test loss: 0.522, Test accuracy: 79.23
Round  39, Train loss: 0.461, Test loss: 0.520, Test accuracy: 79.37
Round  40, Train loss: 0.392, Test loss: 0.516, Test accuracy: 79.37
Round  41, Train loss: 0.586, Test loss: 0.513, Test accuracy: 79.72
Round  42, Train loss: 0.538, Test loss: 0.510, Test accuracy: 79.88
Round  43, Train loss: 0.568, Test loss: 0.509, Test accuracy: 79.72
Round  44, Train loss: 0.529, Test loss: 0.498, Test accuracy: 80.33
Round  45, Train loss: 0.565, Test loss: 0.497, Test accuracy: 80.55
Round  46, Train loss: 0.506, Test loss: 0.492, Test accuracy: 79.95
Round  47, Train loss: 0.447, Test loss: 0.496, Test accuracy: 80.12
Round  48, Train loss: 0.385, Test loss: 0.494, Test accuracy: 80.47
Round  49, Train loss: 0.511, Test loss: 0.491, Test accuracy: 80.13
Round  50, Train loss: 0.427, Test loss: 0.479, Test accuracy: 80.57
Round  51, Train loss: 0.497, Test loss: 0.484, Test accuracy: 80.47
Round  52, Train loss: 0.495, Test loss: 0.479, Test accuracy: 80.82
Round  53, Train loss: 0.413, Test loss: 0.468, Test accuracy: 81.42
Round  54, Train loss: 0.521, Test loss: 0.472, Test accuracy: 81.07
Round  55, Train loss: 0.443, Test loss: 0.469, Test accuracy: 81.33
Round  56, Train loss: 0.427, Test loss: 0.467, Test accuracy: 82.02
Round  57, Train loss: 0.508, Test loss: 0.477, Test accuracy: 81.72
Round  58, Train loss: 0.458, Test loss: 0.472, Test accuracy: 81.50
Round  59, Train loss: 0.364, Test loss: 0.465, Test accuracy: 81.65
Round  60, Train loss: 0.372, Test loss: 0.463, Test accuracy: 82.05
Round  61, Train loss: 0.397, Test loss: 0.458, Test accuracy: 81.93
Round  62, Train loss: 0.438, Test loss: 0.458, Test accuracy: 82.25
Round  63, Train loss: 0.380, Test loss: 0.455, Test accuracy: 82.08
Round  64, Train loss: 0.515, Test loss: 0.473, Test accuracy: 81.05
Round  65, Train loss: 0.429, Test loss: 0.457, Test accuracy: 81.57
Round  66, Train loss: 0.331, Test loss: 0.451, Test accuracy: 82.78
Round  67, Train loss: 0.462, Test loss: 0.450, Test accuracy: 82.38
Round  68, Train loss: 0.375, Test loss: 0.446, Test accuracy: 82.23
Round  69, Train loss: 0.396, Test loss: 0.454, Test accuracy: 82.23
Round  70, Train loss: 0.392, Test loss: 0.444, Test accuracy: 82.87
Round  71, Train loss: 0.434, Test loss: 0.438, Test accuracy: 82.72
Round  72, Train loss: 0.376, Test loss: 0.438, Test accuracy: 82.75
Round  73, Train loss: 0.412, Test loss: 0.449, Test accuracy: 82.23
Round  74, Train loss: 0.387, Test loss: 0.440, Test accuracy: 82.45
Round  75, Train loss: 0.312, Test loss: 0.437, Test accuracy: 82.83
Round  76, Train loss: 0.398, Test loss: 0.432, Test accuracy: 83.02
Round  77, Train loss: 0.289, Test loss: 0.428, Test accuracy: 83.07
Round  78, Train loss: 0.332, Test loss: 0.431, Test accuracy: 83.22
Round  79, Train loss: 0.362, Test loss: 0.434, Test accuracy: 82.90
Round  80, Train loss: 0.337, Test loss: 0.430, Test accuracy: 83.10
Round  81, Train loss: 0.360, Test loss: 0.434, Test accuracy: 83.05
Round  82, Train loss: 0.381, Test loss: 0.427, Test accuracy: 83.03
Round  83, Train loss: 0.272, Test loss: 0.427, Test accuracy: 83.43
Round  84, Train loss: 0.355, Test loss: 0.429, Test accuracy: 83.17
Round  85, Train loss: 0.298, Test loss: 0.429, Test accuracy: 83.03
Round  86, Train loss: 0.346, Test loss: 0.439, Test accuracy: 82.73
Round  87, Train loss: 0.380, Test loss: 0.430, Test accuracy: 83.10
Round  88, Train loss: 0.391, Test loss: 0.433, Test accuracy: 82.68
Round  89, Train loss: 0.353, Test loss: 0.427, Test accuracy: 83.02
Round  90, Train loss: 0.327, Test loss: 0.430, Test accuracy: 83.28
Round  91, Train loss: 0.321, Test loss: 0.424, Test accuracy: 83.57
Round  92, Train loss: 0.317, Test loss: 0.424, Test accuracy: 83.38
Round  93, Train loss: 0.296, Test loss: 0.427, Test accuracy: 83.25
Round  94, Train loss: 0.321, Test loss: 0.423, Test accuracy: 83.42
Round  95, Train loss: 0.341, Test loss: 0.426, Test accuracy: 83.25
Round  96, Train loss: 0.300, Test loss: 0.429, Test accuracy: 83.07
Round  97, Train loss: 0.237, Test loss: 0.422, Test accuracy: 83.68
Round  98, Train loss: 0.253, Test loss: 0.430, Test accuracy: 83.10
Round  99, Train loss: 0.295, Test loss: 0.434, Test accuracy: 83.28
Final Round, Train loss: 0.255, Test loss: 0.423, Test accuracy: 83.85
Average accuracy final 10 rounds: 83.32833333333333
832.1918437480927
[1.1402370929718018, 2.2804741859436035, 3.1574645042419434, 4.034454822540283, 4.858562231063843, 5.682669639587402, 6.6156299114227295, 7.548590183258057, 8.37601900100708, 9.203447818756104, 10.05143666267395, 10.899425506591797, 11.726543188095093, 12.553660869598389, 13.372560262680054, 14.191459655761719, 15.0315523147583, 15.871644973754883, 16.675756692886353, 17.479868412017822, 18.29117226600647, 19.102476119995117, 19.90342617034912, 20.704376220703125, 21.51047945022583, 22.316582679748535, 23.120753526687622, 23.92492437362671, 24.706029653549194, 25.48713493347168, 26.326907873153687, 27.166680812835693, 28.00305414199829, 28.83942747116089, 29.669856786727905, 30.500286102294922, 31.29398798942566, 32.0876898765564, 32.88915705680847, 33.69062423706055, 34.49596619606018, 35.301308155059814, 36.102906942367554, 36.90450572967529, 37.68054747581482, 38.456589221954346, 39.25783324241638, 40.05907726287842, 40.84419012069702, 41.629302978515625, 42.426490783691406, 43.22367858886719, 44.02734661102295, 44.83101463317871, 45.63882088661194, 46.446627140045166, 47.23844814300537, 48.030269145965576, 48.810216426849365, 49.590163707733154, 50.375869274139404, 51.161574840545654, 51.951244592666626, 52.7409143447876, 53.53294897079468, 54.32498359680176, 55.12769889831543, 55.9304141998291, 56.730451583862305, 57.53048896789551, 58.323259592056274, 59.11603021621704, 59.90065598487854, 60.68528175354004, 61.47904634475708, 62.27281093597412, 63.065266370773315, 63.85772180557251, 64.65160799026489, 65.44549417495728, 66.24325442314148, 67.04101467132568, 67.83935594558716, 68.63769721984863, 69.43291568756104, 70.22813415527344, 71.02318572998047, 71.8182373046875, 72.61032032966614, 73.40240335464478, 74.18995547294617, 74.97750759124756, 75.77231478691101, 76.56712198257446, 77.36616635322571, 78.16521072387695, 78.96710753440857, 79.76900434494019, 80.55932354927063, 81.34964275360107, 82.1463782787323, 82.94311380386353, 83.73689365386963, 84.53067350387573, 85.32677817344666, 86.12288284301758, 86.91340327262878, 87.70392370223999, 88.5004494190216, 89.29697513580322, 90.12179398536682, 90.94661283493042, 91.80556225776672, 92.66451168060303, 93.52504897117615, 94.38558626174927, 95.23059296607971, 96.07559967041016, 96.93143820762634, 97.78727674484253, 98.58592557907104, 99.38457441329956, 100.17691540718079, 100.96925640106201, 101.76963257789612, 102.57000875473022, 103.36503219604492, 104.16005563735962, 104.95129084587097, 105.74252605438232, 106.54016256332397, 107.33779907226562, 108.19716739654541, 109.0565357208252, 109.88895392417908, 110.72137212753296, 111.52220368385315, 112.32303524017334, 113.1075668334961, 113.89209842681885, 114.68846917152405, 115.48483991622925, 116.2822437286377, 117.07964754104614, 117.87179517745972, 118.66394281387329, 119.46108293533325, 120.25822305679321, 121.04679131507874, 121.83535957336426, 122.62606978416443, 123.4167799949646, 124.2025580406189, 124.9883360862732, 125.84458827972412, 126.70084047317505, 127.52436256408691, 128.34788465499878, 129.14593625068665, 129.9439878463745, 130.740079164505, 131.5361704826355, 132.3338840007782, 133.1315975189209, 133.92408871650696, 134.71657991409302, 135.520165681839, 136.32375144958496, 137.10792756080627, 137.8921036720276, 138.68506336212158, 139.47802305221558, 140.2816722393036, 141.0853214263916, 141.88217520713806, 142.67902898788452, 143.4815137386322, 144.28399848937988, 145.07711744308472, 145.87023639678955, 146.66418194770813, 147.4581274986267, 148.25903844833374, 149.05994939804077, 149.85133910179138, 150.642728805542, 151.4443929195404, 152.24605703353882, 153.03604078292847, 153.82602453231812, 154.6627790927887, 155.49953365325928, 156.31843376159668, 157.13733386993408, 157.93655824661255, 158.73578262329102, 159.52744436264038, 160.31910610198975, 161.1135880947113, 161.90807008743286, 163.19357085227966, 164.47907161712646]
[19.8, 19.8, 27.416666666666668, 27.416666666666668, 40.233333333333334, 40.233333333333334, 44.95, 44.95, 52.46666666666667, 52.46666666666667, 59.35, 59.35, 62.75, 62.75, 61.96666666666667, 61.96666666666667, 63.2, 63.2, 65.08333333333333, 65.08333333333333, 65.63333333333334, 65.63333333333334, 67.25, 67.25, 66.76666666666667, 66.76666666666667, 68.43333333333334, 68.43333333333334, 71.13333333333334, 71.13333333333334, 72.6, 72.6, 72.76666666666667, 72.76666666666667, 72.71666666666667, 72.71666666666667, 73.3, 73.3, 74.36666666666666, 74.36666666666666, 74.26666666666667, 74.26666666666667, 74.48333333333333, 74.48333333333333, 75.05, 75.05, 75.61666666666666, 75.61666666666666, 75.73333333333333, 75.73333333333333, 76.36666666666666, 76.36666666666666, 76.38333333333334, 76.38333333333334, 76.88333333333334, 76.88333333333334, 77.91666666666667, 77.91666666666667, 77.8, 77.8, 78.06666666666666, 78.06666666666666, 77.25, 77.25, 78.08333333333333, 78.08333333333333, 78.15, 78.15, 78.23333333333333, 78.23333333333333, 78.56666666666666, 78.56666666666666, 78.45, 78.45, 78.71666666666667, 78.71666666666667, 79.23333333333333, 79.23333333333333, 79.36666666666666, 79.36666666666666, 79.36666666666666, 79.36666666666666, 79.71666666666667, 79.71666666666667, 79.88333333333334, 79.88333333333334, 79.71666666666667, 79.71666666666667, 80.33333333333333, 80.33333333333333, 80.55, 80.55, 79.95, 79.95, 80.11666666666666, 80.11666666666666, 80.46666666666667, 80.46666666666667, 80.13333333333334, 80.13333333333334, 80.56666666666666, 80.56666666666666, 80.46666666666667, 80.46666666666667, 80.81666666666666, 80.81666666666666, 81.41666666666667, 81.41666666666667, 81.06666666666666, 81.06666666666666, 81.33333333333333, 81.33333333333333, 82.01666666666667, 82.01666666666667, 81.71666666666667, 81.71666666666667, 81.5, 81.5, 81.65, 81.65, 82.05, 82.05, 81.93333333333334, 81.93333333333334, 82.25, 82.25, 82.08333333333333, 82.08333333333333, 81.05, 81.05, 81.56666666666666, 81.56666666666666, 82.78333333333333, 82.78333333333333, 82.38333333333334, 82.38333333333334, 82.23333333333333, 82.23333333333333, 82.23333333333333, 82.23333333333333, 82.86666666666666, 82.86666666666666, 82.71666666666667, 82.71666666666667, 82.75, 82.75, 82.23333333333333, 82.23333333333333, 82.45, 82.45, 82.83333333333333, 82.83333333333333, 83.01666666666667, 83.01666666666667, 83.06666666666666, 83.06666666666666, 83.21666666666667, 83.21666666666667, 82.9, 82.9, 83.1, 83.1, 83.05, 83.05, 83.03333333333333, 83.03333333333333, 83.43333333333334, 83.43333333333334, 83.16666666666667, 83.16666666666667, 83.03333333333333, 83.03333333333333, 82.73333333333333, 82.73333333333333, 83.1, 83.1, 82.68333333333334, 82.68333333333334, 83.01666666666667, 83.01666666666667, 83.28333333333333, 83.28333333333333, 83.56666666666666, 83.56666666666666, 83.38333333333334, 83.38333333333334, 83.25, 83.25, 83.41666666666667, 83.41666666666667, 83.25, 83.25, 83.06666666666666, 83.06666666666666, 83.68333333333334, 83.68333333333334, 83.1, 83.1, 83.28333333333333, 83.28333333333333, 83.85, 83.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 2.178, Test loss: 1.712, Test accuracy: 39.44
Round   1, Train loss: 1.771, Test loss: 1.468, Test accuracy: 46.25
Round   2, Train loss: 1.605, Test loss: 1.339, Test accuracy: 52.04
Round   3, Train loss: 1.493, Test loss: 1.256, Test accuracy: 55.90
Round   4, Train loss: 1.411, Test loss: 1.195, Test accuracy: 57.81
Round   5, Train loss: 1.308, Test loss: 1.111, Test accuracy: 60.88
Round   6, Train loss: 1.236, Test loss: 1.050, Test accuracy: 63.17
Round   7, Train loss: 1.174, Test loss: 1.026, Test accuracy: 64.24
Round   8, Train loss: 1.153, Test loss: 0.985, Test accuracy: 65.89
Round   9, Train loss: 1.104, Test loss: 0.967, Test accuracy: 66.25
Round  10, Train loss: 1.071, Test loss: 0.934, Test accuracy: 67.66
Round  11, Train loss: 1.031, Test loss: 0.913, Test accuracy: 68.67
Round  12, Train loss: 0.987, Test loss: 0.881, Test accuracy: 69.58
Round  13, Train loss: 0.970, Test loss: 0.881, Test accuracy: 69.63
Round  14, Train loss: 0.919, Test loss: 0.853, Test accuracy: 70.28
Round  15, Train loss: 0.899, Test loss: 0.848, Test accuracy: 71.21
Round  16, Train loss: 0.869, Test loss: 0.831, Test accuracy: 72.09
Round  17, Train loss: 0.895, Test loss: 0.821, Test accuracy: 72.14
Round  18, Train loss: 0.818, Test loss: 0.806, Test accuracy: 72.58
Round  19, Train loss: 0.846, Test loss: 0.796, Test accuracy: 73.11
Round  20, Train loss: 0.786, Test loss: 0.791, Test accuracy: 73.16
Round  21, Train loss: 0.809, Test loss: 0.768, Test accuracy: 74.05
Round  22, Train loss: 0.764, Test loss: 0.779, Test accuracy: 73.65
Round  23, Train loss: 0.749, Test loss: 0.766, Test accuracy: 74.04
Round  24, Train loss: 0.740, Test loss: 0.757, Test accuracy: 74.70
Round  25, Train loss: 0.737, Test loss: 0.754, Test accuracy: 75.02
Round  26, Train loss: 0.698, Test loss: 0.756, Test accuracy: 74.72
Round  27, Train loss: 0.711, Test loss: 0.748, Test accuracy: 75.39
Round  28, Train loss: 0.690, Test loss: 0.752, Test accuracy: 75.09
Round  29, Train loss: 0.685, Test loss: 0.751, Test accuracy: 75.11
Round  30, Train loss: 0.685, Test loss: 0.736, Test accuracy: 75.62
Round  31, Train loss: 0.641, Test loss: 0.748, Test accuracy: 75.56
Round  32, Train loss: 0.625, Test loss: 0.743, Test accuracy: 75.97
Round  33, Train loss: 0.666, Test loss: 0.751, Test accuracy: 75.29
Round  34, Train loss: 0.632, Test loss: 0.744, Test accuracy: 75.69
Round  35, Train loss: 0.623, Test loss: 0.741, Test accuracy: 76.19
Round  36, Train loss: 0.580, Test loss: 0.741, Test accuracy: 76.31
Round  37, Train loss: 0.669, Test loss: 0.731, Test accuracy: 75.91
Round  38, Train loss: 0.582, Test loss: 0.747, Test accuracy: 76.14
Round  39, Train loss: 0.601, Test loss: 0.726, Test accuracy: 75.96
Round  40, Train loss: 0.581, Test loss: 0.729, Test accuracy: 76.13
Round  41, Train loss: 0.579, Test loss: 0.742, Test accuracy: 76.33
Round  42, Train loss: 0.587, Test loss: 0.733, Test accuracy: 76.53
Round  43, Train loss: 0.530, Test loss: 0.742, Test accuracy: 76.45
Round  44, Train loss: 0.563, Test loss: 0.756, Test accuracy: 76.22
Round  45, Train loss: 0.572, Test loss: 0.744, Test accuracy: 76.52
Round  46, Train loss: 0.553, Test loss: 0.749, Test accuracy: 76.82
Round  47, Train loss: 0.541, Test loss: 0.728, Test accuracy: 76.83
Round  48, Train loss: 0.526, Test loss: 0.765, Test accuracy: 76.22
Round  49, Train loss: 0.557, Test loss: 0.737, Test accuracy: 76.79
Round  50, Train loss: 0.514, Test loss: 0.742, Test accuracy: 77.18
Round  51, Train loss: 0.524, Test loss: 0.736, Test accuracy: 77.33
Round  52, Train loss: 0.521, Test loss: 0.744, Test accuracy: 77.12
Round  53, Train loss: 0.552, Test loss: 0.743, Test accuracy: 77.03
Round  54, Train loss: 0.533, Test loss: 0.741, Test accuracy: 76.79
Round  55, Train loss: 0.511, Test loss: 0.745, Test accuracy: 77.38
Round  56, Train loss: 0.501, Test loss: 0.745, Test accuracy: 77.02
Round  57, Train loss: 0.496, Test loss: 0.755, Test accuracy: 77.16
Round  58, Train loss: 0.523, Test loss: 0.733, Test accuracy: 77.33
Round  59, Train loss: 0.487, Test loss: 0.733, Test accuracy: 77.60
Round  60, Train loss: 0.496, Test loss: 0.736, Test accuracy: 77.70
Round  61, Train loss: 0.470, Test loss: 0.737, Test accuracy: 78.08
Round  62, Train loss: 0.470, Test loss: 0.751, Test accuracy: 77.42
Round  63, Train loss: 0.487, Test loss: 0.730, Test accuracy: 77.86
Round  64, Train loss: 0.459, Test loss: 0.741, Test accuracy: 77.77
Round  65, Train loss: 0.460, Test loss: 0.760, Test accuracy: 77.07
Round  66, Train loss: 0.475, Test loss: 0.755, Test accuracy: 77.19
Round  67, Train loss: 0.469, Test loss: 0.745, Test accuracy: 77.23
Round  68, Train loss: 0.431, Test loss: 0.774, Test accuracy: 77.03
Round  69, Train loss: 0.472, Test loss: 0.753, Test accuracy: 77.24
Round  70, Train loss: 0.456, Test loss: 0.761, Test accuracy: 77.24
Round  71, Train loss: 0.512, Test loss: 0.749, Test accuracy: 77.36
Round  72, Train loss: 0.442, Test loss: 0.744, Test accuracy: 77.47
Round  73, Train loss: 0.478, Test loss: 0.759, Test accuracy: 77.50
Round  74, Train loss: 0.467, Test loss: 0.745, Test accuracy: 77.53
Round  75, Train loss: 0.436, Test loss: 0.747, Test accuracy: 77.50
Round  76, Train loss: 0.439, Test loss: 0.757, Test accuracy: 77.64
Round  77, Train loss: 0.448, Test loss: 0.741, Test accuracy: 77.77
Round  78, Train loss: 0.403, Test loss: 0.764, Test accuracy: 77.75
Round  79, Train loss: 0.411, Test loss: 0.787, Test accuracy: 77.47
Round  80, Train loss: 0.379, Test loss: 0.778, Test accuracy: 77.63
Round  81, Train loss: 0.447, Test loss: 0.756, Test accuracy: 78.00
Round  82, Train loss: 0.455, Test loss: 0.766, Test accuracy: 78.12
Round  83, Train loss: 0.420, Test loss: 0.771, Test accuracy: 77.36
Round  84, Train loss: 0.411, Test loss: 0.760, Test accuracy: 77.23
Round  85, Train loss: 0.435, Test loss: 0.764, Test accuracy: 77.59
Round  86, Train loss: 0.407, Test loss: 0.754, Test accuracy: 77.58
Round  87, Train loss: 0.415, Test loss: 0.771, Test accuracy: 77.81
Round  88, Train loss: 0.398, Test loss: 0.766, Test accuracy: 78.00
Round  89, Train loss: 0.376, Test loss: 0.779, Test accuracy: 77.67
Round  90, Train loss: 0.386, Test loss: 0.783, Test accuracy: 77.88
Round  91, Train loss: 0.371, Test loss: 0.782, Test accuracy: 77.81
Round  92, Train loss: 0.366, Test loss: 0.797, Test accuracy: 77.38
Round  93, Train loss: 0.411, Test loss: 0.786, Test accuracy: 78.00
Round  94, Train loss: 0.428, Test loss: 0.783, Test accuracy: 77.36
Round  95, Train loss: 0.362, Test loss: 0.781, Test accuracy: 77.54
Round  96, Train loss: 0.396, Test loss: 0.784, Test accuracy: 77.88
Round  97, Train loss: 0.410, Test loss: 0.768, Test accuracy: 77.94
Round  98, Train loss: 0.377, Test loss: 0.801, Test accuracy: 77.56
Round  99, Train loss: 0.410, Test loss: 0.763, Test accuracy: 77.80
Final Round, Train loss: 0.301, Test loss: 0.781, Test accuracy: 78.51
Average accuracy final 10 rounds: 77.71449999999999
8153.753432750702
[12.774396181106567, 24.480592489242554, 36.275529623031616, 48.05338954925537, 59.80773901939392, 71.66590356826782, 83.45346999168396, 95.24976587295532, 107.31519961357117, 119.34916591644287, 131.00171208381653, 142.7474262714386, 154.99404454231262, 166.75667524337769, 178.5277042388916, 190.29147791862488, 202.8068127632141, 214.59431672096252, 226.41096591949463, 238.15126776695251, 249.95926356315613, 261.6940863132477, 273.44208550453186, 285.1667106151581, 296.8891201019287, 308.59248757362366, 320.3257420063019, 332.0868465900421, 343.76230335235596, 355.41936230659485, 367.1224949359894, 378.80830669403076, 390.54083800315857, 402.2270698547363, 413.9795310497284, 425.8112750053406, 437.5182373523712, 449.20963764190674, 460.9916732311249, 472.7592384815216, 484.62894582748413, 496.43057775497437, 508.21429920196533, 520.662050485611, 532.5236597061157, 544.2483596801758, 555.9631567001343, 567.7737965583801, 580.4306173324585, 592.2039053440094, 603.9931428432465, 615.8345558643341, 627.69091629982, 639.4970717430115, 651.2975964546204, 663.1280453205109, 674.9029269218445, 686.724839925766, 698.4414393901825, 710.1371438503265, 721.9335136413574, 733.6305911540985, 745.3957855701447, 757.0890476703644, 768.8575456142426, 780.635064125061, 792.430125951767, 804.2192192077637, 816.0222070217133, 827.7990131378174, 839.5492916107178, 851.2482116222382, 862.9809527397156, 874.654678106308, 886.3086647987366, 898.0093777179718, 909.7614405155182, 921.5183126926422, 933.3363058567047, 945.1646938323975, 956.9938802719116, 968.7307209968567, 980.5248050689697, 992.3688869476318, 1003.9874403476715, 1015.5822284221649, 1027.172316789627, 1038.817411184311, 1050.4677486419678, 1062.1053807735443, 1073.8441650867462, 1085.5194880962372, 1097.267294883728, 1109.0425217151642, 1120.8122560977936, 1132.5672528743744, 1144.3352699279785, 1155.981495141983, 1167.7699844837189, 1179.4805862903595, 1182.4841561317444]
[39.4425, 46.2525, 52.04, 55.9, 57.8125, 60.885, 63.1725, 64.2375, 65.89, 66.25, 67.655, 68.6725, 69.585, 69.6325, 70.275, 71.2125, 72.095, 72.145, 72.5775, 73.1075, 73.1625, 74.045, 73.6475, 74.04, 74.705, 75.02, 74.715, 75.3875, 75.0875, 75.1075, 75.625, 75.56, 75.97, 75.29, 75.69, 76.19, 76.3075, 75.9075, 76.1425, 75.9625, 76.1275, 76.33, 76.53, 76.4525, 76.2225, 76.5175, 76.82, 76.825, 76.215, 76.7875, 77.1775, 77.3275, 77.1225, 77.0325, 76.7925, 77.3775, 77.0175, 77.1625, 77.335, 77.5975, 77.7025, 78.085, 77.415, 77.855, 77.765, 77.0725, 77.185, 77.23, 77.0325, 77.2375, 77.2375, 77.365, 77.475, 77.495, 77.525, 77.4975, 77.6375, 77.765, 77.7525, 77.4675, 77.63, 78.0025, 78.125, 77.3575, 77.235, 77.5875, 77.58, 77.8075, 78.0025, 77.675, 77.8775, 77.805, 77.3825, 78.0025, 77.355, 77.5375, 77.875, 77.945, 77.5625, 77.8025, 78.5075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.268, Test loss: 2.296, Test accuracy: 10.49
Round   0, Global train loss: 2.268, Global test loss: 2.299, Global test accuracy: 10.04
Round   1, Train loss: 2.289, Test loss: 2.296, Test accuracy: 12.21
Round   1, Global train loss: 2.289, Global test loss: 2.299, Global test accuracy: 10.67
Round   2, Train loss: 2.238, Test loss: 2.293, Test accuracy: 12.71
Round   2, Global train loss: 2.238, Global test loss: 2.299, Global test accuracy: 10.57
Round   3, Train loss: nan, Test loss: nan, Test accuracy: 17.52
Round   3, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 14.97
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 14.97
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 15.16
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 13.26
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 13.26
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 11.67
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 100, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 100, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 101, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 101, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 102, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 102, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 103, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 103, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 104, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 104, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 105, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 105, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 106, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 106, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 107, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 107, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 108, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 108, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 109, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 109, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 110, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 110, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 111, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 111, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 112, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 112, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 113, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 113, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 114, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 114, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 115, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 115, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 116, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 116, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 117, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 117, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 118, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 118, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 119, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 119, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 120, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 120, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 121, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 121, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 122, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 122, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 123, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 123, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 124, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 124, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 125, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 125, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 126, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 126, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 127, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 127, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 128, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 128, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 129, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 129, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 130, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 130, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 131, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 131, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 132, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 132, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Average accuracy final 10 rounds: 13.333333333333337 

Average global accuracy final 10 rounds: 13.333333333333337 

4568.930960893631
[1.8272640705108643, 3.388702154159546, 4.989212274551392, 6.540589809417725, 8.101847887039185, 9.776354551315308, 11.405980348587036, 13.031834840774536, 14.633547306060791, 16.215041160583496, 17.89995241165161, 19.51214051246643, 21.184409856796265, 22.826026678085327, 24.366477727890015, 26.002042531967163, 27.640463829040527, 29.217047691345215, 30.81225299835205, 32.41444230079651, 34.014121532440186, 35.58636260032654, 37.15645956993103, 38.7037832736969, 40.28850793838501, 41.88523030281067, 43.50649976730347, 45.09482192993164, 46.64424276351929, 48.05466055870056, 49.585214138031006, 51.13760733604431, 52.6283540725708, 54.18940448760986, 55.75882172584534, 57.32021951675415, 58.96265196800232, 60.5920193195343, 62.1344096660614, 63.6849365234375, 65.1132264137268, 66.53300929069519, 67.95273351669312, 69.37286686897278, 70.90561962127686, 72.33117246627808, 73.74668455123901, 75.168541431427, 76.59341549873352, 78.02615642547607, 79.44657230377197, 80.87279963493347, 82.29549407958984, 83.71724081039429, 85.13957858085632, 86.5688362121582, 87.99631190299988, 89.4226381778717, 90.84013175964355, 92.26241755485535, 93.69177603721619, 95.23270177841187, 96.65539193153381, 98.0803873538971, 99.50883078575134, 100.93124580383301, 102.35040926933289, 103.7708375453949, 105.19727921485901, 106.77818322181702, 108.21119260787964, 109.6462574005127, 111.08615922927856, 112.52926540374756, 113.96449518203735, 115.40698552131653, 116.84196519851685, 118.37939381599426, 119.90451073646545, 121.32568383216858, 122.76675748825073, 124.18746519088745, 125.6030957698822, 127.02560377120972, 128.53755497932434, 129.9492108821869, 131.36695885658264, 132.7902421951294, 134.2103295326233, 135.6335473060608, 137.04796481132507, 138.47133326530457, 139.89167022705078, 141.30528330802917, 142.7235448360443, 144.13936686515808, 145.55186581611633, 146.9642095565796, 148.37971901893616, 149.79970622062683, 151.21725177764893, 152.6308252811432, 154.04820466041565, 155.46338057518005, 156.87992095947266, 158.32612490653992, 159.83559775352478, 161.2558946609497, 162.67278671264648, 164.0866346359253, 165.4974229335785, 166.9186351299286, 168.3337779045105, 169.74429392814636, 171.15938115119934, 172.57604479789734, 173.9901053905487, 175.4069230556488, 176.82165670394897, 178.24136757850647, 179.6563241481781, 181.0648114681244, 182.4809856414795, 183.8984603881836, 185.3151662349701, 186.72896361351013, 188.148921251297, 189.56884932518005, 190.98999547958374, 192.40658402442932, 193.82737803459167, 195.24937224388123, 196.66462182998657, 198.08550691604614, 199.50957417488098, 200.93088674545288, 202.34619641304016, 203.7625629901886, 205.18209195137024, 206.60189533233643, 208.013277053833, 209.42862010002136, 210.84722423553467, 212.2668228149414, 213.6808795928955, 215.09072494506836, 216.50638937950134, 217.925213098526, 219.33980059623718, 220.75266361236572, 222.17053961753845, 223.59057188034058, 225.00180554389954, 226.41926097869873, 227.8388228416443, 229.25838017463684, 230.67062425613403, 232.08495712280273, 233.50337862968445, 234.91378808021545, 236.33098769187927, 237.74344635009766, 239.16681098937988, 240.58637928962708, 241.99902272224426, 243.42214393615723, 244.84324073791504, 246.26205396652222, 247.67750930786133, 249.10390400886536, 250.5299654006958, 251.94773483276367, 253.35856127738953, 254.7746570110321, 256.1948742866516, 257.613050699234, 259.02610301971436, 260.4412627220154, 261.86333870887756, 263.27378368377686, 264.68544125556946, 266.0954415798187, 267.51765275001526, 268.9410967826843, 270.353431224823, 271.7648718357086, 273.1866133213043, 274.6047077178955, 276.0204048156738, 277.4345529079437, 278.8517282009125, 280.2675852775574, 281.68045353889465, 283.09737038612366, 284.5229558944702, 285.9426746368408, 287.3542401790619, 288.7689254283905, 290.19759488105774, 291.62736678123474, 293.0456290245056, 294.46527194976807, 295.88890194892883, 297.30580496788025, 298.72380208969116, 300.1409149169922, 301.5749568939209, 302.99091243743896, 304.480904340744, 305.95916843414307, 307.4189283847809, 308.9471414089203, 310.40210151672363, 311.8651821613312, 313.3296980857849, 314.8123936653137, 316.2772250175476, 317.72992873191833, 319.1869578361511, 320.64877367019653, 322.2049448490143, 323.75027537345886, 325.24950194358826, 326.67754459381104, 328.09558176994324, 329.52069878578186, 330.9531981945038, 332.37205696105957, 333.78875398635864, 335.32733368873596, 336.75864696502686, 338.19363141059875, 339.62118339538574, 341.0473985671997, 342.47431111335754, 343.9024655818939, 345.33492136001587, 346.7779929637909, 348.20700311660767, 349.6372981071472, 351.0690574645996, 352.4994843006134, 353.9331614971161, 355.3665409088135, 356.79854130744934, 358.2314682006836, 359.65701365470886, 361.0854675769806, 362.51168727874756, 363.9438636302948, 365.37274956703186, 366.8020098209381, 368.2258744239807, 369.64983916282654, 371.0817313194275, 372.5097403526306, 373.9414551258087, 375.37216663360596, 376.8031659126282, 378.2270669937134, 379.6564757823944, 381.0886342525482, 382.52637362480164, 383.95664858818054, 385.384747505188, 386.8124496936798, 388.24642729759216, 389.67389488220215, 391.10365867614746, 392.5321228504181, 393.9557468891144, 395.3830313682556, 396.81045794487, 398.2473027706146, 399.67114758491516, 401.1162519454956, 402.5382807254791, 404.0754282474518, 405.54271364212036, 406.966965675354, 408.39188718795776, 409.81550216674805, 411.236266374588, 412.64872002601624, 414.0668275356293, 415.4897270202637, 416.90470814704895, 418.32233572006226, 419.7321426868439, 421.15665078163147, 422.56844329833984, 423.9832603931427, 425.4085841178894, 426.8173942565918, 428.2313265800476, 429.6485402584076, 431.06388115882874, 432.5165650844574, 433.93720388412476, 435.3544211387634, 437.7347905635834]
[10.491666666666667, 12.208333333333334, 12.708333333333334, 17.525, 14.966666666666667, 14.966666666666667, 15.158333333333333, 13.258333333333333, 13.258333333333333, 11.666666666666666, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.099, Test loss: 1.985, Test accuracy: 26.50
Round   0, Global train loss: 1.099, Global test loss: 2.321, Global test accuracy: 18.37
Round   1, Train loss: 0.909, Test loss: 1.395, Test accuracy: 46.74
Round   1, Global train loss: 0.909, Global test loss: 2.096, Global test accuracy: 27.98
Round   2, Train loss: 0.804, Test loss: 1.249, Test accuracy: 50.42
Round   2, Global train loss: 0.804, Global test loss: 1.895, Global test accuracy: 30.68
Round   3, Train loss: 0.805, Test loss: 1.214, Test accuracy: 53.84
Round   3, Global train loss: 0.805, Global test loss: 2.041, Global test accuracy: 28.77
Round   4, Train loss: 0.823, Test loss: 1.055, Test accuracy: 59.58
Round   4, Global train loss: 0.823, Global test loss: 1.990, Global test accuracy: 31.56
Round   5, Train loss: 0.719, Test loss: 0.887, Test accuracy: 64.21
Round   5, Global train loss: 0.719, Global test loss: 1.751, Global test accuracy: 35.26
Round   6, Train loss: 0.705, Test loss: 0.868, Test accuracy: 64.38
Round   6, Global train loss: 0.705, Global test loss: 1.905, Global test accuracy: 31.39
Round   7, Train loss: 0.663, Test loss: 0.793, Test accuracy: 67.53
Round   7, Global train loss: 0.663, Global test loss: 1.925, Global test accuracy: 33.37
Round   8, Train loss: 0.700, Test loss: 0.773, Test accuracy: 67.00
Round   8, Global train loss: 0.700, Global test loss: 1.783, Global test accuracy: 29.43
Round   9, Train loss: 0.639, Test loss: 0.735, Test accuracy: 68.53
Round   9, Global train loss: 0.639, Global test loss: 1.656, Global test accuracy: 35.59
Round  10, Train loss: 0.701, Test loss: 0.731, Test accuracy: 69.85
Round  10, Global train loss: 0.701, Global test loss: 1.720, Global test accuracy: 36.29
Round  11, Train loss: 0.586, Test loss: 0.734, Test accuracy: 70.42
Round  11, Global train loss: 0.586, Global test loss: 1.801, Global test accuracy: 38.14
Round  12, Train loss: 0.564, Test loss: 0.632, Test accuracy: 74.15
Round  12, Global train loss: 0.564, Global test loss: 1.680, Global test accuracy: 39.96
Round  13, Train loss: 0.532, Test loss: 0.646, Test accuracy: 73.78
Round  13, Global train loss: 0.532, Global test loss: 2.009, Global test accuracy: 35.60
Round  14, Train loss: 0.549, Test loss: 0.590, Test accuracy: 75.42
Round  14, Global train loss: 0.549, Global test loss: 2.059, Global test accuracy: 36.77
Round  15, Train loss: 0.584, Test loss: 0.572, Test accuracy: 76.29
Round  15, Global train loss: 0.584, Global test loss: 1.586, Global test accuracy: 43.47
Round  16, Train loss: 0.577, Test loss: 0.563, Test accuracy: 76.73
Round  16, Global train loss: 0.577, Global test loss: 1.560, Global test accuracy: 44.33
Round  17, Train loss: 0.604, Test loss: 0.575, Test accuracy: 76.81
Round  17, Global train loss: 0.604, Global test loss: 1.395, Global test accuracy: 47.94
Round  18, Train loss: 0.567, Test loss: 0.564, Test accuracy: 76.97
Round  18, Global train loss: 0.567, Global test loss: 1.561, Global test accuracy: 43.54
Round  19, Train loss: 0.560, Test loss: 0.564, Test accuracy: 77.04
Round  19, Global train loss: 0.560, Global test loss: 1.465, Global test accuracy: 46.88
Round  20, Train loss: 0.566, Test loss: 0.551, Test accuracy: 77.65
Round  20, Global train loss: 0.566, Global test loss: 1.504, Global test accuracy: 45.45
Round  21, Train loss: 0.597, Test loss: 0.540, Test accuracy: 77.96
Round  21, Global train loss: 0.597, Global test loss: 1.613, Global test accuracy: 42.72
Round  22, Train loss: 0.525, Test loss: 0.539, Test accuracy: 78.38
Round  22, Global train loss: 0.525, Global test loss: 1.365, Global test accuracy: 49.85
Round  23, Train loss: 0.469, Test loss: 0.542, Test accuracy: 78.31
Round  23, Global train loss: 0.469, Global test loss: 1.549, Global test accuracy: 47.81
Round  24, Train loss: 0.481, Test loss: 0.541, Test accuracy: 78.33
Round  24, Global train loss: 0.481, Global test loss: 1.563, Global test accuracy: 45.93
Round  25, Train loss: 0.410, Test loss: 0.553, Test accuracy: 78.32
Round  25, Global train loss: 0.410, Global test loss: 1.566, Global test accuracy: 48.93
Round  26, Train loss: 0.444, Test loss: 0.539, Test accuracy: 78.60
Round  26, Global train loss: 0.444, Global test loss: 1.435, Global test accuracy: 51.79
Round  27, Train loss: 0.445, Test loss: 0.539, Test accuracy: 78.73
Round  27, Global train loss: 0.445, Global test loss: 1.465, Global test accuracy: 49.73
Round  28, Train loss: 0.409, Test loss: 0.524, Test accuracy: 79.59
Round  28, Global train loss: 0.409, Global test loss: 1.392, Global test accuracy: 51.59
Round  29, Train loss: 0.469, Test loss: 0.522, Test accuracy: 79.65
Round  29, Global train loss: 0.469, Global test loss: 1.242, Global test accuracy: 55.48
Round  30, Train loss: 0.444, Test loss: 0.546, Test accuracy: 78.87
Round  30, Global train loss: 0.444, Global test loss: 1.463, Global test accuracy: 49.42
Round  31, Train loss: 0.415, Test loss: 0.526, Test accuracy: 79.48
Round  31, Global train loss: 0.415, Global test loss: 1.478, Global test accuracy: 49.48
Round  32, Train loss: 0.390, Test loss: 0.501, Test accuracy: 80.45
Round  32, Global train loss: 0.390, Global test loss: 1.413, Global test accuracy: 50.06
Round  33, Train loss: 0.355, Test loss: 0.509, Test accuracy: 80.28
Round  33, Global train loss: 0.355, Global test loss: 1.480, Global test accuracy: 50.60
Round  34, Train loss: 0.439, Test loss: 0.509, Test accuracy: 80.16
Round  34, Global train loss: 0.439, Global test loss: 1.423, Global test accuracy: 52.05
Round  35, Train loss: 0.396, Test loss: 0.495, Test accuracy: 80.74
Round  35, Global train loss: 0.396, Global test loss: 1.274, Global test accuracy: 55.44
Round  36, Train loss: 0.393, Test loss: 0.498, Test accuracy: 80.72
Round  36, Global train loss: 0.393, Global test loss: 1.704, Global test accuracy: 46.76
Round  37, Train loss: 0.429, Test loss: 0.500, Test accuracy: 80.69
Round  37, Global train loss: 0.429, Global test loss: 1.162, Global test accuracy: 59.56
Round  38, Train loss: 0.424, Test loss: 0.494, Test accuracy: 81.06
Round  38, Global train loss: 0.424, Global test loss: 1.232, Global test accuracy: 57.56
Round  39, Train loss: 0.442, Test loss: 0.493, Test accuracy: 80.93
Round  39, Global train loss: 0.442, Global test loss: 1.523, Global test accuracy: 49.84
Round  40, Train loss: 0.303, Test loss: 0.491, Test accuracy: 81.10
Round  40, Global train loss: 0.303, Global test loss: 1.692, Global test accuracy: 46.67
Round  41, Train loss: 0.397, Test loss: 0.498, Test accuracy: 81.14
Round  41, Global train loss: 0.397, Global test loss: 1.308, Global test accuracy: 54.99
Round  42, Train loss: 0.417, Test loss: 0.483, Test accuracy: 81.42
Round  42, Global train loss: 0.417, Global test loss: 1.377, Global test accuracy: 53.76
Round  43, Train loss: 0.375, Test loss: 0.486, Test accuracy: 81.66
Round  43, Global train loss: 0.375, Global test loss: 1.185, Global test accuracy: 59.94
Round  44, Train loss: 0.326, Test loss: 0.490, Test accuracy: 81.53
Round  44, Global train loss: 0.326, Global test loss: 1.133, Global test accuracy: 60.95
Round  45, Train loss: 0.352, Test loss: 0.485, Test accuracy: 81.70
Round  45, Global train loss: 0.352, Global test loss: 1.292, Global test accuracy: 57.20
Round  46, Train loss: 0.327, Test loss: 0.477, Test accuracy: 81.93
Round  46, Global train loss: 0.327, Global test loss: 1.156, Global test accuracy: 59.73
Round  47, Train loss: 0.408, Test loss: 0.488, Test accuracy: 81.78
Round  47, Global train loss: 0.408, Global test loss: 1.202, Global test accuracy: 58.63
Round  48, Train loss: 0.333, Test loss: 0.472, Test accuracy: 82.16
Round  48, Global train loss: 0.333, Global test loss: 1.174, Global test accuracy: 59.15
Round  49, Train loss: 0.284, Test loss: 0.478, Test accuracy: 82.17
Round  49, Global train loss: 0.284, Global test loss: 1.596, Global test accuracy: 52.16
Round  50, Train loss: 0.374, Test loss: 0.474, Test accuracy: 82.53
Round  50, Global train loss: 0.374, Global test loss: 1.388, Global test accuracy: 54.98
Round  51, Train loss: 0.356, Test loss: 0.487, Test accuracy: 81.92
Round  51, Global train loss: 0.356, Global test loss: 1.125, Global test accuracy: 61.09
Round  52, Train loss: 0.315, Test loss: 0.481, Test accuracy: 81.98
Round  52, Global train loss: 0.315, Global test loss: 1.229, Global test accuracy: 57.81
Round  53, Train loss: 0.336, Test loss: 0.495, Test accuracy: 81.62
Round  53, Global train loss: 0.336, Global test loss: 1.354, Global test accuracy: 56.73
Round  54, Train loss: 0.343, Test loss: 0.498, Test accuracy: 81.83
Round  54, Global train loss: 0.343, Global test loss: 1.293, Global test accuracy: 57.72
Round  55, Train loss: 0.306, Test loss: 0.490, Test accuracy: 82.01
Round  55, Global train loss: 0.306, Global test loss: 1.489, Global test accuracy: 54.65
Round  56, Train loss: 0.279, Test loss: 0.499, Test accuracy: 81.96
Round  56, Global train loss: 0.279, Global test loss: 1.448, Global test accuracy: 56.94
Round  57, Train loss: 0.306, Test loss: 0.483, Test accuracy: 82.63
Round  57, Global train loss: 0.306, Global test loss: 1.245, Global test accuracy: 58.22
Round  58, Train loss: 0.293, Test loss: 0.499, Test accuracy: 82.11
Round  58, Global train loss: 0.293, Global test loss: 1.278, Global test accuracy: 58.42
Round  59, Train loss: 0.255, Test loss: 0.493, Test accuracy: 82.37
Round  59, Global train loss: 0.255, Global test loss: 1.410, Global test accuracy: 56.25
Round  60, Train loss: 0.261, Test loss: 0.518, Test accuracy: 81.97
Round  60, Global train loss: 0.261, Global test loss: 1.337, Global test accuracy: 58.41
Round  61, Train loss: 0.254, Test loss: 0.504, Test accuracy: 82.19
Round  61, Global train loss: 0.254, Global test loss: 1.264, Global test accuracy: 60.37
Round  62, Train loss: 0.238, Test loss: 0.514, Test accuracy: 81.92
Round  62, Global train loss: 0.238, Global test loss: 1.258, Global test accuracy: 60.09
Round  63, Train loss: 0.214, Test loss: 0.519, Test accuracy: 81.67
Round  63, Global train loss: 0.214, Global test loss: 1.583, Global test accuracy: 54.78
Round  64, Train loss: 0.262, Test loss: 0.511, Test accuracy: 82.23
Round  64, Global train loss: 0.262, Global test loss: 1.751, Global test accuracy: 50.56
Round  65, Train loss: 0.243, Test loss: 0.486, Test accuracy: 82.86
Round  65, Global train loss: 0.243, Global test loss: 1.471, Global test accuracy: 56.52
Round  66, Train loss: 0.255, Test loss: 0.504, Test accuracy: 82.37
Round  66, Global train loss: 0.255, Global test loss: 1.091, Global test accuracy: 64.38
Round  67, Train loss: 0.257, Test loss: 0.505, Test accuracy: 82.47
Round  67, Global train loss: 0.257, Global test loss: 1.453, Global test accuracy: 57.50
Round  68, Train loss: 0.337, Test loss: 0.490, Test accuracy: 82.73
Round  68, Global train loss: 0.337, Global test loss: 1.326, Global test accuracy: 56.82
Round  69, Train loss: 0.225, Test loss: 0.483, Test accuracy: 83.17
Round  69, Global train loss: 0.225, Global test loss: 1.634, Global test accuracy: 51.69
Round  70, Train loss: 0.278, Test loss: 0.494, Test accuracy: 82.94
Round  70, Global train loss: 0.278, Global test loss: 1.289, Global test accuracy: 61.12
Round  71, Train loss: 0.231, Test loss: 0.492, Test accuracy: 83.09
Round  71, Global train loss: 0.231, Global test loss: 1.122, Global test accuracy: 64.29
Round  72, Train loss: 0.235, Test loss: 0.483, Test accuracy: 83.48
Round  72, Global train loss: 0.235, Global test loss: 1.236, Global test accuracy: 61.64
Round  73, Train loss: 0.330, Test loss: 0.506, Test accuracy: 82.92
Round  73, Global train loss: 0.330, Global test loss: 1.081, Global test accuracy: 64.17
Round  74, Train loss: 0.245, Test loss: 0.503, Test accuracy: 83.03
Round  74, Global train loss: 0.245, Global test loss: 1.166, Global test accuracy: 62.99
Round  75, Train loss: 0.281, Test loss: 0.502, Test accuracy: 82.93
Round  75, Global train loss: 0.281, Global test loss: 1.111, Global test accuracy: 63.55
Round  76, Train loss: 0.222, Test loss: 0.511, Test accuracy: 82.61
Round  76, Global train loss: 0.222, Global test loss: 1.411, Global test accuracy: 57.88
Round  77, Train loss: 0.207, Test loss: 0.497, Test accuracy: 83.09
Round  77, Global train loss: 0.207, Global test loss: 1.379, Global test accuracy: 58.69
Round  78, Train loss: 0.237, Test loss: 0.508, Test accuracy: 83.05
Round  78, Global train loss: 0.237, Global test loss: 1.369, Global test accuracy: 58.92
Round  79, Train loss: 0.317, Test loss: 0.516, Test accuracy: 82.62
Round  79, Global train loss: 0.317, Global test loss: 1.256, Global test accuracy: 60.96
Round  80, Train loss: 0.291, Test loss: 0.503, Test accuracy: 82.88
Round  80, Global train loss: 0.291, Global test loss: 1.204, Global test accuracy: 62.26
Round  81, Train loss: 0.274, Test loss: 0.512, Test accuracy: 83.03
Round  81, Global train loss: 0.274, Global test loss: 1.487, Global test accuracy: 56.69
Round  82, Train loss: 0.254, Test loss: 0.515, Test accuracy: 82.62
Round  82, Global train loss: 0.254, Global test loss: 1.130, Global test accuracy: 63.08
Round  83, Train loss: 0.312, Test loss: 0.530, Test accuracy: 82.36
Round  83, Global train loss: 0.312, Global test loss: 1.261, Global test accuracy: 60.06
Round  84, Train loss: 0.270, Test loss: 0.515, Test accuracy: 83.28
Round  84, Global train loss: 0.270, Global test loss: 1.429, Global test accuracy: 57.68
Round  85, Train loss: 0.259, Test loss: 0.501, Test accuracy: 83.53
Round  85, Global train loss: 0.259, Global test loss: 1.524, Global test accuracy: 55.43
Round  86, Train loss: 0.251, Test loss: 0.490, Test accuracy: 83.76
Round  86, Global train loss: 0.251, Global test loss: 1.146, Global test accuracy: 62.18
Round  87, Train loss: 0.254, Test loss: 0.513, Test accuracy: 83.21
Round  87, Global train loss: 0.254, Global test loss: 1.504, Global test accuracy: 57.30
Round  88, Train loss: 0.254, Test loss: 0.508, Test accuracy: 83.49
Round  88, Global train loss: 0.254, Global test loss: 1.331, Global test accuracy: 59.42
Round  89, Train loss: 0.181, Test loss: 0.512, Test accuracy: 83.87
Round  89, Global train loss: 0.181, Global test loss: 1.306, Global test accuracy: 60.67
Round  90, Train loss: 0.221, Test loss: 0.511, Test accuracy: 84.04
Round  90, Global train loss: 0.221, Global test loss: 1.216, Global test accuracy: 62.58
Round  91, Train loss: 0.188, Test loss: 0.511, Test accuracy: 83.93
Round  91, Global train loss: 0.188, Global test loss: 1.947, Global test accuracy: 50.13
Round  92, Train loss: 0.210, Test loss: 0.520, Test accuracy: 83.60
Round  92, Global train loss: 0.210, Global test loss: 1.309, Global test accuracy: 60.03
Round  93, Train loss: 0.291, Test loss: 0.544, Test accuracy: 82.99
Round  93, Global train loss: 0.291, Global test loss: 1.136, Global test accuracy: 63.84
Round  94, Train loss: 0.154, Test loss: 0.524, Test accuracy: 83.23
Round  94, Global train loss: 0.154, Global test loss: 1.708, Global test accuracy: 56.23
Round  95, Train loss: 0.235, Test loss: 0.526, Test accuracy: 83.18
Round  95, Global train loss: 0.235, Global test loss: 1.457, Global test accuracy: 57.83
Round  96, Train loss: 0.234, Test loss: 0.529, Test accuracy: 83.28
Round  96, Global train loss: 0.234, Global test loss: 1.263, Global test accuracy: 60.35
Round  97, Train loss: 0.184, Test loss: 0.531, Test accuracy: 83.36
Round  97, Global train loss: 0.184, Global test loss: 1.169, Global test accuracy: 64.59
Round  98, Train loss: 0.263, Test loss: 0.521, Test accuracy: 83.62
Round  98, Global train loss: 0.263, Global test loss: 1.318, Global test accuracy: 59.79
Round  99, Train loss: 0.230, Test loss: 0.525, Test accuracy: 83.69
Round  99, Global train loss: 0.230, Global test loss: 1.287, Global test accuracy: 59.71
Final Round, Train loss: 0.170, Test loss: 0.594, Test accuracy: 83.48
Final Round, Global train loss: 0.170, Global test loss: 1.287, Global test accuracy: 59.71
Average accuracy final 10 rounds: 83.4925 

Average global accuracy final 10 rounds: 59.50916666666666 

1779.958650112152
[1.5743067264556885, 3.148613452911377, 4.515291452407837, 5.881969451904297, 7.253840923309326, 8.625712394714355, 9.987810850143433, 11.34990930557251, 12.72817873954773, 14.10644817352295, 15.46575403213501, 16.82505989074707, 18.206594944000244, 19.588129997253418, 20.94210720062256, 22.2960844039917, 23.627065420150757, 24.958046436309814, 26.29230761528015, 27.62656879425049, 28.996606588363647, 30.366644382476807, 31.705337047576904, 33.044029712677, 34.39321303367615, 35.74239635467529, 37.101521015167236, 38.46064567565918, 39.82376313209534, 41.186880588531494, 42.544971227645874, 43.903061866760254, 45.24849987030029, 46.59393787384033, 48.094603061676025, 49.59526824951172, 51.10087609291077, 52.606483936309814, 53.9028754234314, 55.19926691055298, 56.49264407157898, 57.78602123260498, 59.07514524459839, 60.3642692565918, 61.65509104728699, 62.94591283798218, 64.23748517036438, 65.52905750274658, 66.82036232948303, 68.11166715621948, 69.40167307853699, 70.69167900085449, 71.98189973831177, 73.27212047576904, 74.55935645103455, 75.84659242630005, 77.13871455192566, 78.43083667755127, 79.72526955604553, 81.0197024345398, 82.3083245754242, 83.5969467163086, 84.89303660392761, 86.18912649154663, 87.53209114074707, 88.87505578994751, 90.29430341720581, 91.71355104446411, 93.00385117530823, 94.29415130615234, 95.59184169769287, 96.8895320892334, 98.18597269058228, 99.48241329193115, 100.77459979057312, 102.06678628921509, 103.37171173095703, 104.67663717269897, 105.96663999557495, 107.25664281845093, 108.54857063293457, 109.84049844741821, 111.13340044021606, 112.42630243301392, 113.72129726409912, 115.01629209518433, 116.31947875022888, 117.62266540527344, 118.93549871444702, 120.2483320236206, 121.53968000411987, 122.83102798461914, 124.12942552566528, 125.42782306671143, 126.72909212112427, 128.0303611755371, 129.32837581634521, 130.62639045715332, 131.9213845729828, 133.21637868881226, 134.5132315158844, 135.81008434295654, 137.1086766719818, 138.40726900100708, 139.70286464691162, 140.99846029281616, 142.2991545200348, 143.59984874725342, 144.89393186569214, 146.18801498413086, 147.47088384628296, 148.75375270843506, 150.0509009361267, 151.34804916381836, 152.7733759880066, 154.19870281219482, 155.53807020187378, 156.87743759155273, 158.1753785610199, 159.47331953048706, 160.7813377380371, 162.08935594558716, 163.3914122581482, 164.69346857070923, 166.00065231323242, 167.30783605575562, 168.61240363121033, 169.91697120666504, 171.22640752792358, 172.53584384918213, 173.84411358833313, 175.15238332748413, 176.45963144302368, 177.76687955856323, 179.07522106170654, 180.38356256484985, 181.6789824962616, 182.97440242767334, 184.2694752216339, 185.56454801559448, 186.88037252426147, 188.19619703292847, 189.49693870544434, 190.7976803779602, 192.08619022369385, 193.3747000694275, 194.68164467811584, 195.9885892868042, 197.2881007194519, 198.5876121520996, 200.00902938842773, 201.43044662475586, 202.72121286392212, 204.01197910308838, 205.30297183990479, 206.5939645767212, 207.89084672927856, 209.18772888183594, 210.4873218536377, 211.78691482543945, 213.08485579490662, 214.38279676437378, 215.67612648010254, 216.9694561958313, 218.2669608592987, 219.5644655227661, 220.8594422340393, 222.1544189453125, 223.45147466659546, 224.74853038787842, 226.0482518672943, 227.3479733467102, 228.64089369773865, 229.9338140487671, 231.22698616981506, 232.52015829086304, 233.80097651481628, 235.08179473876953, 236.3714668750763, 237.66113901138306, 238.97337436676025, 240.28560972213745, 241.59200382232666, 242.89839792251587, 244.21081686019897, 245.52323579788208, 246.82545351982117, 248.12767124176025, 249.43101382255554, 250.73435640335083, 252.04443383216858, 253.35451126098633, 254.6557114124298, 255.9569115638733, 257.2610890865326, 258.5652666091919, 259.87303352355957, 261.18080043792725, 262.5457441806793, 263.9106879234314, 266.0956931114197, 268.28069829940796]
[26.5, 26.5, 46.74166666666667, 46.74166666666667, 50.425, 50.425, 53.84166666666667, 53.84166666666667, 59.583333333333336, 59.583333333333336, 64.20833333333333, 64.20833333333333, 64.38333333333334, 64.38333333333334, 67.53333333333333, 67.53333333333333, 67.0, 67.0, 68.53333333333333, 68.53333333333333, 69.85, 69.85, 70.41666666666667, 70.41666666666667, 74.15, 74.15, 73.775, 73.775, 75.425, 75.425, 76.29166666666667, 76.29166666666667, 76.73333333333333, 76.73333333333333, 76.80833333333334, 76.80833333333334, 76.975, 76.975, 77.04166666666667, 77.04166666666667, 77.65, 77.65, 77.95833333333333, 77.95833333333333, 78.38333333333334, 78.38333333333334, 78.30833333333334, 78.30833333333334, 78.33333333333333, 78.33333333333333, 78.31666666666666, 78.31666666666666, 78.6, 78.6, 78.73333333333333, 78.73333333333333, 79.59166666666667, 79.59166666666667, 79.65, 79.65, 78.86666666666666, 78.86666666666666, 79.48333333333333, 79.48333333333333, 80.45, 80.45, 80.275, 80.275, 80.15833333333333, 80.15833333333333, 80.74166666666666, 80.74166666666666, 80.725, 80.725, 80.69166666666666, 80.69166666666666, 81.05833333333334, 81.05833333333334, 80.93333333333334, 80.93333333333334, 81.1, 81.1, 81.14166666666667, 81.14166666666667, 81.425, 81.425, 81.65833333333333, 81.65833333333333, 81.525, 81.525, 81.7, 81.7, 81.93333333333334, 81.93333333333334, 81.775, 81.775, 82.15833333333333, 82.15833333333333, 82.16666666666667, 82.16666666666667, 82.525, 82.525, 81.925, 81.925, 81.98333333333333, 81.98333333333333, 81.61666666666666, 81.61666666666666, 81.83333333333333, 81.83333333333333, 82.00833333333334, 82.00833333333334, 81.95833333333333, 81.95833333333333, 82.63333333333334, 82.63333333333334, 82.10833333333333, 82.10833333333333, 82.36666666666666, 82.36666666666666, 81.975, 81.975, 82.19166666666666, 82.19166666666666, 81.91666666666667, 81.91666666666667, 81.66666666666667, 81.66666666666667, 82.23333333333333, 82.23333333333333, 82.85833333333333, 82.85833333333333, 82.36666666666666, 82.36666666666666, 82.46666666666667, 82.46666666666667, 82.73333333333333, 82.73333333333333, 83.16666666666667, 83.16666666666667, 82.94166666666666, 82.94166666666666, 83.09166666666667, 83.09166666666667, 83.48333333333333, 83.48333333333333, 82.925, 82.925, 83.025, 83.025, 82.93333333333334, 82.93333333333334, 82.60833333333333, 82.60833333333333, 83.09166666666667, 83.09166666666667, 83.05, 83.05, 82.61666666666666, 82.61666666666666, 82.875, 82.875, 83.03333333333333, 83.03333333333333, 82.625, 82.625, 82.35833333333333, 82.35833333333333, 83.28333333333333, 83.28333333333333, 83.53333333333333, 83.53333333333333, 83.75833333333334, 83.75833333333334, 83.20833333333333, 83.20833333333333, 83.49166666666666, 83.49166666666666, 83.86666666666666, 83.86666666666666, 84.04166666666667, 84.04166666666667, 83.93333333333334, 83.93333333333334, 83.6, 83.6, 82.99166666666666, 82.99166666666666, 83.23333333333333, 83.23333333333333, 83.18333333333334, 83.18333333333334, 83.275, 83.275, 83.35833333333333, 83.35833333333333, 83.61666666666666, 83.61666666666666, 83.69166666666666, 83.69166666666666, 83.48333333333333, 83.48333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.462, Test loss: 2.108, Test accuracy: 19.06
Round   1, Train loss: 1.010, Test loss: 1.865, Test accuracy: 34.14
Round   2, Train loss: 0.911, Test loss: 1.411, Test accuracy: 41.75
Round   3, Train loss: 0.905, Test loss: 1.161, Test accuracy: 53.52
Round   4, Train loss: 0.772, Test loss: 1.036, Test accuracy: 57.75
Round   5, Train loss: 0.739, Test loss: 1.036, Test accuracy: 58.16
Round   6, Train loss: 0.720, Test loss: 0.732, Test accuracy: 66.24
Round   7, Train loss: 0.706, Test loss: 0.695, Test accuracy: 67.54
Round   8, Train loss: 0.613, Test loss: 0.689, Test accuracy: 68.50
Round   9, Train loss: 0.709, Test loss: 0.677, Test accuracy: 68.31
Round  10, Train loss: 0.654, Test loss: 0.667, Test accuracy: 69.53
Round  11, Train loss: 0.708, Test loss: 0.659, Test accuracy: 69.78
Round  12, Train loss: 0.597, Test loss: 0.662, Test accuracy: 69.76
Round  13, Train loss: 0.543, Test loss: 0.642, Test accuracy: 70.80
Round  14, Train loss: 0.696, Test loss: 0.641, Test accuracy: 71.17
Round  15, Train loss: 0.578, Test loss: 0.625, Test accuracy: 72.37
Round  16, Train loss: 0.617, Test loss: 0.604, Test accuracy: 73.44
Round  17, Train loss: 0.600, Test loss: 0.614, Test accuracy: 73.28
Round  18, Train loss: 0.675, Test loss: 0.620, Test accuracy: 73.21
Round  19, Train loss: 0.614, Test loss: 0.609, Test accuracy: 73.63
Round  20, Train loss: 0.603, Test loss: 0.582, Test accuracy: 74.42
Round  21, Train loss: 0.608, Test loss: 0.583, Test accuracy: 74.28
Round  22, Train loss: 0.572, Test loss: 0.576, Test accuracy: 74.80
Round  23, Train loss: 0.570, Test loss: 0.590, Test accuracy: 74.23
Round  24, Train loss: 0.599, Test loss: 0.544, Test accuracy: 76.79
Round  25, Train loss: 0.565, Test loss: 0.533, Test accuracy: 77.17
Round  26, Train loss: 0.509, Test loss: 0.530, Test accuracy: 77.85
Round  27, Train loss: 0.573, Test loss: 0.524, Test accuracy: 78.25
Round  28, Train loss: 0.551, Test loss: 0.515, Test accuracy: 78.74
Round  29, Train loss: 0.536, Test loss: 0.505, Test accuracy: 79.22
Round  30, Train loss: 0.499, Test loss: 0.515, Test accuracy: 78.69
Round  31, Train loss: 0.461, Test loss: 0.496, Test accuracy: 79.47
Round  32, Train loss: 0.535, Test loss: 0.496, Test accuracy: 79.58
Round  33, Train loss: 0.522, Test loss: 0.503, Test accuracy: 78.61
Round  34, Train loss: 0.500, Test loss: 0.498, Test accuracy: 78.88
Round  35, Train loss: 0.443, Test loss: 0.492, Test accuracy: 79.64
Round  36, Train loss: 0.476, Test loss: 0.492, Test accuracy: 79.53
Round  37, Train loss: 0.508, Test loss: 0.486, Test accuracy: 80.02
Round  38, Train loss: 0.416, Test loss: 0.476, Test accuracy: 80.44
Round  39, Train loss: 0.449, Test loss: 0.479, Test accuracy: 80.41
Round  40, Train loss: 0.410, Test loss: 0.474, Test accuracy: 80.67
Round  41, Train loss: 0.387, Test loss: 0.473, Test accuracy: 80.55
Round  42, Train loss: 0.504, Test loss: 0.457, Test accuracy: 81.35
Round  43, Train loss: 0.411, Test loss: 0.452, Test accuracy: 81.47
Round  44, Train loss: 0.383, Test loss: 0.451, Test accuracy: 81.35
Round  45, Train loss: 0.438, Test loss: 0.446, Test accuracy: 81.93
Round  46, Train loss: 0.430, Test loss: 0.450, Test accuracy: 81.71
Round  47, Train loss: 0.398, Test loss: 0.444, Test accuracy: 81.97
Round  48, Train loss: 0.341, Test loss: 0.445, Test accuracy: 82.11
Round  49, Train loss: 0.479, Test loss: 0.438, Test accuracy: 82.06
Round  50, Train loss: 0.372, Test loss: 0.443, Test accuracy: 82.01
Round  51, Train loss: 0.425, Test loss: 0.441, Test accuracy: 82.08
Round  52, Train loss: 0.391, Test loss: 0.454, Test accuracy: 81.39
Round  53, Train loss: 0.446, Test loss: 0.448, Test accuracy: 81.49
Round  54, Train loss: 0.359, Test loss: 0.439, Test accuracy: 82.16
Round  55, Train loss: 0.424, Test loss: 0.445, Test accuracy: 81.88
Round  56, Train loss: 0.358, Test loss: 0.443, Test accuracy: 82.11
Round  57, Train loss: 0.339, Test loss: 0.439, Test accuracy: 82.38
Round  58, Train loss: 0.319, Test loss: 0.426, Test accuracy: 82.72
Round  59, Train loss: 0.441, Test loss: 0.416, Test accuracy: 83.33
Round  60, Train loss: 0.371, Test loss: 0.416, Test accuracy: 83.15
Round  61, Train loss: 0.351, Test loss: 0.424, Test accuracy: 83.06
Round  62, Train loss: 0.370, Test loss: 0.424, Test accuracy: 82.93
Round  63, Train loss: 0.331, Test loss: 0.420, Test accuracy: 82.85
Round  64, Train loss: 0.356, Test loss: 0.426, Test accuracy: 82.59
Round  65, Train loss: 0.267, Test loss: 0.416, Test accuracy: 82.79
Round  66, Train loss: 0.348, Test loss: 0.410, Test accuracy: 83.24
Round  67, Train loss: 0.330, Test loss: 0.410, Test accuracy: 83.28
Round  68, Train loss: 0.328, Test loss: 0.412, Test accuracy: 83.39
Round  69, Train loss: 0.360, Test loss: 0.410, Test accuracy: 83.28
Round  70, Train loss: 0.328, Test loss: 0.412, Test accuracy: 83.18
Round  71, Train loss: 0.253, Test loss: 0.415, Test accuracy: 83.14
Round  72, Train loss: 0.339, Test loss: 0.410, Test accuracy: 83.31
Round  73, Train loss: 0.361, Test loss: 0.425, Test accuracy: 82.90
Round  74, Train loss: 0.284, Test loss: 0.419, Test accuracy: 83.11
Round  75, Train loss: 0.265, Test loss: 0.424, Test accuracy: 83.31
Round  76, Train loss: 0.380, Test loss: 0.408, Test accuracy: 83.70
Round  77, Train loss: 0.263, Test loss: 0.415, Test accuracy: 83.24
Round  78, Train loss: 0.292, Test loss: 0.420, Test accuracy: 83.10
Round  79, Train loss: 0.364, Test loss: 0.415, Test accuracy: 83.69
Round  80, Train loss: 0.318, Test loss: 0.407, Test accuracy: 83.64
Round  81, Train loss: 0.295, Test loss: 0.411, Test accuracy: 83.29
Round  82, Train loss: 0.292, Test loss: 0.413, Test accuracy: 83.72
Round  83, Train loss: 0.274, Test loss: 0.405, Test accuracy: 84.00
Round  84, Train loss: 0.239, Test loss: 0.416, Test accuracy: 83.59
Round  85, Train loss: 0.287, Test loss: 0.406, Test accuracy: 84.03
Round  86, Train loss: 0.273, Test loss: 0.412, Test accuracy: 83.74
Round  87, Train loss: 0.296, Test loss: 0.410, Test accuracy: 84.14
Round  88, Train loss: 0.358, Test loss: 0.403, Test accuracy: 84.12
Round  89, Train loss: 0.278, Test loss: 0.405, Test accuracy: 84.43
Round  90, Train loss: 0.269, Test loss: 0.406, Test accuracy: 84.29
Round  91, Train loss: 0.222, Test loss: 0.399, Test accuracy: 84.47
Round  92, Train loss: 0.307, Test loss: 0.402, Test accuracy: 84.59
Round  93, Train loss: 0.239, Test loss: 0.399, Test accuracy: 84.66
Round  94, Train loss: 0.277, Test loss: 0.406, Test accuracy: 84.47
Round  95, Train loss: 0.256, Test loss: 0.404, Test accuracy: 84.57
Round  96, Train loss: 0.203, Test loss: 0.405, Test accuracy: 84.47
Round  97, Train loss: 0.245, Test loss: 0.412, Test accuracy: 84.12
Round  98, Train loss: 0.293, Test loss: 0.411, Test accuracy: 84.62
Round  99, Train loss: 0.286, Test loss: 0.401, Test accuracy: 84.41
Final Round, Train loss: 0.230, Test loss: 0.404, Test accuracy: 84.88
Average accuracy final 10 rounds: 84.46583333333334 

1423.541825056076
[1.562321662902832, 3.124643325805664, 4.430826902389526, 5.737010478973389, 6.9843223094940186, 8.231634140014648, 9.475659847259521, 10.719685554504395, 11.963694334030151, 13.207703113555908, 14.45058536529541, 15.693467617034912, 16.941638946533203, 18.189810276031494, 19.44025731086731, 20.690704345703125, 21.935647010803223, 23.18058967590332, 24.431761741638184, 25.682933807373047, 26.925966024398804, 28.16899824142456, 29.42125105857849, 30.673503875732422, 31.91522216796875, 33.15694046020508, 34.40853452682495, 35.660128593444824, 36.9301381111145, 38.20014762878418, 39.43701219558716, 40.67387676239014, 41.91276168823242, 43.15164661407471, 44.38316893577576, 45.61469125747681, 46.85395812988281, 48.09322500228882, 49.328644037246704, 50.56406307220459, 51.875237226486206, 53.18641138076782, 54.44201850891113, 55.69762563705444, 56.94851803779602, 58.1994104385376, 59.45513391494751, 60.71085739135742, 61.96197175979614, 63.21308612823486, 64.47483086585999, 65.73657560348511, 66.98391127586365, 68.23124694824219, 69.48027610778809, 70.72930526733398, 71.97549796104431, 73.22169065475464, 74.47045183181763, 75.71921300888062, 76.97263598442078, 78.22605895996094, 79.47607254981995, 80.72608613967896, 81.97635340690613, 83.2266206741333, 84.47988152503967, 85.73314237594604, 86.97238278388977, 88.2116231918335, 89.45781707763672, 90.70401096343994, 91.95135426521301, 93.19869756698608, 94.44258952140808, 95.68648147583008, 96.9272153377533, 98.16794919967651, 99.40349912643433, 100.63904905319214, 101.88088464736938, 103.12272024154663, 104.36172413825989, 105.60072803497314, 106.82675886154175, 108.05278968811035, 109.27906847000122, 110.50534725189209, 111.74498963356018, 112.98463201522827, 114.23313975334167, 115.48164749145508, 116.72526574134827, 117.96888399124146, 119.2131896018982, 120.45749521255493, 121.69613289833069, 122.93477058410645, 124.17187714576721, 125.40898370742798, 126.6548707485199, 127.90075778961182, 129.1395137310028, 130.3782696723938, 131.62281322479248, 132.86735677719116, 134.11068654060364, 135.3540163040161, 136.58687806129456, 137.819739818573, 139.0594403743744, 140.29914093017578, 141.54631853103638, 142.79349613189697, 144.03926038742065, 145.28502464294434, 146.53118538856506, 147.7773461341858, 149.01609826087952, 150.25485038757324, 151.49633502960205, 152.73781967163086, 153.97478818893433, 155.2117567062378, 156.51615405082703, 157.82055139541626, 159.11795735359192, 160.41536331176758, 161.71389722824097, 163.01243114471436, 164.24432492256165, 165.47621870040894, 166.70693230628967, 167.9376459121704, 169.2534806728363, 170.5693154335022, 171.89063501358032, 173.21195459365845, 174.5288302898407, 175.84570598602295, 177.07926630973816, 178.31282663345337, 179.58442664146423, 180.8560266494751, 182.12174773216248, 183.38746881484985, 184.67205381393433, 185.9566388130188, 187.20870852470398, 188.46077823638916, 189.68389678001404, 190.90701532363892, 192.12953567504883, 193.35205602645874, 194.5793333053589, 195.80661058425903, 197.03584790229797, 198.2650852203369, 199.49819612503052, 200.73130702972412, 201.9574282169342, 203.1835494041443, 204.40517377853394, 205.62679815292358, 206.85367846488953, 208.08055877685547, 209.32453775405884, 210.5685167312622, 211.83856582641602, 213.10861492156982, 214.33541655540466, 215.5622181892395, 216.7967653274536, 218.03131246566772, 219.26738572120667, 220.5034589767456, 221.72865080833435, 222.9538426399231, 224.18162560462952, 225.40940856933594, 226.63139081001282, 227.8533730506897, 229.08585214614868, 230.31833124160767, 231.55634021759033, 232.794349193573, 234.01572394371033, 235.23709869384766, 236.46708917617798, 237.6970796585083, 238.92060017585754, 240.1441206932068, 241.3672432899475, 242.59036588668823, 243.81501936912537, 245.0396728515625, 246.26671934127808, 247.49376583099365, 248.73304224014282, 249.972318649292, 251.95537662506104, 253.93843460083008]
[19.058333333333334, 19.058333333333334, 34.141666666666666, 34.141666666666666, 41.75, 41.75, 53.525, 53.525, 57.75, 57.75, 58.15833333333333, 58.15833333333333, 66.24166666666666, 66.24166666666666, 67.54166666666667, 67.54166666666667, 68.5, 68.5, 68.30833333333334, 68.30833333333334, 69.525, 69.525, 69.775, 69.775, 69.75833333333334, 69.75833333333334, 70.8, 70.8, 71.175, 71.175, 72.36666666666666, 72.36666666666666, 73.44166666666666, 73.44166666666666, 73.28333333333333, 73.28333333333333, 73.20833333333333, 73.20833333333333, 73.63333333333334, 73.63333333333334, 74.41666666666667, 74.41666666666667, 74.28333333333333, 74.28333333333333, 74.8, 74.8, 74.23333333333333, 74.23333333333333, 76.79166666666667, 76.79166666666667, 77.175, 77.175, 77.85, 77.85, 78.25, 78.25, 78.74166666666666, 78.74166666666666, 79.225, 79.225, 78.69166666666666, 78.69166666666666, 79.46666666666667, 79.46666666666667, 79.575, 79.575, 78.60833333333333, 78.60833333333333, 78.875, 78.875, 79.64166666666667, 79.64166666666667, 79.53333333333333, 79.53333333333333, 80.01666666666667, 80.01666666666667, 80.44166666666666, 80.44166666666666, 80.40833333333333, 80.40833333333333, 80.66666666666667, 80.66666666666667, 80.55, 80.55, 81.35, 81.35, 81.475, 81.475, 81.35, 81.35, 81.93333333333334, 81.93333333333334, 81.70833333333333, 81.70833333333333, 81.975, 81.975, 82.10833333333333, 82.10833333333333, 82.05833333333334, 82.05833333333334, 82.00833333333334, 82.00833333333334, 82.08333333333333, 82.08333333333333, 81.39166666666667, 81.39166666666667, 81.49166666666666, 81.49166666666666, 82.15833333333333, 82.15833333333333, 81.88333333333334, 81.88333333333334, 82.10833333333333, 82.10833333333333, 82.375, 82.375, 82.725, 82.725, 83.325, 83.325, 83.15, 83.15, 83.05833333333334, 83.05833333333334, 82.93333333333334, 82.93333333333334, 82.85, 82.85, 82.59166666666667, 82.59166666666667, 82.79166666666667, 82.79166666666667, 83.24166666666666, 83.24166666666666, 83.28333333333333, 83.28333333333333, 83.39166666666667, 83.39166666666667, 83.28333333333333, 83.28333333333333, 83.18333333333334, 83.18333333333334, 83.14166666666667, 83.14166666666667, 83.30833333333334, 83.30833333333334, 82.9, 82.9, 83.10833333333333, 83.10833333333333, 83.30833333333334, 83.30833333333334, 83.7, 83.7, 83.24166666666666, 83.24166666666666, 83.1, 83.1, 83.69166666666666, 83.69166666666666, 83.64166666666667, 83.64166666666667, 83.29166666666667, 83.29166666666667, 83.725, 83.725, 84.0, 84.0, 83.59166666666667, 83.59166666666667, 84.025, 84.025, 83.74166666666666, 83.74166666666666, 84.14166666666667, 84.14166666666667, 84.11666666666666, 84.11666666666666, 84.43333333333334, 84.43333333333334, 84.29166666666667, 84.29166666666667, 84.46666666666667, 84.46666666666667, 84.59166666666667, 84.59166666666667, 84.65833333333333, 84.65833333333333, 84.46666666666667, 84.46666666666667, 84.56666666666666, 84.56666666666666, 84.475, 84.475, 84.11666666666666, 84.11666666666666, 84.61666666666666, 84.61666666666666, 84.40833333333333, 84.40833333333333, 84.88333333333334, 84.88333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.469, Test loss: 1.868, Test accuracy: 33.83
Round   1, Train loss: 0.899, Test loss: 1.892, Test accuracy: 32.92
Round   2, Train loss: 0.816, Test loss: 1.472, Test accuracy: 46.00
Round   3, Train loss: 0.774, Test loss: 1.326, Test accuracy: 50.45
Round   4, Train loss: 0.755, Test loss: 0.905, Test accuracy: 62.46
Round   5, Train loss: 0.744, Test loss: 0.763, Test accuracy: 68.95
Round   6, Train loss: 0.658, Test loss: 0.752, Test accuracy: 70.48
Round   7, Train loss: 0.661, Test loss: 0.710, Test accuracy: 71.88
Round   8, Train loss: 0.610, Test loss: 0.715, Test accuracy: 72.29
Round   9, Train loss: 0.635, Test loss: 0.703, Test accuracy: 72.58
Round  10, Train loss: 0.587, Test loss: 0.715, Test accuracy: 73.10
Round  11, Train loss: 0.616, Test loss: 0.656, Test accuracy: 75.49
Round  12, Train loss: 0.565, Test loss: 0.641, Test accuracy: 75.42
Round  13, Train loss: 0.573, Test loss: 0.622, Test accuracy: 76.84
Round  14, Train loss: 0.628, Test loss: 0.522, Test accuracy: 78.89
Round  15, Train loss: 0.585, Test loss: 0.524, Test accuracy: 79.43
Round  16, Train loss: 0.579, Test loss: 0.517, Test accuracy: 79.48
Round  17, Train loss: 0.487, Test loss: 0.503, Test accuracy: 80.74
Round  18, Train loss: 0.429, Test loss: 0.490, Test accuracy: 80.72
Round  19, Train loss: 0.499, Test loss: 0.491, Test accuracy: 81.03
Round  20, Train loss: 0.416, Test loss: 0.478, Test accuracy: 81.23
Round  21, Train loss: 0.479, Test loss: 0.467, Test accuracy: 81.60
Round  22, Train loss: 0.488, Test loss: 0.457, Test accuracy: 82.48
Round  23, Train loss: 0.462, Test loss: 0.449, Test accuracy: 82.99
Round  24, Train loss: 0.469, Test loss: 0.452, Test accuracy: 83.01
Round  25, Train loss: 0.536, Test loss: 0.447, Test accuracy: 82.87
Round  26, Train loss: 0.450, Test loss: 0.439, Test accuracy: 83.60
Round  27, Train loss: 0.413, Test loss: 0.428, Test accuracy: 83.23
Round  28, Train loss: 0.435, Test loss: 0.427, Test accuracy: 83.72
Round  29, Train loss: 0.394, Test loss: 0.428, Test accuracy: 83.78
Round  30, Train loss: 0.437, Test loss: 0.418, Test accuracy: 84.04
Round  31, Train loss: 0.403, Test loss: 0.414, Test accuracy: 84.14
Round  32, Train loss: 0.385, Test loss: 0.409, Test accuracy: 84.25
Round  33, Train loss: 0.383, Test loss: 0.401, Test accuracy: 84.60
Round  34, Train loss: 0.392, Test loss: 0.398, Test accuracy: 84.71
Round  35, Train loss: 0.432, Test loss: 0.392, Test accuracy: 85.03
Round  36, Train loss: 0.409, Test loss: 0.391, Test accuracy: 85.41
Round  37, Train loss: 0.412, Test loss: 0.388, Test accuracy: 85.27
Round  38, Train loss: 0.366, Test loss: 0.386, Test accuracy: 85.47
Round  39, Train loss: 0.372, Test loss: 0.381, Test accuracy: 85.50
Round  40, Train loss: 0.355, Test loss: 0.377, Test accuracy: 85.42
Round  41, Train loss: 0.392, Test loss: 0.375, Test accuracy: 86.18
Round  42, Train loss: 0.417, Test loss: 0.373, Test accuracy: 85.67
Round  43, Train loss: 0.359, Test loss: 0.375, Test accuracy: 86.04
Round  44, Train loss: 0.373, Test loss: 0.368, Test accuracy: 86.08
Round  45, Train loss: 0.360, Test loss: 0.370, Test accuracy: 85.81
Round  46, Train loss: 0.395, Test loss: 0.368, Test accuracy: 85.90
Round  47, Train loss: 0.292, Test loss: 0.357, Test accuracy: 86.41
Round  48, Train loss: 0.279, Test loss: 0.364, Test accuracy: 86.02
Round  49, Train loss: 0.302, Test loss: 0.349, Test accuracy: 86.62
Round  50, Train loss: 0.337, Test loss: 0.358, Test accuracy: 86.62
Round  51, Train loss: 0.290, Test loss: 0.358, Test accuracy: 86.58
Round  52, Train loss: 0.373, Test loss: 0.351, Test accuracy: 86.53
Round  53, Train loss: 0.315, Test loss: 0.356, Test accuracy: 86.35
Round  54, Train loss: 0.358, Test loss: 0.354, Test accuracy: 86.43
Round  55, Train loss: 0.349, Test loss: 0.350, Test accuracy: 86.65
Round  56, Train loss: 0.306, Test loss: 0.345, Test accuracy: 86.71
Round  57, Train loss: 0.325, Test loss: 0.348, Test accuracy: 86.80
Round  58, Train loss: 0.353, Test loss: 0.344, Test accuracy: 87.04
Round  59, Train loss: 0.318, Test loss: 0.343, Test accuracy: 86.85
Round  60, Train loss: 0.288, Test loss: 0.351, Test accuracy: 86.69
Round  61, Train loss: 0.315, Test loss: 0.341, Test accuracy: 87.05
Round  62, Train loss: 0.273, Test loss: 0.345, Test accuracy: 87.03
Round  63, Train loss: 0.285, Test loss: 0.334, Test accuracy: 87.13
Round  64, Train loss: 0.290, Test loss: 0.335, Test accuracy: 87.16
Round  65, Train loss: 0.309, Test loss: 0.333, Test accuracy: 87.29
Round  66, Train loss: 0.273, Test loss: 0.334, Test accuracy: 87.31
Round  67, Train loss: 0.267, Test loss: 0.341, Test accuracy: 87.03
Round  68, Train loss: 0.277, Test loss: 0.337, Test accuracy: 87.03
Round  69, Train loss: 0.286, Test loss: 0.337, Test accuracy: 87.27
Round  70, Train loss: 0.294, Test loss: 0.330, Test accuracy: 87.44
Round  71, Train loss: 0.267, Test loss: 0.330, Test accuracy: 87.51
Round  72, Train loss: 0.282, Test loss: 0.333, Test accuracy: 87.32
Round  73, Train loss: 0.297, Test loss: 0.332, Test accuracy: 87.37
Round  74, Train loss: 0.281, Test loss: 0.330, Test accuracy: 87.53
Round  75, Train loss: 0.286, Test loss: 0.330, Test accuracy: 87.62
Round  76, Train loss: 0.265, Test loss: 0.333, Test accuracy: 87.44
Round  77, Train loss: 0.274, Test loss: 0.329, Test accuracy: 87.62
Round  78, Train loss: 0.242, Test loss: 0.328, Test accuracy: 87.59
Round  79, Train loss: 0.254, Test loss: 0.338, Test accuracy: 87.45
Round  80, Train loss: 0.287, Test loss: 0.325, Test accuracy: 87.88
Round  81, Train loss: 0.200, Test loss: 0.328, Test accuracy: 87.70
Round  82, Train loss: 0.222, Test loss: 0.327, Test accuracy: 87.90
Round  83, Train loss: 0.252, Test loss: 0.334, Test accuracy: 87.42
Round  84, Train loss: 0.282, Test loss: 0.325, Test accuracy: 87.70
Round  85, Train loss: 0.212, Test loss: 0.328, Test accuracy: 87.46
Round  86, Train loss: 0.237, Test loss: 0.319, Test accuracy: 87.93
Round  87, Train loss: 0.206, Test loss: 0.322, Test accuracy: 87.67
Round  88, Train loss: 0.210, Test loss: 0.329, Test accuracy: 87.72
Round  89, Train loss: 0.238, Test loss: 0.321, Test accuracy: 87.92
Round  90, Train loss: 0.233, Test loss: 0.327, Test accuracy: 87.47
Round  91, Train loss: 0.248, Test loss: 0.328, Test accuracy: 87.53
Round  92, Train loss: 0.256, Test loss: 0.328, Test accuracy: 87.63
Round  93, Train loss: 0.219, Test loss: 0.329, Test accuracy: 87.59
Round  94, Train loss: 0.217, Test loss: 0.322, Test accuracy: 87.89
Round  95, Train loss: 0.222, Test loss: 0.336, Test accuracy: 87.65
Round  96, Train loss: 0.201, Test loss: 0.334, Test accuracy: 87.64
Round  97, Train loss: 0.220, Test loss: 0.332, Test accuracy: 87.96
Round  98, Train loss: 0.248, Test loss: 0.327, Test accuracy: 87.65
Round  99, Train loss: 0.250, Test loss: 0.331, Test accuracy: 87.60
Final Round, Train loss: 0.172, Test loss: 0.327, Test accuracy: 88.01
Average accuracy final 10 rounds: 87.66166666666668
1640.8901307582855
[2.081815004348755, 4.16363000869751, 5.81052303314209, 7.45741605758667, 9.025039672851562, 10.592663288116455, 12.167069911956787, 13.74147653579712, 15.309425830841064, 16.87737512588501, 18.454898357391357, 20.032421588897705, 21.666045665740967, 23.29966974258423, 24.869853496551514, 26.4400372505188, 28.014286756515503, 29.588536262512207, 31.15192985534668, 32.71532344818115, 34.277535915374756, 35.83974838256836, 37.40922260284424, 38.97869682312012, 40.54742169380188, 42.11614656448364, 43.69161653518677, 45.26708650588989, 46.83846473693848, 48.40984296798706, 50.028494358062744, 51.64714574813843, 53.21207666397095, 54.77700757980347, 56.35383224487305, 57.93065690994263, 59.50937604904175, 61.08809518814087, 62.65423035621643, 64.22036552429199, 65.78310465812683, 67.34584379196167, 68.899090051651, 70.45233631134033, 72.01208591461182, 73.5718355178833, 75.1396632194519, 76.70749092102051, 78.27446389198303, 79.84143686294556, 81.3981626033783, 82.95488834381104, 84.52102279663086, 86.08715724945068, 87.6509051322937, 89.21465301513672, 90.79283022880554, 92.37100744247437, 93.9259192943573, 95.48083114624023, 97.05248856544495, 98.62414598464966, 100.1951653957367, 101.76618480682373, 103.33900356292725, 104.91182231903076, 106.4838707447052, 108.05591917037964, 109.609943151474, 111.16396713256836, 112.74455189704895, 114.32513666152954, 115.89680647850037, 117.46847629547119, 119.03486967086792, 120.60126304626465, 122.17435240745544, 123.74744176864624, 125.3409035205841, 126.93436527252197, 128.5085813999176, 130.08279752731323, 131.6700096130371, 133.257221698761, 134.83937883377075, 136.42153596878052, 137.99629402160645, 139.57105207443237, 141.14869713783264, 142.7263422012329, 144.29584503173828, 145.86534786224365, 147.45870804786682, 149.05206823349, 150.6313235759735, 152.21057891845703, 153.78960704803467, 155.3686351776123, 156.96662187576294, 158.56460857391357, 160.15006828308105, 161.73552799224854, 163.3290309906006, 164.92253398895264, 166.51987886428833, 168.11722373962402, 169.68840098381042, 171.25957822799683, 172.83413457870483, 174.40869092941284, 175.98735928535461, 177.5660276412964, 179.142831325531, 180.71963500976562, 182.2913088798523, 183.86298274993896, 185.4231264591217, 186.98327016830444, 188.59138703346252, 190.1995038986206, 191.79105424880981, 193.38260459899902, 194.97333407402039, 196.56406354904175, 198.1484763622284, 199.73288917541504, 201.29822850227356, 202.86356782913208, 204.4469587802887, 206.0303497314453, 207.616872549057, 209.2033953666687, 210.79022479057312, 212.37705421447754, 213.96327805519104, 215.54950189590454, 217.1406512260437, 218.73180055618286, 220.33670377731323, 221.9416069984436, 223.53475046157837, 225.12789392471313, 226.69981265068054, 228.27173137664795, 229.85777354240417, 231.4438157081604, 233.00901317596436, 234.5742106437683, 236.14812898635864, 237.72204732894897, 239.2974672317505, 240.872887134552, 242.45458602905273, 244.03628492355347, 245.61502861976624, 247.193772315979, 248.78079628944397, 250.36782026290894, 251.96927666664124, 253.57073307037354, 255.16661715507507, 256.7625012397766, 258.365309715271, 259.9681181907654, 261.57078075408936, 263.17344331741333, 264.77683544158936, 266.3802275657654, 267.9677255153656, 269.5552234649658, 271.1506133079529, 272.74600315093994, 274.3470847606659, 275.94816637039185, 277.55457520484924, 279.16098403930664, 280.7604660987854, 282.35994815826416, 283.95281863212585, 285.54568910598755, 287.21228075027466, 288.87887239456177, 290.4650604724884, 292.05124855041504, 293.64065742492676, 295.2300662994385, 296.82787704467773, 298.425687789917, 300.01920461654663, 301.61272144317627, 303.1858694553375, 304.7590174674988, 306.3607771396637, 307.9625368118286, 309.5894856452942, 311.21643447875977, 312.81503462791443, 314.4136347770691, 316.0523567199707, 317.6910786628723, 319.97604632377625, 322.2610139846802]
[33.833333333333336, 33.833333333333336, 32.925, 32.925, 46.0, 46.0, 50.45, 50.45, 62.458333333333336, 62.458333333333336, 68.95, 68.95, 70.48333333333333, 70.48333333333333, 71.875, 71.875, 72.29166666666667, 72.29166666666667, 72.575, 72.575, 73.1, 73.1, 75.49166666666666, 75.49166666666666, 75.425, 75.425, 76.84166666666667, 76.84166666666667, 78.89166666666667, 78.89166666666667, 79.43333333333334, 79.43333333333334, 79.48333333333333, 79.48333333333333, 80.74166666666666, 80.74166666666666, 80.71666666666667, 80.71666666666667, 81.03333333333333, 81.03333333333333, 81.23333333333333, 81.23333333333333, 81.6, 81.6, 82.48333333333333, 82.48333333333333, 82.99166666666666, 82.99166666666666, 83.00833333333334, 83.00833333333334, 82.86666666666666, 82.86666666666666, 83.6, 83.6, 83.23333333333333, 83.23333333333333, 83.725, 83.725, 83.775, 83.775, 84.04166666666667, 84.04166666666667, 84.14166666666667, 84.14166666666667, 84.25, 84.25, 84.6, 84.6, 84.70833333333333, 84.70833333333333, 85.025, 85.025, 85.40833333333333, 85.40833333333333, 85.26666666666667, 85.26666666666667, 85.475, 85.475, 85.5, 85.5, 85.425, 85.425, 86.18333333333334, 86.18333333333334, 85.66666666666667, 85.66666666666667, 86.04166666666667, 86.04166666666667, 86.08333333333333, 86.08333333333333, 85.80833333333334, 85.80833333333334, 85.9, 85.9, 86.40833333333333, 86.40833333333333, 86.01666666666667, 86.01666666666667, 86.61666666666666, 86.61666666666666, 86.625, 86.625, 86.575, 86.575, 86.53333333333333, 86.53333333333333, 86.35, 86.35, 86.43333333333334, 86.43333333333334, 86.65, 86.65, 86.70833333333333, 86.70833333333333, 86.8, 86.8, 87.04166666666667, 87.04166666666667, 86.85, 86.85, 86.69166666666666, 86.69166666666666, 87.05, 87.05, 87.025, 87.025, 87.13333333333334, 87.13333333333334, 87.15833333333333, 87.15833333333333, 87.29166666666667, 87.29166666666667, 87.30833333333334, 87.30833333333334, 87.03333333333333, 87.03333333333333, 87.025, 87.025, 87.26666666666667, 87.26666666666667, 87.44166666666666, 87.44166666666666, 87.50833333333334, 87.50833333333334, 87.31666666666666, 87.31666666666666, 87.36666666666666, 87.36666666666666, 87.525, 87.525, 87.625, 87.625, 87.44166666666666, 87.44166666666666, 87.625, 87.625, 87.59166666666667, 87.59166666666667, 87.45, 87.45, 87.88333333333334, 87.88333333333334, 87.7, 87.7, 87.9, 87.9, 87.41666666666667, 87.41666666666667, 87.7, 87.7, 87.45833333333333, 87.45833333333333, 87.93333333333334, 87.93333333333334, 87.66666666666667, 87.66666666666667, 87.71666666666667, 87.71666666666667, 87.91666666666667, 87.91666666666667, 87.475, 87.475, 87.525, 87.525, 87.63333333333334, 87.63333333333334, 87.59166666666667, 87.59166666666667, 87.89166666666667, 87.89166666666667, 87.65, 87.65, 87.64166666666667, 87.64166666666667, 87.95833333333333, 87.95833333333333, 87.65, 87.65, 87.6, 87.6, 88.00833333333334, 88.00833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.099, Test loss: 2.309, Test accuracy: 17.06
Round   1, Train loss: 0.959, Test loss: 2.023, Test accuracy: 34.73
Round   2, Train loss: 0.835, Test loss: 1.987, Test accuracy: 31.16
Round   3, Train loss: 0.797, Test loss: 1.895, Test accuracy: 33.29
Round   4, Train loss: 0.765, Test loss: 1.920, Test accuracy: 28.73
Round   5, Train loss: 0.731, Test loss: 2.178, Test accuracy: 34.75
Round   6, Train loss: 0.680, Test loss: 1.719, Test accuracy: 37.21
Round   7, Train loss: 0.689, Test loss: 1.831, Test accuracy: 36.55
Round   8, Train loss: 0.633, Test loss: 1.992, Test accuracy: 34.52
Round   9, Train loss: 0.573, Test loss: 1.602, Test accuracy: 44.72
Round  10, Train loss: 0.594, Test loss: 1.544, Test accuracy: 43.07
Round  11, Train loss: 0.665, Test loss: 2.079, Test accuracy: 29.85
Round  12, Train loss: 0.568, Test loss: 1.630, Test accuracy: 44.25
Round  13, Train loss: 0.514, Test loss: 1.622, Test accuracy: 46.99
Round  14, Train loss: 0.474, Test loss: 1.797, Test accuracy: 44.43
Round  15, Train loss: 0.478, Test loss: 1.660, Test accuracy: 47.89
Round  16, Train loss: 0.490, Test loss: 1.398, Test accuracy: 53.80
Round  17, Train loss: 0.424, Test loss: 1.550, Test accuracy: 50.12
Round  18, Train loss: 0.463, Test loss: 1.362, Test accuracy: 55.49
Round  19, Train loss: 0.508, Test loss: 1.523, Test accuracy: 49.54
Round  20, Train loss: 0.410, Test loss: 1.741, Test accuracy: 48.95
Round  21, Train loss: 0.454, Test loss: 1.451, Test accuracy: 51.95
Round  22, Train loss: 0.454, Test loss: 1.249, Test accuracy: 57.39
Round  23, Train loss: 0.421, Test loss: 1.492, Test accuracy: 52.16
Round  24, Train loss: 0.437, Test loss: 1.284, Test accuracy: 56.76
Round  25, Train loss: 0.470, Test loss: 1.534, Test accuracy: 47.72
Round  26, Train loss: 0.396, Test loss: 1.196, Test accuracy: 58.84
Round  27, Train loss: 0.423, Test loss: 1.205, Test accuracy: 57.94
Round  28, Train loss: 0.376, Test loss: 1.328, Test accuracy: 56.77
Round  29, Train loss: 0.395, Test loss: 1.330, Test accuracy: 53.85
Round  30, Train loss: 0.377, Test loss: 1.467, Test accuracy: 50.92
Round  31, Train loss: 0.395, Test loss: 1.494, Test accuracy: 50.59
Round  32, Train loss: 0.351, Test loss: 1.459, Test accuracy: 54.06
Round  33, Train loss: 0.410, Test loss: 1.377, Test accuracy: 52.62
Round  34, Train loss: 0.386, Test loss: 1.216, Test accuracy: 59.17
Round  35, Train loss: 0.319, Test loss: 1.192, Test accuracy: 59.84
Round  36, Train loss: 0.426, Test loss: 1.245, Test accuracy: 58.17
Round  37, Train loss: 0.316, Test loss: 1.365, Test accuracy: 54.87
Round  38, Train loss: 0.394, Test loss: 1.807, Test accuracy: 46.27
Round  39, Train loss: 0.335, Test loss: 1.285, Test accuracy: 58.67
Round  40, Train loss: 0.318, Test loss: 1.284, Test accuracy: 58.25
Round  41, Train loss: 0.325, Test loss: 1.159, Test accuracy: 59.66
Round  42, Train loss: 0.301, Test loss: 1.605, Test accuracy: 50.43
Round  43, Train loss: 0.302, Test loss: 1.323, Test accuracy: 56.67
Round  44, Train loss: 0.279, Test loss: 1.160, Test accuracy: 61.73
Round  45, Train loss: 0.294, Test loss: 1.270, Test accuracy: 59.01
Round  46, Train loss: 0.330, Test loss: 1.192, Test accuracy: 60.29
Round  47, Train loss: 0.281, Test loss: 1.131, Test accuracy: 61.93
Round  48, Train loss: 0.276, Test loss: 1.147, Test accuracy: 61.07
Round  49, Train loss: 0.301, Test loss: 1.390, Test accuracy: 56.12
Round  50, Train loss: 0.300, Test loss: 1.111, Test accuracy: 62.79
Round  51, Train loss: 0.304, Test loss: 1.112, Test accuracy: 63.22
Round  52, Train loss: 0.228, Test loss: 1.356, Test accuracy: 58.92
Round  53, Train loss: 0.275, Test loss: 1.246, Test accuracy: 61.55
Round  54, Train loss: 0.271, Test loss: 1.256, Test accuracy: 60.38
Round  55, Train loss: 0.270, Test loss: 1.178, Test accuracy: 61.25
Round  56, Train loss: 0.247, Test loss: 1.071, Test accuracy: 64.27
Round  57, Train loss: 0.266, Test loss: 1.358, Test accuracy: 61.37
Round  58, Train loss: 0.232, Test loss: 1.338, Test accuracy: 60.92
Round  59, Train loss: 0.259, Test loss: 1.370, Test accuracy: 59.89
Round  60, Train loss: 0.255, Test loss: 1.278, Test accuracy: 59.04
Round  61, Train loss: 0.244, Test loss: 1.232, Test accuracy: 60.48
Round  62, Train loss: 0.203, Test loss: 1.274, Test accuracy: 60.22
Round  63, Train loss: 0.180, Test loss: 1.215, Test accuracy: 62.32
Round  64, Train loss: 0.243, Test loss: 1.399, Test accuracy: 58.73
Round  65, Train loss: 0.189, Test loss: 1.362, Test accuracy: 61.67
Round  66, Train loss: 0.245, Test loss: 1.630, Test accuracy: 54.11
Round  67, Train loss: 0.194, Test loss: 1.186, Test accuracy: 63.17
Round  68, Train loss: 0.251, Test loss: 1.160, Test accuracy: 62.97
Round  69, Train loss: 0.219, Test loss: 1.332, Test accuracy: 59.68
Round  70, Train loss: 0.196, Test loss: 1.321, Test accuracy: 60.62
Round  71, Train loss: 0.174, Test loss: 1.071, Test accuracy: 65.92
Round  72, Train loss: 0.198, Test loss: 1.161, Test accuracy: 64.53
Round  73, Train loss: 0.220, Test loss: 1.104, Test accuracy: 64.22
Round  74, Train loss: 0.162, Test loss: 1.226, Test accuracy: 61.77
Round  75, Train loss: 0.216, Test loss: 1.195, Test accuracy: 61.41
Round  76, Train loss: 0.173, Test loss: 1.196, Test accuracy: 61.23
Round  77, Train loss: 0.209, Test loss: 1.318, Test accuracy: 60.67
Round  78, Train loss: 0.219, Test loss: 1.314, Test accuracy: 59.62
Round  79, Train loss: 0.164, Test loss: 1.076, Test accuracy: 65.08
Round  80, Train loss: 0.203, Test loss: 1.152, Test accuracy: 62.15
Round  81, Train loss: 0.195, Test loss: 1.436, Test accuracy: 59.55
Round  82, Train loss: 0.160, Test loss: 1.505, Test accuracy: 56.98
Round  83, Train loss: 0.190, Test loss: 1.284, Test accuracy: 63.66
Round  84, Train loss: 0.196, Test loss: 1.297, Test accuracy: 60.23
Round  85, Train loss: 0.177, Test loss: 1.341, Test accuracy: 60.16
Round  86, Train loss: 0.141, Test loss: 1.237, Test accuracy: 63.16
Round  87, Train loss: 0.131, Test loss: 1.648, Test accuracy: 57.12
Round  88, Train loss: 0.197, Test loss: 1.157, Test accuracy: 63.96
Round  89, Train loss: 0.145, Test loss: 1.288, Test accuracy: 63.27
Round  90, Train loss: 0.167, Test loss: 1.296, Test accuracy: 63.67
Round  91, Train loss: 0.182, Test loss: 1.091, Test accuracy: 66.41
Round  92, Train loss: 0.157, Test loss: 1.423, Test accuracy: 62.30
Round  93, Train loss: 0.143, Test loss: 1.229, Test accuracy: 63.60
Round  94, Train loss: 0.146, Test loss: 1.327, Test accuracy: 62.80
Round  95, Train loss: 0.129, Test loss: 1.278, Test accuracy: 63.71
Round  96, Train loss: 0.168, Test loss: 1.390, Test accuracy: 60.52
Round  97, Train loss: 0.157, Test loss: 1.151, Test accuracy: 64.58
Round  98, Train loss: 0.186, Test loss: 1.127, Test accuracy: 65.25
Round  99, Train loss: 0.187, Test loss: 1.487, Test accuracy: 59.09
Final Round, Train loss: 0.139, Test loss: 1.068, Test accuracy: 66.97
Average accuracy final 10 rounds: 63.193333333333335
2413.3259847164154
[4.031665802001953, 7.577914237976074, 11.133859634399414, 14.681947231292725, 18.2532000541687, 21.85633158683777, 25.36339569091797, 28.877309560775757, 32.40100932121277, 35.89797902107239, 39.40205788612366, 42.95854353904724, 46.46809935569763, 50.16166305541992, 53.89526677131653, 57.614460468292236, 61.27808165550232, 64.78098630905151, 68.40105652809143, 71.91870355606079, 75.41691708564758, 78.93495416641235, 82.40823698043823, 85.90524911880493, 89.41799402236938, 92.91127443313599, 96.42152070999146, 100.12381720542908, 103.61873364448547, 107.13918113708496, 110.6309986114502, 114.12302708625793, 117.55553030967712, 121.00564002990723, 124.43548345565796, 127.88058733940125, 131.31427717208862, 134.7374827861786, 138.1690378189087, 141.61156821250916, 145.05099391937256, 148.5046100616455, 151.93919563293457, 155.35521960258484, 158.93014526367188, 162.36023569107056, 165.7836675643921, 169.20491337776184, 172.6368224620819, 176.0592143535614, 179.4726688861847, 182.88176846504211, 186.3033905029297, 189.731027841568, 193.15316152572632, 196.65373921394348, 200.06796073913574, 203.48260927200317, 206.83393573760986, 210.22741103172302, 213.62137293815613, 217.0245943069458, 220.4288148880005, 223.80166840553284, 227.41954469680786, 231.02354741096497, 234.39482975006104, 237.79566311836243, 241.16688585281372, 244.57691264152527, 247.95839071273804, 251.34797406196594, 254.7396628856659, 258.1259138584137, 261.5308847427368, 264.91580295562744, 268.3840699195862, 271.7702360153198, 275.1623685359955, 278.5589551925659, 281.9418296813965, 285.33988642692566, 288.7122037410736, 292.1082010269165, 295.5249352455139, 298.9105188846588, 302.30867648124695, 305.6895258426666, 309.0848922729492, 312.47750067710876, 315.8777632713318, 319.2865607738495, 322.6463534832001, 326.0520474910736, 329.4199733734131, 332.8075487613678, 336.1944513320923, 339.56530261039734, 342.9713604450226, 346.31961727142334, 349.1822147369385]
[17.058333333333334, 34.733333333333334, 31.158333333333335, 33.291666666666664, 28.725, 34.75, 37.208333333333336, 36.55, 34.516666666666666, 44.71666666666667, 43.06666666666667, 29.85, 44.25, 46.99166666666667, 44.43333333333333, 47.891666666666666, 53.8, 50.11666666666667, 55.49166666666667, 49.541666666666664, 48.95, 51.95, 57.391666666666666, 52.15833333333333, 56.75833333333333, 47.71666666666667, 58.84166666666667, 57.94166666666667, 56.766666666666666, 53.85, 50.925, 50.59166666666667, 54.05833333333333, 52.625, 59.175, 59.84166666666667, 58.166666666666664, 54.86666666666667, 46.266666666666666, 58.666666666666664, 58.25, 59.65833333333333, 50.43333333333333, 56.675, 61.733333333333334, 59.00833333333333, 60.291666666666664, 61.93333333333333, 61.06666666666667, 56.11666666666667, 62.791666666666664, 63.21666666666667, 58.925, 61.55, 60.38333333333333, 61.25, 64.26666666666667, 61.36666666666667, 60.925, 59.891666666666666, 59.041666666666664, 60.483333333333334, 60.21666666666667, 62.31666666666667, 58.733333333333334, 61.666666666666664, 54.108333333333334, 63.166666666666664, 62.96666666666667, 59.68333333333333, 60.625, 65.925, 64.53333333333333, 64.225, 61.766666666666666, 61.40833333333333, 61.233333333333334, 60.675, 59.625, 65.08333333333333, 62.15, 59.55, 56.983333333333334, 63.65833333333333, 60.225, 60.15833333333333, 63.15833333333333, 57.11666666666667, 63.958333333333336, 63.275, 63.675, 66.40833333333333, 62.3, 63.6, 62.8, 63.708333333333336, 60.516666666666666, 64.58333333333333, 65.25, 59.09166666666667, 66.96666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 300, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.306, Test loss: 2.306, Test accuracy: 9.88
Round   0, Global train loss: 2.306, Global test loss: 2.306, Global test accuracy: 9.90
Round   1, Train loss: 2.307, Test loss: 2.306, Test accuracy: 9.85
Round   1, Global train loss: 2.307, Global test loss: 2.306, Global test accuracy: 9.89
Round   2, Train loss: 2.306, Test loss: 2.306, Test accuracy: 9.85
Round   2, Global train loss: 2.306, Global test loss: 2.305, Global test accuracy: 9.90
Round   3, Train loss: 2.306, Test loss: 2.305, Test accuracy: 9.86
Round   3, Global train loss: 2.306, Global test loss: 2.305, Global test accuracy: 9.92
Round   4, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.88
Round   4, Global train loss: 2.305, Global test loss: 2.305, Global test accuracy: 9.89
Round   5, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.89
Round   5, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.92
Round   6, Train loss: 2.305, Test loss: 2.305, Test accuracy: 9.88
Round   6, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.93
Round   7, Train loss: 2.304, Test loss: 2.304, Test accuracy: 9.90
Round   7, Global train loss: 2.304, Global test loss: 2.304, Global test accuracy: 9.94
Round   8, Train loss: 2.305, Test loss: 2.304, Test accuracy: 9.91
Round   8, Global train loss: 2.305, Global test loss: 2.304, Global test accuracy: 9.93
Round   9, Train loss: 2.304, Test loss: 2.304, Test accuracy: 9.90
Round   9, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 9.92
Round  10, Train loss: 2.304, Test loss: 2.303, Test accuracy: 9.87
Round  10, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 9.94
Round  11, Train loss: 2.304, Test loss: 2.303, Test accuracy: 9.85
Round  11, Global train loss: 2.304, Global test loss: 2.303, Global test accuracy: 9.92
Round  12, Train loss: 2.304, Test loss: 2.303, Test accuracy: 9.86
Round  12, Global train loss: 2.304, Global test loss: 2.302, Global test accuracy: 9.88
Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.84
Round  13, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 9.87
Round  14, Train loss: 2.303, Test loss: 2.302, Test accuracy: 9.86
Round  14, Global train loss: 2.303, Global test loss: 2.302, Global test accuracy: 9.88
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.85
Round  15, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 9.92
Round  16, Train loss: 2.303, Test loss: 2.302, Test accuracy: 9.89
Round  16, Global train loss: 2.303, Global test loss: 2.301, Global test accuracy: 9.96
Round  17, Train loss: 2.302, Test loss: 2.301, Test accuracy: 9.91
Round  17, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 9.98
Round  18, Train loss: 2.302, Test loss: 2.301, Test accuracy: 9.95
Round  18, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 9.98
Round  19, Train loss: 2.302, Test loss: 2.301, Test accuracy: 9.96
Round  19, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 9.98
Round  20, Train loss: 2.302, Test loss: 2.301, Test accuracy: 9.96
Round  20, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 9.98
Round  21, Train loss: 2.302, Test loss: 2.300, Test accuracy: 9.96
Round  21, Global train loss: 2.302, Global test loss: 2.300, Global test accuracy: 9.98
Round  22, Train loss: 2.301, Test loss: 2.300, Test accuracy: 9.99
Round  22, Global train loss: 2.301, Global test loss: 2.300, Global test accuracy: 10.03
Round  23, Train loss: 2.301, Test loss: 2.300, Test accuracy: 9.99
Round  23, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.02
Round  24, Train loss: 2.301, Test loss: 2.299, Test accuracy: 10.02
Round  24, Global train loss: 2.301, Global test loss: 2.299, Global test accuracy: 10.01
Round  25, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.04
Round  25, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.06
Round  26, Train loss: 2.300, Test loss: 2.299, Test accuracy: 10.04
Round  26, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.09
Round  27, Train loss: 2.300, Test loss: 2.298, Test accuracy: 10.04
Round  27, Global train loss: 2.300, Global test loss: 2.298, Global test accuracy: 10.05
Round  28, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.09
Round  28, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.06
Round  29, Train loss: 2.299, Test loss: 2.298, Test accuracy: 10.15
Round  29, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.09
Round  30, Train loss: 2.299, Test loss: 2.297, Test accuracy: 10.21
Round  30, Global train loss: 2.299, Global test loss: 2.297, Global test accuracy: 10.26
Round  31, Train loss: 2.299, Test loss: 2.297, Test accuracy: 10.26
Round  31, Global train loss: 2.299, Global test loss: 2.296, Global test accuracy: 10.43
Round  32, Train loss: 2.298, Test loss: 2.297, Test accuracy: 10.28
Round  32, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 10.45
Round  33, Train loss: 2.298, Test loss: 2.296, Test accuracy: 10.40
Round  33, Global train loss: 2.298, Global test loss: 2.295, Global test accuracy: 10.54
Round  34, Train loss: 2.298, Test loss: 2.296, Test accuracy: 10.44
Round  34, Global train loss: 2.298, Global test loss: 2.295, Global test accuracy: 10.46
Round  35, Train loss: 2.297, Test loss: 2.296, Test accuracy: 10.43
Round  35, Global train loss: 2.297, Global test loss: 2.295, Global test accuracy: 10.57
Round  36, Train loss: 2.297, Test loss: 2.295, Test accuracy: 10.55
Round  36, Global train loss: 2.297, Global test loss: 2.294, Global test accuracy: 10.61
Round  37, Train loss: 2.297, Test loss: 2.295, Test accuracy: 10.73
Round  37, Global train loss: 2.297, Global test loss: 2.294, Global test accuracy: 10.81
Round  38, Train loss: 2.296, Test loss: 2.295, Test accuracy: 10.83
Round  38, Global train loss: 2.296, Global test loss: 2.294, Global test accuracy: 10.99
Round  39, Train loss: 2.296, Test loss: 2.294, Test accuracy: 10.88
Round  39, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 10.74
Round  40, Train loss: 2.296, Test loss: 2.294, Test accuracy: 10.96
Round  40, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 11.13
Round  41, Train loss: 2.296, Test loss: 2.294, Test accuracy: 10.87
Round  41, Global train loss: 2.296, Global test loss: 2.293, Global test accuracy: 10.68
Round  42, Train loss: 2.295, Test loss: 2.293, Test accuracy: 10.94
Round  42, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 10.66
Round  43, Train loss: 2.295, Test loss: 2.293, Test accuracy: 11.06
Round  43, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 11.09
Round  44, Train loss: 2.295, Test loss: 2.293, Test accuracy: 11.24
Round  44, Global train loss: 2.295, Global test loss: 2.292, Global test accuracy: 11.22
Round  45, Train loss: 2.294, Test loss: 2.292, Test accuracy: 11.34
Round  45, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 11.43
Round  46, Train loss: 2.294, Test loss: 2.292, Test accuracy: 11.39
Round  46, Global train loss: 2.294, Global test loss: 2.291, Global test accuracy: 11.54
Round  47, Train loss: 2.293, Test loss: 2.291, Test accuracy: 11.67
Round  47, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 11.89
Round  48, Train loss: 2.293, Test loss: 2.291, Test accuracy: 12.00
Round  48, Global train loss: 2.293, Global test loss: 2.290, Global test accuracy: 12.48
Round  49, Train loss: 2.293, Test loss: 2.290, Test accuracy: 12.35
Round  49, Global train loss: 2.293, Global test loss: 2.289, Global test accuracy: 12.96
Round  50, Train loss: 2.292, Test loss: 2.290, Test accuracy: 12.40
Round  50, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 12.53
Round  51, Train loss: 2.292, Test loss: 2.289, Test accuracy: 12.42
Round  51, Global train loss: 2.292, Global test loss: 2.289, Global test accuracy: 12.58
Round  52, Train loss: 2.292, Test loss: 2.289, Test accuracy: 12.55
Round  52, Global train loss: 2.292, Global test loss: 2.288, Global test accuracy: 12.69
Round  53, Train loss: 2.292, Test loss: 2.289, Test accuracy: 12.77
Round  53, Global train loss: 2.292, Global test loss: 2.288, Global test accuracy: 12.78
Round  54, Train loss: 2.291, Test loss: 2.288, Test accuracy: 13.04
Round  54, Global train loss: 2.291, Global test loss: 2.287, Global test accuracy: 13.46
Round  55, Train loss: 2.290, Test loss: 2.288, Test accuracy: 13.29
Round  55, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 14.03
Round  56, Train loss: 2.290, Test loss: 2.287, Test accuracy: 13.61
Round  56, Global train loss: 2.290, Global test loss: 2.287, Global test accuracy: 14.64
Round  57, Train loss: 2.290, Test loss: 2.287, Test accuracy: 14.15
Round  57, Global train loss: 2.290, Global test loss: 2.286, Global test accuracy: 15.25
Round  58, Train loss: 2.289, Test loss: 2.287, Test accuracy: 14.35
Round  58, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 14.95
Round  59, Train loss: 2.290, Test loss: 2.286, Test accuracy: 14.63
Round  59, Global train loss: 2.290, Global test loss: 2.285, Global test accuracy: 15.02
Round  60, Train loss: 2.289, Test loss: 2.286, Test accuracy: 14.64
Round  60, Global train loss: 2.289, Global test loss: 2.285, Global test accuracy: 14.88
Round  61, Train loss: 2.288, Test loss: 2.285, Test accuracy: 15.10
Round  61, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 15.75
Round  62, Train loss: 2.288, Test loss: 2.284, Test accuracy: 15.54
Round  62, Global train loss: 2.288, Global test loss: 2.283, Global test accuracy: 16.14
Round  63, Train loss: 2.288, Test loss: 2.284, Test accuracy: 15.97
Round  63, Global train loss: 2.288, Global test loss: 2.283, Global test accuracy: 16.89
Round  64, Train loss: 2.287, Test loss: 2.283, Test accuracy: 16.61
Round  64, Global train loss: 2.287, Global test loss: 2.282, Global test accuracy: 17.64
Round  65, Train loss: 2.286, Test loss: 2.283, Test accuracy: 16.82
Round  65, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 17.88
Round  66, Train loss: 2.286, Test loss: 2.282, Test accuracy: 17.38
Round  66, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 18.98
Round  67, Train loss: 2.286, Test loss: 2.282, Test accuracy: 17.88
Round  67, Global train loss: 2.286, Global test loss: 2.281, Global test accuracy: 19.04
Round  68, Train loss: 2.286, Test loss: 2.281, Test accuracy: 18.30
Round  68, Global train loss: 2.286, Global test loss: 2.280, Global test accuracy: 19.44
Round  69, Train loss: 2.284, Test loss: 2.281, Test accuracy: 18.28
Round  69, Global train loss: 2.284, Global test loss: 2.279, Global test accuracy: 19.22
Round  70, Train loss: 2.284, Test loss: 2.280, Test accuracy: 18.81
Round  70, Global train loss: 2.284, Global test loss: 2.278, Global test accuracy: 19.16
Round  71, Train loss: 2.284, Test loss: 2.279, Test accuracy: 19.16
Round  71, Global train loss: 2.284, Global test loss: 2.278, Global test accuracy: 20.09
Round  72, Train loss: 2.283, Test loss: 2.279, Test accuracy: 19.20
Round  72, Global train loss: 2.283, Global test loss: 2.277, Global test accuracy: 19.38
Round  73, Train loss: 2.282, Test loss: 2.278, Test accuracy: 18.73
Round  73, Global train loss: 2.282, Global test loss: 2.277, Global test accuracy: 18.38
Round  74, Train loss: 2.281, Test loss: 2.278, Test accuracy: 18.36
Round  74, Global train loss: 2.281, Global test loss: 2.276, Global test accuracy: 18.32
Round  75, Train loss: 2.282, Test loss: 2.277, Test accuracy: 18.05
Round  75, Global train loss: 2.282, Global test loss: 2.276, Global test accuracy: 18.36
Round  76, Train loss: 2.282, Test loss: 2.277, Test accuracy: 17.73
Round  76, Global train loss: 2.282, Global test loss: 2.275, Global test accuracy: 18.70
Round  77, Train loss: 2.280, Test loss: 2.276, Test accuracy: 17.89
Round  77, Global train loss: 2.280, Global test loss: 2.274, Global test accuracy: 18.66
Round  78, Train loss: 2.280, Test loss: 2.275, Test accuracy: 17.90
Round  78, Global train loss: 2.280, Global test loss: 2.273, Global test accuracy: 18.62
Round  79, Train loss: 2.279, Test loss: 2.274, Test accuracy: 18.07
Round  79, Global train loss: 2.279, Global test loss: 2.272, Global test accuracy: 18.60
Round  80, Train loss: 2.279, Test loss: 2.274, Test accuracy: 18.53
Round  80, Global train loss: 2.279, Global test loss: 2.271, Global test accuracy: 19.81
Round  81, Train loss: 2.278, Test loss: 2.273, Test accuracy: 18.73
Round  81, Global train loss: 2.278, Global test loss: 2.270, Global test accuracy: 20.42
Round  82, Train loss: 2.278, Test loss: 2.272, Test accuracy: 19.18
Round  82, Global train loss: 2.278, Global test loss: 2.269, Global test accuracy: 20.30
Round  83, Train loss: 2.277, Test loss: 2.271, Test accuracy: 19.20
Round  83, Global train loss: 2.277, Global test loss: 2.269, Global test accuracy: 20.26
Round  84, Train loss: 2.277, Test loss: 2.270, Test accuracy: 19.49
Round  84, Global train loss: 2.277, Global test loss: 2.268, Global test accuracy: 20.18
Round  85, Train loss: 2.276, Test loss: 2.269, Test accuracy: 19.48
Round  85, Global train loss: 2.276, Global test loss: 2.267, Global test accuracy: 20.18
Round  86, Train loss: 2.275, Test loss: 2.268, Test accuracy: 19.68
Round  86, Global train loss: 2.275, Global test loss: 2.267, Global test accuracy: 19.89
Round  87, Train loss: 2.274, Test loss: 2.267, Test accuracy: 19.74
Round  87, Global train loss: 2.274, Global test loss: 2.265, Global test accuracy: 20.43
Round  88, Train loss: 2.274, Test loss: 2.266, Test accuracy: 19.88
Round  88, Global train loss: 2.274, Global test loss: 2.264, Global test accuracy: 20.54
Round  89, Train loss: 2.273, Test loss: 2.265, Test accuracy: 20.13
Round  89, Global train loss: 2.273, Global test loss: 2.263, Global test accuracy: 20.98
Round  90, Train loss: 2.273, Test loss: 2.264, Test accuracy: 20.13
Round  90, Global train loss: 2.273, Global test loss: 2.262, Global test accuracy: 21.09
Round  91, Train loss: 2.271, Test loss: 2.263, Test accuracy: 20.27
Round  91, Global train loss: 2.271, Global test loss: 2.260, Global test accuracy: 19.88
Round  92, Train loss: 2.271, Test loss: 2.262, Test accuracy: 20.24
Round  92, Global train loss: 2.271, Global test loss: 2.260, Global test accuracy: 20.13
Round  93, Train loss: 2.272, Test loss: 2.261, Test accuracy: 19.85
Round  93, Global train loss: 2.272, Global test loss: 2.259, Global test accuracy: 19.47
Round  94, Train loss: 2.270, Test loss: 2.261, Test accuracy: 19.81
Round  94, Global train loss: 2.270, Global test loss: 2.258, Global test accuracy: 19.88
Round  95, Train loss: 2.268, Test loss: 2.259, Test accuracy: 20.10
Round  95, Global train loss: 2.268, Global test loss: 2.257, Global test accuracy: 20.65
Round  96, Train loss: 2.269, Test loss: 2.259, Test accuracy: 20.24
Round  96, Global train loss: 2.269, Global test loss: 2.257, Global test accuracy: 20.76
Round  97, Train loss: 2.267, Test loss: 2.259, Test accuracy: 20.20
Round  97, Global train loss: 2.267, Global test loss: 2.257, Global test accuracy: 20.94
Round  98, Train loss: 2.268, Test loss: 2.257, Test accuracy: 20.48
Round  98, Global train loss: 2.268, Global test loss: 2.255, Global test accuracy: 21.33
Round  99, Train loss: 2.265, Test loss: 2.257, Test accuracy: 20.68
Round  99, Global train loss: 2.265, Global test loss: 2.254, Global test accuracy: 21.77
Round 100, Train loss: 2.266, Test loss: 2.255, Test accuracy: 20.77
Round 100, Global train loss: 2.266, Global test loss: 2.253, Global test accuracy: 20.88
Round 101, Train loss: 2.264, Test loss: 2.254, Test accuracy: 20.70
Round 101, Global train loss: 2.264, Global test loss: 2.251, Global test accuracy: 21.04
Round 102, Train loss: 2.265, Test loss: 2.253, Test accuracy: 20.83
Round 102, Global train loss: 2.265, Global test loss: 2.250, Global test accuracy: 21.09
Round 103, Train loss: 2.264, Test loss: 2.252, Test accuracy: 20.96
Round 103, Global train loss: 2.264, Global test loss: 2.249, Global test accuracy: 21.21
Round 104, Train loss: 2.263, Test loss: 2.251, Test accuracy: 20.75
Round 104, Global train loss: 2.263, Global test loss: 2.248, Global test accuracy: 21.01
Round 105, Train loss: 2.262, Test loss: 2.249, Test accuracy: 20.90
Round 105, Global train loss: 2.262, Global test loss: 2.246, Global test accuracy: 21.52
Round 106, Train loss: 2.260, Test loss: 2.249, Test accuracy: 20.98
Round 106, Global train loss: 2.260, Global test loss: 2.245, Global test accuracy: 21.80
Round 107, Train loss: 2.260, Test loss: 2.247, Test accuracy: 21.37
Round 107, Global train loss: 2.260, Global test loss: 2.243, Global test accuracy: 21.93
Round 108, Train loss: 2.259, Test loss: 2.246, Test accuracy: 21.26
Round 108, Global train loss: 2.259, Global test loss: 2.242, Global test accuracy: 21.83
Round 109, Train loss: 2.258, Test loss: 2.245, Test accuracy: 20.96
Round 109, Global train loss: 2.258, Global test loss: 2.241, Global test accuracy: 21.05
Round 110, Train loss: 2.257, Test loss: 2.244, Test accuracy: 20.82
Round 110, Global train loss: 2.257, Global test loss: 2.241, Global test accuracy: 20.98
Round 111, Train loss: 2.255, Test loss: 2.242, Test accuracy: 20.95
Round 111, Global train loss: 2.255, Global test loss: 2.239, Global test accuracy: 21.20
Round 112, Train loss: 2.254, Test loss: 2.241, Test accuracy: 20.89
Round 112, Global train loss: 2.254, Global test loss: 2.238, Global test accuracy: 21.34
Round 113, Train loss: 2.256, Test loss: 2.240, Test accuracy: 21.04
Round 113, Global train loss: 2.256, Global test loss: 2.239, Global test accuracy: 22.01
Round 114, Train loss: 2.255, Test loss: 2.240, Test accuracy: 21.23
Round 114, Global train loss: 2.255, Global test loss: 2.238, Global test accuracy: 21.91
Round 115, Train loss: 2.254, Test loss: 2.238, Test accuracy: 21.18
Round 115, Global train loss: 2.254, Global test loss: 2.236, Global test accuracy: 21.84
Round 116, Train loss: 2.252, Test loss: 2.237, Test accuracy: 21.16
Round 116, Global train loss: 2.252, Global test loss: 2.234, Global test accuracy: 21.90
Round 117, Train loss: 2.252, Test loss: 2.237, Test accuracy: 21.21
Round 117, Global train loss: 2.252, Global test loss: 2.234, Global test accuracy: 22.13
Round 118, Train loss: 2.252, Test loss: 2.235, Test accuracy: 20.95
Round 118, Global train loss: 2.252, Global test loss: 2.232, Global test accuracy: 21.88
Round 119, Train loss: 2.252, Test loss: 2.234, Test accuracy: 21.20
Round 119, Global train loss: 2.252, Global test loss: 2.231, Global test accuracy: 22.49
Round 120, Train loss: 2.249, Test loss: 2.233, Test accuracy: 21.41
Round 120, Global train loss: 2.249, Global test loss: 2.230, Global test accuracy: 22.35
Round 121, Train loss: 2.250, Test loss: 2.232, Test accuracy: 21.38
Round 121, Global train loss: 2.250, Global test loss: 2.229, Global test accuracy: 22.89
Round 122, Train loss: 2.247, Test loss: 2.231, Test accuracy: 21.52
Round 122, Global train loss: 2.247, Global test loss: 2.227, Global test accuracy: 23.07
Round 123, Train loss: 2.247, Test loss: 2.230, Test accuracy: 21.64
Round 123, Global train loss: 2.247, Global test loss: 2.225, Global test accuracy: 23.45
Round 124, Train loss: 2.246, Test loss: 2.227, Test accuracy: 22.07
Round 124, Global train loss: 2.246, Global test loss: 2.223, Global test accuracy: 23.13
Round 125, Train loss: 2.248, Test loss: 2.225, Test accuracy: 22.25
Round 125, Global train loss: 2.248, Global test loss: 2.220, Global test accuracy: 23.04
Round 126, Train loss: 2.247, Test loss: 2.225, Test accuracy: 22.27
Round 126, Global train loss: 2.247, Global test loss: 2.221, Global test accuracy: 23.33
Round 127, Train loss: 2.245, Test loss: 2.224, Test accuracy: 22.43
Round 127, Global train loss: 2.245, Global test loss: 2.221, Global test accuracy: 23.21
Round 128, Train loss: 2.246, Test loss: 2.223, Test accuracy: 22.23
Round 128, Global train loss: 2.246, Global test loss: 2.219, Global test accuracy: 22.47
Round 129, Train loss: 2.243, Test loss: 2.221, Test accuracy: 22.61
Round 129, Global train loss: 2.243, Global test loss: 2.217, Global test accuracy: 23.76
Round 130, Train loss: 2.243, Test loss: 2.219, Test accuracy: 22.82
Round 130, Global train loss: 2.243, Global test loss: 2.215, Global test accuracy: 23.65
Round 131, Train loss: 2.243, Test loss: 2.218, Test accuracy: 22.71
Round 131, Global train loss: 2.243, Global test loss: 2.213, Global test accuracy: 23.00
Round 132, Train loss: 2.243, Test loss: 2.217, Test accuracy: 22.34
Round 132, Global train loss: 2.243, Global test loss: 2.212, Global test accuracy: 22.18
Round 133, Train loss: nan, Test loss: nan, Test accuracy: 21.09
Round 133, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 134, Train loss: nan, Test loss: nan, Test accuracy: 17.54
Round 134, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 135, Train loss: nan, Test loss: nan, Test accuracy: 16.31
Round 135, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 136, Train loss: nan, Test loss: nan, Test accuracy: 15.19
Round 136, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 137, Train loss: nan, Test loss: nan, Test accuracy: 14.60
Round 137, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 138, Train loss: nan, Test loss: nan, Test accuracy: 13.35
Round 138, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 139, Train loss: nan, Test loss: nan, Test accuracy: 12.14
Round 139, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 140, Train loss: nan, Test loss: nan, Test accuracy: 11.69
Round 140, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 141, Train loss: nan, Test loss: nan, Test accuracy: 11.69
Round 141, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 142, Train loss: nan, Test loss: nan, Test accuracy: 11.69
Round 142, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 143, Train loss: nan, Test loss: nan, Test accuracy: 10.58
Round 143, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 144, Train loss: nan, Test loss: nan, Test accuracy: 10.58
Round 144, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 145, Train loss: nan, Test loss: nan, Test accuracy: 10.58
Round 145, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 146, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 146, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 147, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 147, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 148, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 148, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 149, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 149, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 150, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 150, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 151, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 151, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 152, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 152, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 153, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 153, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 154, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 154, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 155, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 155, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 156, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 156, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 157, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 157, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 158, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 158, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 159, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 159, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 160, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 160, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 161, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 161, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 162, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 162, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 163, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 163, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 164, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 164, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 165, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 165, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 166, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 166, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 167, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 167, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 168, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 168, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 169, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 169, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 170, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 170, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 171, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 171, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 172, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 172, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 173, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 173, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 174, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 174, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 175, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 175, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 176, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 176, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 177, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 177, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 178, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 178, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 179, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 179, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 180, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 180, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 181, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 181, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 182, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 182, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 183, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 183, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 184, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 184, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 185, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 185, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 186, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 186, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 187, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 187, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 188, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 188, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 189, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 189, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 190, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 190, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 191, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 191, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 192, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 192, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 193, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 193, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 194, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 194, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 195, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 195, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 196, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 196, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 197, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 197, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 198, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 198, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 199, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 199, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 200, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 200, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 201, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 201, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 202, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 202, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 203, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 203, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 204, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 204, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 205, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 205, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 206, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 206, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 207, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 207, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 208, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 208, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 209, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 209, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 210, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 210, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 211, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 211, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 212, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 212, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 213, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 213, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 214, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 214, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 215, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 215, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 216, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 216, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 217, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 217, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 218, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 218, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 219, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 219, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 220, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 220, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 221, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 221, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 222, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 222, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 223, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 223, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 224, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 224, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 225, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 225, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 226, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 226, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 227, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 227, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 228, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 228, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 229, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 229, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 230, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 230, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 231, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 231, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 232, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 232, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 233, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 233, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 234, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 234, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 235, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 235, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 236, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 236, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 237, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 237, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 238, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 238, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 239, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 239, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 240, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 240, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 241, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 241, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 242, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 242, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 243, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 243, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 244, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 244, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 245, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 245, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 246, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 246, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 247, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 247, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 248, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 248, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 249, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 249, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 250, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 250, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 251, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 251, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 252, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 252, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 253, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 253, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 254, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 254, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 255, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 255, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 256, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 256, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 257, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 257, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 258, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 258, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 259, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 259, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 260, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 260, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 261, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 261, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 262, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 262, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 263, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 263, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 264, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 264, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 265, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 265, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 266, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 266, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 267, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 267, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 268, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 268, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 269, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 269, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 270, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 270, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 271, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 271, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 272, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 272, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 273, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 273, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 274, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 274, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 275, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 275, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 276, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 276, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 277, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 277, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 278, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 278, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 279, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 279, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 280, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 280, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 281, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 281, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 282, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 282, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 283, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 283, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 284, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 284, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 285, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 285, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 286, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 286, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 287, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 287, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 288, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 288, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 289, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 289, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 290, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 290, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 291, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 291, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 292, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 292, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 293, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 293, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 294, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 294, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 295, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 295, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 296, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 296, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 297, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 297, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 298, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 298, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Round 299, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Round 299, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

14411.061928987503
[4.8784167766571045, 9.531919002532959, 14.171087265014648, 18.806960105895996, 23.436681509017944, 28.087419986724854, 32.72293043136597, 37.3700008392334, 42.03177833557129, 46.68558931350708, 51.321678161621094, 56.31076407432556, 60.91568422317505, 65.49835872650146, 70.12421751022339, 74.76457905769348, 79.39094567298889, 84.00014805793762, 88.59140086174011, 93.15540862083435, 97.76192259788513, 102.77771043777466, 107.3956949710846, 112.00660347938538, 117.12519407272339, 122.12813878059387, 126.72412180900574, 131.78388667106628, 136.3930287361145, 141.44375586509705, 146.0721459388733, 151.19491314888, 156.40833735466003, 161.0109167098999, 165.62776565551758, 170.25662446022034, 174.89048624038696, 179.5229263305664, 184.18779635429382, 188.84573531150818, 193.49494409561157, 198.14763522148132, 202.79303550720215, 207.45177626609802, 212.09868812561035, 216.75619983673096, 221.41841387748718, 226.08918976783752, 230.7154176235199, 235.35894012451172, 239.99836111068726, 244.65693545341492, 249.30977058410645, 253.93618083000183, 258.580313205719, 263.22563314437866, 268.2778890132904, 272.88084053993225, 277.4742352962494, 282.0909390449524, 286.7167797088623, 291.6223814487457, 296.26705026626587, 300.9055960178375, 305.5522315502167, 310.20500659942627, 314.8373112678528, 319.4799208641052, 324.13191962242126, 328.7746407985687, 333.43524837493896, 338.09104776382446, 342.73255467414856, 347.3410153388977, 351.96441102027893, 356.55844593048096, 361.1630165576935, 365.7743420600891, 370.4331216812134, 375.1105661392212, 379.7773959636688, 384.4335014820099, 389.0391972064972, 393.6917130947113, 398.3612084388733, 403.0006206035614, 407.9807560443878, 412.6084749698639, 417.23549342155457, 421.870836019516, 426.50131464004517, 431.1226363182068, 436.11337208747864, 440.7521541118622, 445.3916218280792, 450.0321829319, 454.6755609512329, 459.3276517391205, 463.9644248485565, 468.602162361145, 473.2250554561615, 477.8512523174286, 482.48035192489624, 487.1208231449127, 491.7593021392822, 496.8495593070984, 501.4626429080963, 506.06980633735657, 510.74752926826477, 515.3675458431244, 519.9930355548859, 524.6449959278107, 529.2655816078186, 533.9096593856812, 538.5933640003204, 543.2505378723145, 547.9206540584564, 552.5922996997833, 557.2397890090942, 562.2259526252747, 566.8644440174103, 571.5025486946106, 576.1377727985382, 580.7824976444244, 585.4301550388336, 590.0453743934631, 594.6803560256958, 599.3396065235138, 603.9759349822998, 608.6005663871765, 613.2358856201172, 617.876800775528, 622.5361726284027, 627.1908268928528, 631.9931762218475, 636.62233710289, 641.2683804035187, 645.9044561386108, 650.5206441879272, 655.1672778129578, 659.807076215744, 664.4523596763611, 669.0902261734009, 673.7144646644592, 678.3463287353516, 682.9617409706116, 687.6004838943481, 692.2419991493225, 696.879369020462, 701.5082807540894, 706.1314504146576, 710.7422065734863, 715.3626646995544, 719.98752617836, 724.6205615997314, 729.2556173801422, 733.8962833881378, 738.5214145183563, 743.1438257694244, 747.7653729915619, 752.3939642906189, 757.0294396877289, 761.6548686027527, 766.2902081012726, 770.9129796028137, 775.547726392746, 780.1822805404663, 784.7991304397583, 789.4408714771271, 794.089527130127, 798.7377145290375, 803.3763737678528, 808.0158054828644, 812.6550779342651, 817.2959101200104, 821.9417979717255, 826.5807292461395, 831.214013338089, 835.8567056655884, 840.4826860427856, 845.1141469478607, 849.7488338947296, 854.39368724823, 859.0433762073517, 863.6694710254669, 868.5955650806427, 873.2350876331329, 877.865558385849, 882.5162534713745, 887.5878269672394, 892.2139105796814, 896.8478906154633, 901.5453531742096, 906.6596088409424, 911.3264148235321, 915.9814941883087, 920.6417765617371, 925.3007354736328, 929.9662714004517, 934.6393458843231, 939.3075034618378, 943.9475133419037, 948.5742206573486, 953.2326765060425, 957.8790819644928, 962.5264813899994, 967.1802067756653, 971.8412098884583, 976.5016477108002, 981.172735452652, 985.8160965442657, 990.4675893783569, 995.4088113307953, 1000.5166702270508, 1005.1659803390503, 1009.8439848423004, 1014.5070216655731, 1019.1861815452576, 1024.3396842479706, 1029.4175112247467, 1034.4587650299072, 1039.1295700073242, 1043.7970852851868, 1048.4323377609253, 1053.0821442604065, 1057.7282965183258, 1062.5293610095978, 1067.181087732315, 1071.867294549942, 1076.5172882080078, 1081.1766736507416, 1085.8588869571686, 1090.5596265792847, 1095.2558529376984, 1100.471444606781, 1105.5211353302002, 1110.6005861759186, 1115.7315809726715, 1120.8368065357208, 1125.3730535507202, 1129.7382786273956, 1134.3370745182037, 1138.8750874996185, 1143.4185886383057, 1147.9640700817108, 1152.3759272098541, 1156.867642402649, 1161.3489890098572, 1165.8286056518555, 1170.3023602962494, 1174.7884809970856, 1179.365999698639, 1183.8038394451141, 1188.161429643631, 1192.5776634216309, 1197.1871738433838, 1201.8011226654053, 1206.3205270767212, 1210.925056695938, 1215.4793591499329, 1219.977905511856, 1225.0570237636566, 1229.6780879497528, 1234.1385219097137, 1238.7127475738525, 1243.2552237510681, 1247.7172157764435, 1252.392460823059, 1257.0072269439697, 1261.6514899730682, 1266.2474918365479, 1270.9036450386047, 1275.547152042389, 1280.053278684616, 1284.569183588028, 1289.731567144394, 1294.731365442276, 1299.34299492836, 1303.9930799007416, 1308.5937643051147, 1313.1798207759857, 1318.2011551856995, 1322.8197348117828, 1327.4256155490875, 1332.0231823921204, 1336.6798567771912, 1341.304622411728, 1345.7909979820251, 1350.4123558998108, 1354.9960193634033, 1359.6280879974365, 1364.2264358997345, 1368.8015880584717, 1373.3300778865814, 1377.9642539024353, 1382.5924592018127, 1387.5806851387024, 1392.5546555519104, 1397.1187789440155, 1401.639104604721, 1403.9319741725922]
[9.8775, 9.8525, 9.8525, 9.8625, 9.8825, 9.8875, 9.8825, 9.9025, 9.9075, 9.8975, 9.8675, 9.85, 9.855, 9.84, 9.8575, 9.8525, 9.8925, 9.915, 9.945, 9.96, 9.965, 9.965, 9.99, 9.9925, 10.0225, 10.0425, 10.0425, 10.0425, 10.0875, 10.15, 10.2125, 10.26, 10.2825, 10.4, 10.4375, 10.4325, 10.55, 10.735, 10.83, 10.8775, 10.9625, 10.865, 10.9375, 11.0625, 11.2375, 11.3425, 11.39, 11.67, 12.0025, 12.3525, 12.405, 12.42, 12.5525, 12.765, 13.045, 13.295, 13.61, 14.1475, 14.3475, 14.6325, 14.64, 15.0975, 15.535, 15.9675, 16.6125, 16.8225, 17.3825, 17.875, 18.3025, 18.2775, 18.8125, 19.16, 19.2, 18.7275, 18.36, 18.055, 17.73, 17.885, 17.9025, 18.075, 18.53, 18.73, 19.18, 19.2025, 19.4925, 19.48, 19.6775, 19.74, 19.875, 20.13, 20.13, 20.265, 20.2425, 19.85, 19.81, 20.1025, 20.2425, 20.1975, 20.485, 20.685, 20.77, 20.7025, 20.83, 20.965, 20.7475, 20.9025, 20.985, 21.37, 21.2575, 20.9575, 20.8225, 20.9525, 20.885, 21.0375, 21.23, 21.18, 21.1625, 21.2075, 20.9475, 21.1975, 21.41, 21.3775, 21.5225, 21.645, 22.065, 22.25, 22.2725, 22.425, 22.23, 22.605, 22.8225, 22.715, 22.345, 21.0925, 17.54, 16.3125, 15.1875, 14.595, 13.3475, 12.145, 11.6925, 11.6925, 11.6925, 10.5825, 10.5825, 10.5825, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.066, Test loss: 1.937, Test accuracy: 28.52
Round   0, Global train loss: 1.066, Global test loss: 2.271, Global test accuracy: 19.43
Round   1, Train loss: 0.906, Test loss: 1.472, Test accuracy: 42.38
Round   1, Global train loss: 0.906, Global test loss: 2.063, Global test accuracy: 24.08
Round   2, Train loss: 0.870, Test loss: 1.340, Test accuracy: 51.46
Round   2, Global train loss: 0.870, Global test loss: 2.072, Global test accuracy: 27.77
Round   3, Train loss: 0.774, Test loss: 1.025, Test accuracy: 60.36
Round   3, Global train loss: 0.774, Global test loss: 1.983, Global test accuracy: 31.91
Round   4, Train loss: 0.716, Test loss: 0.800, Test accuracy: 65.54
Round   4, Global train loss: 0.716, Global test loss: 1.793, Global test accuracy: 31.03
Round   5, Train loss: 0.708, Test loss: 0.777, Test accuracy: 67.57
Round   5, Global train loss: 0.708, Global test loss: 1.983, Global test accuracy: 34.11
Round   6, Train loss: 0.613, Test loss: 0.761, Test accuracy: 68.21
Round   6, Global train loss: 0.613, Global test loss: 2.007, Global test accuracy: 35.38
Round   7, Train loss: 0.602, Test loss: 0.637, Test accuracy: 72.61
Round   7, Global train loss: 0.602, Global test loss: 1.460, Global test accuracy: 50.31
Round   8, Train loss: 0.652, Test loss: 0.614, Test accuracy: 73.92
Round   8, Global train loss: 0.652, Global test loss: 1.546, Global test accuracy: 42.31
Round   9, Train loss: 0.571, Test loss: 0.596, Test accuracy: 74.87
Round   9, Global train loss: 0.571, Global test loss: 1.479, Global test accuracy: 47.35
Round  10, Train loss: 0.570, Test loss: 0.567, Test accuracy: 76.40
Round  10, Global train loss: 0.570, Global test loss: 1.466, Global test accuracy: 49.75
Round  11, Train loss: 0.531, Test loss: 0.553, Test accuracy: 77.06
Round  11, Global train loss: 0.531, Global test loss: 1.411, Global test accuracy: 50.04
Round  12, Train loss: 0.559, Test loss: 0.535, Test accuracy: 78.03
Round  12, Global train loss: 0.559, Global test loss: 1.373, Global test accuracy: 53.38
Round  13, Train loss: 0.628, Test loss: 0.531, Test accuracy: 78.04
Round  13, Global train loss: 0.628, Global test loss: 1.565, Global test accuracy: 48.14
Round  14, Train loss: 0.526, Test loss: 0.527, Test accuracy: 78.36
Round  14, Global train loss: 0.526, Global test loss: 1.433, Global test accuracy: 50.28
Round  15, Train loss: 0.523, Test loss: 0.539, Test accuracy: 77.99
Round  15, Global train loss: 0.523, Global test loss: 1.614, Global test accuracy: 45.61
Round  16, Train loss: 0.468, Test loss: 0.511, Test accuracy: 79.50
Round  16, Global train loss: 0.468, Global test loss: 1.530, Global test accuracy: 49.62
Round  17, Train loss: 0.520, Test loss: 0.521, Test accuracy: 79.06
Round  17, Global train loss: 0.520, Global test loss: 1.445, Global test accuracy: 50.97
Round  18, Train loss: 0.518, Test loss: 0.526, Test accuracy: 78.82
Round  18, Global train loss: 0.518, Global test loss: 1.486, Global test accuracy: 48.78
Round  19, Train loss: 0.400, Test loss: 0.503, Test accuracy: 79.80
Round  19, Global train loss: 0.400, Global test loss: 1.393, Global test accuracy: 53.87
Round  20, Train loss: 0.502, Test loss: 0.505, Test accuracy: 79.59
Round  20, Global train loss: 0.502, Global test loss: 1.449, Global test accuracy: 50.63
Round  21, Train loss: 0.471, Test loss: 0.484, Test accuracy: 80.46
Round  21, Global train loss: 0.471, Global test loss: 1.438, Global test accuracy: 50.49
Round  22, Train loss: 0.485, Test loss: 0.479, Test accuracy: 80.71
Round  22, Global train loss: 0.485, Global test loss: 1.256, Global test accuracy: 55.29
Round  23, Train loss: 0.422, Test loss: 0.472, Test accuracy: 80.78
Round  23, Global train loss: 0.422, Global test loss: 1.297, Global test accuracy: 57.34
Round  24, Train loss: 0.432, Test loss: 0.486, Test accuracy: 80.45
Round  24, Global train loss: 0.432, Global test loss: 1.294, Global test accuracy: 57.09
Round  25, Train loss: 0.411, Test loss: 0.477, Test accuracy: 80.76
Round  25, Global train loss: 0.411, Global test loss: 1.135, Global test accuracy: 60.89
Round  26, Train loss: 0.409, Test loss: 0.479, Test accuracy: 81.02
Round  26, Global train loss: 0.409, Global test loss: 1.268, Global test accuracy: 57.87
Round  27, Train loss: 0.425, Test loss: 0.476, Test accuracy: 81.24
Round  27, Global train loss: 0.425, Global test loss: 1.804, Global test accuracy: 47.03
Round  28, Train loss: 0.412, Test loss: 0.452, Test accuracy: 82.17
Round  28, Global train loss: 0.412, Global test loss: 1.425, Global test accuracy: 53.13
Round  29, Train loss: 0.405, Test loss: 0.439, Test accuracy: 82.75
Round  29, Global train loss: 0.405, Global test loss: 1.199, Global test accuracy: 58.86
Round  30, Train loss: 0.381, Test loss: 0.441, Test accuracy: 82.69
Round  30, Global train loss: 0.381, Global test loss: 1.228, Global test accuracy: 58.91
Round  31, Train loss: 0.372, Test loss: 0.435, Test accuracy: 82.84
Round  31, Global train loss: 0.372, Global test loss: 1.170, Global test accuracy: 61.62
Round  32, Train loss: 0.407, Test loss: 0.429, Test accuracy: 83.16
Round  32, Global train loss: 0.407, Global test loss: 1.326, Global test accuracy: 55.92
Round  33, Train loss: 0.406, Test loss: 0.436, Test accuracy: 83.19
Round  33, Global train loss: 0.406, Global test loss: 1.328, Global test accuracy: 58.44
Round  34, Train loss: 0.303, Test loss: 0.439, Test accuracy: 83.24
Round  34, Global train loss: 0.303, Global test loss: 1.333, Global test accuracy: 59.68
Round  35, Train loss: 0.401, Test loss: 0.424, Test accuracy: 83.92
Round  35, Global train loss: 0.401, Global test loss: 1.441, Global test accuracy: 54.37
Round  36, Train loss: 0.371, Test loss: 0.434, Test accuracy: 83.92
Round  36, Global train loss: 0.371, Global test loss: 1.179, Global test accuracy: 60.28
Round  37, Train loss: 0.277, Test loss: 0.447, Test accuracy: 83.49
Round  37, Global train loss: 0.277, Global test loss: 1.341, Global test accuracy: 59.66
Round  38, Train loss: 0.364, Test loss: 0.448, Test accuracy: 83.27
Round  38, Global train loss: 0.364, Global test loss: 1.096, Global test accuracy: 64.17
Round  39, Train loss: 0.308, Test loss: 0.437, Test accuracy: 83.66
Round  39, Global train loss: 0.308, Global test loss: 1.138, Global test accuracy: 62.62
Round  40, Train loss: 0.334, Test loss: 0.449, Test accuracy: 83.41
Round  40, Global train loss: 0.334, Global test loss: 1.274, Global test accuracy: 60.44
Round  41, Train loss: 0.383, Test loss: 0.458, Test accuracy: 82.99
Round  41, Global train loss: 0.383, Global test loss: 1.055, Global test accuracy: 64.32
Round  42, Train loss: 0.304, Test loss: 0.462, Test accuracy: 82.86
Round  42, Global train loss: 0.304, Global test loss: 1.158, Global test accuracy: 61.71
Round  43, Train loss: 0.309, Test loss: 0.450, Test accuracy: 83.17
Round  43, Global train loss: 0.309, Global test loss: 1.075, Global test accuracy: 64.48
Round  44, Train loss: 0.308, Test loss: 0.435, Test accuracy: 83.64
Round  44, Global train loss: 0.308, Global test loss: 1.088, Global test accuracy: 64.29
Round  45, Train loss: 0.284, Test loss: 0.431, Test accuracy: 83.76
Round  45, Global train loss: 0.284, Global test loss: 1.122, Global test accuracy: 63.76
Round  46, Train loss: 0.377, Test loss: 0.439, Test accuracy: 83.93
Round  46, Global train loss: 0.377, Global test loss: 1.042, Global test accuracy: 65.08
Round  47, Train loss: 0.276, Test loss: 0.450, Test accuracy: 83.68
Round  47, Global train loss: 0.276, Global test loss: 1.538, Global test accuracy: 55.78
Round  48, Train loss: 0.284, Test loss: 0.441, Test accuracy: 84.02
Round  48, Global train loss: 0.284, Global test loss: 0.985, Global test accuracy: 67.87
Round  49, Train loss: 0.301, Test loss: 0.430, Test accuracy: 84.46
Round  49, Global train loss: 0.301, Global test loss: 1.027, Global test accuracy: 65.77
Round  50, Train loss: 0.339, Test loss: 0.443, Test accuracy: 84.21
Round  50, Global train loss: 0.339, Global test loss: 1.107, Global test accuracy: 63.59
Round  51, Train loss: 0.301, Test loss: 0.441, Test accuracy: 84.46
Round  51, Global train loss: 0.301, Global test loss: 1.074, Global test accuracy: 64.53
Round  52, Train loss: 0.232, Test loss: 0.432, Test accuracy: 84.89
Round  52, Global train loss: 0.232, Global test loss: 1.159, Global test accuracy: 64.89
Round  53, Train loss: 0.220, Test loss: 0.430, Test accuracy: 84.94
Round  53, Global train loss: 0.220, Global test loss: 1.466, Global test accuracy: 59.68
Round  54, Train loss: 0.321, Test loss: 0.444, Test accuracy: 84.43
Round  54, Global train loss: 0.321, Global test loss: 1.206, Global test accuracy: 61.78
Round  55, Train loss: 0.208, Test loss: 0.440, Test accuracy: 84.64
Round  55, Global train loss: 0.208, Global test loss: 1.126, Global test accuracy: 65.72
Round  56, Train loss: 0.251, Test loss: 0.439, Test accuracy: 84.81
Round  56, Global train loss: 0.251, Global test loss: 1.290, Global test accuracy: 62.72
Round  57, Train loss: 0.311, Test loss: 0.446, Test accuracy: 84.55
Round  57, Global train loss: 0.311, Global test loss: 1.083, Global test accuracy: 66.25
Round  58, Train loss: 0.196, Test loss: 0.445, Test accuracy: 84.63
Round  58, Global train loss: 0.196, Global test loss: 1.359, Global test accuracy: 62.59
Round  59, Train loss: 0.279, Test loss: 0.450, Test accuracy: 84.49
Round  59, Global train loss: 0.279, Global test loss: 1.113, Global test accuracy: 66.03
Round  60, Train loss: 0.351, Test loss: 0.439, Test accuracy: 84.76
Round  60, Global train loss: 0.351, Global test loss: 1.119, Global test accuracy: 64.48
Round  61, Train loss: 0.246, Test loss: 0.442, Test accuracy: 84.85
Round  61, Global train loss: 0.246, Global test loss: 1.133, Global test accuracy: 64.46
Round  62, Train loss: 0.265, Test loss: 0.434, Test accuracy: 85.08
Round  62, Global train loss: 0.265, Global test loss: 1.051, Global test accuracy: 66.87
Round  63, Train loss: 0.260, Test loss: 0.433, Test accuracy: 85.09
Round  63, Global train loss: 0.260, Global test loss: 1.032, Global test accuracy: 66.56
Round  64, Train loss: 0.279, Test loss: 0.436, Test accuracy: 85.02
Round  64, Global train loss: 0.279, Global test loss: 1.313, Global test accuracy: 59.76
Round  65, Train loss: 0.290, Test loss: 0.434, Test accuracy: 85.07
Round  65, Global train loss: 0.290, Global test loss: 1.152, Global test accuracy: 64.71
Round  66, Train loss: 0.287, Test loss: 0.445, Test accuracy: 84.57
Round  66, Global train loss: 0.287, Global test loss: 1.052, Global test accuracy: 65.29
Round  67, Train loss: 0.214, Test loss: 0.440, Test accuracy: 84.71
Round  67, Global train loss: 0.214, Global test loss: 1.078, Global test accuracy: 67.34
Round  68, Train loss: 0.256, Test loss: 0.437, Test accuracy: 84.89
Round  68, Global train loss: 0.256, Global test loss: 1.172, Global test accuracy: 63.97
Round  69, Train loss: 0.228, Test loss: 0.447, Test accuracy: 84.64
Round  69, Global train loss: 0.228, Global test loss: 1.145, Global test accuracy: 65.64
Round  70, Train loss: 0.300, Test loss: 0.452, Test accuracy: 84.81
Round  70, Global train loss: 0.300, Global test loss: 1.384, Global test accuracy: 57.62
Round  71, Train loss: 0.212, Test loss: 0.437, Test accuracy: 85.28
Round  71, Global train loss: 0.212, Global test loss: 1.204, Global test accuracy: 64.17
Round  72, Train loss: 0.282, Test loss: 0.425, Test accuracy: 85.76
Round  72, Global train loss: 0.282, Global test loss: 0.963, Global test accuracy: 68.31
Round  73, Train loss: 0.164, Test loss: 0.427, Test accuracy: 85.71
Round  73, Global train loss: 0.164, Global test loss: 1.342, Global test accuracy: 62.77
Round  74, Train loss: 0.254, Test loss: 0.430, Test accuracy: 85.52
Round  74, Global train loss: 0.254, Global test loss: 1.242, Global test accuracy: 63.52
Round  75, Train loss: 0.157, Test loss: 0.428, Test accuracy: 85.62
Round  75, Global train loss: 0.157, Global test loss: 1.259, Global test accuracy: 64.52
Round  76, Train loss: 0.313, Test loss: 0.441, Test accuracy: 85.39
Round  76, Global train loss: 0.313, Global test loss: 1.047, Global test accuracy: 66.43
Round  77, Train loss: 0.162, Test loss: 0.446, Test accuracy: 85.21
Round  77, Global train loss: 0.162, Global test loss: 1.436, Global test accuracy: 63.37
Round  78, Train loss: 0.223, Test loss: 0.437, Test accuracy: 85.56
Round  78, Global train loss: 0.223, Global test loss: 0.988, Global test accuracy: 67.90
Round  79, Train loss: 0.183, Test loss: 0.442, Test accuracy: 85.61
Round  79, Global train loss: 0.183, Global test loss: 1.230, Global test accuracy: 65.56
Round  80, Train loss: 0.259, Test loss: 0.448, Test accuracy: 85.48
Round  80, Global train loss: 0.259, Global test loss: 1.005, Global test accuracy: 66.55
Round  81, Train loss: 0.206, Test loss: 0.453, Test accuracy: 85.42
Round  81, Global train loss: 0.206, Global test loss: 0.972, Global test accuracy: 69.59
Round  82, Train loss: 0.266, Test loss: 0.462, Test accuracy: 85.15
Round  82, Global train loss: 0.266, Global test loss: 0.998, Global test accuracy: 68.17
Round  83, Train loss: 0.216, Test loss: 0.455, Test accuracy: 85.43
Round  83, Global train loss: 0.216, Global test loss: 1.077, Global test accuracy: 67.36
Round  84, Train loss: 0.221, Test loss: 0.440, Test accuracy: 85.64
Round  84, Global train loss: 0.221, Global test loss: 1.364, Global test accuracy: 59.50
Round  85, Train loss: 0.194, Test loss: 0.449, Test accuracy: 85.72
Round  85, Global train loss: 0.194, Global test loss: 1.010, Global test accuracy: 69.06
Round  86, Train loss: 0.221, Test loss: 0.447, Test accuracy: 85.88
Round  86, Global train loss: 0.221, Global test loss: 1.174, Global test accuracy: 64.21
Round  87, Train loss: 0.140, Test loss: 0.459, Test accuracy: 85.61
Round  87, Global train loss: 0.140, Global test loss: 1.084, Global test accuracy: 68.57
Round  88, Train loss: 0.192, Test loss: 0.466, Test accuracy: 85.39
Round  88, Global train loss: 0.192, Global test loss: 1.100, Global test accuracy: 68.04
Round  89, Train loss: 0.137, Test loss: 0.478, Test accuracy: 85.21
Round  89, Global train loss: 0.137, Global test loss: 1.306, Global test accuracy: 65.73
Round  90, Train loss: 0.167, Test loss: 0.474, Test accuracy: 85.56
Round  90, Global train loss: 0.167, Global test loss: 1.323, Global test accuracy: 64.33
Round  91, Train loss: 0.151, Test loss: 0.465, Test accuracy: 85.74
Round  91, Global train loss: 0.151, Global test loss: 1.317, Global test accuracy: 63.12
Round  92, Train loss: 0.207, Test loss: 0.462, Test accuracy: 85.70
Round  92, Global train loss: 0.207, Global test loss: 1.011, Global test accuracy: 69.30
Round  93, Train loss: 0.172, Test loss: 0.459, Test accuracy: 85.69
Round  93, Global train loss: 0.172, Global test loss: 1.508, Global test accuracy: 59.18
Round  94, Train loss: 0.158, Test loss: 0.461, Test accuracy: 85.58
Round  94, Global train loss: 0.158, Global test loss: 1.159, Global test accuracy: 67.63
Round  95, Train loss: 0.166, Test loss: 0.461, Test accuracy: 85.64
Round  95, Global train loss: 0.166, Global test loss: 1.367, Global test accuracy: 63.21