nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.150, Test loss: 2.661, Test accuracy: 57.67
Final Round, Global train loss: 0.150, Global test loss: 1.182, Global test accuracy: 60.95
Average accuracy final 10 rounds: 57.138499999999986 

Average global accuracy final 10 rounds: 60.023250000000004 

6067.594718456268
[4.742488622665405, 9.48497724533081, 14.156854391098022, 18.828731536865234, 23.530823945999146, 28.232916355133057, 33.18820238113403, 38.14348840713501, 43.11849617958069, 48.09350395202637, 53.055196046829224, 58.01688814163208, 63.00375318527222, 67.99061822891235, 72.95457172393799, 77.91852521896362, 82.94308567047119, 87.96764612197876, 92.93814635276794, 97.90864658355713, 102.87864875793457, 107.84865093231201, 112.81712746620178, 117.78560400009155, 122.7602686882019, 127.73493337631226, 132.71235251426697, 137.68977165222168, 142.66095089912415, 147.6321301460266, 152.60137629508972, 157.57062244415283, 162.53549122810364, 167.50036001205444, 172.46792674064636, 177.43549346923828, 182.40482378005981, 187.37415409088135, 192.34690356254578, 197.3196530342102, 202.28999042510986, 207.26032781600952, 212.25064086914062, 217.24095392227173, 222.23947477340698, 227.23799562454224, 232.20551705360413, 237.17303848266602, 242.1219606399536, 247.0708827972412, 252.0395143032074, 257.0081458091736, 262.0011372566223, 266.99412870407104, 272.001261472702, 277.008394241333, 282.00214171409607, 286.99588918685913, 291.9788098335266, 296.9617304801941, 301.9529883861542, 306.94424629211426, 311.9292619228363, 316.91427755355835, 321.9144685268402, 326.91465950012207, 331.9014618396759, 336.88826417922974, 341.87231516838074, 346.85636615753174, 351.8372585773468, 356.81815099716187, 361.81300497055054, 366.8078589439392, 371.79375410079956, 376.7796492576599, 381.7644338607788, 386.7492184638977, 391.74130606651306, 396.7333936691284, 401.7278063297272, 406.7222189903259, 411.7268431186676, 416.7314672470093, 421.7281494140625, 426.7248315811157, 431.72469449043274, 436.72455739974976, 441.72472977638245, 446.72490215301514, 451.71745586395264, 456.71000957489014, 461.6777639389038, 466.6455183029175, 471.62974977493286, 476.61398124694824, 481.5824074745178, 486.5508337020874, 491.4987816810608, 496.4467296600342, 501.3980407714844, 506.34935188293457, 511.2813241481781, 516.2132964134216, 521.1569397449493, 526.100583076477, 531.0647823810577, 536.0289816856384, 541.0289542675018, 546.0289268493652, 551.0162489414215, 556.0035710334778, 560.9910957813263, 565.9786205291748, 570.9609124660492, 575.9432044029236, 580.9355270862579, 585.9278497695923, 590.9083850383759, 595.8889203071594, 600.8645992279053, 605.8402781486511, 610.8277721405029, 615.8152661323547, 620.7724874019623, 625.7297086715698, 630.7161529064178, 635.7025971412659, 640.6796271800995, 645.6566572189331, 650.6379172801971, 655.6191773414612, 660.5931930541992, 665.5672087669373, 670.5730817317963, 675.5789546966553, 680.5822672843933, 685.5855798721313, 690.5994212627411, 695.6132626533508, 700.6703410148621, 705.7274193763733, 710.7651975154877, 715.802975654602, 720.8658363819122, 725.9286971092224, 730.944141626358, 735.9595861434937, 740.9742560386658, 745.9889259338379, 751.0138900279999, 756.0388541221619, 761.0437526702881, 766.0486512184143, 771.0618033409119, 776.0749554634094, 781.095329284668, 786.1157031059265, 791.1232149600983, 796.13072681427, 801.1352398395538, 806.1397528648376, 811.1505856513977, 816.1614184379578, 821.1848964691162, 826.2083745002747, 831.2215855121613, 836.2347965240479, 841.2500998973846, 846.2654032707214, 851.2960114479065, 856.3266196250916, 861.3270697593689, 866.3275198936462, 871.3490364551544, 876.3705530166626, 881.3824076652527, 886.3942623138428, 891.420825958252, 896.4473896026611, 901.4574041366577, 906.4674186706543, 911.493093252182, 916.5187678337097, 921.5996322631836, 926.6804966926575, 931.6914637088776, 936.7024307250977, 941.495246887207, 946.2880630493164, 951.0847399234772, 955.8814167976379, 960.640453338623, 965.3994898796082, 970.2669620513916, 975.134434223175, 980.0063991546631, 984.8783640861511, 989.9108626842499, 994.9433612823486, 997.4769344329834, 1000.0105075836182]
[38.23, 38.23, 43.2225, 43.2225, 44.98, 44.98, 46.475, 46.475, 47.4625, 47.4625, 48.1225, 48.1225, 49.5825, 49.5825, 50.32, 50.32, 50.28, 50.28, 50.795, 50.795, 51.7875, 51.7875, 52.545, 52.545, 53.305, 53.305, 53.3875, 53.3875, 53.86, 53.86, 54.2825, 54.2825, 54.4875, 54.4875, 54.78, 54.78, 55.05, 55.05, 55.245, 55.245, 55.525, 55.525, 55.6425, 55.6425, 55.815, 55.815, 56.0925, 56.0925, 56.3025, 56.3025, 56.3175, 56.3175, 56.2575, 56.2575, 56.21, 56.21, 56.4425, 56.4425, 56.3425, 56.3425, 56.7, 56.7, 56.7725, 56.7725, 56.035, 56.035, 56.02, 56.02, 55.8475, 55.8475, 55.8125, 55.8125, 55.6175, 55.6175, 55.4825, 55.4825, 55.945, 55.945, 56.4275, 56.4275, 56.4825, 56.4825, 56.4775, 56.4775, 56.505, 56.505, 56.665, 56.665, 56.63, 56.63, 56.515, 56.515, 56.4775, 56.4775, 56.43, 56.43, 56.8075, 56.8075, 56.8975, 56.8975, 56.85, 56.85, 56.6625, 56.6625, 56.8725, 56.8725, 56.74, 56.74, 56.5575, 56.5575, 56.53, 56.53, 56.75, 56.75, 56.8825, 56.8825, 57.0175, 57.0175, 56.7125, 56.7125, 56.245, 56.245, 56.25, 56.25, 56.215, 56.215, 56.525, 56.525, 56.1275, 56.1275, 56.1975, 56.1975, 56.5175, 56.5175, 56.5325, 56.5325, 56.79, 56.79, 56.7775, 56.7775, 56.9475, 56.9475, 56.845, 56.845, 56.86, 56.86, 56.845, 56.845, 56.785, 56.785, 56.73, 56.73, 56.7575, 56.7575, 56.6875, 56.6875, 56.6425, 56.6425, 56.94, 56.94, 56.7075, 56.7075, 56.82, 56.82, 57.13, 57.13, 57.1525, 57.1525, 57.11, 57.11, 57.125, 57.125, 57.01, 57.01, 57.21, 57.21, 57.095, 57.095, 57.1075, 57.1075, 57.005, 57.005, 57.2925, 57.2925, 57.3025, 57.3025, 57.27, 57.27, 57.1475, 57.1475, 56.9125, 56.9125, 56.7375, 56.7375, 56.9975, 56.9975, 57.36, 57.36, 57.36, 57.36, 57.6675, 57.6675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.180, Test loss: 0.595, Test accuracy: 83.07
Final Round, Global train loss: 0.180, Global test loss: 1.563, Global test accuracy: 56.68
Average accuracy final 10 rounds: 82.16499999999999 

Average global accuracy final 10 rounds: 57.739999999999995 

899.0198812484741
[0.9226284027099609, 1.8452568054199219, 2.5264298915863037, 3.2076029777526855, 3.884532928466797, 4.561462879180908, 5.276228189468384, 5.990993499755859, 6.710627317428589, 7.430261135101318, 8.148753881454468, 8.867246627807617, 9.585114240646362, 10.302981853485107, 11.021570682525635, 11.740159511566162, 12.462019205093384, 13.183878898620605, 13.910226583480835, 14.636574268341064, 15.35160207748413, 16.066629886627197, 16.78230595588684, 17.497982025146484, 18.21539068222046, 18.932799339294434, 19.651270627975464, 20.369741916656494, 21.087677240371704, 21.805612564086914, 22.52765965461731, 23.249706745147705, 23.963576316833496, 24.677445888519287, 25.393879890441895, 26.110313892364502, 26.83025050163269, 27.55018711090088, 28.265772819519043, 28.981358528137207, 29.696735382080078, 30.41211223602295, 31.127750396728516, 31.843388557434082, 32.56274199485779, 33.282095432281494, 33.99316954612732, 34.704243659973145, 35.419711112976074, 36.135178565979004, 36.85340356826782, 37.57162857055664, 38.294235706329346, 39.01684284210205, 39.73447799682617, 40.45211315155029, 41.17451858520508, 41.89692401885986, 42.60977578163147, 43.322627544403076, 44.04047679901123, 44.758326053619385, 45.47840476036072, 46.19848346710205, 46.92324995994568, 47.64801645278931, 48.36507534980774, 49.08213424682617, 49.79875111579895, 50.51536798477173, 51.234041690826416, 51.9527153968811, 52.675750970840454, 53.398786544799805, 54.115275859832764, 54.83176517486572, 55.552809953689575, 56.27385473251343, 56.99305462837219, 57.71225452423096, 58.427273988723755, 59.14229345321655, 59.856001138687134, 60.569708824157715, 61.286991357803345, 62.004273891448975, 62.72489547729492, 63.44551706314087, 64.16266441345215, 64.87981176376343, 65.58807873725891, 66.2963457107544, 67.00255966186523, 67.70877361297607, 68.41463470458984, 69.12049579620361, 69.83387541770935, 70.54725503921509, 71.26173233985901, 71.97620964050293, 72.6941602230072, 73.41211080551147, 74.12684607505798, 74.84158134460449, 75.55791854858398, 76.27425575256348, 76.9872817993164, 77.70030784606934, 78.31814813613892, 78.9359884262085, 79.56072664260864, 80.18546485900879, 80.79854989051819, 81.41163492202759, 82.03572154045105, 82.65980815887451, 83.28528499603271, 83.91076183319092, 84.5240547657013, 85.13734769821167, 85.74960541725159, 86.3618631362915, 86.97329711914062, 87.58473110198975, 88.20769429206848, 88.83065748214722, 89.44559907913208, 90.06054067611694, 90.67989206314087, 91.2992434501648, 91.91626214981079, 92.53328084945679, 93.15097856521606, 93.76867628097534, 94.3836886882782, 94.99870109558105, 95.61585712432861, 96.23301315307617, 96.85536599159241, 97.47771883010864, 98.09476852416992, 98.7118182182312, 99.33052015304565, 99.94922208786011, 100.56776833534241, 101.1863145828247, 101.804203748703, 102.4220929145813, 103.03778171539307, 103.65347051620483, 104.27073836326599, 104.88800621032715, 105.5055205821991, 106.12303495407104, 106.73828506469727, 107.35353517532349, 107.97633266448975, 108.599130153656, 109.21719026565552, 109.83525037765503, 110.45270299911499, 111.07015562057495, 111.68667984008789, 112.30320405960083, 112.92113518714905, 113.53906631469727, 114.15240502357483, 114.76574373245239, 115.38332104682922, 116.00089836120605, 116.61565041542053, 117.23040246963501, 117.8458092212677, 118.46121597290039, 119.0804169178009, 119.69961786270142, 120.3181140422821, 120.9366102218628, 121.55324459075928, 122.16987895965576, 122.78827428817749, 123.40666961669922, 124.02244114875793, 124.63821268081665, 125.25507593154907, 125.8719391822815, 126.49033331871033, 127.10872745513916, 127.72310853004456, 128.33748960494995, 128.9519453048706, 129.56640100479126, 130.18528270721436, 130.80416440963745, 131.4262354373932, 132.04830646514893, 132.6681351661682, 133.2879638671875, 133.90709686279297, 134.52622985839844, 135.7627980709076, 136.99936628341675]
[26.716666666666665, 26.716666666666665, 41.61666666666667, 41.61666666666667, 43.86666666666667, 43.86666666666667, 53.1, 53.1, 57.65, 57.65, 60.5, 60.5, 64.63333333333334, 64.63333333333334, 65.0, 65.0, 69.06666666666666, 69.06666666666666, 72.55, 72.55, 72.51666666666667, 72.51666666666667, 72.71666666666667, 72.71666666666667, 73.46666666666667, 73.46666666666667, 73.65, 73.65, 73.03333333333333, 73.03333333333333, 73.81666666666666, 73.81666666666666, 74.68333333333334, 74.68333333333334, 74.78333333333333, 74.78333333333333, 75.01666666666667, 75.01666666666667, 75.85, 75.85, 76.36666666666666, 76.36666666666666, 76.1, 76.1, 76.88333333333334, 76.88333333333334, 76.85, 76.85, 77.6, 77.6, 77.71666666666667, 77.71666666666667, 77.63333333333334, 77.63333333333334, 78.03333333333333, 78.03333333333333, 77.23333333333333, 77.23333333333333, 77.35, 77.35, 77.53333333333333, 77.53333333333333, 79.25, 79.25, 79.5, 79.5, 79.76666666666667, 79.76666666666667, 79.18333333333334, 79.18333333333334, 79.65, 79.65, 79.73333333333333, 79.73333333333333, 80.05, 80.05, 80.1, 80.1, 80.2, 80.2, 79.85, 79.85, 79.61666666666666, 79.61666666666666, 79.56666666666666, 79.56666666666666, 79.95, 79.95, 80.33333333333333, 80.33333333333333, 80.41666666666667, 80.41666666666667, 80.4, 80.4, 79.16666666666667, 79.16666666666667, 81.05, 81.05, 81.05, 81.05, 80.7, 80.7, 80.85, 80.85, 81.8, 81.8, 81.31666666666666, 81.31666666666666, 80.95, 80.95, 81.28333333333333, 81.28333333333333, 81.35, 81.35, 81.46666666666667, 81.46666666666667, 81.56666666666666, 81.56666666666666, 80.63333333333334, 80.63333333333334, 81.0, 81.0, 80.28333333333333, 80.28333333333333, 80.46666666666667, 80.46666666666667, 80.61666666666666, 80.61666666666666, 80.85, 80.85, 81.26666666666667, 81.26666666666667, 80.83333333333333, 80.83333333333333, 80.96666666666667, 80.96666666666667, 80.98333333333333, 80.98333333333333, 81.48333333333333, 81.48333333333333, 81.4, 81.4, 81.11666666666666, 81.11666666666666, 81.51666666666667, 81.51666666666667, 81.75, 81.75, 81.68333333333334, 81.68333333333334, 81.7, 81.7, 81.18333333333334, 81.18333333333334, 82.05, 82.05, 81.88333333333334, 81.88333333333334, 81.8, 81.8, 82.1, 82.1, 81.28333333333333, 81.28333333333333, 81.3, 81.3, 81.7, 81.7, 82.68333333333334, 82.68333333333334, 82.63333333333334, 82.63333333333334, 82.65, 82.65, 82.26666666666667, 82.26666666666667, 81.36666666666666, 81.36666666666666, 81.98333333333333, 81.98333333333333, 82.13333333333334, 82.13333333333334, 82.4, 82.4, 82.68333333333334, 82.68333333333334, 81.56666666666666, 81.56666666666666, 82.0, 82.0, 82.46666666666667, 82.46666666666667, 82.51666666666667, 82.51666666666667, 82.48333333333333, 82.48333333333333, 81.8, 81.8, 81.6, 81.6, 83.06666666666666, 83.06666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.247, Test loss: 0.433, Test accuracy: 82.85
Average accuracy final 10 rounds: 82.52833333333334 

734.816445350647
[0.9066381454467773, 1.8132762908935547, 2.4688310623168945, 3.1243858337402344, 3.7769086360931396, 4.429431438446045, 5.089171648025513, 5.7489118576049805, 6.40894627571106, 7.068980693817139, 7.725963354110718, 8.382946014404297, 9.036772012710571, 9.690598011016846, 10.346153259277344, 11.001708507537842, 11.653510570526123, 12.305312633514404, 12.962798595428467, 13.62028455734253, 14.281194686889648, 14.942104816436768, 15.605681419372559, 16.26925802230835, 16.93216848373413, 17.595078945159912, 18.259077787399292, 18.923076629638672, 19.58505916595459, 20.247041702270508, 20.904507637023926, 21.561973571777344, 22.21959662437439, 22.877219676971436, 23.531081199645996, 24.184942722320557, 24.835586071014404, 25.486229419708252, 26.13531517982483, 26.784400939941406, 27.440388679504395, 28.096376419067383, 28.7541561126709, 29.411935806274414, 30.06708860397339, 30.722241401672363, 31.37909984588623, 32.0359582901001, 32.697224378585815, 33.35849046707153, 34.01450228691101, 34.67051410675049, 35.326345443725586, 35.982176780700684, 36.63953948020935, 37.29690217971802, 37.95320677757263, 38.609511375427246, 39.266693353652954, 39.92387533187866, 40.58037495613098, 41.2368745803833, 41.89144515991211, 42.54601573944092, 43.199303150177, 43.852590560913086, 44.502418756484985, 45.152246952056885, 45.804445028305054, 46.45664310455322, 47.10715889930725, 47.75767469406128, 48.41017746925354, 49.0626802444458, 49.71540832519531, 50.368136405944824, 51.01779222488403, 51.66744804382324, 52.32162261009216, 52.975797176361084, 53.632189989089966, 54.28858280181885, 54.94442415237427, 55.60026550292969, 56.253467082977295, 56.9066686630249, 57.560285329818726, 58.21390199661255, 58.86849308013916, 59.52308416366577, 60.1774582862854, 60.83183240890503, 61.48340582847595, 62.134979248046875, 62.78460502624512, 63.43423080444336, 64.07987856864929, 64.72552633285522, 65.37285828590393, 66.02019023895264, 66.67294406890869, 67.32569789886475, 67.97594857215881, 68.62619924545288, 69.27414798736572, 69.92209672927856, 70.57216954231262, 71.22224235534668, 71.87428259849548, 72.52632284164429, 73.18380093574524, 73.84127902984619, 74.4935245513916, 75.14577007293701, 75.79782009124756, 76.4498701095581, 77.10213899612427, 77.75440788269043, 78.40907144546509, 79.06373500823975, 79.71899938583374, 80.37426376342773, 81.02411675453186, 81.67396974563599, 82.3276059627533, 82.9812421798706, 83.6359510421753, 84.29065990447998, 84.94821095466614, 85.6057620048523, 86.26306962966919, 86.92037725448608, 87.57110095024109, 88.2218246459961, 88.87197542190552, 89.52212619781494, 90.17320489883423, 90.82428359985352, 91.47955656051636, 92.1348295211792, 92.78551006317139, 93.43619060516357, 94.08477091789246, 94.73335123062134, 95.38270831108093, 96.03206539154053, 96.68636202812195, 97.34065866470337, 97.99669885635376, 98.65273904800415, 99.30497074127197, 99.9572024345398, 100.60963702201843, 101.26207160949707, 101.91914772987366, 102.57622385025024, 103.22604465484619, 103.87586545944214, 104.5311176776886, 105.18636989593506, 105.83996224403381, 106.49355459213257, 107.14479923248291, 107.79604387283325, 108.44669246673584, 109.09734106063843, 109.75456881523132, 110.41179656982422, 111.06971454620361, 111.72763252258301, 112.37801671028137, 113.02840089797974, 113.6785078048706, 114.32861471176147, 114.98565649986267, 115.64269828796387, 116.29853010177612, 116.95436191558838, 117.60653042793274, 118.2586989402771, 118.91086769104004, 119.56303644180298, 120.21685194969177, 120.87066745758057, 121.52229285240173, 122.1739182472229, 122.8247754573822, 123.4756326675415, 124.12478065490723, 124.77392864227295, 125.42569017410278, 126.07745170593262, 126.73284149169922, 127.38823127746582, 128.04182505607605, 128.69541883468628, 129.35824823379517, 130.02107763290405, 130.68354082107544, 131.34600400924683, 132.53681540489197, 133.7276268005371]
[26.433333333333334, 26.433333333333334, 31.066666666666666, 31.066666666666666, 36.266666666666666, 36.266666666666666, 45.31666666666667, 45.31666666666667, 46.266666666666666, 46.266666666666666, 52.71666666666667, 52.71666666666667, 62.38333333333333, 62.38333333333333, 62.6, 62.6, 65.25, 65.25, 64.7, 64.7, 65.25, 65.25, 68.01666666666667, 68.01666666666667, 69.01666666666667, 69.01666666666667, 69.85, 69.85, 70.51666666666667, 70.51666666666667, 71.1, 71.1, 71.2, 71.2, 71.8, 71.8, 72.28333333333333, 72.28333333333333, 72.63333333333334, 72.63333333333334, 72.25, 72.25, 72.55, 72.55, 73.86666666666666, 73.86666666666666, 74.88333333333334, 74.88333333333334, 74.73333333333333, 74.73333333333333, 74.41666666666667, 74.41666666666667, 76.5, 76.5, 76.73333333333333, 76.73333333333333, 76.75, 76.75, 76.05, 76.05, 76.66666666666667, 76.66666666666667, 76.45, 76.45, 76.95, 76.95, 77.86666666666666, 77.86666666666666, 77.85, 77.85, 77.58333333333333, 77.58333333333333, 77.56666666666666, 77.56666666666666, 78.2, 78.2, 77.46666666666667, 77.46666666666667, 77.68333333333334, 77.68333333333334, 78.03333333333333, 78.03333333333333, 78.33333333333333, 78.33333333333333, 78.43333333333334, 78.43333333333334, 79.11666666666666, 79.11666666666666, 79.08333333333333, 79.08333333333333, 79.3, 79.3, 79.25, 79.25, 79.65, 79.65, 79.55, 79.55, 79.31666666666666, 79.31666666666666, 79.56666666666666, 79.56666666666666, 80.33333333333333, 80.33333333333333, 80.1, 80.1, 80.31666666666666, 80.31666666666666, 80.03333333333333, 80.03333333333333, 81.03333333333333, 81.03333333333333, 80.5, 80.5, 80.66666666666667, 80.66666666666667, 80.61666666666666, 80.61666666666666, 81.28333333333333, 81.28333333333333, 80.65, 80.65, 80.7, 80.7, 80.71666666666667, 80.71666666666667, 80.98333333333333, 80.98333333333333, 81.53333333333333, 81.53333333333333, 81.96666666666667, 81.96666666666667, 81.8, 81.8, 81.61666666666666, 81.61666666666666, 81.63333333333334, 81.63333333333334, 81.86666666666666, 81.86666666666666, 81.36666666666666, 81.36666666666666, 82.2, 82.2, 81.68333333333334, 81.68333333333334, 82.28333333333333, 82.28333333333333, 82.28333333333333, 82.28333333333333, 82.36666666666666, 82.36666666666666, 81.85, 81.85, 81.35, 81.35, 81.31666666666666, 81.31666666666666, 81.55, 81.55, 81.56666666666666, 81.56666666666666, 82.23333333333333, 82.23333333333333, 82.55, 82.55, 82.33333333333333, 82.33333333333333, 82.16666666666667, 82.16666666666667, 82.11666666666666, 82.11666666666666, 82.73333333333333, 82.73333333333333, 82.41666666666667, 82.41666666666667, 82.46666666666667, 82.46666666666667, 82.33333333333333, 82.33333333333333, 81.58333333333333, 81.58333333333333, 82.66666666666667, 82.66666666666667, 82.7, 82.7, 82.4, 82.4, 82.8, 82.8, 82.53333333333333, 82.53333333333333, 82.6, 82.6, 82.43333333333334, 82.43333333333334, 82.8, 82.8, 82.76666666666667, 82.76666666666667, 82.85, 82.85]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.329, Test loss: 0.892, Test accuracy: 75.49
Average accuracy final 10 rounds: 75.7635 

4978.052330255508
[5.016364097595215, 10.03272819519043, 15.021852731704712, 20.010977268218994, 25.034399032592773, 30.057820796966553, 34.90882849693298, 39.759836196899414, 44.59508180618286, 49.43032741546631, 54.416077852249146, 59.40182828903198, 64.54967498779297, 69.69752168655396, 74.75168418884277, 79.80584669113159, 84.74154949188232, 89.67725229263306, 94.58444285392761, 99.49163341522217, 104.36101365089417, 109.23039388656616, 114.17147731781006, 119.11256074905396, 123.97526264190674, 128.83796453475952, 133.88061094284058, 138.92325735092163, 143.88637113571167, 148.8494849205017, 153.84309649467468, 158.83670806884766, 163.7175235748291, 168.59833908081055, 173.48394918441772, 178.3695592880249, 183.2554383277893, 188.1413173675537, 193.0747368335724, 198.00815629959106, 202.8911736011505, 207.77419090270996, 212.61869049072266, 217.46319007873535, 221.73857259750366, 226.01395511627197, 230.2473168373108, 234.4806785583496, 238.73281693458557, 242.98495531082153, 247.24767136573792, 251.5103874206543, 256.44268798828125, 261.3749885559082, 266.23819041252136, 271.1013922691345, 275.95155334472656, 280.8017144203186, 285.67646312713623, 290.55121183395386, 295.4610822200775, 300.3709526062012, 305.2804594039917, 310.1899662017822, 314.47093892097473, 318.75191164016724, 323.035197019577, 327.3184823989868, 331.5806770324707, 335.8428716659546, 340.0957589149475, 344.34864616394043, 348.65726041793823, 352.96587467193604, 357.29550075531006, 361.6251268386841, 365.96872425079346, 370.31232166290283, 374.58078122138977, 378.8492407798767, 383.135858297348, 387.42247581481934, 391.6760139465332, 395.92955207824707, 400.1881294250488, 404.4467067718506, 408.6939480304718, 412.941189289093, 417.19660782814026, 421.4520263671875, 425.65939116477966, 429.8667559623718, 434.1024887561798, 438.3382215499878, 442.62885069847107, 446.91947984695435, 451.15947127342224, 455.39946269989014, 459.6005084514618, 463.80155420303345, 468.06679582595825, 472.33203744888306, 477.2542231082916, 482.1764087677002, 487.11854219436646, 492.0606756210327, 496.9359793663025, 501.81128311157227, 507.2284183502197, 512.6455535888672, 518.3100275993347, 523.9745016098022, 528.6596302986145, 533.3447589874268, 538.2057814598083, 543.0668039321899, 547.8272640705109, 552.5877242088318, 557.4575109481812, 562.3272976875305, 566.8908259868622, 571.4543542861938, 576.039089679718, 580.6238250732422, 585.3540341854095, 590.0842432975769, 594.8915441036224, 599.698844909668, 604.4750990867615, 609.251353263855, 614.2089111804962, 619.1664690971375, 623.9562058448792, 628.7459425926208, 633.5803246498108, 638.4147067070007, 643.2011141777039, 647.987521648407, 652.8501620292664, 657.7128024101257, 662.1263394355774, 666.539876461029, 671.3507761955261, 676.1616759300232, 680.465677022934, 684.7696781158447, 689.1209251880646, 693.4721722602844, 697.774495601654, 702.0768189430237, 706.4717905521393, 710.8667621612549, 715.1990637779236, 719.5313653945923, 723.8259115219116, 728.120457649231, 732.3946084976196, 736.6687593460083, 741.0356528759003, 745.4025464057922, 749.7058577537537, 754.0091691017151, 758.3387532234192, 762.6683373451233, 766.9340143203735, 771.1996912956238, 775.551634311676, 779.9035773277283, 784.2263782024384, 788.5491790771484, 792.8751554489136, 797.2011318206787, 801.458464384079, 805.7157969474792, 809.9635057449341, 814.2112145423889, 818.5627727508545, 822.9143309593201, 827.2803149223328, 831.6462988853455, 835.9975986480713, 840.3488984107971, 844.6264789104462, 848.9040594100952, 853.1377351284027, 857.3714108467102, 861.7002296447754, 866.0290484428406, 870.4176051616669, 874.8061618804932, 879.1613688468933, 883.5165758132935, 887.7727463245392, 892.0289168357849, 896.6960990428925, 901.36328125, 906.1062226295471, 910.8491640090942, 915.5700781345367, 920.2909922599792, 922.2660989761353, 924.2412056922913]
[38.25, 38.25, 45.8775, 45.8775, 50.5125, 50.5125, 54.01, 54.01, 57.635, 57.635, 60.48, 60.48, 62.2475, 62.2475, 63.6175, 63.6175, 64.925, 64.925, 65.9675, 65.9675, 67.1775, 67.1775, 67.8375, 67.8375, 67.8025, 67.8025, 68.82, 68.82, 69.085, 69.085, 69.24, 69.24, 70.13, 70.13, 70.905, 70.905, 71.2625, 71.2625, 71.8375, 71.8375, 72.03, 72.03, 72.615, 72.615, 72.46, 72.46, 73.0575, 73.0575, 73.1875, 73.1875, 72.8225, 72.8225, 73.5175, 73.5175, 73.25, 73.25, 73.7775, 73.7775, 73.8825, 73.8825, 74.0725, 74.0725, 74.3725, 74.3725, 74.315, 74.315, 74.445, 74.445, 74.0475, 74.0475, 74.685, 74.685, 74.5475, 74.5475, 74.655, 74.655, 74.6025, 74.6025, 74.65, 74.65, 74.8725, 74.8725, 75.2475, 75.2475, 75.1125, 75.1125, 75.195, 75.195, 75.0925, 75.0925, 75.23, 75.23, 75.1675, 75.1675, 74.6525, 74.6525, 74.7975, 74.7975, 75.5775, 75.5775, 75.615, 75.615, 75.5325, 75.5325, 75.1425, 75.1425, 75.1525, 75.1525, 75.29, 75.29, 75.2525, 75.2525, 75.3575, 75.3575, 75.5475, 75.5475, 75.295, 75.295, 75.4925, 75.4925, 75.18, 75.18, 76.0675, 76.0675, 75.725, 75.725, 75.5275, 75.5275, 75.6675, 75.6675, 75.4475, 75.4475, 75.6475, 75.6475, 75.7425, 75.7425, 75.16, 75.16, 75.3575, 75.3575, 75.57, 75.57, 75.74, 75.74, 75.6125, 75.6125, 75.76, 75.76, 75.67, 75.67, 75.6575, 75.6575, 75.6025, 75.6025, 75.9225, 75.9225, 75.6925, 75.6925, 75.9325, 75.9325, 75.7875, 75.7875, 75.61, 75.61, 75.6, 75.6, 75.75, 75.75, 75.4775, 75.4775, 75.8725, 75.8725, 75.8475, 75.8475, 75.83, 75.83, 76.0825, 76.0825, 76.15, 76.15, 75.935, 75.935, 75.6, 75.6, 75.6325, 75.6325, 75.64, 75.64, 75.805, 75.805, 75.6275, 75.6275, 75.9075, 75.9075, 75.9175, 75.9175, 75.85, 75.85, 75.72, 75.72, 75.4875, 75.4875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.039, Test loss: 1.018, Test accuracy: 81.18
Average accuracy final 10 rounds: 80.81083333333332 

1525.32390832901
[1.6208698749542236, 3.2417397499084473, 4.632605075836182, 6.023470401763916, 7.437958478927612, 8.852446556091309, 10.263679504394531, 11.674912452697754, 13.057212829589844, 14.439513206481934, 15.820663213729858, 17.201813220977783, 18.627142667770386, 20.05247211456299, 21.680169343948364, 23.30786657333374, 25.020594120025635, 26.73332166671753, 28.57537007331848, 30.417418479919434, 32.126182317733765, 33.834946155548096, 35.61389994621277, 37.39285373687744, 39.12099885940552, 40.849143981933594, 42.47017979621887, 44.09121561050415, 45.49595856666565, 46.90070152282715, 48.22790336608887, 49.555105209350586, 50.89416265487671, 52.23322010040283, 53.59433317184448, 54.95544624328613, 56.27628827095032, 57.5971302986145, 58.91270303726196, 60.228275775909424, 61.538084268569946, 62.84789276123047, 64.17459273338318, 65.50129270553589, 66.84126138687134, 68.18123006820679, 69.49655318260193, 70.81187629699707, 72.12195348739624, 73.43203067779541, 74.75620698928833, 76.08038330078125, 77.44745135307312, 78.81451940536499, 80.1260416507721, 81.4375638961792, 82.7322256565094, 84.0268874168396, 85.34268736839294, 86.65848731994629, 88.00957608222961, 89.36066484451294, 90.70291900634766, 92.04517316818237, 93.48508954048157, 94.92500591278076, 96.37869691848755, 97.83238792419434, 99.31472063064575, 100.79705333709717, 102.2723708152771, 103.74768829345703, 105.22857928276062, 106.70947027206421, 108.20044040679932, 109.69141054153442, 111.17132759094238, 112.65124464035034, 114.00807642936707, 115.36490821838379, 116.83996319770813, 118.31501817703247, 119.71087574958801, 121.10673332214355, 122.40632963180542, 123.70592594146729, 125.0405957698822, 126.37526559829712, 127.67134428024292, 128.96742296218872, 130.2758502960205, 131.5842776298523, 132.90530061721802, 134.22632360458374, 135.5730893611908, 136.91985511779785, 138.20404243469238, 139.4882297515869, 140.78959250450134, 142.09095525741577, 143.38153886795044, 144.6721224784851, 145.98043298721313, 147.28874349594116, 148.58746194839478, 149.8861804008484, 151.2061631679535, 152.5261459350586, 153.8451235294342, 155.16410112380981, 156.4401822090149, 157.71626329421997, 159.01641654968262, 160.31656980514526, 161.62801051139832, 162.93945121765137, 164.23700499534607, 165.53455877304077, 166.8500461578369, 168.16553354263306, 169.49557447433472, 170.82561540603638, 172.13578009605408, 173.44594478607178, 174.74215602874756, 176.03836727142334, 177.37210726737976, 178.70584726333618, 180.03239059448242, 181.35893392562866, 182.63878870010376, 183.91864347457886, 185.20522737503052, 186.49181127548218, 187.8013458251953, 189.11088037490845, 190.40705180168152, 191.7032232284546, 192.98386406898499, 194.26450490951538, 195.56598472595215, 196.86746454238892, 198.1790406703949, 199.49061679840088, 200.78637671470642, 202.08213663101196, 203.35110759735107, 204.62007856369019, 205.92165899276733, 207.22323942184448, 208.518319606781, 209.81339979171753, 211.10063362121582, 212.3878674507141, 213.67630338668823, 214.96473932266235, 216.25551962852478, 217.5462999343872, 218.8150336742401, 220.08376741409302, 221.3598186969757, 222.6358699798584, 223.92673516273499, 225.21760034561157, 226.5041651725769, 227.79072999954224, 229.06721997261047, 230.3437099456787, 231.6251790523529, 232.9066481590271, 234.20207858085632, 235.49750900268555, 236.7833390235901, 238.06916904449463, 239.34013175964355, 240.61109447479248, 241.93008160591125, 243.24906873703003, 244.53820061683655, 245.82733249664307, 247.11425065994263, 248.4011688232422, 249.70358777046204, 251.00600671768188, 252.30924677848816, 253.61248683929443, 254.90816736221313, 256.20384788513184, 257.5241551399231, 258.84446239471436, 260.1410028934479, 261.4375433921814, 262.7369203567505, 264.0362973213196, 265.3390166759491, 266.6417360305786, 267.9190969467163, 269.196457862854, 270.4734652042389, 271.7504725456238, 273.86665654182434, 275.9828405380249]
[25.758333333333333, 25.758333333333333, 40.63333333333333, 40.63333333333333, 47.075, 47.075, 56.7, 56.7, 59.175, 59.175, 60.416666666666664, 60.416666666666664, 63.875, 63.875, 67.94166666666666, 67.94166666666666, 67.94166666666666, 67.94166666666666, 70.45, 70.45, 71.66666666666667, 71.66666666666667, 70.95, 70.95, 72.44166666666666, 72.44166666666666, 73.55, 73.55, 73.83333333333333, 73.83333333333333, 74.25, 74.25, 75.25833333333334, 75.25833333333334, 75.74166666666666, 75.74166666666666, 76.275, 76.275, 75.61666666666666, 75.61666666666666, 76.83333333333333, 76.83333333333333, 76.75833333333334, 76.75833333333334, 76.48333333333333, 76.48333333333333, 76.14166666666667, 76.14166666666667, 76.98333333333333, 76.98333333333333, 77.09166666666667, 77.09166666666667, 78.18333333333334, 78.18333333333334, 78.51666666666667, 78.51666666666667, 78.575, 78.575, 78.3, 78.3, 78.64166666666667, 78.64166666666667, 78.76666666666667, 78.76666666666667, 79.18333333333334, 79.18333333333334, 79.175, 79.175, 79.23333333333333, 79.23333333333333, 79.35833333333333, 79.35833333333333, 78.825, 78.825, 78.55833333333334, 78.55833333333334, 78.59166666666667, 78.59166666666667, 78.65833333333333, 78.65833333333333, 79.00833333333334, 79.00833333333334, 79.5, 79.5, 79.74166666666666, 79.74166666666666, 80.05, 80.05, 80.3, 80.3, 79.65833333333333, 79.65833333333333, 79.75833333333334, 79.75833333333334, 80.05833333333334, 80.05833333333334, 80.225, 80.225, 80.175, 80.175, 80.09166666666667, 80.09166666666667, 80.075, 80.075, 80.06666666666666, 80.06666666666666, 79.65833333333333, 79.65833333333333, 79.88333333333334, 79.88333333333334, 79.45, 79.45, 79.575, 79.575, 80.05, 80.05, 80.29166666666667, 80.29166666666667, 80.06666666666666, 80.06666666666666, 80.35833333333333, 80.35833333333333, 80.24166666666666, 80.24166666666666, 80.55, 80.55, 80.25833333333334, 80.25833333333334, 80.48333333333333, 80.48333333333333, 80.56666666666666, 80.56666666666666, 80.625, 80.625, 80.525, 80.525, 80.41666666666667, 80.41666666666667, 79.725, 79.725, 79.475, 79.475, 79.575, 79.575, 80.29166666666667, 80.29166666666667, 80.375, 80.375, 80.35, 80.35, 80.59166666666667, 80.59166666666667, 80.275, 80.275, 79.98333333333333, 79.98333333333333, 80.08333333333333, 80.08333333333333, 80.6, 80.6, 80.75, 80.75, 80.73333333333333, 80.73333333333333, 81.025, 81.025, 80.51666666666667, 80.51666666666667, 80.575, 80.575, 80.6, 80.6, 80.825, 80.825, 81.06666666666666, 81.06666666666666, 80.81666666666666, 80.81666666666666, 80.48333333333333, 80.48333333333333, 80.68333333333334, 80.68333333333334, 80.50833333333334, 80.50833333333334, 80.53333333333333, 80.53333333333333, 80.99166666666666, 80.99166666666666, 80.85833333333333, 80.85833333333333, 80.86666666666666, 80.86666666666666, 80.83333333333333, 80.83333333333333, 80.81666666666666, 80.81666666666666, 80.925, 80.925, 81.09166666666667, 81.09166666666667, 81.18333333333334, 81.18333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.049, Test loss: 0.746, Test accuracy: 68.45
Average accuracy final 10 rounds: 67.58583333333333
1839.7670996189117
[]
[29.808333333333334, 27.775, 34.11666666666667, 38.59166666666667, 46.80833333333333, 47.81666666666667, 44.65, 43.65833333333333, 44.53333333333333, 45.45, 45.30833333333333, 46.74166666666667, 51.166666666666664, 51.40833333333333, 52.108333333333334, 52.858333333333334, 51.65833333333333, 52.63333333333333, 54.81666666666667, 53.88333333333333, 55.35, 55.141666666666666, 53.05, 54.525, 54.775, 55.833333333333336, 55.975, 54.925, 56.358333333333334, 55.375, 54.74166666666667, 55.81666666666667, 55.75, 56.35, 56.166666666666664, 55.15833333333333, 58.166666666666664, 58.09166666666667, 58.583333333333336, 59.275, 58.625, 58.725, 59.61666666666667, 58.24166666666667, 57.875, 58.7, 60.34166666666667, 61.333333333333336, 61.19166666666667, 61.375, 62.583333333333336, 61.891666666666666, 61.38333333333333, 61.11666666666667, 62.05, 62.525, 63.46666666666667, 62.983333333333334, 62.916666666666664, 64.775, 64.75, 63.9, 64.20833333333333, 63.925, 65.59166666666667, 65.325, 65.05833333333334, 65.8, 64.8, 65.25833333333334, 64.29166666666667, 65.38333333333334, 64.29166666666667, 64.70833333333333, 65.29166666666667, 64.85, 66.2, 65.13333333333334, 66.54166666666667, 65.525, 64.61666666666666, 65.70833333333333, 65.9, 65.90833333333333, 66.89166666666667, 66.7, 65.725, 65.975, 65.225, 66.40833333333333, 66.44166666666666, 65.84166666666667, 65.50833333333334, 68.01666666666667, 68.05, 67.76666666666667, 69.19166666666666, 67.51666666666667, 68.89166666666667, 68.63333333333334, 68.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.573, Test loss: 0.558, Test accuracy: 77.44
Average accuracy final 10 rounds: 77.0725
Average global accuracy final 10 rounds: 77.0725
1357.46582198143
[]
[30.208333333333332, 44.84166666666667, 44.9, 51.03333333333333, 55.825, 62.50833333333333, 61.45, 62.625, 64.6, 65.84166666666667, 65.26666666666667, 64.99166666666666, 66.425, 68.48333333333333, 68.65833333333333, 68.2, 69.71666666666667, 70.18333333333334, 70.51666666666667, 70.325, 70.16666666666667, 70.80833333333334, 70.85, 70.99166666666666, 70.925, 71.775, 71.44166666666666, 70.53333333333333, 72.09166666666667, 72.525, 73.19166666666666, 72.25, 73.65833333333333, 74.75, 74.85833333333333, 74.65833333333333, 75.00833333333334, 75.28333333333333, 75.20833333333333, 74.90833333333333, 74.7, 74.35833333333333, 74.59166666666667, 74.5, 74.8, 74.20833333333333, 74.375, 74.34166666666667, 74.775, 74.85833333333333, 74.525, 74.1, 74.10833333333333, 74.175, 75.425, 75.10833333333333, 75.75, 76.15, 76.55, 76.96666666666667, 77.6, 77.45833333333333, 77.65833333333333, 77.48333333333333, 76.68333333333334, 76.44166666666666, 76.91666666666667, 76.65, 76.46666666666667, 76.2, 76.75833333333334, 76.76666666666667, 76.36666666666666, 75.81666666666666, 76.28333333333333, 76.35833333333333, 76.91666666666667, 77.375, 77.16666666666667, 77.1, 76.70833333333333, 76.98333333333333, 76.9, 77.44166666666666, 77.325, 77.425, 77.375, 77.06666666666666, 77.2, 77.3, 77.23333333333333, 77.09166666666667, 77.875, 78.03333333333333, 77.31666666666666, 76.775, 76.35833333333333, 76.71666666666667, 76.15, 77.175, 77.44166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.163, Test loss: 1.070, Test accuracy: 66.91
Average accuracy final 10 rounds: 62.64999999999999
2407.990555047989
[3.8178532123565674, 7.424312591552734, 10.79268193244934, 14.202137470245361, 17.60018038749695, 20.97981882095337, 24.30562424659729, 27.70861029624939, 31.088043689727783, 34.49903106689453, 37.88311982154846, 41.29109597206116, 44.67161750793457, 48.08990716934204, 51.50739765167236, 54.90304183959961, 58.29339027404785, 61.73516511917114, 65.15950775146484, 68.54207682609558, 71.93524408340454, 75.36072707176208, 78.75400280952454, 82.13612127304077, 85.50204253196716, 88.8886296749115, 92.29623222351074, 95.69445538520813, 99.04831075668335, 102.39633393287659, 105.80082058906555, 109.20182013511658, 112.64568281173706, 116.05007982254028, 119.47193765640259, 122.8953685760498, 126.35168027877808, 129.6994550228119, 133.11949372291565, 136.54532957077026, 139.99552512168884, 143.4227077960968, 146.85330486297607, 150.30621671676636, 153.81890869140625, 157.28755712509155, 160.88517022132874, 164.41802501678467, 167.81214904785156, 171.18428540229797, 174.55242943763733, 177.9549331665039, 181.36996793746948, 184.8591866493225, 188.17762923240662, 191.59168100357056, 194.94247269630432, 198.32164692878723, 201.64906287193298, 205.03868579864502, 208.46787095069885, 211.86685037612915, 215.24874448776245, 218.62479376792908, 222.04002594947815, 225.4133267402649, 228.83696150779724, 232.209144115448, 235.61276984214783, 239.0166666507721, 242.45007300376892, 245.84758734703064, 249.29664063453674, 252.63941025733948, 256.0370662212372, 259.4197027683258, 262.85014843940735, 266.25413703918457, 269.6597855091095, 273.06925201416016, 276.6849069595337, 280.2855968475342, 283.7042033672333, 287.06886863708496, 290.46539187431335, 293.7843236923218, 297.20274114608765, 300.52839255332947, 304.1619379520416, 307.77367329597473, 311.3371753692627, 314.62992906570435, 317.98955249786377, 321.6785349845886, 325.39179611206055, 329.0553059577942, 332.7420039176941, 336.36754989624023, 340.01090455055237, 343.4203300476074, 346.33541464805603]
[20.008333333333333, 27.033333333333335, 24.233333333333334, 28.558333333333334, 35.791666666666664, 34.56666666666667, 36.833333333333336, 38.166666666666664, 43.608333333333334, 36.325, 47.24166666666667, 46.05, 37.025, 45.96666666666667, 48.15833333333333, 46.166666666666664, 52.30833333333333, 44.925, 47.575, 52.55, 48.13333333333333, 50.44166666666667, 51.475, 46.583333333333336, 47.791666666666664, 53.85, 43.108333333333334, 53.7, 53.525, 54.833333333333336, 57.8, 56.71666666666667, 54.95, 51.358333333333334, 57.516666666666666, 55.1, 58.34166666666667, 55.075, 51.78333333333333, 53.983333333333334, 56.81666666666667, 56.43333333333333, 54.925, 56.36666666666667, 52.775, 55.625, 56.99166666666667, 60.675, 57.391666666666666, 55.00833333333333, 58.56666666666667, 63.18333333333333, 61.00833333333333, 62.375, 62.358333333333334, 58.3, 61.50833333333333, 57.43333333333333, 56.4, 59.583333333333336, 61.5, 62.416666666666664, 56.84166666666667, 59.541666666666664, 62.05833333333333, 61.641666666666666, 59.55, 61.43333333333333, 59.291666666666664, 58.80833333333333, 60.858333333333334, 62.833333333333336, 60.958333333333336, 62.233333333333334, 55.31666666666667, 56.68333333333333, 58.28333333333333, 62.075, 60.475, 60.391666666666666, 60.525, 62.416666666666664, 64.675, 62.19166666666667, 60.291666666666664, 60.4, 61.391666666666666, 60.225, 63.38333333333333, 63.458333333333336, 63.30833333333333, 63.34166666666667, 63.766666666666666, 57.541666666666664, 63.075, 64.325, 62.50833333333333, 61.358333333333334, 64.4, 62.875, 66.90833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 58235 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53709 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.215, Test loss: 0.373, Test accuracy: 85.83
Average accuracy final 10 rounds: 85.36333333333333
1649.208075761795
[2.1210596561431885, 4.242119312286377, 5.909784317016602, 7.577449321746826, 9.18256664276123, 10.787683963775635, 12.360110998153687, 13.932538032531738, 15.45419454574585, 16.97585105895996, 18.527546644210815, 20.07924222946167, 21.78557562828064, 23.49190902709961, 25.146480560302734, 26.80105209350586, 28.399035215377808, 29.997018337249756, 31.63840675354004, 33.27979516983032, 34.91660165786743, 36.55340814590454, 38.15125322341919, 39.74909830093384, 41.39940142631531, 43.04970455169678, 44.74054789543152, 46.43139123916626, 47.954999685287476, 49.47860813140869, 51.06847095489502, 52.65833377838135, 54.251904010772705, 55.84547424316406, 57.428446769714355, 59.01141929626465, 60.604586601257324, 62.19775390625, 63.82208490371704, 65.44641590118408, 66.97963571548462, 68.51285552978516, 70.10514616966248, 71.6974368095398, 73.34346842765808, 74.98950004577637, 76.55047535896301, 78.11145067214966, 79.64500164985657, 81.17855262756348, 82.75786113739014, 84.3371696472168, 85.90829396247864, 87.47941827774048, 89.01172184944153, 90.54402542114258, 92.13007664680481, 93.71612787246704, 95.2695426940918, 96.82295751571655, 98.33693027496338, 99.8509030342102, 101.3977038860321, 102.944504737854, 104.58964037895203, 106.23477602005005, 107.78309679031372, 109.33141756057739, 110.89743280410767, 112.46344804763794, 114.07480955123901, 115.68617105484009, 117.25644540786743, 118.82671976089478, 120.37913012504578, 121.93154048919678, 123.54799365997314, 125.16444683074951, 126.71835041046143, 128.27225399017334, 129.80702304840088, 131.34179210662842, 132.91081476211548, 134.47983741760254, 136.0504858493805, 137.62113428115845, 139.1550648212433, 140.68899536132812, 142.38507604599, 144.08115673065186, 145.7536027431488, 147.42604875564575, 148.97497463226318, 150.52390050888062, 152.08985686302185, 153.6558132171631, 155.22841596603394, 156.80101871490479, 158.3558759689331, 159.91073322296143, 161.4502923488617, 162.98985147476196, 164.56381630897522, 166.13778114318848, 167.71016883850098, 169.28255653381348, 170.92397952079773, 172.56540250778198, 174.22550177574158, 175.88560104370117, 177.53349566459656, 179.18139028549194, 180.8905544281006, 182.59971857070923, 184.31727766990662, 186.034836769104, 187.73688316345215, 189.4389295578003, 191.07892489433289, 192.71892023086548, 194.45369744300842, 196.18847465515137, 197.862774848938, 199.5370750427246, 201.21692609786987, 202.89677715301514, 204.62144780158997, 206.3461184501648, 208.07420778274536, 209.80229711532593, 211.49585461616516, 213.1894121170044, 214.89941787719727, 216.60942363739014, 218.31918168067932, 220.0289397239685, 221.73769521713257, 223.44645071029663, 225.19645953178406, 226.94646835327148, 228.5862340927124, 230.22599983215332, 231.78826332092285, 233.35052680969238, 234.92745757102966, 236.50438833236694, 238.0334050655365, 239.56242179870605, 241.0853295326233, 242.60823726654053, 244.15000700950623, 245.69177675247192, 247.22805333137512, 248.76432991027832, 250.28749299049377, 251.81065607070923, 253.3352768421173, 254.8598976135254, 256.38132524490356, 257.90275287628174, 259.44001483917236, 260.977276802063, 262.60183548927307, 264.22639417648315, 265.8518168926239, 267.47723960876465, 268.99873447418213, 270.5202293395996, 272.1016516685486, 273.68307399749756, 275.2586770057678, 276.8342800140381, 278.3497042655945, 279.8651285171509, 281.3862998485565, 282.90747117996216, 284.4321222305298, 285.9567732810974, 287.4823694229126, 289.0079655647278, 290.52937173843384, 292.0507779121399, 293.59717059135437, 295.14356327056885, 296.67238569259644, 298.201208114624, 299.7455995082855, 301.289990901947, 302.86350560188293, 304.43702030181885, 305.99471712112427, 307.5524139404297, 309.0825674533844, 310.6127209663391, 312.258731842041, 313.9047427177429, 315.54587864875793, 317.18701457977295, 318.79011034965515, 320.39320611953735, 322.5570077896118, 324.7208094596863]
[20.683333333333334, 20.683333333333334, 38.475, 38.475, 46.49166666666667, 46.49166666666667, 58.425, 58.425, 61.041666666666664, 61.041666666666664, 63.208333333333336, 63.208333333333336, 65.79166666666667, 65.79166666666667, 68.56666666666666, 68.56666666666666, 70.975, 70.975, 72.48333333333333, 72.48333333333333, 73.325, 73.325, 73.63333333333334, 73.63333333333334, 74.45833333333333, 74.45833333333333, 74.775, 74.775, 75.35, 75.35, 75.61666666666666, 75.61666666666666, 75.94166666666666, 75.94166666666666, 76.63333333333334, 76.63333333333334, 76.575, 76.575, 77.73333333333333, 77.73333333333333, 77.58333333333333, 77.58333333333333, 77.89166666666667, 77.89166666666667, 78.85, 78.85, 79.06666666666666, 79.06666666666666, 79.025, 79.025, 79.55, 79.55, 79.55, 79.55, 80.05833333333334, 80.05833333333334, 80.35, 80.35, 80.375, 80.375, 80.64166666666667, 80.64166666666667, 80.35, 80.35, 80.39166666666667, 80.39166666666667, 81.325, 81.325, 81.475, 81.475, 80.95833333333333, 80.95833333333333, 80.55, 80.55, 80.875, 80.875, 80.5, 80.5, 81.80833333333334, 81.80833333333334, 81.71666666666667, 81.71666666666667, 81.93333333333334, 81.93333333333334, 82.00833333333334, 82.00833333333334, 82.225, 82.225, 82.31666666666666, 82.31666666666666, 83.09166666666667, 83.09166666666667, 82.825, 82.825, 82.65833333333333, 82.65833333333333, 83.1, 83.1, 82.55, 82.55, 82.90833333333333, 82.90833333333333, 83.525, 83.525, 83.18333333333334, 83.18333333333334, 83.64166666666667, 83.64166666666667, 83.175, 83.175, 83.65, 83.65, 83.85, 83.85, 83.70833333333333, 83.70833333333333, 83.84166666666667, 83.84166666666667, 84.16666666666667, 84.16666666666667, 84.24166666666666, 84.24166666666666, 84.25833333333334, 84.25833333333334, 84.21666666666667, 84.21666666666667, 84.51666666666667, 84.51666666666667, 84.23333333333333, 84.23333333333333, 84.36666666666666, 84.36666666666666, 84.40833333333333, 84.40833333333333, 84.56666666666666, 84.56666666666666, 84.25833333333334, 84.25833333333334, 84.39166666666667, 84.39166666666667, 84.15833333333333, 84.15833333333333, 84.34166666666667, 84.34166666666667, 84.65, 84.65, 84.45833333333333, 84.45833333333333, 84.41666666666667, 84.41666666666667, 84.86666666666666, 84.86666666666666, 84.89166666666667, 84.89166666666667, 84.73333333333333, 84.73333333333333, 84.425, 84.425, 84.70833333333333, 84.70833333333333, 85.05833333333334, 85.05833333333334, 84.45, 84.45, 84.70833333333333, 84.70833333333333, 84.74166666666666, 84.74166666666666, 84.98333333333333, 84.98333333333333, 85.26666666666667, 85.26666666666667, 85.08333333333333, 85.08333333333333, 84.96666666666667, 84.96666666666667, 85.15833333333333, 85.15833333333333, 85.725, 85.725, 85.01666666666667, 85.01666666666667, 85.24166666666666, 85.24166666666666, 85.56666666666666, 85.56666666666666, 85.18333333333334, 85.18333333333334, 85.45, 85.45, 85.44166666666666, 85.44166666666666, 85.44166666666666, 85.44166666666666, 85.3, 85.3, 85.49166666666666, 85.49166666666666, 85.5, 85.5, 85.825, 85.825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.058, Test loss: 1.028, Test accuracy: 80.04
Final Round, Global train loss: 0.058, Global test loss: 1.989, Global test accuracy: 32.63
Average accuracy final 10 rounds: 79.59916666666666 

Average global accuracy final 10 rounds: 26.029999999999998 

1810.3538954257965
[1.609360694885254, 3.218721389770508, 4.5869140625, 5.955106735229492, 7.385869979858398, 8.816633224487305, 10.437080144882202, 12.0575270652771, 13.49044394493103, 14.923360824584961, 16.391185760498047, 17.859010696411133, 19.292522192001343, 20.726033687591553, 22.21331524848938, 23.700596809387207, 25.165102243423462, 26.629607677459717, 28.083008527755737, 29.536409378051758, 30.99995517730713, 32.4635009765625, 33.916221141815186, 35.36894130706787, 36.78381824493408, 38.19869518280029, 39.68005394935608, 41.161412715911865, 42.631410360336304, 44.10140800476074, 45.56819772720337, 47.034987449645996, 48.48869466781616, 49.94240188598633, 51.391117811203, 52.83983373641968, 54.278706789016724, 55.71757984161377, 57.18713450431824, 58.656689167022705, 60.12229132652283, 61.58789348602295, 63.040940046310425, 64.4939866065979, 65.95756435394287, 67.42114210128784, 68.82442283630371, 70.22770357131958, 71.67771863937378, 73.12773370742798, 74.61517429351807, 76.10261487960815, 77.54547882080078, 78.98834276199341, 80.40457725524902, 81.82081174850464, 83.11109495162964, 84.40137815475464, 85.68555355072021, 86.96972894668579, 88.2375316619873, 89.50533437728882, 90.77452182769775, 92.04370927810669, 93.33271193504333, 94.62171459197998, 95.92311501502991, 97.22451543807983, 98.49691653251648, 99.76931762695312, 101.03294324874878, 102.29656887054443, 103.58773016929626, 104.8788914680481, 106.19453954696655, 107.51018762588501, 108.79258584976196, 110.07498407363892, 111.38984417915344, 112.70470428466797, 114.01017117500305, 115.31563806533813, 116.5949215888977, 117.87420511245728, 119.18523287773132, 120.49626064300537, 121.78318476676941, 123.07010889053345, 124.33020377159119, 125.59029865264893, 126.86766910552979, 128.14503955841064, 129.42099142074585, 130.69694328308105, 131.98879528045654, 133.28064727783203, 134.57226634025574, 135.86388540267944, 137.1340527534485, 138.40422010421753, 139.68161153793335, 140.95900297164917, 142.25648522377014, 143.5539674758911, 144.81346988677979, 146.07297229766846, 147.33317828178406, 148.59338426589966, 149.88426899909973, 151.1751537322998, 152.44908237457275, 153.7230110168457, 155.01020693778992, 156.29740285873413, 157.5663321018219, 158.83526134490967, 160.10675621032715, 161.37825107574463, 162.66262650489807, 163.9470019340515, 165.23445844650269, 166.52191495895386, 167.8206114768982, 169.11930799484253, 170.42356991767883, 171.72783184051514, 173.02721667289734, 174.32660150527954, 175.60636448860168, 176.88612747192383, 178.1758735179901, 179.4656195640564, 180.7625377178192, 182.05945587158203, 183.36642241477966, 184.6733889579773, 185.9723606109619, 187.27133226394653, 188.54228281974792, 189.81323337554932, 191.07346606254578, 192.33369874954224, 193.60178756713867, 194.8698763847351, 196.15021634101868, 197.43055629730225, 198.69078493118286, 199.95101356506348, 201.21370315551758, 202.47639274597168, 203.74169826507568, 205.0070037841797, 206.28002738952637, 207.55305099487305, 208.80338406562805, 210.05371713638306, 211.29867005348206, 212.54362297058105, 213.7814223766327, 215.01922178268433, 216.27518391609192, 217.5311460494995, 218.77878189086914, 220.02641773223877, 221.2603404521942, 222.49426317214966, 223.7465488910675, 224.99883460998535, 226.24948859214783, 227.5001425743103, 228.74225902557373, 229.98437547683716, 231.223308801651, 232.46224212646484, 233.71166586875916, 234.96108961105347, 236.21267795562744, 237.46426630020142, 238.69967484474182, 239.93508338928223, 241.1787292957306, 242.42237520217896, 243.66723942756653, 244.9121036529541, 246.14795517921448, 247.38380670547485, 248.61320281028748, 249.8425989151001, 251.09008240699768, 252.33756589889526, 253.57844352722168, 254.8193211555481, 256.0584440231323, 257.29756689071655, 258.54084300994873, 259.7841191291809, 261.04276633262634, 262.3014135360718, 263.5741217136383, 264.84682989120483, 267.2532362937927, 269.6596426963806]
[33.0, 33.0, 47.166666666666664, 47.166666666666664, 52.275, 52.275, 61.13333333333333, 61.13333333333333, 60.19166666666667, 60.19166666666667, 62.833333333333336, 62.833333333333336, 61.516666666666666, 61.516666666666666, 65.15, 65.15, 63.975, 63.975, 68.08333333333333, 68.08333333333333, 66.9, 66.9, 67.025, 67.025, 71.45833333333333, 71.45833333333333, 67.71666666666667, 67.71666666666667, 69.60833333333333, 69.60833333333333, 70.875, 70.875, 75.7, 75.7, 75.975, 75.975, 76.225, 76.225, 75.76666666666667, 75.76666666666667, 75.725, 75.725, 75.48333333333333, 75.48333333333333, 75.46666666666667, 75.46666666666667, 75.80833333333334, 75.80833333333334, 75.91666666666667, 75.91666666666667, 76.81666666666666, 76.81666666666666, 77.1, 77.1, 77.55833333333334, 77.55833333333334, 77.39166666666667, 77.39166666666667, 77.41666666666667, 77.41666666666667, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.55833333333334, 77.84166666666667, 77.84166666666667, 77.35, 77.35, 77.79166666666667, 77.79166666666667, 78.3, 78.3, 78.39166666666667, 78.39166666666667, 78.375, 78.375, 78.375, 78.375, 78.725, 78.725, 78.625, 78.625, 78.41666666666667, 78.41666666666667, 78.49166666666666, 78.49166666666666, 78.65, 78.65, 78.8, 78.8, 79.00833333333334, 79.00833333333334, 78.825, 78.825, 78.875, 78.875, 78.9, 78.9, 78.825, 78.825, 78.86666666666666, 78.86666666666666, 79.14166666666667, 79.14166666666667, 79.40833333333333, 79.40833333333333, 79.31666666666666, 79.31666666666666, 79.28333333333333, 79.28333333333333, 79.24166666666666, 79.24166666666666, 79.16666666666667, 79.16666666666667, 79.05833333333334, 79.05833333333334, 78.75833333333334, 78.75833333333334, 78.70833333333333, 78.70833333333333, 78.24166666666666, 78.24166666666666, 78.58333333333333, 78.58333333333333, 78.93333333333334, 78.93333333333334, 78.85833333333333, 78.85833333333333, 78.78333333333333, 78.78333333333333, 79.25833333333334, 79.25833333333334, 79.58333333333333, 79.58333333333333, 79.50833333333334, 79.50833333333334, 79.19166666666666, 79.19166666666666, 79.33333333333333, 79.33333333333333, 79.375, 79.375, 79.475, 79.475, 78.85, 78.85, 79.16666666666667, 79.16666666666667, 79.49166666666666, 79.49166666666666, 79.24166666666666, 79.24166666666666, 78.94166666666666, 78.94166666666666, 79.11666666666666, 79.11666666666666, 79.56666666666666, 79.56666666666666, 79.68333333333334, 79.68333333333334, 79.41666666666667, 79.41666666666667, 79.5, 79.5, 79.45833333333333, 79.45833333333333, 79.34166666666667, 79.34166666666667, 79.175, 79.175, 79.16666666666667, 79.16666666666667, 79.21666666666667, 79.21666666666667, 79.3, 79.3, 79.36666666666666, 79.36666666666666, 79.55833333333334, 79.55833333333334, 79.64166666666667, 79.64166666666667, 79.425, 79.425, 79.43333333333334, 79.43333333333334, 79.65, 79.65, 79.60833333333333, 79.60833333333333, 79.875, 79.875, 79.68333333333334, 79.68333333333334, 79.58333333333333, 79.58333333333333, 79.64166666666667, 79.64166666666667, 79.45, 79.45, 80.04166666666667, 80.04166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.152, Test loss: 0.495, Test accuracy: 86.00
Final Round, Global train loss: 0.152, Global test loss: 1.462, Global test accuracy: 59.88
Average accuracy final 10 rounds: 85.4775 

Average global accuracy final 10 rounds: 66.64916666666667 

1809.4219617843628
[1.6397664546966553, 3.2795329093933105, 4.659959077835083, 6.0403852462768555, 7.453839302062988, 8.867293357849121, 10.279869318008423, 11.692445278167725, 13.091654062271118, 14.490862846374512, 15.90920114517212, 17.327539443969727, 18.789653778076172, 20.251768112182617, 21.701507568359375, 23.151247024536133, 24.582286596298218, 26.013326168060303, 27.470691919326782, 28.92805767059326, 30.35317611694336, 31.778294563293457, 33.240174770355225, 34.70205497741699, 36.1061224937439, 37.5101900100708, 38.93978238105774, 40.36937475204468, 41.80388951301575, 43.238404273986816, 44.67634105682373, 46.114277839660645, 47.53536558151245, 48.95645332336426, 50.37647557258606, 51.79649782180786, 53.2147536277771, 54.63300943374634, 56.053404569625854, 57.47379970550537, 58.91081881523132, 60.347837924957275, 61.76723337173462, 63.18662881851196, 64.62022423744202, 66.05381965637207, 67.48720598220825, 68.92059230804443, 70.36402583122253, 71.80745935440063, 73.23584508895874, 74.66423082351685, 76.10158252716064, 77.53893423080444, 78.97589063644409, 80.41284704208374, 81.84371995925903, 83.27459287643433, 84.71752572059631, 86.1604585647583, 87.60700869560242, 89.05355882644653, 90.4875876903534, 91.92161655426025, 93.34927201271057, 94.77692747116089, 96.21775269508362, 97.65857791900635, 99.09440040588379, 100.53022289276123, 101.9706563949585, 103.41108989715576, 104.86259341239929, 106.31409692764282, 107.75558996200562, 109.19708299636841, 110.638742685318, 112.08040237426758, 113.52139663696289, 114.9623908996582, 116.39064598083496, 117.81890106201172, 119.24556469917297, 120.67222833633423, 122.10238528251648, 123.53254222869873, 124.96743988990784, 126.40233755111694, 127.82263350486755, 129.24292945861816, 130.67671823501587, 132.11050701141357, 133.54994368553162, 134.98938035964966, 136.42730450630188, 137.8652286529541, 139.29307556152344, 140.72092247009277, 141.9581413269043, 143.19536018371582, 144.4235188961029, 145.65167760849, 146.8777916431427, 148.1039056777954, 149.33883476257324, 150.57376384735107, 151.79204058647156, 153.01031732559204, 154.2300362586975, 155.44975519180298, 156.68337774276733, 157.9170002937317, 159.14023232460022, 160.36346435546875, 161.5948839187622, 162.82630348205566, 164.05385756492615, 165.28141164779663, 166.517502784729, 167.75359392166138, 168.98597836494446, 170.21836280822754, 171.43905019760132, 172.6597375869751, 173.88647508621216, 175.11321258544922, 176.3466558456421, 177.58009910583496, 178.80455422401428, 180.0290093421936, 181.26016402244568, 182.49131870269775, 183.7291555404663, 184.96699237823486, 186.1867651939392, 187.40653800964355, 188.63043022155762, 189.85432243347168, 191.0774998664856, 192.3006772994995, 193.51960921287537, 194.73854112625122, 195.9716923236847, 197.20484352111816, 198.4348418712616, 199.66484022140503, 200.89417910575867, 202.1235179901123, 203.35323429107666, 204.58295059204102, 205.81695675849915, 207.05096292495728, 208.27856922149658, 209.5061755180359, 210.73494124412537, 211.96370697021484, 213.19663667678833, 214.42956638336182, 215.66809391975403, 216.90662145614624, 218.14791917800903, 219.38921689987183, 220.61961603164673, 221.85001516342163, 223.09099125862122, 224.3319673538208, 225.56358003616333, 226.79519271850586, 228.02914571762085, 229.26309871673584, 230.4932360649109, 231.72337341308594, 232.9562873840332, 234.18920135498047, 235.41745257377625, 236.64570379257202, 237.87344908714294, 239.10119438171387, 240.3270444869995, 241.55289459228516, 242.78830814361572, 244.0237216949463, 245.24405670166016, 246.46439170837402, 247.6918272972107, 248.91926288604736, 250.13965725898743, 251.3600516319275, 252.58591151237488, 253.81177139282227, 255.0415506362915, 256.27132987976074, 257.50398778915405, 258.73664569854736, 260.2014629840851, 261.6662802696228, 263.10877561569214, 264.5512709617615, 265.9933488368988, 267.43542671203613, 269.844131231308, 272.25283575057983]
[29.383333333333333, 29.383333333333333, 34.666666666666664, 34.666666666666664, 49.59166666666667, 49.59166666666667, 53.69166666666667, 53.69166666666667, 57.375, 57.375, 68.44166666666666, 68.44166666666666, 66.55833333333334, 66.55833333333334, 69.40833333333333, 69.40833333333333, 68.45, 68.45, 72.175, 72.175, 72.75833333333334, 72.75833333333334, 73.84166666666667, 73.84166666666667, 73.36666666666666, 73.36666666666666, 74.99166666666666, 74.99166666666666, 75.65833333333333, 75.65833333333333, 75.71666666666667, 75.71666666666667, 78.15833333333333, 78.15833333333333, 78.99166666666666, 78.99166666666666, 79.69166666666666, 79.69166666666666, 79.71666666666667, 79.71666666666667, 79.925, 79.925, 80.69166666666666, 80.69166666666666, 80.83333333333333, 80.83333333333333, 80.63333333333334, 80.63333333333334, 80.825, 80.825, 81.31666666666666, 81.31666666666666, 81.425, 81.425, 81.93333333333334, 81.93333333333334, 82.00833333333334, 82.00833333333334, 82.35, 82.35, 82.73333333333333, 82.73333333333333, 82.75833333333334, 82.75833333333334, 82.43333333333334, 82.43333333333334, 82.15833333333333, 82.15833333333333, 82.50833333333334, 82.50833333333334, 82.625, 82.625, 82.71666666666667, 82.71666666666667, 82.95, 82.95, 83.0, 83.0, 83.38333333333334, 83.38333333333334, 83.48333333333333, 83.48333333333333, 84.04166666666667, 84.04166666666667, 83.46666666666667, 83.46666666666667, 83.125, 83.125, 83.48333333333333, 83.48333333333333, 83.23333333333333, 83.23333333333333, 83.225, 83.225, 83.15, 83.15, 83.99166666666666, 83.99166666666666, 84.26666666666667, 84.26666666666667, 84.25833333333334, 84.25833333333334, 84.54166666666667, 84.54166666666667, 84.175, 84.175, 83.975, 83.975, 84.10833333333333, 84.10833333333333, 84.48333333333333, 84.48333333333333, 84.45833333333333, 84.45833333333333, 84.35, 84.35, 84.13333333333334, 84.13333333333334, 84.575, 84.575, 84.175, 84.175, 84.275, 84.275, 84.59166666666667, 84.59166666666667, 83.76666666666667, 83.76666666666667, 83.45, 83.45, 83.85, 83.85, 84.275, 84.275, 84.05, 84.05, 84.63333333333334, 84.63333333333334, 84.75833333333334, 84.75833333333334, 84.78333333333333, 84.78333333333333, 85.21666666666667, 85.21666666666667, 85.28333333333333, 85.28333333333333, 85.43333333333334, 85.43333333333334, 85.30833333333334, 85.30833333333334, 85.16666666666667, 85.16666666666667, 84.65, 84.65, 84.525, 84.525, 84.08333333333333, 84.08333333333333, 84.08333333333333, 84.08333333333333, 83.65833333333333, 83.65833333333333, 84.15, 84.15, 84.325, 84.325, 84.53333333333333, 84.53333333333333, 84.66666666666667, 84.66666666666667, 84.95833333333333, 84.95833333333333, 85.23333333333333, 85.23333333333333, 84.825, 84.825, 84.925, 84.925, 85.41666666666667, 85.41666666666667, 85.75, 85.75, 85.73333333333333, 85.73333333333333, 85.5, 85.5, 85.59166666666667, 85.59166666666667, 85.65, 85.65, 85.66666666666667, 85.66666666666667, 85.25833333333334, 85.25833333333334, 85.23333333333333, 85.23333333333333, 85.23333333333333, 85.23333333333333, 85.15833333333333, 85.15833333333333, 86.0, 86.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.198, Test loss: 0.361, Test accuracy: 86.38
Average accuracy final 10 rounds: 86.1525 

1433.0313503742218
[1.4755096435546875, 2.951019287109375, 4.200707912445068, 5.450396537780762, 6.744216680526733, 8.038036823272705, 9.331805944442749, 10.625575065612793, 11.92225193977356, 13.218928813934326, 14.512876033782959, 15.806823253631592, 17.10746669769287, 18.40811014175415, 19.714025259017944, 21.01994037628174, 22.314350843429565, 23.608761310577393, 24.903976440429688, 26.199191570281982, 27.50011134147644, 28.8010311126709, 30.0981228351593, 31.395214557647705, 32.69799613952637, 34.00077772140503, 35.29912352561951, 36.597469329833984, 37.89577007293701, 39.19407081604004, 40.49856185913086, 41.80305290222168, 43.09563088417053, 44.388208866119385, 45.68792152404785, 46.98763418197632, 48.28886914253235, 49.59010410308838, 50.88963556289673, 52.18916702270508, 53.48998308181763, 54.790799140930176, 56.0921847820282, 57.39357042312622, 58.69026708602905, 59.986963748931885, 61.28575897216797, 62.58455419540405, 63.82748985290527, 65.0704255104065, 66.2511990070343, 67.43197250366211, 68.61703729629517, 69.80210208892822, 70.99278163909912, 72.18346118927002, 73.38140296936035, 74.57934474945068, 75.76774072647095, 76.95613670349121, 78.14191269874573, 79.32768869400024, 80.51622915267944, 81.70476961135864, 82.88314986228943, 84.06153011322021, 85.23700380325317, 86.41247749328613, 87.61003470420837, 88.80759191513062, 89.99589467048645, 91.18419742584229, 92.46706628799438, 93.74993515014648, 94.96737790107727, 96.18482065200806, 97.39849877357483, 98.6121768951416, 99.83559036254883, 101.05900382995605, 102.27837562561035, 103.49774742126465, 104.72874426841736, 105.95974111557007, 107.17912697792053, 108.398512840271, 109.60721826553345, 110.8159236907959, 112.03023767471313, 113.24455165863037, 114.46323466300964, 115.68191766738892, 116.89377641677856, 118.10563516616821, 119.32572770118713, 120.54582023620605, 121.77186393737793, 122.9979076385498, 124.213210105896, 125.42851257324219, 126.6313750743866, 127.834237575531, 129.02779936790466, 130.22136116027832, 131.42115235328674, 132.62094354629517, 133.8170313835144, 135.01311922073364, 136.21355414390564, 137.41398906707764, 138.60934925079346, 139.80470943450928, 141.00792503356934, 142.2111406326294, 143.4118595123291, 144.6125783920288, 145.81774520874023, 147.02291202545166, 148.22012424468994, 149.41733646392822, 150.58669257164001, 151.7560486793518, 152.94318962097168, 154.13033056259155, 155.32486844062805, 156.51940631866455, 157.697979927063, 158.87655353546143, 160.06118202209473, 161.24581050872803, 162.43342447280884, 163.62103843688965, 164.81826949119568, 166.0155005455017, 167.2058253288269, 168.3961501121521, 169.59420132637024, 170.79225254058838, 171.99920415878296, 173.20615577697754, 174.40605759620667, 175.6059594154358, 176.80620765686035, 178.0064558982849, 179.20330786705017, 180.40015983581543, 181.6003715991974, 182.80058336257935, 183.99808382987976, 185.19558429718018, 186.38034296035767, 187.56510162353516, 188.7550904750824, 189.94507932662964, 191.14783930778503, 192.35059928894043, 193.57190775871277, 194.7932162284851, 196.00668811798096, 197.2201600074768, 198.4513177871704, 199.682475566864, 200.90106463432312, 202.11965370178223, 203.34878826141357, 204.57792282104492, 205.81984901428223, 207.06177520751953, 208.2855474948883, 209.50931978225708, 210.72883415222168, 211.94834852218628, 213.19816064834595, 214.44797277450562, 215.6746950149536, 216.9014172554016, 218.10995721817017, 219.31849718093872, 220.52472448349, 221.73095178604126, 222.94525742530823, 224.1595630645752, 225.564546585083, 226.96953010559082, 228.2545042037964, 229.53947830200195, 230.92589592933655, 232.31231355667114, 233.6695272922516, 235.02674102783203, 236.42631483078003, 237.82588863372803, 239.21650218963623, 240.60711574554443, 241.96674704551697, 243.3263783454895, 244.71929502487183, 246.11221170425415, 247.50866603851318, 248.90512037277222, 250.9609739780426, 253.016827583313]
[23.433333333333334, 23.433333333333334, 32.1, 32.1, 47.075, 47.075, 47.15, 47.15, 57.93333333333333, 57.93333333333333, 61.833333333333336, 61.833333333333336, 63.625, 63.625, 67.68333333333334, 67.68333333333334, 69.54166666666667, 69.54166666666667, 68.15, 68.15, 68.28333333333333, 68.28333333333333, 69.56666666666666, 69.56666666666666, 74.99166666666666, 74.99166666666666, 76.41666666666667, 76.41666666666667, 77.175, 77.175, 77.36666666666666, 77.36666666666666, 76.725, 76.725, 77.56666666666666, 77.56666666666666, 77.09166666666667, 77.09166666666667, 78.28333333333333, 78.28333333333333, 78.19166666666666, 78.19166666666666, 78.29166666666667, 78.29166666666667, 79.225, 79.225, 78.8, 78.8, 79.73333333333333, 79.73333333333333, 80.225, 80.225, 79.99166666666666, 79.99166666666666, 80.45833333333333, 80.45833333333333, 79.81666666666666, 79.81666666666666, 81.175, 81.175, 81.2, 81.2, 81.225, 81.225, 81.73333333333333, 81.73333333333333, 81.425, 81.425, 81.325, 81.325, 81.66666666666667, 81.66666666666667, 82.10833333333333, 82.10833333333333, 82.06666666666666, 82.06666666666666, 82.75, 82.75, 82.725, 82.725, 82.95, 82.95, 83.14166666666667, 83.14166666666667, 82.66666666666667, 82.66666666666667, 83.49166666666666, 83.49166666666666, 83.58333333333333, 83.58333333333333, 82.88333333333334, 82.88333333333334, 83.70833333333333, 83.70833333333333, 83.91666666666667, 83.91666666666667, 84.125, 84.125, 83.525, 83.525, 84.34166666666667, 84.34166666666667, 84.425, 84.425, 84.275, 84.275, 84.45, 84.45, 84.41666666666667, 84.41666666666667, 84.26666666666667, 84.26666666666667, 84.39166666666667, 84.39166666666667, 84.74166666666666, 84.74166666666666, 84.38333333333334, 84.38333333333334, 84.45, 84.45, 84.49166666666666, 84.49166666666666, 84.2, 84.2, 84.96666666666667, 84.96666666666667, 85.14166666666667, 85.14166666666667, 84.675, 84.675, 84.84166666666667, 84.84166666666667, 84.94166666666666, 84.94166666666666, 84.81666666666666, 84.81666666666666, 85.0, 85.0, 84.975, 84.975, 84.99166666666666, 84.99166666666666, 85.075, 85.075, 85.49166666666666, 85.49166666666666, 85.51666666666667, 85.51666666666667, 85.69166666666666, 85.69166666666666, 85.7, 85.7, 85.325, 85.325, 85.40833333333333, 85.40833333333333, 85.45833333333333, 85.45833333333333, 85.63333333333334, 85.63333333333334, 86.04166666666667, 86.04166666666667, 85.9, 85.9, 85.86666666666666, 85.86666666666666, 85.78333333333333, 85.78333333333333, 85.8, 85.8, 85.14166666666667, 85.14166666666667, 85.8, 85.8, 86.175, 86.175, 85.81666666666666, 85.81666666666666, 85.98333333333333, 85.98333333333333, 86.15833333333333, 86.15833333333333, 86.03333333333333, 86.03333333333333, 86.225, 86.225, 86.00833333333334, 86.00833333333334, 86.4, 86.4, 86.25833333333334, 86.25833333333334, 86.125, 86.125, 85.95833333333333, 85.95833333333333, 86.18333333333334, 86.18333333333334, 86.175, 86.175, 86.375, 86.375]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.134, Test loss: 0.403, Test accuracy: 86.56
Average accuracy final 10 rounds: 86.09083333333334 

1516.2543060779572
[1.7956461906433105, 3.591292381286621, 5.089701175689697, 6.588109970092773, 7.85438084602356, 9.120651721954346, 10.408975839614868, 11.69729995727539, 12.981102705001831, 14.264905452728271, 15.554450035095215, 16.843994617462158, 18.130687475204468, 19.417380332946777, 20.70384931564331, 21.990318298339844, 23.279972791671753, 24.569627285003662, 25.854844570159912, 27.140061855316162, 28.425593614578247, 29.711125373840332, 30.986578702926636, 32.26203203201294, 33.531495332717896, 34.80095863342285, 36.07835292816162, 37.35574722290039, 38.631632566452026, 39.90751791000366, 41.16582703590393, 42.4241361618042, 43.68896555900574, 44.953794956207275, 46.23128533363342, 47.50877571105957, 48.7774338722229, 50.04609203338623, 51.31737756729126, 52.58866310119629, 53.841341733932495, 55.0940203666687, 56.35911560058594, 57.624210834503174, 58.897464990615845, 60.170719146728516, 61.4506402015686, 62.73056125640869, 64.00493359565735, 65.279305934906, 66.55811810493469, 67.83693027496338, 69.12797236442566, 70.41901445388794, 71.78127861022949, 73.14354276657104, 74.44832968711853, 75.75311660766602, 77.00709700584412, 78.26107740402222, 79.52731394767761, 80.79355049133301, 82.04506993293762, 83.29658937454224, 84.55178737640381, 85.80698537826538, 87.23744225502014, 88.6678991317749, 90.14072871208191, 91.61355829238892, 93.05008220672607, 94.48660612106323, 95.95469307899475, 97.42278003692627, 98.87942337989807, 100.33606672286987, 101.82012557983398, 103.3041844367981, 104.76520013809204, 106.22621583938599, 107.70606637001038, 109.18591690063477, 110.66948652267456, 112.15305614471436, 113.63764929771423, 115.12224245071411, 116.62148094177246, 118.12071943283081, 119.60390281677246, 121.08708620071411, 122.55634379386902, 124.02560138702393, 125.3330614566803, 126.64052152633667, 127.94018530845642, 129.23984909057617, 130.52927088737488, 131.81869268417358, 133.1198136806488, 134.42093467712402, 135.8597092628479, 137.29848384857178, 138.73770213127136, 140.17692041397095, 141.61989545822144, 143.06287050247192, 144.51976084709167, 145.97665119171143, 147.46188855171204, 148.94712591171265, 150.44058275222778, 151.93403959274292, 153.39830422401428, 154.86256885528564, 156.31949615478516, 157.77642345428467, 159.23646759986877, 160.69651174545288, 162.14958810806274, 163.6026644706726, 165.05786848068237, 166.51307249069214, 167.9812662601471, 169.44946002960205, 170.9161353111267, 172.38281059265137, 173.85022854804993, 175.3176465034485, 176.77318835258484, 178.2287302017212, 179.6328477859497, 181.03696537017822, 182.41485285758972, 183.79274034500122, 185.17067337036133, 186.54860639572144, 187.93468356132507, 189.3207607269287, 190.77244639396667, 192.22413206100464, 193.6750476360321, 195.12596321105957, 196.58963799476624, 198.0533127784729, 199.51473760604858, 200.97616243362427, 202.430104970932, 203.88404750823975, 205.3448531627655, 206.80565881729126, 208.2714512348175, 209.73724365234375, 211.19639801979065, 212.65555238723755, 214.15092086791992, 215.6462893486023, 217.15879559516907, 218.67130184173584, 220.2025294303894, 221.73375701904297, 223.1816110610962, 224.6294651031494, 226.0863926410675, 227.5433201789856, 229.00704789161682, 230.47077560424805, 231.93016862869263, 233.3895616531372, 234.84454154968262, 236.29952144622803, 237.75826025009155, 239.21699905395508, 240.67382645606995, 242.13065385818481, 243.58878803253174, 245.04692220687866, 246.48352551460266, 247.92012882232666, 249.19037675857544, 250.46062469482422, 251.73296332359314, 253.00530195236206, 254.28241991996765, 255.55953788757324, 256.81009316444397, 258.0606484413147, 259.31260228157043, 260.5645561218262, 261.8153796195984, 263.0662031173706, 264.3133924007416, 265.56058168411255, 266.81202387809753, 268.0634660720825, 269.32081389427185, 270.5781617164612, 271.8379604816437, 273.0977592468262, 274.3610579967499, 275.6243567466736, 277.61983013153076, 279.61530351638794]
[26.4, 26.4, 35.90833333333333, 35.90833333333333, 43.858333333333334, 43.858333333333334, 59.40833333333333, 59.40833333333333, 60.55833333333333, 60.55833333333333, 60.825, 60.825, 67.25, 67.25, 71.8, 71.8, 72.29166666666667, 72.29166666666667, 72.23333333333333, 72.23333333333333, 73.625, 73.625, 74.49166666666666, 74.49166666666666, 75.53333333333333, 75.53333333333333, 73.8, 73.8, 76.09166666666667, 76.09166666666667, 76.98333333333333, 76.98333333333333, 78.775, 78.775, 79.35, 79.35, 79.11666666666666, 79.11666666666666, 78.75833333333334, 78.75833333333334, 78.86666666666666, 78.86666666666666, 80.29166666666667, 80.29166666666667, 81.24166666666666, 81.24166666666666, 81.01666666666667, 81.01666666666667, 81.91666666666667, 81.91666666666667, 81.80833333333334, 81.80833333333334, 81.60833333333333, 81.60833333333333, 82.03333333333333, 82.03333333333333, 81.80833333333334, 81.80833333333334, 83.04166666666667, 83.04166666666667, 82.45833333333333, 82.45833333333333, 82.9, 82.9, 83.325, 83.325, 83.48333333333333, 83.48333333333333, 83.84166666666667, 83.84166666666667, 83.88333333333334, 83.88333333333334, 84.05, 84.05, 84.05, 84.05, 84.29166666666667, 84.29166666666667, 84.4, 84.4, 84.14166666666667, 84.14166666666667, 84.80833333333334, 84.80833333333334, 84.10833333333333, 84.10833333333333, 85.1, 85.1, 84.4, 84.4, 84.525, 84.525, 84.99166666666666, 84.99166666666666, 84.725, 84.725, 85.1, 85.1, 85.1, 85.1, 85.18333333333334, 85.18333333333334, 85.2, 85.2, 85.25, 85.25, 85.09166666666667, 85.09166666666667, 85.20833333333333, 85.20833333333333, 85.725, 85.725, 85.68333333333334, 85.68333333333334, 85.45833333333333, 85.45833333333333, 85.6, 85.6, 85.23333333333333, 85.23333333333333, 85.65833333333333, 85.65833333333333, 85.5, 85.5, 85.70833333333333, 85.70833333333333, 85.58333333333333, 85.58333333333333, 86.00833333333334, 86.00833333333334, 85.60833333333333, 85.60833333333333, 86.03333333333333, 86.03333333333333, 85.59166666666667, 85.59166666666667, 85.89166666666667, 85.89166666666667, 85.48333333333333, 85.48333333333333, 85.60833333333333, 85.60833333333333, 85.71666666666667, 85.71666666666667, 85.63333333333334, 85.63333333333334, 85.825, 85.825, 85.75833333333334, 85.75833333333334, 85.825, 85.825, 85.325, 85.325, 85.68333333333334, 85.68333333333334, 85.2, 85.2, 85.93333333333334, 85.93333333333334, 85.525, 85.525, 85.39166666666667, 85.39166666666667, 85.94166666666666, 85.94166666666666, 86.03333333333333, 86.03333333333333, 86.08333333333333, 86.08333333333333, 86.09166666666667, 86.09166666666667, 85.85, 85.85, 86.28333333333333, 86.28333333333333, 85.95, 85.95, 86.38333333333334, 86.38333333333334, 86.00833333333334, 86.00833333333334, 86.275, 86.275, 86.11666666666666, 86.11666666666666, 86.0, 86.0, 86.25, 86.25, 86.04166666666667, 86.04166666666667, 85.85, 85.85, 86.01666666666667, 86.01666666666667, 86.26666666666667, 86.26666666666667, 86.08333333333333, 86.08333333333333, 86.55833333333334, 86.55833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.039, Test loss: 1.024, Test accuracy: 81.47
Average accuracy final 10 rounds: 81.13250000000001 

1604.8656768798828
[1.7969813346862793, 3.5939626693725586, 5.120118856430054, 6.646275043487549, 8.172393083572388, 9.698511123657227, 11.210453033447266, 12.722394943237305, 14.224781274795532, 15.72716760635376, 17.240439414978027, 18.753711223602295, 20.257832765579224, 21.761954307556152, 23.269670009613037, 24.777385711669922, 26.29940915107727, 27.82143259048462, 29.345702171325684, 30.869971752166748, 32.38777804374695, 33.90558433532715, 35.415860176086426, 36.9261360168457, 38.4373300075531, 39.9485239982605, 41.46706676483154, 42.98560953140259, 44.509315490722656, 46.033021450042725, 47.54535961151123, 49.057697772979736, 50.57048320770264, 52.08326864242554, 53.60569643974304, 55.12812423706055, 56.65041399002075, 58.17270374298096, 59.69227623939514, 61.211848735809326, 62.733927965164185, 64.25600719451904, 65.78424572944641, 67.31248426437378, 68.83303594589233, 70.35358762741089, 71.87055492401123, 73.38752222061157, 74.90781211853027, 76.42810201644897, 77.95488905906677, 79.48167610168457, 81.00399851799011, 82.52632093429565, 84.04654693603516, 85.56677293777466, 87.10035634040833, 88.63393974304199, 90.14591217041016, 91.65788459777832, 93.17038154602051, 94.6828784942627, 96.20209097862244, 97.72130346298218, 99.22564578056335, 100.72998809814453, 102.23423600196838, 103.73848390579224, 105.24636721611023, 106.75425052642822, 108.26986122131348, 109.78547191619873, 111.29747676849365, 112.80948162078857, 114.31955933570862, 115.82963705062866, 117.35249471664429, 118.87535238265991, 120.39486455917358, 121.91437673568726, 123.43125176429749, 124.94812679290771, 126.45550394058228, 127.96288108825684, 129.4738006591797, 130.98472023010254, 132.48633432388306, 133.98794841766357, 135.48137617111206, 136.97480392456055, 138.48857831954956, 140.00235271453857, 141.51864790916443, 143.03494310379028, 144.54151272773743, 146.04808235168457, 147.56853246688843, 149.08898258209229, 150.53289914131165, 151.976815700531, 153.42506980895996, 154.87332391738892, 156.3196985721588, 157.7660732269287, 159.22006559371948, 160.67405796051025, 162.12424278259277, 163.5744276046753, 165.0307137966156, 166.4869999885559, 167.9910478591919, 169.49509572982788, 170.95792031288147, 172.42074489593506, 173.9204969406128, 175.42024898529053, 176.92569088935852, 178.4311327934265, 179.94108390808105, 181.4510350227356, 182.95999693870544, 184.4689588546753, 185.9261772632599, 187.38339567184448, 188.85245513916016, 190.32151460647583, 191.7738425731659, 193.22617053985596, 194.7330219745636, 196.23987340927124, 197.76336860656738, 199.28686380386353, 200.80635261535645, 202.32584142684937, 203.8550832271576, 205.38432502746582, 206.91830825805664, 208.45229148864746, 209.96017169952393, 211.4680519104004, 212.98570704460144, 214.5033621788025, 216.01825881004333, 217.53315544128418, 218.99563360214233, 220.4581117630005, 221.96869039535522, 223.47926902770996, 224.9897656440735, 226.500262260437, 228.02217149734497, 229.54408073425293, 231.07269096374512, 232.6013011932373, 234.1197485923767, 235.6381959915161, 237.14821028709412, 238.65822458267212, 240.16645765304565, 241.6746907234192, 243.17918848991394, 244.6836862564087, 246.21524477005005, 247.7468032836914, 249.32965278625488, 250.91250228881836, 252.42833638191223, 253.9441704750061, 255.4669051170349, 256.9896397590637, 258.5038146972656, 260.01798963546753, 261.52624320983887, 263.0344967842102, 264.5427963733673, 266.0510959625244, 267.55970525741577, 269.06831455230713, 270.5859205722809, 272.10352659225464, 273.7028341293335, 275.30214166641235, 276.8217487335205, 278.34135580062866, 279.8177013397217, 281.2940468788147, 282.7761125564575, 284.25817823410034, 285.7363336086273, 287.2144889831543, 288.70549416542053, 290.19649934768677, 291.6814422607422, 293.1663851737976, 294.6586880683899, 296.1509909629822, 297.6381936073303, 299.12539625167847, 300.60366678237915, 302.08193731307983, 304.4156129360199, 306.74928855895996]
[28.908333333333335, 28.908333333333335, 43.71666666666667, 43.71666666666667, 49.916666666666664, 49.916666666666664, 56.625, 56.625, 61.175, 61.175, 63.25, 63.25, 66.60833333333333, 66.60833333333333, 68.40833333333333, 68.40833333333333, 72.725, 72.725, 72.85833333333333, 72.85833333333333, 73.14166666666667, 73.14166666666667, 74.0, 74.0, 73.775, 73.775, 73.19166666666666, 73.19166666666666, 73.23333333333333, 73.23333333333333, 73.675, 73.675, 74.05, 74.05, 75.04166666666667, 75.04166666666667, 76.03333333333333, 76.03333333333333, 76.43333333333334, 76.43333333333334, 76.325, 76.325, 76.525, 76.525, 77.25833333333334, 77.25833333333334, 77.35833333333333, 77.35833333333333, 77.29166666666667, 77.29166666666667, 77.36666666666666, 77.36666666666666, 77.48333333333333, 77.48333333333333, 78.43333333333334, 78.43333333333334, 78.53333333333333, 78.53333333333333, 78.65833333333333, 78.65833333333333, 78.54166666666667, 78.54166666666667, 78.60833333333333, 78.60833333333333, 78.59166666666667, 78.59166666666667, 79.3, 79.3, 79.06666666666666, 79.06666666666666, 78.725, 78.725, 79.05, 79.05, 78.89166666666667, 78.89166666666667, 79.05, 79.05, 79.31666666666666, 79.31666666666666, 79.06666666666666, 79.06666666666666, 78.94166666666666, 78.94166666666666, 79.075, 79.075, 79.75833333333334, 79.75833333333334, 79.4, 79.4, 79.18333333333334, 79.18333333333334, 79.25, 79.25, 79.125, 79.125, 79.23333333333333, 79.23333333333333, 79.46666666666667, 79.46666666666667, 79.50833333333334, 79.50833333333334, 79.3, 79.3, 79.45, 79.45, 79.25, 79.25, 79.625, 79.625, 79.81666666666666, 79.81666666666666, 79.65, 79.65, 79.85, 79.85, 80.53333333333333, 80.53333333333333, 80.15, 80.15, 80.49166666666666, 80.49166666666666, 80.725, 80.725, 80.18333333333334, 80.18333333333334, 80.66666666666667, 80.66666666666667, 80.65, 80.65, 80.9, 80.9, 80.925, 80.925, 80.73333333333333, 80.73333333333333, 80.125, 80.125, 80.31666666666666, 80.31666666666666, 80.38333333333334, 80.38333333333334, 80.48333333333333, 80.48333333333333, 80.35, 80.35, 80.79166666666667, 80.79166666666667, 80.83333333333333, 80.83333333333333, 81.13333333333334, 81.13333333333334, 81.03333333333333, 81.03333333333333, 81.10833333333333, 81.10833333333333, 80.83333333333333, 80.83333333333333, 80.85833333333333, 80.85833333333333, 80.41666666666667, 80.41666666666667, 80.50833333333334, 80.50833333333334, 80.46666666666667, 80.46666666666667, 80.91666666666667, 80.91666666666667, 80.675, 80.675, 81.24166666666666, 81.24166666666666, 81.76666666666667, 81.76666666666667, 81.41666666666667, 81.41666666666667, 81.35, 81.35, 81.575, 81.575, 81.70833333333333, 81.70833333333333, 81.525, 81.525, 81.525, 81.525, 81.34166666666667, 81.34166666666667, 81.06666666666666, 81.06666666666666, 81.14166666666667, 81.14166666666667, 80.975, 80.975, 80.70833333333333, 80.70833333333333, 80.71666666666667, 80.71666666666667, 80.61666666666666, 80.61666666666666, 81.475, 81.475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.123, Test loss: 1.661, Test accuracy: 45.12
Average accuracy final 10 rounds: 44.653749999999995
6737.586879968643
[]
[33.475, 33.585, 32.9175, 31.1975, 30.67, 32.07, 30.58, 28.7975, 28.71, 27.765, 28.635, 28.915, 29.115, 29.2775, 29.94, 29.8325, 30.1225, 30.78, 30.955, 32.1525, 32.61, 33.8625, 33.91, 34.0225, 34.0275, 35.0025, 34.7975, 35.685, 37.2425, 36.3, 37.3325, 36.835, 36.965, 37.485, 37.825, 38.5825, 37.9925, 38.04, 38.9375, 39.1775, 39.9475, 40.555, 40.6825, 40.825, 41.0175, 40.475, 40.6125, 40.6875, 41.365, 41.285, 41.9975, 41.37, 40.8925, 41.4475, 42.295, 42.02, 43.2, 42.1325, 42.99, 42.1325, 42.835, 43.375, 43.4975, 42.82, 43.83, 43.24, 44.0125, 44.2025, 43.2975, 43.94, 44.075, 43.7975, 43.9175, 43.6575, 44.36, 43.805, 43.4575, 43.955, 44.1525, 44.465, 43.825, 44.495, 44.8175, 44.29, 44.23, 44.6075, 44.42, 44.82, 44.255, 44.1975, 43.5875, 44.64, 44.5125, 44.2575, 45.055, 44.43, 45.3825, 44.9625, 45.175, 44.535, 45.115]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.507, Test loss: 0.495, Test accuracy: 80.39
Average accuracy final 10 rounds: 80.67277777777777
Average global accuracy final 10 rounds: 80.67277777777777
2470.4859805107117
[]
[29.783333333333335, 40.35, 50.37222222222222, 51.227777777777774, 59.455555555555556, 59.71666666666667, 66.93888888888888, 67.17222222222222, 67.82222222222222, 67.22777777777777, 68.50555555555556, 71.02777777777777, 70.17222222222222, 72.0, 72.44444444444444, 73.97222222222223, 74.06666666666666, 74.46666666666667, 74.25555555555556, 75.12222222222222, 75.11111111111111, 75.35555555555555, 76.34444444444445, 76.68888888888888, 77.12777777777778, 76.78333333333333, 76.36666666666666, 75.96666666666667, 76.97222222222223, 77.01666666666667, 77.77222222222223, 77.87777777777778, 77.39444444444445, 77.8, 77.55555555555556, 78.14444444444445, 78.17777777777778, 77.80555555555556, 78.2611111111111, 78.41666666666667, 78.58333333333333, 78.52222222222223, 78.9, 78.31111111111112, 78.54444444444445, 78.56666666666666, 78.37777777777778, 79.40555555555555, 79.31666666666666, 79.22222222222223, 78.79444444444445, 78.91666666666667, 78.57222222222222, 79.07777777777778, 78.56666666666666, 78.94444444444444, 78.63888888888889, 78.64444444444445, 78.74444444444444, 78.55555555555556, 79.16666666666667, 78.70555555555555, 79.27777777777777, 79.03888888888889, 79.08333333333333, 79.33333333333333, 79.64444444444445, 79.71666666666667, 79.7611111111111, 79.59444444444445, 79.53888888888889, 79.61111111111111, 79.7388888888889, 79.92222222222222, 79.52222222222223, 79.45, 79.67222222222222, 79.86111111111111, 79.69444444444444, 79.88333333333334, 80.11111111111111, 81.15, 81.1, 80.75, 80.97777777777777, 80.75, 80.59444444444445, 80.51666666666667, 80.48333333333333, 80.34444444444445, 80.10555555555555, 80.33333333333333, 80.79444444444445, 80.84444444444445, 80.82222222222222, 80.63333333333334, 80.58333333333333, 80.63888888888889, 81.15555555555555, 80.81666666666666, 80.38888888888889]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.136, Test loss: 0.996, Test accuracy: 69.29
Average accuracy final 10 rounds: 63.56166666666667
4318.522317647934
[6.889089345932007, 13.379938125610352, 19.871089935302734, 26.291999101638794, 32.7468945980072, 39.08646273612976, 45.45636463165283, 51.832624197006226, 57.79362392425537, 63.6944363117218, 69.61005139350891, 75.51584267616272, 81.54111886024475, 87.81118893623352, 94.10471057891846, 100.40024328231812, 106.59066343307495, 112.45386481285095, 118.36789727210999, 124.20775365829468, 130.05597376823425, 136.0465109348297, 141.9956169128418, 147.8767364025116, 153.8174431324005, 159.69649410247803, 165.6033639907837, 171.60775351524353, 177.82947635650635, 184.1214063167572, 190.5224530696869, 196.82574152946472, 203.037442445755, 209.33034992218018, 215.6127827167511, 222.0885512828827, 228.42728519439697, 234.73612880706787, 241.5247461795807, 247.7915849685669, 253.67447090148926, 259.9855263233185, 265.89516949653625, 271.82261538505554, 277.7512798309326, 283.74527406692505, 289.67381262779236, 295.61896300315857, 301.5474660396576, 307.5898985862732, 313.7143437862396, 319.6212785243988, 325.79940390586853, 332.28428387641907, 338.83065128326416, 344.81204080581665, 350.66956305503845, 356.57138562202454, 362.5058150291443, 368.5477213859558, 374.655136346817, 380.66278553009033, 386.6537821292877, 392.6176154613495, 398.68842697143555, 404.74337244033813, 411.10759687423706, 417.58724880218506, 423.8863413333893, 430.24070835113525, 436.33792185783386, 442.5197448730469, 448.65787625312805, 454.7097351551056, 460.89683651924133, 467.38024950027466, 473.84700107574463, 480.20548939704895, 486.2364785671234, 492.2860474586487, 498.2568094730377, 504.22540855407715, 510.4714992046356, 516.459629535675, 522.5456929206848, 528.5228555202484, 534.6221752166748, 540.60724568367, 546.5691208839417, 552.5870201587677, 558.653520822525, 564.6638054847717, 570.6421239376068, 576.6745908260345, 582.6424517631531, 588.6880135536194, 595.2505388259888, 601.66251039505, 607.996682882309, 614.295129776001, 617.9010043144226]
[21.988888888888887, 23.005555555555556, 33.31111111111111, 36.98888888888889, 37.077777777777776, 31.122222222222224, 44.794444444444444, 37.18888888888889, 48.85, 45.73888888888889, 50.138888888888886, 45.46111111111111, 48.85, 49.083333333333336, 50.51111111111111, 47.083333333333336, 49.11666666666667, 53.111111111111114, 52.62222222222222, 49.705555555555556, 58.21666666666667, 59.09444444444444, 54.22222222222222, 57.85, 57.15555555555556, 62.36666666666667, 51.111111111111114, 51.21666666666667, 62.544444444444444, 63.044444444444444, 61.71111111111111, 56.27777777777778, 54.81111111111111, 54.80555555555556, 57.59444444444444, 58.65, 50.13333333333333, 49.78888888888889, 61.96666666666667, 62.15, 62.62777777777778, 56.8, 60.388888888888886, 62.05, 56.81111111111111, 54.98888888888889, 62.727777777777774, 54.15, 57.611111111111114, 56.666666666666664, 60.94444444444444, 63.75, 57.516666666666666, 61.88333333333333, 62.47222222222222, 54.227777777777774, 62.266666666666666, 66.04444444444445, 58.23888888888889, 61.66111111111111, 59.65555555555556, 62.13333333333333, 62.96666666666667, 58.86666666666667, 67.55555555555556, 61.69444444444444, 54.88333333333333, 59.27777777777778, 61.016666666666666, 61.44444444444444, 62.33888888888889, 62.8, 63.25555555555555, 61.05, 61.68888888888889, 63.97222222222222, 58.02777777777778, 59.83888888888889, 67.21111111111111, 63.28888888888889, 59.17777777777778, 59.30555555555556, 65.70555555555555, 58.48888888888889, 66.52777777777777, 66.81111111111112, 64.34444444444445, 66.81666666666666, 64.89444444444445, 54.56111111111111, 65.85555555555555, 64.61111111111111, 62.977777777777774, 66.35555555555555, 64.86111111111111, 58.84444444444444, 57.85, 65.29444444444445, 64.5111111111111, 64.45555555555555, 69.29444444444445]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 51070 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 56394 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.048, Test loss: 0.898, Test accuracy: 82.44
Final Round, Global train loss: 0.048, Global test loss: 2.497, Global test accuracy: 15.87
Average accuracy final 10 rounds: 81.95083333333332 

Average global accuracy final 10 rounds: 13.294166666666664 

2237.1260335445404
[1.9257347583770752, 3.8514695167541504, 5.399789571762085, 6.9481096267700195, 8.596626281738281, 10.245142936706543, 11.870348453521729, 13.495553970336914, 15.154125690460205, 16.812697410583496, 18.50194811820984, 20.19119882583618, 21.833808660507202, 23.476418495178223, 25.137561321258545, 26.798704147338867, 28.4065945148468, 30.014484882354736, 31.628566026687622, 33.24264717102051, 34.82912731170654, 36.41560745239258, 38.05254316329956, 39.68947887420654, 41.282389879226685, 42.875300884246826, 44.469475746154785, 46.063650608062744, 47.692304611206055, 49.320958614349365, 50.93431901931763, 52.54767942428589, 54.18925905227661, 55.830838680267334, 57.41578698158264, 59.00073528289795, 60.56119465827942, 62.12165403366089, 63.71519374847412, 65.30873346328735, 66.92211866378784, 68.53550386428833, 70.10703015327454, 71.67855644226074, 73.24349689483643, 74.80843734741211, 76.26027297973633, 77.71210861206055, 79.16790509223938, 80.62370157241821, 82.09410619735718, 83.56451082229614, 85.0282084941864, 86.49190616607666, 87.9832706451416, 89.47463512420654, 90.9139986038208, 92.35336208343506, 93.80578565597534, 95.25820922851562, 96.73054528236389, 98.20288133621216, 99.6239070892334, 101.04493284225464, 102.48804664611816, 103.93116044998169, 105.40120577812195, 106.8712511062622, 108.34481143951416, 109.81837177276611, 111.2409462928772, 112.66352081298828, 114.098797082901, 115.53407335281372, 117.01771569252014, 118.50135803222656, 119.96499133110046, 121.42862462997437, 122.8809266090393, 124.33322858810425, 125.78838658332825, 127.24354457855225, 128.70283031463623, 130.16211605072021, 131.63138961791992, 133.10066318511963, 134.5407977104187, 135.98093223571777, 137.44453024864197, 138.90812826156616, 140.50158286094666, 142.09503746032715, 143.54506850242615, 144.99509954452515, 146.46613121032715, 147.93716287612915, 149.5235676765442, 151.10997247695923, 152.6839861869812, 154.25799989700317, 155.8249912261963, 157.3919825553894, 158.9674825668335, 160.5429825782776, 162.13970375061035, 163.73642492294312, 165.3066601753235, 166.87689542770386, 172.60875129699707, 178.34060716629028, 179.7999665737152, 181.25932598114014, 182.74484872817993, 184.23037147521973, 185.66117405891418, 187.09197664260864, 188.5869722366333, 190.08196783065796, 191.56374979019165, 193.04553174972534, 194.5009582042694, 195.95638465881348, 197.3896028995514, 198.8228211402893, 200.27343320846558, 201.72404527664185, 203.19583821296692, 204.667631149292, 206.14026355743408, 207.61289596557617, 209.0271508693695, 210.44140577316284, 211.87623405456543, 213.31106233596802, 214.774311542511, 216.23756074905396, 217.719895362854, 219.20222997665405, 220.66086339950562, 222.11949682235718, 223.55100083351135, 224.98250484466553, 226.44027090072632, 227.8980369567871, 229.36554646492004, 230.83305597305298, 232.26692485809326, 233.70079374313354, 235.21958947181702, 236.7383852005005, 238.2053782939911, 239.6723713874817, 241.12073373794556, 242.56909608840942, 244.02102327346802, 245.4729504585266, 246.91773986816406, 248.3625292778015, 249.81997108459473, 251.27741289138794, 252.72008347511292, 254.1627540588379, 255.6162428855896, 257.0697317123413, 258.5297849178314, 259.98983812332153, 261.4280424118042, 262.86624670028687, 264.3212308883667, 265.77621507644653, 267.22916769981384, 268.68212032318115, 270.16068935394287, 271.6392583847046, 273.08868312835693, 274.5381078720093, 275.9757618904114, 277.4134159088135, 278.8659303188324, 280.3184447288513, 281.72966265678406, 283.1408805847168, 284.6721079349518, 286.20333528518677, 287.7275366783142, 289.25173807144165, 290.7106468677521, 292.1695556640625, 293.69757747650146, 295.22559928894043, 296.63483452796936, 298.0440697669983, 299.6558289527893, 301.2675881385803, 302.8032190799713, 304.3388500213623, 305.74564576148987, 307.15244150161743, 308.7129635810852, 310.273485660553, 312.9373400211334, 315.60119438171387]
[23.916666666666668, 23.916666666666668, 39.63333333333333, 39.63333333333333, 50.766666666666666, 50.766666666666666, 57.34166666666667, 57.34166666666667, 60.825, 60.825, 67.58333333333333, 67.58333333333333, 67.925, 67.925, 71.23333333333333, 71.23333333333333, 73.6, 73.6, 74.35833333333333, 74.35833333333333, 75.03333333333333, 75.03333333333333, 74.90833333333333, 74.90833333333333, 74.825, 74.825, 75.775, 75.775, 76.925, 76.925, 77.25, 77.25, 77.65, 77.65, 77.58333333333333, 77.58333333333333, 77.5, 77.5, 77.65, 77.65, 77.70833333333333, 77.70833333333333, 78.10833333333333, 78.10833333333333, 78.93333333333334, 78.93333333333334, 78.95833333333333, 78.95833333333333, 79.175, 79.175, 78.71666666666667, 78.71666666666667, 79.20833333333333, 79.20833333333333, 79.75, 79.75, 79.73333333333333, 79.73333333333333, 79.95, 79.95, 79.95833333333333, 79.95833333333333, 79.88333333333334, 79.88333333333334, 80.00833333333334, 80.00833333333334, 80.25833333333334, 80.25833333333334, 79.8, 79.8, 79.24166666666666, 79.24166666666666, 79.675, 79.675, 80.33333333333333, 80.33333333333333, 80.3, 80.3, 80.2, 80.2, 80.475, 80.475, 80.15, 80.15, 80.35, 80.35, 80.29166666666667, 80.29166666666667, 80.34166666666667, 80.34166666666667, 80.55833333333334, 80.55833333333334, 80.78333333333333, 80.78333333333333, 80.81666666666666, 80.81666666666666, 80.81666666666666, 80.81666666666666, 81.26666666666667, 81.26666666666667, 81.60833333333333, 81.60833333333333, 81.225, 81.225, 81.525, 81.525, 81.80833333333334, 81.80833333333334, 82.18333333333334, 82.18333333333334, 82.16666666666667, 82.16666666666667, 82.44166666666666, 82.44166666666666, 81.975, 81.975, 81.925, 81.925, 81.9, 81.9, 81.68333333333334, 81.68333333333334, 81.85, 81.85, 81.9, 81.9, 81.70833333333333, 81.70833333333333, 81.6, 81.6, 81.49166666666666, 81.49166666666666, 81.625, 81.625, 81.16666666666667, 81.16666666666667, 80.86666666666666, 80.86666666666666, 81.35, 81.35, 81.16666666666667, 81.16666666666667, 81.18333333333334, 81.18333333333334, 81.70833333333333, 81.70833333333333, 81.875, 81.875, 81.58333333333333, 81.58333333333333, 81.85, 81.85, 81.71666666666667, 81.71666666666667, 81.7, 81.7, 81.825, 81.825, 81.96666666666667, 81.96666666666667, 82.05, 82.05, 82.18333333333334, 82.18333333333334, 81.85, 81.85, 82.075, 82.075, 81.875, 81.875, 82.025, 82.025, 81.58333333333333, 81.58333333333333, 81.79166666666667, 81.79166666666667, 82.0, 82.0, 81.88333333333334, 81.88333333333334, 82.1, 82.1, 82.35833333333333, 82.35833333333333, 81.94166666666666, 81.94166666666666, 81.875, 81.875, 81.94166666666666, 81.94166666666666, 81.93333333333334, 81.93333333333334, 81.925, 81.925, 81.75, 81.75, 81.73333333333333, 81.73333333333333, 81.95, 81.95, 82.44166666666666, 82.44166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.317, Test loss: 1.191, Test accuracy: 71.97
Final Round, Global train loss: 0.317, Global test loss: 3.211, Global test accuracy: 14.22
Average accuracy final 10 rounds: 71.815 

Average global accuracy final 10 rounds: 15.577499999999999 

6652.343185901642
[5.206949472427368, 10.413898944854736, 15.399176359176636, 20.384453773498535, 25.558358907699585, 30.732264041900635, 35.741981506347656, 40.75169897079468, 45.61420035362244, 50.476701736450195, 55.3486909866333, 60.220680236816406, 65.18267607688904, 70.14467191696167, 75.07249855995178, 80.0003252029419, 84.88485598564148, 89.76938676834106, 94.5079493522644, 99.24651193618774, 104.01252675056458, 108.7785415649414, 113.77869629859924, 118.77885103225708, 123.60765719413757, 128.43646335601807, 133.2918872833252, 138.14731121063232, 142.97089672088623, 147.79448223114014, 152.56023454666138, 157.32598686218262, 162.1428792476654, 166.9597716331482, 171.86259150505066, 176.76541137695312, 181.65081024169922, 186.5362091064453, 191.42760586738586, 196.31900262832642, 201.39650535583496, 206.4740080833435, 211.48750758171082, 216.50100708007812, 221.71723747253418, 226.93346786499023, 232.04008865356445, 237.14670944213867, 242.1528754234314, 247.15904140472412, 252.14587473869324, 257.13270807266235, 262.2162537574768, 267.29979944229126, 272.3068935871124, 277.3139877319336, 282.3581783771515, 287.4023690223694, 292.05458974838257, 296.70681047439575, 301.54318714141846, 306.37956380844116, 311.47803711891174, 316.5765104293823, 321.4637339115143, 326.35095739364624, 331.10910177230835, 335.86724615097046, 340.62966561317444, 345.3920850753784, 350.16734790802, 354.9426107406616, 360.10403633117676, 365.2654619216919, 370.3884434700012, 375.51142501831055, 380.5595769882202, 385.6077289581299, 390.6027035713196, 395.5976781845093, 400.4103653430939, 405.22305250167847, 410.0272991657257, 414.83154582977295, 419.6309030056, 424.430260181427, 429.26396322250366, 434.0976662635803, 438.89133167266846, 443.6849970817566, 448.46669816970825, 453.2483992576599, 458.0424041748047, 462.83640909194946, 467.6045820713043, 472.3727550506592, 477.13354301452637, 481.89433097839355, 486.7198419570923, 491.545352935791, 496.3397674560547, 501.13418197631836, 505.9273157119751, 510.72044944763184, 515.5023968219757, 520.2843441963196, 525.0723929405212, 529.8604416847229, 534.6932964324951, 539.5261511802673, 544.3367826938629, 549.1474142074585, 553.9594223499298, 558.7714304924011, 563.5392892360687, 568.3071479797363, 573.0835537910461, 577.859959602356, 582.6853182315826, 587.5106768608093, 592.3050093650818, 597.0993418693542, 601.8904409408569, 606.6815400123596, 611.4745564460754, 616.2675728797913, 621.0552341938019, 625.8428955078125, 630.6419410705566, 635.4409866333008, 640.2486510276794, 645.0563154220581, 649.850670337677, 654.6450252532959, 659.4610025882721, 664.2769799232483, 669.091899394989, 673.9068188667297, 678.7089493274689, 683.511079788208, 688.294290304184, 693.0775008201599, 697.8669764995575, 702.6564521789551, 707.4688124656677, 712.2811727523804, 717.0843765735626, 721.8875803947449, 726.6951060295105, 731.5026316642761, 736.2509694099426, 740.9993071556091, 745.7878239154816, 750.576340675354, 755.3573040962219, 760.1382675170898, 764.9437754154205, 769.7492833137512, 774.5702655315399, 779.3912477493286, 784.1494977474213, 788.9077477455139, 793.6737501621246, 798.4397525787354, 803.213353395462, 807.9869542121887, 812.7662436962128, 817.5455331802368, 822.3316271305084, 827.11772108078, 831.8942642211914, 836.6708073616028, 841.4477517604828, 846.2246961593628, 850.9982707500458, 855.7718453407288, 860.5481142997742, 865.3243832588196, 870.087753534317, 874.8511238098145, 879.6254065036774, 884.3996891975403, 889.1655902862549, 893.9314913749695, 898.7296853065491, 903.5278792381287, 908.3284559249878, 913.1290326118469, 917.9316971302032, 922.7343616485596, 927.5432524681091, 932.3521432876587, 937.1363778114319, 941.9206123352051, 946.732218503952, 951.543824672699, 956.3102865219116, 961.0767483711243, 965.8872895240784, 970.6978306770325, 973.1100273132324, 975.5222239494324]
[17.48, 17.48, 25.455, 25.455, 28.515, 28.515, 30.835, 30.835, 36.8325, 36.8325, 39.8525, 39.8525, 44.36, 44.36, 47.215, 47.215, 50.4075, 50.4075, 51.355, 51.355, 52.6475, 52.6475, 53.175, 53.175, 53.445, 53.445, 54.2, 54.2, 57.78, 57.78, 59.165, 59.165, 59.9025, 59.9025, 60.4, 60.4, 61.07, 61.07, 62.48, 62.48, 63.495, 63.495, 64.29, 64.29, 64.36, 64.36, 65.4775, 65.4775, 65.775, 65.775, 66.1125, 66.1125, 66.355, 66.355, 66.39, 66.39, 66.5925, 66.5925, 66.735, 66.735, 67.0075, 67.0075, 67.33, 67.33, 67.375, 67.375, 67.485, 67.485, 67.7825, 67.7825, 67.955, 67.955, 68.1625, 68.1625, 68.0825, 68.0825, 68.255, 68.255, 68.5025, 68.5025, 68.4675, 68.4675, 68.4425, 68.4425, 68.7, 68.7, 68.62, 68.62, 68.65, 68.65, 68.89, 68.89, 69.0825, 69.0825, 69.4625, 69.4625, 69.33, 69.33, 69.63, 69.63, 69.6525, 69.6525, 69.8225, 69.8225, 69.9575, 69.9575, 70.0875, 70.0875, 70.0875, 70.0875, 70.07, 70.07, 69.9225, 69.9225, 70.13, 70.13, 70.2725, 70.2725, 70.3625, 70.3625, 70.48, 70.48, 70.5175, 70.5175, 70.5975, 70.5975, 70.43, 70.43, 70.3475, 70.3475, 70.625, 70.625, 70.5525, 70.5525, 70.58, 70.58, 70.7075, 70.7075, 70.6675, 70.6675, 70.7, 70.7, 70.8725, 70.8725, 70.41, 70.41, 70.4425, 70.4425, 70.57, 70.57, 70.91, 70.91, 71.2925, 71.2925, 71.265, 71.265, 71.3475, 71.3475, 71.1925, 71.1925, 71.34, 71.34, 71.2775, 71.2775, 71.2925, 71.2925, 71.52, 71.52, 71.4325, 71.4325, 71.47, 71.47, 71.47, 71.47, 71.5925, 71.5925, 71.7875, 71.7875, 71.835, 71.835, 71.565, 71.565, 71.54, 71.54, 71.94, 71.94, 71.955, 71.955, 71.805, 71.805, 71.96, 71.96, 71.9075, 71.9075, 71.875, 71.875, 71.785, 71.785, 71.8175, 71.8175, 71.965, 71.965]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.396, Test loss: 0.752, Test accuracy: 76.48
Average accuracy final 10 rounds: 75.8665 

4643.868995189667
[4.474827766418457, 8.949655532836914, 13.359313726425171, 17.768971920013428, 22.175304412841797, 26.581636905670166, 30.99447512626648, 35.40731334686279, 39.83250713348389, 44.25770092010498, 48.68129205703735, 53.10488319396973, 57.103360176086426, 61.101837158203125, 65.10451769828796, 69.1071982383728, 73.12974119186401, 77.15228414535522, 81.19462442398071, 85.2369647026062, 89.26997494697571, 93.30298519134521, 97.32497692108154, 101.34696865081787, 105.368479013443, 109.38998937606812, 113.40540146827698, 117.42081356048584, 121.4017174243927, 125.38262128829956, 129.3714144229889, 133.36020755767822, 137.36423254013062, 141.368257522583, 145.34458327293396, 149.3209090232849, 153.30506443977356, 157.2892198562622, 161.32786345481873, 165.36650705337524, 169.4114580154419, 173.45640897750854, 177.49431943893433, 181.5322299003601, 185.5340449810028, 189.5358600616455, 193.55797481536865, 197.5800895690918, 201.58593273162842, 205.59177589416504, 209.61758065223694, 213.64338541030884, 217.69126892089844, 221.73915243148804, 225.6753284931183, 229.61150455474854, 233.6131556034088, 237.6148066520691, 241.58916020393372, 245.56351375579834, 249.5188934803009, 253.47427320480347, 257.5002360343933, 261.52619886398315, 265.5297944545746, 269.533390045166, 273.5689232349396, 277.60445642471313, 281.6141710281372, 285.6238856315613, 289.65786385536194, 293.6918420791626, 297.6770360469818, 301.662230014801, 305.63224172592163, 309.60225343704224, 313.60370111465454, 317.60514879226685, 321.5339765548706, 325.46280431747437, 329.4073247909546, 333.3518452644348, 337.35739612579346, 341.3629469871521, 345.3761706352234, 349.3893942832947, 353.4025378227234, 357.4156813621521, 361.4119324684143, 365.4081835746765, 369.43982696533203, 373.47147035598755, 377.4737377166748, 381.47600507736206, 385.51124835014343, 389.5464916229248, 393.55568265914917, 397.56487369537354, 401.5316939353943, 405.49851417541504, 409.4829580783844, 413.46740198135376, 417.4132270812988, 421.3590521812439, 425.31868600845337, 429.27831983566284, 433.24209856987, 437.20587730407715, 441.18972539901733, 445.1735734939575, 449.187504529953, 453.2014355659485, 457.1992597579956, 461.1970839500427, 465.17859411239624, 469.16010427474976, 473.1302001476288, 477.1002960205078, 481.08807015419006, 485.0758442878723, 489.11235785484314, 493.14887142181396, 497.15084958076477, 501.1528277397156, 505.08910846710205, 509.0253891944885, 513.0045411586761, 516.9836931228638, 520.9791853427887, 524.9746775627136, 528.9393746852875, 532.9040718078613, 536.871821641922, 540.8395714759827, 544.8281192779541, 548.8166670799255, 552.8387115001678, 556.8607559204102, 560.8546805381775, 564.8486051559448, 568.8197290897369, 572.790853023529, 576.7674434185028, 580.7440338134766, 584.7594137191772, 588.7747936248779, 592.7629754543304, 596.751157283783, 600.7003111839294, 604.6494650840759, 608.6132826805115, 612.577100276947, 616.5616323947906, 620.5461645126343, 624.5409154891968, 628.5356664657593, 632.5474035739899, 636.5591406822205, 640.5500736236572, 644.541006565094, 648.5225894451141, 652.5041723251343, 656.4699625968933, 660.4357528686523, 664.4412322044373, 668.4467115402222, 672.4490838050842, 676.4514560699463, 680.3881494998932, 684.3248429298401, 688.2671473026276, 692.209451675415, 696.1949682235718, 700.1804847717285, 704.1418528556824, 708.1032209396362, 712.0499622821808, 715.9967036247253, 719.9636046886444, 723.9305057525635, 727.940761089325, 731.9510164260864, 735.950624704361, 739.9502329826355, 743.9485599994659, 747.9468870162964, 751.9269440174103, 755.9070010185242, 759.8712801933289, 763.8355593681335, 767.8097031116486, 771.7838468551636, 775.7821755409241, 779.7805042266846, 783.7376370429993, 787.694769859314, 791.6392605304718, 795.5837512016296, 799.5954008102417, 803.6070504188538, 805.5177428722382, 807.4284353256226]
[12.2575, 12.2575, 18.2225, 18.2225, 24.045, 24.045, 31.9725, 31.9725, 35.7525, 35.7525, 40.0275, 40.0275, 45.08, 45.08, 48.42, 48.42, 52.33, 52.33, 56.3675, 56.3675, 58.4975, 58.4975, 60.105, 60.105, 61.405, 61.405, 62.7475, 62.7475, 63.7925, 63.7925, 64.815, 64.815, 65.44, 65.44, 66.2125, 66.2125, 66.6075, 66.6075, 65.59, 65.59, 67.18, 67.18, 68.13, 68.13, 68.3375, 68.3375, 68.635, 68.635, 69.1775, 69.1775, 69.4875, 69.4875, 69.685, 69.685, 70.33, 70.33, 70.825, 70.825, 71.1625, 71.1625, 71.78, 71.78, 72.215, 72.215, 72.35, 72.35, 72.23, 72.23, 72.4625, 72.4625, 72.54, 72.54, 72.525, 72.525, 72.735, 72.735, 73.2525, 73.2525, 73.5175, 73.5175, 73.48, 73.48, 73.1675, 73.1675, 73.3325, 73.3325, 73.5225, 73.5225, 73.79, 73.79, 73.8375, 73.8375, 73.5475, 73.5475, 73.8425, 73.8425, 73.81, 73.81, 74.35, 74.35, 74.395, 74.395, 74.175, 74.175, 74.53, 74.53, 74.2725, 74.2725, 74.8975, 74.8975, 74.3825, 74.3825, 74.3875, 74.3875, 74.7075, 74.7075, 74.7925, 74.7925, 74.84, 74.84, 74.595, 74.595, 74.8, 74.8, 75.1, 75.1, 75.0875, 75.0875, 74.875, 74.875, 75.045, 75.045, 75.0725, 75.0725, 75.2475, 75.2475, 74.7725, 74.7725, 74.83, 74.83, 74.8525, 74.8525, 75.6475, 75.6475, 75.46, 75.46, 75.2275, 75.2275, 75.2575, 75.2575, 75.13, 75.13, 75.095, 75.095, 74.925, 74.925, 75.705, 75.705, 75.69, 75.69, 75.4375, 75.4375, 75.505, 75.505, 75.6925, 75.6925, 75.675, 75.675, 75.7625, 75.7625, 75.48, 75.48, 75.0825, 75.0825, 75.7275, 75.7275, 75.7125, 75.7125, 75.445, 75.445, 75.49, 75.49, 75.8025, 75.8025, 75.57, 75.57, 76.235, 76.235, 75.615, 75.615, 76.1825, 76.1825, 76.035, 76.035, 75.8625, 75.8625, 75.7075, 75.7075, 76.165, 76.165, 76.4825, 76.4825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.306, Test loss: 0.838, Test accuracy: 76.53
Average accuracy final 10 rounds: 76.674 

5222.922810554504
[4.85055947303772, 9.70111894607544, 14.493492603302002, 19.285866260528564, 24.08557438850403, 28.885282516479492, 33.682504653930664, 38.479726791381836, 43.2538058757782, 48.02788496017456, 52.83842206001282, 57.648959159851074, 62.49572730064392, 67.34249544143677, 72.1919276714325, 77.04135990142822, 81.89384460449219, 86.74632930755615, 91.57309651374817, 96.39986371994019, 101.2474627494812, 106.09506177902222, 110.92895030975342, 115.76283884048462, 120.55913519859314, 125.35543155670166, 130.17822909355164, 135.0010266304016, 139.83089995384216, 144.66077327728271, 149.49689650535583, 154.33301973342896, 159.14685463905334, 163.96068954467773, 168.77234387397766, 173.5839982032776, 178.7618625164032, 183.9397268295288, 189.0700991153717, 194.2004714012146, 199.35057830810547, 204.50068521499634, 209.68380331993103, 214.86692142486572, 220.02922439575195, 225.19152736663818, 230.32964992523193, 235.46777248382568, 240.64759135246277, 245.82741022109985, 251.01208329200745, 256.19675636291504, 261.34810757637024, 266.49945878982544, 271.75900077819824, 277.01854276657104, 282.1785228252411, 287.33850288391113, 292.6119112968445, 297.88531970977783, 302.96417570114136, 308.0430316925049, 313.193320274353, 318.3436088562012, 323.55235981941223, 328.7611107826233, 333.9827461242676, 339.20438146591187, 344.4475848674774, 349.69078826904297, 354.902161359787, 360.113534450531, 365.3066346645355, 370.49973487854004, 375.6707820892334, 380.84182929992676, 386.0768768787384, 391.31192445755005, 396.49197697639465, 401.67202949523926, 406.8422529697418, 412.0124764442444, 417.1897542476654, 422.3670320510864, 427.5206537246704, 432.6742753982544, 437.8195035457611, 442.9647316932678, 448.1005370616913, 453.23634243011475, 458.3868749141693, 463.5374073982239, 468.7676351070404, 473.99786281585693, 479.2307391166687, 484.46361541748047, 489.67778158187866, 494.89194774627686, 500.1318256855011, 505.37170362472534, 510.5938594341278, 515.8160152435303, 521.0212733745575, 526.2265315055847, 530.9281349182129, 535.6297383308411, 542.2421262264252, 548.8545141220093, 554.0256249904633, 559.1967358589172, 564.3699407577515, 569.5431456565857, 574.7374374866486, 579.9317293167114, 585.1722111701965, 590.4126930236816, 595.6588203907013, 600.904947757721, 606.0768754482269, 611.2488031387329, 616.455178976059, 621.661554813385, 626.9351286888123, 632.2087025642395, 637.3914811611176, 642.5742597579956, 647.7836797237396, 652.9930996894836, 658.2068748474121, 663.4206500053406, 668.6517758369446, 673.8829016685486, 679.0732634067535, 684.2636251449585, 689.4891362190247, 694.7146472930908, 699.9454584121704, 705.17626953125, 710.1610231399536, 715.1457767486572, 720.0401709079742, 724.9345650672913, 729.7930750846863, 734.6515851020813, 739.3918154239655, 744.1320457458496, 748.3938665390015, 752.6556873321533, 757.015186548233, 761.3746857643127, 765.672801733017, 769.9709177017212, 774.2768585681915, 778.5827994346619, 782.8658194541931, 787.1488394737244, 791.4540274143219, 795.7592153549194, 800.0679948329926, 804.3767743110657, 808.6728904247284, 812.9690065383911, 817.2772207260132, 821.5854349136353, 825.8576350212097, 830.1298351287842, 834.5772342681885, 839.0246334075928, 843.3268666267395, 847.6290998458862, 851.9701368808746, 856.311173915863, 860.6174132823944, 864.9236526489258, 869.2402498722076, 873.5568470954895, 877.8982427120209, 882.2396383285522, 886.517083644867, 890.7945289611816, 895.0877387523651, 899.3809485435486, 903.728524684906, 908.0761008262634, 912.392637014389, 916.7091732025146, 921.0167238712311, 925.3242745399475, 929.6455283164978, 933.9667820930481, 938.3885226249695, 942.8102631568909, 947.1631455421448, 951.5160279273987, 955.8602566719055, 960.2044854164124, 964.5209624767303, 968.8374395370483, 973.1879858970642, 977.5385322570801, 979.5178127288818, 981.4970932006836]
[12.33, 12.33, 16.18, 16.18, 19.2925, 19.2925, 24.4175, 24.4175, 31.2525, 31.2525, 37.98, 37.98, 45.1525, 45.1525, 48.8075, 48.8075, 52.2275, 52.2275, 56.53, 56.53, 60.225, 60.225, 63.4325, 63.4325, 65.56, 65.56, 66.7, 66.7, 67.63, 67.63, 68.3225, 68.3225, 69.5075, 69.5075, 70.5925, 70.5925, 71.1275, 71.1275, 71.0575, 71.0575, 71.4725, 71.4725, 71.8325, 71.8325, 72.205, 72.205, 73.1075, 73.1075, 73.265, 73.265, 73.1075, 73.1075, 73.6975, 73.6975, 73.845, 73.845, 74.145, 74.145, 73.9625, 73.9625, 74.34, 74.34, 74.4425, 74.4425, 74.39, 74.39, 74.725, 74.725, 74.5325, 74.5325, 74.5925, 74.5925, 75.035, 75.035, 75.09, 75.09, 75.3225, 75.3225, 75.1975, 75.1975, 75.44, 75.44, 75.59, 75.59, 75.115, 75.115, 74.8275, 74.8275, 75.3675, 75.3675, 75.795, 75.795, 75.455, 75.455, 75.51, 75.51, 75.855, 75.855, 75.76, 75.76, 75.8025, 75.8025, 76.0375, 76.0375, 76.06, 76.06, 75.935, 75.935, 76.0775, 76.0775, 75.5925, 75.5925, 75.98, 75.98, 76.1575, 76.1575, 76.1075, 76.1075, 75.825, 75.825, 76.1075, 76.1075, 76.055, 76.055, 75.8575, 75.8575, 75.9325, 75.9325, 75.8625, 75.8625, 76.1075, 76.1075, 76.1425, 76.1425, 76.35, 76.35, 76.1575, 76.1575, 76.12, 76.12, 76.3175, 76.3175, 76.09, 76.09, 76.2875, 76.2875, 75.885, 75.885, 75.85, 75.85, 76.2425, 76.2425, 76.2, 76.2, 76.3025, 76.3025, 76.43, 76.43, 76.5925, 76.5925, 76.49, 76.49, 76.555, 76.555, 76.3675, 76.3675, 76.7975, 76.7975, 76.495, 76.495, 76.32, 76.32, 76.5075, 76.5075, 76.4725, 76.4725, 76.47, 76.47, 76.3275, 76.3275, 76.7425, 76.7425, 76.5875, 76.5875, 76.9325, 76.9325, 76.98, 76.98, 76.085, 76.085, 76.7575, 76.7575, 76.835, 76.835, 76.55, 76.55, 76.75, 76.75, 76.52, 76.52, 76.5325, 76.5325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.038, Test loss: 0.991, Test accuracy: 81.71
Average accuracy final 10 rounds: 81.43999999999998 

1520.711226940155
[1.810398817062378, 3.620797634124756, 5.167631149291992, 6.7144646644592285, 8.247933626174927, 9.781402587890625, 11.316368103027344, 12.851333618164062, 14.40787386894226, 15.964414119720459, 17.51363778114319, 19.062861442565918, 20.5873966217041, 22.111931800842285, 23.638489484786987, 25.16504716873169, 26.720673084259033, 28.276298999786377, 29.817112922668457, 31.357926845550537, 32.884809732437134, 34.41169261932373, 35.961631298065186, 37.51156997680664, 39.07858943939209, 40.64560890197754, 42.187907457351685, 43.73020601272583, 45.25007081031799, 46.769935607910156, 48.05704069137573, 49.34414577484131, 50.62887120246887, 51.913596630096436, 53.20301580429077, 54.49243497848511, 55.759114027023315, 57.02579307556152, 58.30340003967285, 59.58100700378418, 60.873884201049805, 62.16676139831543, 63.468284606933594, 64.76980781555176, 66.04031372070312, 67.31081962585449, 68.57219815254211, 69.83357667922974, 71.10696291923523, 72.38034915924072, 73.672842502594, 74.96533584594727, 76.25343179702759, 77.54152774810791, 78.80396699905396, 80.06640625, 81.33394694328308, 82.60148763656616, 83.88506865501404, 85.16864967346191, 86.46220254898071, 87.75575542449951, 89.03436279296875, 90.31297016143799, 91.57812905311584, 92.8432879447937, 94.22225403785706, 95.60122013092041, 96.89339804649353, 98.18557596206665, 99.47727060317993, 100.76896524429321, 102.02677941322327, 103.28459358215332, 104.58702492713928, 105.88945627212524, 107.17020511627197, 108.4509539604187, 109.73267436027527, 111.01439476013184, 112.28320145606995, 113.55200815200806, 114.83208990097046, 116.11217164993286, 117.42656898498535, 118.74096632003784, 120.01163792610168, 121.28230953216553, 122.55594444274902, 123.82957935333252, 125.1112539768219, 126.39292860031128, 127.67582321166992, 128.95871782302856, 130.24372673034668, 131.5287356376648, 132.80320024490356, 134.07766485214233, 135.35140109062195, 136.62513732910156, 137.9179563522339, 139.2107753753662, 140.50214767456055, 141.79351997375488, 143.06622433662415, 144.3389286994934, 145.62029480934143, 146.90166091918945, 148.18385863304138, 149.4660563468933, 150.74232387542725, 152.01859140396118, 153.2913269996643, 154.56406259536743, 155.8464343547821, 157.12880611419678, 158.4340262413025, 159.7392463684082, 161.0270311832428, 162.3148159980774, 163.58083605766296, 164.84685611724854, 166.10794162750244, 167.36902713775635, 168.66140222549438, 169.95377731323242, 171.23901271820068, 172.52424812316895, 173.7988088130951, 175.07336950302124, 176.3299524784088, 177.5865354537964, 178.88224411010742, 180.17795276641846, 181.48476839065552, 182.79158401489258, 184.08096837997437, 185.37035274505615, 186.6390838623047, 187.90781497955322, 189.18977165222168, 190.47172832489014, 191.7607672214508, 193.04980611801147, 194.3319127559662, 195.6140193939209, 196.8827667236328, 198.15151405334473, 199.420170545578, 200.68882703781128, 201.97306632995605, 203.25730562210083, 204.5326235294342, 205.80794143676758, 207.07544469833374, 208.3429479598999, 209.6096739768982, 210.87639999389648, 212.1684286594391, 213.4604573249817, 214.75110030174255, 216.04174327850342, 217.30158162117004, 218.56141996383667, 219.8250789642334, 221.08873796463013, 222.3806653022766, 223.6725926399231, 224.96614003181458, 226.25968742370605, 227.53735423088074, 228.81502103805542, 230.08350563049316, 231.3519902229309, 232.61933159828186, 233.8866729736328, 235.18177366256714, 236.47687435150146, 237.7538001537323, 239.03072595596313, 240.314701795578, 241.59867763519287, 242.8773307800293, 244.15598392486572, 245.4374759197235, 246.7189679145813, 248.00069880485535, 249.2824296951294, 250.5725393295288, 251.86264896392822, 253.14734482765198, 254.43204069137573, 255.71014714241028, 256.9882535934448, 258.25642800331116, 259.5246024131775, 260.7944645881653, 262.0643267631531, 263.33837246894836, 264.61241817474365, 266.7022023200989, 268.7919864654541]
[23.283333333333335, 23.283333333333335, 35.5, 35.5, 39.05833333333333, 39.05833333333333, 41.541666666666664, 41.541666666666664, 42.391666666666666, 42.391666666666666, 54.916666666666664, 54.916666666666664, 57.8, 57.8, 65.89166666666667, 65.89166666666667, 67.74166666666666, 67.74166666666666, 69.59166666666667, 69.59166666666667, 71.2, 71.2, 71.29166666666667, 71.29166666666667, 72.09166666666667, 72.09166666666667, 74.20833333333333, 74.20833333333333, 75.49166666666666, 75.49166666666666, 75.34166666666667, 75.34166666666667, 76.075, 76.075, 76.56666666666666, 76.56666666666666, 76.86666666666666, 76.86666666666666, 76.31666666666666, 76.31666666666666, 77.4, 77.4, 77.125, 77.125, 78.26666666666667, 78.26666666666667, 77.51666666666667, 77.51666666666667, 77.54166666666667, 77.54166666666667, 78.41666666666667, 78.41666666666667, 78.63333333333334, 78.63333333333334, 78.975, 78.975, 77.98333333333333, 77.98333333333333, 78.2, 78.2, 78.78333333333333, 78.78333333333333, 79.15, 79.15, 79.03333333333333, 79.03333333333333, 79.41666666666667, 79.41666666666667, 79.1, 79.1, 79.0, 79.0, 79.40833333333333, 79.40833333333333, 79.3, 79.3, 79.55, 79.55, 79.61666666666666, 79.61666666666666, 79.64166666666667, 79.64166666666667, 80.03333333333333, 80.03333333333333, 80.4, 80.4, 80.625, 80.625, 80.58333333333333, 80.58333333333333, 80.375, 80.375, 80.43333333333334, 80.43333333333334, 79.68333333333334, 79.68333333333334, 80.01666666666667, 80.01666666666667, 79.99166666666666, 79.99166666666666, 80.28333333333333, 80.28333333333333, 80.49166666666666, 80.49166666666666, 80.68333333333334, 80.68333333333334, 80.75833333333334, 80.75833333333334, 80.4, 80.4, 80.325, 80.325, 80.075, 80.075, 79.85, 79.85, 80.03333333333333, 80.03333333333333, 80.33333333333333, 80.33333333333333, 80.81666666666666, 80.81666666666666, 81.00833333333334, 81.00833333333334, 81.15833333333333, 81.15833333333333, 80.73333333333333, 80.73333333333333, 81.09166666666667, 81.09166666666667, 80.88333333333334, 80.88333333333334, 80.63333333333334, 80.63333333333334, 80.775, 80.775, 81.15833333333333, 81.15833333333333, 81.30833333333334, 81.30833333333334, 81.1, 81.1, 81.21666666666667, 81.21666666666667, 80.71666666666667, 80.71666666666667, 81.11666666666666, 81.11666666666666, 81.45, 81.45, 81.49166666666666, 81.49166666666666, 81.8, 81.8, 81.75833333333334, 81.75833333333334, 81.575, 81.575, 81.20833333333333, 81.20833333333333, 81.04166666666667, 81.04166666666667, 81.35833333333333, 81.35833333333333, 81.46666666666667, 81.46666666666667, 81.44166666666666, 81.44166666666666, 81.00833333333334, 81.00833333333334, 81.06666666666666, 81.06666666666666, 81.275, 81.275, 81.25, 81.25, 81.46666666666667, 81.46666666666667, 81.30833333333334, 81.30833333333334, 81.15, 81.15, 81.21666666666667, 81.21666666666667, 81.3, 81.3, 81.21666666666667, 81.21666666666667, 81.45, 81.45, 81.46666666666667, 81.46666666666667, 81.66666666666667, 81.66666666666667, 81.59166666666667, 81.59166666666667, 81.64166666666667, 81.64166666666667, 81.7, 81.7, 81.70833333333333, 81.70833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Final Round, Train loss: 0.068, Test loss: 0.764, Test accuracy: 67.43
Average accuracy final 10 rounds: 68.16166666666666
2179.865911245346
[]
[21.55, 34.766666666666666, 38.99166666666667, 41.1, 46.65, 50.575, 56.516666666666666, 59.225, 59.75, 60.24166666666667, 59.3, 60.2, 61.25833333333333, 61.43333333333333, 60.44166666666667, 60.9, 60.475, 61.8, 61.4, 61.65, 62.59166666666667, 63.49166666666667, 64.60833333333333, 64.075, 64.475, 64.925, 64.65833333333333, 64.98333333333333, 65.86666666666666, 66.49166666666666, 65.88333333333334, 64.99166666666666, 66.15, 65.88333333333334, 66.35, 65.96666666666667, 65.4, 66.03333333333333, 66.28333333333333, 67.29166666666667, 66.43333333333334, 67.30833333333334, 66.89166666666667, 67.39166666666667, 66.825, 67.525, 67.95, 68.3, 69.275, 69.26666666666667, 67.79166666666667, 68.48333333333333, 69.075, 68.48333333333333, 69.84166666666667, 70.175, 70.88333333333334, 69.86666666666666, 69.6, 70.49166666666666, 69.66666666666667, 68.49166666666666, 68.50833333333334, 69.09166666666667, 69.34166666666667, 69.275, 68.93333333333334, 67.95, 68.73333333333333, 68.74166666666666, 68.09166666666667, 68.01666666666667, 68.275, 67.61666666666666, 67.375, 67.475, 67.01666666666667, 66.425, 65.88333333333334, 66.90833333333333, 67.19166666666666, 68.03333333333333, 68.71666666666667, 69.025, 68.66666666666667, 68.775, 68.65833333333333, 68.93333333333334, 67.89166666666667, 68.28333333333333, 68.53333333333333, 68.8, 69.20833333333333, 68.8, 67.61666666666666, 68.38333333333334, 68.4, 66.975, 67.35833333333333, 67.54166666666667, 67.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.575, Test loss: 0.564, Test accuracy: 77.26
Average accuracy final 10 rounds: 78.05
Average global accuracy final 10 rounds: 78.05
1434.424687385559
[]
[20.975, 33.45, 41.233333333333334, 40.666666666666664, 46.55, 53.95, 57.583333333333336, 61.75, 61.208333333333336, 63.475, 65.4, 65.73333333333333, 67.43333333333334, 68.98333333333333, 69.06666666666666, 68.85833333333333, 70.10833333333333, 70.46666666666667, 72.30833333333334, 72.175, 72.13333333333334, 71.66666666666667, 71.425, 72.34166666666667, 73.00833333333334, 72.84166666666667, 72.90833333333333, 72.41666666666667, 73.44166666666666, 73.30833333333334, 73.50833333333334, 73.9, 73.69166666666666, 73.94166666666666, 73.475, 74.13333333333334, 74.74166666666666, 74.93333333333334, 75.35833333333333, 75.3, 75.16666666666667, 74.475, 74.775, 74.06666666666666, 74.25833333333334, 74.95833333333333, 75.81666666666666, 75.85, 76.825, 76.59166666666667, 76.20833333333333, 76.26666666666667, 76.05833333333334, 75.59166666666667, 76.33333333333333, 76.7, 76.05833333333334, 76.26666666666667, 77.025, 76.76666666666667, 76.225, 76.53333333333333, 76.48333333333333, 76.74166666666666, 76.44166666666666, 76.175, 76.7, 76.69166666666666, 76.25833333333334, 77.06666666666666, 77.40833333333333, 76.88333333333334, 77.30833333333334, 77.55, 77.775, 77.35, 77.30833333333334, 76.43333333333334, 75.88333333333334, 76.61666666666666, 76.60833333333333, 76.625, 76.29166666666667, 76.65833333333333, 76.83333333333333, 77.125, 77.25833333333334, 77.425, 77.925, 77.88333333333334, 78.125, 77.96666666666667, 77.96666666666667, 77.94166666666666, 78.05833333333334, 78.1, 78.5, 78.25833333333334, 77.89166666666667, 77.69166666666666, 77.25833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: nan, Test loss: nan, Test accuracy: 10.00
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 10.00
Average accuracy final 10 rounds: 10.0 

Average global accuracy final 10 rounds: 10.0 

1616.7553980350494
[1.7957804203033447, 3.363603115081787, 4.934453725814819, 6.546543836593628, 8.12610936164856, 9.697511672973633, 11.257782220840454, 12.824114799499512, 14.385583877563477, 15.938385963439941, 17.48066735267639, 19.03608512878418, 20.600558757781982, 22.165849208831787, 23.73994016647339, 25.312582969665527, 26.872599363327026, 28.434995889663696, 29.99668598175049, 31.553125143051147, 33.105048179626465, 34.64519119262695, 36.182689905166626, 37.7285053730011, 39.269891023635864, 40.81485033035278, 42.364293336868286, 43.92036318778992, 45.46657586097717, 47.017234802246094, 48.57058668136597, 50.11940884590149, 51.66435980796814, 53.20307469367981, 54.743789196014404, 56.28882026672363, 57.8437876701355, 59.400208473205566, 60.953566551208496, 62.5029673576355, 64.05904459953308, 65.61532282829285, 67.15802311897278, 68.71868991851807, 70.26294827461243, 71.80817651748657, 73.35948944091797, 74.91479539871216, 76.46474313735962, 78.00398349761963, 79.55581974983215, 81.11576771736145, 82.65692353248596, 84.20361113548279, 85.74970626831055, 87.29812026023865, 88.85014462471008, 90.40292453765869, 91.95628643035889, 93.50530934333801, 95.04551100730896, 96.59565472602844, 97.96990370750427, 99.35114455223083, 100.72536182403564, 102.1026566028595, 103.47041964530945, 104.85583829879761, 106.2298903465271, 107.602365732193, 108.97315287590027, 110.34665751457214, 111.7121570110321, 113.08705186843872, 114.4638876914978, 115.83863735198975, 117.209956407547, 118.56518816947937, 119.94321179389954, 121.33657598495483, 122.71650290489197, 124.09860610961914, 125.48173880577087, 126.85370230674744, 128.23856139183044, 129.61962485313416, 131.0037546157837, 132.3939483165741, 133.78257989883423, 135.15962171554565, 136.5459394454956, 137.94224762916565, 139.3396475315094, 140.73281335830688, 142.1275191307068, 143.52408623695374, 144.90820360183716, 146.28906083106995, 147.6788775920868, 149.05578351020813, 151.36073422431946]
[10.041666666666666, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Final Round, Train loss: 0.187, Test loss: 2.502, Test accuracy: 25.09
Average accuracy final 10 rounds: 22.730833333333333
2488.1307775974274
[3.702800750732422, 7.1673338413238525, 10.650895833969116, 14.115853309631348, 17.544371128082275, 21.370955228805542, 25.189160585403442, 29.001147985458374, 32.4481635093689, 35.88465642929077, 39.35194110870361, 42.81688356399536, 46.257253885269165, 49.696146726608276, 53.11839008331299, 56.5478937625885, 59.98455238342285, 63.417855978012085, 66.86990976333618, 70.30725145339966, 73.74441456794739, 77.20366644859314, 80.73477625846863, 84.23179244995117, 87.68786597251892, 91.199223279953, 94.68405747413635, 98.16055679321289, 101.645681142807, 105.10876965522766, 108.61303329467773, 112.09662628173828, 115.59626126289368, 119.07920718193054, 122.56576013565063, 126.06611275672913, 129.51969361305237, 133.0023958683014, 136.49626970291138, 139.9585838317871, 143.4545295238495, 146.93131017684937, 150.45903182029724, 153.99552416801453, 157.53095626831055, 161.08278250694275, 164.55534029006958, 168.0325584411621, 171.55859541893005, 175.0676999092102, 178.57216930389404, 182.10721254348755, 185.6713593006134, 189.22002387046814, 192.75122475624084, 196.307630777359, 199.8367052078247, 203.4045271873474, 207.0002088546753, 210.5769419670105, 214.15132999420166, 217.74465131759644, 221.32312417030334, 224.9087154865265, 228.44298768043518, 231.9875409603119, 235.52738642692566, 239.05574750900269, 242.56309270858765, 246.08927655220032, 249.6479902267456, 253.2026767730713, 256.6884367465973, 260.16636967658997, 263.6634714603424, 267.1536509990692, 270.670622587204, 274.23228001594543, 277.7708010673523, 281.3532724380493, 284.86366748809814, 288.39115953445435, 291.90020275115967, 295.41754245758057, 298.9925603866577, 302.54923939704895, 306.11913800239563, 309.6151473522186, 313.1399793624878, 316.62854385375977, 320.1332564353943, 323.6452012062073, 327.1975257396698, 330.7178695201874, 334.235769033432, 337.78889417648315, 341.33141112327576, 344.84884428977966, 348.38776659965515, 351.9505305290222, 354.8839204311371]
[13.341666666666667, 19.433333333333334, 19.025, 22.75, 22.016666666666666, 21.866666666666667, 23.066666666666666, 19.45, 22.075, 21.341666666666665, 23.225, 22.0, 21.791666666666668, 20.791666666666668, 21.008333333333333, 21.708333333333332, 23.825, 22.2, 21.208333333333332, 22.383333333333333, 22.483333333333334, 22.75, 20.958333333333332, 19.166666666666668, 20.616666666666667, 20.641666666666666, 20.825, 18.191666666666666, 21.7, 22.975, 23.391666666666666, 21.191666666666666, 22.416666666666668, 24.1, 23.441666666666666, 23.233333333333334, 22.958333333333332, 24.341666666666665, 23.75, 20.966666666666665, 22.458333333333332, 22.55, 22.825, 24.025, 25.383333333333333, 23.333333333333332, 23.933333333333334, 24.158333333333335, 23.075, 24.925, 24.066666666666666, 21.508333333333333, 22.383333333333333, 23.091666666666665, 22.6, 24.325, 23.316666666666666, 24.758333333333333, 24.891666666666666, 23.491666666666667, 23.358333333333334, 23.641666666666666, 22.933333333333334, 23.608333333333334, 22.35, 22.458333333333332, 24.425, 24.333333333333332, 22.633333333333333, 23.541666666666668, 20.258333333333333, 23.441666666666666, 25.191666666666666, 23.2, 22.083333333333332, 24.675, 21.8, 22.008333333333333, 22.141666666666666, 23.166666666666668, 21.7, 24.166666666666668, 24.975, 22.416666666666668, 22.425, 22.958333333333332, 21.641666666666666, 23.508333333333333, 23.575, 22.35, 21.558333333333334, 22.291666666666668, 23.166666666666668, 18.958333333333332, 21.833333333333332, 24.441666666666666, 23.325, 23.483333333333334, 24.658333333333335, 23.591666666666665, 25.091666666666665]
python: can't open file 'main_fedpac_k.py': [Errno 2] No such file or directory
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.203, Test loss: 0.343, Test accuracy: 87.17
Average accuracy final 10 rounds: 86.73166666666668
1826.8115682601929
[2.2585396766662598, 4.5170793533325195, 6.383645057678223, 8.250210762023926, 10.131341218948364, 12.012471675872803, 13.867666006088257, 15.722860336303711, 17.568108081817627, 19.413355827331543, 21.258291959762573, 23.103228092193604, 24.91394305229187, 26.724658012390137, 28.5796902179718, 30.434722423553467, 32.26464533805847, 34.09456825256348, 35.913376331329346, 37.732184410095215, 39.56690955162048, 41.40163469314575, 43.233163356781006, 45.06469202041626, 46.87876772880554, 48.692843437194824, 50.50407791137695, 52.31531238555908, 54.092246294021606, 55.86918020248413, 57.63658261299133, 59.403985023498535, 61.216637134552, 63.02928924560547, 64.84057569503784, 66.65186214447021, 68.47453141212463, 70.29720067977905, 72.12236189842224, 73.94752311706543, 75.77142763137817, 77.59533214569092, 79.43795585632324, 81.28057956695557, 83.09888291358948, 84.91718626022339, 86.7376651763916, 88.55814409255981, 90.39913487434387, 92.24012565612793, 94.07523822784424, 95.91035079956055, 97.72667837142944, 99.54300594329834, 101.37609100341797, 103.2091760635376, 105.04620099067688, 106.88322591781616, 108.72498321533203, 110.5667405128479, 112.4014687538147, 114.2361969947815, 116.09161043167114, 117.94702386856079, 119.7767767906189, 121.606529712677, 123.41922736167908, 125.23192501068115, 127.06318426132202, 128.8944435119629, 130.72902250289917, 132.56360149383545, 134.40446281433105, 136.24532413482666, 138.08168363571167, 139.91804313659668, 141.7531373500824, 143.58823156356812, 145.44829845428467, 147.30836534500122, 149.1420612335205, 150.9757571220398, 152.79434776306152, 154.61293840408325, 156.44595313072205, 158.27896785736084, 160.10922837257385, 161.93948888778687, 163.76972222328186, 165.59995555877686, 167.42218923568726, 169.24442291259766, 171.07346105575562, 172.90249919891357, 174.7565610408783, 176.61062288284302, 178.48073935508728, 180.35085582733154, 182.19921159744263, 184.0475673675537, 185.882803440094, 187.71803951263428, 189.5660228729248, 191.41400623321533, 193.26479816436768, 195.11559009552002, 196.95751643180847, 198.79944276809692, 200.64291143417358, 202.48638010025024, 204.3194077014923, 206.15243530273438, 208.00253200531006, 209.85262870788574, 211.69718503952026, 213.54174137115479, 215.37623834609985, 217.21073532104492, 219.05089664459229, 220.89105796813965, 222.73709726333618, 224.58313655853271, 226.44400715827942, 228.30487775802612, 230.14429473876953, 231.98371171951294, 233.81215023994446, 235.64058876037598, 237.49261283874512, 239.34463691711426, 241.18881034851074, 243.03298377990723, 244.86916995048523, 246.70535612106323, 248.54045033454895, 250.37554454803467, 252.22761750221252, 254.07969045639038, 255.9073302745819, 257.73497009277344, 259.55916714668274, 261.38336420059204, 263.2222821712494, 265.06120014190674, 266.878865480423, 268.6965308189392, 270.5303108692169, 272.36409091949463, 274.20201683044434, 276.03994274139404, 277.88354110717773, 279.7271394729614, 281.5445399284363, 283.36194038391113, 285.2245280742645, 287.0871157646179, 288.92115235328674, 290.75518894195557, 292.59805560112, 294.4409222602844, 296.26549100875854, 298.09005975723267, 299.9067916870117, 301.72352361679077, 303.53878569602966, 305.35404777526855, 307.1679615974426, 308.9818754196167, 310.7987711429596, 312.6156668663025, 314.43528270721436, 316.2548985481262, 318.06826853752136, 319.8816385269165, 321.7033395767212, 323.5250406265259, 325.33474564552307, 327.14445066452026, 328.9660692214966, 330.7876877784729, 332.6043176651001, 334.4209475517273, 336.2217321395874, 338.0225167274475, 339.85211753845215, 341.6817183494568, 343.502810716629, 345.32390308380127, 347.13706040382385, 348.95021772384644, 350.7798306941986, 352.6094436645508, 354.42859983444214, 356.2477560043335, 358.0743820667267, 359.9010081291199, 361.72971296310425, 363.5584177970886, 365.380074262619, 367.2017307281494, 369.44371128082275, 371.6856918334961]
[16.016666666666666, 16.016666666666666, 32.21666666666667, 32.21666666666667, 40.90833333333333, 40.90833333333333, 45.675, 45.675, 52.975, 52.975, 54.4, 54.4, 56.71666666666667, 56.71666666666667, 62.733333333333334, 62.733333333333334, 67.84166666666667, 67.84166666666667, 68.625, 68.625, 69.6, 69.6, 70.475, 70.475, 73.54166666666667, 73.54166666666667, 74.74166666666666, 74.74166666666666, 76.0, 76.0, 76.66666666666667, 76.66666666666667, 76.56666666666666, 76.56666666666666, 77.4, 77.4, 77.45, 77.45, 77.9, 77.9, 78.725, 78.725, 79.55, 79.55, 79.71666666666667, 79.71666666666667, 79.24166666666666, 79.24166666666666, 79.81666666666666, 79.81666666666666, 79.89166666666667, 79.89166666666667, 80.43333333333334, 80.43333333333334, 80.54166666666667, 80.54166666666667, 80.825, 80.825, 80.38333333333334, 80.38333333333334, 81.03333333333333, 81.03333333333333, 81.525, 81.525, 81.60833333333333, 81.60833333333333, 82.2, 82.2, 82.35, 82.35, 82.10833333333333, 82.10833333333333, 82.35, 82.35, 82.48333333333333, 82.48333333333333, 82.55833333333334, 82.55833333333334, 82.44166666666666, 82.44166666666666, 83.1, 83.1, 83.45833333333333, 83.45833333333333, 83.125, 83.125, 83.85833333333333, 83.85833333333333, 83.90833333333333, 83.90833333333333, 83.6, 83.6, 83.68333333333334, 83.68333333333334, 83.875, 83.875, 84.375, 84.375, 84.325, 84.325, 84.21666666666667, 84.21666666666667, 84.125, 84.125, 84.04166666666667, 84.04166666666667, 84.11666666666666, 84.11666666666666, 84.66666666666667, 84.66666666666667, 84.575, 84.575, 84.36666666666666, 84.36666666666666, 83.75833333333334, 83.75833333333334, 84.49166666666666, 84.49166666666666, 84.48333333333333, 84.48333333333333, 84.75, 84.75, 85.19166666666666, 85.19166666666666, 84.95833333333333, 84.95833333333333, 85.16666666666667, 85.16666666666667, 85.24166666666666, 85.24166666666666, 85.43333333333334, 85.43333333333334, 84.675, 84.675, 85.70833333333333, 85.70833333333333, 85.16666666666667, 85.16666666666667, 85.18333333333334, 85.18333333333334, 85.625, 85.625, 85.14166666666667, 85.14166666666667, 85.61666666666666, 85.61666666666666, 86.1, 86.1, 85.625, 85.625, 86.08333333333333, 86.08333333333333, 86.33333333333333, 86.33333333333333, 86.05833333333334, 86.05833333333334, 85.95833333333333, 85.95833333333333, 86.4, 86.4, 86.10833333333333, 86.10833333333333, 86.38333333333334, 86.38333333333334, 86.21666666666667, 86.21666666666667, 85.925, 85.925, 86.55, 86.55, 86.30833333333334, 86.30833333333334, 85.95833333333333, 85.95833333333333, 86.3, 86.3, 85.75, 85.75, 86.20833333333333, 86.20833333333333, 86.425, 86.425, 86.575, 86.575, 86.01666666666667, 86.01666666666667, 86.775, 86.775, 86.96666666666667, 86.96666666666667, 87.04166666666667, 87.04166666666667, 86.45833333333333, 86.45833333333333, 86.79166666666667, 86.79166666666667, 87.125, 87.125, 87.14166666666667, 87.14166666666667, 87.175, 87.175]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.569, Test loss: 1.635, Test accuracy: 82.81
Final Round, Global train loss: 1.569, Global test loss: 1.624, Global test accuracy: 83.87
Average accuracy final 10 rounds: 82.83824999999999 

Average global accuracy final 10 rounds: 83.8545 

5128.283280849457
[3.7181150913238525, 7.436230182647705, 10.977999687194824, 14.519769191741943, 18.094101905822754, 21.668434619903564, 25.183805227279663, 28.69917583465576, 32.28873586654663, 35.8782958984375, 39.40274786949158, 42.927199840545654, 46.37346291542053, 49.81972599029541, 53.38289952278137, 56.946073055267334, 60.47951674461365, 64.01296043395996, 67.60303044319153, 71.1931004524231, 74.92000675201416, 78.64691305160522, 82.28367042541504, 85.92042779922485, 89.46422481536865, 93.00802183151245, 96.08239006996155, 99.15675830841064, 102.26866579055786, 105.38057327270508, 108.47283267974854, 111.56509208679199, 114.6128671169281, 117.66064214706421, 120.82632803916931, 123.99201393127441, 127.15467548370361, 130.3173370361328, 133.29061341285706, 136.2638897895813, 139.1533169746399, 142.0427441596985, 145.03592228889465, 148.02910041809082, 150.98916602134705, 153.94923162460327, 156.886004447937, 159.82277727127075, 162.7527244091034, 165.68267154693604, 168.59157466888428, 171.50047779083252, 174.38895535469055, 177.27743291854858, 180.18263173103333, 183.08783054351807, 186.00352668762207, 188.91922283172607, 191.8543884754181, 194.7895541191101, 197.7096564769745, 200.62975883483887, 203.55509996414185, 206.48044109344482, 209.37005352973938, 212.25966596603394, 215.1530909538269, 218.04651594161987, 220.95928812026978, 223.87206029891968, 226.80947875976562, 229.74689722061157, 232.7947278022766, 235.84255838394165, 238.88722491264343, 241.93189144134521, 244.92565321922302, 247.91941499710083, 250.90645694732666, 253.8934988975525, 256.8913805484772, 259.88926219940186, 262.89466762542725, 265.90007305145264, 268.9222557544708, 271.944438457489, 274.8971083164215, 277.849778175354, 280.8033695220947, 283.75696086883545, 286.6512405872345, 289.54552030563354, 292.4418203830719, 295.33812046051025, 298.25298285484314, 301.167845249176, 304.10470366477966, 307.0415620803833, 310.03222823143005, 313.0228943824768, 315.973650932312, 318.9244074821472, 321.93254137039185, 324.9406752586365, 327.8877305984497, 330.83478593826294, 333.8164336681366, 336.79808139801025, 339.7386965751648, 342.67931175231934, 345.6760470867157, 348.67278242111206, 351.62199330329895, 354.57120418548584, 357.4320101737976, 360.2928161621094, 363.19631123542786, 366.09980630874634, 368.99814891815186, 371.8964915275574, 374.84274101257324, 377.7889904975891, 380.73648500442505, 383.683979511261, 386.7233898639679, 389.7628002166748, 392.7475163936615, 395.7322325706482, 398.7899615764618, 401.8476905822754, 404.7803418636322, 407.712993144989, 410.661493062973, 413.60999298095703, 416.562908411026, 419.51582384109497, 422.5420620441437, 425.5683002471924, 428.5717890262604, 431.57527780532837, 434.68694519996643, 437.7986125946045, 440.8445565700531, 443.8905005455017, 446.9338684082031, 449.97723627090454, 453.0345959663391, 456.0919556617737, 459.2335367202759, 462.3751177787781, 465.52579402923584, 468.6764702796936, 471.7769033908844, 474.8773365020752, 477.8677353858948, 480.85813426971436, 483.8186933994293, 486.7792525291443, 489.74434542655945, 492.7094383239746, 495.71014380455017, 498.71084928512573, 501.73432302474976, 504.7577967643738, 507.81712532043457, 510.87645387649536, 513.9371960163116, 516.9979381561279, 520.034793138504, 523.0716481208801, 526.1151752471924, 529.1587023735046, 532.3095026016235, 535.4603028297424, 538.6152992248535, 541.7702956199646, 544.9134206771851, 548.0565457344055, 551.1130766868591, 554.1696076393127, 557.1112158298492, 560.0528240203857, 562.881189584732, 565.7095551490784, 568.6180477142334, 571.5265402793884, 574.4895348548889, 577.4525294303894, 580.516428232193, 583.5803270339966, 586.6702740192413, 589.7602210044861, 592.748583316803, 595.7369456291199, 598.6441912651062, 601.5514369010925, 604.506382226944, 607.4613275527954, 610.445366859436, 613.4294061660767, 614.9254283905029, 616.4214506149292]
[36.67, 36.67, 65.56, 65.56, 73.2125, 73.2125, 74.3325, 74.3325, 75.0375, 75.0375, 77.0475, 77.0475, 77.4025, 77.4025, 77.54, 77.54, 77.6025, 77.6025, 77.75, 77.75, 78.48, 78.48, 78.55, 78.55, 80.7625, 80.7625, 80.8225, 80.8225, 81.12, 81.12, 81.13, 81.13, 81.18, 81.18, 81.255, 81.255, 81.46, 81.46, 81.4375, 81.4375, 81.4325, 81.4325, 81.865, 81.865, 81.865, 81.865, 81.87, 81.87, 81.915, 81.915, 81.9125, 81.9125, 81.9975, 81.9975, 81.9925, 81.9925, 82.3125, 82.3125, 82.3025, 82.3025, 82.3075, 82.3075, 82.3225, 82.3225, 82.4875, 82.4875, 82.52, 82.52, 82.53, 82.53, 82.5275, 82.5275, 82.555, 82.555, 82.615, 82.615, 82.6075, 82.6075, 82.6, 82.6, 82.58, 82.58, 82.5575, 82.5575, 82.5875, 82.5875, 82.5925, 82.5925, 82.59, 82.59, 82.61, 82.61, 82.655, 82.655, 82.6475, 82.6475, 82.68, 82.68, 82.65, 82.65, 82.6375, 82.6375, 82.675, 82.675, 82.67, 82.67, 82.685, 82.685, 82.71, 82.71, 82.705, 82.705, 82.7075, 82.7075, 82.735, 82.735, 82.715, 82.715, 82.7075, 82.7075, 82.715, 82.715, 82.715, 82.715, 82.715, 82.715, 82.7275, 82.7275, 82.7425, 82.7425, 82.7275, 82.7275, 82.735, 82.735, 82.75, 82.75, 82.7425, 82.7425, 82.7175, 82.7175, 82.7325, 82.7325, 82.74, 82.74, 82.77, 82.77, 82.7575, 82.7575, 82.765, 82.765, 82.75, 82.75, 82.795, 82.795, 82.7875, 82.7875, 82.815, 82.815, 82.8225, 82.8225, 82.8225, 82.8225, 82.8325, 82.8325, 82.815, 82.815, 82.83, 82.83, 82.815, 82.815, 82.8175, 82.8175, 82.84, 82.84, 82.8125, 82.8125, 82.8025, 82.8025, 82.8175, 82.8175, 82.835, 82.835, 82.8375, 82.8375, 82.845, 82.845, 82.8625, 82.8625, 82.85, 82.85, 82.835, 82.835, 82.825, 82.825, 82.85, 82.85, 82.8275, 82.8275, 82.815, 82.815, 82.8125, 82.8125]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.62
Final Round, Global train loss: 1.469, Global test loss: 1.496, Global test accuracy: 96.76
Average accuracy final 10 rounds: 96.61975000000001 

Average global accuracy final 10 rounds: 96.78474999999999 

5354.560883283615
[3.5008339881896973, 7.0016679763793945, 10.412813425064087, 13.82395887374878, 17.283701419830322, 20.743443965911865, 24.328362703323364, 27.913281440734863, 31.14948058128357, 34.385679721832275, 37.97315716743469, 41.56063461303711, 45.167555809020996, 48.77447700500488, 52.30479454994202, 55.83511209487915, 59.25495409965515, 62.67479610443115, 66.1460223197937, 69.61724853515625, 73.12502861022949, 76.63280868530273, 80.22500324249268, 83.81719779968262, 87.38355302810669, 90.94990825653076, 94.49768042564392, 98.04545259475708, 101.61843466758728, 105.19141674041748, 108.79081130027771, 112.39020586013794, 115.97012996673584, 119.55005407333374, 122.98296976089478, 126.41588544845581, 129.6868507862091, 132.9578161239624, 136.4694013595581, 139.9809865951538, 143.5393843650818, 147.09778213500977, 150.61608171463013, 154.1343812942505, 157.63135075569153, 161.12832021713257, 164.68414568901062, 168.23997116088867, 171.78532338142395, 175.33067560195923, 178.82609915733337, 182.32152271270752, 185.6961064338684, 189.0706901550293, 192.46092891693115, 195.851167678833, 199.31975984573364, 202.78835201263428, 206.2990210056305, 209.8096899986267, 213.3500518798828, 216.89041376113892, 220.40821647644043, 223.92601919174194, 227.43214678764343, 230.93827438354492, 234.5128173828125, 238.08736038208008, 241.62598943710327, 245.16461849212646, 248.69336533546448, 252.2221121788025, 255.59779977798462, 258.97348737716675, 262.43833684921265, 265.90318632125854, 269.4254958629608, 272.9478054046631, 276.5328321456909, 280.11785888671875, 283.6824417114258, 287.2470245361328, 290.7050635814667, 294.16310262680054, 297.6222252845764, 301.0813479423523, 304.5642759799957, 308.04720401763916, 311.49316334724426, 314.93912267684937, 318.44459199905396, 321.95006132125854, 325.4119553565979, 328.87384939193726, 332.400830745697, 335.9278120994568, 339.5812015533447, 343.23459100723267, 346.87479734420776, 350.51500368118286, 354.1167097091675, 357.7184157371521, 361.2245066165924, 364.7305974960327, 368.2139542102814, 371.69731092453003, 375.1585605144501, 378.6198101043701, 382.16866421699524, 385.71751832962036, 389.2553400993347, 392.7931618690491, 396.2649760246277, 399.7367901802063, 403.31472420692444, 406.8926582336426, 410.4324429035187, 413.9722275733948, 417.5040924549103, 421.0359573364258, 424.51775097846985, 427.9995446205139, 431.4662272930145, 434.93290996551514, 438.41764974594116, 441.9023895263672, 445.4406991004944, 448.9790086746216, 452.57916021347046, 456.17931175231934, 459.76827335357666, 463.357234954834, 466.9008162021637, 470.4443974494934, 473.9389750957489, 477.4335527420044, 480.93746995925903, 484.4413871765137, 487.9930028915405, 491.5446186065674, 495.0784909725189, 498.61236333847046, 502.1659846305847, 505.719605922699, 509.2662556171417, 512.8129053115845, 516.3498485088348, 519.8867917060852, 523.3421049118042, 526.7974181175232, 530.2535717487335, 533.7097253799438, 537.2359735965729, 540.7622218132019, 544.2885818481445, 547.8149418830872, 551.3233485221863, 554.8317551612854, 558.334748506546, 561.8377418518066, 565.3072893619537, 568.7768368721008, 572.2313084602356, 575.6857800483704, 579.2066020965576, 582.7274241447449, 586.2965993881226, 589.8657746315002, 593.4016563892365, 596.9375381469727, 600.0001397132874, 603.062741279602, 606.0157046318054, 608.9686679840088, 612.097772359848, 615.2268767356873, 618.3549697399139, 621.4830627441406, 624.6458878517151, 627.8087129592896, 631.0021600723267, 634.1956071853638, 637.2120006084442, 640.2283940315247, 643.1879603862762, 646.1475267410278, 649.3398802280426, 652.5322337150574, 655.7355213165283, 658.9388089179993, 662.0571999549866, 665.1755909919739, 668.1320102214813, 671.0884294509888, 673.9944620132446, 676.9004945755005, 679.9741809368134, 683.0478672981262, 686.1891684532166, 689.3304696083069, 690.9011540412903, 692.4718384742737]
[45.82, 45.82, 70.9325, 70.9325, 77.475, 77.475, 79.9675, 79.9675, 80.4625, 80.4625, 83.4775, 83.4775, 83.6775, 83.6775, 83.8575, 83.8575, 84.2175, 84.2175, 84.4625, 84.4625, 86.3875, 86.3875, 88.3725, 88.3725, 89.6475, 89.6475, 90.6, 90.6, 90.895, 90.895, 91.06, 91.06, 92.5025, 92.5025, 92.81, 92.81, 92.925, 92.925, 93.0175, 93.0175, 93.47, 93.47, 93.645, 93.645, 93.8525, 93.8525, 93.97, 93.97, 94.06, 94.06, 94.285, 94.285, 94.3625, 94.3625, 94.37, 94.37, 95.0375, 95.0375, 95.1775, 95.1775, 95.2325, 95.2325, 95.2725, 95.2725, 95.335, 95.335, 95.4375, 95.4375, 95.5375, 95.5375, 95.585, 95.585, 95.605, 95.605, 95.6725, 95.6725, 95.67, 95.67, 95.6775, 95.6775, 95.7475, 95.7475, 95.7875, 95.7875, 95.85, 95.85, 95.8825, 95.8825, 95.9, 95.9, 95.93, 95.93, 95.9675, 95.9675, 95.98, 95.98, 95.98, 95.98, 96.005, 96.005, 96.045, 96.045, 96.0825, 96.0825, 96.075, 96.075, 96.105, 96.105, 96.145, 96.145, 96.1975, 96.1975, 96.2175, 96.2175, 96.2375, 96.2375, 96.2375, 96.2375, 96.255, 96.255, 96.2575, 96.2575, 96.25, 96.25, 96.2775, 96.2775, 96.3275, 96.3275, 96.33, 96.33, 96.3375, 96.3375, 96.325, 96.325, 96.375, 96.375, 96.415, 96.415, 96.4475, 96.4475, 96.4625, 96.4625, 96.465, 96.465, 96.485, 96.485, 96.4925, 96.4925, 96.5225, 96.5225, 96.56, 96.56, 96.5625, 96.5625, 96.58, 96.58, 96.56, 96.56, 96.6, 96.6, 96.5825, 96.5825, 96.5575, 96.5575, 96.5625, 96.5625, 96.5725, 96.5725, 96.59, 96.59, 96.58, 96.58, 96.5575, 96.5575, 96.5375, 96.5375, 96.5725, 96.5725, 96.5775, 96.5775, 96.6, 96.6, 96.59, 96.59, 96.5975, 96.5975, 96.5975, 96.5975, 96.6325, 96.6325, 96.6425, 96.6425, 96.65, 96.65, 96.6325, 96.6325, 96.6175, 96.6175, 96.6375, 96.6375, 96.625, 96.625]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.473, Test loss: 1.504, Test accuracy: 95.84
Average accuracy final 10 rounds: 95.77475 

4080.53817486763
[3.5436933040618896, 7.087386608123779, 10.440189123153687, 13.792991638183594, 17.1362407207489, 20.47948980331421, 23.82097625732422, 27.16246271133423, 30.529860258102417, 33.897257804870605, 37.23268103599548, 40.56810426712036, 43.76357173919678, 46.95903921127319, 50.325594902038574, 53.692150592803955, 57.01488542556763, 60.3376202583313, 63.656582832336426, 66.97554540634155, 70.37677717208862, 73.7780089378357, 77.17382550239563, 80.56964206695557, 83.75983500480652, 86.95002794265747, 90.2533028125763, 93.55657768249512, 96.83024501800537, 100.10391235351562, 103.39430737495422, 106.68470239639282, 110.10418605804443, 113.52366971969604, 116.92540264129639, 120.32713556289673, 123.72880530357361, 127.13047504425049, 130.83212995529175, 134.533784866333, 137.89340686798096, 141.2530288696289, 144.58417749404907, 147.91532611846924, 151.29340720176697, 154.6714882850647, 157.89494919776917, 161.11841011047363, 164.53392934799194, 167.94944858551025, 171.34181880950928, 174.7341890335083, 178.0432004928589, 181.35221195220947, 184.51421236991882, 187.67621278762817, 191.05162262916565, 194.42703247070312, 197.63407397270203, 200.84111547470093, 204.15728974342346, 207.473464012146, 210.80966138839722, 214.14585876464844, 217.34055471420288, 220.53525066375732, 223.93181467056274, 227.32837867736816, 230.7351520061493, 234.14192533493042, 237.46690154075623, 240.79187774658203, 244.2737843990326, 247.75569105148315, 251.22439217567444, 254.69309329986572, 257.99252438545227, 261.2919554710388, 264.6553256511688, 268.0186958312988, 271.3389537334442, 274.6592116355896, 277.97889471054077, 281.29857778549194, 284.7529866695404, 288.20739555358887, 291.6759305000305, 295.14446544647217, 298.5122170448303, 301.8799686431885, 305.3331410884857, 308.78631353378296, 312.240713596344, 315.69511365890503, 319.05710458755493, 322.41909551620483, 325.8161129951477, 329.2131304740906, 332.5633313655853, 335.9135322570801, 339.22990798950195, 342.5462837219238, 345.8285219669342, 349.1107602119446, 352.4607939720154, 355.8108277320862, 359.03914642333984, 362.2674651145935, 365.72641348838806, 369.1853618621826, 372.6028401851654, 376.0203185081482, 379.227756023407, 382.43519353866577, 385.8123035430908, 389.18941354751587, 392.2593584060669, 395.3293032646179, 398.18514037132263, 401.04097747802734, 404.0670254230499, 407.0930733680725, 410.10745429992676, 413.121835231781, 416.1606559753418, 419.1994767189026, 422.2646095752716, 425.3297424316406, 428.41131591796875, 431.4928894042969, 434.3546621799469, 437.2164349555969, 440.21711921691895, 443.21780347824097, 446.1570100784302, 449.0962166786194, 452.27299547195435, 455.4497742652893, 458.5999126434326, 461.7500510215759, 464.65719270706177, 467.5643343925476, 470.5516526699066, 473.5389709472656, 476.60002756118774, 479.66108417510986, 482.5771265029907, 485.4931688308716, 488.5284321308136, 491.5636954307556, 494.6086754798889, 497.6536555290222, 500.620646238327, 503.58763694763184, 506.5511620044708, 509.5146870613098, 512.4027011394501, 515.2907152175903, 518.2113561630249, 521.1319971084595, 524.2038140296936, 527.2756309509277, 530.2502989768982, 533.2249670028687, 536.1133382320404, 539.0017094612122, 542.0308299064636, 545.0599503517151, 548.0369277000427, 551.0139050483704, 553.9788484573364, 556.9437918663025, 559.9554963111877, 562.967200756073, 565.9743139743805, 568.981427192688, 572.1659762859344, 575.3505253791809, 578.2098324298859, 581.0691394805908, 584.010175704956, 586.9512119293213, 590.0683906078339, 593.1855692863464, 596.2158017158508, 599.2460341453552, 602.147976398468, 605.0499186515808, 608.1548049449921, 611.2596912384033, 614.2541072368622, 617.248523235321, 620.1372134685516, 623.0259037017822, 626.0652313232422, 629.1045589447021, 632.0774574279785, 635.0503559112549, 638.0748624801636, 641.0993690490723, 642.5745513439178, 644.0497336387634]
[20.345, 20.345, 33.91, 33.91, 56.715, 56.715, 62.24, 62.24, 68.855, 68.855, 72.9075, 72.9075, 74.7175, 74.7175, 79.34, 79.34, 80.4375, 80.4375, 81.96, 81.96, 83.8, 83.8, 85.4875, 85.4875, 86.4975, 86.4975, 89.07, 89.07, 89.395, 89.395, 90.9625, 90.9625, 91.1175, 91.1175, 91.34, 91.34, 91.495, 91.495, 91.7825, 91.7825, 92.27, 92.27, 92.2275, 92.2275, 92.2775, 92.2775, 92.5975, 92.5975, 92.8075, 92.8075, 93.0725, 93.0725, 93.2525, 93.2525, 93.525, 93.525, 93.555, 93.555, 93.6, 93.6, 93.705, 93.705, 93.7525, 93.7525, 93.7725, 93.7725, 93.925, 93.925, 93.87, 93.87, 93.93, 93.93, 94.035, 94.035, 94.1475, 94.1475, 94.305, 94.305, 94.5, 94.5, 94.4675, 94.4675, 94.49, 94.49, 94.53, 94.53, 94.65, 94.65, 94.6775, 94.6775, 94.6375, 94.6375, 94.6975, 94.6975, 94.6975, 94.6975, 94.7375, 94.7375, 94.7675, 94.7675, 94.755, 94.755, 94.815, 94.815, 94.8825, 94.8825, 94.975, 94.975, 94.9725, 94.9725, 94.9275, 94.9275, 95.03, 95.03, 95.1375, 95.1375, 95.12, 95.12, 95.18, 95.18, 95.11, 95.11, 95.2175, 95.2175, 95.2975, 95.2975, 95.3325, 95.3325, 95.315, 95.315, 95.3475, 95.3475, 95.35, 95.35, 95.3475, 95.3475, 95.3975, 95.3975, 95.4075, 95.4075, 95.4075, 95.4075, 95.4875, 95.4875, 95.5075, 95.5075, 95.465, 95.465, 95.5475, 95.5475, 95.5475, 95.5475, 95.605, 95.605, 95.61, 95.61, 95.57, 95.57, 95.5575, 95.5575, 95.5775, 95.5775, 95.6225, 95.6225, 95.545, 95.545, 95.57, 95.57, 95.6525, 95.6525, 95.63, 95.63, 95.66, 95.66, 95.69, 95.69, 95.695, 95.695, 95.685, 95.685, 95.6875, 95.6875, 95.72, 95.72, 95.7375, 95.7375, 95.8125, 95.8125, 95.8, 95.8, 95.79, 95.79, 95.7925, 95.7925, 95.795, 95.795, 95.8025, 95.8025, 95.81, 95.81, 95.845, 95.845]
/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.55
Average accuracy final 10 rounds: 96.62175 

4229.800408601761
[3.590367317199707, 7.180734634399414, 10.63374924659729, 14.086763858795166, 17.64733362197876, 21.207903385162354, 24.75559663772583, 28.303289890289307, 31.854926586151123, 35.40656328201294, 38.976954221725464, 42.54734516143799, 45.785444259643555, 49.02354335784912, 52.197959661483765, 55.37237596511841, 58.56443166732788, 61.75648736953735, 65.06044292449951, 68.36439847946167, 71.40032196044922, 74.43624544143677, 77.63399767875671, 80.83174991607666, 84.0678939819336, 87.30403804779053, 90.44207072257996, 93.58010339736938, 96.7727427482605, 99.96538209915161, 103.13190340995789, 106.29842472076416, 109.3699300289154, 112.44143533706665, 115.61734390258789, 118.79325246810913, 121.8318190574646, 124.87038564682007, 128.14345479011536, 131.41652393341064, 134.66362953186035, 137.91073513031006, 140.93741250038147, 143.96408987045288, 147.10919094085693, 150.254292011261, 153.36098384857178, 156.46767568588257, 159.56624245643616, 162.66480922698975, 165.8441126346588, 169.02341604232788, 172.14095449447632, 175.25849294662476, 178.49903082847595, 181.73956871032715, 184.93387484550476, 188.12818098068237, 191.13447642326355, 194.14077186584473, 197.45638990402222, 200.7720079421997, 204.01421236991882, 207.25641679763794, 210.30851483345032, 213.3606128692627, 216.50065803527832, 219.64070320129395, 222.80652570724487, 225.9723482131958, 229.04899859428406, 232.12564897537231, 235.29710221290588, 238.46855545043945, 241.55801486968994, 244.64747428894043, 247.87190532684326, 251.0963363647461, 254.69994282722473, 258.30354928970337, 261.77280044555664, 265.2420516014099, 268.6493082046509, 272.05656480789185, 275.69129037857056, 279.32601594924927, 282.86303424835205, 286.40005254745483, 290.04158186912537, 293.6831111907959, 296.94104647636414, 300.1989817619324, 303.32330203056335, 306.44762229919434, 309.60967922210693, 312.77173614501953, 316.07175612449646, 319.3717761039734, 322.8943030834198, 326.4168300628662, 330.02551078796387, 333.6341915130615, 336.955445766449, 340.2767000198364, 343.42410612106323, 346.57151222229004, 349.78426003456116, 352.9970078468323, 356.1734402179718, 359.3498725891113, 362.42325592041016, 365.496639251709, 368.7380123138428, 371.97938537597656, 375.19241547584534, 378.4054455757141, 381.43768787384033, 384.46993017196655, 387.73009514808655, 390.99026012420654, 394.24316453933716, 397.4960689544678, 400.5908000469208, 403.6855311393738, 407.09891152381897, 410.51229190826416, 413.88115429878235, 417.25001668930054, 420.5684280395508, 423.886839389801, 427.1193127632141, 430.3517861366272, 433.81376671791077, 437.27574729919434, 440.61393666267395, 443.95212602615356, 447.2650291919708, 450.5779323577881, 453.9123909473419, 457.24684953689575, 460.6061806678772, 463.96551179885864, 467.4560477733612, 470.94658374786377, 474.2762379646301, 477.6058921813965, 480.86036443710327, 484.11483669281006, 487.4709575176239, 490.82707834243774, 494.2187285423279, 497.610378742218, 500.90689754486084, 504.20341634750366, 507.4922249317169, 510.7810335159302, 514.0918891429901, 517.40274477005, 520.6930696964264, 523.9833946228027, 527.2632184028625, 530.5430421829224, 534.1739695072174, 537.8048968315125, 541.1972715854645, 544.5896463394165, 547.9855587482452, 551.381471157074, 554.6648418903351, 557.9482126235962, 561.3023982048035, 564.6565837860107, 568.0519516468048, 571.4473195075989, 574.874589920044, 578.301860332489, 581.5905561447144, 584.8792519569397, 588.3006408214569, 591.7220296859741, 595.1115231513977, 598.5010166168213, 601.7803688049316, 605.059720993042, 608.371702671051, 611.6836843490601, 615.0168874263763, 618.3500905036926, 621.6385049819946, 624.9269194602966, 628.233728647232, 631.5405378341675, 634.8932204246521, 638.2459030151367, 641.5671257972717, 644.8883485794067, 648.0605247020721, 651.2327008247375, 654.3285365104675, 657.4243721961975, 659.0174729824066, 660.6105737686157]
[43.335, 43.335, 69.3925, 69.3925, 74.03, 74.03, 77.3525, 77.3525, 79.2675, 79.2675, 80.285, 80.285, 80.9825, 80.9825, 81.37, 81.37, 82.4075, 82.4075, 82.8025, 82.8025, 83.06, 83.06, 83.74, 83.74, 83.75, 83.75, 84.38, 84.38, 84.5325, 84.5325, 84.7475, 84.7475, 84.8, 84.8, 84.94, 84.94, 85.125, 85.125, 85.155, 85.155, 85.225, 85.225, 85.3, 85.3, 85.6, 85.6, 86.0925, 86.0925, 86.62, 86.62, 86.685, 86.685, 87.19, 87.19, 88.11, 88.11, 89.9225, 89.9225, 91.7425, 91.7425, 93.205, 93.205, 93.8125, 93.8125, 94.68, 94.68, 94.8425, 94.8425, 95.395, 95.395, 95.4825, 95.4825, 95.615, 95.615, 95.64, 95.64, 95.625, 95.625, 95.6875, 95.6875, 95.765, 95.765, 95.8675, 95.8675, 95.8925, 95.8925, 95.9125, 95.9125, 95.9925, 95.9925, 96.02, 96.02, 96.0675, 96.0675, 96.11, 96.11, 96.125, 96.125, 96.1, 96.1, 96.185, 96.185, 96.23, 96.23, 96.2375, 96.2375, 96.24, 96.24, 96.3075, 96.3075, 96.365, 96.365, 96.355, 96.355, 96.4, 96.4, 96.395, 96.395, 96.4075, 96.4075, 96.4125, 96.4125, 96.4225, 96.4225, 96.42, 96.42, 96.415, 96.415, 96.4025, 96.4025, 96.455, 96.455, 96.465, 96.465, 96.4375, 96.4375, 96.4975, 96.4975, 96.525, 96.525, 96.5025, 96.5025, 96.4675, 96.4675, 96.5375, 96.5375, 96.5325, 96.5325, 96.5175, 96.5175, 96.5475, 96.5475, 96.5175, 96.5175, 96.505, 96.505, 96.5025, 96.5025, 96.5475, 96.5475, 96.58, 96.58, 96.57, 96.57, 96.5525, 96.5525, 96.5675, 96.5675, 96.5925, 96.5925, 96.5775, 96.5775, 96.5825, 96.5825, 96.5575, 96.5575, 96.5575, 96.5575, 96.545, 96.545, 96.615, 96.615, 96.5975, 96.5975, 96.605, 96.605, 96.5925, 96.5925, 96.63, 96.63, 96.645, 96.645, 96.63, 96.63, 96.6375, 96.6375, 96.635, 96.635, 96.63, 96.63, 96.5525, 96.5525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.288, Test loss: 2.229, Test accuracy: 24.02
Round   1, Train loss: 1.998, Test loss: 1.808, Test accuracy: 70.13
Round   2, Train loss: 1.696, Test loss: 1.692, Test accuracy: 79.59
Round   3, Train loss: 1.648, Test loss: 1.657, Test accuracy: 81.32
Round   4, Train loss: 1.620, Test loss: 1.651, Test accuracy: 81.75
Round   5, Train loss: 1.621, Test loss: 1.645, Test accuracy: 82.23
Round   6, Train loss: 1.610, Test loss: 1.642, Test accuracy: 82.39
Round   7, Train loss: 1.605, Test loss: 1.640, Test accuracy: 82.54
Round   8, Train loss: 1.596, Test loss: 1.637, Test accuracy: 82.61
Round   9, Train loss: 1.530, Test loss: 1.587, Test accuracy: 88.48
Round  10, Train loss: 1.520, Test loss: 1.583, Test accuracy: 88.58
Round  11, Train loss: 1.517, Test loss: 1.577, Test accuracy: 89.16
Round  12, Train loss: 1.516, Test loss: 1.572, Test accuracy: 89.42
Round  13, Train loss: 1.498, Test loss: 1.570, Test accuracy: 89.66
Round  14, Train loss: 1.495, Test loss: 1.569, Test accuracy: 89.71
Round  15, Train loss: 1.499, Test loss: 1.568, Test accuracy: 89.69
Round  16, Train loss: 1.500, Test loss: 1.565, Test accuracy: 89.98
Round  17, Train loss: 1.496, Test loss: 1.565, Test accuracy: 90.01
Round  18, Train loss: 1.494, Test loss: 1.564, Test accuracy: 90.02
Round  19, Train loss: 1.500, Test loss: 1.563, Test accuracy: 90.17
Round  20, Train loss: 1.490, Test loss: 1.563, Test accuracy: 90.18
Round  21, Train loss: 1.491, Test loss: 1.563, Test accuracy: 90.15
Round  22, Train loss: 1.491, Test loss: 1.562, Test accuracy: 90.19
Round  23, Train loss: 1.490, Test loss: 1.562, Test accuracy: 90.19
Round  24, Train loss: 1.487, Test loss: 1.561, Test accuracy: 90.25
Round  25, Train loss: 1.486, Test loss: 1.561, Test accuracy: 90.27
Round  26, Train loss: 1.489, Test loss: 1.561, Test accuracy: 90.25
Round  27, Train loss: 1.488, Test loss: 1.561, Test accuracy: 90.25
Round  28, Train loss: 1.488, Test loss: 1.561, Test accuracy: 90.27
Round  29, Train loss: 1.488, Test loss: 1.561, Test accuracy: 90.28
Round  30, Train loss: 1.488, Test loss: 1.560, Test accuracy: 90.31
Round  31, Train loss: 1.490, Test loss: 1.560, Test accuracy: 90.26
Round  32, Train loss: 1.487, Test loss: 1.560, Test accuracy: 90.32
Round  33, Train loss: 1.485, Test loss: 1.561, Test accuracy: 90.31
Round  34, Train loss: 1.483, Test loss: 1.560, Test accuracy: 90.30
Round  35, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.30
Round  36, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.33
Round  37, Train loss: 1.486, Test loss: 1.560, Test accuracy: 90.36
Round  38, Train loss: 1.486, Test loss: 1.560, Test accuracy: 90.33
Round  39, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.31
Round  40, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.32
Round  41, Train loss: 1.488, Test loss: 1.560, Test accuracy: 90.33
Round  42, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.31
Round  43, Train loss: 1.482, Test loss: 1.560, Test accuracy: 90.32
Round  44, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.31
Round  45, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.31
Round  46, Train loss: 1.484, Test loss: 1.560, Test accuracy: 90.30
Round  47, Train loss: 1.483, Test loss: 1.560, Test accuracy: 90.29
Round  48, Train loss: 1.483, Test loss: 1.560, Test accuracy: 90.31
Round  49, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.33
Round  50, Train loss: 1.485, Test loss: 1.560, Test accuracy: 90.31
Round  51, Train loss: 1.487, Test loss: 1.559, Test accuracy: 90.31
Round  52, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.33
Round  53, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.33
Round  54, Train loss: 1.486, Test loss: 1.559, Test accuracy: 90.33
Round  55, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.32
Round  56, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.34
Round  57, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.34
Round  58, Train loss: 1.486, Test loss: 1.559, Test accuracy: 90.34
Round  59, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.31
Round  60, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.33
Round  61, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.32
Round  62, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.32
Round  63, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.34
Round  64, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.33
Round  65, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.34
Round  66, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.37
Round  67, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.38
Round  68, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.39
Round  69, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.40
Round  70, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.41
Round  71, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.40
Round  72, Train loss: 1.485, Test loss: 1.559, Test accuracy: 90.38
Round  73, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.40
Round  74, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.38
Round  75, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.37
Round  76, Train loss: 1.485, Test loss: 1.559, Test accuracy: 90.38
Round  77, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.39
Round  78, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.35
Round  79, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.36
Round  80, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.35
Round  81, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.38
Round  82, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.37
Round  83, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.36
Round  84, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.38
Round  85, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.42
Round  86, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.40
Round  87, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.41
Round  88, Train loss: 1.481, Test loss: 1.559, Test accuracy: 90.39
Round  89, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.40
Round  90, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.40
Round  91, Train loss: 1.483, Test loss: 1.559, Test accuracy: 90.39
Round  92, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.37
Round  93, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.36
Round  94, Train loss: 1.481, Test loss: 1.559, Test accuracy: 90.36
Round  95, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.36
Round  96, Train loss: 1.479, Test loss: 1.559, Test accuracy: 90.33
Round  97, Train loss: 1.479, Test loss: 1.559, Test accuracy: 90.34
Round  98, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.35
Round  99, Train loss: 1.484, Test loss: 1.559, Test accuracy: 90.36/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.482, Test loss: 1.559, Test accuracy: 90.34
Average accuracy final 10 rounds: 90.36250000000001 

4672.535085201263
[3.6477677822113037, 7.295535564422607, 11.023440599441528, 14.75134563446045, 18.457818508148193, 22.164291381835938, 25.91098189353943, 29.65767240524292, 33.35524797439575, 37.052823543548584, 40.82396054267883, 44.59509754180908, 48.339759349823, 52.084421157836914, 55.795087814331055, 59.505754470825195, 63.19267773628235, 66.8796010017395, 70.62231922149658, 74.36503744125366, 78.18448400497437, 82.00393056869507, 85.82644414901733, 89.6489577293396, 93.40296506881714, 97.15697240829468, 100.90161943435669, 104.6462664604187, 108.38823127746582, 112.13019609451294, 115.85307741165161, 119.57595872879028, 123.31617903709412, 127.05639934539795, 130.84057068824768, 134.6247420310974, 138.38329696655273, 142.14185190200806, 145.8845899105072, 149.62732791900635, 153.33928632736206, 157.05124473571777, 160.7976176738739, 164.54399061203003, 168.4353108406067, 172.32663106918335, 176.0479772090912, 179.76932334899902, 183.63338255882263, 187.49744176864624, 191.26957893371582, 195.0417160987854, 198.79753065109253, 202.55334520339966, 206.2267665863037, 209.90018796920776, 213.62999510765076, 217.35980224609375, 221.1559932231903, 224.95218420028687, 228.7015359401703, 232.4508876800537, 236.186616897583, 239.9223461151123, 243.69170928001404, 247.46107244491577, 251.22704148292542, 254.99301052093506, 258.73279881477356, 262.47258710861206, 266.16510462760925, 269.85762214660645, 273.60419511795044, 277.35076808929443, 281.17618703842163, 285.0016059875488, 288.7417709827423, 292.4819359779358, 296.21289134025574, 299.9438467025757, 303.6597716808319, 307.37569665908813, 311.16312885284424, 314.95056104660034, 318.77779626846313, 322.6050314903259, 326.4124286174774, 330.2198257446289, 334.1548888683319, 338.0899519920349, 342.0176899433136, 345.9454278945923, 349.76782298088074, 353.5902180671692, 357.42740654945374, 361.2645950317383, 365.1538531780243, 369.0431113243103, 372.9365222454071, 376.8299331665039, 380.6816260814667, 384.53331899642944, 388.3946123123169, 392.25590562820435, 396.0961844921112, 399.93646335601807, 403.7384407520294, 407.54041814804077, 411.46809911727905, 415.39578008651733, 419.31293201446533, 423.23008394241333, 427.2059087753296, 431.18173360824585, 435.13049483299255, 439.07925605773926, 443.0300512313843, 446.9808464050293, 450.9241564273834, 454.86746644973755, 458.85717272758484, 462.84687900543213, 466.7568004131317, 470.6667218208313, 474.59265303611755, 478.5185842514038, 482.5102422237396, 486.50190019607544, 490.45590829849243, 494.4099164009094, 498.34374356269836, 502.2775707244873, 506.26553606987, 510.2535014152527, 514.2384948730469, 518.2234883308411, 522.129894733429, 526.0363011360168, 529.9530589580536, 533.8698167800903, 537.9027469158173, 541.9356770515442, 546.0047438144684, 550.0738105773926, 554.008022069931, 557.9422335624695, 561.7785067558289, 565.6147799491882, 569.4475045204163, 573.2802290916443, 577.1063916683197, 580.9325542449951, 584.6909408569336, 588.4493274688721, 592.3298344612122, 596.2103414535522, 600.319256067276, 604.4281706809998, 608.448582649231, 612.4689946174622, 616.4017872810364, 620.3345799446106, 624.2719588279724, 628.2093377113342, 632.2160801887512, 636.2228226661682, 640.2274332046509, 644.2320437431335, 648.2218885421753, 652.211733341217, 656.2252488136292, 660.2387642860413, 664.2878377437592, 668.336911201477, 672.2978353500366, 676.2587594985962, 680.1104273796082, 683.9620952606201, 687.8248264789581, 691.6875576972961, 695.7054460048676, 699.723334312439, 703.7335298061371, 707.7437252998352, 711.6379017829895, 715.5320782661438, 719.4038910865784, 723.2757039070129, 727.1975705623627, 731.1194372177124, 735.0705251693726, 739.0216131210327, 742.9652879238129, 746.908962726593, 750.7862122058868, 754.6634616851807, 758.728150844574, 762.7928400039673, 766.7982473373413, 770.8036546707153, 772.6889853477478, 774.5743160247803]
[24.015, 24.015, 70.1325, 70.1325, 79.59, 79.59, 81.3175, 81.3175, 81.745, 81.745, 82.2325, 82.2325, 82.39, 82.39, 82.5425, 82.5425, 82.6075, 82.6075, 88.48, 88.48, 88.5775, 88.5775, 89.1625, 89.1625, 89.415, 89.415, 89.655, 89.655, 89.71, 89.71, 89.685, 89.685, 89.9775, 89.9775, 90.01, 90.01, 90.02, 90.02, 90.1725, 90.1725, 90.1825, 90.1825, 90.1525, 90.1525, 90.195, 90.195, 90.19, 90.19, 90.2525, 90.2525, 90.27, 90.27, 90.2525, 90.2525, 90.255, 90.255, 90.27, 90.27, 90.28, 90.28, 90.315, 90.315, 90.2575, 90.2575, 90.3175, 90.3175, 90.315, 90.315, 90.3025, 90.3025, 90.295, 90.295, 90.3325, 90.3325, 90.3625, 90.3625, 90.325, 90.325, 90.3075, 90.3075, 90.3225, 90.3225, 90.3275, 90.3275, 90.31, 90.31, 90.3225, 90.3225, 90.3075, 90.3075, 90.315, 90.315, 90.3025, 90.3025, 90.2925, 90.2925, 90.3075, 90.3075, 90.325, 90.325, 90.315, 90.315, 90.315, 90.315, 90.3275, 90.3275, 90.3325, 90.3325, 90.3275, 90.3275, 90.32, 90.32, 90.345, 90.345, 90.3425, 90.3425, 90.3375, 90.3375, 90.315, 90.315, 90.325, 90.325, 90.3175, 90.3175, 90.32, 90.32, 90.345, 90.345, 90.3325, 90.3325, 90.3425, 90.3425, 90.37, 90.37, 90.3775, 90.3775, 90.3925, 90.3925, 90.4025, 90.4025, 90.41, 90.41, 90.3975, 90.3975, 90.3775, 90.3775, 90.3975, 90.3975, 90.375, 90.375, 90.3725, 90.3725, 90.38, 90.38, 90.3875, 90.3875, 90.3525, 90.3525, 90.36, 90.36, 90.35, 90.35, 90.3775, 90.3775, 90.3725, 90.3725, 90.3625, 90.3625, 90.3775, 90.3775, 90.415, 90.415, 90.4025, 90.4025, 90.4075, 90.4075, 90.39, 90.39, 90.4, 90.4, 90.4, 90.4, 90.39, 90.39, 90.3675, 90.3675, 90.355, 90.355, 90.36, 90.36, 90.355, 90.355, 90.335, 90.335, 90.345, 90.345, 90.3525, 90.3525, 90.365, 90.365, 90.34, 90.34]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.711, Test loss: 2.295, Test accuracy: 17.27
Round   1, Train loss: 1.613, Test loss: 2.262, Test accuracy: 39.23
Round   2, Train loss: 1.460, Test loss: 2.172, Test accuracy: 50.86
Round   3, Train loss: 1.338, Test loss: 2.082, Test accuracy: 63.71
Round   4, Train loss: 1.254, Test loss: 2.014, Test accuracy: 71.25
Round   5, Train loss: 1.221, Test loss: 1.982, Test accuracy: 74.48
Round   6, Train loss: 1.216, Test loss: 1.960, Test accuracy: 77.58
Round   7, Train loss: 1.207, Test loss: 1.948, Test accuracy: 78.28
Round   8, Train loss: 1.197, Test loss: 1.938, Test accuracy: 79.77
Round   9, Train loss: 1.169, Test loss: 1.925, Test accuracy: 82.10
Round  10, Train loss: 1.152, Test loss: 1.913, Test accuracy: 82.79
Round  11, Train loss: 1.146, Test loss: 1.905, Test accuracy: 83.14
Round  12, Train loss: 1.146, Test loss: 1.898, Test accuracy: 84.08
Round  13, Train loss: 1.135, Test loss: 1.893, Test accuracy: 84.53
Round  14, Train loss: 1.130, Test loss: 1.887, Test accuracy: 84.96
Round  15, Train loss: 1.134, Test loss: 1.884, Test accuracy: 85.28
Round  16, Train loss: 1.127, Test loss: 1.880, Test accuracy: 85.28
Round  17, Train loss: 1.133, Test loss: 1.877, Test accuracy: 85.16
Round  18, Train loss: 1.128, Test loss: 1.876, Test accuracy: 84.88
Round  19, Train loss: 1.123, Test loss: 1.873, Test accuracy: 84.52
Round  20, Train loss: 1.128, Test loss: 1.873, Test accuracy: 84.30
Round  21, Train loss: 1.121, Test loss: 1.872, Test accuracy: 83.82
Round  22, Train loss: 1.120, Test loss: 1.870, Test accuracy: 83.75
Round  23, Train loss: 1.122, Test loss: 1.869, Test accuracy: 83.59
Round  24, Train loss: 1.123, Test loss: 1.870, Test accuracy: 83.08
Round  25, Train loss: 1.121, Test loss: 1.869, Test accuracy: 83.17
Round  26, Train loss: 1.116, Test loss: 1.868, Test accuracy: 83.05
Round  27, Train loss: 1.117, Test loss: 1.867, Test accuracy: 82.92
Round  28, Train loss: 1.118, Test loss: 1.867, Test accuracy: 82.53
Round  29, Train loss: 1.116, Test loss: 1.866, Test accuracy: 82.45
Round  30, Train loss: 1.116, Test loss: 1.864, Test accuracy: 82.38
Round  31, Train loss: 1.117, Test loss: 1.865, Test accuracy: 81.97
Round  32, Train loss: 1.118, Test loss: 1.866, Test accuracy: 81.68
Round  33, Train loss: 1.116, Test loss: 1.865, Test accuracy: 81.58
Round  34, Train loss: 1.117, Test loss: 1.866, Test accuracy: 81.35
Round  35, Train loss: 1.115, Test loss: 1.866, Test accuracy: 80.93
Round  36, Train loss: 1.115, Test loss: 1.866, Test accuracy: 80.80
Round  37, Train loss: 1.111, Test loss: 1.866, Test accuracy: 80.50
Round  38, Train loss: 1.115, Test loss: 1.866, Test accuracy: 80.27
Round  39, Train loss: 1.116, Test loss: 1.867, Test accuracy: 80.08
Round  40, Train loss: 1.113, Test loss: 1.866, Test accuracy: 79.97
Round  41, Train loss: 1.114, Test loss: 1.866, Test accuracy: 79.87
Round  42, Train loss: 1.115, Test loss: 1.867, Test accuracy: 79.53
Round  43, Train loss: 1.113, Test loss: 1.868, Test accuracy: 79.32
Round  44, Train loss: 1.115, Test loss: 1.869, Test accuracy: 78.96
Round  45, Train loss: 1.116, Test loss: 1.869, Test accuracy: 78.96
Round  46, Train loss: 1.114, Test loss: 1.869, Test accuracy: 78.66
Round  47, Train loss: 1.116, Test loss: 1.870, Test accuracy: 78.30
Round  48, Train loss: 1.113, Test loss: 1.871, Test accuracy: 78.19
Round  49, Train loss: 1.109, Test loss: 1.871, Test accuracy: 77.86
Round  50, Train loss: 1.114, Test loss: 1.873, Test accuracy: 77.57
Round  51, Train loss: 1.114, Test loss: 1.873, Test accuracy: 77.47
Round  52, Train loss: 1.114, Test loss: 1.874, Test accuracy: 77.33
Round  53, Train loss: 1.114, Test loss: 1.875, Test accuracy: 77.13
Round  54, Train loss: 1.110, Test loss: 1.875, Test accuracy: 77.14
Round  55, Train loss: 1.112, Test loss: 1.875, Test accuracy: 76.96
Round  56, Train loss: 1.113, Test loss: 1.875, Test accuracy: 76.97
Round  57, Train loss: 1.112, Test loss: 1.877, Test accuracy: 76.47
Round  58, Train loss: 1.112, Test loss: 1.877, Test accuracy: 76.48
Round  59, Train loss: 1.111, Test loss: 1.877, Test accuracy: 76.33
Round  60, Train loss: 1.108, Test loss: 1.877, Test accuracy: 76.08
Round  61, Train loss: 1.115, Test loss: 1.879, Test accuracy: 75.97
Round  62, Train loss: 1.113, Test loss: 1.879, Test accuracy: 75.88
Round  63, Train loss: 1.111, Test loss: 1.880, Test accuracy: 75.83
Round  64, Train loss: 1.113, Test loss: 1.881, Test accuracy: 75.69
Round  65, Train loss: 1.111, Test loss: 1.881, Test accuracy: 75.49
Round  66, Train loss: 1.109, Test loss: 1.881, Test accuracy: 75.53
Round  67, Train loss: 1.111, Test loss: 1.881, Test accuracy: 75.23
Round  68, Train loss: 1.109, Test loss: 1.882, Test accuracy: 75.03
Round  69, Train loss: 1.110, Test loss: 1.882, Test accuracy: 74.97
Round  70, Train loss: 1.109, Test loss: 1.882, Test accuracy: 74.83
Round  71, Train loss: 1.108, Test loss: 1.882, Test accuracy: 74.72
Round  72, Train loss: 1.109, Test loss: 1.882, Test accuracy: 74.47
Round  73, Train loss: 1.109, Test loss: 1.883, Test accuracy: 74.41
Round  74, Train loss: 1.110, Test loss: 1.883, Test accuracy: 74.09
Round  75, Train loss: 1.112, Test loss: 1.884, Test accuracy: 73.94
Round  76, Train loss: 1.109, Test loss: 1.884, Test accuracy: 73.91
Round  77, Train loss: 1.109, Test loss: 1.884, Test accuracy: 73.82
Round  78, Train loss: 1.111, Test loss: 1.885, Test accuracy: 73.69
Round  79, Train loss: 1.108, Test loss: 1.884, Test accuracy: 73.62
Round  80, Train loss: 1.109, Test loss: 1.885, Test accuracy: 73.54
Round  81, Train loss: 1.111, Test loss: 1.886, Test accuracy: 73.49
Round  82, Train loss: 1.110, Test loss: 1.886, Test accuracy: 73.39
Round  83, Train loss: 1.110, Test loss: 1.886, Test accuracy: 73.30
Round  84, Train loss: 1.108, Test loss: 1.886, Test accuracy: 73.27
Round  85, Train loss: 1.108, Test loss: 1.887, Test accuracy: 73.09
Round  86, Train loss: 1.110, Test loss: 1.888, Test accuracy: 73.08
Round  87, Train loss: 1.110, Test loss: 1.888, Test accuracy: 72.92
Round  88, Train loss: 1.107, Test loss: 1.888, Test accuracy: 72.93
Round  89, Train loss: 1.109, Test loss: 1.888, Test accuracy: 72.90
Round  90, Train loss: 1.112, Test loss: 1.889, Test accuracy: 72.67
Round  91, Train loss: 1.112, Test loss: 1.889, Test accuracy: 72.46
Round  92, Train loss: 1.107, Test loss: 1.889, Test accuracy: 72.46
Round  93, Train loss: 1.108, Test loss: 1.890, Test accuracy: 72.29
Round  94, Train loss: 1.108, Test loss: 1.890, Test accuracy: 72.06
Round  95, Train loss: 1.108, Test loss: 1.891, Test accuracy: 71.94
Round  96, Train loss: 1.108, Test loss: 1.891, Test accuracy: 71.99
Round  97, Train loss: 1.108, Test loss: 1.891, Test accuracy: 71.89
Round  98, Train loss: 1.108, Test loss: 1.892, Test accuracy: 71.71
Round  99, Train loss: 1.109, Test loss: 1.891, Test accuracy: 71.72
Final Round, Train loss: 1.108, Test loss: 1.892, Test accuracy: 71.54
Average accuracy final 10 rounds: 72.11999999999999
1534.7390975952148
[]
[17.275, 39.225, 50.858333333333334, 63.708333333333336, 71.25, 74.48333333333333, 77.58333333333333, 78.28333333333333, 79.76666666666667, 82.1, 82.79166666666667, 83.14166666666667, 84.075, 84.525, 84.95833333333333, 85.28333333333333, 85.28333333333333, 85.15833333333333, 84.875, 84.51666666666667, 84.3, 83.81666666666666, 83.75, 83.59166666666667, 83.075, 83.175, 83.05, 82.925, 82.53333333333333, 82.45, 82.375, 81.96666666666667, 81.68333333333334, 81.58333333333333, 81.35, 80.93333333333334, 80.8, 80.5, 80.26666666666667, 80.075, 79.96666666666667, 79.86666666666666, 79.53333333333333, 79.31666666666666, 78.95833333333333, 78.95833333333333, 78.65833333333333, 78.3, 78.19166666666666, 77.85833333333333, 77.56666666666666, 77.475, 77.33333333333333, 77.13333333333334, 77.14166666666667, 76.95833333333333, 76.96666666666667, 76.475, 76.48333333333333, 76.325, 76.08333333333333, 75.975, 75.88333333333334, 75.825, 75.69166666666666, 75.49166666666666, 75.53333333333333, 75.23333333333333, 75.03333333333333, 74.975, 74.825, 74.725, 74.46666666666667, 74.40833333333333, 74.09166666666667, 73.94166666666666, 73.90833333333333, 73.81666666666666, 73.69166666666666, 73.625, 73.54166666666667, 73.49166666666666, 73.39166666666667, 73.3, 73.26666666666667, 73.09166666666667, 73.08333333333333, 72.925, 72.93333333333334, 72.9, 72.675, 72.45833333333333, 72.45833333333333, 72.29166666666667, 72.05833333333334, 71.94166666666666, 71.99166666666666, 71.89166666666667, 71.70833333333333, 71.725, 71.54166666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.282, Test loss: 2.265, Test accuracy: 23.13
Round   1, Train loss: 2.164, Test loss: 2.214, Test accuracy: 28.43
Round   2, Train loss: 2.127, Test loss: 2.224, Test accuracy: 22.98
Round   3, Train loss: 1.853, Test loss: 2.134, Test accuracy: 39.37
Round   4, Train loss: 1.874, Test loss: 2.095, Test accuracy: 43.64
Round   5, Train loss: 1.684, Test loss: 2.071, Test accuracy: 44.30
Round   6, Train loss: 1.325, Test loss: 2.071, Test accuracy: 45.98
Round   7, Train loss: 2.049, Test loss: 2.113, Test accuracy: 37.53
Round   8, Train loss: 1.578, Test loss: 2.113, Test accuracy: 37.59
Round   9, Train loss: 1.703, Test loss: 2.078, Test accuracy: 40.42
Round  10, Train loss: 1.680, Test loss: 2.154, Test accuracy: 32.77
Round  11, Train loss: 1.288, Test loss: 2.069, Test accuracy: 39.87
Round  12, Train loss: 0.862, Test loss: 1.993, Test accuracy: 47.49
Round  13, Train loss: 1.079, Test loss: 2.009, Test accuracy: 45.58
Round  14, Train loss: 0.991, Test loss: 2.015, Test accuracy: 46.20
Round  15, Train loss: 0.110, Test loss: 1.955, Test accuracy: 51.37
Round  16, Train loss: 1.585, Test loss: 1.951, Test accuracy: 52.55
Round  17, Train loss: 0.986, Test loss: 1.926, Test accuracy: 55.59
Round  18, Train loss: 0.713, Test loss: 1.889, Test accuracy: 59.05
Round  19, Train loss: 1.159, Test loss: 1.884, Test accuracy: 59.64
Round  20, Train loss: 0.773, Test loss: 1.868, Test accuracy: 60.64
Round  21, Train loss: 0.519, Test loss: 1.809, Test accuracy: 66.17
Round  22, Train loss: 0.973, Test loss: 1.773, Test accuracy: 69.27
Round  23, Train loss: 0.885, Test loss: 1.744, Test accuracy: 72.16
Round  24, Train loss: -0.128, Test loss: 1.734, Test accuracy: 72.98
Round  25, Train loss: 0.409, Test loss: 1.735, Test accuracy: 72.85
Round  26, Train loss: -0.362, Test loss: 1.738, Test accuracy: 72.50
Round  27, Train loss: 0.824, Test loss: 1.732, Test accuracy: 73.24
Round  28, Train loss: 0.788, Test loss: 1.722, Test accuracy: 74.24
Round  29, Train loss: 0.719, Test loss: 1.710, Test accuracy: 75.42
Round  30, Train loss: 0.673, Test loss: 1.722, Test accuracy: 74.11
Round  31, Train loss: 0.771, Test loss: 1.723, Test accuracy: 74.04
Round  32, Train loss: 0.627, Test loss: 1.705, Test accuracy: 75.71
Round  33, Train loss: 0.505, Test loss: 1.695, Test accuracy: 76.61
Round  34, Train loss: 0.478, Test loss: 1.694, Test accuracy: 76.71
Round  35, Train loss: 0.762, Test loss: 1.695, Test accuracy: 76.53
Round  36, Train loss: 0.509, Test loss: 1.689, Test accuracy: 77.15
Round  37, Train loss: 0.308, Test loss: 1.676, Test accuracy: 78.47
Round  38, Train loss: 0.660, Test loss: 1.688, Test accuracy: 77.25
Round  39, Train loss: 0.544, Test loss: 1.696, Test accuracy: 76.49
Round  40, Train loss: 0.331, Test loss: 1.696, Test accuracy: 76.47
Round  41, Train loss: 0.626, Test loss: 1.692, Test accuracy: 76.88
Round  42, Train loss: 0.299, Test loss: 1.690, Test accuracy: 77.09
Round  43, Train loss: 0.390, Test loss: 1.690, Test accuracy: 77.10
Round  44, Train loss: 0.311, Test loss: 1.686, Test accuracy: 77.43
Round  45, Train loss: 0.027, Test loss: 1.691, Test accuracy: 76.94
Round  46, Train loss: -0.049, Test loss: 1.688, Test accuracy: 77.25
Round  47, Train loss: 0.634, Test loss: 1.684, Test accuracy: 77.66
Round  48, Train loss: 0.165, Test loss: 1.687, Test accuracy: 77.42
Round  49, Train loss: -0.212, Test loss: 1.686, Test accuracy: 77.42
Round  50, Train loss: 0.596, Test loss: 1.668, Test accuracy: 79.35
Round  51, Train loss: 0.432, Test loss: 1.682, Test accuracy: 77.86
Round  52, Train loss: 0.467, Test loss: 1.676, Test accuracy: 78.42
Round  53, Train loss: 0.071, Test loss: 1.673, Test accuracy: 78.84
Round  54, Train loss: 0.914, Test loss: 1.680, Test accuracy: 78.08
Round  55, Train loss: 0.588, Test loss: 1.684, Test accuracy: 77.66
Round  56, Train loss: 0.515, Test loss: 1.676, Test accuracy: 78.47
Round  57, Train loss: 0.310, Test loss: 1.689, Test accuracy: 77.10
Round  58, Train loss: 0.354, Test loss: 1.687, Test accuracy: 77.37
Round  59, Train loss: -0.068, Test loss: 1.684, Test accuracy: 77.66
Round  60, Train loss: 0.720, Test loss: 1.676, Test accuracy: 78.52
Round  61, Train loss: 0.275, Test loss: 1.683, Test accuracy: 77.81
Round  62, Train loss: 0.560, Test loss: 1.682, Test accuracy: 77.86
Round  63, Train loss: 0.244, Test loss: 1.682, Test accuracy: 77.92
Round  64, Train loss: 0.660, Test loss: 1.676, Test accuracy: 78.47
Round  65, Train loss: 0.111, Test loss: 1.680, Test accuracy: 78.14
Round  66, Train loss: 0.640, Test loss: 1.683, Test accuracy: 77.69
Round  67, Train loss: 0.542, Test loss: 1.684, Test accuracy: 77.68
Round  68, Train loss: 0.567, Test loss: 1.673, Test accuracy: 78.82
Round  69, Train loss: 0.759, Test loss: 1.671, Test accuracy: 79.05
Round  70, Train loss: 0.493, Test loss: 1.670, Test accuracy: 79.08
Round  71, Train loss: 0.412, Test loss: 1.671, Test accuracy: 79.06
Round  72, Train loss: 0.527, Test loss: 1.666, Test accuracy: 79.53
Round  73, Train loss: 0.248, Test loss: 1.669, Test accuracy: 79.15
Round  74, Train loss: -0.113, Test loss: 1.668, Test accuracy: 79.27
Round  75, Train loss: 0.729, Test loss: 1.661, Test accuracy: 80.04
Round  76, Train loss: 0.532, Test loss: 1.669, Test accuracy: 79.22
Round  77, Train loss: 0.690, Test loss: 1.676, Test accuracy: 78.51
Round  78, Train loss: 0.540, Test loss: 1.671, Test accuracy: 78.97
Round  79, Train loss: 0.609, Test loss: 1.665, Test accuracy: 79.63
Round  80, Train loss: 0.343, Test loss: 1.662, Test accuracy: 80.00
Round  81, Train loss: 0.512, Test loss: 1.654, Test accuracy: 80.71
Round  82, Train loss: 0.527, Test loss: 1.661, Test accuracy: 79.99
Round  83, Train loss: 0.648, Test loss: 1.674, Test accuracy: 78.64
Round  84, Train loss: 0.480, Test loss: 1.658, Test accuracy: 80.20
Round  85, Train loss: 0.733, Test loss: 1.666, Test accuracy: 79.46
Round  86, Train loss: 0.731, Test loss: 1.680, Test accuracy: 77.97
Round  87, Train loss: 0.761, Test loss: 1.673, Test accuracy: 78.75
Round  88, Train loss: 0.439, Test loss: 1.681, Test accuracy: 77.97
Round  89, Train loss: 0.581, Test loss: 1.683, Test accuracy: 77.81
Round  90, Train loss: 0.872, Test loss: 1.682, Test accuracy: 77.89
Round  91, Train loss: 0.747, Test loss: 1.667, Test accuracy: 79.36
Round  92, Train loss: 0.585, Test loss: 1.668, Test accuracy: 79.26
Round  93, Train loss: 0.632, Test loss: 1.668, Test accuracy: 79.30
Round  94, Train loss: 0.599, Test loss: 1.658, Test accuracy: 80.28
Round  95, Train loss: 0.670, Test loss: 1.659, Test accuracy: 80.19
Round  96, Train loss: 0.635, Test loss: 1.660, Test accuracy: 80.08
Round  97, Train loss: 0.542, Test loss: 1.651, Test accuracy: 80.98
Round  98, Train loss: 0.809, Test loss: 1.660, Test accuracy: 80.08
Round  99, Train loss: 0.643, Test loss: 1.658, Test accuracy: 80.29
Final Round, Train loss: 1.626, Test loss: 1.621, Test accuracy: 84.39
Average accuracy final 10 rounds: 79.76875000000001
Average global accuracy final 10 rounds: 79.76875000000001
3520.310885667801
[]
[23.13, 28.4275, 22.98, 39.3675, 43.64, 44.305, 45.985, 37.53, 37.595, 40.4175, 32.7675, 39.8675, 47.4875, 45.58, 46.2, 51.365, 52.5525, 55.5925, 59.0475, 59.64, 60.64, 66.175, 69.2675, 72.16, 72.98, 72.85, 72.5025, 73.2425, 74.2425, 75.425, 74.11, 74.0425, 75.7075, 76.615, 76.71, 76.53, 77.1525, 78.465, 77.25, 76.4925, 76.475, 76.88, 77.095, 77.1025, 77.4325, 76.94, 77.245, 77.6575, 77.42, 77.425, 79.3475, 77.8575, 78.425, 78.84, 78.0825, 77.66, 78.4675, 77.1, 77.3675, 77.66, 78.515, 77.805, 77.865, 77.9225, 78.475, 78.14, 77.6925, 77.6825, 78.8175, 79.05, 79.075, 79.055, 79.5325, 79.1525, 79.27, 80.04, 79.2175, 78.5125, 78.9725, 79.63, 79.995, 80.7125, 79.9925, 78.64, 80.205, 79.46, 77.97, 78.7525, 77.9675, 77.815, 77.885, 79.3625, 79.26, 79.295, 80.275, 80.1925, 80.075, 80.98, 80.075, 80.2875, 84.395]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.68
Round   0, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.67
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.67
Round   1, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.68
Round   2, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.71
Round   2, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.73
Round   3, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.69
Round   3, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.70
Round   4, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.70
Round   4, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.73
Round   5, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.71
Round   5, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.74
Round   6, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.73
Round   6, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round   7, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.74
Round   7, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78
Round   8, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.73
Round   8, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78
Round   9, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round   9, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.78
Round  10, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.74
Round  10, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  11, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round  11, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  12, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.75
Round  12, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  13, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.79
Round  13, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  14, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.81
Round  14, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  15, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.80
Round  15, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  16, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.79
Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.76
Round  17, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.79
Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.81
Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.79
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.82
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.81
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.81
Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.77
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.82
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.81
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.84
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.83
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.83
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.83
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.85
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.87
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.87
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.87
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.88
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.86
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.88
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.88
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.87
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.89
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.90
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.89
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.91
Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.90
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.99
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.93
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.99
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.95
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 12.99
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 12.98
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.04
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.04
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.01
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.00
Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.01
Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.00
Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.02
Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.02
Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.03
Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.07
Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.05
Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.12
Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.06
Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.08
Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.06
Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.05
Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.04
Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 13.01
Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.04
Round  58, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 13.01
Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 13.00
Round  59, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.94
Round  60, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  60, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.92
Round  61, Train loss: 2.302, Test loss: 2.301, Test accuracy: 12.96
Round  61, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 12.97
Round  62, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  62, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.01
Round  63, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  63, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.92
Round  64, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.98
Round  64, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.96
Round  65, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.97
Round  65, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 12.99
Round  66, Train loss: 2.301, Test loss: 2.301, Test accuracy: 12.99
Round  66, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.02
Round  67, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.03
Round  67, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.10
Round  68, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.07
Round  68, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.13
Round  69, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.11
Round  69, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.17
Round  70, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.15
Round  70, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.18
Round  71, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.16
Round  71, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.17
Round  72, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.19
Round  72, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.23
Round  73, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.21
Round  73, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.24
Round  74, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.23
Round  74, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.21
Round  75, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.25
Round  75, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.30
Round  76, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.27
Round  76, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.29
Round  77, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.29
Round  77, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.31
Round  78, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.31
Round  78, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.37
Round  79, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.34
Round  79, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.35
Round  80, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.36
Round  80, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.38
Round  81, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.38
Round  81, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.37
Round  82, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.41
Round  82, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.48
Round  83, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.41
Round  83, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.48
Round  84, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.45
Round  84, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.55
Round  85, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.47
Round  85, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.57
Round  86, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.53
Round  86, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.61
Round  87, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.59
Round  87, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.61
Round  88, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.60
Round  88, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.62
Round  89, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.63
Round  89, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.61
Round  90, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.62
Round  90, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.62
Round  91, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.62
Round  91, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.62
Round  92, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.59
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.59
Round  93, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.58
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.60
Round  94, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.56
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.53
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.55
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.52
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.56
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.56/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.55
Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.52
Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.52
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.50
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.50
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.50
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.51
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 13.50
Average accuracy final 10 rounds: 13.566499999999998 

Average global accuracy final 10 rounds: 13.5555 

4456.784124851227
[3.563004970550537, 6.844911575317383, 10.113284349441528, 13.369019985198975, 16.777698755264282, 19.985750198364258, 23.090543031692505, 26.355243921279907, 29.861509084701538, 33.335506200790405, 37.133392333984375, 40.604469299316406, 44.275830030441284, 47.77281093597412, 51.54963541030884, 55.085001707077026, 58.6940541267395, 62.29011154174805, 65.71761894226074, 69.33883023262024, 72.88821411132812, 76.63048911094666, 80.85748505592346, 84.96566438674927, 89.14780497550964, 93.43360352516174, 97.6964704990387, 101.84296250343323, 105.91579961776733, 110.17804551124573, 114.43949675559998, 118.58232879638672, 122.64569807052612, 126.83494901657104, 131.11475229263306, 135.3394103050232, 139.31694984436035, 143.61375379562378, 147.8525369167328, 152.1171452999115, 156.20720553398132, 160.37719750404358, 164.4779567718506, 168.4903745651245, 172.50467443466187, 175.996084690094, 179.9003505706787, 183.76956009864807, 187.6663055419922, 191.6525375843048, 195.58306908607483, 199.1410427093506, 202.9242503643036, 206.6460633277893, 210.1560537815094, 213.83495235443115, 217.45771312713623, 220.9034388065338, 224.51352429389954, 228.77167463302612, 232.43568968772888, 236.59611701965332, 240.54107213020325, 244.53014278411865, 248.35969281196594, 252.0544831752777, 255.92817854881287, 259.5439975261688, 263.4795219898224, 267.2376878261566, 271.03114008903503, 274.8858449459076, 278.4307191371918, 282.0954644680023, 286.0071361064911, 289.9729504585266, 293.6545720100403, 297.3953070640564, 301.06560492515564, 304.71927070617676, 308.57178831100464, 312.36102771759033, 316.1681203842163, 320.3183743953705, 324.08771538734436, 327.72293996810913, 331.3751826286316, 335.2053372859955, 339.17204999923706, 342.87774896621704, 346.72114634513855, 350.8236138820648, 354.6647002696991, 358.65835332870483, 362.47550201416016, 366.2647008895874, 369.94736433029175, 373.60834312438965, 377.2339856624603, 381.00479674339294, 382.8956968784332]
[12.675, 12.6725, 12.705, 12.69, 12.695, 12.705, 12.7275, 12.74, 12.7275, 12.7475, 12.745, 12.75, 12.75, 12.785, 12.8075, 12.8, 12.795, 12.795, 12.8125, 12.82, 12.81, 12.82, 12.8375, 12.83, 12.8475, 12.8675, 12.8725, 12.8725, 12.8625, 12.8775, 12.8825, 12.88, 12.88, 12.895, 12.89, 12.895, 12.9025, 12.8925, 12.9025, 12.9275, 12.9525, 12.9825, 13.0, 13.005, 13.02, 12.9975, 12.9975, 13.0075, 13.0175, 13.015, 13.015, 13.0025, 13.015, 13.03, 13.05, 13.06, 13.0575, 13.035, 13.035, 12.9975, 12.98, 12.9625, 12.9825, 12.9775, 12.985, 12.975, 12.9875, 13.03, 13.07, 13.1075, 13.1475, 13.1575, 13.1875, 13.205, 13.2325, 13.2525, 13.2725, 13.285, 13.3125, 13.3375, 13.3575, 13.3825, 13.4075, 13.415, 13.445, 13.47, 13.525, 13.585, 13.6025, 13.6275, 13.6225, 13.62, 13.5925, 13.5775, 13.5625, 13.555, 13.565, 13.5525, 13.515, 13.5025, 13.5125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.301, Test loss: 2.297, Test accuracy: 36.38
Round   1, Train loss: 2.296, Test loss: 2.286, Test accuracy: 55.43
Round   2, Train loss: 2.283, Test loss: 2.210, Test accuracy: 40.73
Round   3, Train loss: 2.196, Test loss: 1.977, Test accuracy: 51.12
Round   4, Train loss: 1.986, Test loss: 1.827, Test accuracy: 66.53
Round   5, Train loss: 1.910, Test loss: 1.761, Test accuracy: 73.19
Round   6, Train loss: 1.796, Test loss: 1.731, Test accuracy: 74.70
Round   7, Train loss: 1.741, Test loss: 1.720, Test accuracy: 75.20
Round   8, Train loss: 1.782, Test loss: 1.711, Test accuracy: 75.80
Round   9, Train loss: 1.718, Test loss: 1.708, Test accuracy: 75.88
Round  10, Train loss: 1.719, Test loss: 1.705, Test accuracy: 76.02
Round  11, Train loss: 1.705, Test loss: 1.703, Test accuracy: 76.15
Round  12, Train loss: 1.695, Test loss: 1.701, Test accuracy: 76.33
Round  13, Train loss: 1.703, Test loss: 1.699, Test accuracy: 76.43
Round  14, Train loss: 1.702, Test loss: 1.699, Test accuracy: 76.39
Round  15, Train loss: 1.702, Test loss: 1.696, Test accuracy: 76.42
Round  16, Train loss: 1.693, Test loss: 1.657, Test accuracy: 81.42
Round  17, Train loss: 1.666, Test loss: 1.646, Test accuracy: 82.30
Round  18, Train loss: 1.648, Test loss: 1.640, Test accuracy: 82.56
Round  19, Train loss: 1.639, Test loss: 1.638, Test accuracy: 82.58
Round  20, Train loss: 1.627, Test loss: 1.634, Test accuracy: 83.15
Round  21, Train loss: 1.616, Test loss: 1.632, Test accuracy: 83.07
Round  22, Train loss: 1.608, Test loss: 1.630, Test accuracy: 83.39
Round  23, Train loss: 1.629, Test loss: 1.629, Test accuracy: 83.44
Round  24, Train loss: 1.607, Test loss: 1.627, Test accuracy: 83.56
Round  25, Train loss: 1.609, Test loss: 1.627, Test accuracy: 83.64
Round  26, Train loss: 1.604, Test loss: 1.626, Test accuracy: 83.76
Round  27, Train loss: 1.606, Test loss: 1.624, Test accuracy: 83.85
Round  28, Train loss: 1.602, Test loss: 1.624, Test accuracy: 83.84
Round  29, Train loss: 1.600, Test loss: 1.622, Test accuracy: 84.03
Round  30, Train loss: 1.601, Test loss: 1.621, Test accuracy: 84.07
Round  31, Train loss: 1.599, Test loss: 1.620, Test accuracy: 84.22
Round  32, Train loss: 1.602, Test loss: 1.620, Test accuracy: 84.12
Round  33, Train loss: 1.598, Test loss: 1.619, Test accuracy: 84.27
Round  34, Train loss: 1.604, Test loss: 1.618, Test accuracy: 84.29
Round  35, Train loss: 1.601, Test loss: 1.618, Test accuracy: 84.37
Round  36, Train loss: 1.596, Test loss: 1.618, Test accuracy: 84.39
Round  37, Train loss: 1.598, Test loss: 1.617, Test accuracy: 84.56
Round  38, Train loss: 1.594, Test loss: 1.616, Test accuracy: 84.67
Round  39, Train loss: 1.590, Test loss: 1.617, Test accuracy: 84.47
Round  40, Train loss: 1.595, Test loss: 1.617, Test accuracy: 84.38
Round  41, Train loss: 1.598, Test loss: 1.615, Test accuracy: 84.75
Round  42, Train loss: 1.585, Test loss: 1.614, Test accuracy: 84.89
Round  43, Train loss: 1.591, Test loss: 1.615, Test accuracy: 84.58
Round  44, Train loss: 1.593, Test loss: 1.614, Test accuracy: 84.78
Round  45, Train loss: 1.584, Test loss: 1.614, Test accuracy: 84.72
Round  46, Train loss: 1.583, Test loss: 1.614, Test accuracy: 84.78
Round  47, Train loss: 1.590, Test loss: 1.613, Test accuracy: 84.74
Round  48, Train loss: 1.585, Test loss: 1.613, Test accuracy: 84.83
Round  49, Train loss: 1.582, Test loss: 1.612, Test accuracy: 84.95
Round  50, Train loss: 1.587, Test loss: 1.612, Test accuracy: 84.87
Round  51, Train loss: 1.587, Test loss: 1.613, Test accuracy: 84.82
Round  52, Train loss: 1.585, Test loss: 1.612, Test accuracy: 84.90
Round  53, Train loss: 1.589, Test loss: 1.612, Test accuracy: 84.92
Round  54, Train loss: 1.581, Test loss: 1.612, Test accuracy: 84.88
Round  55, Train loss: 1.579, Test loss: 1.612, Test accuracy: 85.00
Round  56, Train loss: 1.583, Test loss: 1.611, Test accuracy: 84.90
Round  57, Train loss: 1.583, Test loss: 1.611, Test accuracy: 84.93
Round  58, Train loss: 1.589, Test loss: 1.611, Test accuracy: 84.96
Round  59, Train loss: 1.583, Test loss: 1.611, Test accuracy: 84.97
Round  60, Train loss: 1.586, Test loss: 1.611, Test accuracy: 85.04
Round  61, Train loss: 1.582, Test loss: 1.612, Test accuracy: 85.03
Round  62, Train loss: 1.581, Test loss: 1.611, Test accuracy: 85.11
Round  63, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.12
Round  64, Train loss: 1.582, Test loss: 1.610, Test accuracy: 85.12
Round  65, Train loss: 1.579, Test loss: 1.610, Test accuracy: 85.21
Round  66, Train loss: 1.581, Test loss: 1.609, Test accuracy: 85.27
Round  67, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.15
Round  68, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.31
Round  69, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.15
Round  70, Train loss: 1.574, Test loss: 1.610, Test accuracy: 85.17
Round  71, Train loss: 1.577, Test loss: 1.609, Test accuracy: 85.27
Round  72, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.25
Round  73, Train loss: 1.584, Test loss: 1.609, Test accuracy: 85.25
Round  74, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.26
Round  75, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.28
Round  76, Train loss: 1.580, Test loss: 1.609, Test accuracy: 85.26
Round  77, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.24
Round  78, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.31
Round  79, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.26
Round  80, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.37
Round  81, Train loss: 1.580, Test loss: 1.608, Test accuracy: 85.21
Round  82, Train loss: 1.576, Test loss: 1.608, Test accuracy: 85.19
Round  83, Train loss: 1.585, Test loss: 1.607, Test accuracy: 85.31
Round  84, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.29
Round  85, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.38
Round  86, Train loss: 1.580, Test loss: 1.607, Test accuracy: 85.39
Round  87, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.41
Round  88, Train loss: 1.581, Test loss: 1.607, Test accuracy: 85.46
Round  89, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.33
Round  90, Train loss: 1.570, Test loss: 1.607, Test accuracy: 85.33
Round  91, Train loss: 1.577, Test loss: 1.607, Test accuracy: 85.29
Round  92, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.45
Round  93, Train loss: 1.573, Test loss: 1.607, Test accuracy: 85.27
Round  94, Train loss: 1.568, Test loss: 1.607, Test accuracy: 85.36
Round  95, Train loss: 1.578, Test loss: 1.607, Test accuracy: 85.46
Round  96, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.44
Round  97, Train loss: 1.581, Test loss: 1.607, Test accuracy: 85.33
Round  98, Train loss: 1.569, Test loss: 1.606, Test accuracy: 85.39
Round  99, Train loss: 1.569, Test loss: 1.607, Test accuracy: 85.32
Final Round, Train loss: 1.571, Test loss: 1.606, Test accuracy: 85.45
Average accuracy final 10 rounds: 85.36416666666668
1962.2931978702545
[2.9166135787963867, 5.701465845108032, 8.430794954299927, 11.147584438323975, 13.815101861953735, 16.535646438598633, 19.146953105926514, 21.857017040252686, 24.494184255599976, 27.31550884246826, 29.9602472782135, 32.714112997055054, 35.41555666923523, 38.29101324081421, 41.06843447685242, 43.90212845802307, 46.688786029815674, 49.56585359573364, 52.409799575805664, 55.289629220962524, 58.17054510116577, 61.132683753967285, 63.785239696502686, 66.58046960830688, 69.39220476150513, 72.04444742202759, 74.8735785484314, 77.64364075660706, 80.36051297187805, 83.16076517105103, 85.93339443206787, 88.64104747772217, 91.41827964782715, 94.2054831981659, 96.93939161300659, 99.81444096565247, 102.58756995201111, 105.4806444644928, 108.41298246383667, 111.31443190574646, 114.04365396499634, 116.81951665878296, 119.46334528923035, 122.18340611457825, 124.93215417861938, 127.65069675445557, 130.40764808654785, 133.12152457237244, 135.8441183567047, 138.5625765323639, 141.28390383720398, 144.14867758750916, 146.91201281547546, 149.5974977016449, 152.37688970565796, 155.16041135787964, 157.83188128471375, 160.60110664367676, 163.34746146202087, 166.10838150978088, 168.99288630485535, 171.81706380844116, 174.5810956954956, 177.38907098770142, 180.13450980186462, 182.97094750404358, 185.7263445854187, 188.41181635856628, 191.25742840766907, 194.01888918876648, 196.70667457580566, 199.4042992591858, 202.15383791923523, 204.8887119293213, 207.66705346107483, 210.40288496017456, 213.04926800727844, 215.89037823677063, 218.66741132736206, 221.36758685112, 224.028235912323, 226.68187069892883, 229.3491997718811, 232.03138637542725, 234.53996634483337, 237.12726879119873, 239.72631216049194, 242.22225069999695, 244.77424001693726, 247.24487233161926, 249.7609624862671, 252.21740198135376, 254.74209666252136, 257.1865646839142, 259.65668749809265, 262.02766370773315, 264.3393430709839, 266.77892994880676, 269.19803524017334, 271.65176129341125, 273.67871832847595]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[36.38333333333333, 55.43333333333333, 40.725, 51.11666666666667, 66.525, 73.19166666666666, 74.7, 75.2, 75.8, 75.88333333333334, 76.01666666666667, 76.15, 76.325, 76.43333333333334, 76.39166666666667, 76.41666666666667, 81.41666666666667, 82.3, 82.55833333333334, 82.575, 83.15, 83.06666666666666, 83.39166666666667, 83.44166666666666, 83.55833333333334, 83.64166666666667, 83.75833333333334, 83.85, 83.84166666666667, 84.025, 84.06666666666666, 84.225, 84.11666666666666, 84.26666666666667, 84.29166666666667, 84.36666666666666, 84.39166666666667, 84.55833333333334, 84.66666666666667, 84.46666666666667, 84.38333333333334, 84.75, 84.89166666666667, 84.58333333333333, 84.775, 84.71666666666667, 84.78333333333333, 84.74166666666666, 84.825, 84.95, 84.86666666666666, 84.81666666666666, 84.9, 84.925, 84.875, 85.0, 84.9, 84.93333333333334, 84.95833333333333, 84.96666666666667, 85.04166666666667, 85.025, 85.10833333333333, 85.11666666666666, 85.125, 85.20833333333333, 85.26666666666667, 85.15, 85.30833333333334, 85.15, 85.175, 85.26666666666667, 85.25, 85.25, 85.25833333333334, 85.275, 85.25833333333334, 85.24166666666666, 85.30833333333334, 85.25833333333334, 85.36666666666666, 85.20833333333333, 85.19166666666666, 85.30833333333334, 85.29166666666667, 85.375, 85.39166666666667, 85.40833333333333, 85.45833333333333, 85.33333333333333, 85.33333333333333, 85.29166666666667, 85.45, 85.26666666666667, 85.35833333333333, 85.45833333333333, 85.44166666666666, 85.33333333333333, 85.39166666666667, 85.31666666666666, 85.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.312, Test loss: 2.276, Test accuracy: 29.29
Round   1, Train loss: 2.209, Test loss: 2.034, Test accuracy: 50.45
Round   2, Train loss: 1.937, Test loss: 1.876, Test accuracy: 67.10
Round   3, Train loss: 1.813, Test loss: 1.770, Test accuracy: 78.14
Round   4, Train loss: 1.717, Test loss: 1.706, Test accuracy: 82.55
Round   5, Train loss: 1.670, Test loss: 1.685, Test accuracy: 83.61
Round   6, Train loss: 1.664, Test loss: 1.656, Test accuracy: 84.75
Round   7, Train loss: 1.630, Test loss: 1.629, Test accuracy: 87.52
Round   8, Train loss: 1.601, Test loss: 1.597, Test accuracy: 90.30
Round   9, Train loss: 1.581, Test loss: 1.577, Test accuracy: 91.53
Round  10, Train loss: 1.558, Test loss: 1.568, Test accuracy: 92.36
Round  11, Train loss: 1.559, Test loss: 1.552, Test accuracy: 93.16
Round  12, Train loss: 1.548, Test loss: 1.541, Test accuracy: 94.10
Round  13, Train loss: 1.541, Test loss: 1.533, Test accuracy: 94.83
Round  14, Train loss: 1.532, Test loss: 1.530, Test accuracy: 95.07
Round  15, Train loss: 1.532, Test loss: 1.525, Test accuracy: 95.48
Round  16, Train loss: 1.523, Test loss: 1.522, Test accuracy: 95.70
Round  17, Train loss: 1.518, Test loss: 1.520, Test accuracy: 95.80
Round  18, Train loss: 1.516, Test loss: 1.518, Test accuracy: 95.95
Round  19, Train loss: 1.511, Test loss: 1.516, Test accuracy: 96.14
Round  20, Train loss: 1.515, Test loss: 1.514, Test accuracy: 96.30
Round  21, Train loss: 1.507, Test loss: 1.513, Test accuracy: 96.41
Round  22, Train loss: 1.506, Test loss: 1.511, Test accuracy: 96.50
Round  23, Train loss: 1.505, Test loss: 1.510, Test accuracy: 96.57
Round  24, Train loss: 1.503, Test loss: 1.509, Test accuracy: 96.61
Round  25, Train loss: 1.501, Test loss: 1.508, Test accuracy: 96.63
Round  26, Train loss: 1.501, Test loss: 1.506, Test accuracy: 96.68
Round  27, Train loss: 1.499, Test loss: 1.505, Test accuracy: 96.80
Round  28, Train loss: 1.495, Test loss: 1.505, Test accuracy: 96.84
Round  29, Train loss: 1.495, Test loss: 1.503, Test accuracy: 96.88
Round  30, Train loss: 1.492, Test loss: 1.503, Test accuracy: 97.00
Round  31, Train loss: 1.490, Test loss: 1.502, Test accuracy: 97.04
Round  32, Train loss: 1.493, Test loss: 1.501, Test accuracy: 97.06
Round  33, Train loss: 1.491, Test loss: 1.500, Test accuracy: 97.17
Round  34, Train loss: 1.490, Test loss: 1.500, Test accuracy: 97.17
Round  35, Train loss: 1.487, Test loss: 1.500, Test accuracy: 97.17
Round  36, Train loss: 1.485, Test loss: 1.500, Test accuracy: 97.23
Round  37, Train loss: 1.484, Test loss: 1.499, Test accuracy: 97.30
Round  38, Train loss: 1.485, Test loss: 1.499, Test accuracy: 97.33
Round  39, Train loss: 1.485, Test loss: 1.498, Test accuracy: 97.36
Round  40, Train loss: 1.486, Test loss: 1.498, Test accuracy: 97.36
Round  41, Train loss: 1.488, Test loss: 1.497, Test accuracy: 97.39
Round  42, Train loss: 1.482, Test loss: 1.497, Test accuracy: 97.43
Round  43, Train loss: 1.483, Test loss: 1.496, Test accuracy: 97.47
Round  44, Train loss: 1.479, Test loss: 1.496, Test accuracy: 97.45
Round  45, Train loss: 1.481, Test loss: 1.496, Test accuracy: 97.50
Round  46, Train loss: 1.483, Test loss: 1.495, Test accuracy: 97.55
Round  47, Train loss: 1.482, Test loss: 1.495, Test accuracy: 97.56
Round  48, Train loss: 1.480, Test loss: 1.495, Test accuracy: 97.56
Round  49, Train loss: 1.480, Test loss: 1.495, Test accuracy: 97.56
Round  50, Train loss: 1.481, Test loss: 1.494, Test accuracy: 97.61
Round  51, Train loss: 1.480, Test loss: 1.494, Test accuracy: 97.61
Round  52, Train loss: 1.479, Test loss: 1.494, Test accuracy: 97.56
Round  53, Train loss: 1.478, Test loss: 1.494, Test accuracy: 97.60
Round  54, Train loss: 1.478, Test loss: 1.494, Test accuracy: 97.56
Round  55, Train loss: 1.478, Test loss: 1.493, Test accuracy: 97.63
Round  56, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.63
Round  57, Train loss: 1.476, Test loss: 1.493, Test accuracy: 97.61
Round  58, Train loss: 1.476, Test loss: 1.493, Test accuracy: 97.64
Round  59, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.71
Round  60, Train loss: 1.476, Test loss: 1.493, Test accuracy: 97.70
Round  61, Train loss: 1.474, Test loss: 1.493, Test accuracy: 97.68
Round  62, Train loss: 1.475, Test loss: 1.492, Test accuracy: 97.69
Round  63, Train loss: 1.476, Test loss: 1.492, Test accuracy: 97.70
Round  64, Train loss: 1.474, Test loss: 1.492, Test accuracy: 97.71
Round  65, Train loss: 1.476, Test loss: 1.492, Test accuracy: 97.76
Round  66, Train loss: 1.474, Test loss: 1.492, Test accuracy: 97.72
Round  67, Train loss: 1.473, Test loss: 1.492, Test accuracy: 97.70
Round  68, Train loss: 1.474, Test loss: 1.491, Test accuracy: 97.72
Round  69, Train loss: 1.474, Test loss: 1.491, Test accuracy: 97.76
Round  70, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.78
Round  71, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.75
Round  72, Train loss: 1.473, Test loss: 1.491, Test accuracy: 97.76
Round  73, Train loss: 1.473, Test loss: 1.491, Test accuracy: 97.76
Round  74, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.75
Round  75, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.73
Round  76, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.75
Round  77, Train loss: 1.471, Test loss: 1.491, Test accuracy: 97.76
Round  78, Train loss: 1.472, Test loss: 1.491, Test accuracy: 97.78
Round  79, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.82
Round  80, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.82
Round  81, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.77
Round  82, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.82
Round  83, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.84
Round  84, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.83
Round  85, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.83
Round  86, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.81
Round  87, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.83
Round  88, Train loss: 1.469, Test loss: 1.490, Test accuracy: 97.82
Round  89, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.84
Round  90, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.85
Round  91, Train loss: 1.470, Test loss: 1.490, Test accuracy: 97.86
Round  92, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.89
Round  93, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.90
Round  94, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.88/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.85
Round  96, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.89
Round  97, Train loss: 1.469, Test loss: 1.489, Test accuracy: 97.89
Round  98, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.93
Round  99, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.93
Final Round, Train loss: 1.468, Test loss: 1.489, Test accuracy: 97.88
Average accuracy final 10 rounds: 97.88575
4065.6579229831696
[4.258806943893433, 9.135666847229004, 13.919945478439331, 18.659745454788208, 23.554295778274536, 28.088661193847656, 32.66466665267944, 37.40535044670105, 42.12899971008301, 46.885504484176636, 51.64134359359741, 56.411473512649536, 61.27231979370117, 66.07889938354492, 70.81152534484863, 75.50058603286743, 80.30264282226562, 85.15670347213745, 89.72140741348267, 94.27651929855347, 98.81743383407593, 103.32467818260193, 108.13547039031982, 112.78366947174072, 117.44611382484436, 122.2769525051117, 127.11836671829224, 132.0878872871399, 136.7537145614624, 141.57327508926392, 146.55067610740662, 151.09959292411804, 155.92909598350525, 160.50071740150452, 165.2265944480896, 170.09798574447632, 174.79245495796204, 179.88526439666748, 185.03010535240173, 189.83518958091736, 195.02642393112183, 200.17092561721802, 205.07381224632263, 210.18011593818665, 215.12967014312744, 219.97485399246216, 225.04704666137695, 229.821843624115, 235.0090367794037, 239.99924969673157, 244.88504147529602, 249.85563945770264, 254.75758528709412, 259.6139886379242, 264.46868920326233, 269.4775640964508, 274.344580411911, 279.16671681404114, 283.978812456131, 288.9727215766907, 293.8784439563751, 298.81807374954224, 303.7026436328888, 308.7263469696045, 313.5924620628357, 318.55541491508484, 323.37658286094666, 328.2840015888214, 333.20031476020813, 337.9790642261505, 342.74918031692505, 347.65023398399353, 352.562344789505, 357.4730894565582, 362.5315773487091, 367.57241654396057, 372.51822447776794, 377.3701570034027, 382.35587072372437, 387.26207542419434, 392.24706411361694, 397.06879019737244, 401.93450903892517, 406.9589014053345, 411.96694779396057, 416.70707273483276, 421.7163519859314, 426.5905590057373, 431.4868173599243, 436.6483163833618, 441.75985741615295, 446.38347363471985, 451.36948800086975, 456.3048770427704, 461.0505177974701, 465.984171628952, 470.9807913303375, 475.9226450920105, 480.887642621994, 485.8836131095886, 487.8531684875488]
[29.29, 50.4525, 67.1, 78.1375, 82.5475, 83.6125, 84.75, 87.52, 90.3025, 91.535, 92.365, 93.1625, 94.1, 94.8275, 95.0725, 95.4825, 95.705, 95.8025, 95.955, 96.1425, 96.2975, 96.405, 96.4975, 96.57, 96.615, 96.6275, 96.6825, 96.8025, 96.8425, 96.88, 97.0, 97.0425, 97.055, 97.175, 97.1725, 97.1725, 97.235, 97.295, 97.325, 97.355, 97.36, 97.3925, 97.4275, 97.465, 97.4525, 97.5025, 97.545, 97.565, 97.555, 97.5575, 97.6125, 97.6075, 97.5625, 97.5975, 97.5625, 97.6275, 97.63, 97.6125, 97.6375, 97.7075, 97.6975, 97.6825, 97.685, 97.7, 97.7125, 97.7575, 97.725, 97.7, 97.715, 97.76, 97.78, 97.7525, 97.76, 97.7575, 97.75, 97.7325, 97.7475, 97.76, 97.7825, 97.8225, 97.8225, 97.77, 97.8175, 97.8375, 97.8275, 97.8325, 97.8125, 97.83, 97.82, 97.8375, 97.8475, 97.855, 97.895, 97.9025, 97.875, 97.85, 97.885, 97.8925, 97.9275, 97.9275, 97.8825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.313, Test loss: 2.278, Test accuracy: 37.90
Round   1, Train loss: 2.218, Test loss: 2.075, Test accuracy: 43.07
Round   2, Train loss: 2.002, Test loss: 1.913, Test accuracy: 65.47
Round   3, Train loss: 1.856, Test loss: 1.816, Test accuracy: 74.91
Round   4, Train loss: 1.788, Test loss: 1.764, Test accuracy: 80.35
Round   5, Train loss: 1.737, Test loss: 1.728, Test accuracy: 82.84
Round   6, Train loss: 1.704, Test loss: 1.679, Test accuracy: 89.27
Round   7, Train loss: 1.661, Test loss: 1.630, Test accuracy: 91.64
Round   8, Train loss: 1.631, Test loss: 1.613, Test accuracy: 92.72
Round   9, Train loss: 1.607, Test loss: 1.605, Test accuracy: 93.49
Round  10, Train loss: 1.618, Test loss: 1.574, Test accuracy: 94.22
Round  11, Train loss: 1.581, Test loss: 1.576, Test accuracy: 94.69
Round  12, Train loss: 1.594, Test loss: 1.559, Test accuracy: 94.94
Round  13, Train loss: 1.580, Test loss: 1.551, Test accuracy: 95.36
Round  14, Train loss: 1.568, Test loss: 1.548, Test accuracy: 95.55
Round  15, Train loss: 1.564, Test loss: 1.544, Test accuracy: 95.83
Round  16, Train loss: 1.565, Test loss: 1.536, Test accuracy: 96.03
Round  17, Train loss: 1.552, Test loss: 1.535, Test accuracy: 96.24
Round  18, Train loss: 1.547, Test loss: 1.533, Test accuracy: 96.39
Round  19, Train loss: 1.544, Test loss: 1.531, Test accuracy: 96.53
Round  20, Train loss: 1.547, Test loss: 1.528, Test accuracy: 96.66
Round  21, Train loss: 1.537, Test loss: 1.528, Test accuracy: 96.62
Round  22, Train loss: 1.537, Test loss: 1.526, Test accuracy: 96.71
Round  23, Train loss: 1.534, Test loss: 1.524, Test accuracy: 96.69
Round  24, Train loss: 1.537, Test loss: 1.519, Test accuracy: 96.79
Round  25, Train loss: 1.527, Test loss: 1.521, Test accuracy: 96.83
Round  26, Train loss: 1.531, Test loss: 1.518, Test accuracy: 96.98
Round  27, Train loss: 1.525, Test loss: 1.518, Test accuracy: 97.11
Round  28, Train loss: 1.523, Test loss: 1.516, Test accuracy: 97.11
Round  29, Train loss: 1.521, Test loss: 1.517, Test accuracy: 97.10
Round  30, Train loss: 1.521, Test loss: 1.514, Test accuracy: 97.16
Round  31, Train loss: 1.518, Test loss: 1.513, Test accuracy: 97.17
Round  32, Train loss: 1.516, Test loss: 1.513, Test accuracy: 97.29
Round  33, Train loss: 1.516, Test loss: 1.512, Test accuracy: 97.36
Round  34, Train loss: 1.517, Test loss: 1.511, Test accuracy: 97.39
Round  35, Train loss: 1.514, Test loss: 1.510, Test accuracy: 97.41
Round  36, Train loss: 1.510, Test loss: 1.511, Test accuracy: 97.41
Round  37, Train loss: 1.511, Test loss: 1.510, Test accuracy: 97.45
Round  38, Train loss: 1.511, Test loss: 1.509, Test accuracy: 97.38
Round  39, Train loss: 1.511, Test loss: 1.508, Test accuracy: 97.45
Round  40, Train loss: 1.506, Test loss: 1.509, Test accuracy: 97.52
Round  41, Train loss: 1.508, Test loss: 1.507, Test accuracy: 97.59
Round  42, Train loss: 1.507, Test loss: 1.507, Test accuracy: 97.47
Round  43, Train loss: 1.504, Test loss: 1.507, Test accuracy: 97.59
Round  44, Train loss: 1.504, Test loss: 1.506, Test accuracy: 97.62
Round  45, Train loss: 1.504, Test loss: 1.505, Test accuracy: 97.64
Round  46, Train loss: 1.503, Test loss: 1.505, Test accuracy: 97.73
Round  47, Train loss: 1.505, Test loss: 1.504, Test accuracy: 97.62
Round  48, Train loss: 1.502, Test loss: 1.504, Test accuracy: 97.61
Round  49, Train loss: 1.501, Test loss: 1.504, Test accuracy: 97.72
Round  50, Train loss: 1.501, Test loss: 1.504, Test accuracy: 97.78
Round  51, Train loss: 1.499, Test loss: 1.504, Test accuracy: 97.82
Round  52, Train loss: 1.500, Test loss: 1.504, Test accuracy: 97.72
Round  53, Train loss: 1.502, Test loss: 1.502, Test accuracy: 97.81
Round  54, Train loss: 1.500, Test loss: 1.502, Test accuracy: 97.79
Round  55, Train loss: 1.498, Test loss: 1.502, Test accuracy: 97.81
Round  56, Train loss: 1.498, Test loss: 1.502, Test accuracy: 97.84
Round  57, Train loss: 1.497, Test loss: 1.502, Test accuracy: 97.83
Round  58, Train loss: 1.498, Test loss: 1.501, Test accuracy: 97.85
Round  59, Train loss: 1.499, Test loss: 1.501, Test accuracy: 97.86
Round  60, Train loss: 1.495, Test loss: 1.501, Test accuracy: 97.91
Round  61, Train loss: 1.494, Test loss: 1.501, Test accuracy: 97.86
Round  62, Train loss: 1.495, Test loss: 1.501, Test accuracy: 97.89
Round  63, Train loss: 1.497, Test loss: 1.500, Test accuracy: 97.93
Round  64, Train loss: 1.495, Test loss: 1.500, Test accuracy: 97.91
Round  65, Train loss: 1.496, Test loss: 1.499, Test accuracy: 97.94
Round  66, Train loss: 1.492, Test loss: 1.500, Test accuracy: 98.00
Round  67, Train loss: 1.493, Test loss: 1.500, Test accuracy: 97.94
Round  68, Train loss: 1.495, Test loss: 1.499, Test accuracy: 97.99
Round  69, Train loss: 1.496, Test loss: 1.499, Test accuracy: 97.97
Round  70, Train loss: 1.493, Test loss: 1.499, Test accuracy: 97.95
Round  71, Train loss: 1.493, Test loss: 1.499, Test accuracy: 97.98
Round  72, Train loss: 1.492, Test loss: 1.499, Test accuracy: 97.99
Round  73, Train loss: 1.490, Test loss: 1.499, Test accuracy: 98.03
Round  74, Train loss: 1.493, Test loss: 1.498, Test accuracy: 98.06
Round  75, Train loss: 1.492, Test loss: 1.498, Test accuracy: 98.07
Round  76, Train loss: 1.490, Test loss: 1.498, Test accuracy: 98.10
Round  77, Train loss: 1.490, Test loss: 1.498, Test accuracy: 98.04
Round  78, Train loss: 1.489, Test loss: 1.499, Test accuracy: 98.04
Round  79, Train loss: 1.491, Test loss: 1.498, Test accuracy: 98.02
Round  80, Train loss: 1.491, Test loss: 1.498, Test accuracy: 97.98
Round  81, Train loss: 1.490, Test loss: 1.497, Test accuracy: 98.05
Round  82, Train loss: 1.490, Test loss: 1.498, Test accuracy: 98.02
Round  83, Train loss: 1.490, Test loss: 1.497, Test accuracy: 98.01
Round  84, Train loss: 1.489, Test loss: 1.497, Test accuracy: 98.04
Round  85, Train loss: 1.489, Test loss: 1.497, Test accuracy: 98.03
Round  86, Train loss: 1.489, Test loss: 1.498, Test accuracy: 98.03
Round  87, Train loss: 1.491, Test loss: 1.496, Test accuracy: 97.98
Round  88, Train loss: 1.489, Test loss: 1.497, Test accuracy: 97.99
Round  89, Train loss: 1.488, Test loss: 1.497, Test accuracy: 98.02
Round  90, Train loss: 1.488, Test loss: 1.497, Test accuracy: 97.99
Round  91, Train loss: 1.489, Test loss: 1.497, Test accuracy: 98.03
Round  92, Train loss: 1.488, Test loss: 1.497, Test accuracy: 98.08
Round  93, Train loss: 1.488, Test loss: 1.497, Test accuracy: 98.03/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.489, Test loss: 1.496, Test accuracy: 98.02
Round  95, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.02
Round  96, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.01
Round  97, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.02
Round  98, Train loss: 1.486, Test loss: 1.496, Test accuracy: 98.05
Round  99, Train loss: 1.488, Test loss: 1.496, Test accuracy: 98.03
Final Round, Train loss: 1.471, Test loss: 1.495, Test accuracy: 98.02
Average accuracy final 10 rounds: 98.02825
5641.264214038849
[5.142660856246948, 10.285321712493896, 15.044090270996094, 19.80285882949829, 24.437625646591187, 29.072392463684082, 33.807934284210205, 38.54347610473633, 43.29310154914856, 48.04272699356079, 52.74139618873596, 57.44006538391113, 62.2307026386261, 67.02133989334106, 71.68405938148499, 76.3467788696289, 81.12433528900146, 85.90189170837402, 90.67327308654785, 95.44465446472168, 100.0849916934967, 104.72532892227173, 109.47695827484131, 114.22858762741089, 118.85130858421326, 123.47402954101562, 128.21656894683838, 132.95910835266113, 138.05643463134766, 143.15376091003418, 148.24607253074646, 153.33838415145874, 158.57029390335083, 163.80220365524292, 168.89083552360535, 173.97946739196777, 178.96956729888916, 183.95966720581055, 188.88198733329773, 193.8043074607849, 198.92466187477112, 204.04501628875732, 209.18075275421143, 214.31648921966553, 219.27853393554688, 224.24057865142822, 229.19767951965332, 234.15478038787842, 239.18778109550476, 244.2207818031311, 249.26553440093994, 254.31028699874878, 259.57146859169006, 264.83265018463135, 269.84436798095703, 274.8560857772827, 279.90522146224976, 284.9543571472168, 289.71537160873413, 294.47638607025146, 299.41870951652527, 304.3610329627991, 309.5144565105438, 314.6678800582886, 319.7549777030945, 324.8420753479004, 329.828200340271, 334.8143253326416, 339.67829608917236, 344.5422668457031, 349.46812176704407, 354.393976688385, 359.20357394218445, 364.0131711959839, 368.963862657547, 373.9145541191101, 378.67484974861145, 383.4351453781128, 388.19992089271545, 392.9646964073181, 397.82954573631287, 402.6943950653076, 407.7163333892822, 412.73827171325684, 417.6678891181946, 422.5975065231323, 427.32967925071716, 432.061851978302, 437.0623686313629, 442.0628852844238, 447.0706226825714, 452.078360080719, 457.00001764297485, 461.9216752052307, 466.6787989139557, 471.43592262268066, 476.18525314331055, 480.93458366394043, 486.021488904953, 491.1083941459656, 496.3482496738434, 501.5881052017212, 506.7235288619995, 511.85895252227783, 516.942587852478, 522.0262231826782, 527.0688395500183, 532.1114559173584, 537.1208183765411, 542.1301808357239, 547.1173183917999, 552.104455947876, 557.0703837871552, 562.0363116264343, 567.0209717750549, 572.0056319236755, 576.8366861343384, 581.6677403450012, 586.5764253139496, 591.485110282898, 596.5684745311737, 601.6518387794495, 606.7627131938934, 611.8735876083374, 616.9580178260803, 622.0424480438232, 627.0408024787903, 632.0391569137573, 637.0106530189514, 641.9821491241455, 646.8092725276947, 651.6363959312439, 656.6392018795013, 661.6420078277588, 666.613495349884, 671.5849828720093, 676.6414198875427, 681.6978569030762, 686.592217206955, 691.4865775108337, 696.3758065700531, 701.2650356292725, 706.0847380161285, 710.9044404029846, 715.9765250682831, 721.0486097335815, 726.0760095119476, 731.1034092903137, 735.9815702438354, 740.8597311973572, 746.0175940990448, 751.1754570007324, 756.248197555542, 761.3209381103516, 766.1251785755157, 770.9294190406799, 776.0238146781921, 781.1182103157043, 786.1613342761993, 791.2044582366943, 796.033243894577, 800.8620295524597, 805.8255779743195, 810.7891263961792, 815.9829812049866, 821.176836013794, 826.056788444519, 830.9367408752441, 835.8677105903625, 840.798680305481, 845.9573028087616, 851.1159253120422, 856.3972098827362, 861.6784944534302, 866.8239042758942, 871.9693140983582, 876.9410855770111, 881.9128570556641, 886.8435614109039, 891.7742657661438, 896.5229852199554, 901.2717046737671, 906.1796281337738, 911.0875515937805, 916.0722362995148, 921.056921005249, 926.1925911903381, 931.3282613754272, 936.3957207202911, 941.463180065155, 946.4913566112518, 951.5195331573486, 956.5628724098206, 961.6062116622925, 966.6823327541351, 971.7584538459778, 976.8554196357727, 981.9523854255676, 986.784656047821, 991.6169266700745, 993.6165988445282, 995.6162710189819]
[37.895, 37.895, 43.0725, 43.0725, 65.4675, 65.4675, 74.9075, 74.9075, 80.35, 80.35, 82.845, 82.845, 89.265, 89.265, 91.635, 91.635, 92.7225, 92.7225, 93.49, 93.49, 94.2225, 94.2225, 94.69, 94.69, 94.935, 94.935, 95.3575, 95.3575, 95.55, 95.55, 95.8275, 95.8275, 96.03, 96.03, 96.24, 96.24, 96.395, 96.395, 96.53, 96.53, 96.6625, 96.6625, 96.625, 96.625, 96.7125, 96.7125, 96.695, 96.695, 96.7875, 96.7875, 96.83, 96.83, 96.985, 96.985, 97.105, 97.105, 97.1075, 97.1075, 97.1, 97.1, 97.1575, 97.1575, 97.175, 97.175, 97.29, 97.29, 97.3575, 97.3575, 97.395, 97.395, 97.405, 97.405, 97.41, 97.41, 97.45, 97.45, 97.375, 97.375, 97.4475, 97.4475, 97.515, 97.515, 97.595, 97.595, 97.47, 97.47, 97.5925, 97.5925, 97.6175, 97.6175, 97.6425, 97.6425, 97.7275, 97.7275, 97.6175, 97.6175, 97.615, 97.615, 97.725, 97.725, 97.78, 97.78, 97.8175, 97.8175, 97.7175, 97.7175, 97.815, 97.815, 97.7875, 97.7875, 97.81, 97.81, 97.8425, 97.8425, 97.835, 97.835, 97.85, 97.85, 97.855, 97.855, 97.9075, 97.9075, 97.86, 97.86, 97.89, 97.89, 97.9275, 97.9275, 97.905, 97.905, 97.935, 97.935, 98.0, 98.0, 97.935, 97.935, 97.9925, 97.9925, 97.965, 97.965, 97.95, 97.95, 97.98, 97.98, 97.9875, 97.9875, 98.0275, 98.0275, 98.055, 98.055, 98.07, 98.07, 98.1, 98.1, 98.04, 98.04, 98.0425, 98.0425, 98.0175, 98.0175, 97.9775, 97.9775, 98.0525, 98.0525, 98.015, 98.015, 98.01, 98.01, 98.0425, 98.0425, 98.0325, 98.0325, 98.035, 98.035, 97.9825, 97.9825, 97.9875, 97.9875, 98.0175, 98.0175, 97.9875, 97.9875, 98.03, 98.03, 98.085, 98.085, 98.03, 98.03, 98.02, 98.02, 98.0175, 98.0175, 98.01, 98.01, 98.0175, 98.0175, 98.05, 98.05, 98.035, 98.035, 98.0225, 98.0225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.189, Test loss: 2.135, Test accuracy: 46.78
Round   0, Global train loss: 2.189, Global test loss: 2.248, Global test accuracy: 40.11
Round   1, Train loss: 1.872, Test loss: 1.905, Test accuracy: 62.91
Round   1, Global train loss: 1.872, Global test loss: 2.106, Global test accuracy: 43.58
Round   2, Train loss: 1.628, Test loss: 1.764, Test accuracy: 73.74
Round   2, Global train loss: 1.628, Global test loss: 2.055, Global test accuracy: 43.01
Round   3, Train loss: 1.507, Test loss: 1.730, Test accuracy: 75.08
Round   3, Global train loss: 1.507, Global test loss: 2.133, Global test accuracy: 32.49
Round   4, Train loss: 1.528, Test loss: 1.680, Test accuracy: 81.71
Round   4, Global train loss: 1.528, Global test loss: 2.094, Global test accuracy: 41.36
Round   5, Train loss: 1.502, Test loss: 1.723, Test accuracy: 74.17
Round   5, Global train loss: 1.502, Global test loss: 2.235, Global test accuracy: 21.03
Round   6, Train loss: 1.594, Test loss: 1.646, Test accuracy: 83.16
Round   6, Global train loss: 1.594, Global test loss: 2.016, Global test accuracy: 48.42
Round   7, Train loss: 1.557, Test loss: 1.607, Test accuracy: 86.57
Round   7, Global train loss: 1.557, Global test loss: 2.044, Global test accuracy: 41.50
Round   8, Train loss: 1.639, Test loss: 1.567, Test accuracy: 90.39
Round   8, Global train loss: 1.639, Global test loss: 2.015, Global test accuracy: 45.27
Round   9, Train loss: 1.590, Test loss: 1.561, Test accuracy: 90.48
Round   9, Global train loss: 1.590, Global test loss: 2.057, Global test accuracy: 39.63
Round  10, Train loss: 1.481, Test loss: 1.567, Test accuracy: 91.65
Round  10, Global train loss: 1.481, Global test loss: 2.094, Global test accuracy: 48.10
Round  11, Train loss: 1.491, Test loss: 1.546, Test accuracy: 91.86
Round  11, Global train loss: 1.491, Global test loss: 2.053, Global test accuracy: 44.57
Round  12, Train loss: 1.500, Test loss: 1.531, Test accuracy: 93.39
Round  12, Global train loss: 1.500, Global test loss: 2.096, Global test accuracy: 40.41
Round  13, Train loss: 1.503, Test loss: 1.521, Test accuracy: 94.53
Round  13, Global train loss: 1.503, Global test loss: 2.141, Global test accuracy: 30.52
Round  14, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.58
Round  14, Global train loss: 1.478, Global test loss: 2.076, Global test accuracy: 34.43
Round  15, Train loss: 1.479, Test loss: 1.518, Test accuracy: 94.75
Round  15, Global train loss: 1.479, Global test loss: 2.088, Global test accuracy: 40.64
Round  16, Train loss: 1.525, Test loss: 1.518, Test accuracy: 94.72
Round  16, Global train loss: 1.525, Global test loss: 2.065, Global test accuracy: 48.98
Round  17, Train loss: 1.522, Test loss: 1.518, Test accuracy: 94.71
Round  17, Global train loss: 1.522, Global test loss: 2.082, Global test accuracy: 42.44
Round  18, Train loss: 1.471, Test loss: 1.518, Test accuracy: 94.69
Round  18, Global train loss: 1.471, Global test loss: 2.045, Global test accuracy: 39.62
Round  19, Train loss: 1.525, Test loss: 1.517, Test accuracy: 94.72
Round  19, Global train loss: 1.525, Global test loss: 2.036, Global test accuracy: 42.08
Round  20, Train loss: 1.471, Test loss: 1.517, Test accuracy: 94.77
Round  20, Global train loss: 1.471, Global test loss: 2.050, Global test accuracy: 40.53
Round  21, Train loss: 1.472, Test loss: 1.517, Test accuracy: 94.74
Round  21, Global train loss: 1.472, Global test loss: 1.978, Global test accuracy: 50.65
Round  22, Train loss: 1.467, Test loss: 1.516, Test accuracy: 94.79
Round  22, Global train loss: 1.467, Global test loss: 2.068, Global test accuracy: 38.83
Round  23, Train loss: 1.469, Test loss: 1.516, Test accuracy: 94.85
Round  23, Global train loss: 1.469, Global test loss: 2.048, Global test accuracy: 49.86
Round  24, Train loss: 1.522, Test loss: 1.516, Test accuracy: 94.87
Round  24, Global train loss: 1.522, Global test loss: 2.213, Global test accuracy: 21.55
Round  25, Train loss: 1.471, Test loss: 1.516, Test accuracy: 94.83
Round  25, Global train loss: 1.471, Global test loss: 1.966, Global test accuracy: 55.07
Round  26, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.86
Round  26, Global train loss: 1.467, Global test loss: 2.137, Global test accuracy: 28.52
Round  27, Train loss: 1.522, Test loss: 1.515, Test accuracy: 94.88
Round  27, Global train loss: 1.522, Global test loss: 2.070, Global test accuracy: 42.42
Round  28, Train loss: 1.471, Test loss: 1.515, Test accuracy: 94.89
Round  28, Global train loss: 1.471, Global test loss: 2.122, Global test accuracy: 31.80
Round  29, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.86
Round  29, Global train loss: 1.467, Global test loss: 2.032, Global test accuracy: 42.52
Round  30, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.83
Round  30, Global train loss: 1.469, Global test loss: 2.042, Global test accuracy: 43.09
Round  31, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.83
Round  31, Global train loss: 1.467, Global test loss: 1.992, Global test accuracy: 49.28
Round  32, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.83
Round  32, Global train loss: 1.469, Global test loss: 2.068, Global test accuracy: 33.58
Round  33, Train loss: 1.470, Test loss: 1.515, Test accuracy: 94.84
Round  33, Global train loss: 1.470, Global test loss: 2.178, Global test accuracy: 26.13
Round  34, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.82
Round  34, Global train loss: 1.469, Global test loss: 2.077, Global test accuracy: 38.85
Round  35, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.85
Round  35, Global train loss: 1.468, Global test loss: 2.088, Global test accuracy: 33.26
Round  36, Train loss: 1.467, Test loss: 1.514, Test accuracy: 94.82
Round  36, Global train loss: 1.467, Global test loss: 2.057, Global test accuracy: 37.43
Round  37, Train loss: 1.521, Test loss: 1.514, Test accuracy: 94.81
Round  37, Global train loss: 1.521, Global test loss: 1.977, Global test accuracy: 46.81
Round  38, Train loss: 1.469, Test loss: 1.514, Test accuracy: 94.81
Round  38, Global train loss: 1.469, Global test loss: 2.078, Global test accuracy: 45.12
Round  39, Train loss: 1.520, Test loss: 1.514, Test accuracy: 94.84
Round  39, Global train loss: 1.520, Global test loss: 2.024, Global test accuracy: 42.52
Round  40, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.86
Round  40, Global train loss: 1.468, Global test loss: 2.056, Global test accuracy: 56.56
Round  41, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.84
Round  41, Global train loss: 1.468, Global test loss: 2.115, Global test accuracy: 35.08
Round  42, Train loss: 1.467, Test loss: 1.514, Test accuracy: 94.86
Round  42, Global train loss: 1.467, Global test loss: 1.985, Global test accuracy: 48.52
Round  43, Train loss: 1.467, Test loss: 1.514, Test accuracy: 94.86
Round  43, Global train loss: 1.467, Global test loss: 2.222, Global test accuracy: 21.31
Round  44, Train loss: 1.521, Test loss: 1.514, Test accuracy: 94.84
Round  44, Global train loss: 1.521, Global test loss: 2.031, Global test accuracy: 42.62
Round  45, Train loss: 1.521, Test loss: 1.514, Test accuracy: 94.85
Round  45, Global train loss: 1.521, Global test loss: 2.071, Global test accuracy: 40.25
Round  46, Train loss: 1.519, Test loss: 1.514, Test accuracy: 94.84
Round  46, Global train loss: 1.519, Global test loss: 2.091, Global test accuracy: 33.14
Round  47, Train loss: 1.519, Test loss: 1.514, Test accuracy: 94.85
Round  47, Global train loss: 1.519, Global test loss: 2.022, Global test accuracy: 42.66
Round  48, Train loss: 1.522, Test loss: 1.514, Test accuracy: 94.86
Round  48, Global train loss: 1.522, Global test loss: 2.029, Global test accuracy: 42.52
Round  49, Train loss: 1.466, Test loss: 1.514, Test accuracy: 94.83
Round  49, Global train loss: 1.466, Global test loss: 2.195, Global test accuracy: 26.44
Round  50, Train loss: 1.470, Test loss: 1.514, Test accuracy: 94.83
Round  50, Global train loss: 1.470, Global test loss: 2.140, Global test accuracy: 34.96
Round  51, Train loss: 1.468, Test loss: 1.514, Test accuracy: 94.83
Round  51, Global train loss: 1.468, Global test loss: 2.109, Global test accuracy: 31.50
Round  52, Train loss: 1.469, Test loss: 1.514, Test accuracy: 94.88
Round  52, Global train loss: 1.469, Global test loss: 2.195, Global test accuracy: 21.29
Round  53, Train loss: 1.470, Test loss: 1.514, Test accuracy: 94.90
Round  53, Global train loss: 1.470, Global test loss: 2.073, Global test accuracy: 40.35
Round  54, Train loss: 1.466, Test loss: 1.514, Test accuracy: 94.88
Round  54, Global train loss: 1.466, Global test loss: 2.015, Global test accuracy: 41.91
Round  55, Train loss: 1.470, Test loss: 1.514, Test accuracy: 94.90
Round  55, Global train loss: 1.470, Global test loss: 1.934, Global test accuracy: 53.49
Round  56, Train loss: 1.465, Test loss: 1.514, Test accuracy: 94.88
Round  56, Global train loss: 1.465, Global test loss: 2.057, Global test accuracy: 40.18
Round  57, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.93
Round  57, Global train loss: 1.468, Global test loss: 2.066, Global test accuracy: 45.77
Round  58, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  58, Global train loss: 1.467, Global test loss: 2.074, Global test accuracy: 39.14
Round  59, Train loss: 1.518, Test loss: 1.513, Test accuracy: 94.90
Round  59, Global train loss: 1.518, Global test loss: 1.997, Global test accuracy: 47.20
Round  60, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.91
Round  60, Global train loss: 1.468, Global test loss: 2.067, Global test accuracy: 43.90
Round  61, Train loss: 1.521, Test loss: 1.513, Test accuracy: 94.92
Round  61, Global train loss: 1.521, Global test loss: 2.022, Global test accuracy: 40.94
Round  62, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  62, Global train loss: 1.466, Global test loss: 2.081, Global test accuracy: 40.07
Round  63, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.93
Round  63, Global train loss: 1.468, Global test loss: 2.014, Global test accuracy: 52.92
Round  64, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  64, Global train loss: 1.467, Global test loss: 2.037, Global test accuracy: 42.60
Round  65, Train loss: 1.520, Test loss: 1.513, Test accuracy: 94.92
Round  65, Global train loss: 1.520, Global test loss: 2.058, Global test accuracy: 41.58
Round  66, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  66, Global train loss: 1.467, Global test loss: 2.126, Global test accuracy: 36.23
Round  67, Train loss: 1.518, Test loss: 1.513, Test accuracy: 94.91
Round  67, Global train loss: 1.518, Global test loss: 2.044, Global test accuracy: 41.83
Round  68, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  68, Global train loss: 1.466, Global test loss: 2.129, Global test accuracy: 35.77
Round  69, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.92
Round  69, Global train loss: 1.468, Global test loss: 2.073, Global test accuracy: 43.75
Round  70, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  70, Global train loss: 1.467, Global test loss: 2.112, Global test accuracy: 33.42
Round  71, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.90
Round  71, Global train loss: 1.466, Global test loss: 2.094, Global test accuracy: 42.43
Round  72, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.91
Round  72, Global train loss: 1.466, Global test loss: 1.980, Global test accuracy: 51.45
Round  73, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.92
Round  73, Global train loss: 1.465, Global test loss: 1.987, Global test accuracy: 50.17
Round  74, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.91
Round  74, Global train loss: 1.466, Global test loss: 2.065, Global test accuracy: 38.45
Round  75, Train loss: 1.522, Test loss: 1.513, Test accuracy: 94.91
Round  75, Global train loss: 1.522, Global test loss: 1.977, Global test accuracy: 48.99
Round  76, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  76, Global train loss: 1.467, Global test loss: 2.077, Global test accuracy: 36.02
Round  77, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  77, Global train loss: 1.467, Global test loss: 2.015, Global test accuracy: 45.07
Round  78, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  78, Global train loss: 1.467, Global test loss: 2.115, Global test accuracy: 31.48
Round  79, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  79, Global train loss: 1.467, Global test loss: 2.035, Global test accuracy: 47.30
Round  80, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  80, Global train loss: 1.466, Global test loss: 2.135, Global test accuracy: 42.07
Round  81, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.92
Round  81, Global train loss: 1.465, Global test loss: 2.038, Global test accuracy: 43.31
Round  82, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.92
Round  82, Global train loss: 1.467, Global test loss: 1.994, Global test accuracy: 51.80
Round  83, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.92
Round  83, Global train loss: 1.465, Global test loss: 1.980, Global test accuracy: 48.42
Round  84, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.92
Round  84, Global train loss: 1.466, Global test loss: 2.080, Global test accuracy: 37.26
Round  85, Train loss: 1.466, Test loss: 1.513, Test accuracy: 94.93
Round  85, Global train loss: 1.466, Global test loss: 2.107, Global test accuracy: 32.42
Round  86, Train loss: 1.470, Test loss: 1.513, Test accuracy: 94.93
Round  86, Global train loss: 1.470, Global test loss: 2.055, Global test accuracy: 44.11
Round  87, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  87, Global train loss: 1.467, Global test loss: 2.013, Global test accuracy: 42.76
Round  88, Train loss: 1.519, Test loss: 1.513, Test accuracy: 94.93
Round  88, Global train loss: 1.519, Global test loss: 2.162, Global test accuracy: 29.87
Round  89, Train loss: 1.469, Test loss: 1.513, Test accuracy: 94.93
Round  89, Global train loss: 1.469, Global test loss: 2.003, Global test accuracy: 42.72
Round  90, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  90, Global train loss: 1.467, Global test loss: 2.022, Global test accuracy: 42.02
Round  91, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  91, Global train loss: 1.467, Global test loss: 1.963, Global test accuracy: 52.12
Round  92, Train loss: 1.467, Test loss: 1.513, Test accuracy: 94.93
Round  92, Global train loss: 1.467, Global test loss: 2.044, Global test accuracy: 42.41
Round  93, Train loss: 1.464, Test loss: 1.513, Test accuracy: 94.94
Round  93, Global train loss: 1.464, Global test loss: 2.102, Global test accuracy: 33.47
Round  94, Train loss: 1.465, Test loss: 1.513, Test accuracy: 94.95
Round  94, Global train loss: 1.465, Global test loss: 2.111, Global test accuracy: 40.60
Round  95, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.96
Round  95, Global train loss: 1.468, Global test loss: 2.023, Global test accuracy: 42.00/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.518, Test loss: 1.513, Test accuracy: 94.96
Round  96, Global train loss: 1.518, Global test loss: 1.994, Global test accuracy: 43.80
Round  97, Train loss: 1.520, Test loss: 1.513, Test accuracy: 94.96
Round  97, Global train loss: 1.520, Global test loss: 2.111, Global test accuracy: 36.60
Round  98, Train loss: 1.520, Test loss: 1.513, Test accuracy: 94.96
Round  98, Global train loss: 1.520, Global test loss: 2.103, Global test accuracy: 34.70
Round  99, Train loss: 1.468, Test loss: 1.513, Test accuracy: 94.96
Round  99, Global train loss: 1.468, Global test loss: 2.093, Global test accuracy: 35.73
Final Round, Train loss: 1.483, Test loss: 1.513, Test accuracy: 94.94
Final Round, Global train loss: 1.483, Global test loss: 2.093, Global test accuracy: 35.73
Average accuracy final 10 rounds: 94.94833333333335 

Average global accuracy final 10 rounds: 40.34499999999999 

1911.5511236190796
[1.2639381885528564, 2.527876377105713, 3.7014577388763428, 4.875039100646973, 6.083296775817871, 7.2915544509887695, 8.507723569869995, 9.72389268875122, 10.980223178863525, 12.23655366897583, 13.356053113937378, 14.475552558898926, 15.650567293167114, 16.825582027435303, 17.972347259521484, 19.119112491607666, 20.23951292037964, 21.35991334915161, 22.526528120040894, 23.693142890930176, 24.803438186645508, 25.91373348236084, 26.971678972244263, 28.029624462127686, 29.14976692199707, 30.269909381866455, 31.40818214416504, 32.54645490646362, 33.71655488014221, 34.8866548538208, 36.00002598762512, 37.11339712142944, 38.199564933776855, 39.28573274612427, 40.42039394378662, 41.555055141448975, 42.68776750564575, 43.82047986984253, 44.945833921432495, 46.07118797302246, 47.26418852806091, 48.457189083099365, 49.559171199798584, 50.6611533164978, 51.79882478713989, 52.93649625778198, 54.07388639450073, 55.21127653121948, 56.39854288101196, 57.58580923080444, 58.73570656776428, 59.88560390472412, 60.97811007499695, 62.070616245269775, 63.203935623168945, 64.33725500106812, 65.50130677223206, 66.665358543396, 67.78414916992188, 68.90293979644775, 70.11558485031128, 71.3282299041748, 72.42890524864197, 73.52958059310913, 74.67145609855652, 75.8133316040039, 76.91583728790283, 78.01834297180176, 79.06724047660828, 80.1161379814148, 81.27856802940369, 82.44099807739258, 83.59374380111694, 84.74648952484131, 85.7588620185852, 86.7712345123291, 87.91962552070618, 89.06801652908325, 90.15345358848572, 91.23889064788818, 92.26404237747192, 93.28919410705566, 94.31506824493408, 95.3409423828125, 96.38133192062378, 97.42172145843506, 98.57297158241272, 99.72422170639038, 100.86607003211975, 102.00791835784912, 103.05544829368591, 104.1029782295227, 105.33887267112732, 106.57476711273193, 107.64475607872009, 108.71474504470825, 109.96144247055054, 111.20813989639282, 112.5357825756073, 113.86342525482178, 114.98757553100586, 116.11172580718994, 117.20671939849854, 118.30171298980713, 119.50160455703735, 120.70149612426758, 121.80102682113647, 122.90055751800537, 124.04266166687012, 125.18476581573486, 126.29741835594177, 127.41007089614868, 128.61638474464417, 129.82269859313965, 130.9980490207672, 132.17339944839478, 133.25670218467712, 134.34000492095947, 135.49072980880737, 136.64145469665527, 137.68992447853088, 138.7383942604065, 139.74087953567505, 140.7433648109436, 141.8928828239441, 143.04240083694458, 144.07038259506226, 145.09836435317993, 146.14215922355652, 147.1859540939331, 148.27381920814514, 149.36168432235718, 150.34738087654114, 151.3330774307251, 152.39156770706177, 153.45005798339844, 154.52679061889648, 155.60352325439453, 156.6166009902954, 157.6296787261963, 158.7794497013092, 159.92922067642212, 161.0112602710724, 162.09329986572266, 163.2027039527893, 164.31210803985596, 165.46001410484314, 166.60792016983032, 167.73900723457336, 168.8700942993164, 169.93286275863647, 170.99563121795654, 172.0992467403412, 173.20286226272583, 174.41485357284546, 175.6268448829651, 176.75102424621582, 177.87520360946655, 178.94963431358337, 180.0240650177002, 181.11159658432007, 182.19912815093994, 183.34119153022766, 184.48325490951538, 185.5728883743286, 186.66252183914185, 187.72818326950073, 188.79384469985962, 189.90443778038025, 191.01503086090088, 192.1086564064026, 193.2022819519043, 194.2865550518036, 195.37082815170288, 196.4686086177826, 197.5663890838623, 198.6850700378418, 199.8037509918213, 200.9175295829773, 202.0313081741333, 203.1129412651062, 204.1945743560791, 205.27321600914001, 206.35185766220093, 207.4562087059021, 208.56055974960327, 209.65938353538513, 210.758207321167, 211.77587938308716, 212.79355144500732, 213.9712793827057, 215.14900732040405, 216.20695853233337, 217.2649097442627, 218.3611798286438, 219.4574499130249, 220.58982038497925, 221.7221908569336, 222.7709400653839, 223.81968927383423, 225.75887060165405, 227.69805192947388]
[46.78333333333333, 46.78333333333333, 62.90833333333333, 62.90833333333333, 73.74166666666666, 73.74166666666666, 75.08333333333333, 75.08333333333333, 81.70833333333333, 81.70833333333333, 74.16666666666667, 74.16666666666667, 83.15833333333333, 83.15833333333333, 86.56666666666666, 86.56666666666666, 90.39166666666667, 90.39166666666667, 90.48333333333333, 90.48333333333333, 91.65, 91.65, 91.85833333333333, 91.85833333333333, 93.39166666666667, 93.39166666666667, 94.525, 94.525, 94.575, 94.575, 94.75, 94.75, 94.725, 94.725, 94.70833333333333, 94.70833333333333, 94.69166666666666, 94.69166666666666, 94.71666666666667, 94.71666666666667, 94.76666666666667, 94.76666666666667, 94.74166666666666, 94.74166666666666, 94.79166666666667, 94.79166666666667, 94.85, 94.85, 94.86666666666666, 94.86666666666666, 94.83333333333333, 94.83333333333333, 94.85833333333333, 94.85833333333333, 94.875, 94.875, 94.89166666666667, 94.89166666666667, 94.85833333333333, 94.85833333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.83333333333333, 94.84166666666667, 94.84166666666667, 94.81666666666666, 94.81666666666666, 94.85, 94.85, 94.81666666666666, 94.81666666666666, 94.80833333333334, 94.80833333333334, 94.80833333333334, 94.80833333333334, 94.84166666666667, 94.84166666666667, 94.85833333333333, 94.85833333333333, 94.84166666666667, 94.84166666666667, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.85833333333333, 94.84166666666667, 94.84166666666667, 94.85, 94.85, 94.84166666666667, 94.84166666666667, 94.85, 94.85, 94.85833333333333, 94.85833333333333, 94.825, 94.825, 94.825, 94.825, 94.825, 94.825, 94.875, 94.875, 94.9, 94.9, 94.88333333333334, 94.88333333333334, 94.9, 94.9, 94.88333333333334, 94.88333333333334, 94.93333333333334, 94.93333333333334, 94.925, 94.925, 94.9, 94.9, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.93333333333334, 94.93333333333334, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.9, 94.9, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.90833333333333, 94.90833333333333, 94.90833333333333, 94.90833333333333, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.925, 94.925, 94.925, 94.925, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.91666666666667, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.93333333333334, 94.94166666666666, 94.94166666666666, 94.95, 94.95, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.95833333333333, 94.94166666666666, 94.94166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.277, Test loss: 2.188, Test accuracy: 37.61
Round   0, Global train loss: 2.277, Global test loss: 2.189, Global test accuracy: 37.65
Round   1, Train loss: 1.905, Test loss: 1.749, Test accuracy: 75.63
Round   1, Global train loss: 1.905, Global test loss: 1.697, Global test accuracy: 80.48
Round   2, Train loss: 1.661, Test loss: 1.710, Test accuracy: 77.62
Round   2, Global train loss: 1.661, Global test loss: 1.633, Global test accuracy: 83.94
Round   3, Train loss: 1.627, Test loss: 1.643, Test accuracy: 83.07
Round   3, Global train loss: 1.627, Global test loss: 1.620, Global test accuracy: 84.62
Round   4, Train loss: 1.614, Test loss: 1.639, Test accuracy: 83.38
Round   4, Global train loss: 1.614, Global test loss: 1.613, Global test accuracy: 85.28
Round   5, Train loss: 1.603, Test loss: 1.632, Test accuracy: 83.80
Round   5, Global train loss: 1.603, Global test loss: 1.608, Global test accuracy: 85.70
Round   6, Train loss: 1.598, Test loss: 1.625, Test accuracy: 84.27
Round   6, Global train loss: 1.598, Global test loss: 1.606, Global test accuracy: 85.72
Round   7, Train loss: 1.598, Test loss: 1.618, Test accuracy: 84.73
Round   7, Global train loss: 1.598, Global test loss: 1.604, Global test accuracy: 85.96
Round   8, Train loss: 1.590, Test loss: 1.616, Test accuracy: 84.94
Round   8, Global train loss: 1.590, Global test loss: 1.600, Global test accuracy: 86.31
Round   9, Train loss: 1.592, Test loss: 1.610, Test accuracy: 85.34
Round   9, Global train loss: 1.592, Global test loss: 1.599, Global test accuracy: 86.41
Round  10, Train loss: 1.589, Test loss: 1.606, Test accuracy: 85.72
Round  10, Global train loss: 1.589, Global test loss: 1.598, Global test accuracy: 86.56
Round  11, Train loss: 1.586, Test loss: 1.604, Test accuracy: 85.87
Round  11, Global train loss: 1.586, Global test loss: 1.596, Global test accuracy: 86.83
Round  12, Train loss: 1.584, Test loss: 1.603, Test accuracy: 86.03
Round  12, Global train loss: 1.584, Global test loss: 1.595, Global test accuracy: 86.82
Round  13, Train loss: 1.579, Test loss: 1.600, Test accuracy: 86.22
Round  13, Global train loss: 1.579, Global test loss: 1.594, Global test accuracy: 86.91
Round  14, Train loss: 1.579, Test loss: 1.599, Test accuracy: 86.40
Round  14, Global train loss: 1.579, Global test loss: 1.593, Global test accuracy: 86.95
Round  15, Train loss: 1.574, Test loss: 1.598, Test accuracy: 86.53
Round  15, Global train loss: 1.574, Global test loss: 1.592, Global test accuracy: 86.99
Round  16, Train loss: 1.574, Test loss: 1.597, Test accuracy: 86.60
Round  16, Global train loss: 1.574, Global test loss: 1.591, Global test accuracy: 87.11
Round  17, Train loss: 1.576, Test loss: 1.596, Test accuracy: 86.66
Round  17, Global train loss: 1.576, Global test loss: 1.590, Global test accuracy: 87.28
Round  18, Train loss: 1.577, Test loss: 1.595, Test accuracy: 86.72
Round  18, Global train loss: 1.577, Global test loss: 1.589, Global test accuracy: 87.34
Round  19, Train loss: 1.573, Test loss: 1.594, Test accuracy: 86.83
Round  19, Global train loss: 1.573, Global test loss: 1.588, Global test accuracy: 87.26
Round  20, Train loss: 1.571, Test loss: 1.593, Test accuracy: 86.95
Round  20, Global train loss: 1.571, Global test loss: 1.587, Global test accuracy: 87.43
Round  21, Train loss: 1.571, Test loss: 1.592, Test accuracy: 87.01
Round  21, Global train loss: 1.571, Global test loss: 1.587, Global test accuracy: 87.45
Round  22, Train loss: 1.573, Test loss: 1.592, Test accuracy: 87.10
Round  22, Global train loss: 1.573, Global test loss: 1.587, Global test accuracy: 87.50
Round  23, Train loss: 1.575, Test loss: 1.591, Test accuracy: 87.13
Round  23, Global train loss: 1.575, Global test loss: 1.586, Global test accuracy: 87.50
Round  24, Train loss: 1.570, Test loss: 1.590, Test accuracy: 87.17
Round  24, Global train loss: 1.570, Global test loss: 1.586, Global test accuracy: 87.66
Round  25, Train loss: 1.572, Test loss: 1.590, Test accuracy: 87.23
Round  25, Global train loss: 1.572, Global test loss: 1.586, Global test accuracy: 87.64
Round  26, Train loss: 1.569, Test loss: 1.589, Test accuracy: 87.34
Round  26, Global train loss: 1.569, Global test loss: 1.585, Global test accuracy: 87.75
Round  27, Train loss: 1.568, Test loss: 1.588, Test accuracy: 87.41
Round  27, Global train loss: 1.568, Global test loss: 1.584, Global test accuracy: 87.83
Round  28, Train loss: 1.569, Test loss: 1.588, Test accuracy: 87.43
Round  28, Global train loss: 1.569, Global test loss: 1.584, Global test accuracy: 87.83
Round  29, Train loss: 1.566, Test loss: 1.587, Test accuracy: 87.45
Round  29, Global train loss: 1.566, Global test loss: 1.583, Global test accuracy: 87.84
Round  30, Train loss: 1.568, Test loss: 1.586, Test accuracy: 87.57
Round  30, Global train loss: 1.568, Global test loss: 1.583, Global test accuracy: 87.90
Round  31, Train loss: 1.565, Test loss: 1.586, Test accuracy: 87.64
Round  31, Global train loss: 1.565, Global test loss: 1.582, Global test accuracy: 87.90
Round  32, Train loss: 1.565, Test loss: 1.586, Test accuracy: 87.61
Round  32, Global train loss: 1.565, Global test loss: 1.582, Global test accuracy: 87.95
Round  33, Train loss: 1.571, Test loss: 1.585, Test accuracy: 87.68
Round  33, Global train loss: 1.571, Global test loss: 1.581, Global test accuracy: 87.96
Round  34, Train loss: 1.566, Test loss: 1.585, Test accuracy: 87.72
Round  34, Global train loss: 1.566, Global test loss: 1.581, Global test accuracy: 88.00
Round  35, Train loss: 1.563, Test loss: 1.584, Test accuracy: 87.78
Round  35, Global train loss: 1.563, Global test loss: 1.581, Global test accuracy: 88.10
Round  36, Train loss: 1.566, Test loss: 1.584, Test accuracy: 87.84
Round  36, Global train loss: 1.566, Global test loss: 1.580, Global test accuracy: 88.18
Round  37, Train loss: 1.564, Test loss: 1.583, Test accuracy: 87.92
Round  37, Global train loss: 1.564, Global test loss: 1.580, Global test accuracy: 88.06
Round  38, Train loss: 1.563, Test loss: 1.582, Test accuracy: 87.95
Round  38, Global train loss: 1.563, Global test loss: 1.580, Global test accuracy: 88.05
Round  39, Train loss: 1.563, Test loss: 1.582, Test accuracy: 87.99
Round  39, Global train loss: 1.563, Global test loss: 1.580, Global test accuracy: 88.10
Round  40, Train loss: 1.566, Test loss: 1.582, Test accuracy: 88.03
Round  40, Global train loss: 1.566, Global test loss: 1.579, Global test accuracy: 88.25
Round  41, Train loss: 1.561, Test loss: 1.581, Test accuracy: 88.05
Round  41, Global train loss: 1.561, Global test loss: 1.579, Global test accuracy: 88.19
Round  42, Train loss: 1.560, Test loss: 1.581, Test accuracy: 88.05
Round  42, Global train loss: 1.560, Global test loss: 1.579, Global test accuracy: 88.19
Round  43, Train loss: 1.559, Test loss: 1.581, Test accuracy: 88.06
Round  43, Global train loss: 1.559, Global test loss: 1.578, Global test accuracy: 88.28
Round  44, Train loss: 1.563, Test loss: 1.580, Test accuracy: 88.13
Round  44, Global train loss: 1.563, Global test loss: 1.578, Global test accuracy: 88.28
Round  45, Train loss: 1.560, Test loss: 1.580, Test accuracy: 88.16
Round  45, Global train loss: 1.560, Global test loss: 1.578, Global test accuracy: 88.30
Round  46, Train loss: 1.562, Test loss: 1.580, Test accuracy: 88.16
Round  46, Global train loss: 1.562, Global test loss: 1.578, Global test accuracy: 88.24
Round  47, Train loss: 1.559, Test loss: 1.580, Test accuracy: 88.16
Round  47, Global train loss: 1.559, Global test loss: 1.577, Global test accuracy: 88.36
Round  48, Train loss: 1.557, Test loss: 1.579, Test accuracy: 88.14
Round  48, Global train loss: 1.557, Global test loss: 1.577, Global test accuracy: 88.44
Round  49, Train loss: 1.561, Test loss: 1.577, Test accuracy: 88.43
Round  49, Global train loss: 1.561, Global test loss: 1.575, Global test accuracy: 88.40
Round  50, Train loss: 1.503, Test loss: 1.561, Test accuracy: 90.12
Round  50, Global train loss: 1.503, Global test loss: 1.514, Global test accuracy: 95.17
Round  51, Train loss: 1.486, Test loss: 1.545, Test accuracy: 91.88
Round  51, Global train loss: 1.486, Global test loss: 1.510, Global test accuracy: 95.57
Round  52, Train loss: 1.481, Test loss: 1.540, Test accuracy: 92.41
Round  52, Global train loss: 1.481, Global test loss: 1.507, Global test accuracy: 95.74
Round  53, Train loss: 1.483, Test loss: 1.529, Test accuracy: 93.47
Round  53, Global train loss: 1.483, Global test loss: 1.506, Global test accuracy: 95.80
Round  54, Train loss: 1.478, Test loss: 1.525, Test accuracy: 93.95
Round  54, Global train loss: 1.478, Global test loss: 1.504, Global test accuracy: 95.97
Round  55, Train loss: 1.478, Test loss: 1.513, Test accuracy: 95.15
Round  55, Global train loss: 1.478, Global test loss: 1.503, Global test accuracy: 96.03
Round  56, Train loss: 1.475, Test loss: 1.513, Test accuracy: 95.22
Round  56, Global train loss: 1.475, Global test loss: 1.502, Global test accuracy: 96.19
Round  57, Train loss: 1.475, Test loss: 1.511, Test accuracy: 95.33
Round  57, Global train loss: 1.475, Global test loss: 1.502, Global test accuracy: 96.22
Round  58, Train loss: 1.477, Test loss: 1.506, Test accuracy: 95.79
Round  58, Global train loss: 1.477, Global test loss: 1.502, Global test accuracy: 96.11
Round  59, Train loss: 1.476, Test loss: 1.506, Test accuracy: 95.82
Round  59, Global train loss: 1.476, Global test loss: 1.501, Global test accuracy: 96.21
Round  60, Train loss: 1.473, Test loss: 1.505, Test accuracy: 95.88
Round  60, Global train loss: 1.473, Global test loss: 1.499, Global test accuracy: 96.37
Round  61, Train loss: 1.474, Test loss: 1.505, Test accuracy: 95.92
Round  61, Global train loss: 1.474, Global test loss: 1.499, Global test accuracy: 96.33
Round  62, Train loss: 1.473, Test loss: 1.504, Test accuracy: 95.93
Round  62, Global train loss: 1.473, Global test loss: 1.499, Global test accuracy: 96.47
Round  63, Train loss: 1.474, Test loss: 1.504, Test accuracy: 96.02
Round  63, Global train loss: 1.474, Global test loss: 1.498, Global test accuracy: 96.44
Round  64, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.09
Round  64, Global train loss: 1.475, Global test loss: 1.498, Global test accuracy: 96.41
Round  65, Train loss: 1.473, Test loss: 1.502, Test accuracy: 96.12
Round  65, Global train loss: 1.473, Global test loss: 1.497, Global test accuracy: 96.54
Round  66, Train loss: 1.471, Test loss: 1.501, Test accuracy: 96.17
Round  66, Global train loss: 1.471, Global test loss: 1.497, Global test accuracy: 96.61
Round  67, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.21
Round  67, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.56
Round  68, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.30
Round  68, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.65
Round  69, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.36
Round  69, Global train loss: 1.472, Global test loss: 1.496, Global test accuracy: 96.79
Round  70, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.40
Round  70, Global train loss: 1.471, Global test loss: 1.496, Global test accuracy: 96.77
Round  71, Train loss: 1.473, Test loss: 1.499, Test accuracy: 96.46
Round  71, Global train loss: 1.473, Global test loss: 1.496, Global test accuracy: 96.75
Round  72, Train loss: 1.472, Test loss: 1.498, Test accuracy: 96.56
Round  72, Global train loss: 1.472, Global test loss: 1.495, Global test accuracy: 96.85
Round  73, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.61
Round  73, Global train loss: 1.471, Global test loss: 1.495, Global test accuracy: 96.86
Round  74, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.64
Round  74, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.81
Round  75, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.66
Round  75, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.91
Round  76, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.66
Round  76, Global train loss: 1.471, Global test loss: 1.494, Global test accuracy: 96.91
Round  77, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.64
Round  77, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.86
Round  78, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.67
Round  78, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.90
Round  79, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.70
Round  79, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.85
Round  80, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.70
Round  80, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.78
Round  81, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.71
Round  81, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.86
Round  82, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.71
Round  82, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.97
Round  83, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.72
Round  83, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.86
Round  84, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.72
Round  84, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.77
Round  85, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.75
Round  85, Global train loss: 1.471, Global test loss: 1.493, Global test accuracy: 96.91
Round  86, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.75
Round  86, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.91
Round  87, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.75
Round  87, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.88
Round  88, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.76
Round  88, Global train loss: 1.470, Global test loss: 1.494, Global test accuracy: 96.84
Round  89, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.79
Round  89, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 96.95
Round  90, Train loss: 1.468, Test loss: 1.495, Test accuracy: 96.79
Round  90, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.89
Round  91, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.81
Round  91, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 96.93
Round  92, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.82
Round  92, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.04
Round  93, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.83
Round  93, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 97.05
Round  94, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.82
Round  94, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.97
Round  95, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.82
Round  95, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 97.03/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.85
Round  96, Global train loss: 1.468, Global test loss: 1.492, Global test accuracy: 97.00
Round  97, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.86
Round  97, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.06
Round  98, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.85
Round  98, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 97.03
Round  99, Train loss: 1.469, Test loss: 1.494, Test accuracy: 96.85
Round  99, Global train loss: 1.469, Global test loss: 1.492, Global test accuracy: 96.95
Final Round, Train loss: 1.467, Test loss: 1.493, Test accuracy: 96.90
Final Round, Global train loss: 1.467, Global test loss: 1.492, Global test accuracy: 96.95
Average accuracy final 10 rounds: 96.83024999999999 

Average global accuracy final 10 rounds: 96.993 

6195.196897745132
[4.328021049499512, 8.656042098999023, 12.233827352523804, 15.811612606048584, 19.366617918014526, 22.92162322998047, 26.320698261260986, 29.719773292541504, 33.32843208312988, 36.93709087371826, 40.582886695861816, 44.22868251800537, 47.921260833740234, 51.6138391494751, 55.38750624656677, 59.16117334365845, 62.88951230049133, 66.61785125732422, 70.36571907997131, 74.11358690261841, 77.64364671707153, 81.17370653152466, 84.72921562194824, 88.28472471237183, 92.17254137992859, 96.06035804748535, 99.82534098625183, 103.59032392501831, 107.22282338142395, 110.85532283782959, 114.34169864654541, 117.82807445526123, 121.56597518920898, 125.30387592315674, 129.03696608543396, 132.77005624771118, 136.66104936599731, 140.55204248428345, 144.13570356369019, 147.71936464309692, 151.35888171195984, 154.99839878082275, 158.7974889278412, 162.59657907485962, 166.374694108963, 170.1528091430664, 173.82034516334534, 177.48788118362427, 181.17644238471985, 184.86500358581543, 188.50431370735168, 192.14362382888794, 195.67182779312134, 199.20003175735474, 202.79309225082397, 206.3861527442932, 209.79846620559692, 213.21077966690063, 216.77644801139832, 220.342116355896, 224.0157446861267, 227.68937301635742, 231.28022813796997, 234.87108325958252, 238.28319764137268, 241.69531202316284, 245.31000757217407, 248.9247031211853, 252.5582251548767, 256.1917471885681, 259.7501058578491, 263.3084645271301, 266.85959792137146, 270.4107313156128, 273.9492793083191, 277.4878273010254, 281.15986132621765, 284.8318953514099, 288.4057080745697, 291.9795207977295, 295.51510667800903, 299.0506925582886, 302.48532152175903, 305.9199504852295, 309.3937075138092, 312.8674645423889, 316.4403007030487, 320.0131368637085, 323.6871826648712, 327.36122846603394, 330.8869786262512, 334.4127287864685, 337.85875034332275, 341.304771900177, 344.9512758255005, 348.597779750824, 352.15462923049927, 355.71147871017456, 359.13899779319763, 362.5665168762207, 366.0231547355652, 369.47979259490967, 372.8701739311218, 376.260555267334, 379.791522026062, 383.32248878479004, 386.9863214492798, 390.65015411376953, 394.46179246902466, 398.2734308242798, 401.8970127105713, 405.5205945968628, 409.1043961048126, 412.68819761276245, 416.44082474708557, 420.1934518814087, 423.99629759788513, 427.7991433143616, 431.45573830604553, 435.1123332977295, 438.51364946365356, 441.91496562957764, 445.53325510025024, 449.15154457092285, 452.71889209747314, 456.28623962402344, 459.83713555336, 463.38803148269653, 466.75388741493225, 470.11974334716797, 473.64824533462524, 477.1767473220825, 481.00931549072266, 484.8418836593628, 488.65664863586426, 492.4714136123657, 495.97749066352844, 499.48356771469116, 503.25620794296265, 507.02884817123413, 510.88106989860535, 514.7332916259766, 518.4029834270477, 522.0726752281189, 525.4894318580627, 528.9061884880066, 532.4265768527985, 535.9469652175903, 539.5195081233978, 543.0920510292053, 546.6256515979767, 550.159252166748, 553.6072797775269, 557.0553073883057, 560.5764858722687, 564.0976643562317, 567.5530970096588, 571.0085296630859, 574.5382883548737, 578.0680470466614, 581.6809959411621, 585.2939448356628, 589.0232903957367, 592.7526359558105, 596.4751915931702, 600.1977472305298, 603.8269894123077, 607.4562315940857, 610.9729309082031, 614.4896302223206, 617.9726052284241, 621.4555802345276, 625.0929992198944, 628.7304182052612, 632.3368566036224, 635.9432950019836, 639.9393119812012, 643.9353289604187, 647.8507506847382, 651.7661724090576, 655.6519756317139, 659.5377788543701, 663.5432126522064, 667.5486464500427, 671.7995293140411, 676.0504121780396, 680.2080862522125, 684.3657603263855, 688.5445492267609, 692.7233381271362, 696.8907985687256, 701.0582590103149, 705.222954750061, 709.3876504898071, 713.589518070221, 717.7913856506348, 722.0573630332947, 726.3233404159546, 730.5150344371796, 734.7067284584045, 736.8268640041351, 738.9469995498657]
[37.6125, 37.6125, 75.6275, 75.6275, 77.6175, 77.6175, 83.0725, 83.0725, 83.3775, 83.3775, 83.7975, 83.7975, 84.2675, 84.2675, 84.7275, 84.7275, 84.9425, 84.9425, 85.345, 85.345, 85.715, 85.715, 85.8725, 85.8725, 86.0275, 86.0275, 86.2225, 86.2225, 86.3975, 86.3975, 86.53, 86.53, 86.6025, 86.6025, 86.66, 86.66, 86.725, 86.725, 86.83, 86.83, 86.955, 86.955, 87.0125, 87.0125, 87.1, 87.1, 87.1325, 87.1325, 87.1725, 87.1725, 87.2275, 87.2275, 87.34, 87.34, 87.405, 87.405, 87.4325, 87.4325, 87.455, 87.455, 87.57, 87.57, 87.635, 87.635, 87.6125, 87.6125, 87.68, 87.68, 87.72, 87.72, 87.7775, 87.7775, 87.8425, 87.8425, 87.915, 87.915, 87.95, 87.95, 87.9925, 87.9925, 88.035, 88.035, 88.045, 88.045, 88.05, 88.05, 88.065, 88.065, 88.1325, 88.1325, 88.16, 88.16, 88.155, 88.155, 88.155, 88.155, 88.1375, 88.1375, 88.4275, 88.4275, 90.125, 90.125, 91.88, 91.88, 92.41, 92.41, 93.475, 93.475, 93.95, 93.95, 95.1525, 95.1525, 95.22, 95.22, 95.325, 95.325, 95.7875, 95.7875, 95.8175, 95.8175, 95.875, 95.875, 95.9175, 95.9175, 95.9325, 95.9325, 96.02, 96.02, 96.0925, 96.0925, 96.1175, 96.1175, 96.175, 96.175, 96.2075, 96.2075, 96.3025, 96.3025, 96.3625, 96.3625, 96.4, 96.4, 96.4625, 96.4625, 96.5575, 96.5575, 96.6075, 96.6075, 96.635, 96.635, 96.655, 96.655, 96.655, 96.655, 96.645, 96.645, 96.6675, 96.6675, 96.7025, 96.7025, 96.7025, 96.7025, 96.7075, 96.7075, 96.71, 96.71, 96.72, 96.72, 96.715, 96.715, 96.7475, 96.7475, 96.745, 96.745, 96.75, 96.75, 96.7575, 96.7575, 96.79, 96.79, 96.79, 96.79, 96.815, 96.815, 96.82, 96.82, 96.8275, 96.8275, 96.8225, 96.8225, 96.82, 96.82, 96.85, 96.85, 96.8575, 96.8575, 96.8475, 96.8475, 96.8525, 96.8525, 96.9025, 96.9025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.290, Test accuracy: 32.42
Round   1, Train loss: 2.249, Test loss: 2.133, Test accuracy: 37.03
Round   2, Train loss: 2.007, Test loss: 1.893, Test accuracy: 63.08
Round   3, Train loss: 1.759, Test loss: 1.754, Test accuracy: 75.12
Round   4, Train loss: 1.652, Test loss: 1.667, Test accuracy: 82.14
Round   5, Train loss: 1.597, Test loss: 1.622, Test accuracy: 85.56
Round   6, Train loss: 1.578, Test loss: 1.603, Test accuracy: 87.14
Round   7, Train loss: 1.573, Test loss: 1.583, Test accuracy: 88.94
Round   8, Train loss: 1.552, Test loss: 1.579, Test accuracy: 89.14
Round   9, Train loss: 1.549, Test loss: 1.567, Test accuracy: 90.30
Round  10, Train loss: 1.544, Test loss: 1.563, Test accuracy: 90.49
Round  11, Train loss: 1.545, Test loss: 1.559, Test accuracy: 90.96
Round  12, Train loss: 1.544, Test loss: 1.548, Test accuracy: 91.99
Round  13, Train loss: 1.535, Test loss: 1.547, Test accuracy: 92.03
Round  14, Train loss: 1.531, Test loss: 1.544, Test accuracy: 92.30
Round  15, Train loss: 1.524, Test loss: 1.541, Test accuracy: 92.54
Round  16, Train loss: 1.527, Test loss: 1.539, Test accuracy: 92.65
Round  17, Train loss: 1.525, Test loss: 1.538, Test accuracy: 92.74
Round  18, Train loss: 1.517, Test loss: 1.539, Test accuracy: 92.70
Round  19, Train loss: 1.517, Test loss: 1.538, Test accuracy: 92.75
Round  20, Train loss: 1.518, Test loss: 1.536, Test accuracy: 92.94
Round  21, Train loss: 1.518, Test loss: 1.533, Test accuracy: 93.18
Round  22, Train loss: 1.513, Test loss: 1.531, Test accuracy: 93.39
Round  23, Train loss: 1.512, Test loss: 1.529, Test accuracy: 93.55
Round  24, Train loss: 1.511, Test loss: 1.528, Test accuracy: 93.66
Round  25, Train loss: 1.508, Test loss: 1.527, Test accuracy: 93.85
Round  26, Train loss: 1.505, Test loss: 1.526, Test accuracy: 93.84
Round  27, Train loss: 1.507, Test loss: 1.525, Test accuracy: 93.89
Round  28, Train loss: 1.504, Test loss: 1.524, Test accuracy: 94.00
Round  29, Train loss: 1.501, Test loss: 1.524, Test accuracy: 94.08
Round  30, Train loss: 1.502, Test loss: 1.523, Test accuracy: 94.10
Round  31, Train loss: 1.504, Test loss: 1.522, Test accuracy: 94.19
Round  32, Train loss: 1.500, Test loss: 1.521, Test accuracy: 94.31
Round  33, Train loss: 1.499, Test loss: 1.520, Test accuracy: 94.35
Round  34, Train loss: 1.498, Test loss: 1.520, Test accuracy: 94.40
Round  35, Train loss: 1.495, Test loss: 1.519, Test accuracy: 94.51
Round  36, Train loss: 1.497, Test loss: 1.519, Test accuracy: 94.48
Round  37, Train loss: 1.493, Test loss: 1.519, Test accuracy: 94.50
Round  38, Train loss: 1.493, Test loss: 1.518, Test accuracy: 94.58
Round  39, Train loss: 1.493, Test loss: 1.517, Test accuracy: 94.61
Round  40, Train loss: 1.492, Test loss: 1.516, Test accuracy: 94.72
Round  41, Train loss: 1.491, Test loss: 1.515, Test accuracy: 94.85
Round  42, Train loss: 1.492, Test loss: 1.515, Test accuracy: 94.87
Round  43, Train loss: 1.490, Test loss: 1.515, Test accuracy: 94.91
Round  44, Train loss: 1.489, Test loss: 1.515, Test accuracy: 94.78
Round  45, Train loss: 1.493, Test loss: 1.514, Test accuracy: 94.89
Round  46, Train loss: 1.488, Test loss: 1.514, Test accuracy: 94.98
Round  47, Train loss: 1.491, Test loss: 1.513, Test accuracy: 95.00
Round  48, Train loss: 1.488, Test loss: 1.513, Test accuracy: 94.94
Round  49, Train loss: 1.488, Test loss: 1.513, Test accuracy: 94.99
Round  50, Train loss: 1.486, Test loss: 1.513, Test accuracy: 94.99
Round  51, Train loss: 1.488, Test loss: 1.511, Test accuracy: 95.16
Round  52, Train loss: 1.486, Test loss: 1.512, Test accuracy: 95.15
Round  53, Train loss: 1.486, Test loss: 1.511, Test accuracy: 95.19
Round  54, Train loss: 1.485, Test loss: 1.511, Test accuracy: 95.23
Round  55, Train loss: 1.486, Test loss: 1.510, Test accuracy: 95.28
Round  56, Train loss: 1.484, Test loss: 1.510, Test accuracy: 95.28
Round  57, Train loss: 1.484, Test loss: 1.510, Test accuracy: 95.32
Round  58, Train loss: 1.485, Test loss: 1.510, Test accuracy: 95.34
Round  59, Train loss: 1.482, Test loss: 1.509, Test accuracy: 95.44
Round  60, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.51
Round  61, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.54
Round  62, Train loss: 1.483, Test loss: 1.508, Test accuracy: 95.48
Round  63, Train loss: 1.482, Test loss: 1.508, Test accuracy: 95.55
Round  64, Train loss: 1.481, Test loss: 1.507, Test accuracy: 95.56
Round  65, Train loss: 1.482, Test loss: 1.507, Test accuracy: 95.57
Round  66, Train loss: 1.481, Test loss: 1.507, Test accuracy: 95.64
Round  67, Train loss: 1.482, Test loss: 1.507, Test accuracy: 95.58
Round  68, Train loss: 1.481, Test loss: 1.507, Test accuracy: 95.60
Round  69, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.68
Round  70, Train loss: 1.481, Test loss: 1.506, Test accuracy: 95.71
Round  71, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.70
Round  72, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.73
Round  73, Train loss: 1.479, Test loss: 1.506, Test accuracy: 95.76
Round  74, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.73
Round  75, Train loss: 1.479, Test loss: 1.506, Test accuracy: 95.70
Round  76, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.73
Round  77, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.79
Round  78, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.81
Round  79, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.82
Round  80, Train loss: 1.479, Test loss: 1.505, Test accuracy: 95.82
Round  81, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.81
Round  82, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.81
Round  83, Train loss: 1.478, Test loss: 1.505, Test accuracy: 95.78
Round  84, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.84
Round  85, Train loss: 1.477, Test loss: 1.504, Test accuracy: 95.86
Round  86, Train loss: 1.477, Test loss: 1.504, Test accuracy: 95.84
Round  87, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.90
Round  88, Train loss: 1.478, Test loss: 1.503, Test accuracy: 95.90
Round  89, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.88
Round  90, Train loss: 1.478, Test loss: 1.504, Test accuracy: 95.89
Round  91, Train loss: 1.476, Test loss: 1.504, Test accuracy: 95.86
Round  92, Train loss: 1.477, Test loss: 1.504, Test accuracy: 95.89
Round  93, Train loss: 1.476, Test loss: 1.503, Test accuracy: 95.92
Round  94, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.94
Round  95, Train loss: 1.476, Test loss: 1.503, Test accuracy: 95.92
Round  96, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.94
Round  97, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.94
Round  98, Train loss: 1.475, Test loss: 1.503, Test accuracy: 95.90/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.00
Final Round, Train loss: 1.473, Test loss: 1.503, Test accuracy: 96.00
Average accuracy final 10 rounds: 95.92125000000001 

5055.900146245956
[3.770472764968872, 7.540945529937744, 11.193896532058716, 14.846847534179688, 18.486830711364746, 22.126813888549805, 25.674368381500244, 29.221922874450684, 32.83134412765503, 36.440765380859375, 40.156097412109375, 43.871429443359375, 47.611443758010864, 51.35145807266235, 54.92490315437317, 58.498348236083984, 62.04292845726013, 65.58750867843628, 69.25226593017578, 72.91702318191528, 76.54789447784424, 80.1787657737732, 83.7350206375122, 87.29127550125122, 90.76462841033936, 94.23798131942749, 97.92938256263733, 101.62078380584717, 105.41699314117432, 109.21320247650146, 112.72685217857361, 116.24050188064575, 119.75422143936157, 123.26794099807739, 126.89236187934875, 130.51678276062012, 134.15819668769836, 137.7996106147766, 141.31707215309143, 144.83453369140625, 148.33265948295593, 151.83078527450562, 155.44263887405396, 159.0544924736023, 162.6395845413208, 166.2246766090393, 169.5831413269043, 172.9416060447693, 176.4235224723816, 179.9054388999939, 183.51801800727844, 187.130597114563, 190.6697859764099, 194.20897483825684, 197.6245243549347, 201.04007387161255, 204.59679675102234, 208.15351963043213, 211.70966863632202, 215.2658176422119, 218.81061029434204, 222.35540294647217, 225.88000321388245, 229.40460348129272, 232.98751640319824, 236.57042932510376, 240.2518970966339, 243.93336486816406, 247.4856719970703, 251.03797912597656, 254.62200665473938, 258.2060341835022, 261.8925211429596, 265.579008102417, 269.2625324726105, 272.94605684280396, 276.6918921470642, 280.43772745132446, 284.20812606811523, 287.978524684906, 291.66976261138916, 295.3610005378723, 299.1417453289032, 302.9224901199341, 306.6382477283478, 310.3540053367615, 314.00088810920715, 317.64777088165283, 321.3874809741974, 325.12719106674194, 328.9436225891113, 332.7600541114807, 336.36236810684204, 339.96468210220337, 343.721759557724, 347.47883701324463, 351.28741931915283, 355.09600162506104, 358.7161228656769, 362.3362441062927, 366.0266463756561, 369.71704864501953, 373.47442746162415, 377.23180627822876, 380.86772775650024, 384.50364923477173, 388.34601068496704, 392.18837213516235, 395.82367181777954, 399.45897150039673, 403.1073637008667, 406.75575590133667, 410.55601263046265, 414.3562693595886, 418.1413447856903, 421.926420211792, 425.6101698875427, 429.29391956329346, 432.89132952690125, 436.48873949050903, 440.3516263961792, 444.21451330184937, 447.9824733734131, 451.7504334449768, 455.3999516963959, 459.04946994781494, 462.8023202419281, 466.55517053604126, 470.31137108802795, 474.06757164001465, 477.8596887588501, 481.65180587768555, 485.34379291534424, 489.03577995300293, 492.7288017272949, 496.4218235015869, 500.11673378944397, 503.811644077301, 507.55518913269043, 511.29873418807983, 515.0056052207947, 518.7124762535095, 522.3926792144775, 526.0728821754456, 529.6594295501709, 533.2459769248962, 536.8447754383087, 540.4435739517212, 544.1395304203033, 547.8354868888855, 551.4792845249176, 555.1230821609497, 558.7689762115479, 562.414870262146, 566.0052773952484, 569.5956845283508, 573.3489322662354, 577.1021800041199, 580.7584772109985, 584.4147744178772, 588.0723028182983, 591.7298312187195, 595.3480713367462, 598.966311454773, 602.6972298622131, 606.4281482696533, 610.1093754768372, 613.790602684021, 617.5474054813385, 621.304208278656, 624.9252066612244, 628.5462050437927, 632.1355810165405, 635.7249569892883, 639.4190785884857, 643.1132001876831, 646.8580379486084, 650.6028757095337, 654.2744085788727, 657.9459414482117, 661.6201884746552, 665.2944355010986, 668.9910151958466, 672.6875948905945, 676.3632171154022, 680.03883934021, 683.8591341972351, 687.6794290542603, 691.3564584255219, 695.0334877967834, 698.7902374267578, 702.5469870567322, 706.3003823757172, 710.0537776947021, 713.8349816799164, 717.6161856651306, 721.305447101593, 724.9947085380554, 728.6983458995819, 732.4019832611084, 734.2377140522003, 736.0734448432922]
[32.4175, 32.4175, 37.03, 37.03, 63.0775, 63.0775, 75.1175, 75.1175, 82.1425, 82.1425, 85.56, 85.56, 87.14, 87.14, 88.94, 88.94, 89.14, 89.14, 90.3, 90.3, 90.4875, 90.4875, 90.96, 90.96, 91.99, 91.99, 92.0325, 92.0325, 92.295, 92.295, 92.5425, 92.5425, 92.6475, 92.6475, 92.74, 92.74, 92.6975, 92.6975, 92.745, 92.745, 92.94, 92.94, 93.1825, 93.1825, 93.395, 93.395, 93.5525, 93.5525, 93.6575, 93.6575, 93.8475, 93.8475, 93.84, 93.84, 93.895, 93.895, 94.0, 94.0, 94.085, 94.085, 94.1025, 94.1025, 94.19, 94.19, 94.3125, 94.3125, 94.3525, 94.3525, 94.4025, 94.4025, 94.5125, 94.5125, 94.4825, 94.4825, 94.505, 94.505, 94.585, 94.585, 94.605, 94.605, 94.715, 94.715, 94.8475, 94.8475, 94.8725, 94.8725, 94.905, 94.905, 94.775, 94.775, 94.8875, 94.8875, 94.985, 94.985, 95.0025, 95.0025, 94.9375, 94.9375, 94.99, 94.99, 94.99, 94.99, 95.1625, 95.1625, 95.15, 95.15, 95.195, 95.195, 95.23, 95.23, 95.2775, 95.2775, 95.2775, 95.2775, 95.32, 95.32, 95.345, 95.345, 95.445, 95.445, 95.5075, 95.5075, 95.54, 95.54, 95.4775, 95.4775, 95.5525, 95.5525, 95.5625, 95.5625, 95.57, 95.57, 95.6375, 95.6375, 95.5775, 95.5775, 95.5975, 95.5975, 95.6775, 95.6775, 95.7075, 95.7075, 95.705, 95.705, 95.73, 95.73, 95.7575, 95.7575, 95.735, 95.735, 95.7, 95.7, 95.7325, 95.7325, 95.7925, 95.7925, 95.8125, 95.8125, 95.8175, 95.8175, 95.8175, 95.8175, 95.805, 95.805, 95.805, 95.805, 95.775, 95.775, 95.8375, 95.8375, 95.855, 95.855, 95.845, 95.845, 95.9, 95.9, 95.9, 95.9, 95.875, 95.875, 95.89, 95.89, 95.865, 95.865, 95.895, 95.895, 95.92, 95.92, 95.935, 95.935, 95.925, 95.925, 95.945, 95.945, 95.935, 95.935, 95.9, 95.9, 96.0025, 96.0025, 96.005, 96.005]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.286, Test loss: 2.213, Test accuracy: 27.58
Round   1, Train loss: 1.974, Test loss: 1.823, Test accuracy: 68.16
Round   2, Train loss: 1.702, Test loss: 1.704, Test accuracy: 77.57
Round   3, Train loss: 1.653, Test loss: 1.652, Test accuracy: 81.87
Round   4, Train loss: 1.629, Test loss: 1.635, Test accuracy: 83.12
Round   5, Train loss: 1.622, Test loss: 1.629, Test accuracy: 83.62
Round   6, Train loss: 1.606, Test loss: 1.620, Test accuracy: 84.39
Round   7, Train loss: 1.600, Test loss: 1.611, Test accuracy: 85.39
Round   8, Train loss: 1.586, Test loss: 1.606, Test accuracy: 85.88
Round   9, Train loss: 1.585, Test loss: 1.598, Test accuracy: 86.63
Round  10, Train loss: 1.586, Test loss: 1.595, Test accuracy: 86.98
Round  11, Train loss: 1.576, Test loss: 1.592, Test accuracy: 87.16
Round  12, Train loss: 1.595, Test loss: 1.589, Test accuracy: 87.42
Round  13, Train loss: 1.550, Test loss: 1.585, Test accuracy: 87.89
Round  14, Train loss: 1.578, Test loss: 1.567, Test accuracy: 89.82
Round  15, Train loss: 1.530, Test loss: 1.556, Test accuracy: 91.00
Round  16, Train loss: 1.512, Test loss: 1.549, Test accuracy: 91.56
Round  17, Train loss: 1.504, Test loss: 1.541, Test accuracy: 92.40
Round  18, Train loss: 1.505, Test loss: 1.531, Test accuracy: 93.40
Round  19, Train loss: 1.496, Test loss: 1.530, Test accuracy: 93.44
Round  20, Train loss: 1.498, Test loss: 1.523, Test accuracy: 94.12
Round  21, Train loss: 1.493, Test loss: 1.519, Test accuracy: 94.55
Round  22, Train loss: 1.491, Test loss: 1.518, Test accuracy: 94.62
Round  23, Train loss: 1.493, Test loss: 1.512, Test accuracy: 95.26
Round  24, Train loss: 1.492, Test loss: 1.511, Test accuracy: 95.42
Round  25, Train loss: 1.487, Test loss: 1.511, Test accuracy: 95.37
Round  26, Train loss: 1.487, Test loss: 1.509, Test accuracy: 95.57
Round  27, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.62
Round  28, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.59
Round  29, Train loss: 1.484, Test loss: 1.507, Test accuracy: 95.75
Round  30, Train loss: 1.482, Test loss: 1.506, Test accuracy: 95.79
Round  31, Train loss: 1.482, Test loss: 1.506, Test accuracy: 95.81
Round  32, Train loss: 1.481, Test loss: 1.506, Test accuracy: 95.81
Round  33, Train loss: 1.480, Test loss: 1.506, Test accuracy: 95.82
Round  34, Train loss: 1.483, Test loss: 1.505, Test accuracy: 95.89
Round  35, Train loss: 1.480, Test loss: 1.505, Test accuracy: 95.92
Round  36, Train loss: 1.478, Test loss: 1.504, Test accuracy: 96.01
Round  37, Train loss: 1.480, Test loss: 1.504, Test accuracy: 96.07
Round  38, Train loss: 1.479, Test loss: 1.503, Test accuracy: 96.11
Round  39, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.02
Round  40, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.11
Round  41, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.21
Round  42, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.20
Round  43, Train loss: 1.477, Test loss: 1.502, Test accuracy: 96.17
Round  44, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.21
Round  45, Train loss: 1.476, Test loss: 1.501, Test accuracy: 96.22
Round  46, Train loss: 1.474, Test loss: 1.502, Test accuracy: 96.19
Round  47, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.23
Round  48, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19
Round  49, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19
Round  50, Train loss: 1.475, Test loss: 1.501, Test accuracy: 96.19
Round  51, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.25
Round  52, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.26
Round  53, Train loss: 1.474, Test loss: 1.501, Test accuracy: 96.27
Round  54, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.31
Round  55, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.30
Round  56, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.23
Round  57, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.35
Round  58, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.33
Round  59, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.37
Round  60, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.33
Round  61, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.35
Round  62, Train loss: 1.473, Test loss: 1.499, Test accuracy: 96.43
Round  63, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.44
Round  64, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.43
Round  65, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.38
Round  66, Train loss: 1.474, Test loss: 1.499, Test accuracy: 96.39
Round  67, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.43
Round  68, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.42
Round  69, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.45
Round  70, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.45
Round  71, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.45
Round  72, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.44
Round  73, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.41
Round  74, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.43
Round  75, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.44
Round  76, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.44
Round  77, Train loss: 1.470, Test loss: 1.499, Test accuracy: 96.45
Round  78, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.50
Round  79, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.50
Round  80, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.43
Round  81, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.48
Round  82, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.49
Round  83, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.50
Round  84, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.55
Round  85, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.56
Round  86, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.59
Round  87, Train loss: 1.469, Test loss: 1.498, Test accuracy: 96.56
Round  88, Train loss: 1.471, Test loss: 1.498, Test accuracy: 96.56
Round  89, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.51
Round  90, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.52
Round  91, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.56
Round  92, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  93, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.62
Round  94, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.62
Round  95, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  96, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.62
Round  97, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.61
Round  98, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.65/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.60
Final Round, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.60
Average accuracy final 10 rounds: 96.60125000000002 

5146.573312520981
[4.458303451538086, 8.916606903076172, 13.355732202529907, 17.794857501983643, 22.42243790626526, 27.050018310546875, 31.63066267967224, 36.21130704879761, 40.61610412597656, 45.02090120315552, 49.58607888221741, 54.1512565612793, 58.54452300071716, 62.93778944015503, 67.3659598827362, 71.79413032531738, 76.29548740386963, 80.79684448242188, 85.52058601379395, 90.24432754516602, 94.93474817276001, 99.625168800354, 104.23228716850281, 108.83940553665161, 113.29285192489624, 117.74629831314087, 122.31684374809265, 126.88738918304443, 131.63823294639587, 136.38907670974731, 141.054851770401, 145.7206268310547, 150.15317559242249, 154.58572435379028, 158.95872449874878, 163.33172464370728, 167.39677906036377, 171.46183347702026, 175.36926674842834, 179.27670001983643, 183.14829516410828, 187.01989030838013, 190.8140013217926, 194.60811233520508, 198.45685386657715, 202.30559539794922, 206.09298419952393, 209.88037300109863, 213.7561218738556, 217.63187074661255, 221.49764347076416, 225.36341619491577, 229.16742849349976, 232.97144079208374, 236.78398513793945, 240.59652948379517, 244.43052005767822, 248.26451063156128, 252.0473186969757, 255.83012676239014, 259.6512014865875, 263.4722762107849, 267.336056470871, 271.19983673095703, 274.9384672641754, 278.6770977973938, 282.4383313655853, 286.19956493377686, 289.98763632774353, 293.7757077217102, 297.7463104724884, 301.7169132232666, 305.5000057220459, 309.2830982208252, 313.0031840801239, 316.7232699394226, 320.43761229515076, 324.1519546508789, 328.0272352695465, 331.9025158882141, 335.8390974998474, 339.7756791114807, 343.5042781829834, 347.2328772544861, 350.97466254234314, 354.7164478302002, 358.5481812953949, 362.3799147605896, 366.31887316703796, 370.2578315734863, 374.16518235206604, 378.07253313064575, 381.81149077415466, 385.5504484176636, 389.39490509033203, 393.2393617630005, 397.14451265335083, 401.0496635437012, 404.9487590789795, 408.8478546142578, 412.57490849494934, 416.30196237564087, 420.1469078063965, 423.9918532371521, 427.9079885482788, 431.8241238594055, 435.70342445373535, 439.5827250480652, 443.32956886291504, 447.0764126777649, 450.82083082199097, 454.56524896621704, 458.3528187274933, 462.14038848876953, 465.90378046035767, 469.6671724319458, 473.46956419944763, 477.27195596694946, 481.0035047531128, 484.7350535392761, 488.53151774406433, 492.32798194885254, 496.12220883369446, 499.9164357185364, 503.6652183532715, 507.4140009880066, 511.16329646110535, 514.9125919342041, 518.7468817234039, 522.5811715126038, 526.3970584869385, 530.2129454612732, 533.9878680706024, 537.7627906799316, 541.5231380462646, 545.2834854125977, 549.0527865886688, 552.82208776474, 556.4022777080536, 559.9824676513672, 563.4493193626404, 566.9161710739136, 570.5050768852234, 574.0939826965332, 577.6790244579315, 581.2640662193298, 584.8568322658539, 588.4495983123779, 591.9793770313263, 595.5091557502747, 599.1367921829224, 602.7644286155701, 606.3385846614838, 609.9127407073975, 613.4508438110352, 616.9889469146729, 620.4735898971558, 623.9582328796387, 627.5827493667603, 631.2072658538818, 634.8789203166962, 638.5505747795105, 642.0905072689056, 645.6304397583008, 649.1559278964996, 652.6814160346985, 656.2595338821411, 659.8376517295837, 663.4306991100311, 667.0237464904785, 670.4487106800079, 673.8736748695374, 677.4661660194397, 681.058657169342, 684.6235370635986, 688.1884169578552, 691.6563813686371, 695.124345779419, 698.6297724246979, 702.1351990699768, 705.7089025974274, 709.2826061248779, 712.875422000885, 716.4682378768921, 719.9323990345001, 723.3965601921082, 727.0278613567352, 730.6591625213623, 734.1449775695801, 737.6307926177979, 741.1550853252411, 744.6793780326843, 748.2851660251617, 751.8909540176392, 755.4013369083405, 758.9117197990417, 762.4566621780396, 766.0016045570374, 769.555112361908, 773.1086201667786, 774.7474224567413, 776.3862247467041]
[27.58, 27.58, 68.1575, 68.1575, 77.5725, 77.5725, 81.8675, 81.8675, 83.12, 83.12, 83.6175, 83.6175, 84.3875, 84.3875, 85.3925, 85.3925, 85.8825, 85.8825, 86.63, 86.63, 86.985, 86.985, 87.1625, 87.1625, 87.425, 87.425, 87.8875, 87.8875, 89.8175, 89.8175, 91.0, 91.0, 91.565, 91.565, 92.4025, 92.4025, 93.4025, 93.4025, 93.44, 93.44, 94.1175, 94.1175, 94.545, 94.545, 94.62, 94.62, 95.2575, 95.2575, 95.415, 95.415, 95.37, 95.37, 95.5675, 95.5675, 95.6175, 95.6175, 95.595, 95.595, 95.7525, 95.7525, 95.7925, 95.7925, 95.81, 95.81, 95.8075, 95.8075, 95.8225, 95.8225, 95.89, 95.89, 95.925, 95.925, 96.0125, 96.0125, 96.0725, 96.0725, 96.115, 96.115, 96.0175, 96.0175, 96.1125, 96.1125, 96.21, 96.21, 96.1975, 96.1975, 96.1725, 96.1725, 96.2075, 96.2075, 96.2225, 96.2225, 96.185, 96.185, 96.2275, 96.2275, 96.1875, 96.1875, 96.19, 96.19, 96.195, 96.195, 96.2525, 96.2525, 96.26, 96.26, 96.27, 96.27, 96.3125, 96.3125, 96.295, 96.295, 96.235, 96.235, 96.35, 96.35, 96.33, 96.33, 96.3725, 96.3725, 96.3275, 96.3275, 96.35, 96.35, 96.4325, 96.4325, 96.435, 96.435, 96.43, 96.43, 96.3775, 96.3775, 96.395, 96.395, 96.43, 96.43, 96.42, 96.42, 96.4475, 96.4475, 96.455, 96.455, 96.4525, 96.4525, 96.4425, 96.4425, 96.405, 96.405, 96.4275, 96.4275, 96.44, 96.44, 96.4425, 96.4425, 96.455, 96.455, 96.4975, 96.4975, 96.505, 96.505, 96.43, 96.43, 96.4775, 96.4775, 96.49, 96.49, 96.505, 96.505, 96.5475, 96.5475, 96.5625, 96.5625, 96.59, 96.59, 96.5575, 96.5575, 96.555, 96.555, 96.5125, 96.5125, 96.5225, 96.5225, 96.565, 96.565, 96.585, 96.585, 96.625, 96.625, 96.6225, 96.6225, 96.6125, 96.6125, 96.62, 96.62, 96.6075, 96.6075, 96.65, 96.65, 96.6025, 96.6025, 96.6025, 96.6025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.300, Test accuracy: 24.78
Round   1, Train loss: 2.297, Test loss: 2.297, Test accuracy: 30.50
Round   2, Train loss: 2.295, Test loss: 2.294, Test accuracy: 30.50
Round   3, Train loss: 2.288, Test loss: 2.289, Test accuracy: 29.83
Round   4, Train loss: 2.280, Test loss: 2.280, Test accuracy: 29.65
Round   5, Train loss: 2.253, Test loss: 2.254, Test accuracy: 28.85
Round   6, Train loss: 2.212, Test loss: 2.212, Test accuracy: 28.62
Round   7, Train loss: 2.146, Test loss: 2.166, Test accuracy: 34.28
Round   8, Train loss: 2.126, Test loss: 2.125, Test accuracy: 38.58
Round   9, Train loss: 2.066, Test loss: 2.073, Test accuracy: 44.05
Round  10, Train loss: 1.988, Test loss: 2.018, Test accuracy: 51.72
Round  11, Train loss: 1.959, Test loss: 1.970, Test accuracy: 53.98
Round  12, Train loss: 1.911, Test loss: 1.931, Test accuracy: 56.67
Round  13, Train loss: 1.865, Test loss: 1.901, Test accuracy: 60.88
Round  14, Train loss: 1.808, Test loss: 1.882, Test accuracy: 62.22
Round  15, Train loss: 1.800, Test loss: 1.858, Test accuracy: 64.00
Round  16, Train loss: 1.779, Test loss: 1.836, Test accuracy: 66.75
Round  17, Train loss: 1.726, Test loss: 1.827, Test accuracy: 67.03
Round  18, Train loss: 1.710, Test loss: 1.817, Test accuracy: 67.65
Round  19, Train loss: 1.733, Test loss: 1.806, Test accuracy: 68.28
Round  20, Train loss: 1.712, Test loss: 1.799, Test accuracy: 68.53
Round  21, Train loss: 1.729, Test loss: 1.778, Test accuracy: 70.07
Round  22, Train loss: 1.701, Test loss: 1.773, Test accuracy: 70.30
Round  23, Train loss: 1.682, Test loss: 1.766, Test accuracy: 70.52
Round  24, Train loss: 1.666, Test loss: 1.756, Test accuracy: 73.23
Round  25, Train loss: 1.636, Test loss: 1.748, Test accuracy: 73.82
Round  26, Train loss: 1.613, Test loss: 1.742, Test accuracy: 74.23
Round  27, Train loss: 1.621, Test loss: 1.735, Test accuracy: 74.57
Round  28, Train loss: 1.608, Test loss: 1.730, Test accuracy: 75.00
Round  29, Train loss: 1.605, Test loss: 1.724, Test accuracy: 75.27
Round  30, Train loss: 1.609, Test loss: 1.720, Test accuracy: 75.65
Round  31, Train loss: 1.594, Test loss: 1.716, Test accuracy: 76.03
Round  32, Train loss: 1.575, Test loss: 1.714, Test accuracy: 76.03
Round  33, Train loss: 1.576, Test loss: 1.712, Test accuracy: 76.13
Round  34, Train loss: 1.582, Test loss: 1.711, Test accuracy: 76.22
Round  35, Train loss: 1.581, Test loss: 1.709, Test accuracy: 76.32
Round  36, Train loss: 1.569, Test loss: 1.708, Test accuracy: 76.37
Round  37, Train loss: 1.586, Test loss: 1.707, Test accuracy: 76.42
Round  38, Train loss: 1.575, Test loss: 1.704, Test accuracy: 76.53
Round  39, Train loss: 1.535, Test loss: 1.676, Test accuracy: 80.77
Round  40, Train loss: 1.527, Test loss: 1.667, Test accuracy: 81.27
Round  41, Train loss: 1.506, Test loss: 1.661, Test accuracy: 81.93
Round  42, Train loss: 1.493, Test loss: 1.657, Test accuracy: 82.12
Round  43, Train loss: 1.491, Test loss: 1.654, Test accuracy: 82.27
Round  44, Train loss: 1.493, Test loss: 1.652, Test accuracy: 82.20
Round  45, Train loss: 1.496, Test loss: 1.648, Test accuracy: 82.63
Round  46, Train loss: 1.481, Test loss: 1.647, Test accuracy: 82.73
Round  47, Train loss: 1.486, Test loss: 1.645, Test accuracy: 82.85
Round  48, Train loss: 1.488, Test loss: 1.643, Test accuracy: 83.02
Round  49, Train loss: 1.486, Test loss: 1.643, Test accuracy: 82.97
Round  50, Train loss: 1.475, Test loss: 1.642, Test accuracy: 83.00
Round  51, Train loss: 1.480, Test loss: 1.640, Test accuracy: 83.02
Round  52, Train loss: 1.481, Test loss: 1.641, Test accuracy: 83.12
Round  53, Train loss: 1.482, Test loss: 1.640, Test accuracy: 83.15
Round  54, Train loss: 1.473, Test loss: 1.640, Test accuracy: 83.13
Round  55, Train loss: 1.478, Test loss: 1.640, Test accuracy: 83.15
Round  56, Train loss: 1.472, Test loss: 1.639, Test accuracy: 83.10
Round  57, Train loss: 1.476, Test loss: 1.638, Test accuracy: 83.22
Round  58, Train loss: 1.478, Test loss: 1.638, Test accuracy: 83.08
Round  59, Train loss: 1.477, Test loss: 1.638, Test accuracy: 83.10
Round  60, Train loss: 1.475, Test loss: 1.637, Test accuracy: 83.22
Round  61, Train loss: 1.475, Test loss: 1.637, Test accuracy: 83.17
Round  62, Train loss: 1.475, Test loss: 1.637, Test accuracy: 83.22
Round  63, Train loss: 1.476, Test loss: 1.637, Test accuracy: 83.27
Round  64, Train loss: 1.467, Test loss: 1.637, Test accuracy: 83.18
Round  65, Train loss: 1.476, Test loss: 1.637, Test accuracy: 83.17
Round  66, Train loss: 1.476, Test loss: 1.638, Test accuracy: 83.18
Round  67, Train loss: 1.471, Test loss: 1.637, Test accuracy: 83.28
Round  68, Train loss: 1.471, Test loss: 1.637, Test accuracy: 83.18
Round  69, Train loss: 1.474, Test loss: 1.637, Test accuracy: 83.08
Round  70, Train loss: 1.476, Test loss: 1.636, Test accuracy: 83.22
Round  71, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.15
Round  72, Train loss: 1.474, Test loss: 1.636, Test accuracy: 83.13
Round  73, Train loss: 1.478, Test loss: 1.636, Test accuracy: 83.15
Round  74, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.07
Round  75, Train loss: 1.473, Test loss: 1.636, Test accuracy: 83.13
Round  76, Train loss: 1.473, Test loss: 1.636, Test accuracy: 83.10
Round  77, Train loss: 1.474, Test loss: 1.636, Test accuracy: 83.08
Round  78, Train loss: 1.473, Test loss: 1.636, Test accuracy: 83.10
Round  79, Train loss: 1.477, Test loss: 1.636, Test accuracy: 83.07
Round  80, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.07
Round  81, Train loss: 1.475, Test loss: 1.636, Test accuracy: 83.07
Round  82, Train loss: 1.472, Test loss: 1.635, Test accuracy: 83.12
Round  83, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.13
Round  84, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.12
Round  85, Train loss: 1.471, Test loss: 1.635, Test accuracy: 83.15
Round  86, Train loss: 1.470, Test loss: 1.635, Test accuracy: 83.15
Round  87, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.05
Round  88, Train loss: 1.470, Test loss: 1.635, Test accuracy: 83.10
Round  89, Train loss: 1.470, Test loss: 1.635, Test accuracy: 83.12
Round  90, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.08
Round  91, Train loss: 1.472, Test loss: 1.635, Test accuracy: 83.12
Round  92, Train loss: 1.473, Test loss: 1.635, Test accuracy: 83.10
Round  93, Train loss: 1.475, Test loss: 1.635, Test accuracy: 83.12
Round  94, Train loss: 1.472, Test loss: 1.634, Test accuracy: 83.05
Round  95, Train loss: 1.476, Test loss: 1.634, Test accuracy: 83.08
Round  96, Train loss: 1.473, Test loss: 1.634, Test accuracy: 83.08
Round  97, Train loss: 1.475, Test loss: 1.634, Test accuracy: 83.10
Round  98, Train loss: 1.475, Test loss: 1.634, Test accuracy: 83.12
Round  99, Train loss: 1.478, Test loss: 1.635, Test accuracy: 83.08/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.472, Test loss: 1.634, Test accuracy: 83.22
Average accuracy final 10 rounds: 83.09333333333333 

731.9901342391968
[0.6391983032226562, 1.2783966064453125, 1.8654911518096924, 2.4525856971740723, 3.034757375717163, 3.616929054260254, 4.136734485626221, 4.6565399169921875, 5.198365688323975, 5.740191459655762, 6.258649110794067, 6.777106761932373, 7.340189218521118, 7.903271675109863, 8.433603525161743, 8.963935375213623, 9.46847152709961, 9.973007678985596, 10.512972831726074, 11.052937984466553, 11.580363273620605, 12.107788562774658, 12.647832155227661, 13.187875747680664, 13.709259271621704, 14.230642795562744, 14.748307943344116, 15.265973091125488, 15.810805320739746, 16.355637550354004, 16.894309282302856, 17.43298101425171, 17.951533794403076, 18.470086574554443, 18.99966025352478, 19.529233932495117, 20.0368754863739, 20.544517040252686, 21.09084725379944, 21.63717746734619, 22.142024040222168, 22.646870613098145, 23.169878482818604, 23.692886352539062, 24.212071657180786, 24.73125696182251, 25.236923456192017, 25.742589950561523, 26.25899624824524, 26.775402545928955, 27.31085205078125, 27.846301555633545, 28.372254371643066, 28.898207187652588, 29.435012340545654, 29.97181749343872, 30.48974061012268, 31.00766372680664, 31.539568662643433, 32.071473598480225, 32.58342146873474, 33.09536933898926, 33.623335123062134, 34.15130090713501, 34.69672513008118, 35.242149353027344, 35.75456929206848, 36.26698923110962, 36.795308351516724, 37.32362747192383, 37.878312826156616, 38.432998180389404, 38.96994376182556, 39.50688934326172, 40.04806876182556, 40.589248180389404, 41.11412334442139, 41.63899850845337, 42.16348671913147, 42.68797492980957, 43.22293758392334, 43.75790023803711, 44.274624824523926, 44.79134941101074, 45.32703924179077, 45.8627290725708, 46.389689207077026, 46.91664934158325, 47.44992733001709, 47.98320531845093, 48.48983955383301, 48.99647378921509, 49.51246404647827, 50.028454303741455, 50.570643186569214, 51.11283206939697, 51.64701175689697, 52.18119144439697, 52.70453667640686, 53.22788190841675, 53.783621311187744, 54.33936071395874, 54.875335693359375, 55.41131067276001, 55.954819202423096, 56.49832773208618, 57.02813649177551, 57.557945251464844, 58.11192083358765, 58.66589641571045, 59.220948934555054, 59.77600145339966, 60.297343015670776, 60.818684577941895, 61.31932735443115, 61.81997013092041, 62.37077522277832, 62.92158031463623, 63.46300029754639, 64.00442028045654, 64.54496955871582, 65.0855188369751, 65.62561964988708, 66.16572046279907, 66.72922325134277, 67.29272603988647, 67.82836842536926, 68.36401081085205, 68.88929390907288, 69.4145770072937, 69.96890830993652, 70.52323961257935, 71.07453632354736, 71.62583303451538, 72.16946148872375, 72.71308994293213, 73.24405455589294, 73.77501916885376, 74.28044104576111, 74.78586292266846, 75.3494644165039, 75.91306591033936, 76.44299960136414, 76.97293329238892, 77.50799584388733, 78.04305839538574, 78.59666061401367, 79.1502628326416, 79.70123291015625, 80.2522029876709, 80.778315782547, 81.3044285774231, 81.83393335342407, 82.36343812942505, 82.9020848274231, 83.44073152542114, 83.9949460029602, 84.54916048049927, 85.06814360618591, 85.58712673187256, 86.1210150718689, 86.65490341186523, 87.17307829856873, 87.69125318527222, 88.22336053848267, 88.75546789169312, 89.30192518234253, 89.84838247299194, 90.3848946094513, 90.92140674591064, 91.47529458999634, 92.02918243408203, 92.55845594406128, 93.08772945404053, 93.59572052955627, 94.10371160507202, 94.6441445350647, 95.18457746505737, 95.7295618057251, 96.27454614639282, 96.83733439445496, 97.40012264251709, 97.93120884895325, 98.4622950553894, 98.98944997787476, 99.51660490036011, 100.05062103271484, 100.58463716506958, 101.11128258705139, 101.6379280090332, 102.16739416122437, 102.69686031341553, 103.25537419319153, 103.81388807296753, 104.35599398612976, 104.89809989929199, 105.45996069908142, 106.02182149887085, 106.5453188419342, 107.06881618499756, 108.15942358970642, 109.25003099441528]
[24.783333333333335, 24.783333333333335, 30.5, 30.5, 30.5, 30.5, 29.833333333333332, 29.833333333333332, 29.65, 29.65, 28.85, 28.85, 28.616666666666667, 28.616666666666667, 34.28333333333333, 34.28333333333333, 38.583333333333336, 38.583333333333336, 44.05, 44.05, 51.71666666666667, 51.71666666666667, 53.983333333333334, 53.983333333333334, 56.666666666666664, 56.666666666666664, 60.88333333333333, 60.88333333333333, 62.21666666666667, 62.21666666666667, 64.0, 64.0, 66.75, 66.75, 67.03333333333333, 67.03333333333333, 67.65, 67.65, 68.28333333333333, 68.28333333333333, 68.53333333333333, 68.53333333333333, 70.06666666666666, 70.06666666666666, 70.3, 70.3, 70.51666666666667, 70.51666666666667, 73.23333333333333, 73.23333333333333, 73.81666666666666, 73.81666666666666, 74.23333333333333, 74.23333333333333, 74.56666666666666, 74.56666666666666, 75.0, 75.0, 75.26666666666667, 75.26666666666667, 75.65, 75.65, 76.03333333333333, 76.03333333333333, 76.03333333333333, 76.03333333333333, 76.13333333333334, 76.13333333333334, 76.21666666666667, 76.21666666666667, 76.31666666666666, 76.31666666666666, 76.36666666666666, 76.36666666666666, 76.41666666666667, 76.41666666666667, 76.53333333333333, 76.53333333333333, 80.76666666666667, 80.76666666666667, 81.26666666666667, 81.26666666666667, 81.93333333333334, 81.93333333333334, 82.11666666666666, 82.11666666666666, 82.26666666666667, 82.26666666666667, 82.2, 82.2, 82.63333333333334, 82.63333333333334, 82.73333333333333, 82.73333333333333, 82.85, 82.85, 83.01666666666667, 83.01666666666667, 82.96666666666667, 82.96666666666667, 83.0, 83.0, 83.01666666666667, 83.01666666666667, 83.11666666666666, 83.11666666666666, 83.15, 83.15, 83.13333333333334, 83.13333333333334, 83.15, 83.15, 83.1, 83.1, 83.21666666666667, 83.21666666666667, 83.08333333333333, 83.08333333333333, 83.1, 83.1, 83.21666666666667, 83.21666666666667, 83.16666666666667, 83.16666666666667, 83.21666666666667, 83.21666666666667, 83.26666666666667, 83.26666666666667, 83.18333333333334, 83.18333333333334, 83.16666666666667, 83.16666666666667, 83.18333333333334, 83.18333333333334, 83.28333333333333, 83.28333333333333, 83.18333333333334, 83.18333333333334, 83.08333333333333, 83.08333333333333, 83.21666666666667, 83.21666666666667, 83.15, 83.15, 83.13333333333334, 83.13333333333334, 83.15, 83.15, 83.06666666666666, 83.06666666666666, 83.13333333333334, 83.13333333333334, 83.1, 83.1, 83.08333333333333, 83.08333333333333, 83.1, 83.1, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.06666666666666, 83.11666666666666, 83.11666666666666, 83.13333333333334, 83.13333333333334, 83.11666666666666, 83.11666666666666, 83.15, 83.15, 83.15, 83.15, 83.05, 83.05, 83.1, 83.1, 83.11666666666666, 83.11666666666666, 83.08333333333333, 83.08333333333333, 83.11666666666666, 83.11666666666666, 83.1, 83.1, 83.11666666666666, 83.11666666666666, 83.05, 83.05, 83.08333333333333, 83.08333333333333, 83.08333333333333, 83.08333333333333, 83.1, 83.1, 83.11666666666666, 83.11666666666666, 83.08333333333333, 83.08333333333333, 83.21666666666667, 83.21666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.719, Test loss: 2.299, Test accuracy: 18.68
Round   1, Train loss: 1.699, Test loss: 2.294, Test accuracy: 22.30
Round   2, Train loss: 1.655, Test loss: 2.285, Test accuracy: 30.58
Round   3, Train loss: 1.554, Test loss: 2.267, Test accuracy: 42.13
Round   4, Train loss: 1.449, Test loss: 2.233, Test accuracy: 52.02
Round   5, Train loss: 1.372, Test loss: 2.183, Test accuracy: 60.17
Round   6, Train loss: 1.340, Test loss: 2.149, Test accuracy: 64.22
Round   7, Train loss: 1.287, Test loss: 2.123, Test accuracy: 66.57
Round   8, Train loss: 1.256, Test loss: 2.105, Test accuracy: 68.43
Round   9, Train loss: 1.237, Test loss: 2.085, Test accuracy: 70.62
Round  10, Train loss: 1.223, Test loss: 2.071, Test accuracy: 74.23
Round  11, Train loss: 1.221, Test loss: 2.062, Test accuracy: 76.15
Round  12, Train loss: 1.201, Test loss: 2.051, Test accuracy: 76.50
Round  13, Train loss: 1.207, Test loss: 2.044, Test accuracy: 76.58
Round  14, Train loss: 1.194, Test loss: 2.037, Test accuracy: 78.17
Round  15, Train loss: 1.184, Test loss: 2.032, Test accuracy: 77.93
Round  16, Train loss: 1.184, Test loss: 2.027, Test accuracy: 78.05
Round  17, Train loss: 1.182, Test loss: 2.021, Test accuracy: 78.28
Round  18, Train loss: 1.185, Test loss: 2.018, Test accuracy: 78.12
Round  19, Train loss: 1.189, Test loss: 2.015, Test accuracy: 78.33
Round  20, Train loss: 1.180, Test loss: 2.011, Test accuracy: 78.07
Round  21, Train loss: 1.165, Test loss: 2.008, Test accuracy: 77.73
Round  22, Train loss: 1.172, Test loss: 2.006, Test accuracy: 77.65
Round  23, Train loss: 1.173, Test loss: 2.003, Test accuracy: 77.53
Round  24, Train loss: 1.176, Test loss: 2.001, Test accuracy: 77.32
Round  25, Train loss: 1.170, Test loss: 2.000, Test accuracy: 77.02
Round  26, Train loss: 1.173, Test loss: 1.999, Test accuracy: 77.12
Round  27, Train loss: 1.178, Test loss: 1.997, Test accuracy: 76.87
Round  28, Train loss: 1.179, Test loss: 1.997, Test accuracy: 76.48
Round  29, Train loss: 1.174, Test loss: 1.995, Test accuracy: 76.07
Round  30, Train loss: 1.177, Test loss: 1.994, Test accuracy: 75.78
Round  31, Train loss: 1.181, Test loss: 1.993, Test accuracy: 75.80
Round  32, Train loss: 1.175, Test loss: 1.992, Test accuracy: 75.68
Round  33, Train loss: 1.175, Test loss: 1.992, Test accuracy: 75.02
Round  34, Train loss: 1.174, Test loss: 1.992, Test accuracy: 74.72
Round  35, Train loss: 1.174, Test loss: 1.991, Test accuracy: 74.53
Round  36, Train loss: 1.171, Test loss: 1.990, Test accuracy: 74.25
Round  37, Train loss: 1.174, Test loss: 1.990, Test accuracy: 74.12
Round  38, Train loss: 1.174, Test loss: 1.989, Test accuracy: 74.27
Round  39, Train loss: 1.172, Test loss: 1.989, Test accuracy: 73.92
Round  40, Train loss: 1.175, Test loss: 1.989, Test accuracy: 73.65
Round  41, Train loss: 1.177, Test loss: 1.989, Test accuracy: 73.40
Round  42, Train loss: 1.176, Test loss: 1.990, Test accuracy: 73.20
Round  43, Train loss: 1.164, Test loss: 1.989, Test accuracy: 72.88
Round  44, Train loss: 1.172, Test loss: 1.987, Test accuracy: 72.48
Round  45, Train loss: 1.168, Test loss: 1.988, Test accuracy: 72.25
Round  46, Train loss: 1.176, Test loss: 1.987, Test accuracy: 72.18
Round  47, Train loss: 1.168, Test loss: 1.988, Test accuracy: 71.77
Round  48, Train loss: 1.165, Test loss: 1.988, Test accuracy: 71.62
Round  49, Train loss: 1.169, Test loss: 1.988, Test accuracy: 71.18
Round  50, Train loss: 1.169, Test loss: 1.988, Test accuracy: 70.92
Round  51, Train loss: 1.172, Test loss: 1.987, Test accuracy: 70.87
Round  52, Train loss: 1.170, Test loss: 1.987, Test accuracy: 70.83
Round  53, Train loss: 1.176, Test loss: 1.987, Test accuracy: 70.50
Round  54, Train loss: 1.167, Test loss: 1.988, Test accuracy: 70.27
Round  55, Train loss: 1.169, Test loss: 1.987, Test accuracy: 70.28
Round  56, Train loss: 1.172, Test loss: 1.986, Test accuracy: 70.23
Round  57, Train loss: 1.166, Test loss: 1.986, Test accuracy: 69.92
Round  58, Train loss: 1.176, Test loss: 1.987, Test accuracy: 69.83
Round  59, Train loss: 1.178, Test loss: 1.987, Test accuracy: 69.47
Round  60, Train loss: 1.177, Test loss: 1.987, Test accuracy: 69.47
Round  61, Train loss: 1.177, Test loss: 1.987, Test accuracy: 69.52
Round  62, Train loss: 1.175, Test loss: 1.987, Test accuracy: 69.32
Round  63, Train loss: 1.169, Test loss: 1.987, Test accuracy: 69.05
Round  64, Train loss: 1.166, Test loss: 1.987, Test accuracy: 69.00
Round  65, Train loss: 1.169, Test loss: 1.988, Test accuracy: 68.85
Round  66, Train loss: 1.179, Test loss: 1.987, Test accuracy: 68.75
Round  67, Train loss: 1.164, Test loss: 1.987, Test accuracy: 68.22
Round  68, Train loss: 1.174, Test loss: 1.987, Test accuracy: 68.10
Round  69, Train loss: 1.171, Test loss: 1.988, Test accuracy: 67.85
Round  70, Train loss: 1.168, Test loss: 1.988, Test accuracy: 67.72
Round  71, Train loss: 1.173, Test loss: 1.989, Test accuracy: 67.42
Round  72, Train loss: 1.175, Test loss: 1.989, Test accuracy: 67.35
Round  73, Train loss: 1.168, Test loss: 1.988, Test accuracy: 67.10
Round  74, Train loss: 1.170, Test loss: 1.989, Test accuracy: 67.07
Round  75, Train loss: 1.173, Test loss: 1.989, Test accuracy: 66.83
Round  76, Train loss: 1.170, Test loss: 1.989, Test accuracy: 66.52
Round  77, Train loss: 1.160, Test loss: 1.989, Test accuracy: 66.28
Round  78, Train loss: 1.172, Test loss: 1.989, Test accuracy: 66.45
Round  79, Train loss: 1.174, Test loss: 1.990, Test accuracy: 66.07
Round  80, Train loss: 1.173, Test loss: 1.989, Test accuracy: 66.07
Round  81, Train loss: 1.162, Test loss: 1.989, Test accuracy: 65.88
Round  82, Train loss: 1.175, Test loss: 1.989, Test accuracy: 66.07
Round  83, Train loss: 1.170, Test loss: 1.990, Test accuracy: 65.92
Round  84, Train loss: 1.174, Test loss: 1.990, Test accuracy: 65.68
Round  85, Train loss: 1.170, Test loss: 1.990, Test accuracy: 65.72
Round  86, Train loss: 1.177, Test loss: 1.990, Test accuracy: 65.63
Round  87, Train loss: 1.181, Test loss: 1.990, Test accuracy: 65.48
Round  88, Train loss: 1.169, Test loss: 1.991, Test accuracy: 65.27
Round  89, Train loss: 1.169, Test loss: 1.990, Test accuracy: 65.15
Round  90, Train loss: 1.161, Test loss: 1.990, Test accuracy: 65.18
Round  91, Train loss: 1.169, Test loss: 1.991, Test accuracy: 64.93
Round  92, Train loss: 1.164, Test loss: 1.991, Test accuracy: 64.97
Round  93, Train loss: 1.165, Test loss: 1.991, Test accuracy: 64.73
Round  94, Train loss: 1.173, Test loss: 1.991, Test accuracy: 64.73
Round  95, Train loss: 1.178, Test loss: 1.992, Test accuracy: 64.62
Round  96, Train loss: 1.171, Test loss: 1.991, Test accuracy: 64.67
Round  97, Train loss: 1.167, Test loss: 1.992, Test accuracy: 64.50
Round  98, Train loss: 1.172, Test loss: 1.992, Test accuracy: 64.25
Round  99, Train loss: 1.180, Test loss: 1.992, Test accuracy: 64.25
Final Round, Train loss: 1.170, Test loss: 1.993, Test accuracy: 63.75
Average accuracy final 10 rounds: 64.68333333333334
831.1946139335632
[]
[18.683333333333334, 22.3, 30.583333333333332, 42.13333333333333, 52.016666666666666, 60.166666666666664, 64.21666666666667, 66.56666666666666, 68.43333333333334, 70.61666666666666, 74.23333333333333, 76.15, 76.5, 76.58333333333333, 78.16666666666667, 77.93333333333334, 78.05, 78.28333333333333, 78.11666666666666, 78.33333333333333, 78.06666666666666, 77.73333333333333, 77.65, 77.53333333333333, 77.31666666666666, 77.01666666666667, 77.11666666666666, 76.86666666666666, 76.48333333333333, 76.06666666666666, 75.78333333333333, 75.8, 75.68333333333334, 75.01666666666667, 74.71666666666667, 74.53333333333333, 74.25, 74.11666666666666, 74.26666666666667, 73.91666666666667, 73.65, 73.4, 73.2, 72.88333333333334, 72.48333333333333, 72.25, 72.18333333333334, 71.76666666666667, 71.61666666666666, 71.18333333333334, 70.91666666666667, 70.86666666666666, 70.83333333333333, 70.5, 70.26666666666667, 70.28333333333333, 70.23333333333333, 69.91666666666667, 69.83333333333333, 69.46666666666667, 69.46666666666667, 69.51666666666667, 69.31666666666666, 69.05, 69.0, 68.85, 68.75, 68.21666666666667, 68.1, 67.85, 67.71666666666667, 67.41666666666667, 67.35, 67.1, 67.06666666666666, 66.83333333333333, 66.51666666666667, 66.28333333333333, 66.45, 66.06666666666666, 66.06666666666666, 65.88333333333334, 66.06666666666666, 65.91666666666667, 65.68333333333334, 65.71666666666667, 65.63333333333334, 65.48333333333333, 65.26666666666667, 65.15, 65.18333333333334, 64.93333333333334, 64.96666666666667, 64.73333333333333, 64.73333333333333, 64.61666666666666, 64.66666666666667, 64.5, 64.25, 64.25, 63.75]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.96
Round   1, Train loss: 2.311, Test loss: 2.299, Test accuracy: 16.91
Round   2, Train loss: 2.301, Test loss: 2.297, Test accuracy: 18.95
Round   3, Train loss: 2.294, Test loss: 2.295, Test accuracy: 21.01
Round   4, Train loss: 2.291, Test loss: 2.293, Test accuracy: 21.01
Round   5, Train loss: 2.289, Test loss: 2.291, Test accuracy: 22.27
Round   6, Train loss: 2.291, Test loss: 2.292, Test accuracy: 21.47
Round   7, Train loss: 2.275, Test loss: 2.285, Test accuracy: 21.71
Round   8, Train loss: 2.225, Test loss: 2.270, Test accuracy: 26.27
Round   9, Train loss: 2.227, Test loss: 2.263, Test accuracy: 26.17
Round  10, Train loss: 2.197, Test loss: 2.257, Test accuracy: 25.22
Round  11, Train loss: 1.999, Test loss: 2.219, Test accuracy: 31.28
Round  12, Train loss: 2.002, Test loss: 2.191, Test accuracy: 34.77
Round  13, Train loss: 1.962, Test loss: 2.179, Test accuracy: 35.17
Round  14, Train loss: 1.437, Test loss: 2.103, Test accuracy: 44.08
Round  15, Train loss: 1.970, Test loss: 2.107, Test accuracy: 43.86
Round  16, Train loss: 2.212, Test loss: 2.145, Test accuracy: 38.16
Round  17, Train loss: 1.340, Test loss: 2.065, Test accuracy: 45.31
Round  18, Train loss: 1.671, Test loss: 2.060, Test accuracy: 45.45
Round  19, Train loss: 2.182, Test loss: 2.069, Test accuracy: 42.58
Round  20, Train loss: 1.270, Test loss: 2.029, Test accuracy: 45.02
Round  21, Train loss: 1.400, Test loss: 1.998, Test accuracy: 48.05
Round  22, Train loss: 0.947, Test loss: 1.946, Test accuracy: 52.28
Round  23, Train loss: 1.398, Test loss: 1.968, Test accuracy: 50.72
Round  24, Train loss: 2.440, Test loss: 2.052, Test accuracy: 42.01
Round  25, Train loss: 2.412, Test loss: 2.104, Test accuracy: 35.15
Round  26, Train loss: 1.420, Test loss: 2.034, Test accuracy: 42.52
Round  27, Train loss: 1.307, Test loss: 2.004, Test accuracy: 45.84
Round  28, Train loss: 0.929, Test loss: 1.912, Test accuracy: 55.67
Round  29, Train loss: 1.346, Test loss: 1.901, Test accuracy: 57.63
Round  30, Train loss: 1.644, Test loss: 1.902, Test accuracy: 58.23
Round  31, Train loss: 2.161, Test loss: 1.965, Test accuracy: 52.66
Round  32, Train loss: 1.481, Test loss: 1.948, Test accuracy: 54.91
Round  33, Train loss: 1.574, Test loss: 1.937, Test accuracy: 55.01
Round  34, Train loss: 0.806, Test loss: 1.895, Test accuracy: 59.31
Round  35, Train loss: 1.244, Test loss: 1.906, Test accuracy: 58.02
Round  36, Train loss: 0.900, Test loss: 1.860, Test accuracy: 62.09
Round  37, Train loss: 1.206, Test loss: 1.881, Test accuracy: 60.08
Round  38, Train loss: 0.956, Test loss: 1.846, Test accuracy: 62.92
Round  39, Train loss: 0.926, Test loss: 1.839, Test accuracy: 63.12
Round  40, Train loss: 0.807, Test loss: 1.818, Test accuracy: 65.47
Round  41, Train loss: 0.612, Test loss: 1.799, Test accuracy: 67.25
Round  42, Train loss: 0.234, Test loss: 1.788, Test accuracy: 68.16
Round  43, Train loss: 0.470, Test loss: 1.765, Test accuracy: 70.78
Round  44, Train loss: 0.504, Test loss: 1.781, Test accuracy: 69.03
Round  45, Train loss: 0.381, Test loss: 1.783, Test accuracy: 68.61
Round  46, Train loss: 0.773, Test loss: 1.806, Test accuracy: 66.27
Round  47, Train loss: 0.447, Test loss: 1.780, Test accuracy: 68.96
Round  48, Train loss: 0.486, Test loss: 1.780, Test accuracy: 69.03
Round  49, Train loss: 0.503, Test loss: 1.779, Test accuracy: 68.98
Round  50, Train loss: 0.471, Test loss: 1.758, Test accuracy: 71.20
Round  51, Train loss: 0.213, Test loss: 1.767, Test accuracy: 70.06
Round  52, Train loss: 0.098, Test loss: 1.745, Test accuracy: 72.32
Round  53, Train loss: 0.447, Test loss: 1.749, Test accuracy: 71.87
Round  54, Train loss: 0.150, Test loss: 1.723, Test accuracy: 74.50
Round  55, Train loss: 0.544, Test loss: 1.754, Test accuracy: 71.43
Round  56, Train loss: 0.400, Test loss: 1.766, Test accuracy: 70.14
Round  57, Train loss: 0.113, Test loss: 1.728, Test accuracy: 73.97
Round  58, Train loss: 0.046, Test loss: 1.733, Test accuracy: 73.42
Round  59, Train loss: 0.300, Test loss: 1.752, Test accuracy: 71.53
Round  60, Train loss: -0.239, Test loss: 1.722, Test accuracy: 74.41
Round  61, Train loss: -0.013, Test loss: 1.728, Test accuracy: 73.84
Round  62, Train loss: -0.022, Test loss: 1.729, Test accuracy: 73.61
Round  63, Train loss: -0.350, Test loss: 1.730, Test accuracy: 73.34
Round  64, Train loss: -0.616, Test loss: 1.697, Test accuracy: 76.68
Round  65, Train loss: 0.308, Test loss: 1.723, Test accuracy: 74.23
Round  66, Train loss: 0.290, Test loss: 1.735, Test accuracy: 72.94
Round  67, Train loss: -0.469, Test loss: 1.692, Test accuracy: 77.27
Round  68, Train loss: 0.415, Test loss: 1.716, Test accuracy: 74.92
Round  69, Train loss: 0.210, Test loss: 1.718, Test accuracy: 74.67
Round  70, Train loss: 0.275, Test loss: 1.725, Test accuracy: 74.12
Round  71, Train loss: 0.015, Test loss: 1.712, Test accuracy: 75.54
Round  72, Train loss: -0.007, Test loss: 1.699, Test accuracy: 76.85
Round  73, Train loss: 0.273, Test loss: 1.710, Test accuracy: 75.83
Round  74, Train loss: -0.341, Test loss: 1.695, Test accuracy: 77.37
Round  75, Train loss: -0.147, Test loss: 1.703, Test accuracy: 76.71
Round  76, Train loss: -0.070, Test loss: 1.716, Test accuracy: 75.48
Round  77, Train loss: 0.061, Test loss: 1.696, Test accuracy: 77.49
Round  78, Train loss: -0.013, Test loss: 1.697, Test accuracy: 77.05
Round  79, Train loss: 0.106, Test loss: 1.691, Test accuracy: 77.65
Round  80, Train loss: 0.006, Test loss: 1.711, Test accuracy: 75.58
Round  81, Train loss: -0.066, Test loss: 1.701, Test accuracy: 76.46
Round  82, Train loss: -0.198, Test loss: 1.685, Test accuracy: 78.28
Round  83, Train loss: -0.247, Test loss: 1.688, Test accuracy: 77.85
Round  84, Train loss: -0.468, Test loss: 1.691, Test accuracy: 77.44
Round  85, Train loss: -0.323, Test loss: 1.686, Test accuracy: 77.97
Round  86, Train loss: -0.320, Test loss: 1.685, Test accuracy: 77.96
Round  87, Train loss: 0.004, Test loss: 1.691, Test accuracy: 77.33
Round  88, Train loss: -0.307, Test loss: 1.680, Test accuracy: 78.43
Round  89, Train loss: -0.243, Test loss: 1.690, Test accuracy: 77.47
Round  90, Train loss: -0.594, Test loss: 1.677, Test accuracy: 78.72
Round  91, Train loss: -0.170, Test loss: 1.686, Test accuracy: 77.88
Round  92, Train loss: -0.276, Test loss: 1.690, Test accuracy: 77.55
Round  93, Train loss: -0.245, Test loss: 1.679, Test accuracy: 78.69
Round  94, Train loss: -0.401, Test loss: 1.677, Test accuracy: 78.96
Round  95, Train loss: -0.342, Test loss: 1.682, Test accuracy: 78.34
Round  96, Train loss: -0.518, Test loss: 1.678, Test accuracy: 78.67
Round  97, Train loss: -0.259, Test loss: 1.682, Test accuracy: 78.27
Round  98, Train loss: -0.515, Test loss: 1.686, Test accuracy: 77.90
Round  99, Train loss: -0.287, Test loss: 1.675, Test accuracy: 78.99
Final Round, Train loss: 1.710, Test loss: 1.707, Test accuracy: 76.31
Average accuracy final 10 rounds: 78.39750000000001
Average global accuracy final 10 rounds: 78.39750000000001
1292.2029466629028
[]
[13.958333333333334, 16.908333333333335, 18.95, 21.008333333333333, 21.008333333333333, 22.275, 21.466666666666665, 21.708333333333332, 26.266666666666666, 26.166666666666668, 25.216666666666665, 31.283333333333335, 34.766666666666666, 35.175, 44.075, 43.858333333333334, 38.15833333333333, 45.30833333333333, 45.45, 42.575, 45.025, 48.05, 52.28333333333333, 50.71666666666667, 42.00833333333333, 35.15, 42.516666666666666, 45.84166666666667, 55.675, 57.63333333333333, 58.225, 52.65833333333333, 54.90833333333333, 55.00833333333333, 59.30833333333333, 58.016666666666666, 62.09166666666667, 60.083333333333336, 62.925, 63.11666666666667, 65.46666666666667, 67.25, 68.15833333333333, 70.775, 69.025, 68.60833333333333, 66.26666666666667, 68.95833333333333, 69.03333333333333, 68.98333333333333, 71.2, 70.05833333333334, 72.31666666666666, 71.86666666666666, 74.5, 71.43333333333334, 70.14166666666667, 73.96666666666667, 73.425, 71.53333333333333, 74.40833333333333, 73.84166666666667, 73.60833333333333, 73.34166666666667, 76.68333333333334, 74.23333333333333, 72.94166666666666, 77.26666666666667, 74.91666666666667, 74.675, 74.125, 75.54166666666667, 76.85, 75.83333333333333, 77.36666666666666, 76.70833333333333, 75.48333333333333, 77.49166666666666, 77.05, 77.65, 75.575, 76.45833333333333, 78.28333333333333, 77.85, 77.44166666666666, 77.975, 77.95833333333333, 77.33333333333333, 78.43333333333334, 77.475, 78.71666666666667, 77.88333333333334, 77.55, 78.69166666666666, 78.95833333333333, 78.34166666666667, 78.675, 78.26666666666667, 77.9, 78.99166666666666, 76.30833333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.37
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.38
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.38
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.39
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.37
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.39
Round   3, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.38
Round   3, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.39
Round   4, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.38
Round   4, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.39
Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.39
Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 9.38
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 9.44
Round   7, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.39
Round   7, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.43
Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round   9, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round   9, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  10, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  10, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  11, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  11, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  12, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  12, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  13, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  13, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  14, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  15, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  15, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 9.44
Round  16, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.45
Round  16, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  17, Train loss: 2.302, Test loss: 2.303, Test accuracy: 9.44
Round  17, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  18, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.44
Round  18, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  19, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.44
Round  19, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  20, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.45
Round  20, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.47
Round  21, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.46
Round  21, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  22, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.46
Round  22, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  23, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  24, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.47
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.48
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.49
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  59, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  61, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  62, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  63, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  63, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  64, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  65, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.51
Round  65, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  66, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  66, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  67, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  67, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  68, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  68, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  69, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.52
Round  69, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  70, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  71, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  71, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  72, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  72, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  73, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  73, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  74, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.53
Round  74, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  75, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.54
Round  75, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  76, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.55
Round  76, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  77, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  77, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.54
Round  78, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.57
Round  78, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.54
Round  79, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.57
Round  79, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.54
Round  80, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  80, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  81, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  81, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.57
Round  82, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.57
Round  83, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.58
Round  83, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  84, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.58
Round  84, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.59
Round  85, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.58
Round  85, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  86, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.60
Round  86, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  87, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.60
Round  87, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  88, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.61
Round  88, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.59
Round  89, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.60
Round  89, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.62
Round  90, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.63
Round  90, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.63
Round  91, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.65
Round  91, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.68
Round  92, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.65
Round  92, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.64
Round  93, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.64
Round  93, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.64
Round  94, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.65
Round  94, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Round  95, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.66
Round  95, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Round  96, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.66
Round  96, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.70
Round  97, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.65
Round  97, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.66
Round  98, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Round  99, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.68
Round  99, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Final Round, Train loss: 2.301, Test loss: 2.301, Test accuracy: 9.69
Final Round, Global train loss: 2.301, Global test loss: 2.301, Global test accuracy: 9.66
Average accuracy final 10 rounds: 9.6525 

Average global accuracy final 10 rounds: 9.658333333333331 

1555.272881269455
[1.296515703201294, 2.5437402725219727, 3.7805838584899902, 4.935051679611206, 6.125579833984375, 7.404958724975586, 8.690626382827759, 9.933756351470947, 11.12629508972168, 12.365580797195435, 13.67121434211731, 14.960506916046143, 16.202524662017822, 17.448606252670288, 18.705224990844727, 19.984943866729736, 21.24941635131836, 22.47550654411316, 23.71184277534485, 24.989012241363525, 26.285255670547485, 27.545510053634644, 28.801198720932007, 30.02093744277954, 31.277238130569458, 32.579107999801636, 33.88041305541992, 35.10579586029053, 36.319775104522705, 37.636680126190186, 38.90259289741516, 40.17058873176575, 41.40043020248413, 42.67914843559265, 43.96069669723511, 45.212814807891846, 46.415985107421875, 47.65088963508606, 48.980292558670044, 50.27839469909668, 51.536787033081055, 52.80252003669739, 54.04987573623657, 55.31317138671875, 56.61258268356323, 57.90086269378662, 59.188443660736084, 60.452582597732544, 61.74173545837402, 63.04247498512268, 64.25906538963318, 65.46394491195679, 66.64602088928223, 67.85388779640198, 69.05333685874939, 70.168527841568, 71.41051816940308, 72.77644538879395, 74.09136056900024, 75.38304018974304, 76.63316583633423, 77.90718221664429, 79.1943199634552, 80.4692370891571, 81.70615458488464, 82.91433215141296, 84.18845891952515, 85.43416833877563, 86.73744893074036, 88.0253746509552, 89.25355410575867, 90.54275107383728, 91.87713551521301, 93.14519500732422, 94.42100024223328, 95.6778872013092, 97.0012788772583, 98.29925680160522, 99.61009955406189, 100.83241248130798, 102.0585560798645, 103.28940320014954, 104.44377207756042, 105.59585881233215, 106.82708978652954, 108.09629893302917, 109.30764961242676, 110.48341083526611, 111.64731931686401, 112.92252564430237, 114.19318270683289, 115.52127242088318, 116.84069418907166, 118.07437992095947, 119.32302522659302, 120.62693881988525, 121.91274690628052, 123.21844553947449, 124.52076411247253, 125.7873899936676, 127.94734025001526]
[9.366666666666667, 9.375, 9.366666666666667, 9.375, 9.375, 9.391666666666667, 9.383333333333333, 9.391666666666667, 9.425, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.45, 9.441666666666666, 9.441666666666666, 9.441666666666666, 9.45, 9.458333333333334, 9.458333333333334, 9.475, 9.475, 9.475, 9.483333333333333, 9.483333333333333, 9.491666666666667, 9.491666666666667, 9.483333333333333, 9.483333333333333, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.483333333333333, 9.483333333333333, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.491666666666667, 9.5, 9.508333333333333, 9.508333333333333, 9.508333333333333, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.5, 9.508333333333333, 9.516666666666667, 9.516666666666667, 9.516666666666667, 9.516666666666667, 9.533333333333333, 9.533333333333333, 9.533333333333333, 9.525, 9.525, 9.541666666666666, 9.55, 9.558333333333334, 9.566666666666666, 9.566666666666666, 9.558333333333334, 9.558333333333334, 9.566666666666666, 9.583333333333334, 9.583333333333334, 9.583333333333334, 9.6, 9.6, 9.608333333333333, 9.6, 9.633333333333333, 9.65, 9.65, 9.641666666666667, 9.65, 9.658333333333333, 9.658333333333333, 9.65, 9.658333333333333, 9.675, 9.691666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.298, Test accuracy: 18.69
Round   1, Train loss: 2.297, Test loss: 2.287, Test accuracy: 21.94
Round   2, Train loss: 2.282, Test loss: 2.209, Test accuracy: 33.19
Round   3, Train loss: 2.195, Test loss: 2.051, Test accuracy: 45.93
Round   4, Train loss: 2.064, Test loss: 1.913, Test accuracy: 56.42
Round   5, Train loss: 1.957, Test loss: 1.824, Test accuracy: 66.60
Round   6, Train loss: 1.846, Test loss: 1.766, Test accuracy: 71.67
Round   7, Train loss: 1.809, Test loss: 1.746, Test accuracy: 72.81
Round   8, Train loss: 1.795, Test loss: 1.734, Test accuracy: 73.57
Round   9, Train loss: 1.805, Test loss: 1.730, Test accuracy: 73.77
Round  10, Train loss: 1.719, Test loss: 1.725, Test accuracy: 74.13
Round  11, Train loss: 1.723, Test loss: 1.722, Test accuracy: 74.28
Round  12, Train loss: 1.717, Test loss: 1.719, Test accuracy: 74.49
Round  13, Train loss: 1.702, Test loss: 1.715, Test accuracy: 74.53
Round  14, Train loss: 1.693, Test loss: 1.658, Test accuracy: 81.48
Round  15, Train loss: 1.658, Test loss: 1.647, Test accuracy: 82.08
Round  16, Train loss: 1.649, Test loss: 1.641, Test accuracy: 82.65
Round  17, Train loss: 1.637, Test loss: 1.636, Test accuracy: 82.93
Round  18, Train loss: 1.639, Test loss: 1.634, Test accuracy: 83.08
Round  19, Train loss: 1.615, Test loss: 1.631, Test accuracy: 83.33
Round  20, Train loss: 1.613, Test loss: 1.629, Test accuracy: 83.49
Round  21, Train loss: 1.613, Test loss: 1.628, Test accuracy: 83.62
Round  22, Train loss: 1.609, Test loss: 1.626, Test accuracy: 83.93
Round  23, Train loss: 1.612, Test loss: 1.625, Test accuracy: 83.87
Round  24, Train loss: 1.605, Test loss: 1.625, Test accuracy: 83.79
Round  25, Train loss: 1.596, Test loss: 1.624, Test accuracy: 83.97
Round  26, Train loss: 1.600, Test loss: 1.622, Test accuracy: 84.08
Round  27, Train loss: 1.595, Test loss: 1.622, Test accuracy: 84.03
Round  28, Train loss: 1.604, Test loss: 1.621, Test accuracy: 84.25
Round  29, Train loss: 1.585, Test loss: 1.620, Test accuracy: 84.23
Round  30, Train loss: 1.591, Test loss: 1.620, Test accuracy: 84.26
Round  31, Train loss: 1.595, Test loss: 1.619, Test accuracy: 84.34
Round  32, Train loss: 1.595, Test loss: 1.619, Test accuracy: 84.47
Round  33, Train loss: 1.592, Test loss: 1.617, Test accuracy: 84.53
Round  34, Train loss: 1.601, Test loss: 1.617, Test accuracy: 84.61
Round  35, Train loss: 1.585, Test loss: 1.618, Test accuracy: 84.46
Round  36, Train loss: 1.585, Test loss: 1.617, Test accuracy: 84.59
Round  37, Train loss: 1.590, Test loss: 1.616, Test accuracy: 84.69
Round  38, Train loss: 1.595, Test loss: 1.615, Test accuracy: 84.71
Round  39, Train loss: 1.592, Test loss: 1.615, Test accuracy: 84.64
Round  40, Train loss: 1.581, Test loss: 1.615, Test accuracy: 84.69
Round  41, Train loss: 1.581, Test loss: 1.613, Test accuracy: 84.97
Round  42, Train loss: 1.587, Test loss: 1.613, Test accuracy: 84.88
Round  43, Train loss: 1.590, Test loss: 1.613, Test accuracy: 84.77
Round  44, Train loss: 1.593, Test loss: 1.613, Test accuracy: 84.75
Round  45, Train loss: 1.583, Test loss: 1.613, Test accuracy: 84.87
Round  46, Train loss: 1.577, Test loss: 1.611, Test accuracy: 85.06
Round  47, Train loss: 1.583, Test loss: 1.611, Test accuracy: 85.01
Round  48, Train loss: 1.585, Test loss: 1.611, Test accuracy: 85.03
Round  49, Train loss: 1.571, Test loss: 1.611, Test accuracy: 85.08
Round  50, Train loss: 1.580, Test loss: 1.611, Test accuracy: 85.05
Round  51, Train loss: 1.590, Test loss: 1.610, Test accuracy: 85.27
Round  52, Train loss: 1.573, Test loss: 1.611, Test accuracy: 85.08
Round  53, Train loss: 1.575, Test loss: 1.610, Test accuracy: 85.16
Round  54, Train loss: 1.574, Test loss: 1.610, Test accuracy: 85.12
Round  55, Train loss: 1.579, Test loss: 1.609, Test accuracy: 85.30
Round  56, Train loss: 1.587, Test loss: 1.609, Test accuracy: 85.18
Round  57, Train loss: 1.574, Test loss: 1.609, Test accuracy: 85.33
Round  58, Train loss: 1.584, Test loss: 1.608, Test accuracy: 85.38
Round  59, Train loss: 1.577, Test loss: 1.609, Test accuracy: 85.24
Round  60, Train loss: 1.586, Test loss: 1.608, Test accuracy: 85.43
Round  61, Train loss: 1.577, Test loss: 1.608, Test accuracy: 85.43
Round  62, Train loss: 1.578, Test loss: 1.608, Test accuracy: 85.39
Round  63, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.45
Round  64, Train loss: 1.575, Test loss: 1.607, Test accuracy: 85.44
Round  65, Train loss: 1.582, Test loss: 1.606, Test accuracy: 85.47
Round  66, Train loss: 1.575, Test loss: 1.606, Test accuracy: 85.43
Round  67, Train loss: 1.572, Test loss: 1.607, Test accuracy: 85.47
Round  68, Train loss: 1.579, Test loss: 1.607, Test accuracy: 85.39
Round  69, Train loss: 1.567, Test loss: 1.606, Test accuracy: 85.45
Round  70, Train loss: 1.575, Test loss: 1.607, Test accuracy: 85.53
Round  71, Train loss: 1.570, Test loss: 1.606, Test accuracy: 85.55
Round  72, Train loss: 1.573, Test loss: 1.605, Test accuracy: 85.54
Round  73, Train loss: 1.568, Test loss: 1.605, Test accuracy: 85.58
Round  74, Train loss: 1.565, Test loss: 1.605, Test accuracy: 85.61
Round  75, Train loss: 1.580, Test loss: 1.605, Test accuracy: 85.59
Round  76, Train loss: 1.569, Test loss: 1.604, Test accuracy: 85.60
Round  77, Train loss: 1.573, Test loss: 1.605, Test accuracy: 85.64
Round  78, Train loss: 1.571, Test loss: 1.605, Test accuracy: 85.65
Round  79, Train loss: 1.563, Test loss: 1.605, Test accuracy: 85.67
Round  80, Train loss: 1.571, Test loss: 1.604, Test accuracy: 85.83
Round  81, Train loss: 1.579, Test loss: 1.603, Test accuracy: 85.75
Round  82, Train loss: 1.577, Test loss: 1.603, Test accuracy: 85.83
Round  83, Train loss: 1.569, Test loss: 1.603, Test accuracy: 85.78
Round  84, Train loss: 1.564, Test loss: 1.602, Test accuracy: 85.69
Round  85, Train loss: 1.554, Test loss: 1.545, Test accuracy: 92.22
Round  86, Train loss: 1.519, Test loss: 1.537, Test accuracy: 93.00
Round  87, Train loss: 1.519, Test loss: 1.532, Test accuracy: 93.51
Round  88, Train loss: 1.502, Test loss: 1.531, Test accuracy: 93.71
Round  89, Train loss: 1.497, Test loss: 1.529, Test accuracy: 93.74
Round  90, Train loss: 1.498, Test loss: 1.527, Test accuracy: 93.78
Round  91, Train loss: 1.493, Test loss: 1.526, Test accuracy: 94.02
Round  92, Train loss: 1.490, Test loss: 1.525, Test accuracy: 94.09
Round  93, Train loss: 1.491, Test loss: 1.524, Test accuracy: 94.03
Round  94, Train loss: 1.487, Test loss: 1.523, Test accuracy: 94.23
Round  95, Train loss: 1.491, Test loss: 1.522, Test accuracy: 94.26
Round  96, Train loss: 1.486, Test loss: 1.522, Test accuracy: 94.23
Round  97, Train loss: 1.487, Test loss: 1.521, Test accuracy: 94.32
Round  98, Train loss: 1.484, Test loss: 1.520, Test accuracy: 94.34
Round  99, Train loss: 1.481, Test loss: 1.520, Test accuracy: 94.42
Final Round, Train loss: 1.483, Test loss: 1.519, Test accuracy: 94.47
Average accuracy final 10 rounds: 94.1725
2069.144362926483
[2.9927899837493896, 5.8816680908203125, 8.890999794006348, 11.886844396591187, 14.979207515716553, 18.066613912582397, 21.00369954109192, 23.759321689605713, 26.658794403076172, 29.56064224243164, 32.42268490791321, 35.43816828727722, 38.36240339279175, 41.31727361679077, 44.30153131484985, 47.12334656715393, 49.949159383773804, 52.84457087516785, 55.537757873535156, 58.20240354537964, 60.91028451919556, 63.60237741470337, 66.31991314888, 69.06357622146606, 71.79670572280884, 74.49636387825012, 77.15881776809692, 79.96046328544617, 82.74874377250671, 85.39342784881592, 88.01861619949341, 90.75813794136047, 93.4405460357666, 96.08859753608704, 98.82972478866577, 101.47641253471375, 104.14930725097656, 106.87806558609009, 109.57522463798523, 112.23128390312195, 114.99866223335266, 117.74171805381775, 120.45593929290771, 123.22562217712402, 126.0602331161499, 128.8398745059967, 131.5860140323639, 134.33413577079773, 137.05174136161804, 139.7586898803711, 142.50806140899658, 145.25220561027527, 147.9472005367279, 150.70571303367615, 153.42732739448547, 156.15795993804932, 158.83659839630127, 161.617534160614, 164.33725953102112, 167.01012563705444, 169.84504175186157, 172.6495943069458, 175.44039011001587, 178.28120803833008, 181.01665091514587, 183.7176558971405, 186.40964460372925, 189.25217533111572, 191.98696064949036, 194.70819234848022, 197.45889616012573, 200.16790509223938, 202.9610493183136, 205.74654078483582, 208.54315209388733, 211.24657011032104, 213.9597508907318, 216.7381591796875, 219.42955255508423, 222.2246551513672, 224.95598030090332, 227.69688391685486, 230.3969111442566, 233.05483865737915, 235.75652956962585, 238.41553044319153, 241.41834998130798, 244.2991805076599, 247.10794138908386, 249.96021223068237, 252.83194732666016, 255.6760811805725, 258.53437757492065, 261.4297516345978, 264.33861541748047, 267.2691822052002, 270.231502532959, 273.1639609336853, 276.0233166217804, 278.9738256931305, 281.49768567085266]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[18.691666666666666, 21.941666666666666, 33.19166666666667, 45.93333333333333, 56.425, 66.6, 71.66666666666667, 72.80833333333334, 73.56666666666666, 73.76666666666667, 74.13333333333334, 74.275, 74.49166666666666, 74.525, 81.48333333333333, 82.075, 82.65, 82.93333333333334, 83.08333333333333, 83.33333333333333, 83.49166666666666, 83.625, 83.93333333333334, 83.86666666666666, 83.79166666666667, 83.975, 84.075, 84.025, 84.25, 84.23333333333333, 84.25833333333334, 84.34166666666667, 84.475, 84.53333333333333, 84.60833333333333, 84.45833333333333, 84.59166666666667, 84.69166666666666, 84.70833333333333, 84.64166666666667, 84.69166666666666, 84.975, 84.88333333333334, 84.76666666666667, 84.75, 84.86666666666666, 85.05833333333334, 85.00833333333334, 85.025, 85.075, 85.05, 85.26666666666667, 85.08333333333333, 85.15833333333333, 85.125, 85.3, 85.18333333333334, 85.325, 85.375, 85.24166666666666, 85.43333333333334, 85.43333333333334, 85.39166666666667, 85.45, 85.44166666666666, 85.46666666666667, 85.43333333333334, 85.46666666666667, 85.39166666666667, 85.45, 85.525, 85.55, 85.54166666666667, 85.575, 85.60833333333333, 85.59166666666667, 85.6, 85.64166666666667, 85.65, 85.66666666666667, 85.825, 85.75, 85.825, 85.78333333333333, 85.69166666666666, 92.21666666666667, 93.0, 93.50833333333334, 93.70833333333333, 93.74166666666666, 93.78333333333333, 94.01666666666667, 94.09166666666667, 94.03333333333333, 94.23333333333333, 94.25833333333334, 94.23333333333333, 94.31666666666666, 94.34166666666667, 94.41666666666667, 94.475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.321, Test loss: 2.300, Test accuracy: 22.34
Round   1, Train loss: 2.299, Test loss: 2.297, Test accuracy: 27.68
Round   2, Train loss: 2.295, Test loss: 2.292, Test accuracy: 30.08
Round   3, Train loss: 2.289, Test loss: 2.281, Test accuracy: 31.11
Round   4, Train loss: 2.275, Test loss: 2.252, Test accuracy: 31.01
Round   5, Train loss: 2.222, Test loss: 2.192, Test accuracy: 35.16
Round   6, Train loss: 2.166, Test loss: 2.129, Test accuracy: 41.21
Round   7, Train loss: 2.108, Test loss: 2.058, Test accuracy: 53.32
Round   8, Train loss: 2.008, Test loss: 1.987, Test accuracy: 56.12
Round   9, Train loss: 1.952, Test loss: 1.942, Test accuracy: 57.00
Round  10, Train loss: 1.915, Test loss: 1.917, Test accuracy: 58.02
Round  11, Train loss: 1.897, Test loss: 1.883, Test accuracy: 61.45
Round  12, Train loss: 1.860, Test loss: 1.852, Test accuracy: 64.92
Round  13, Train loss: 1.836, Test loss: 1.822, Test accuracy: 68.61
Round  14, Train loss: 1.798, Test loss: 1.800, Test accuracy: 70.56
Round  15, Train loss: 1.786, Test loss: 1.774, Test accuracy: 72.78
Round  16, Train loss: 1.756, Test loss: 1.763, Test accuracy: 73.56
Round  17, Train loss: 1.751, Test loss: 1.754, Test accuracy: 74.28
Round  18, Train loss: 1.744, Test loss: 1.738, Test accuracy: 75.88
Round  19, Train loss: 1.727, Test loss: 1.723, Test accuracy: 77.72
Round  20, Train loss: 1.733, Test loss: 1.704, Test accuracy: 79.67
Round  21, Train loss: 1.703, Test loss: 1.695, Test accuracy: 80.35
Round  22, Train loss: 1.680, Test loss: 1.686, Test accuracy: 81.09
Round  23, Train loss: 1.685, Test loss: 1.675, Test accuracy: 82.15
Round  24, Train loss: 1.663, Test loss: 1.671, Test accuracy: 82.42
Round  25, Train loss: 1.672, Test loss: 1.666, Test accuracy: 82.70
Round  26, Train loss: 1.660, Test loss: 1.662, Test accuracy: 82.89
Round  27, Train loss: 1.657, Test loss: 1.658, Test accuracy: 83.19
Round  28, Train loss: 1.647, Test loss: 1.653, Test accuracy: 83.21
Round  29, Train loss: 1.631, Test loss: 1.651, Test accuracy: 83.47
Round  30, Train loss: 1.637, Test loss: 1.646, Test accuracy: 83.79
Round  31, Train loss: 1.634, Test loss: 1.640, Test accuracy: 84.27
Round  32, Train loss: 1.610, Test loss: 1.621, Test accuracy: 87.23
Round  33, Train loss: 1.586, Test loss: 1.603, Test accuracy: 89.03
Round  34, Train loss: 1.572, Test loss: 1.590, Test accuracy: 90.55
Round  35, Train loss: 1.571, Test loss: 1.578, Test accuracy: 91.62
Round  36, Train loss: 1.566, Test loss: 1.570, Test accuracy: 92.45
Round  37, Train loss: 1.555, Test loss: 1.564, Test accuracy: 92.95
Round  38, Train loss: 1.552, Test loss: 1.558, Test accuracy: 93.34
Round  39, Train loss: 1.541, Test loss: 1.556, Test accuracy: 93.46
Round  40, Train loss: 1.547, Test loss: 1.553, Test accuracy: 93.67
Round  41, Train loss: 1.535, Test loss: 1.551, Test accuracy: 93.84
Round  42, Train loss: 1.542, Test loss: 1.549, Test accuracy: 93.97
Round  43, Train loss: 1.538, Test loss: 1.547, Test accuracy: 94.06
Round  44, Train loss: 1.537, Test loss: 1.545, Test accuracy: 94.12
Round  45, Train loss: 1.525, Test loss: 1.544, Test accuracy: 94.22
Round  46, Train loss: 1.523, Test loss: 1.543, Test accuracy: 94.27
Round  47, Train loss: 1.534, Test loss: 1.541, Test accuracy: 94.42
Round  48, Train loss: 1.529, Test loss: 1.539, Test accuracy: 94.48
Round  49, Train loss: 1.525, Test loss: 1.538, Test accuracy: 94.60
Round  50, Train loss: 1.525, Test loss: 1.536, Test accuracy: 94.67
Round  51, Train loss: 1.523, Test loss: 1.535, Test accuracy: 94.70
Round  52, Train loss: 1.516, Test loss: 1.535, Test accuracy: 94.76
Round  53, Train loss: 1.513, Test loss: 1.534, Test accuracy: 94.86
Round  54, Train loss: 1.518, Test loss: 1.533, Test accuracy: 94.91
Round  55, Train loss: 1.513, Test loss: 1.532, Test accuracy: 95.01
Round  56, Train loss: 1.505, Test loss: 1.532, Test accuracy: 95.02
Round  57, Train loss: 1.512, Test loss: 1.530, Test accuracy: 95.15
Round  58, Train loss: 1.511, Test loss: 1.529, Test accuracy: 95.19
Round  59, Train loss: 1.512, Test loss: 1.528, Test accuracy: 95.23
Round  60, Train loss: 1.506, Test loss: 1.527, Test accuracy: 95.24
Round  61, Train loss: 1.502, Test loss: 1.527, Test accuracy: 95.39
Round  62, Train loss: 1.504, Test loss: 1.527, Test accuracy: 95.37
Round  63, Train loss: 1.508, Test loss: 1.526, Test accuracy: 95.44
Round  64, Train loss: 1.505, Test loss: 1.525, Test accuracy: 95.46
Round  65, Train loss: 1.497, Test loss: 1.525, Test accuracy: 95.47
Round  66, Train loss: 1.497, Test loss: 1.525, Test accuracy: 95.48
Round  67, Train loss: 1.511, Test loss: 1.524, Test accuracy: 95.49
Round  68, Train loss: 1.504, Test loss: 1.523, Test accuracy: 95.55
Round  69, Train loss: 1.505, Test loss: 1.522, Test accuracy: 95.55
Round  70, Train loss: 1.500, Test loss: 1.522, Test accuracy: 95.63
Round  71, Train loss: 1.501, Test loss: 1.521, Test accuracy: 95.65
Round  72, Train loss: 1.494, Test loss: 1.521, Test accuracy: 95.65
Round  73, Train loss: 1.497, Test loss: 1.521, Test accuracy: 95.72
Round  74, Train loss: 1.492, Test loss: 1.520, Test accuracy: 95.72
Round  75, Train loss: 1.494, Test loss: 1.520, Test accuracy: 95.68
Round  76, Train loss: 1.500, Test loss: 1.520, Test accuracy: 95.73
Round  77, Train loss: 1.490, Test loss: 1.519, Test accuracy: 95.76
Round  78, Train loss: 1.496, Test loss: 1.519, Test accuracy: 95.83
Round  79, Train loss: 1.496, Test loss: 1.518, Test accuracy: 95.91
Round  80, Train loss: 1.490, Test loss: 1.518, Test accuracy: 95.86
Round  81, Train loss: 1.496, Test loss: 1.517, Test accuracy: 95.93
Round  82, Train loss: 1.490, Test loss: 1.517, Test accuracy: 95.89
Round  83, Train loss: 1.492, Test loss: 1.517, Test accuracy: 95.95
Round  84, Train loss: 1.488, Test loss: 1.517, Test accuracy: 95.96
Round  85, Train loss: 1.492, Test loss: 1.517, Test accuracy: 95.92
Round  86, Train loss: 1.485, Test loss: 1.517, Test accuracy: 95.96
Round  87, Train loss: 1.494, Test loss: 1.515, Test accuracy: 95.92
Round  88, Train loss: 1.489, Test loss: 1.515, Test accuracy: 95.99
Round  89, Train loss: 1.490, Test loss: 1.515, Test accuracy: 96.05
Round  90, Train loss: 1.487, Test loss: 1.515, Test accuracy: 96.00
Round  91, Train loss: 1.486, Test loss: 1.514, Test accuracy: 95.99
Round  92, Train loss: 1.488, Test loss: 1.515, Test accuracy: 95.94
Round  93, Train loss: 1.487, Test loss: 1.514, Test accuracy: 96.07
Round  94, Train loss: 1.489, Test loss: 1.514, Test accuracy: 96.08/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.488, Test loss: 1.514, Test accuracy: 96.08
Round  96, Train loss: 1.483, Test loss: 1.514, Test accuracy: 96.12
Round  97, Train loss: 1.482, Test loss: 1.513, Test accuracy: 96.10
Round  98, Train loss: 1.485, Test loss: 1.513, Test accuracy: 96.12
Round  99, Train loss: 1.481, Test loss: 1.513, Test accuracy: 96.17
Final Round, Train loss: 1.479, Test loss: 1.512, Test accuracy: 96.14
Average accuracy final 10 rounds: 96.0675
1327.185010433197
[1.7159051895141602, 3.1605288982391357, 4.722837924957275, 6.281842470169067, 7.838411569595337, 9.37998914718628, 10.948380947113037, 12.452411413192749, 14.007824420928955, 15.596550941467285, 17.21559190750122, 18.771520614624023, 20.265828609466553, 21.81656813621521, 23.400384187698364, 24.95111918449402, 26.440386533737183, 28.00104308128357, 29.564133405685425, 31.075271129608154, 32.60119271278381, 34.19217586517334, 35.777597427368164, 37.26559853553772, 38.81113576889038, 40.3956983089447, 41.94485259056091, 43.48512125015259, 45.024569511413574, 46.55535912513733, 48.13686013221741, 49.710166692733765, 51.26903820037842, 52.793166637420654, 54.31903553009033, 55.90640616416931, 57.49973464012146, 59.0353889465332, 60.566038370132446, 62.18936848640442, 63.746201515197754, 65.2408516407013, 66.79448580741882, 68.35920262336731, 69.89752697944641, 71.42917561531067, 72.96751642227173, 74.51490044593811, 76.01963829994202, 77.50177955627441, 79.05284976959229, 80.60615539550781, 82.10699391365051, 83.59517502784729, 85.10196018218994, 86.6337022781372, 88.14510989189148, 89.64647650718689, 91.15241527557373, 92.65123271942139, 94.1782591342926, 95.72388458251953, 97.25774598121643, 98.79560017585754, 100.31614708900452, 101.83112597465515, 103.33936595916748, 104.87051939964294, 106.34578919410706, 107.82695841789246, 109.38693594932556, 110.95604991912842, 112.50047779083252, 114.00989007949829, 115.54317235946655, 117.07850909233093, 118.60197114944458, 120.12841296195984, 121.69947743415833, 123.26164054870605, 124.82790541648865, 126.36808395385742, 127.89842963218689, 129.45013809204102, 130.97098112106323, 132.5174117088318, 134.0576343536377, 135.55792808532715, 137.09208512306213, 138.6006965637207, 140.14606070518494, 141.7685353755951, 143.34727811813354, 144.915607213974, 146.46660661697388, 148.04602789878845, 149.65621995925903, 151.19128370285034, 152.75182628631592, 154.30840301513672, 156.3378667831421]
[22.341666666666665, 27.683333333333334, 30.083333333333332, 31.108333333333334, 31.008333333333333, 35.15833333333333, 41.208333333333336, 53.31666666666667, 56.11666666666667, 57.0, 58.016666666666666, 61.45, 64.925, 68.60833333333333, 70.55833333333334, 72.78333333333333, 73.55833333333334, 74.275, 75.88333333333334, 77.71666666666667, 79.66666666666667, 80.35, 81.09166666666667, 82.15, 82.41666666666667, 82.7, 82.89166666666667, 83.19166666666666, 83.20833333333333, 83.46666666666667, 83.79166666666667, 84.26666666666667, 87.23333333333333, 89.025, 90.55, 91.625, 92.45, 92.95, 93.34166666666667, 93.45833333333333, 93.675, 93.84166666666667, 93.975, 94.05833333333334, 94.11666666666666, 94.225, 94.26666666666667, 94.41666666666667, 94.48333333333333, 94.6, 94.675, 94.7, 94.75833333333334, 94.85833333333333, 94.90833333333333, 95.00833333333334, 95.01666666666667, 95.15, 95.19166666666666, 95.23333333333333, 95.24166666666666, 95.39166666666667, 95.36666666666666, 95.44166666666666, 95.45833333333333, 95.475, 95.48333333333333, 95.49166666666666, 95.55, 95.55, 95.63333333333334, 95.65, 95.65, 95.71666666666667, 95.725, 95.68333333333334, 95.73333333333333, 95.75833333333334, 95.83333333333333, 95.90833333333333, 95.85833333333333, 95.93333333333334, 95.89166666666667, 95.95, 95.95833333333333, 95.925, 95.95833333333333, 95.925, 95.99166666666666, 96.05, 96.0, 95.99166666666666, 95.94166666666666, 96.06666666666666, 96.08333333333333, 96.075, 96.125, 96.1, 96.11666666666666, 96.175, 96.14166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.319, Test loss: 2.299, Test accuracy: 23.36
Round   1, Train loss: 2.310, Test loss: 2.293, Test accuracy: 21.61
Round   2, Train loss: 2.299, Test loss: 2.282, Test accuracy: 22.11
Round   3, Train loss: 2.281, Test loss: 2.253, Test accuracy: 32.76
Round   4, Train loss: 2.220, Test loss: 2.159, Test accuracy: 40.36
Round   5, Train loss: 2.141, Test loss: 2.096, Test accuracy: 52.17
Round   6, Train loss: 2.073, Test loss: 2.022, Test accuracy: 56.08
Round   7, Train loss: 2.004, Test loss: 1.972, Test accuracy: 57.25
Round   8, Train loss: 1.957, Test loss: 1.943, Test accuracy: 58.43
Round   9, Train loss: 1.951, Test loss: 1.916, Test accuracy: 59.89
Round  10, Train loss: 1.923, Test loss: 1.892, Test accuracy: 63.64
Round  11, Train loss: 1.878, Test loss: 1.866, Test accuracy: 67.99
Round  12, Train loss: 1.866, Test loss: 1.834, Test accuracy: 70.67
Round  13, Train loss: 1.835, Test loss: 1.811, Test accuracy: 73.18
Round  14, Train loss: 1.824, Test loss: 1.785, Test accuracy: 75.31
Round  15, Train loss: 1.800, Test loss: 1.763, Test accuracy: 78.51
Round  16, Train loss: 1.800, Test loss: 1.727, Test accuracy: 83.77
Round  17, Train loss: 1.749, Test loss: 1.706, Test accuracy: 86.10
Round  18, Train loss: 1.727, Test loss: 1.689, Test accuracy: 87.57
Round  19, Train loss: 1.710, Test loss: 1.669, Test accuracy: 88.78
Round  20, Train loss: 1.699, Test loss: 1.658, Test accuracy: 89.47
Round  21, Train loss: 1.686, Test loss: 1.644, Test accuracy: 90.22
Round  22, Train loss: 1.675, Test loss: 1.632, Test accuracy: 90.92
Round  23, Train loss: 1.662, Test loss: 1.624, Test accuracy: 91.26
Round  24, Train loss: 1.653, Test loss: 1.619, Test accuracy: 91.75
Round  25, Train loss: 1.644, Test loss: 1.612, Test accuracy: 92.16
Round  26, Train loss: 1.635, Test loss: 1.609, Test accuracy: 92.32
Round  27, Train loss: 1.627, Test loss: 1.606, Test accuracy: 92.53
Round  28, Train loss: 1.632, Test loss: 1.595, Test accuracy: 92.93
Round  29, Train loss: 1.618, Test loss: 1.593, Test accuracy: 93.08
Round  30, Train loss: 1.612, Test loss: 1.587, Test accuracy: 93.29
Round  31, Train loss: 1.600, Test loss: 1.587, Test accuracy: 93.45
Round  32, Train loss: 1.599, Test loss: 1.584, Test accuracy: 93.64
Round  33, Train loss: 1.609, Test loss: 1.578, Test accuracy: 93.68
Round  34, Train loss: 1.603, Test loss: 1.574, Test accuracy: 93.97
Round  35, Train loss: 1.592, Test loss: 1.573, Test accuracy: 94.06
Round  36, Train loss: 1.589, Test loss: 1.571, Test accuracy: 93.97
Round  37, Train loss: 1.588, Test loss: 1.568, Test accuracy: 94.10
Round  38, Train loss: 1.579, Test loss: 1.567, Test accuracy: 94.30
Round  39, Train loss: 1.580, Test loss: 1.566, Test accuracy: 94.37
Round  40, Train loss: 1.573, Test loss: 1.564, Test accuracy: 94.51
Round  41, Train loss: 1.577, Test loss: 1.561, Test accuracy: 94.72
Round  42, Train loss: 1.580, Test loss: 1.557, Test accuracy: 94.78
Round  43, Train loss: 1.562, Test loss: 1.558, Test accuracy: 94.92
Round  44, Train loss: 1.559, Test loss: 1.557, Test accuracy: 94.98
Round  45, Train loss: 1.570, Test loss: 1.555, Test accuracy: 94.97
Round  46, Train loss: 1.561, Test loss: 1.554, Test accuracy: 95.09
Round  47, Train loss: 1.557, Test loss: 1.553, Test accuracy: 95.18
Round  48, Train loss: 1.552, Test loss: 1.553, Test accuracy: 95.26
Round  49, Train loss: 1.559, Test loss: 1.551, Test accuracy: 95.28
Round  50, Train loss: 1.549, Test loss: 1.551, Test accuracy: 95.37
Round  51, Train loss: 1.562, Test loss: 1.548, Test accuracy: 95.47
Round  52, Train loss: 1.547, Test loss: 1.549, Test accuracy: 95.51
Round  53, Train loss: 1.549, Test loss: 1.547, Test accuracy: 95.61
Round  54, Train loss: 1.545, Test loss: 1.547, Test accuracy: 95.64
Round  55, Train loss: 1.545, Test loss: 1.545, Test accuracy: 95.72
Round  56, Train loss: 1.546, Test loss: 1.544, Test accuracy: 95.75
Round  57, Train loss: 1.536, Test loss: 1.545, Test accuracy: 95.82
Round  58, Train loss: 1.540, Test loss: 1.544, Test accuracy: 95.82
Round  59, Train loss: 1.535, Test loss: 1.543, Test accuracy: 95.83
Round  60, Train loss: 1.537, Test loss: 1.544, Test accuracy: 95.83
Round  61, Train loss: 1.537, Test loss: 1.542, Test accuracy: 95.86
Round  62, Train loss: 1.531, Test loss: 1.543, Test accuracy: 95.81
Round  63, Train loss: 1.530, Test loss: 1.542, Test accuracy: 95.95
Round  64, Train loss: 1.537, Test loss: 1.540, Test accuracy: 95.92
Round  65, Train loss: 1.538, Test loss: 1.538, Test accuracy: 96.00
Round  66, Train loss: 1.539, Test loss: 1.537, Test accuracy: 95.98
Round  67, Train loss: 1.532, Test loss: 1.537, Test accuracy: 96.08
Round  68, Train loss: 1.532, Test loss: 1.537, Test accuracy: 96.12
Round  69, Train loss: 1.526, Test loss: 1.537, Test accuracy: 96.15
Round  70, Train loss: 1.526, Test loss: 1.537, Test accuracy: 96.28
Round  71, Train loss: 1.529, Test loss: 1.535, Test accuracy: 96.23
Round  72, Train loss: 1.529, Test loss: 1.534, Test accuracy: 96.22
Round  73, Train loss: 1.531, Test loss: 1.534, Test accuracy: 96.22
Round  74, Train loss: 1.528, Test loss: 1.533, Test accuracy: 96.28
Round  75, Train loss: 1.527, Test loss: 1.534, Test accuracy: 96.36
Round  76, Train loss: 1.532, Test loss: 1.531, Test accuracy: 96.31
Round  77, Train loss: 1.522, Test loss: 1.532, Test accuracy: 96.34
Round  78, Train loss: 1.521, Test loss: 1.532, Test accuracy: 96.29
Round  79, Train loss: 1.523, Test loss: 1.531, Test accuracy: 96.36
Round  80, Train loss: 1.516, Test loss: 1.532, Test accuracy: 96.44
Round  81, Train loss: 1.526, Test loss: 1.529, Test accuracy: 96.42
Round  82, Train loss: 1.525, Test loss: 1.528, Test accuracy: 96.40
Round  83, Train loss: 1.521, Test loss: 1.529, Test accuracy: 96.44
Round  84, Train loss: 1.517, Test loss: 1.529, Test accuracy: 96.50
Round  85, Train loss: 1.517, Test loss: 1.528, Test accuracy: 96.45
Round  86, Train loss: 1.518, Test loss: 1.528, Test accuracy: 96.49
Round  87, Train loss: 1.515, Test loss: 1.529, Test accuracy: 96.57
Round  88, Train loss: 1.518, Test loss: 1.528, Test accuracy: 96.58
Round  89, Train loss: 1.516, Test loss: 1.528, Test accuracy: 96.51
Round  90, Train loss: 1.512, Test loss: 1.529, Test accuracy: 96.53
Round  91, Train loss: 1.524, Test loss: 1.524, Test accuracy: 96.54
Round  92, Train loss: 1.508, Test loss: 1.528, Test accuracy: 96.59
Round  93, Train loss: 1.513, Test loss: 1.527, Test accuracy: 96.62/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.514, Test loss: 1.526, Test accuracy: 96.62
Round  95, Train loss: 1.516, Test loss: 1.525, Test accuracy: 96.54
Round  96, Train loss: 1.512, Test loss: 1.526, Test accuracy: 96.65
Round  97, Train loss: 1.512, Test loss: 1.525, Test accuracy: 96.61
Round  98, Train loss: 1.514, Test loss: 1.524, Test accuracy: 96.62
Round  99, Train loss: 1.513, Test loss: 1.524, Test accuracy: 96.63
Final Round, Train loss: 1.488, Test loss: 1.521, Test accuracy: 96.65
Average accuracy final 10 rounds: 96.59583333333333
1693.2628610134125
[1.6007492542266846, 3.201498508453369, 4.668906927108765, 6.13631534576416, 7.627040147781372, 9.117764949798584, 10.58167552947998, 12.045586109161377, 13.51627516746521, 14.986964225769043, 16.425663471221924, 17.864362716674805, 19.340399980545044, 20.816437244415283, 22.2784264087677, 23.740415573120117, 25.28264093399048, 26.82486629486084, 28.330231428146362, 29.835596561431885, 31.4157874584198, 32.995978355407715, 34.49530506134033, 35.99463176727295, 37.439531087875366, 38.88443040847778, 40.37499284744263, 41.86555528640747, 43.49058651924133, 45.115617752075195, 46.71575951576233, 48.31590127944946, 49.7827730178833, 51.24964475631714, 52.75566244125366, 54.261680126190186, 55.72817850112915, 57.194676876068115, 58.68742847442627, 60.180180072784424, 61.63012886047363, 63.08007764816284, 64.57895994186401, 66.07784223556519, 67.5604248046875, 69.04300737380981, 70.49345922470093, 71.94391107559204, 73.40006041526794, 74.85620975494385, 76.27078533172607, 77.6853609085083, 79.1080870628357, 80.53081321716309, 81.97188115119934, 83.4129490852356, 84.79431056976318, 86.17567205429077, 87.59480714797974, 89.0139422416687, 90.4741849899292, 91.9344277381897, 93.40758204460144, 94.88073635101318, 96.3544909954071, 97.82824563980103, 99.26066064834595, 100.69307565689087, 102.1187527179718, 103.54442977905273, 105.07919955253601, 106.61396932601929, 108.1098062992096, 109.6056432723999, 111.10025906562805, 112.5948748588562, 114.0429949760437, 115.4911150932312, 117.04266548156738, 118.59421586990356, 120.0807580947876, 121.56730031967163, 123.00121402740479, 124.43512773513794, 125.9273145198822, 127.41950130462646, 128.96745800971985, 130.51541471481323, 132.0442657470703, 133.5731167793274, 135.04398798942566, 136.51485919952393, 138.0479211807251, 139.58098316192627, 141.15656089782715, 142.73213863372803, 144.24044370651245, 145.74874877929688, 147.2050976753235, 148.6614465713501, 150.18115258216858, 151.70085859298706, 153.20267844200134, 154.70449829101562, 156.17203950881958, 157.63958072662354, 159.0923569202423, 160.54513311386108, 162.02555012702942, 163.50596714019775, 164.95637583732605, 166.40678453445435, 167.90065550804138, 169.39452648162842, 170.87303972244263, 172.35155296325684, 173.8224458694458, 175.29333877563477, 176.6989827156067, 178.1046266555786, 179.54962801933289, 180.99462938308716, 182.48202872276306, 183.96942806243896, 185.40248489379883, 186.8355417251587, 188.2669780254364, 189.6984143257141, 191.08818554878235, 192.4779567718506, 193.89773225784302, 195.31750774383545, 196.6954026222229, 198.07329750061035, 199.49537444114685, 200.91745138168335, 202.3173053264618, 203.71715927124023, 205.07503581047058, 206.43291234970093, 207.87988591194153, 209.32685947418213, 210.74513483047485, 212.16341018676758, 213.57349562644958, 214.9835810661316, 216.40282607078552, 217.82207107543945, 219.22742128372192, 220.6327714920044, 222.0240683555603, 223.4153652191162, 224.8778760433197, 226.3403868675232, 227.77745938301086, 229.21453189849854, 230.62326860427856, 232.0320053100586, 233.47248554229736, 234.91296577453613, 236.37745022773743, 237.84193468093872, 239.25577330589294, 240.66961193084717, 242.0808289051056, 243.492045879364, 245.01292037963867, 246.53379487991333, 247.9948925971985, 249.45599031448364, 250.95548629760742, 252.4549822807312, 253.8008270263672, 255.14667177200317, 256.5209274291992, 257.89518308639526, 259.22068071365356, 260.54617834091187, 261.86401748657227, 263.18185663223267, 264.50573563575745, 265.8296146392822, 267.1439983844757, 268.4583821296692, 269.7589452266693, 271.05950832366943, 272.4323093891144, 273.8051104545593, 275.15602588653564, 276.50694131851196, 277.7917494773865, 279.076557636261, 280.446044921875, 281.815532207489, 283.1621677875519, 284.50880336761475, 285.8687906265259, 287.228777885437, 288.5408537387848, 289.85292959213257, 291.6215317249298, 293.39013385772705]
[23.358333333333334, 23.358333333333334, 21.608333333333334, 21.608333333333334, 22.108333333333334, 22.108333333333334, 32.75833333333333, 32.75833333333333, 40.358333333333334, 40.358333333333334, 52.166666666666664, 52.166666666666664, 56.075, 56.075, 57.25, 57.25, 58.43333333333333, 58.43333333333333, 59.891666666666666, 59.891666666666666, 63.641666666666666, 63.641666666666666, 67.99166666666666, 67.99166666666666, 70.675, 70.675, 73.18333333333334, 73.18333333333334, 75.30833333333334, 75.30833333333334, 78.50833333333334, 78.50833333333334, 83.76666666666667, 83.76666666666667, 86.1, 86.1, 87.56666666666666, 87.56666666666666, 88.78333333333333, 88.78333333333333, 89.475, 89.475, 90.21666666666667, 90.21666666666667, 90.925, 90.925, 91.25833333333334, 91.25833333333334, 91.75, 91.75, 92.15833333333333, 92.15833333333333, 92.31666666666666, 92.31666666666666, 92.53333333333333, 92.53333333333333, 92.93333333333334, 92.93333333333334, 93.075, 93.075, 93.29166666666667, 93.29166666666667, 93.45, 93.45, 93.64166666666667, 93.64166666666667, 93.68333333333334, 93.68333333333334, 93.975, 93.975, 94.05833333333334, 94.05833333333334, 93.96666666666667, 93.96666666666667, 94.1, 94.1, 94.3, 94.3, 94.36666666666666, 94.36666666666666, 94.50833333333334, 94.50833333333334, 94.71666666666667, 94.71666666666667, 94.78333333333333, 94.78333333333333, 94.91666666666667, 94.91666666666667, 94.98333333333333, 94.98333333333333, 94.96666666666667, 94.96666666666667, 95.09166666666667, 95.09166666666667, 95.18333333333334, 95.18333333333334, 95.25833333333334, 95.25833333333334, 95.275, 95.275, 95.36666666666666, 95.36666666666666, 95.475, 95.475, 95.50833333333334, 95.50833333333334, 95.60833333333333, 95.60833333333333, 95.64166666666667, 95.64166666666667, 95.71666666666667, 95.71666666666667, 95.75, 95.75, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.83333333333333, 95.83333333333333, 95.825, 95.825, 95.85833333333333, 95.85833333333333, 95.80833333333334, 95.80833333333334, 95.95, 95.95, 95.925, 95.925, 96.0, 96.0, 95.98333333333333, 95.98333333333333, 96.075, 96.075, 96.125, 96.125, 96.15, 96.15, 96.28333333333333, 96.28333333333333, 96.23333333333333, 96.23333333333333, 96.225, 96.225, 96.21666666666667, 96.21666666666667, 96.275, 96.275, 96.35833333333333, 96.35833333333333, 96.30833333333334, 96.30833333333334, 96.34166666666667, 96.34166666666667, 96.29166666666667, 96.29166666666667, 96.35833333333333, 96.35833333333333, 96.44166666666666, 96.44166666666666, 96.41666666666667, 96.41666666666667, 96.4, 96.4, 96.44166666666666, 96.44166666666666, 96.5, 96.5, 96.45, 96.45, 96.49166666666666, 96.49166666666666, 96.56666666666666, 96.56666666666666, 96.58333333333333, 96.58333333333333, 96.50833333333334, 96.50833333333334, 96.53333333333333, 96.53333333333333, 96.54166666666667, 96.54166666666667, 96.59166666666667, 96.59166666666667, 96.61666666666666, 96.61666666666666, 96.625, 96.625, 96.54166666666667, 96.54166666666667, 96.65, 96.65, 96.60833333333333, 96.60833333333333, 96.61666666666666, 96.61666666666666, 96.63333333333334, 96.63333333333334, 96.65, 96.65]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.126, Test loss: 2.151, Test accuracy: 29.76
Round   0, Global train loss: 2.126, Global test loss: 2.302, Global test accuracy: 10.00
Round   1, Train loss: 1.847, Test loss: 1.999, Test accuracy: 49.08
Round   1, Global train loss: 1.847, Global test loss: 2.283, Global test accuracy: 14.70
Round   2, Train loss: 1.696, Test loss: 1.925, Test accuracy: 56.77
Round   2, Global train loss: 1.696, Global test loss: 2.267, Global test accuracy: 16.22
Round   3, Train loss: 1.651, Test loss: 1.879, Test accuracy: 60.56
Round   3, Global train loss: 1.651, Global test loss: 2.280, Global test accuracy: 14.67
Round   4, Train loss: 1.630, Test loss: 1.845, Test accuracy: 61.90
Round   4, Global train loss: 1.630, Global test loss: 2.260, Global test accuracy: 18.23
Round   5, Train loss: 1.624, Test loss: 1.827, Test accuracy: 63.01
Round   5, Global train loss: 1.624, Global test loss: 2.278, Global test accuracy: 15.72
Round   6, Train loss: 1.720, Test loss: 1.762, Test accuracy: 70.00
Round   6, Global train loss: 1.720, Global test loss: 2.299, Global test accuracy: 10.96
Round   7, Train loss: 1.531, Test loss: 1.757, Test accuracy: 70.89
Round   7, Global train loss: 1.531, Global test loss: 2.279, Global test accuracy: 17.37
Round   8, Train loss: 1.681, Test loss: 1.722, Test accuracy: 75.15
Round   8, Global train loss: 1.681, Global test loss: 2.264, Global test accuracy: 16.76
Round   9, Train loss: 1.699, Test loss: 1.690, Test accuracy: 77.21
Round   9, Global train loss: 1.699, Global test loss: 2.271, Global test accuracy: 14.02
Round  10, Train loss: 1.728, Test loss: 1.662, Test accuracy: 80.35
Round  10, Global train loss: 1.728, Global test loss: 2.250, Global test accuracy: 18.71
Round  11, Train loss: 1.696, Test loss: 1.660, Test accuracy: 80.42
Round  11, Global train loss: 1.696, Global test loss: 2.262, Global test accuracy: 17.42
Round  12, Train loss: 1.745, Test loss: 1.620, Test accuracy: 84.80
Round  12, Global train loss: 1.745, Global test loss: 2.256, Global test accuracy: 19.93
Round  13, Train loss: 1.586, Test loss: 1.618, Test accuracy: 84.91
Round  13, Global train loss: 1.586, Global test loss: 2.276, Global test accuracy: 16.03
Round  14, Train loss: 1.638, Test loss: 1.617, Test accuracy: 84.92
Round  14, Global train loss: 1.638, Global test loss: 2.269, Global test accuracy: 17.98
Round  15, Train loss: 1.678, Test loss: 1.605, Test accuracy: 86.27
Round  15, Global train loss: 1.678, Global test loss: 2.267, Global test accuracy: 18.33
Round  16, Train loss: 1.527, Test loss: 1.604, Test accuracy: 86.34
Round  16, Global train loss: 1.527, Global test loss: 2.253, Global test accuracy: 18.27
Round  17, Train loss: 1.687, Test loss: 1.604, Test accuracy: 86.37
Round  17, Global train loss: 1.687, Global test loss: 2.262, Global test accuracy: 18.24
Round  18, Train loss: 1.477, Test loss: 1.601, Test accuracy: 86.47
Round  18, Global train loss: 1.477, Global test loss: 2.272, Global test accuracy: 15.85
Round  19, Train loss: 1.632, Test loss: 1.601, Test accuracy: 86.49
Round  19, Global train loss: 1.632, Global test loss: 2.263, Global test accuracy: 18.25
Round  20, Train loss: 1.692, Test loss: 1.599, Test accuracy: 86.62
Round  20, Global train loss: 1.692, Global test loss: 2.254, Global test accuracy: 18.68
Round  21, Train loss: 1.576, Test loss: 1.599, Test accuracy: 86.64
Round  21, Global train loss: 1.576, Global test loss: 2.342, Global test accuracy: 10.68
Round  22, Train loss: 1.525, Test loss: 1.599, Test accuracy: 86.65
Round  22, Global train loss: 1.525, Global test loss: 2.257, Global test accuracy: 18.33
Round  23, Train loss: 1.686, Test loss: 1.599, Test accuracy: 86.64
Round  23, Global train loss: 1.686, Global test loss: 2.266, Global test accuracy: 17.77
Round  24, Train loss: 1.576, Test loss: 1.598, Test accuracy: 86.66
Round  24, Global train loss: 1.576, Global test loss: 2.292, Global test accuracy: 13.32
Round  25, Train loss: 1.522, Test loss: 1.598, Test accuracy: 86.65
Round  25, Global train loss: 1.522, Global test loss: 2.300, Global test accuracy: 12.94
Round  26, Train loss: 1.582, Test loss: 1.598, Test accuracy: 86.65
Round  26, Global train loss: 1.582, Global test loss: 2.252, Global test accuracy: 19.36
Round  27, Train loss: 1.630, Test loss: 1.598, Test accuracy: 86.63
Round  27, Global train loss: 1.630, Global test loss: 2.299, Global test accuracy: 13.66
Round  28, Train loss: 1.521, Test loss: 1.598, Test accuracy: 86.63
Round  28, Global train loss: 1.521, Global test loss: 2.296, Global test accuracy: 14.43
Round  29, Train loss: 1.579, Test loss: 1.598, Test accuracy: 86.62
Round  29, Global train loss: 1.579, Global test loss: 2.247, Global test accuracy: 20.47
Round  30, Train loss: 1.580, Test loss: 1.598, Test accuracy: 86.62
Round  30, Global train loss: 1.580, Global test loss: 2.261, Global test accuracy: 18.46
Round  31, Train loss: 1.522, Test loss: 1.598, Test accuracy: 86.59
Round  31, Global train loss: 1.522, Global test loss: 2.272, Global test accuracy: 16.93
Round  32, Train loss: 1.523, Test loss: 1.597, Test accuracy: 86.62
Round  32, Global train loss: 1.523, Global test loss: 2.262, Global test accuracy: 19.32
Round  33, Train loss: 1.632, Test loss: 1.594, Test accuracy: 86.92
Round  33, Global train loss: 1.632, Global test loss: 2.267, Global test accuracy: 15.46
Round  34, Train loss: 1.579, Test loss: 1.594, Test accuracy: 86.92
Round  34, Global train loss: 1.579, Global test loss: 2.252, Global test accuracy: 18.93
Round  35, Train loss: 1.568, Test loss: 1.584, Test accuracy: 87.97
Round  35, Global train loss: 1.568, Global test loss: 2.265, Global test accuracy: 16.12
Round  36, Train loss: 1.578, Test loss: 1.584, Test accuracy: 87.95
Round  36, Global train loss: 1.578, Global test loss: 2.278, Global test accuracy: 15.25
Round  37, Train loss: 1.523, Test loss: 1.584, Test accuracy: 87.96
Round  37, Global train loss: 1.523, Global test loss: 2.265, Global test accuracy: 17.39
Round  38, Train loss: 1.530, Test loss: 1.581, Test accuracy: 88.20
Round  38, Global train loss: 1.530, Global test loss: 2.269, Global test accuracy: 16.70
Round  39, Train loss: 1.550, Test loss: 1.568, Test accuracy: 89.68
Round  39, Global train loss: 1.550, Global test loss: 2.272, Global test accuracy: 18.33
Round  40, Train loss: 1.590, Test loss: 1.554, Test accuracy: 90.98
Round  40, Global train loss: 1.590, Global test loss: 2.273, Global test accuracy: 16.75
Round  41, Train loss: 1.520, Test loss: 1.554, Test accuracy: 90.97
Round  41, Global train loss: 1.520, Global test loss: 2.306, Global test accuracy: 12.16
Round  42, Train loss: 1.526, Test loss: 1.554, Test accuracy: 90.97
Round  42, Global train loss: 1.526, Global test loss: 2.297, Global test accuracy: 14.34
Round  43, Train loss: 1.527, Test loss: 1.553, Test accuracy: 90.97
Round  43, Global train loss: 1.527, Global test loss: 2.280, Global test accuracy: 14.66
Round  44, Train loss: 1.468, Test loss: 1.553, Test accuracy: 90.97
Round  44, Global train loss: 1.468, Global test loss: 2.253, Global test accuracy: 19.60
Round  45, Train loss: 1.524, Test loss: 1.553, Test accuracy: 90.99
Round  45, Global train loss: 1.524, Global test loss: 2.270, Global test accuracy: 17.71
Round  46, Train loss: 1.575, Test loss: 1.553, Test accuracy: 91.00
Round  46, Global train loss: 1.575, Global test loss: 2.299, Global test accuracy: 12.93
Round  47, Train loss: 1.524, Test loss: 1.553, Test accuracy: 91.00
Round  47, Global train loss: 1.524, Global test loss: 2.273, Global test accuracy: 18.33
Round  48, Train loss: 1.466, Test loss: 1.553, Test accuracy: 91.01
Round  48, Global train loss: 1.466, Global test loss: 2.267, Global test accuracy: 15.80
Round  49, Train loss: 1.469, Test loss: 1.552, Test accuracy: 91.06
Round  49, Global train loss: 1.469, Global test loss: 2.294, Global test accuracy: 12.97
Round  50, Train loss: 1.469, Test loss: 1.552, Test accuracy: 91.07
Round  50, Global train loss: 1.469, Global test loss: 2.274, Global test accuracy: 15.83
Round  51, Train loss: 1.468, Test loss: 1.552, Test accuracy: 91.08
Round  51, Global train loss: 1.468, Global test loss: 2.274, Global test accuracy: 16.68
Round  52, Train loss: 1.522, Test loss: 1.552, Test accuracy: 91.08
Round  52, Global train loss: 1.522, Global test loss: 2.306, Global test accuracy: 12.05
Round  53, Train loss: 1.575, Test loss: 1.552, Test accuracy: 91.10
Round  53, Global train loss: 1.575, Global test loss: 2.261, Global test accuracy: 18.22
Round  54, Train loss: 1.575, Test loss: 1.552, Test accuracy: 91.12
Round  54, Global train loss: 1.575, Global test loss: 2.245, Global test accuracy: 19.62
Round  55, Train loss: 1.468, Test loss: 1.552, Test accuracy: 91.12
Round  55, Global train loss: 1.468, Global test loss: 2.306, Global test accuracy: 13.11
Round  56, Train loss: 1.523, Test loss: 1.552, Test accuracy: 91.08
Round  56, Global train loss: 1.523, Global test loss: 2.249, Global test accuracy: 20.26
Round  57, Train loss: 1.522, Test loss: 1.552, Test accuracy: 91.08
Round  57, Global train loss: 1.522, Global test loss: 2.291, Global test accuracy: 15.69
Round  58, Train loss: 1.470, Test loss: 1.552, Test accuracy: 91.07
Round  58, Global train loss: 1.470, Global test loss: 2.249, Global test accuracy: 19.38
Round  59, Train loss: 1.560, Test loss: 1.537, Test accuracy: 92.62
Round  59, Global train loss: 1.560, Global test loss: 2.250, Global test accuracy: 18.77
Round  60, Train loss: 1.523, Test loss: 1.537, Test accuracy: 92.63
Round  60, Global train loss: 1.523, Global test loss: 2.256, Global test accuracy: 19.07
Round  61, Train loss: 1.469, Test loss: 1.536, Test accuracy: 92.65
Round  61, Global train loss: 1.469, Global test loss: 2.244, Global test accuracy: 21.09
Round  62, Train loss: 1.520, Test loss: 1.536, Test accuracy: 92.65
Round  62, Global train loss: 1.520, Global test loss: 2.293, Global test accuracy: 12.70
Round  63, Train loss: 1.471, Test loss: 1.536, Test accuracy: 92.62
Round  63, Global train loss: 1.471, Global test loss: 2.284, Global test accuracy: 16.11
Round  64, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.67
Round  64, Global train loss: 1.521, Global test loss: 2.272, Global test accuracy: 16.35
Round  65, Train loss: 1.467, Test loss: 1.536, Test accuracy: 92.67
Round  65, Global train loss: 1.467, Global test loss: 2.267, Global test accuracy: 18.02
Round  66, Train loss: 1.574, Test loss: 1.536, Test accuracy: 92.65
Round  66, Global train loss: 1.574, Global test loss: 2.256, Global test accuracy: 18.19
Round  67, Train loss: 1.466, Test loss: 1.536, Test accuracy: 92.67
Round  67, Global train loss: 1.466, Global test loss: 2.302, Global test accuracy: 13.32
Round  68, Train loss: 1.519, Test loss: 1.536, Test accuracy: 92.67
Round  68, Global train loss: 1.519, Global test loss: 2.256, Global test accuracy: 17.89
Round  69, Train loss: 1.471, Test loss: 1.536, Test accuracy: 92.62
Round  69, Global train loss: 1.471, Global test loss: 2.274, Global test accuracy: 15.32
Round  70, Train loss: 1.519, Test loss: 1.536, Test accuracy: 92.62
Round  70, Global train loss: 1.519, Global test loss: 2.259, Global test accuracy: 18.02
Round  71, Train loss: 1.519, Test loss: 1.536, Test accuracy: 92.62
Round  71, Global train loss: 1.519, Global test loss: 2.288, Global test accuracy: 11.99
Round  72, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.62
Round  72, Global train loss: 1.521, Global test loss: 2.256, Global test accuracy: 18.80
Round  73, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.62
Round  73, Global train loss: 1.468, Global test loss: 2.285, Global test accuracy: 15.28
Round  74, Train loss: 1.467, Test loss: 1.536, Test accuracy: 92.65
Round  74, Global train loss: 1.467, Global test loss: 2.262, Global test accuracy: 17.24
Round  75, Train loss: 1.520, Test loss: 1.536, Test accuracy: 92.63
Round  75, Global train loss: 1.520, Global test loss: 2.276, Global test accuracy: 16.94
Round  76, Train loss: 1.463, Test loss: 1.536, Test accuracy: 92.65
Round  76, Global train loss: 1.463, Global test loss: 2.271, Global test accuracy: 15.87
Round  77, Train loss: 1.465, Test loss: 1.536, Test accuracy: 92.64
Round  77, Global train loss: 1.465, Global test loss: 2.263, Global test accuracy: 16.65
Round  78, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.63
Round  78, Global train loss: 1.521, Global test loss: 2.266, Global test accuracy: 18.27
Round  79, Train loss: 1.520, Test loss: 1.536, Test accuracy: 92.65
Round  79, Global train loss: 1.520, Global test loss: 2.272, Global test accuracy: 16.33
Round  80, Train loss: 1.469, Test loss: 1.536, Test accuracy: 92.65
Round  80, Global train loss: 1.469, Global test loss: 2.291, Global test accuracy: 14.07
Round  81, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.65
Round  81, Global train loss: 1.468, Global test loss: 2.292, Global test accuracy: 12.82
Round  82, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.66
Round  82, Global train loss: 1.468, Global test loss: 2.299, Global test accuracy: 13.32
Round  83, Train loss: 1.471, Test loss: 1.536, Test accuracy: 92.67
Round  83, Global train loss: 1.471, Global test loss: 2.290, Global test accuracy: 12.68
Round  84, Train loss: 1.574, Test loss: 1.536, Test accuracy: 92.65
Round  84, Global train loss: 1.574, Global test loss: 2.260, Global test accuracy: 18.32
Round  85, Train loss: 1.468, Test loss: 1.536, Test accuracy: 92.66
Round  85, Global train loss: 1.468, Global test loss: 2.271, Global test accuracy: 16.30
Round  86, Train loss: 1.524, Test loss: 1.536, Test accuracy: 92.67
Round  86, Global train loss: 1.524, Global test loss: 2.272, Global test accuracy: 18.33
Round  87, Train loss: 1.574, Test loss: 1.536, Test accuracy: 92.66
Round  87, Global train loss: 1.574, Global test loss: 2.257, Global test accuracy: 18.48
Round  88, Train loss: 1.522, Test loss: 1.536, Test accuracy: 92.67
Round  88, Global train loss: 1.522, Global test loss: 2.270, Global test accuracy: 18.27
Round  89, Train loss: 1.466, Test loss: 1.536, Test accuracy: 92.67
Round  89, Global train loss: 1.466, Global test loss: 2.257, Global test accuracy: 18.12
Round  90, Train loss: 1.470, Test loss: 1.536, Test accuracy: 92.67
Round  90, Global train loss: 1.470, Global test loss: 2.277, Global test accuracy: 14.73
Round  91, Train loss: 1.521, Test loss: 1.536, Test accuracy: 92.66
Round  91, Global train loss: 1.521, Global test loss: 2.259, Global test accuracy: 18.12
Round  92, Train loss: 1.470, Test loss: 1.535, Test accuracy: 92.66
Round  92, Global train loss: 1.470, Global test loss: 2.261, Global test accuracy: 17.77
Round  93, Train loss: 1.573, Test loss: 1.535, Test accuracy: 92.67
Round  93, Global train loss: 1.573, Global test loss: 2.255, Global test accuracy: 18.41
Round  94, Train loss: 1.467, Test loss: 1.535, Test accuracy: 92.66
Round  94, Global train loss: 1.467, Global test loss: 2.272, Global test accuracy: 15.89
Round  95, Train loss: 1.518, Test loss: 1.535, Test accuracy: 92.66
Round  95, Global train loss: 1.518, Global test loss: 2.270, Global test accuracy: 16.51/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.471, Test loss: 1.535, Test accuracy: 92.67
Round  96, Global train loss: 1.471, Global test loss: 2.250, Global test accuracy: 19.71
Round  97, Train loss: 1.466, Test loss: 1.535, Test accuracy: 92.69
Round  97, Global train loss: 1.466, Global test loss: 2.276, Global test accuracy: 17.48
Round  98, Train loss: 1.466, Test loss: 1.535, Test accuracy: 92.71
Round  98, Global train loss: 1.466, Global test loss: 2.282, Global test accuracy: 14.94
Round  99, Train loss: 1.521, Test loss: 1.535, Test accuracy: 92.70
Round  99, Global train loss: 1.521, Global test loss: 2.267, Global test accuracy: 18.24
Final Round, Train loss: 1.499, Test loss: 1.535, Test accuracy: 92.67
Final Round, Global train loss: 1.499, Global test loss: 2.267, Global test accuracy: 18.24
Average accuracy final 10 rounds: 92.67416666666665 

Average global accuracy final 10 rounds: 17.181666666666665 

1822.9176616668701
[1.2023556232452393, 2.4047112464904785, 3.5385971069335938, 4.672482967376709, 5.7117815017700195, 6.75108003616333, 7.879024028778076, 9.006968021392822, 10.108105182647705, 11.209242343902588, 12.391313791275024, 13.573385238647461, 14.729222774505615, 15.88506031036377, 16.966779947280884, 18.048499584197998, 19.18014645576477, 20.311793327331543, 21.430498361587524, 22.549203395843506, 23.633347988128662, 24.71749258041382, 25.804816961288452, 26.892141342163086, 28.045157194137573, 29.19817304611206, 30.33489489555359, 31.471616744995117, 32.59690070152283, 33.72218465805054, 34.8810601234436, 36.03993558883667, 37.08724927902222, 38.134562969207764, 39.26082444190979, 40.387085914611816, 41.51132345199585, 42.63556098937988, 43.79666018486023, 44.957759380340576, 46.10708498954773, 47.25641059875488, 48.379685163497925, 49.50295972824097, 50.65385842323303, 51.8047571182251, 52.960922956466675, 54.11708879470825, 55.253185510635376, 56.3892822265625, 57.453086853027344, 58.51689147949219, 59.74452877044678, 60.97216606140137, 62.13459396362305, 63.29702186584473, 64.4407148361206, 65.58440780639648, 66.77611970901489, 67.9678316116333, 69.10297966003418, 70.23812770843506, 71.37557888031006, 72.51303005218506, 73.67464256286621, 74.83625507354736, 76.03168869018555, 77.22712230682373, 78.343679189682, 79.46023607254028, 80.63022994995117, 81.80022382736206, 82.93287658691406, 84.06552934646606, 85.25621104240417, 86.44689273834229, 87.61020350456238, 88.77351427078247, 89.93762159347534, 91.10172891616821, 92.33379578590393, 93.56586265563965, 94.72280979156494, 95.87975692749023, 97.00329756736755, 98.12683820724487, 99.28851509094238, 100.45019197463989, 101.6339111328125, 102.81763029098511, 103.93028163909912, 105.04293298721313, 106.2461290359497, 107.44932508468628, 108.64392828941345, 109.83853149414062, 110.93943500518799, 112.04033851623535, 113.17028498649597, 114.30023145675659, 115.4282066822052, 116.55618190765381, 117.75453996658325, 118.9528980255127, 120.11414456367493, 121.27539110183716, 122.42499017715454, 123.57458925247192, 124.781907081604, 125.98922491073608, 127.14071559906006, 128.29220628738403, 129.41173672676086, 130.5312671661377, 131.65902590751648, 132.78678464889526, 133.9834394454956, 135.18009424209595, 136.30688285827637, 137.4336714744568, 138.63273239135742, 139.83179330825806, 141.0597424507141, 142.28769159317017, 143.42555046081543, 144.5634093284607, 145.72198295593262, 146.88055658340454, 148.0146906375885, 149.14882469177246, 150.31812644004822, 151.48742818832397, 152.6364300251007, 153.78543186187744, 154.96235990524292, 156.1392879486084, 157.3272898197174, 158.51529169082642, 159.68573808670044, 160.85618448257446, 161.98214554786682, 163.10810661315918, 164.2264587879181, 165.344810962677, 166.53973054885864, 167.73465013504028, 168.87084245681763, 170.00703477859497, 171.16222834587097, 172.31742191314697, 173.50186562538147, 174.68630933761597, 175.83835554122925, 176.99040174484253, 178.14206981658936, 179.29373788833618, 180.43966698646545, 181.58559608459473, 182.75863003730774, 183.93166399002075, 185.0176763534546, 186.10368871688843, 187.2703413963318, 188.43699407577515, 189.5921013355255, 190.74720859527588, 191.91555547714233, 193.0839023590088, 194.25844287872314, 195.4329833984375, 196.59115743637085, 197.7493314743042, 198.94427490234375, 200.1392183303833, 201.26100540161133, 202.38279247283936, 203.51870441436768, 204.654616355896, 205.77916383743286, 206.90371131896973, 208.06525707244873, 209.22680282592773, 210.37768054008484, 211.52855825424194, 212.75907850265503, 213.98959875106812, 215.1810405254364, 216.3724822998047, 217.5012447834015, 218.6300072669983, 219.66290426254272, 220.69580125808716, 221.69253993034363, 222.6892786026001, 223.67192482948303, 224.65457105636597, 225.63051080703735, 226.60645055770874, 227.59084010124207, 228.5752296447754, 230.2439935207367, 231.912757396698]
[29.758333333333333, 29.758333333333333, 49.083333333333336, 49.083333333333336, 56.775, 56.775, 60.55833333333333, 60.55833333333333, 61.9, 61.9, 63.00833333333333, 63.00833333333333, 70.0, 70.0, 70.89166666666667, 70.89166666666667, 75.15, 75.15, 77.20833333333333, 77.20833333333333, 80.35, 80.35, 80.41666666666667, 80.41666666666667, 84.8, 84.8, 84.90833333333333, 84.90833333333333, 84.91666666666667, 84.91666666666667, 86.26666666666667, 86.26666666666667, 86.34166666666667, 86.34166666666667, 86.36666666666666, 86.36666666666666, 86.475, 86.475, 86.49166666666666, 86.49166666666666, 86.61666666666666, 86.61666666666666, 86.64166666666667, 86.64166666666667, 86.65, 86.65, 86.64166666666667, 86.64166666666667, 86.65833333333333, 86.65833333333333, 86.65, 86.65, 86.65, 86.65, 86.63333333333334, 86.63333333333334, 86.63333333333334, 86.63333333333334, 86.625, 86.625, 86.61666666666666, 86.61666666666666, 86.59166666666667, 86.59166666666667, 86.61666666666666, 86.61666666666666, 86.925, 86.925, 86.925, 86.925, 87.96666666666667, 87.96666666666667, 87.95, 87.95, 87.95833333333333, 87.95833333333333, 88.2, 88.2, 89.68333333333334, 89.68333333333334, 90.98333333333333, 90.98333333333333, 90.975, 90.975, 90.975, 90.975, 90.975, 90.975, 90.96666666666667, 90.96666666666667, 90.99166666666666, 90.99166666666666, 91.0, 91.0, 91.0, 91.0, 91.00833333333334, 91.00833333333334, 91.05833333333334, 91.05833333333334, 91.06666666666666, 91.06666666666666, 91.075, 91.075, 91.08333333333333, 91.08333333333333, 91.1, 91.1, 91.11666666666666, 91.11666666666666, 91.11666666666666, 91.11666666666666, 91.08333333333333, 91.08333333333333, 91.075, 91.075, 91.06666666666666, 91.06666666666666, 92.625, 92.625, 92.63333333333334, 92.63333333333334, 92.65, 92.65, 92.65, 92.65, 92.625, 92.625, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.65, 92.65, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.61666666666666, 92.61666666666666, 92.625, 92.625, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.625, 92.625, 92.65, 92.65, 92.63333333333334, 92.63333333333334, 92.65, 92.65, 92.64166666666667, 92.64166666666667, 92.63333333333334, 92.63333333333334, 92.65, 92.65, 92.65, 92.65, 92.65, 92.65, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.65, 92.65, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.675, 92.675, 92.66666666666667, 92.66666666666667, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.66666666666667, 92.66666666666667, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.65833333333333, 92.675, 92.675, 92.69166666666666, 92.69166666666666, 92.70833333333333, 92.70833333333333, 92.7, 92.7, 92.675, 92.675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.85
Round   0, Global train loss: 2.301, Global test loss: 2.303, Global test accuracy: 10.16
Round   1, Train loss: 2.300, Test loss: 2.300, Test accuracy: 19.43
Round   1, Global train loss: 2.300, Global test loss: 2.302, Global test accuracy: 11.66
Round   2, Train loss: 2.299, Test loss: 2.299, Test accuracy: 22.77
Round   2, Global train loss: 2.299, Global test loss: 2.302, Global test accuracy: 12.34
Round   3, Train loss: 2.298, Test loss: 2.297, Test accuracy: 26.84
Round   3, Global train loss: 2.298, Global test loss: 2.301, Global test accuracy: 12.57
Round   4, Train loss: 2.295, Test loss: 2.295, Test accuracy: 28.52
Round   4, Global train loss: 2.295, Global test loss: 2.300, Global test accuracy: 14.28
Round   5, Train loss: 2.292, Test loss: 2.289, Test accuracy: 29.78
Round   5, Global train loss: 2.292, Global test loss: 2.299, Global test accuracy: 13.59
Round   6, Train loss: 2.282, Test loss: 2.278, Test accuracy: 27.83
Round   6, Global train loss: 2.282, Global test loss: 2.296, Global test accuracy: 13.60
Round   7, Train loss: 2.246, Test loss: 2.250, Test accuracy: 30.41
Round   7, Global train loss: 2.246, Global test loss: 2.287, Global test accuracy: 14.40
Round   8, Train loss: 2.176, Test loss: 2.197, Test accuracy: 34.02
Round   8, Global train loss: 2.176, Global test loss: 2.278, Global test accuracy: 15.25
Round   9, Train loss: 2.138, Test loss: 2.159, Test accuracy: 37.22
Round   9, Global train loss: 2.138, Global test loss: 2.274, Global test accuracy: 16.35
Round  10, Train loss: 2.118, Test loss: 2.110, Test accuracy: 40.42
Round  10, Global train loss: 2.118, Global test loss: 2.266, Global test accuracy: 17.02
Round  11, Train loss: 2.084, Test loss: 2.092, Test accuracy: 42.42
Round  11, Global train loss: 2.084, Global test loss: 2.275, Global test accuracy: 15.39
Round  12, Train loss: 2.063, Test loss: 2.077, Test accuracy: 43.14
Round  12, Global train loss: 2.063, Global test loss: 2.282, Global test accuracy: 14.99
Round  13, Train loss: 2.064, Test loss: 2.063, Test accuracy: 43.35
Round  13, Global train loss: 2.064, Global test loss: 2.268, Global test accuracy: 17.56
Round  14, Train loss: 2.035, Test loss: 2.036, Test accuracy: 44.50
Round  14, Global train loss: 2.035, Global test loss: 2.274, Global test accuracy: 15.93
Round  15, Train loss: 2.019, Test loss: 2.007, Test accuracy: 46.36
Round  15, Global train loss: 2.019, Global test loss: 2.260, Global test accuracy: 18.32
Round  16, Train loss: 2.037, Test loss: 2.001, Test accuracy: 46.60
Round  16, Global train loss: 2.037, Global test loss: 2.271, Global test accuracy: 16.67
Round  17, Train loss: 2.065, Test loss: 2.004, Test accuracy: 46.15
Round  17, Global train loss: 2.065, Global test loss: 2.269, Global test accuracy: 16.92
Round  18, Train loss: 2.042, Test loss: 2.001, Test accuracy: 46.52
Round  18, Global train loss: 2.042, Global test loss: 2.269, Global test accuracy: 17.32
Round  19, Train loss: 2.024, Test loss: 1.990, Test accuracy: 47.64
Round  19, Global train loss: 2.024, Global test loss: 2.285, Global test accuracy: 15.12
Round  20, Train loss: 2.030, Test loss: 1.985, Test accuracy: 48.12
Round  20, Global train loss: 2.030, Global test loss: 2.267, Global test accuracy: 17.47
Round  21, Train loss: 2.057, Test loss: 1.991, Test accuracy: 47.39
Round  21, Global train loss: 2.057, Global test loss: 2.274, Global test accuracy: 17.02
Round  22, Train loss: 2.015, Test loss: 1.986, Test accuracy: 47.70
Round  22, Global train loss: 2.015, Global test loss: 2.257, Global test accuracy: 18.96
Round  23, Train loss: 1.971, Test loss: 1.982, Test accuracy: 48.15
Round  23, Global train loss: 1.971, Global test loss: 2.268, Global test accuracy: 17.32
Round  24, Train loss: 2.046, Test loss: 1.996, Test accuracy: 46.65
Round  24, Global train loss: 2.046, Global test loss: 2.262, Global test accuracy: 18.40
Round  25, Train loss: 2.047, Test loss: 1.988, Test accuracy: 47.52
Round  25, Global train loss: 2.047, Global test loss: 2.270, Global test accuracy: 17.16
Round  26, Train loss: 1.984, Test loss: 1.986, Test accuracy: 47.70
Round  26, Global train loss: 1.984, Global test loss: 2.263, Global test accuracy: 18.06
Round  27, Train loss: 1.988, Test loss: 1.974, Test accuracy: 48.87
Round  27, Global train loss: 1.988, Global test loss: 2.267, Global test accuracy: 17.62
Round  28, Train loss: 1.993, Test loss: 1.984, Test accuracy: 47.76
Round  28, Global train loss: 1.993, Global test loss: 2.261, Global test accuracy: 18.20
Round  29, Train loss: 2.029, Test loss: 1.982, Test accuracy: 47.91
Round  29, Global train loss: 2.029, Global test loss: 2.264, Global test accuracy: 17.79
Round  30, Train loss: 1.994, Test loss: 1.976, Test accuracy: 48.51
Round  30, Global train loss: 1.994, Global test loss: 2.277, Global test accuracy: 16.48
Round  31, Train loss: 2.019, Test loss: 1.980, Test accuracy: 48.09
Round  31, Global train loss: 2.019, Global test loss: 2.258, Global test accuracy: 19.20
Round  32, Train loss: 2.018, Test loss: 1.979, Test accuracy: 48.08
Round  32, Global train loss: 2.018, Global test loss: 2.266, Global test accuracy: 17.76
Round  33, Train loss: 1.953, Test loss: 1.979, Test accuracy: 48.22
Round  33, Global train loss: 1.953, Global test loss: 2.262, Global test accuracy: 18.18
Round  34, Train loss: 2.019, Test loss: 1.992, Test accuracy: 46.64
Round  34, Global train loss: 2.019, Global test loss: 2.266, Global test accuracy: 17.71
Round  35, Train loss: 2.020, Test loss: 1.985, Test accuracy: 47.36
Round  35, Global train loss: 2.020, Global test loss: 2.271, Global test accuracy: 17.23
Round  36, Train loss: 2.003, Test loss: 1.983, Test accuracy: 47.49
Round  36, Global train loss: 2.003, Global test loss: 2.262, Global test accuracy: 18.12
Round  37, Train loss: 2.002, Test loss: 1.980, Test accuracy: 47.88
Round  37, Global train loss: 2.002, Global test loss: 2.265, Global test accuracy: 17.68
Round  38, Train loss: 1.882, Test loss: 1.975, Test accuracy: 48.41
Round  38, Global train loss: 1.882, Global test loss: 2.260, Global test accuracy: 18.37
Round  39, Train loss: 1.971, Test loss: 1.976, Test accuracy: 48.38
Round  39, Global train loss: 1.971, Global test loss: 2.266, Global test accuracy: 17.57
Round  40, Train loss: 1.952, Test loss: 1.978, Test accuracy: 48.08
Round  40, Global train loss: 1.952, Global test loss: 2.262, Global test accuracy: 18.33
Round  41, Train loss: 1.972, Test loss: 1.977, Test accuracy: 48.07
Round  41, Global train loss: 1.972, Global test loss: 2.262, Global test accuracy: 18.50
Round  42, Train loss: 2.015, Test loss: 1.969, Test accuracy: 48.96
Round  42, Global train loss: 2.015, Global test loss: 2.283, Global test accuracy: 15.47
Round  43, Train loss: 1.959, Test loss: 1.963, Test accuracy: 49.56
Round  43, Global train loss: 1.959, Global test loss: 2.279, Global test accuracy: 15.90
Round  44, Train loss: 1.951, Test loss: 1.963, Test accuracy: 49.60
Round  44, Global train loss: 1.951, Global test loss: 2.267, Global test accuracy: 17.33
Round  45, Train loss: 1.961, Test loss: 1.972, Test accuracy: 48.49
Round  45, Global train loss: 1.961, Global test loss: 2.263, Global test accuracy: 18.19
Round  46, Train loss: 2.022, Test loss: 1.972, Test accuracy: 48.48
Round  46, Global train loss: 2.022, Global test loss: 2.277, Global test accuracy: 16.34
Round  47, Train loss: 1.943, Test loss: 1.973, Test accuracy: 48.48
Round  47, Global train loss: 1.943, Global test loss: 2.259, Global test accuracy: 18.73
Round  48, Train loss: 1.993, Test loss: 1.971, Test accuracy: 48.64
Round  48, Global train loss: 1.993, Global test loss: 2.257, Global test accuracy: 18.74
Round  49, Train loss: 1.863, Test loss: 1.961, Test accuracy: 49.68
Round  49, Global train loss: 1.863, Global test loss: 2.255, Global test accuracy: 19.23
Round  50, Train loss: 1.929, Test loss: 1.964, Test accuracy: 49.35
Round  50, Global train loss: 1.929, Global test loss: 2.274, Global test accuracy: 16.22
Round  51, Train loss: 1.974, Test loss: 1.959, Test accuracy: 49.90
Round  51, Global train loss: 1.974, Global test loss: 2.273, Global test accuracy: 17.03
Round  52, Train loss: 1.968, Test loss: 1.958, Test accuracy: 49.92
Round  52, Global train loss: 1.968, Global test loss: 2.261, Global test accuracy: 18.31
Round  53, Train loss: 1.981, Test loss: 1.963, Test accuracy: 49.49
Round  53, Global train loss: 1.981, Global test loss: 2.262, Global test accuracy: 18.23
Round  54, Train loss: 1.968, Test loss: 1.964, Test accuracy: 49.40
Round  54, Global train loss: 1.968, Global test loss: 2.276, Global test accuracy: 16.55
Round  55, Train loss: 1.972, Test loss: 1.964, Test accuracy: 49.40
Round  55, Global train loss: 1.972, Global test loss: 2.265, Global test accuracy: 17.52
Round  56, Train loss: 1.943, Test loss: 1.961, Test accuracy: 49.66
Round  56, Global train loss: 1.943, Global test loss: 2.271, Global test accuracy: 17.25
Round  57, Train loss: 1.964, Test loss: 1.956, Test accuracy: 50.18
Round  57, Global train loss: 1.964, Global test loss: 2.268, Global test accuracy: 17.12
Round  58, Train loss: 2.008, Test loss: 1.951, Test accuracy: 50.74
Round  58, Global train loss: 2.008, Global test loss: 2.272, Global test accuracy: 15.95
Round  59, Train loss: 1.966, Test loss: 1.941, Test accuracy: 51.76
Round  59, Global train loss: 1.966, Global test loss: 2.262, Global test accuracy: 18.20
Round  60, Train loss: 1.917, Test loss: 1.938, Test accuracy: 52.09
Round  60, Global train loss: 1.917, Global test loss: 2.277, Global test accuracy: 16.57
Round  61, Train loss: 1.906, Test loss: 1.936, Test accuracy: 52.28
Round  61, Global train loss: 1.906, Global test loss: 2.277, Global test accuracy: 15.99
Round  62, Train loss: 1.955, Test loss: 1.936, Test accuracy: 52.29
Round  62, Global train loss: 1.955, Global test loss: 2.264, Global test accuracy: 17.67
Round  63, Train loss: 1.884, Test loss: 1.932, Test accuracy: 52.62
Round  63, Global train loss: 1.884, Global test loss: 2.270, Global test accuracy: 17.14
Round  64, Train loss: 1.910, Test loss: 1.925, Test accuracy: 53.45
Round  64, Global train loss: 1.910, Global test loss: 2.252, Global test accuracy: 19.48
Round  65, Train loss: 1.939, Test loss: 1.926, Test accuracy: 53.33
Round  65, Global train loss: 1.939, Global test loss: 2.281, Global test accuracy: 15.77
Round  66, Train loss: 1.874, Test loss: 1.920, Test accuracy: 53.94
Round  66, Global train loss: 1.874, Global test loss: 2.271, Global test accuracy: 17.27
Round  67, Train loss: 1.904, Test loss: 1.923, Test accuracy: 53.70
Round  67, Global train loss: 1.904, Global test loss: 2.265, Global test accuracy: 17.39
Round  68, Train loss: 1.935, Test loss: 1.924, Test accuracy: 53.52
Round  68, Global train loss: 1.935, Global test loss: 2.285, Global test accuracy: 15.21
Round  69, Train loss: 1.928, Test loss: 1.921, Test accuracy: 53.86
Round  69, Global train loss: 1.928, Global test loss: 2.269, Global test accuracy: 17.30
Round  70, Train loss: 1.958, Test loss: 1.916, Test accuracy: 54.24
Round  70, Global train loss: 1.958, Global test loss: 2.271, Global test accuracy: 17.28
Round  71, Train loss: 1.893, Test loss: 1.915, Test accuracy: 54.30
Round  71, Global train loss: 1.893, Global test loss: 2.268, Global test accuracy: 17.57
Round  72, Train loss: 1.938, Test loss: 1.916, Test accuracy: 54.28
Round  72, Global train loss: 1.938, Global test loss: 2.269, Global test accuracy: 17.23
Round  73, Train loss: 1.900, Test loss: 1.913, Test accuracy: 54.63
Round  73, Global train loss: 1.900, Global test loss: 2.270, Global test accuracy: 16.88
Round  74, Train loss: 1.898, Test loss: 1.912, Test accuracy: 54.76
Round  74, Global train loss: 1.898, Global test loss: 2.281, Global test accuracy: 15.90
Round  75, Train loss: 1.930, Test loss: 1.908, Test accuracy: 55.09
Round  75, Global train loss: 1.930, Global test loss: 2.261, Global test accuracy: 18.18
Round  76, Train loss: 1.912, Test loss: 1.914, Test accuracy: 54.43
Round  76, Global train loss: 1.912, Global test loss: 2.256, Global test accuracy: 18.77
Round  77, Train loss: 1.927, Test loss: 1.911, Test accuracy: 54.73
Round  77, Global train loss: 1.927, Global test loss: 2.262, Global test accuracy: 18.12
Round  78, Train loss: 1.911, Test loss: 1.905, Test accuracy: 55.31
Round  78, Global train loss: 1.911, Global test loss: 2.262, Global test accuracy: 17.68
Round  79, Train loss: 1.913, Test loss: 1.907, Test accuracy: 55.25
Round  79, Global train loss: 1.913, Global test loss: 2.265, Global test accuracy: 17.60
Round  80, Train loss: 1.869, Test loss: 1.899, Test accuracy: 56.04
Round  80, Global train loss: 1.869, Global test loss: 2.268, Global test accuracy: 17.02
Round  81, Train loss: 1.846, Test loss: 1.900, Test accuracy: 56.08
Round  81, Global train loss: 1.846, Global test loss: 2.265, Global test accuracy: 17.84
Round  82, Train loss: 1.889, Test loss: 1.900, Test accuracy: 56.09
Round  82, Global train loss: 1.889, Global test loss: 2.271, Global test accuracy: 16.49
Round  83, Train loss: 1.887, Test loss: 1.904, Test accuracy: 55.53
Round  83, Global train loss: 1.887, Global test loss: 2.258, Global test accuracy: 18.32
Round  84, Train loss: 1.887, Test loss: 1.903, Test accuracy: 55.60
Round  84, Global train loss: 1.887, Global test loss: 2.258, Global test accuracy: 18.77
Round  85, Train loss: 1.872, Test loss: 1.903, Test accuracy: 55.72
Round  85, Global train loss: 1.872, Global test loss: 2.257, Global test accuracy: 18.70
Round  86, Train loss: 1.847, Test loss: 1.903, Test accuracy: 55.73
Round  86, Global train loss: 1.847, Global test loss: 2.262, Global test accuracy: 18.09
Round  87, Train loss: 1.829, Test loss: 1.902, Test accuracy: 55.66
Round  87, Global train loss: 1.829, Global test loss: 2.258, Global test accuracy: 18.79
Round  88, Train loss: 1.887, Test loss: 1.904, Test accuracy: 55.45
Round  88, Global train loss: 1.887, Global test loss: 2.264, Global test accuracy: 17.43
Round  89, Train loss: 1.887, Test loss: 1.903, Test accuracy: 55.57
Round  89, Global train loss: 1.887, Global test loss: 2.265, Global test accuracy: 17.60
Round  90, Train loss: 1.823, Test loss: 1.899, Test accuracy: 56.04
Round  90, Global train loss: 1.823, Global test loss: 2.264, Global test accuracy: 17.82
Round  91, Train loss: 1.862, Test loss: 1.898, Test accuracy: 56.19
Round  91, Global train loss: 1.862, Global test loss: 2.283, Global test accuracy: 15.33
Round  92, Train loss: 1.863, Test loss: 1.898, Test accuracy: 56.16
Round  92, Global train loss: 1.863, Global test loss: 2.261, Global test accuracy: 18.47
Round  93, Train loss: 1.858, Test loss: 1.909, Test accuracy: 55.08
Round  93, Global train loss: 1.858, Global test loss: 2.257, Global test accuracy: 18.93
Round  94, Train loss: 1.851, Test loss: 1.908, Test accuracy: 55.11
Round  94, Global train loss: 1.851, Global test loss: 2.259, Global test accuracy: 18.61
Round  95, Train loss: 1.870, Test loss: 1.902, Test accuracy: 55.73
Round  95, Global train loss: 1.870, Global test loss: 2.277, Global test accuracy: 16.24/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.928, Test loss: 1.891, Test accuracy: 56.85
Round  96, Global train loss: 1.928, Global test loss: 2.266, Global test accuracy: 17.61
Round  97, Train loss: 1.880, Test loss: 1.887, Test accuracy: 57.28
Round  97, Global train loss: 1.880, Global test loss: 2.274, Global test accuracy: 16.18
Round  98, Train loss: 1.879, Test loss: 1.891, Test accuracy: 56.86
Round  98, Global train loss: 1.879, Global test loss: 2.270, Global test accuracy: 17.02
Round  99, Train loss: 1.895, Test loss: 1.895, Test accuracy: 56.40
Round  99, Global train loss: 1.895, Global test loss: 2.261, Global test accuracy: 18.00
Final Round, Train loss: 1.845, Test loss: 1.885, Test accuracy: 57.27
Final Round, Global train loss: 1.845, Global test loss: 2.261, Global test accuracy: 18.00
Average accuracy final 10 rounds: 56.17 

Average global accuracy final 10 rounds: 17.42 

1799.8145911693573
[1.3159425258636475, 2.631885051727295, 3.831759452819824, 5.0316338539123535, 6.171195983886719, 7.310758113861084, 8.463684320449829, 9.616610527038574, 10.786265850067139, 11.955921173095703, 13.061208009719849, 14.166494846343994, 15.2741379737854, 16.381781101226807, 17.53905725479126, 18.696333408355713, 19.795631408691406, 20.8949294090271, 21.951335430145264, 23.007741451263428, 24.150272846221924, 25.29280424118042, 26.44931197166443, 27.605819702148438, 28.68204116821289, 29.758262634277344, 30.88025736808777, 32.00225210189819, 33.11403751373291, 34.22582292556763, 35.35825490951538, 36.490686893463135, 37.61275792121887, 38.73482894897461, 39.84144568443298, 40.94806241989136, 42.13667345046997, 43.325284481048584, 44.4098002910614, 45.49431610107422, 46.576963901519775, 47.65961170196533, 48.7870237827301, 49.91443586349487, 51.013325452804565, 52.11221504211426, 53.19741892814636, 54.28262281417847, 55.38697147369385, 56.49132013320923, 57.64729118347168, 58.80326223373413, 59.897380113601685, 60.99149799346924, 62.09899044036865, 63.206482887268066, 64.35217356681824, 65.49786424636841, 66.66073179244995, 67.8235993385315, 68.9341492652893, 70.04469919204712, 71.19604659080505, 72.34739398956299, 73.56592178344727, 74.78444957733154, 75.85504102706909, 76.92563247680664, 78.08150506019592, 79.2373776435852, 80.42908382415771, 81.62079000473022, 82.7816092967987, 83.94242858886719, 85.06629586219788, 86.19016313552856, 87.34170937538147, 88.49325561523438, 89.65933895111084, 90.8254222869873, 91.89752531051636, 92.96962833404541, 94.09196376800537, 95.21429920196533, 96.29400897026062, 97.37371873855591, 98.51908087730408, 99.66444301605225, 100.73501801490784, 101.80559301376343, 102.90532994270325, 104.00506687164307, 105.15611839294434, 106.3071699142456, 107.35923147201538, 108.41129302978516, 109.48007440567017, 110.54885578155518, 111.65725564956665, 112.76565551757812, 113.88474655151367, 115.00383758544922, 116.1015100479126, 117.19918251037598, 118.29058074951172, 119.38197898864746, 120.56247353553772, 121.74296808242798, 122.8605432510376, 123.97811841964722, 125.04835176467896, 126.1185851097107, 127.26333165168762, 128.40807819366455, 129.54012727737427, 130.67217636108398, 131.75904893875122, 132.84592151641846, 133.98553681373596, 135.12515211105347, 136.31965827941895, 137.51416444778442, 138.53855967521667, 139.56295490264893, 140.64682698249817, 141.7306990623474, 142.85450530052185, 143.9783115386963, 145.07406949996948, 146.16982746124268, 147.26026940345764, 148.3507113456726, 149.44165873527527, 150.53260612487793, 151.65622401237488, 152.77984189987183, 153.86505556106567, 154.95026922225952, 156.05805492401123, 157.16584062576294, 158.21313905715942, 159.2604374885559, 160.41804599761963, 161.57565450668335, 162.6578528881073, 163.74005126953125, 164.8143322467804, 165.88861322402954, 167.01967358589172, 168.1507339477539, 169.26363229751587, 170.37653064727783, 171.4637303352356, 172.55093002319336, 173.66663360595703, 174.7823371887207, 175.9279980659485, 177.07365894317627, 178.17564749717712, 179.27763605117798, 180.35859322547913, 181.43955039978027, 182.57999086380005, 183.72043132781982, 184.84283876419067, 185.96524620056152, 187.0258014202118, 188.08635663986206, 189.19652318954468, 190.3066897392273, 191.47163105010986, 192.63657236099243, 193.72435450553894, 194.81213665008545, 195.90543341636658, 196.9987301826477, 198.16257405281067, 199.32641792297363, 200.38005828857422, 201.4336986541748, 202.55785536766052, 203.68201208114624, 204.82567524909973, 205.96933841705322, 207.13588571548462, 208.30243301391602, 209.40849018096924, 210.51454734802246, 211.64920949935913, 212.7838716506958, 213.91191792488098, 215.03996419906616, 216.1642608642578, 217.28855752944946, 218.3996560573578, 219.5107545852661, 220.5837414264679, 221.65672826766968, 222.7964973449707, 223.93626642227173, 225.78283667564392, 227.6294069290161]
[13.85, 13.85, 19.425, 19.425, 22.775, 22.775, 26.841666666666665, 26.841666666666665, 28.525, 28.525, 29.783333333333335, 29.783333333333335, 27.833333333333332, 27.833333333333332, 30.408333333333335, 30.408333333333335, 34.016666666666666, 34.016666666666666, 37.21666666666667, 37.21666666666667, 40.425, 40.425, 42.416666666666664, 42.416666666666664, 43.141666666666666, 43.141666666666666, 43.35, 43.35, 44.5, 44.5, 46.358333333333334, 46.358333333333334, 46.6, 46.6, 46.15, 46.15, 46.525, 46.525, 47.641666666666666, 47.641666666666666, 48.11666666666667, 48.11666666666667, 47.391666666666666, 47.391666666666666, 47.7, 47.7, 48.15, 48.15, 46.65, 46.65, 47.516666666666666, 47.516666666666666, 47.7, 47.7, 48.86666666666667, 48.86666666666667, 47.75833333333333, 47.75833333333333, 47.90833333333333, 47.90833333333333, 48.50833333333333, 48.50833333333333, 48.09166666666667, 48.09166666666667, 48.083333333333336, 48.083333333333336, 48.21666666666667, 48.21666666666667, 46.641666666666666, 46.641666666666666, 47.358333333333334, 47.358333333333334, 47.49166666666667, 47.49166666666667, 47.88333333333333, 47.88333333333333, 48.40833333333333, 48.40833333333333, 48.375, 48.375, 48.075, 48.075, 48.06666666666667, 48.06666666666667, 48.958333333333336, 48.958333333333336, 49.55833333333333, 49.55833333333333, 49.6, 49.6, 48.49166666666667, 48.49166666666667, 48.475, 48.475, 48.475, 48.475, 48.641666666666666, 48.641666666666666, 49.68333333333333, 49.68333333333333, 49.35, 49.35, 49.9, 49.9, 49.916666666666664, 49.916666666666664, 49.49166666666667, 49.49166666666667, 49.4, 49.4, 49.4, 49.4, 49.65833333333333, 49.65833333333333, 50.18333333333333, 50.18333333333333, 50.74166666666667, 50.74166666666667, 51.75833333333333, 51.75833333333333, 52.09166666666667, 52.09166666666667, 52.28333333333333, 52.28333333333333, 52.291666666666664, 52.291666666666664, 52.625, 52.625, 53.45, 53.45, 53.333333333333336, 53.333333333333336, 53.94166666666667, 53.94166666666667, 53.7, 53.7, 53.516666666666666, 53.516666666666666, 53.858333333333334, 53.858333333333334, 54.24166666666667, 54.24166666666667, 54.3, 54.3, 54.28333333333333, 54.28333333333333, 54.63333333333333, 54.63333333333333, 54.75833333333333, 54.75833333333333, 55.09166666666667, 55.09166666666667, 54.43333333333333, 54.43333333333333, 54.733333333333334, 54.733333333333334, 55.30833333333333, 55.30833333333333, 55.25, 55.25, 56.041666666666664, 56.041666666666664, 56.075, 56.075, 56.09166666666667, 56.09166666666667, 55.53333333333333, 55.53333333333333, 55.6, 55.6, 55.71666666666667, 55.71666666666667, 55.733333333333334, 55.733333333333334, 55.65833333333333, 55.65833333333333, 55.45, 55.45, 55.56666666666667, 55.56666666666667, 56.041666666666664, 56.041666666666664, 56.19166666666667, 56.19166666666667, 56.15833333333333, 56.15833333333333, 55.075, 55.075, 55.108333333333334, 55.108333333333334, 55.733333333333334, 55.733333333333334, 56.85, 56.85, 57.28333333333333, 57.28333333333333, 56.858333333333334, 56.858333333333334, 56.4, 56.4, 57.266666666666666, 57.266666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.02
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.28
Round   2, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.26
Round   3, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.16
Round   4, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.60
Round   5, Train loss: 2.301, Test loss: 2.301, Test accuracy: 11.48
Round   6, Train loss: 2.300, Test loss: 2.301, Test accuracy: 12.51
Round   7, Train loss: 2.301, Test loss: 2.301, Test accuracy: 13.56
Round   8, Train loss: 2.300, Test loss: 2.301, Test accuracy: 14.25
Round   9, Train loss: 2.298, Test loss: 2.300, Test accuracy: 14.94
Round  10, Train loss: 2.298, Test loss: 2.299, Test accuracy: 16.04
Round  11, Train loss: 2.297, Test loss: 2.299, Test accuracy: 17.35
Round  12, Train loss: 2.297, Test loss: 2.297, Test accuracy: 17.71
Round  13, Train loss: 2.291, Test loss: 2.295, Test accuracy: 18.14
Round  14, Train loss: 2.283, Test loss: 2.291, Test accuracy: 16.70
Round  15, Train loss: 2.272, Test loss: 2.282, Test accuracy: 16.98
Round  16, Train loss: 2.251, Test loss: 2.271, Test accuracy: 19.87
Round  17, Train loss: 2.251, Test loss: 2.255, Test accuracy: 22.48
Round  18, Train loss: 2.200, Test loss: 2.231, Test accuracy: 23.42
Round  19, Train loss: 2.142, Test loss: 2.195, Test accuracy: 25.82
Round  20, Train loss: 2.091, Test loss: 2.152, Test accuracy: 30.57
Round  21, Train loss: 2.058, Test loss: 2.117, Test accuracy: 34.36
Round  22, Train loss: 2.001, Test loss: 2.096, Test accuracy: 36.09
Round  23, Train loss: 2.023, Test loss: 2.060, Test accuracy: 39.95
Round  24, Train loss: 1.974, Test loss: 2.035, Test accuracy: 42.66
Round  25, Train loss: 1.965, Test loss: 2.010, Test accuracy: 45.38
Round  26, Train loss: 1.953, Test loss: 1.996, Test accuracy: 46.67
Round  27, Train loss: 1.890, Test loss: 1.983, Test accuracy: 48.27
Round  28, Train loss: 1.990, Test loss: 1.964, Test accuracy: 50.34
Round  29, Train loss: 1.944, Test loss: 1.941, Test accuracy: 52.94
Round  30, Train loss: 1.914, Test loss: 1.937, Test accuracy: 53.41
Round  31, Train loss: 1.925, Test loss: 1.914, Test accuracy: 55.41
Round  32, Train loss: 1.907, Test loss: 1.910, Test accuracy: 55.71
Round  33, Train loss: 1.871, Test loss: 1.906, Test accuracy: 56.07
Round  34, Train loss: 1.872, Test loss: 1.903, Test accuracy: 56.23
Round  35, Train loss: 1.899, Test loss: 1.901, Test accuracy: 56.41
Round  36, Train loss: 1.873, Test loss: 1.899, Test accuracy: 56.58
Round  37, Train loss: 1.868, Test loss: 1.893, Test accuracy: 57.14
Round  38, Train loss: 1.872, Test loss: 1.888, Test accuracy: 57.77
Round  39, Train loss: 1.848, Test loss: 1.883, Test accuracy: 58.10
Round  40, Train loss: 1.840, Test loss: 1.880, Test accuracy: 58.45
Round  41, Train loss: 1.895, Test loss: 1.875, Test accuracy: 58.91
Round  42, Train loss: 1.885, Test loss: 1.874, Test accuracy: 59.03
Round  43, Train loss: 1.828, Test loss: 1.872, Test accuracy: 59.20
Round  44, Train loss: 1.831, Test loss: 1.870, Test accuracy: 59.27
Round  45, Train loss: 1.822, Test loss: 1.866, Test accuracy: 59.77
Round  46, Train loss: 1.825, Test loss: 1.864, Test accuracy: 59.88
Round  47, Train loss: 1.804, Test loss: 1.864, Test accuracy: 59.95
Round  48, Train loss: 1.838, Test loss: 1.863, Test accuracy: 59.92
Round  49, Train loss: 1.784, Test loss: 1.863, Test accuracy: 59.84
Round  50, Train loss: 1.854, Test loss: 1.862, Test accuracy: 59.91
Round  51, Train loss: 1.869, Test loss: 1.860, Test accuracy: 60.14
Round  52, Train loss: 1.850, Test loss: 1.858, Test accuracy: 60.31
Round  53, Train loss: 1.825, Test loss: 1.858, Test accuracy: 60.42
Round  54, Train loss: 1.802, Test loss: 1.858, Test accuracy: 60.43
Round  55, Train loss: 1.868, Test loss: 1.857, Test accuracy: 60.38
Round  56, Train loss: 1.809, Test loss: 1.857, Test accuracy: 60.43
Round  57, Train loss: 1.818, Test loss: 1.851, Test accuracy: 61.13
Round  58, Train loss: 1.801, Test loss: 1.850, Test accuracy: 61.24
Round  59, Train loss: 1.794, Test loss: 1.849, Test accuracy: 61.13
Round  60, Train loss: 1.761, Test loss: 1.849, Test accuracy: 61.19
Round  61, Train loss: 1.832, Test loss: 1.849, Test accuracy: 61.23
Round  62, Train loss: 1.829, Test loss: 1.849, Test accuracy: 61.22
Round  63, Train loss: 1.823, Test loss: 1.848, Test accuracy: 61.27
Round  64, Train loss: 1.824, Test loss: 1.849, Test accuracy: 61.29
Round  65, Train loss: 1.826, Test loss: 1.848, Test accuracy: 61.31
Round  66, Train loss: 1.770, Test loss: 1.846, Test accuracy: 61.58
Round  67, Train loss: 1.877, Test loss: 1.846, Test accuracy: 61.42
Round  68, Train loss: 1.858, Test loss: 1.845, Test accuracy: 61.58
Round  69, Train loss: 1.797, Test loss: 1.845, Test accuracy: 61.58
Round  70, Train loss: 1.761, Test loss: 1.844, Test accuracy: 61.66
Round  71, Train loss: 1.776, Test loss: 1.845, Test accuracy: 61.62
Round  72, Train loss: 1.829, Test loss: 1.844, Test accuracy: 61.78
Round  73, Train loss: 1.846, Test loss: 1.843, Test accuracy: 61.75
Round  74, Train loss: 1.819, Test loss: 1.843, Test accuracy: 61.76
Round  75, Train loss: 1.862, Test loss: 1.843, Test accuracy: 61.73
Round  76, Train loss: 1.862, Test loss: 1.843, Test accuracy: 61.84
Round  77, Train loss: 1.812, Test loss: 1.842, Test accuracy: 61.92
Round  78, Train loss: 1.820, Test loss: 1.842, Test accuracy: 61.88
Round  79, Train loss: 1.822, Test loss: 1.841, Test accuracy: 61.92
Round  80, Train loss: 1.815, Test loss: 1.841, Test accuracy: 61.94
Round  81, Train loss: 1.817, Test loss: 1.838, Test accuracy: 62.23
Round  82, Train loss: 1.784, Test loss: 1.838, Test accuracy: 62.29
Round  83, Train loss: 1.813, Test loss: 1.838, Test accuracy: 62.23
Round  84, Train loss: 1.801, Test loss: 1.837, Test accuracy: 62.36
Round  85, Train loss: 1.784, Test loss: 1.837, Test accuracy: 62.38
Round  86, Train loss: 1.816, Test loss: 1.837, Test accuracy: 62.38
Round  87, Train loss: 1.816, Test loss: 1.836, Test accuracy: 62.45
Round  88, Train loss: 1.848, Test loss: 1.836, Test accuracy: 62.53
Round  89, Train loss: 1.751, Test loss: 1.835, Test accuracy: 62.45
Round  90, Train loss: 1.825, Test loss: 1.835, Test accuracy: 62.46
Round  91, Train loss: 1.799, Test loss: 1.835, Test accuracy: 62.52
Round  92, Train loss: 1.837, Test loss: 1.834, Test accuracy: 62.52
Round  93, Train loss: 1.819, Test loss: 1.833, Test accuracy: 62.71
Round  94, Train loss: 1.806, Test loss: 1.833, Test accuracy: 62.74
Round  95, Train loss: 1.809, Test loss: 1.833, Test accuracy: 62.66
Round  96, Train loss: 1.797, Test loss: 1.830, Test accuracy: 63.02
Round  97, Train loss: 1.847, Test loss: 1.830, Test accuracy: 63.00
Round  98, Train loss: 1.800, Test loss: 1.829, Test accuracy: 63.02/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.817, Test loss: 1.829, Test accuracy: 63.02
Final Round, Train loss: 1.797, Test loss: 1.828, Test accuracy: 63.17
Average accuracy final 10 rounds: 62.767500000000005 

1410.395344734192
[1.1953210830688477, 2.3906421661376953, 3.4845526218414307, 4.578463077545166, 5.642160892486572, 6.7058587074279785, 7.8509345054626465, 8.996010303497314, 10.138628005981445, 11.281245708465576, 12.40322756767273, 13.525209426879883, 14.581189155578613, 15.637168884277344, 16.724204301834106, 17.81123971939087, 18.9644296169281, 20.117619514465332, 21.213669776916504, 22.309720039367676, 23.3784601688385, 24.447200298309326, 25.531481504440308, 26.61576271057129, 27.744181871414185, 28.87260103225708, 30.036638736724854, 31.200676441192627, 32.24859070777893, 33.296504974365234, 34.37670063972473, 35.45689630508423, 36.58472752571106, 37.71255874633789, 38.845258474349976, 39.97795820236206, 41.02566313743591, 42.073368072509766, 43.12613773345947, 44.17890739440918, 45.22411918640137, 46.269330978393555, 47.410780906677246, 48.55223083496094, 49.65454697608948, 50.75686311721802, 51.846312046051025, 52.93576097488403, 53.99504780769348, 55.05433464050293, 56.143678426742554, 57.23302221298218, 58.35278034210205, 59.472538471221924, 60.54545211791992, 61.61836576461792, 62.68355965614319, 63.74875354766846, 64.80830812454224, 65.86786270141602, 66.89372515678406, 67.9195876121521, 69.03536581993103, 70.15114402770996, 71.20132946968079, 72.25151491165161, 73.31281447410583, 74.37411403656006, 75.48823285102844, 76.60235166549683, 77.78713130950928, 78.97191095352173, 80.03572988510132, 81.09954881668091, 82.16669511795044, 83.23384141921997, 84.2934000492096, 85.35295867919922, 86.44344019889832, 87.53392171859741, 88.63122153282166, 89.7285213470459, 90.74749660491943, 91.76647186279297, 92.84387183189392, 93.92127180099487, 95.01589488983154, 96.11051797866821, 97.22450637817383, 98.33849477767944, 99.50386142730713, 100.66922807693481, 101.70821642875671, 102.74720478057861, 103.8283178806305, 104.90943098068237, 106.1405999660492, 107.37176895141602, 108.48522448539734, 109.59868001937866, 110.57725358009338, 111.5558271408081, 112.58451986312866, 113.61321258544922, 114.6735475063324, 115.73388242721558, 116.82829546928406, 117.92270851135254, 119.01426792144775, 120.10582733154297, 121.16088724136353, 122.21594715118408, 123.24824404716492, 124.28054094314575, 125.37849545478821, 126.47644996643066, 127.58934497833252, 128.70223999023438, 129.80991768836975, 130.91759538650513, 131.9768307209015, 133.03606605529785, 134.13354301452637, 135.23101997375488, 136.3462061882019, 137.46139240264893, 138.52216386795044, 139.58293533325195, 140.64947271347046, 141.71601009368896, 142.7547607421875, 143.79351139068604, 144.8611671924591, 145.92882299423218, 147.00883603096008, 148.088849067688, 149.12974572181702, 150.17064237594604, 151.21062660217285, 152.25061082839966, 153.31525301933289, 154.3798952102661, 155.46372771263123, 156.54756021499634, 157.65326690673828, 158.75897359848022, 159.86094784736633, 160.96292209625244, 162.04820728302002, 163.1334924697876, 164.2283341884613, 165.323175907135, 166.44380974769592, 167.56444358825684, 168.66448545455933, 169.76452732086182, 170.84324049949646, 171.9219536781311, 172.9674048423767, 174.01285600662231, 175.05019879341125, 176.0875415802002, 177.08019185066223, 178.07284212112427, 179.05434370040894, 180.0358452796936, 181.01662254333496, 181.99739980697632, 182.9389100074768, 183.8804202079773, 184.85938596725464, 185.83835172653198, 186.86545658111572, 187.89256143569946, 188.90847826004028, 189.9243950843811, 190.85285925865173, 191.78132343292236, 192.78739190101624, 193.7934603691101, 194.83647680282593, 195.87949323654175, 196.8626925945282, 197.84589195251465, 198.84387755393982, 199.841863155365, 200.78817677497864, 201.73449039459229, 202.67895364761353, 203.62341690063477, 204.6202142238617, 205.61701154708862, 206.65031099319458, 207.68361043930054, 208.70673894882202, 209.7298674583435, 210.68770098686218, 211.64553451538086, 212.62820482254028, 213.6108751296997, 215.24676728248596, 216.88265943527222]
[10.016666666666667, 10.016666666666667, 10.275, 10.275, 10.258333333333333, 10.258333333333333, 10.158333333333333, 10.158333333333333, 10.6, 10.6, 11.483333333333333, 11.483333333333333, 12.508333333333333, 12.508333333333333, 13.558333333333334, 13.558333333333334, 14.25, 14.25, 14.941666666666666, 14.941666666666666, 16.041666666666668, 16.041666666666668, 17.35, 17.35, 17.708333333333332, 17.708333333333332, 18.141666666666666, 18.141666666666666, 16.7, 16.7, 16.975, 16.975, 19.866666666666667, 19.866666666666667, 22.475, 22.475, 23.416666666666668, 23.416666666666668, 25.816666666666666, 25.816666666666666, 30.566666666666666, 30.566666666666666, 34.358333333333334, 34.358333333333334, 36.09166666666667, 36.09166666666667, 39.95, 39.95, 42.65833333333333, 42.65833333333333, 45.38333333333333, 45.38333333333333, 46.666666666666664, 46.666666666666664, 48.266666666666666, 48.266666666666666, 50.34166666666667, 50.34166666666667, 52.94166666666667, 52.94166666666667, 53.40833333333333, 53.40833333333333, 55.40833333333333, 55.40833333333333, 55.708333333333336, 55.708333333333336, 56.06666666666667, 56.06666666666667, 56.225, 56.225, 56.40833333333333, 56.40833333333333, 56.583333333333336, 56.583333333333336, 57.141666666666666, 57.141666666666666, 57.766666666666666, 57.766666666666666, 58.1, 58.1, 58.45, 58.45, 58.90833333333333, 58.90833333333333, 59.03333333333333, 59.03333333333333, 59.2, 59.2, 59.275, 59.275, 59.766666666666666, 59.766666666666666, 59.88333333333333, 59.88333333333333, 59.95, 59.95, 59.925, 59.925, 59.84166666666667, 59.84166666666667, 59.90833333333333, 59.90833333333333, 60.141666666666666, 60.141666666666666, 60.30833333333333, 60.30833333333333, 60.425, 60.425, 60.43333333333333, 60.43333333333333, 60.38333333333333, 60.38333333333333, 60.43333333333333, 60.43333333333333, 61.13333333333333, 61.13333333333333, 61.24166666666667, 61.24166666666667, 61.13333333333333, 61.13333333333333, 61.19166666666667, 61.19166666666667, 61.233333333333334, 61.233333333333334, 61.21666666666667, 61.21666666666667, 61.266666666666666, 61.266666666666666, 61.291666666666664, 61.291666666666664, 61.30833333333333, 61.30833333333333, 61.575, 61.575, 61.416666666666664, 61.416666666666664, 61.575, 61.575, 61.575, 61.575, 61.65833333333333, 61.65833333333333, 61.625, 61.625, 61.78333333333333, 61.78333333333333, 61.75, 61.75, 61.75833333333333, 61.75833333333333, 61.733333333333334, 61.733333333333334, 61.84166666666667, 61.84166666666667, 61.916666666666664, 61.916666666666664, 61.88333333333333, 61.88333333333333, 61.916666666666664, 61.916666666666664, 61.94166666666667, 61.94166666666667, 62.225, 62.225, 62.291666666666664, 62.291666666666664, 62.233333333333334, 62.233333333333334, 62.358333333333334, 62.358333333333334, 62.375, 62.375, 62.375, 62.375, 62.45, 62.45, 62.53333333333333, 62.53333333333333, 62.45, 62.45, 62.458333333333336, 62.458333333333336, 62.525, 62.525, 62.516666666666666, 62.516666666666666, 62.708333333333336, 62.708333333333336, 62.74166666666667, 62.74166666666667, 62.65833333333333, 62.65833333333333, 63.025, 63.025, 63.0, 63.0, 63.016666666666666, 63.016666666666666, 63.025, 63.025, 63.175, 63.175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.303, Test accuracy: 10.62
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 12.01
Round   2, Train loss: 2.299, Test loss: 2.302, Test accuracy: 12.03
Round   3, Train loss: 2.299, Test loss: 2.301, Test accuracy: 12.33
Round   4, Train loss: 2.295, Test loss: 2.301, Test accuracy: 11.93
Round   5, Train loss: 2.293, Test loss: 2.299, Test accuracy: 13.73
Round   6, Train loss: 2.284, Test loss: 2.295, Test accuracy: 13.34
Round   7, Train loss: 2.267, Test loss: 2.288, Test accuracy: 16.11
Round   8, Train loss: 2.237, Test loss: 2.269, Test accuracy: 18.26
Round   9, Train loss: 2.165, Test loss: 2.230, Test accuracy: 23.12
Round  10, Train loss: 2.066, Test loss: 2.184, Test accuracy: 28.17
Round  11, Train loss: 2.008, Test loss: 2.132, Test accuracy: 33.22
Round  12, Train loss: 1.957, Test loss: 2.077, Test accuracy: 38.81
Round  13, Train loss: 1.902, Test loss: 2.033, Test accuracy: 43.13
Round  14, Train loss: 1.918, Test loss: 1.986, Test accuracy: 48.44
Round  15, Train loss: 1.930, Test loss: 1.962, Test accuracy: 50.73
Round  16, Train loss: 1.897, Test loss: 1.934, Test accuracy: 53.43
Round  17, Train loss: 1.883, Test loss: 1.911, Test accuracy: 55.44
Round  18, Train loss: 1.841, Test loss: 1.902, Test accuracy: 56.45
Round  19, Train loss: 1.856, Test loss: 1.894, Test accuracy: 57.34
Round  20, Train loss: 1.857, Test loss: 1.883, Test accuracy: 58.40
Round  21, Train loss: 1.814, Test loss: 1.871, Test accuracy: 59.58
Round  22, Train loss: 1.831, Test loss: 1.868, Test accuracy: 59.77
Round  23, Train loss: 1.804, Test loss: 1.857, Test accuracy: 60.79
Round  24, Train loss: 1.793, Test loss: 1.845, Test accuracy: 61.83
Round  25, Train loss: 1.831, Test loss: 1.841, Test accuracy: 62.48
Round  26, Train loss: 1.811, Test loss: 1.834, Test accuracy: 63.09
Round  27, Train loss: 1.774, Test loss: 1.831, Test accuracy: 63.38
Round  28, Train loss: 1.801, Test loss: 1.830, Test accuracy: 63.29
Round  29, Train loss: 1.770, Test loss: 1.824, Test accuracy: 64.01
Round  30, Train loss: 1.825, Test loss: 1.822, Test accuracy: 64.27
Round  31, Train loss: 1.733, Test loss: 1.819, Test accuracy: 64.34
Round  32, Train loss: 1.764, Test loss: 1.818, Test accuracy: 64.50
Round  33, Train loss: 1.775, Test loss: 1.817, Test accuracy: 64.58
Round  34, Train loss: 1.791, Test loss: 1.816, Test accuracy: 64.57
Round  35, Train loss: 1.768, Test loss: 1.811, Test accuracy: 65.17
Round  36, Train loss: 1.778, Test loss: 1.806, Test accuracy: 65.48
Round  37, Train loss: 1.791, Test loss: 1.804, Test accuracy: 65.71
Round  38, Train loss: 1.738, Test loss: 1.804, Test accuracy: 65.76
Round  39, Train loss: 1.724, Test loss: 1.802, Test accuracy: 65.93
Round  40, Train loss: 1.769, Test loss: 1.801, Test accuracy: 66.03
Round  41, Train loss: 1.726, Test loss: 1.800, Test accuracy: 66.09
Round  42, Train loss: 1.733, Test loss: 1.800, Test accuracy: 66.12
Round  43, Train loss: 1.740, Test loss: 1.799, Test accuracy: 66.08
Round  44, Train loss: 1.733, Test loss: 1.798, Test accuracy: 66.23
Round  45, Train loss: 1.749, Test loss: 1.798, Test accuracy: 66.31
Round  46, Train loss: 1.753, Test loss: 1.793, Test accuracy: 66.89
Round  47, Train loss: 1.746, Test loss: 1.792, Test accuracy: 67.03
Round  48, Train loss: 1.767, Test loss: 1.791, Test accuracy: 67.05
Round  49, Train loss: 1.777, Test loss: 1.791, Test accuracy: 67.01
Round  50, Train loss: 1.744, Test loss: 1.790, Test accuracy: 67.17
Round  51, Train loss: 1.782, Test loss: 1.790, Test accuracy: 67.22
Round  52, Train loss: 1.755, Test loss: 1.790, Test accuracy: 67.17
Round  53, Train loss: 1.750, Test loss: 1.790, Test accuracy: 67.17
Round  54, Train loss: 1.734, Test loss: 1.789, Test accuracy: 67.19
Round  55, Train loss: 1.790, Test loss: 1.787, Test accuracy: 67.47
Round  56, Train loss: 1.725, Test loss: 1.785, Test accuracy: 67.66
Round  57, Train loss: 1.760, Test loss: 1.784, Test accuracy: 67.74
Round  58, Train loss: 1.741, Test loss: 1.784, Test accuracy: 67.78
Round  59, Train loss: 1.752, Test loss: 1.784, Test accuracy: 67.78
Round  60, Train loss: 1.758, Test loss: 1.784, Test accuracy: 67.77
Round  61, Train loss: 1.734, Test loss: 1.783, Test accuracy: 67.92
Round  62, Train loss: 1.763, Test loss: 1.782, Test accuracy: 67.88
Round  63, Train loss: 1.751, Test loss: 1.782, Test accuracy: 67.84
Round  64, Train loss: 1.696, Test loss: 1.782, Test accuracy: 67.97
Round  65, Train loss: 1.735, Test loss: 1.782, Test accuracy: 67.97
Round  66, Train loss: 1.760, Test loss: 1.779, Test accuracy: 68.28
Round  67, Train loss: 1.725, Test loss: 1.778, Test accuracy: 68.37
Round  68, Train loss: 1.798, Test loss: 1.779, Test accuracy: 68.28
Round  69, Train loss: 1.758, Test loss: 1.779, Test accuracy: 68.17
Round  70, Train loss: 1.741, Test loss: 1.779, Test accuracy: 68.25
Round  71, Train loss: 1.733, Test loss: 1.779, Test accuracy: 68.21
Round  72, Train loss: 1.734, Test loss: 1.777, Test accuracy: 68.34
Round  73, Train loss: 1.765, Test loss: 1.776, Test accuracy: 68.52
Round  74, Train loss: 1.668, Test loss: 1.774, Test accuracy: 68.71
Round  75, Train loss: 1.709, Test loss: 1.773, Test accuracy: 68.80
Round  76, Train loss: 1.756, Test loss: 1.772, Test accuracy: 68.79
Round  77, Train loss: 1.701, Test loss: 1.772, Test accuracy: 68.88
Round  78, Train loss: 1.725, Test loss: 1.772, Test accuracy: 68.90
Round  79, Train loss: 1.698, Test loss: 1.772, Test accuracy: 68.88
Round  80, Train loss: 1.723, Test loss: 1.768, Test accuracy: 69.30
Round  81, Train loss: 1.696, Test loss: 1.767, Test accuracy: 69.39
Round  82, Train loss: 1.736, Test loss: 1.767, Test accuracy: 69.45
Round  83, Train loss: 1.720, Test loss: 1.766, Test accuracy: 69.59
Round  84, Train loss: 1.720, Test loss: 1.762, Test accuracy: 69.86
Round  85, Train loss: 1.738, Test loss: 1.762, Test accuracy: 69.83
Round  86, Train loss: 1.673, Test loss: 1.762, Test accuracy: 69.85
Round  87, Train loss: 1.703, Test loss: 1.762, Test accuracy: 69.86
Round  88, Train loss: 1.744, Test loss: 1.761, Test accuracy: 69.92
Round  89, Train loss: 1.689, Test loss: 1.761, Test accuracy: 69.92
Round  90, Train loss: 1.743, Test loss: 1.761, Test accuracy: 69.97
Round  91, Train loss: 1.694, Test loss: 1.761, Test accuracy: 70.00
Round  92, Train loss: 1.704, Test loss: 1.761, Test accuracy: 70.03
Round  93, Train loss: 1.746, Test loss: 1.760, Test accuracy: 70.04
Round  94, Train loss: 1.718, Test loss: 1.760, Test accuracy: 70.03
Round  95, Train loss: 1.702, Test loss: 1.760, Test accuracy: 70.12
Round  96, Train loss: 1.710, Test loss: 1.760, Test accuracy: 70.09
Round  97, Train loss: 1.745, Test loss: 1.760, Test accuracy: 70.08
Round  98, Train loss: 1.687, Test loss: 1.759, Test accuracy: 70.15/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.729, Test loss: 1.760, Test accuracy: 70.14
Final Round, Train loss: 1.712, Test loss: 1.753, Test accuracy: 70.76
Average accuracy final 10 rounds: 70.065 

1411.3310551643372
[1.2434194087982178, 2.4868388175964355, 3.6330347061157227, 4.77923059463501, 5.94815731048584, 7.11708402633667, 8.28020715713501, 9.44333028793335, 10.59785008430481, 11.75236988067627, 13.01192593574524, 14.271481990814209, 15.427062034606934, 16.582642078399658, 17.76407480239868, 18.945507526397705, 20.011295318603516, 21.077083110809326, 22.039695978164673, 23.00230884552002, 24.03781032562256, 25.073311805725098, 26.132901906967163, 27.19249200820923, 28.22310972213745, 29.253727436065674, 30.26509714126587, 31.276466846466064, 32.275551557540894, 33.27463626861572, 34.28298997879028, 35.291343688964844, 36.35448122024536, 37.41761875152588, 38.477121114730835, 39.53662347793579, 40.56234622001648, 41.58806896209717, 42.62438893318176, 43.66070890426636, 44.72260046005249, 45.78449201583862, 46.78482460975647, 47.785157203674316, 48.837958335876465, 49.89075946807861, 50.895864725112915, 51.90096998214722, 52.89484643936157, 53.88872289657593, 54.955222606658936, 56.02172231674194, 57.02206373214722, 58.02240514755249, 59.0744104385376, 60.126415729522705, 61.183475971221924, 62.24053621292114, 63.25527310371399, 64.27000999450684, 65.3289155960083, 66.38782119750977, 67.43613481521606, 68.48444843292236, 69.52557468414307, 70.56670093536377, 71.59034967422485, 72.61399841308594, 73.61457228660583, 74.61514616012573, 75.68397569656372, 76.75280523300171, 77.79640769958496, 78.84001016616821, 79.83878588676453, 80.83756160736084, 81.87647700309753, 82.91539239883423, 83.95816159248352, 85.00093078613281, 86.04325461387634, 87.08557844161987, 88.14171481132507, 89.19785118103027, 90.20716667175293, 91.21648216247559, 92.30107355117798, 93.38566493988037, 94.45029211044312, 95.51491928100586, 96.52743530273438, 97.53995132446289, 98.57233119010925, 99.60471105575562, 100.67427158355713, 101.74383211135864, 102.7764527797699, 103.80907344818115, 104.88765621185303, 105.9662389755249, 107.03574442863464, 108.10524988174438, 109.13321137428284, 110.16117286682129, 111.15287446975708, 112.14457607269287, 113.19066596031189, 114.23675584793091, 115.33257007598877, 116.42838430404663, 117.4433662891388, 118.45834827423096, 119.54473757743835, 120.63112688064575, 121.7090117931366, 122.78689670562744, 123.83207988739014, 124.87726306915283, 125.88171815872192, 126.88617324829102, 127.95266127586365, 129.01914930343628, 130.07614254951477, 131.13313579559326, 132.17159223556519, 133.2100486755371, 134.2867259979248, 135.3634033203125, 136.4076225757599, 137.45184183120728, 138.49465990066528, 139.5374779701233, 140.58067727088928, 141.62387657165527, 142.71184611320496, 143.79981565475464, 144.8951804637909, 145.99054527282715, 147.01340293884277, 148.0362606048584, 149.0990834236145, 150.1619062423706, 151.23397183418274, 152.30603742599487, 153.34991478919983, 154.39379215240479, 155.44854307174683, 156.50329399108887, 157.61710262298584, 158.7309112548828, 159.84610056877136, 160.9612898826599, 162.0606415271759, 163.1599931716919, 164.25751757621765, 165.3550419807434, 166.4311420917511, 167.5072422027588, 168.57439827919006, 169.64155435562134, 170.65431475639343, 171.66707515716553, 172.69915914535522, 173.73124313354492, 174.75571274757385, 175.78018236160278, 176.77702379226685, 177.7738652229309, 178.813072681427, 179.8522801399231, 180.9103889465332, 181.9684977531433, 183.01368021965027, 184.05886268615723, 185.0916690826416, 186.12447547912598, 187.16099071502686, 188.19750595092773, 189.20050024986267, 190.2034945487976, 191.22012901306152, 192.23676347732544, 193.28760647773743, 194.3384494781494, 195.33851766586304, 196.33858585357666, 197.38899421691895, 198.43940258026123, 199.47675704956055, 200.51411151885986, 201.52437949180603, 202.5346474647522, 203.5060260295868, 204.4774045944214, 205.48436641693115, 206.49132823944092, 207.5139262676239, 208.53652429580688, 209.57773971557617, 210.61895513534546, 212.25915622711182, 213.89935731887817]
[10.616666666666667, 10.616666666666667, 12.008333333333333, 12.008333333333333, 12.033333333333333, 12.033333333333333, 12.333333333333334, 12.333333333333334, 11.933333333333334, 11.933333333333334, 13.733333333333333, 13.733333333333333, 13.341666666666667, 13.341666666666667, 16.108333333333334, 16.108333333333334, 18.258333333333333, 18.258333333333333, 23.125, 23.125, 28.166666666666668, 28.166666666666668, 33.21666666666667, 33.21666666666667, 38.80833333333333, 38.80833333333333, 43.13333333333333, 43.13333333333333, 48.44166666666667, 48.44166666666667, 50.733333333333334, 50.733333333333334, 53.43333333333333, 53.43333333333333, 55.44166666666667, 55.44166666666667, 56.45, 56.45, 57.34166666666667, 57.34166666666667, 58.4, 58.4, 59.575, 59.575, 59.766666666666666, 59.766666666666666, 60.791666666666664, 60.791666666666664, 61.833333333333336, 61.833333333333336, 62.483333333333334, 62.483333333333334, 63.09166666666667, 63.09166666666667, 63.38333333333333, 63.38333333333333, 63.291666666666664, 63.291666666666664, 64.00833333333334, 64.00833333333334, 64.26666666666667, 64.26666666666667, 64.34166666666667, 64.34166666666667, 64.5, 64.5, 64.58333333333333, 64.58333333333333, 64.56666666666666, 64.56666666666666, 65.175, 65.175, 65.48333333333333, 65.48333333333333, 65.70833333333333, 65.70833333333333, 65.75833333333334, 65.75833333333334, 65.93333333333334, 65.93333333333334, 66.025, 66.025, 66.09166666666667, 66.09166666666667, 66.125, 66.125, 66.075, 66.075, 66.23333333333333, 66.23333333333333, 66.30833333333334, 66.30833333333334, 66.89166666666667, 66.89166666666667, 67.03333333333333, 67.03333333333333, 67.05, 67.05, 67.00833333333334, 67.00833333333334, 67.175, 67.175, 67.225, 67.225, 67.175, 67.175, 67.16666666666667, 67.16666666666667, 67.19166666666666, 67.19166666666666, 67.46666666666667, 67.46666666666667, 67.65833333333333, 67.65833333333333, 67.74166666666666, 67.74166666666666, 67.775, 67.775, 67.78333333333333, 67.78333333333333, 67.76666666666667, 67.76666666666667, 67.925, 67.925, 67.88333333333334, 67.88333333333334, 67.84166666666667, 67.84166666666667, 67.96666666666667, 67.96666666666667, 67.96666666666667, 67.96666666666667, 68.275, 68.275, 68.36666666666666, 68.36666666666666, 68.275, 68.275, 68.175, 68.175, 68.25, 68.25, 68.20833333333333, 68.20833333333333, 68.34166666666667, 68.34166666666667, 68.51666666666667, 68.51666666666667, 68.70833333333333, 68.70833333333333, 68.8, 68.8, 68.79166666666667, 68.79166666666667, 68.88333333333334, 68.88333333333334, 68.9, 68.9, 68.875, 68.875, 69.3, 69.3, 69.39166666666667, 69.39166666666667, 69.45, 69.45, 69.59166666666667, 69.59166666666667, 69.85833333333333, 69.85833333333333, 69.83333333333333, 69.83333333333333, 69.85, 69.85, 69.85833333333333, 69.85833333333333, 69.925, 69.925, 69.925, 69.925, 69.975, 69.975, 70.0, 70.0, 70.025, 70.025, 70.04166666666667, 70.04166666666667, 70.025, 70.025, 70.11666666666666, 70.11666666666666, 70.09166666666667, 70.09166666666667, 70.08333333333333, 70.08333333333333, 70.15, 70.15, 70.14166666666667, 70.14166666666667, 70.75833333333334, 70.75833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.72
Round   1, Train loss: 2.300, Test loss: 2.301, Test accuracy: 14.75
Round   2, Train loss: 2.296, Test loss: 2.299, Test accuracy: 20.36
Round   3, Train loss: 2.298, Test loss: 2.296, Test accuracy: 23.12
Round   4, Train loss: 2.285, Test loss: 2.290, Test accuracy: 23.07
Round   5, Train loss: 2.270, Test loss: 2.277, Test accuracy: 19.75
Round   6, Train loss: 2.249, Test loss: 2.254, Test accuracy: 20.85
Round   7, Train loss: 2.215, Test loss: 2.211, Test accuracy: 27.61
Round   8, Train loss: 2.109, Test loss: 2.146, Test accuracy: 35.69
Round   9, Train loss: 1.973, Test loss: 2.079, Test accuracy: 45.23
Round  10, Train loss: 1.916, Test loss: 2.006, Test accuracy: 51.13
Round  11, Train loss: 1.842, Test loss: 1.943, Test accuracy: 57.56
Round  12, Train loss: 1.715, Test loss: 1.895, Test accuracy: 61.35
Round  13, Train loss: 1.765, Test loss: 1.832, Test accuracy: 67.48
Round  14, Train loss: 1.656, Test loss: 1.782, Test accuracy: 72.02
Round  15, Train loss: 1.640, Test loss: 1.747, Test accuracy: 75.03
Round  16, Train loss: 1.564, Test loss: 1.731, Test accuracy: 76.21
Round  17, Train loss: 1.573, Test loss: 1.718, Test accuracy: 77.27
Round  18, Train loss: 1.609, Test loss: 1.676, Test accuracy: 80.65
Round  19, Train loss: 1.532, Test loss: 1.668, Test accuracy: 81.35
Round  20, Train loss: 1.535, Test loss: 1.662, Test accuracy: 81.74
Round  21, Train loss: 1.526, Test loss: 1.658, Test accuracy: 81.89
Round  22, Train loss: 1.540, Test loss: 1.655, Test accuracy: 82.17
Round  23, Train loss: 1.512, Test loss: 1.648, Test accuracy: 82.69
Round  24, Train loss: 1.538, Test loss: 1.641, Test accuracy: 83.25
Round  25, Train loss: 1.540, Test loss: 1.633, Test accuracy: 84.32
Round  26, Train loss: 1.525, Test loss: 1.628, Test accuracy: 84.65
Round  27, Train loss: 1.495, Test loss: 1.626, Test accuracy: 84.76
Round  28, Train loss: 1.504, Test loss: 1.623, Test accuracy: 85.03
Round  29, Train loss: 1.489, Test loss: 1.622, Test accuracy: 85.10
Round  30, Train loss: 1.495, Test loss: 1.619, Test accuracy: 85.26
Round  31, Train loss: 1.483, Test loss: 1.618, Test accuracy: 85.34
Round  32, Train loss: 1.492, Test loss: 1.615, Test accuracy: 85.73
Round  33, Train loss: 1.486, Test loss: 1.614, Test accuracy: 85.85
Round  34, Train loss: 1.503, Test loss: 1.613, Test accuracy: 85.97
Round  35, Train loss: 1.486, Test loss: 1.611, Test accuracy: 86.10
Round  36, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.97
Round  37, Train loss: 1.498, Test loss: 1.610, Test accuracy: 86.12
Round  38, Train loss: 1.498, Test loss: 1.610, Test accuracy: 86.16
Round  39, Train loss: 1.486, Test loss: 1.609, Test accuracy: 86.15
Round  40, Train loss: 1.486, Test loss: 1.609, Test accuracy: 86.18
Round  41, Train loss: 1.481, Test loss: 1.609, Test accuracy: 86.21
Round  42, Train loss: 1.484, Test loss: 1.609, Test accuracy: 86.29
Round  43, Train loss: 1.481, Test loss: 1.608, Test accuracy: 86.22
Round  44, Train loss: 1.484, Test loss: 1.608, Test accuracy: 86.25
Round  45, Train loss: 1.482, Test loss: 1.608, Test accuracy: 86.19
Round  46, Train loss: 1.479, Test loss: 1.608, Test accuracy: 86.17
Round  47, Train loss: 1.480, Test loss: 1.608, Test accuracy: 86.20
Round  48, Train loss: 1.498, Test loss: 1.608, Test accuracy: 86.13
Round  49, Train loss: 1.483, Test loss: 1.608, Test accuracy: 86.11
Round  50, Train loss: 1.479, Test loss: 1.607, Test accuracy: 86.17
Round  51, Train loss: 1.483, Test loss: 1.607, Test accuracy: 86.12
Round  52, Train loss: 1.480, Test loss: 1.607, Test accuracy: 86.19
Round  53, Train loss: 1.496, Test loss: 1.607, Test accuracy: 86.17
Round  54, Train loss: 1.498, Test loss: 1.607, Test accuracy: 86.17
Round  55, Train loss: 1.479, Test loss: 1.607, Test accuracy: 86.13
Round  56, Train loss: 1.481, Test loss: 1.607, Test accuracy: 86.14
Round  57, Train loss: 1.482, Test loss: 1.607, Test accuracy: 86.12
Round  58, Train loss: 1.479, Test loss: 1.607, Test accuracy: 86.13
Round  59, Train loss: 1.482, Test loss: 1.607, Test accuracy: 86.06
Round  60, Train loss: 1.480, Test loss: 1.606, Test accuracy: 86.08
Round  61, Train loss: 1.477, Test loss: 1.606, Test accuracy: 86.07
Round  62, Train loss: 1.477, Test loss: 1.606, Test accuracy: 86.11
Round  63, Train loss: 1.480, Test loss: 1.606, Test accuracy: 86.11
Round  64, Train loss: 1.498, Test loss: 1.606, Test accuracy: 86.16
Round  65, Train loss: 1.496, Test loss: 1.606, Test accuracy: 86.17
Round  66, Train loss: 1.474, Test loss: 1.606, Test accuracy: 86.11
Round  67, Train loss: 1.476, Test loss: 1.606, Test accuracy: 86.12
Round  68, Train loss: 1.499, Test loss: 1.606, Test accuracy: 86.05
Round  69, Train loss: 1.480, Test loss: 1.606, Test accuracy: 86.09
Round  70, Train loss: 1.481, Test loss: 1.606, Test accuracy: 86.07
Round  71, Train loss: 1.475, Test loss: 1.606, Test accuracy: 86.09
Round  72, Train loss: 1.476, Test loss: 1.606, Test accuracy: 86.12
Round  73, Train loss: 1.479, Test loss: 1.606, Test accuracy: 86.15
Round  74, Train loss: 1.474, Test loss: 1.606, Test accuracy: 86.14
Round  75, Train loss: 1.482, Test loss: 1.605, Test accuracy: 86.17
Round  76, Train loss: 1.499, Test loss: 1.605, Test accuracy: 86.20
Round  77, Train loss: 1.475, Test loss: 1.605, Test accuracy: 86.18
Round  78, Train loss: 1.497, Test loss: 1.605, Test accuracy: 86.18
Round  79, Train loss: 1.495, Test loss: 1.605, Test accuracy: 86.17
Round  80, Train loss: 1.481, Test loss: 1.605, Test accuracy: 86.15
Round  81, Train loss: 1.478, Test loss: 1.605, Test accuracy: 86.22
Round  82, Train loss: 1.475, Test loss: 1.605, Test accuracy: 86.22
Round  83, Train loss: 1.499, Test loss: 1.605, Test accuracy: 86.22
Round  84, Train loss: 1.497, Test loss: 1.605, Test accuracy: 86.22
Round  85, Train loss: 1.483, Test loss: 1.605, Test accuracy: 86.27
Round  86, Train loss: 1.490, Test loss: 1.605, Test accuracy: 86.25
Round  87, Train loss: 1.493, Test loss: 1.605, Test accuracy: 86.28
Round  88, Train loss: 1.479, Test loss: 1.605, Test accuracy: 86.28
Round  89, Train loss: 1.495, Test loss: 1.605, Test accuracy: 86.30
Round  90, Train loss: 1.473, Test loss: 1.605, Test accuracy: 86.30
Round  91, Train loss: 1.476, Test loss: 1.605, Test accuracy: 86.27
Round  92, Train loss: 1.479, Test loss: 1.605, Test accuracy: 86.27
Round  93, Train loss: 1.478, Test loss: 1.605, Test accuracy: 86.25
Round  94, Train loss: 1.475, Test loss: 1.605, Test accuracy: 86.31
Round  95, Train loss: 1.495, Test loss: 1.604, Test accuracy: 86.31
Round  96, Train loss: 1.475, Test loss: 1.604, Test accuracy: 86.31
Round  97, Train loss: 1.476, Test loss: 1.604, Test accuracy: 86.28
Round  98, Train loss: 1.491, Test loss: 1.604, Test accuracy: 86.29
Round  99, Train loss: 1.480, Test loss: 1.604, Test accuracy: 86.29/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.483, Test loss: 1.604, Test accuracy: 86.27
Average accuracy final 10 rounds: 86.2875 

1519.20134806633
[1.228949785232544, 2.457899570465088, 3.6858813762664795, 4.913863182067871, 6.1386024951934814, 7.363341808319092, 8.577703952789307, 9.792066097259521, 10.982769250869751, 12.17347240447998, 13.38670301437378, 14.599933624267578, 15.839403867721558, 17.078874111175537, 18.346723079681396, 19.614572048187256, 20.827849864959717, 22.041127681732178, 23.23894953727722, 24.436771392822266, 25.700887203216553, 26.96500301361084, 28.208917140960693, 29.452831268310547, 30.670962810516357, 31.889094352722168, 33.1030387878418, 34.316983222961426, 35.588298082351685, 36.85961294174194, 38.08071732521057, 39.3018217086792, 40.4174587726593, 41.533095836639404, 42.66562366485596, 43.79815149307251, 45.01783895492554, 46.237526416778564, 47.42383408546448, 48.61014175415039, 49.83614540100098, 51.06214904785156, 52.32585835456848, 53.5895676612854, 54.84317684173584, 56.09678602218628, 57.31841206550598, 58.540038108825684, 59.752925872802734, 60.965813636779785, 62.18910264968872, 63.412391662597656, 64.67216897010803, 65.93194627761841, 67.14805436134338, 68.36416244506836, 69.53237342834473, 70.7005844116211, 71.88545203208923, 73.07031965255737, 74.27493238449097, 75.47954511642456, 76.67761540412903, 77.8756856918335, 79.06944704055786, 80.26320838928223, 81.48451972007751, 82.7058310508728, 83.98406600952148, 85.26230096817017, 86.49910187721252, 87.73590278625488, 88.94040179252625, 90.14490079879761, 91.36187028884888, 92.57883977890015, 93.82311081886292, 95.06738185882568, 96.32632946968079, 97.58527708053589, 98.79288506507874, 100.00049304962158, 101.25661134719849, 102.51272964477539, 103.78309774398804, 105.05346584320068, 106.35785961151123, 107.66225337982178, 108.86090755462646, 110.05956172943115, 111.241872549057, 112.42418336868286, 113.64359068870544, 114.86299800872803, 116.09699606895447, 117.33099412918091, 118.55250263214111, 119.77401113510132, 121.01903438568115, 122.26405763626099, 123.53332233428955, 124.80258703231812, 126.04976320266724, 127.29693937301636, 128.5220239162445, 129.74710845947266, 130.96543407440186, 132.18375968933105, 133.3896713256836, 134.59558296203613, 135.87615275382996, 137.15672254562378, 138.41773438453674, 139.6787462234497, 140.9012405872345, 142.1237349510193, 143.33013582229614, 144.536536693573, 145.7555000782013, 146.9744634628296, 148.33191871643066, 149.68937397003174, 151.01310443878174, 152.33683490753174, 153.64590501785278, 154.95497512817383, 156.16961121559143, 157.38424730300903, 158.63375854492188, 159.88326978683472, 161.13818788528442, 162.39310598373413, 163.58611416816711, 164.7791223526001, 165.9704031944275, 167.16168403625488, 168.38344955444336, 169.60521507263184, 170.83307194709778, 172.06092882156372, 173.28875160217285, 174.51657438278198, 175.72827577590942, 176.93997716903687, 178.16180443763733, 179.3836317062378, 180.6149172782898, 181.8462028503418, 183.0815098285675, 184.3168168067932, 185.51855969429016, 186.7203025817871, 187.9799201488495, 189.23953771591187, 190.63514375686646, 192.03074979782104, 193.36900353431702, 194.707257270813, 196.07885646820068, 197.45045566558838, 198.7171437740326, 199.9838318824768, 201.2842674255371, 202.5847029685974, 203.87075567245483, 205.15680837631226, 206.33253645896912, 207.50826454162598, 208.65892481803894, 209.8095850944519, 210.83885312080383, 211.86812114715576, 212.9787769317627, 214.08943271636963, 215.12330746650696, 216.1571822166443, 217.29195380210876, 218.42672538757324, 219.57131910324097, 220.7159128189087, 221.92386078834534, 223.13180875778198, 224.32290768623352, 225.51400661468506, 226.7128038406372, 227.91160106658936, 229.06770396232605, 230.22380685806274, 231.3664402961731, 232.50907373428345, 233.67693519592285, 234.84479665756226, 236.00303769111633, 237.1612787246704, 238.3420135974884, 239.5227484703064, 240.69009590148926, 241.85744333267212, 243.00533175468445, 244.15322017669678, 246.00859832763672, 247.86397647857666]
[10.725, 10.725, 14.75, 14.75, 20.358333333333334, 20.358333333333334, 23.125, 23.125, 23.075, 23.075, 19.75, 19.75, 20.85, 20.85, 27.608333333333334, 27.608333333333334, 35.69166666666667, 35.69166666666667, 45.233333333333334, 45.233333333333334, 51.13333333333333, 51.13333333333333, 57.55833333333333, 57.55833333333333, 61.35, 61.35, 67.48333333333333, 67.48333333333333, 72.01666666666667, 72.01666666666667, 75.03333333333333, 75.03333333333333, 76.20833333333333, 76.20833333333333, 77.26666666666667, 77.26666666666667, 80.65, 80.65, 81.35, 81.35, 81.74166666666666, 81.74166666666666, 81.89166666666667, 81.89166666666667, 82.175, 82.175, 82.69166666666666, 82.69166666666666, 83.25, 83.25, 84.31666666666666, 84.31666666666666, 84.65, 84.65, 84.75833333333334, 84.75833333333334, 85.03333333333333, 85.03333333333333, 85.1, 85.1, 85.25833333333334, 85.25833333333334, 85.34166666666667, 85.34166666666667, 85.73333333333333, 85.73333333333333, 85.85, 85.85, 85.975, 85.975, 86.1, 86.1, 85.975, 85.975, 86.11666666666666, 86.11666666666666, 86.15833333333333, 86.15833333333333, 86.15, 86.15, 86.18333333333334, 86.18333333333334, 86.20833333333333, 86.20833333333333, 86.29166666666667, 86.29166666666667, 86.21666666666667, 86.21666666666667, 86.25, 86.25, 86.19166666666666, 86.19166666666666, 86.16666666666667, 86.16666666666667, 86.2, 86.2, 86.13333333333334, 86.13333333333334, 86.10833333333333, 86.10833333333333, 86.16666666666667, 86.16666666666667, 86.125, 86.125, 86.19166666666666, 86.19166666666666, 86.16666666666667, 86.16666666666667, 86.16666666666667, 86.16666666666667, 86.13333333333334, 86.13333333333334, 86.14166666666667, 86.14166666666667, 86.125, 86.125, 86.13333333333334, 86.13333333333334, 86.05833333333334, 86.05833333333334, 86.08333333333333, 86.08333333333333, 86.06666666666666, 86.06666666666666, 86.10833333333333, 86.10833333333333, 86.10833333333333, 86.10833333333333, 86.15833333333333, 86.15833333333333, 86.16666666666667, 86.16666666666667, 86.10833333333333, 86.10833333333333, 86.11666666666666, 86.11666666666666, 86.05, 86.05, 86.09166666666667, 86.09166666666667, 86.06666666666666, 86.06666666666666, 86.09166666666667, 86.09166666666667, 86.125, 86.125, 86.15, 86.15, 86.14166666666667, 86.14166666666667, 86.175, 86.175, 86.2, 86.2, 86.18333333333334, 86.18333333333334, 86.18333333333334, 86.18333333333334, 86.16666666666667, 86.16666666666667, 86.15, 86.15, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.21666666666667, 86.26666666666667, 86.26666666666667, 86.25, 86.25, 86.28333333333333, 86.28333333333333, 86.275, 86.275, 86.3, 86.3, 86.3, 86.3, 86.26666666666667, 86.26666666666667, 86.26666666666667, 86.26666666666667, 86.25, 86.25, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.30833333333334, 86.28333333333333, 86.28333333333333, 86.29166666666667, 86.29166666666667, 86.29166666666667, 86.29166666666667, 86.26666666666667, 86.26666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.703, Test loss: 2.299, Test accuracy: 16.58
Round   1, Train loss: 1.668, Test loss: 2.290, Test accuracy: 25.09
Round   2, Train loss: 1.547, Test loss: 2.270, Test accuracy: 31.14
Round   3, Train loss: 1.483, Test loss: 2.236, Test accuracy: 39.77
Round   4, Train loss: 1.506, Test loss: 2.199, Test accuracy: 45.83
Round   5, Train loss: 1.445, Test loss: 2.153, Test accuracy: 49.41
Round   6, Train loss: 1.377, Test loss: 2.118, Test accuracy: 50.36
Round   7, Train loss: 1.413, Test loss: 2.094, Test accuracy: 51.16
Round   8, Train loss: 1.337, Test loss: 2.076, Test accuracy: 50.98
Round   9, Train loss: 1.370, Test loss: 2.058, Test accuracy: 51.63
Round  10, Train loss: 1.394, Test loss: 2.046, Test accuracy: 52.41
Round  11, Train loss: 1.366, Test loss: 2.037, Test accuracy: 52.89
Round  12, Train loss: 1.378, Test loss: 2.037, Test accuracy: 51.93
Round  13, Train loss: 1.329, Test loss: 2.026, Test accuracy: 52.29
Round  14, Train loss: 1.313, Test loss: 2.020, Test accuracy: 52.97
Round  15, Train loss: 1.437, Test loss: 2.009, Test accuracy: 53.19
Round  16, Train loss: 1.303, Test loss: 2.009, Test accuracy: 52.55
Round  17, Train loss: 1.312, Test loss: 2.002, Test accuracy: 52.87
Round  18, Train loss: 1.320, Test loss: 1.990, Test accuracy: 53.74
Round  19, Train loss: 1.302, Test loss: 1.984, Test accuracy: 54.42
Round  20, Train loss: 1.312, Test loss: 1.980, Test accuracy: 54.27
Round  21, Train loss: 1.363, Test loss: 1.970, Test accuracy: 55.86
Round  22, Train loss: 1.333, Test loss: 1.965, Test accuracy: 56.39
Round  23, Train loss: 1.296, Test loss: 1.958, Test accuracy: 56.50
Round  24, Train loss: 1.287, Test loss: 1.958, Test accuracy: 56.46
Round  25, Train loss: 1.327, Test loss: 1.958, Test accuracy: 55.38
Round  26, Train loss: 1.347, Test loss: 1.958, Test accuracy: 55.34
Round  27, Train loss: 1.338, Test loss: 1.958, Test accuracy: 55.02
Round  28, Train loss: 1.300, Test loss: 1.957, Test accuracy: 55.06
Round  29, Train loss: 1.281, Test loss: 1.959, Test accuracy: 54.58
Round  30, Train loss: 1.284, Test loss: 1.955, Test accuracy: 54.93
Round  31, Train loss: 1.352, Test loss: 1.950, Test accuracy: 55.86
Round  32, Train loss: 1.312, Test loss: 1.951, Test accuracy: 55.29
Round  33, Train loss: 1.288, Test loss: 1.951, Test accuracy: 55.23
Round  34, Train loss: 1.316, Test loss: 1.946, Test accuracy: 55.73
Round  35, Train loss: 1.335, Test loss: 1.946, Test accuracy: 55.58
Round  36, Train loss: 1.337, Test loss: 1.948, Test accuracy: 55.08
Round  37, Train loss: 1.272, Test loss: 1.947, Test accuracy: 55.07
Round  38, Train loss: 1.331, Test loss: 1.947, Test accuracy: 54.92
Round  39, Train loss: 1.302, Test loss: 1.943, Test accuracy: 55.61
Round  40, Train loss: 1.299, Test loss: 1.942, Test accuracy: 55.48
Round  41, Train loss: 1.352, Test loss: 1.940, Test accuracy: 55.59
Round  42, Train loss: 1.229, Test loss: 1.937, Test accuracy: 55.80
Round  43, Train loss: 1.292, Test loss: 1.937, Test accuracy: 55.99
Round  44, Train loss: 1.275, Test loss: 1.935, Test accuracy: 55.78
Round  45, Train loss: 1.280, Test loss: 1.936, Test accuracy: 55.50
Round  46, Train loss: 1.329, Test loss: 1.936, Test accuracy: 55.38
Round  47, Train loss: 1.304, Test loss: 1.939, Test accuracy: 54.95
Round  48, Train loss: 1.332, Test loss: 1.941, Test accuracy: 54.63
Round  49, Train loss: 1.360, Test loss: 1.944, Test accuracy: 54.42
Round  50, Train loss: 1.304, Test loss: 1.941, Test accuracy: 54.61
Round  51, Train loss: 1.318, Test loss: 1.940, Test accuracy: 54.64
Round  52, Train loss: 1.287, Test loss: 1.940, Test accuracy: 54.62
Round  53, Train loss: 1.277, Test loss: 1.941, Test accuracy: 54.36
Round  54, Train loss: 1.300, Test loss: 1.942, Test accuracy: 54.26
Round  55, Train loss: 1.370, Test loss: 1.944, Test accuracy: 54.13
Round  56, Train loss: 1.278, Test loss: 1.947, Test accuracy: 53.72
Round  57, Train loss: 1.334, Test loss: 1.947, Test accuracy: 53.63
Round  58, Train loss: 1.318, Test loss: 1.949, Test accuracy: 53.23
Round  59, Train loss: 1.298, Test loss: 1.946, Test accuracy: 53.52
Round  60, Train loss: 1.240, Test loss: 1.946, Test accuracy: 53.55
Round  61, Train loss: 1.330, Test loss: 1.949, Test accuracy: 53.27
Round  62, Train loss: 1.312, Test loss: 1.952, Test accuracy: 52.73
Round  63, Train loss: 1.308, Test loss: 1.953, Test accuracy: 52.58
Round  64, Train loss: 1.276, Test loss: 1.955, Test accuracy: 52.11
Round  65, Train loss: 1.318, Test loss: 1.956, Test accuracy: 51.98
Round  66, Train loss: 1.297, Test loss: 1.957, Test accuracy: 51.61
Round  67, Train loss: 1.302, Test loss: 1.958, Test accuracy: 51.60
Round  68, Train loss: 1.255, Test loss: 1.958, Test accuracy: 51.42
Round  69, Train loss: 1.270, Test loss: 1.959, Test accuracy: 51.33
Round  70, Train loss: 1.313, Test loss: 1.960, Test accuracy: 51.18
Round  71, Train loss: 1.273, Test loss: 1.960, Test accuracy: 51.25
Round  72, Train loss: 1.256, Test loss: 1.961, Test accuracy: 50.90
Round  73, Train loss: 1.285, Test loss: 1.963, Test accuracy: 50.64
Round  74, Train loss: 1.279, Test loss: 1.963, Test accuracy: 50.66
Round  75, Train loss: 1.307, Test loss: 1.964, Test accuracy: 50.51
Round  76, Train loss: 1.278, Test loss: 1.964, Test accuracy: 50.25
Round  77, Train loss: 1.277, Test loss: 1.968, Test accuracy: 49.90
Round  78, Train loss: 1.297, Test loss: 1.969, Test accuracy: 49.75
Round  79, Train loss: 1.296, Test loss: 1.972, Test accuracy: 49.33
Round  80, Train loss: 1.265, Test loss: 1.968, Test accuracy: 49.74
Round  81, Train loss: 1.261, Test loss: 1.968, Test accuracy: 49.83
Round  82, Train loss: 1.321, Test loss: 1.967, Test accuracy: 49.82
Round  83, Train loss: 1.297, Test loss: 1.967, Test accuracy: 49.93
Round  84, Train loss: 1.253, Test loss: 1.966, Test accuracy: 50.02
Round  85, Train loss: 1.238, Test loss: 1.972, Test accuracy: 49.30
Round  86, Train loss: 1.254, Test loss: 1.970, Test accuracy: 49.45
Round  87, Train loss: 1.346, Test loss: 1.972, Test accuracy: 49.26
Round  88, Train loss: 1.271, Test loss: 1.972, Test accuracy: 49.31
Round  89, Train loss: 1.252, Test loss: 1.972, Test accuracy: 49.16
Round  90, Train loss: 1.237, Test loss: 1.971, Test accuracy: 49.25
Round  91, Train loss: 1.311, Test loss: 1.974, Test accuracy: 48.75
Round  92, Train loss: 1.275, Test loss: 1.975, Test accuracy: 48.57
Round  93, Train loss: 1.297, Test loss: 1.978, Test accuracy: 48.20
Round  94, Train loss: 1.275, Test loss: 1.977, Test accuracy: 48.53
Round  95, Train loss: 1.274, Test loss: 1.977, Test accuracy: 48.42
Round  96, Train loss: 1.344, Test loss: 1.978, Test accuracy: 48.17
Round  97, Train loss: 1.353, Test loss: 1.981, Test accuracy: 47.83
Round  98, Train loss: 1.296, Test loss: 1.981, Test accuracy: 47.85
Round  99, Train loss: 1.305, Test loss: 1.983, Test accuracy: 47.55
Final Round, Train loss: 1.277, Test loss: 1.985, Test accuracy: 47.46
Average accuracy final 10 rounds: 48.31083333333333
1727.8235869407654
[]
[16.583333333333332, 25.091666666666665, 31.141666666666666, 39.775, 45.825, 49.40833333333333, 50.358333333333334, 51.15833333333333, 50.983333333333334, 51.63333333333333, 52.40833333333333, 52.891666666666666, 51.93333333333333, 52.291666666666664, 52.96666666666667, 53.19166666666667, 52.55, 52.86666666666667, 53.74166666666667, 54.425, 54.275, 55.858333333333334, 56.391666666666666, 56.5, 56.458333333333336, 55.375, 55.34166666666667, 55.025, 55.05833333333333, 54.583333333333336, 54.93333333333333, 55.858333333333334, 55.291666666666664, 55.233333333333334, 55.733333333333334, 55.583333333333336, 55.075, 55.06666666666667, 54.916666666666664, 55.608333333333334, 55.475, 55.59166666666667, 55.8, 55.99166666666667, 55.78333333333333, 55.5, 55.375, 54.95, 54.63333333333333, 54.425, 54.608333333333334, 54.641666666666666, 54.625, 54.358333333333334, 54.25833333333333, 54.13333333333333, 53.71666666666667, 53.63333333333333, 53.233333333333334, 53.525, 53.55, 53.275, 52.725, 52.575, 52.108333333333334, 51.975, 51.608333333333334, 51.6, 51.416666666666664, 51.325, 51.18333333333333, 51.25, 50.9, 50.641666666666666, 50.65833333333333, 50.50833333333333, 50.25, 49.9, 49.75, 49.325, 49.74166666666667, 49.833333333333336, 49.81666666666667, 49.93333333333333, 50.016666666666666, 49.3, 49.45, 49.25833333333333, 49.30833333333333, 49.15833333333333, 49.25, 48.75, 48.56666666666667, 48.2, 48.53333333333333, 48.416666666666664, 48.166666666666664, 47.825, 47.85, 47.55, 47.458333333333336]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.301, Test loss: 2.302, Test accuracy: 12.45
Round   1, Train loss: 2.300, Test loss: 2.302, Test accuracy: 14.29
Round   2, Train loss: 2.289, Test loss: 2.302, Test accuracy: 12.72
Round   3, Train loss: 2.290, Test loss: 2.302, Test accuracy: 13.22
Round   4, Train loss: 2.294, Test loss: 2.301, Test accuracy: 14.57
Round   5, Train loss: 2.306, Test loss: 2.301, Test accuracy: 16.90
Round   6, Train loss: 2.311, Test loss: 2.299, Test accuracy: 19.22
Round   7, Train loss: 2.298, Test loss: 2.300, Test accuracy: 19.46
Round   8, Train loss: 2.294, Test loss: 2.300, Test accuracy: 17.65
Round   9, Train loss: 2.291, Test loss: 2.299, Test accuracy: 17.59
Round  10, Train loss: 2.303, Test loss: 2.299, Test accuracy: 17.13
Round  11, Train loss: 2.255, Test loss: 2.296, Test accuracy: 15.96
Round  12, Train loss: 2.281, Test loss: 2.293, Test accuracy: 19.19
Round  13, Train loss: 2.255, Test loss: 2.289, Test accuracy: 17.93
Round  14, Train loss: 2.278, Test loss: 2.294, Test accuracy: 13.62
Round  15, Train loss: 2.253, Test loss: 2.285, Test accuracy: 15.69
Round  16, Train loss: 2.241, Test loss: 2.283, Test accuracy: 15.86
Round  17, Train loss: 2.223, Test loss: 2.275, Test accuracy: 19.22
Round  18, Train loss: 2.244, Test loss: 2.273, Test accuracy: 18.98
Round  19, Train loss: 2.157, Test loss: 2.260, Test accuracy: 20.23
Round  20, Train loss: 2.264, Test loss: 2.268, Test accuracy: 17.54
Round  21, Train loss: 2.179, Test loss: 2.264, Test accuracy: 19.19
Round  22, Train loss: 2.141, Test loss: 2.254, Test accuracy: 20.13
Round  23, Train loss: 2.016, Test loss: 2.230, Test accuracy: 23.50
Round  24, Train loss: 1.989, Test loss: 2.208, Test accuracy: 26.27
Round  25, Train loss: 2.119, Test loss: 2.225, Test accuracy: 24.73
Round  26, Train loss: 1.843, Test loss: 2.177, Test accuracy: 30.41
Round  27, Train loss: 1.961, Test loss: 2.196, Test accuracy: 28.66
Round  28, Train loss: 1.829, Test loss: 2.168, Test accuracy: 30.54
Round  29, Train loss: 1.776, Test loss: 2.162, Test accuracy: 31.81
Round  30, Train loss: 1.739, Test loss: 2.145, Test accuracy: 33.94
Round  31, Train loss: 1.914, Test loss: 2.164, Test accuracy: 29.74
Round  32, Train loss: 1.789, Test loss: 2.164, Test accuracy: 30.24
Round  33, Train loss: 1.248, Test loss: 2.122, Test accuracy: 34.65
Round  34, Train loss: 0.665, Test loss: 2.041, Test accuracy: 43.08
Round  35, Train loss: 1.575, Test loss: 2.120, Test accuracy: 36.76
Round  36, Train loss: 1.680, Test loss: 2.162, Test accuracy: 32.48
Round  37, Train loss: 1.065, Test loss: 2.107, Test accuracy: 36.17
Round  38, Train loss: 0.877, Test loss: 2.062, Test accuracy: 40.62
Round  39, Train loss: 1.130, Test loss: 2.091, Test accuracy: 37.57
Round  40, Train loss: 0.763, Test loss: 2.099, Test accuracy: 37.22
Round  41, Train loss: 0.647, Test loss: 2.049, Test accuracy: 40.84
Round  42, Train loss: 1.165, Test loss: 2.083, Test accuracy: 38.61
Round  43, Train loss: -0.194, Test loss: 2.027, Test accuracy: 45.23
Round  44, Train loss: 0.895, Test loss: 2.054, Test accuracy: 43.12
Round  45, Train loss: 0.520, Test loss: 2.067, Test accuracy: 41.02
Round  46, Train loss: 0.030, Test loss: 1.969, Test accuracy: 49.43
Round  47, Train loss: 1.561, Test loss: 2.083, Test accuracy: 39.05
Round  48, Train loss: 0.749, Test loss: 2.077, Test accuracy: 39.51
Round  49, Train loss: 0.082, Test loss: 2.022, Test accuracy: 44.52
Round  50, Train loss: -0.299, Test loss: 1.965, Test accuracy: 50.42
Round  51, Train loss: -0.682, Test loss: 1.934, Test accuracy: 54.13
Round  52, Train loss: 1.181, Test loss: 2.016, Test accuracy: 46.72
Round  53, Train loss: -0.403, Test loss: 1.958, Test accuracy: 51.88
Round  54, Train loss: -0.696, Test loss: 1.927, Test accuracy: 54.51
Round  55, Train loss: -0.461, Test loss: 1.943, Test accuracy: 53.34
Round  56, Train loss: 0.555, Test loss: 2.022, Test accuracy: 45.77
Round  57, Train loss: -0.259, Test loss: 1.984, Test accuracy: 48.95
Round  58, Train loss: -0.527, Test loss: 1.953, Test accuracy: 51.52
Round  59, Train loss: -0.360, Test loss: 1.928, Test accuracy: 54.44
Round  60, Train loss: -0.101, Test loss: 1.937, Test accuracy: 53.67
Round  61, Train loss: -0.695, Test loss: 1.936, Test accuracy: 54.23
Round  62, Train loss: -0.639, Test loss: 1.921, Test accuracy: 55.52
Round  63, Train loss: -0.720, Test loss: 1.906, Test accuracy: 57.22
Round  64, Train loss: -0.788, Test loss: 1.914, Test accuracy: 55.92
Round  65, Train loss: -0.467, Test loss: 1.908, Test accuracy: 56.72
Round  66, Train loss: -1.409, Test loss: 1.897, Test accuracy: 57.53
Round  67, Train loss: -1.117, Test loss: 1.895, Test accuracy: 57.71
Round  68, Train loss: -1.760, Test loss: 1.887, Test accuracy: 58.43
Round  69, Train loss: -0.966, Test loss: 1.884, Test accuracy: 58.57
Round  70, Train loss: -1.860, Test loss: 1.859, Test accuracy: 61.01
Round  71, Train loss: -1.200, Test loss: 1.834, Test accuracy: 63.05
Round  72, Train loss: -0.301, Test loss: 1.878, Test accuracy: 59.17
Round  73, Train loss: -2.127, Test loss: 1.825, Test accuracy: 63.92
Round  74, Train loss: -1.362, Test loss: 1.830, Test accuracy: 63.43
Round  75, Train loss: -1.199, Test loss: 1.838, Test accuracy: 62.68
Round  76, Train loss: -3.078, Test loss: 1.820, Test accuracy: 64.11
Round  77, Train loss: -1.528, Test loss: 1.824, Test accuracy: 63.87
Round  78, Train loss: -1.307, Test loss: 1.810, Test accuracy: 65.35
Round  79, Train loss: -1.819, Test loss: 1.823, Test accuracy: 63.89
Round  80, Train loss: -2.469, Test loss: 1.820, Test accuracy: 63.99
Round  81, Train loss: -2.723, Test loss: 1.825, Test accuracy: 63.46
Round  82, Train loss: -1.940, Test loss: 1.818, Test accuracy: 64.19
Round  83, Train loss: -1.968, Test loss: 1.821, Test accuracy: 63.90
Round  84, Train loss: -2.486, Test loss: 1.798, Test accuracy: 66.22
Round  85, Train loss: -2.585, Test loss: 1.810, Test accuracy: 65.08
Round  86, Train loss: -1.087, Test loss: 1.821, Test accuracy: 64.35
Round  87, Train loss: -1.490, Test loss: 1.836, Test accuracy: 62.86
Round  88, Train loss: -1.902, Test loss: 1.816, Test accuracy: 64.77
Round  89, Train loss: -2.133, Test loss: 1.808, Test accuracy: 65.28
Round  90, Train loss: -1.837, Test loss: 1.794, Test accuracy: 66.87
Round  91, Train loss: -2.347, Test loss: 1.775, Test accuracy: 68.72
Round  92, Train loss: -2.339, Test loss: 1.802, Test accuracy: 66.07
Round  93, Train loss: -1.760, Test loss: 1.812, Test accuracy: 64.95
Round  94, Train loss: -1.679, Test loss: 1.806, Test accuracy: 65.59
Round  95, Train loss: -2.802, Test loss: 1.797, Test accuracy: 66.40
Round  96, Train loss: -2.631, Test loss: 1.798, Test accuracy: 66.20
Round  97, Train loss: -3.066, Test loss: 1.801, Test accuracy: 65.87
Round  98, Train loss: -3.015, Test loss: 1.810, Test accuracy: 64.94
Round  99, Train loss: -1.744, Test loss: 1.813, Test accuracy: 64.62
Final Round, Train loss: 1.926, Test loss: 1.872, Test accuracy: 60.41
Average accuracy final 10 rounds: 66.0225
Average global accuracy final 10 rounds: 66.0225
1272.344036102295
[]
[12.45, 14.291666666666666, 12.725, 13.216666666666667, 14.566666666666666, 16.9, 19.216666666666665, 19.458333333333332, 17.65, 17.591666666666665, 17.133333333333333, 15.958333333333334, 19.191666666666666, 17.925, 13.625, 15.691666666666666, 15.858333333333333, 19.216666666666665, 18.983333333333334, 20.233333333333334, 17.541666666666668, 19.191666666666666, 20.133333333333333, 23.5, 26.266666666666666, 24.733333333333334, 30.408333333333335, 28.658333333333335, 30.541666666666668, 31.808333333333334, 33.94166666666667, 29.741666666666667, 30.241666666666667, 34.65, 43.075, 36.75833333333333, 32.475, 36.175, 40.61666666666667, 37.56666666666667, 37.21666666666667, 40.84166666666667, 38.608333333333334, 45.233333333333334, 43.125, 41.025, 49.43333333333333, 39.05, 39.50833333333333, 44.525, 50.425, 54.13333333333333, 46.71666666666667, 51.88333333333333, 54.50833333333333, 53.34166666666667, 45.775, 48.95, 51.516666666666666, 54.44166666666667, 53.666666666666664, 54.233333333333334, 55.525, 57.21666666666667, 55.925, 56.71666666666667, 57.53333333333333, 57.708333333333336, 58.43333333333333, 58.56666666666667, 61.00833333333333, 63.05, 59.166666666666664, 63.916666666666664, 63.43333333333333, 62.68333333333333, 64.10833333333333, 63.86666666666667, 65.35, 63.891666666666666, 63.99166666666667, 63.458333333333336, 64.19166666666666, 63.9, 66.225, 65.075, 64.35, 62.858333333333334, 64.76666666666667, 65.28333333333333, 66.86666666666666, 68.71666666666667, 66.06666666666666, 64.95, 65.59166666666667, 66.4, 66.2, 65.86666666666666, 64.94166666666666, 64.625, 60.40833333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.59
Round   0, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round   3, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round   3, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round   5, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round   5, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.58
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.59
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  10, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  10, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.61
Round  11, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  11, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  13, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  14, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  14, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.61
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  16, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  16, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  19, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  19, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  20, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  20, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  21, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  21, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  22, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  22, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  23, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  23, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.64
Round  24, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  24, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  25, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  25, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.64
Round  26, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  26, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  27, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  27, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  28, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  28, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  29, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  29, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  30, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  30, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  31, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  31, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  32, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  32, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  33, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  33, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  34, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  34, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  35, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  35, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  36, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  36, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.64
Round  37, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  37, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  38, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.69
Round  38, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  39, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.69
Round  39, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  40, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.69
Round  40, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  41, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  41, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  42, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  42, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  43, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  43, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.63
Round  44, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  44, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  45, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  45, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.69
Round  46, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  46, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  47, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  47, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  48, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  48, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.69
Round  49, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  49, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  50, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  50, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  51, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  51, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  52, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  52, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  53, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  53, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  54, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  54, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  55, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  55, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.66
Round  56, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  56, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  57, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  57, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  58, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  58, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  59, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  59, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  60, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  60, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  61, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  61, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  62, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  62, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  63, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  63, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.68
Round  64, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  64, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  65, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  65, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.66
Round  66, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  66, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  67, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  67, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  68, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.63
Round  68, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.68
Round  69, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  69, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.69
Round  70, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.68
Round  70, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.65
Round  71, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  71, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  72, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  72, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.67
Round  73, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  73, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  74, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  74, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.66
Round  75, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  75, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.65
Round  76, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  76, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.68
Round  77, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.66
Round  77, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.68
Round  78, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  78, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  79, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.67
Round  79, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Round  80, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  80, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  81, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  81, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  82, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.64
Round  82, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.62
Round  83, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  83, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  84, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.62
Round  84, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  85, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  85, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.57
Round  86, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  86, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.56
Round  87, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  87, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  88, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.64
Round  88, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.58
Round  89, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  89, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.58
Round  90, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.67
Round  90, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.59
Round  91, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  91, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.56
Round  92, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.66
Round  92, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.57
Round  93, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  93, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Round  94, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.65
Round  94, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.63
Round  95, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  95, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.61
Round  96, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.70
Round  96, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  97, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.68
Round  97, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Round  98, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.65
Round  98, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.61
Round  99, Train loss: 2.303, Test loss: 2.303, Test accuracy: 10.63
Round  99, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 10.60
Final Round, Train loss: 2.302, Test loss: 2.303, Test accuracy: 10.62
Final Round, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 10.60
Average accuracy final 10 rounds: 10.665249999999999 

Average global accuracy final 10 rounds: 10.597 

4541.732900381088
[4.233440399169922, 8.628674745559692, 12.872212886810303, 16.971180200576782, 21.02176523208618, 25.344435691833496, 29.564435720443726, 33.82705545425415, 37.93192744255066, 42.17180562019348, 46.48314023017883, 50.66766905784607, 54.782055616378784, 59.00771880149841, 63.29819083213806, 67.46133327484131, 71.19016122817993, 74.85294198989868, 78.62465357780457, 82.38174104690552, 86.03063035011292, 89.7167558670044, 93.45078372955322, 97.25076580047607, 100.91772532463074, 104.46229243278503, 108.16703271865845, 111.90305948257446, 115.54484677314758, 119.21344995498657, 122.8233687877655, 126.56663751602173, 130.26028156280518, 133.9266061782837, 137.53187441825867, 141.23988151550293, 144.89210391044617, 148.5216405391693, 152.2807698249817, 155.91435050964355, 159.6288981437683, 163.33736062049866, 166.9344620704651, 170.4853355884552, 174.23309540748596, 177.97084999084473, 181.67170548439026, 185.4084701538086, 189.24585223197937, 192.86841773986816, 196.58643007278442, 200.26547265052795, 203.9519636631012, 207.6613781452179, 211.38427066802979, 214.94473123550415, 218.5338237285614, 222.26783561706543, 225.9862744808197, 229.53127932548523, 233.18420028686523, 236.8303027153015, 240.56882190704346, 244.33360695838928, 248.01930737495422, 251.70212721824646, 255.3956253528595, 259.06131505966187, 262.67195320129395, 266.24862146377563, 270.03430247306824, 273.7917938232422, 277.4953553676605, 281.2104799747467, 284.9575412273407, 288.56370782852173, 292.2417998313904, 296.0700452327728, 299.6733593940735, 303.2353367805481, 306.8538432121277, 310.5597529411316, 313.99596071243286, 317.54749059677124, 321.26671862602234, 324.9285924434662, 328.4833791255951, 332.069721698761, 335.9496033191681, 339.64915561676025, 343.12381863594055, 346.8443057537079, 350.51737785339355, 354.15037989616394, 357.62620520591736, 361.27969884872437, 364.96374559402466, 368.61221742630005, 372.1472487449646, 375.73175859451294, 377.5743238925934]
[10.5875, 10.635, 10.645, 10.64, 10.645, 10.6375, 10.6575, 10.64, 10.6225, 10.625, 10.63, 10.64, 10.61, 10.6075, 10.62, 10.61, 10.6275, 10.63, 10.64, 10.675, 10.6375, 10.6325, 10.615, 10.63, 10.64, 10.64, 10.6325, 10.65, 10.6375, 10.64, 10.655, 10.6525, 10.67, 10.6775, 10.6725, 10.655, 10.6625, 10.68, 10.685, 10.69, 10.69, 10.675, 10.6725, 10.6675, 10.6725, 10.6675, 10.6775, 10.6775, 10.665, 10.6725, 10.6675, 10.6675, 10.6725, 10.6575, 10.65, 10.6575, 10.66, 10.665, 10.675, 10.67, 10.6725, 10.665, 10.66, 10.6725, 10.6725, 10.6675, 10.675, 10.6675, 10.635, 10.67, 10.6775, 10.675, 10.6725, 10.6725, 10.655, 10.6625, 10.65, 10.6575, 10.66, 10.67, 10.6775, 10.625, 10.6425, 10.625, 10.62, 10.64, 10.65, 10.66, 10.6375, 10.6275, 10.6675, 10.675, 10.66, 10.6475, 10.6525, 10.6775, 10.7, 10.6825, 10.655, 10.635, 10.62]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.302, Test loss: 2.302, Test accuracy: 11.66
Round   1, Train loss: 2.301, Test loss: 2.302, Test accuracy: 13.26
Round   2, Train loss: 2.299, Test loss: 2.299, Test accuracy: 12.08
Round   3, Train loss: 2.290, Test loss: 2.291, Test accuracy: 14.36
Round   4, Train loss: 2.246, Test loss: 2.278, Test accuracy: 16.69
Round   5, Train loss: 2.162, Test loss: 2.281, Test accuracy: 15.98
Round   6, Train loss: 2.130, Test loss: 2.271, Test accuracy: 16.42
Round   7, Train loss: 2.044, Test loss: 2.261, Test accuracy: 18.48
Round   8, Train loss: 2.101, Test loss: 2.265, Test accuracy: 17.67
Round   9, Train loss: 2.098, Test loss: 2.270, Test accuracy: 17.37
Round  10, Train loss: 2.052, Test loss: 2.268, Test accuracy: 17.74
Round  11, Train loss: 2.041, Test loss: 2.271, Test accuracy: 16.83
Round  12, Train loss: 2.003, Test loss: 2.267, Test accuracy: 17.31
Round  13, Train loss: 1.998, Test loss: 2.262, Test accuracy: 17.77
Round  14, Train loss: 1.994, Test loss: 2.267, Test accuracy: 17.74
Round  15, Train loss: 1.996, Test loss: 2.264, Test accuracy: 17.82
Round  16, Train loss: 1.921, Test loss: 2.265, Test accuracy: 18.26
Round  17, Train loss: 1.972, Test loss: 2.265, Test accuracy: 17.92
Round  18, Train loss: 1.980, Test loss: 2.268, Test accuracy: 17.41
Round  19, Train loss: 1.929, Test loss: 2.255, Test accuracy: 19.19
Round  20, Train loss: 1.942, Test loss: 2.260, Test accuracy: 19.15
Round  21, Train loss: 1.933, Test loss: 2.257, Test accuracy: 18.39
Round  22, Train loss: 1.982, Test loss: 2.264, Test accuracy: 17.92
Round  23, Train loss: 1.920, Test loss: 2.263, Test accuracy: 18.17
Round  24, Train loss: 1.913, Test loss: 2.256, Test accuracy: 18.96
Round  25, Train loss: 1.923, Test loss: 2.263, Test accuracy: 18.07
Round  26, Train loss: 1.920, Test loss: 2.268, Test accuracy: 17.35
Round  27, Train loss: 1.920, Test loss: 2.268, Test accuracy: 17.46
Round  28, Train loss: 1.896, Test loss: 2.259, Test accuracy: 18.19
Round  29, Train loss: 1.878, Test loss: 2.258, Test accuracy: 18.69
Round  30, Train loss: 1.877, Test loss: 2.263, Test accuracy: 18.06
Round  31, Train loss: 1.896, Test loss: 2.266, Test accuracy: 17.88
Round  32, Train loss: 1.876, Test loss: 2.251, Test accuracy: 19.49
Round  33, Train loss: 1.880, Test loss: 2.254, Test accuracy: 19.08
Round  34, Train loss: 1.925, Test loss: 2.260, Test accuracy: 18.47
Round  35, Train loss: 1.890, Test loss: 2.261, Test accuracy: 18.16
Round  36, Train loss: 1.900, Test loss: 2.259, Test accuracy: 18.71
Round  37, Train loss: 1.908, Test loss: 2.262, Test accuracy: 18.01
Round  38, Train loss: 1.963, Test loss: 2.259, Test accuracy: 18.03
Round  39, Train loss: 1.876, Test loss: 2.259, Test accuracy: 17.82
Round  40, Train loss: 1.866, Test loss: 2.260, Test accuracy: 18.31
Round  41, Train loss: 1.823, Test loss: 2.263, Test accuracy: 17.97
Round  42, Train loss: 1.821, Test loss: 2.269, Test accuracy: 17.54
Round  43, Train loss: 1.889, Test loss: 2.259, Test accuracy: 18.99
Round  44, Train loss: 1.909, Test loss: 2.254, Test accuracy: 19.00
Round  45, Train loss: 1.806, Test loss: 2.251, Test accuracy: 19.14
Round  46, Train loss: 1.850, Test loss: 2.263, Test accuracy: 18.33
Round  47, Train loss: 1.848, Test loss: 2.257, Test accuracy: 19.21
Round  48, Train loss: 1.897, Test loss: 2.258, Test accuracy: 18.67
Round  49, Train loss: 1.874, Test loss: 2.259, Test accuracy: 18.56
Round  50, Train loss: 1.839, Test loss: 2.260, Test accuracy: 18.33
Round  51, Train loss: 1.865, Test loss: 2.264, Test accuracy: 17.50
Round  52, Train loss: 1.834, Test loss: 2.264, Test accuracy: 17.57
Round  53, Train loss: 1.834, Test loss: 2.267, Test accuracy: 17.91
Round  54, Train loss: 1.834, Test loss: 2.254, Test accuracy: 18.71
Round  55, Train loss: 1.855, Test loss: 2.257, Test accuracy: 18.30
Round  56, Train loss: 1.865, Test loss: 2.270, Test accuracy: 16.74
Round  57, Train loss: 1.833, Test loss: 2.269, Test accuracy: 16.79
Round  58, Train loss: 1.852, Test loss: 2.272, Test accuracy: 16.52
Round  59, Train loss: 1.823, Test loss: 2.260, Test accuracy: 18.24
Round  60, Train loss: 1.871, Test loss: 2.264, Test accuracy: 17.62
Round  61, Train loss: 1.860, Test loss: 2.264, Test accuracy: 17.78
Round  62, Train loss: 1.814, Test loss: 2.275, Test accuracy: 16.51
Round  63, Train loss: 1.818, Test loss: 2.263, Test accuracy: 18.00
Round  64, Train loss: 1.856, Test loss: 2.269, Test accuracy: 16.87
Round  65, Train loss: 1.838, Test loss: 2.261, Test accuracy: 17.70
Round  66, Train loss: 1.859, Test loss: 2.268, Test accuracy: 16.93
Round  67, Train loss: 1.787, Test loss: 2.257, Test accuracy: 18.81
Round  68, Train loss: 1.781, Test loss: 2.270, Test accuracy: 16.84
Round  69, Train loss: 1.789, Test loss: 2.274, Test accuracy: 16.02
Round  70, Train loss: 1.826, Test loss: 2.269, Test accuracy: 17.11
Round  71, Train loss: 1.838, Test loss: 2.272, Test accuracy: 16.82
Round  72, Train loss: 1.801, Test loss: 2.264, Test accuracy: 17.73
Round  73, Train loss: 1.806, Test loss: 2.271, Test accuracy: 17.01
Round  74, Train loss: 1.765, Test loss: 2.264, Test accuracy: 17.89
Round  75, Train loss: 1.813, Test loss: 2.267, Test accuracy: 17.55
Round  76, Train loss: 1.787, Test loss: 2.259, Test accuracy: 18.45
Round  77, Train loss: 1.867, Test loss: 2.272, Test accuracy: 16.80
Round  78, Train loss: 1.798, Test loss: 2.266, Test accuracy: 17.23
Round  79, Train loss: 1.807, Test loss: 2.270, Test accuracy: 17.14
Round  80, Train loss: 1.756, Test loss: 2.277, Test accuracy: 16.31
Round  81, Train loss: 1.810, Test loss: 2.272, Test accuracy: 16.87
Round  82, Train loss: 1.739, Test loss: 2.258, Test accuracy: 18.52
Round  83, Train loss: 1.776, Test loss: 2.269, Test accuracy: 17.41
Round  84, Train loss: 1.796, Test loss: 2.280, Test accuracy: 15.44
Round  85, Train loss: 1.809, Test loss: 2.259, Test accuracy: 18.68
Round  86, Train loss: 1.789, Test loss: 2.281, Test accuracy: 15.28
Round  87, Train loss: 1.768, Test loss: 2.261, Test accuracy: 18.03
Round  88, Train loss: 1.786, Test loss: 2.270, Test accuracy: 17.09
Round  89, Train loss: 1.788, Test loss: 2.266, Test accuracy: 16.98
Round  90, Train loss: 1.795, Test loss: 2.261, Test accuracy: 17.67
Round  91, Train loss: 1.740, Test loss: 2.267, Test accuracy: 17.41
Round  92, Train loss: 1.728, Test loss: 2.257, Test accuracy: 18.73
Round  93, Train loss: 1.741, Test loss: 2.270, Test accuracy: 16.89
Round  94, Train loss: 1.735, Test loss: 2.282, Test accuracy: 15.61
Round  95, Train loss: 1.781, Test loss: 2.281, Test accuracy: 15.32
Round  96, Train loss: 1.732, Test loss: 2.273, Test accuracy: 16.52
Round  97, Train loss: 1.720, Test loss: 2.288, Test accuracy: 14.79
Round  98, Train loss: 1.728, Test loss: 2.286, Test accuracy: 15.17
Round  99, Train loss: 1.725, Test loss: 2.283, Test accuracy: 15.41
Final Round, Train loss: 1.711, Test loss: 2.278, Test accuracy: 15.94
Average accuracy final 10 rounds: 16.351111111111113
2908.96786403656
[4.454253673553467, 8.496688604354858, 12.72618579864502, 17.01198172569275, 21.204593896865845, 25.22544574737549, 29.258583068847656, 33.21639537811279, 37.212037563323975, 41.12995004653931, 45.167765617370605, 49.04067349433899, 53.049028635025024, 56.950806856155396, 60.94308924674988, 64.8909330368042, 68.8615152835846, 72.78733110427856, 76.74017095565796, 80.70743179321289, 84.6599109172821, 88.66151523590088, 92.6311342716217, 96.62000060081482, 100.53038144111633, 104.48222732543945, 108.3799500465393, 112.40574097633362, 116.30369234085083, 120.27775502204895, 124.21446752548218, 128.301367521286, 132.27770805358887, 136.24923276901245, 140.25135850906372, 144.2638189792633, 148.1376371383667, 152.13156843185425, 156.05326557159424, 160.11280584335327, 164.04106426239014, 168.0609769821167, 171.977290391922, 176.06095242500305, 180.0383493900299, 184.07629346847534, 188.0093810558319, 192.06090784072876, 196.03522324562073, 200.00201535224915, 203.9544792175293, 207.93817019462585, 211.8839554786682, 215.8278923034668, 219.8157057762146, 223.80366969108582, 227.79058980941772, 231.7617223262787, 235.6683931350708, 239.63783240318298, 243.5391490459442, 247.48851132392883, 251.4739580154419, 255.48739314079285, 259.43885254859924, 263.47995829582214, 267.51110768318176, 271.43953108787537, 275.4486405849457, 279.347722530365, 283.3081443309784, 287.23785519599915, 291.16975021362305, 295.07959032058716, 298.98710894584656, 302.9223051071167, 306.8779447078705, 310.8519341945648, 314.8463091850281, 318.89371490478516, 322.90503764152527, 326.9030804634094, 330.85570645332336, 334.88677501678467, 338.86546635627747, 342.8467905521393, 346.87114334106445, 350.8158357143402, 354.73274660110474, 358.6884047985077, 362.63134241104126, 366.66346073150635, 370.6761157512665, 374.6265993118286, 378.6267902851105, 382.53706550598145, 386.5937180519104, 390.5758926868439, 394.56249380111694, 398.56261682510376, 400.76931953430176]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[11.655555555555555, 13.255555555555556, 12.077777777777778, 14.355555555555556, 16.694444444444443, 15.977777777777778, 16.42222222222222, 18.483333333333334, 17.67222222222222, 17.372222222222224, 17.744444444444444, 16.833333333333332, 17.305555555555557, 17.772222222222222, 17.738888888888887, 17.822222222222223, 18.261111111111113, 17.916666666666668, 17.405555555555555, 19.194444444444443, 19.15, 18.38888888888889, 17.916666666666668, 18.166666666666668, 18.961111111111112, 18.066666666666666, 17.35, 17.455555555555556, 18.194444444444443, 18.694444444444443, 18.06111111111111, 17.883333333333333, 19.494444444444444, 19.083333333333332, 18.47222222222222, 18.155555555555555, 18.705555555555556, 18.005555555555556, 18.02777777777778, 17.816666666666666, 18.305555555555557, 17.97222222222222, 17.538888888888888, 18.994444444444444, 19.0, 19.144444444444446, 18.32777777777778, 19.211111111111112, 18.666666666666668, 18.555555555555557, 18.333333333333332, 17.5, 17.566666666666666, 17.91111111111111, 18.711111111111112, 18.3, 16.744444444444444, 16.794444444444444, 16.516666666666666, 18.238888888888887, 17.616666666666667, 17.77777777777778, 16.505555555555556, 18.0, 16.866666666666667, 17.7, 16.933333333333334, 18.81111111111111, 16.83888888888889, 16.022222222222222, 17.105555555555554, 16.816666666666666, 17.727777777777778, 17.011111111111113, 17.88888888888889, 17.55, 18.45, 16.8, 17.227777777777778, 17.144444444444446, 16.305555555555557, 16.872222222222224, 18.522222222222222, 17.41111111111111, 15.438888888888888, 18.683333333333334, 15.283333333333333, 18.033333333333335, 17.08888888888889, 16.983333333333334, 17.67222222222222, 17.41111111111111, 18.733333333333334, 16.88888888888889, 15.605555555555556, 15.322222222222223, 16.516666666666666, 14.78888888888889, 15.166666666666666, 15.405555555555555, 15.938888888888888]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.303, Test accuracy: 10.08
Round   1, Train loss: 2.302, Test loss: 2.302, Test accuracy: 10.17
Round   2, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.58
Round   3, Train loss: 2.301, Test loss: 2.302, Test accuracy: 10.75
Round   4, Train loss: 2.301, Test loss: 2.302, Test accuracy: 11.23
Round   5, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.46
Round   6, Train loss: 2.299, Test loss: 2.301, Test accuracy: 11.49
Round   7, Train loss: 2.299, Test loss: 2.300, Test accuracy: 11.78
Round   8, Train loss: 2.296, Test loss: 2.299, Test accuracy: 12.10
Round   9, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.05
Round  10, Train loss: 2.293, Test loss: 2.298, Test accuracy: 12.68
Round  11, Train loss: 2.285, Test loss: 2.294, Test accuracy: 12.68
Round  12, Train loss: 2.296, Test loss: 2.293, Test accuracy: 12.03
Round  13, Train loss: 2.279, Test loss: 2.287, Test accuracy: 13.41
Round  14, Train loss: 2.271, Test loss: 2.280, Test accuracy: 14.72
Round  15, Train loss: 2.264, Test loss: 2.270, Test accuracy: 17.25
Round  16, Train loss: 2.234, Test loss: 2.259, Test accuracy: 18.91
Round  17, Train loss: 2.234, Test loss: 2.249, Test accuracy: 20.32
Round  18, Train loss: 2.196, Test loss: 2.235, Test accuracy: 21.67
Round  19, Train loss: 2.199, Test loss: 2.226, Test accuracy: 23.74
Round  20, Train loss: 2.176, Test loss: 2.206, Test accuracy: 25.77
Round  21, Train loss: 2.172, Test loss: 2.194, Test accuracy: 26.77
Round  22, Train loss: 2.121, Test loss: 2.178, Test accuracy: 28.41
Round  23, Train loss: 2.149, Test loss: 2.159, Test accuracy: 30.20
Round  24, Train loss: 2.090, Test loss: 2.143, Test accuracy: 32.09
Round  25, Train loss: 2.092, Test loss: 2.133, Test accuracy: 34.01
Round  26, Train loss: 2.121, Test loss: 2.109, Test accuracy: 38.61
Round  27, Train loss: 2.065, Test loss: 2.090, Test accuracy: 40.62
Round  28, Train loss: 2.060, Test loss: 2.074, Test accuracy: 42.57
Round  29, Train loss: 2.041, Test loss: 2.054, Test accuracy: 45.52
Round  30, Train loss: 2.026, Test loss: 2.031, Test accuracy: 47.88
Round  31, Train loss: 2.005, Test loss: 2.015, Test accuracy: 49.43
Round  32, Train loss: 1.967, Test loss: 1.988, Test accuracy: 51.96
Round  33, Train loss: 1.956, Test loss: 1.969, Test accuracy: 53.84
Round  34, Train loss: 1.922, Test loss: 1.955, Test accuracy: 54.38
Round  35, Train loss: 1.928, Test loss: 1.944, Test accuracy: 55.62
Round  36, Train loss: 1.924, Test loss: 1.939, Test accuracy: 56.71
Round  37, Train loss: 1.919, Test loss: 1.926, Test accuracy: 57.54
Round  38, Train loss: 1.896, Test loss: 1.924, Test accuracy: 58.17
Round  39, Train loss: 1.881, Test loss: 1.918, Test accuracy: 58.66
Round  40, Train loss: 1.894, Test loss: 1.909, Test accuracy: 60.04
Round  41, Train loss: 1.865, Test loss: 1.897, Test accuracy: 60.90
Round  42, Train loss: 1.918, Test loss: 1.895, Test accuracy: 61.32
Round  43, Train loss: 1.942, Test loss: 1.888, Test accuracy: 62.23
Round  44, Train loss: 1.878, Test loss: 1.881, Test accuracy: 62.92
Round  45, Train loss: 1.887, Test loss: 1.879, Test accuracy: 63.13
Round  46, Train loss: 1.830, Test loss: 1.863, Test accuracy: 63.71
Round  47, Train loss: 1.845, Test loss: 1.860, Test accuracy: 64.31
Round  48, Train loss: 1.862, Test loss: 1.859, Test accuracy: 64.72
Round  49, Train loss: 1.905, Test loss: 1.857, Test accuracy: 65.74
Round  50, Train loss: 1.849, Test loss: 1.859, Test accuracy: 65.80
Round  51, Train loss: 1.850, Test loss: 1.850, Test accuracy: 66.37
Round  52, Train loss: 1.856, Test loss: 1.840, Test accuracy: 67.05
Round  53, Train loss: 1.828, Test loss: 1.841, Test accuracy: 67.12
Round  54, Train loss: 1.820, Test loss: 1.834, Test accuracy: 67.26
Round  55, Train loss: 1.814, Test loss: 1.837, Test accuracy: 67.41
Round  56, Train loss: 1.782, Test loss: 1.828, Test accuracy: 67.23
Round  57, Train loss: 1.821, Test loss: 1.827, Test accuracy: 67.51
Round  58, Train loss: 1.808, Test loss: 1.818, Test accuracy: 68.62
Round  59, Train loss: 1.780, Test loss: 1.812, Test accuracy: 68.69
Round  60, Train loss: 1.803, Test loss: 1.814, Test accuracy: 68.74
Round  61, Train loss: 1.767, Test loss: 1.809, Test accuracy: 68.71
Round  62, Train loss: 1.845, Test loss: 1.812, Test accuracy: 69.07
Round  63, Train loss: 1.781, Test loss: 1.806, Test accuracy: 69.33
Round  64, Train loss: 1.771, Test loss: 1.800, Test accuracy: 70.28
Round  65, Train loss: 1.785, Test loss: 1.797, Test accuracy: 70.24
Round  66, Train loss: 1.766, Test loss: 1.795, Test accuracy: 70.41
Round  67, Train loss: 1.780, Test loss: 1.797, Test accuracy: 70.25
Round  68, Train loss: 1.711, Test loss: 1.788, Test accuracy: 70.47
Round  69, Train loss: 1.755, Test loss: 1.787, Test accuracy: 70.44
Round  70, Train loss: 1.794, Test loss: 1.791, Test accuracy: 70.61
Round  71, Train loss: 1.754, Test loss: 1.787, Test accuracy: 70.71
Round  72, Train loss: 1.798, Test loss: 1.794, Test accuracy: 70.78
Round  73, Train loss: 1.758, Test loss: 1.782, Test accuracy: 71.17
Round  74, Train loss: 1.780, Test loss: 1.786, Test accuracy: 71.24
Round  75, Train loss: 1.733, Test loss: 1.781, Test accuracy: 71.30
Round  76, Train loss: 1.722, Test loss: 1.778, Test accuracy: 71.30
Round  77, Train loss: 1.774, Test loss: 1.777, Test accuracy: 71.70
Round  78, Train loss: 1.752, Test loss: 1.777, Test accuracy: 71.97
Round  79, Train loss: 1.760, Test loss: 1.778, Test accuracy: 72.04
Round  80, Train loss: 1.740, Test loss: 1.771, Test accuracy: 72.34
Round  81, Train loss: 1.726, Test loss: 1.771, Test accuracy: 72.28
Round  82, Train loss: 1.768, Test loss: 1.766, Test accuracy: 72.92
Round  83, Train loss: 1.745, Test loss: 1.766, Test accuracy: 73.10
Round  84, Train loss: 1.755, Test loss: 1.762, Test accuracy: 73.62
Round  85, Train loss: 1.699, Test loss: 1.756, Test accuracy: 73.97
Round  86, Train loss: 1.705, Test loss: 1.756, Test accuracy: 73.92
Round  87, Train loss: 1.754, Test loss: 1.750, Test accuracy: 74.58
Round  88, Train loss: 1.697, Test loss: 1.750, Test accuracy: 74.81
Round  89, Train loss: 1.708, Test loss: 1.747, Test accuracy: 74.87
Round  90, Train loss: 1.734, Test loss: 1.751, Test accuracy: 74.87
Round  91, Train loss: 1.679, Test loss: 1.739, Test accuracy: 74.89
Round  92, Train loss: 1.685, Test loss: 1.745, Test accuracy: 74.95
Round  93, Train loss: 1.714, Test loss: 1.747, Test accuracy: 74.85
Round  94, Train loss: 1.722, Test loss: 1.745, Test accuracy: 75.10/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.701, Test loss: 1.737, Test accuracy: 75.08
Round  96, Train loss: 1.666, Test loss: 1.739, Test accuracy: 75.15
Round  97, Train loss: 1.705, Test loss: 1.735, Test accuracy: 75.17
Round  98, Train loss: 1.697, Test loss: 1.735, Test accuracy: 75.58
Round  99, Train loss: 1.699, Test loss: 1.737, Test accuracy: 75.73
Final Round, Train loss: 1.681, Test loss: 1.717, Test accuracy: 77.19
Average accuracy final 10 rounds: 75.13583333333334
1162.1134142875671
[1.455955982208252, 2.8837199211120605, 4.195188045501709, 5.574090957641602, 6.928565740585327, 8.237805128097534, 9.60829496383667, 10.943595170974731, 12.326197862625122, 13.65784740447998, 14.98026442527771, 16.307101488113403, 17.64836025238037, 18.992987394332886, 20.362298488616943, 21.710030555725098, 23.01760244369507, 24.40182900428772, 25.71857523918152, 27.072836875915527, 28.421063899993896, 29.758962631225586, 31.128390789031982, 32.406797885894775, 33.80563926696777, 35.12088441848755, 36.44829559326172, 37.81863331794739, 39.10480260848999, 40.46075630187988, 41.760828495025635, 43.0645432472229, 44.427125215530396, 45.69123148918152, 47.06895112991333, 48.379348278045654, 49.70473670959473, 51.046119689941406, 52.31316614151001, 53.72250819206238, 55.03148341178894, 56.40004348754883, 57.78754806518555, 59.070483684539795, 60.41689157485962, 61.697121143341064, 63.05089449882507, 64.4066092967987, 65.8112633228302, 67.27587270736694, 68.60357737541199, 69.98849296569824, 71.32703852653503, 72.68657875061035, 74.01083397865295, 75.35262656211853, 76.70721507072449, 78.00981187820435, 79.35609865188599, 80.6710467338562, 81.97749066352844, 83.35441517829895, 84.6508641242981, 86.11858820915222, 87.44244837760925, 88.87381362915039, 90.22169423103333, 91.56659984588623, 93.01800274848938, 94.38633346557617, 95.759357213974, 97.19325542449951, 98.58259844779968, 100.00190734863281, 101.36133098602295, 102.7586121559143, 104.07745361328125, 105.34601020812988, 106.62376856803894, 107.89460945129395, 109.20811176300049, 110.50448799133301, 111.79920768737793, 113.10476183891296, 114.39125537872314, 115.67057633399963, 116.96786236763, 118.29608368873596, 119.5689492225647, 120.82469987869263, 122.14071273803711, 123.36653017997742, 124.66726088523865, 125.91245555877686, 127.17548298835754, 128.45949411392212, 129.73386812210083, 131.04814863204956, 132.30098366737366, 133.58597612380981, 135.27929139137268]
[10.083333333333334, 10.166666666666666, 10.583333333333334, 10.75, 11.233333333333333, 11.458333333333334, 11.491666666666667, 11.783333333333333, 12.1, 12.05, 12.683333333333334, 12.683333333333334, 12.033333333333333, 13.408333333333333, 14.716666666666667, 17.25, 18.908333333333335, 20.325, 21.666666666666668, 23.741666666666667, 25.766666666666666, 26.766666666666666, 28.408333333333335, 30.2, 32.09166666666667, 34.00833333333333, 38.608333333333334, 40.61666666666667, 42.56666666666667, 45.516666666666666, 47.875, 49.43333333333333, 51.958333333333336, 53.84166666666667, 54.38333333333333, 55.625, 56.708333333333336, 57.541666666666664, 58.166666666666664, 58.65833333333333, 60.041666666666664, 60.9, 61.31666666666667, 62.225, 62.925, 63.13333333333333, 63.708333333333336, 64.30833333333334, 64.725, 65.74166666666666, 65.8, 66.36666666666666, 67.05, 67.125, 67.25833333333334, 67.40833333333333, 67.23333333333333, 67.50833333333334, 68.625, 68.69166666666666, 68.74166666666666, 68.70833333333333, 69.06666666666666, 69.325, 70.275, 70.24166666666666, 70.40833333333333, 70.25, 70.46666666666667, 70.44166666666666, 70.60833333333333, 70.70833333333333, 70.78333333333333, 71.175, 71.24166666666666, 71.3, 71.3, 71.7, 71.975, 72.04166666666667, 72.34166666666667, 72.275, 72.91666666666667, 73.1, 73.625, 73.96666666666667, 73.925, 74.58333333333333, 74.80833333333334, 74.86666666666666, 74.86666666666666, 74.89166666666667, 74.95, 74.85, 75.1, 75.075, 75.15, 75.16666666666667, 75.575, 75.73333333333333, 77.19166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.302, Test accuracy: 9.97
Round   1, Train loss: 2.315, Test loss: 2.302, Test accuracy: 9.88
Round   2, Train loss: 2.310, Test loss: 2.302, Test accuracy: 10.07
Round   3, Train loss: 2.306, Test loss: 2.301, Test accuracy: 10.04
Round   4, Train loss: 2.305, Test loss: 2.301, Test accuracy: 11.08
Round   5, Train loss: 2.302, Test loss: 2.300, Test accuracy: 11.49
Round   6, Train loss: 2.299, Test loss: 2.298, Test accuracy: 12.42
Round   7, Train loss: 2.297, Test loss: 2.296, Test accuracy: 13.12
Round   8, Train loss: 2.291, Test loss: 2.293, Test accuracy: 13.85
Round   9, Train loss: 2.282, Test loss: 2.287, Test accuracy: 15.88
Round  10, Train loss: 2.257, Test loss: 2.276, Test accuracy: 18.17
Round  11, Train loss: 2.237, Test loss: 2.265, Test accuracy: 18.22
Round  12, Train loss: 2.224, Test loss: 2.255, Test accuracy: 19.61
Round  13, Train loss: 2.213, Test loss: 2.243, Test accuracy: 20.98
Round  14, Train loss: 2.194, Test loss: 2.233, Test accuracy: 21.75
Round  15, Train loss: 2.159, Test loss: 2.217, Test accuracy: 23.51
Round  16, Train loss: 2.203, Test loss: 2.205, Test accuracy: 25.97
Round  17, Train loss: 2.160, Test loss: 2.188, Test accuracy: 27.37
Round  18, Train loss: 2.133, Test loss: 2.175, Test accuracy: 28.68
Round  19, Train loss: 2.122, Test loss: 2.158, Test accuracy: 30.69
Round  20, Train loss: 2.101, Test loss: 2.143, Test accuracy: 33.08
Round  21, Train loss: 2.082, Test loss: 2.123, Test accuracy: 35.08
Round  22, Train loss: 2.116, Test loss: 2.114, Test accuracy: 35.28
Round  23, Train loss: 2.054, Test loss: 2.099, Test accuracy: 37.10
Round  24, Train loss: 2.039, Test loss: 2.085, Test accuracy: 39.16
Round  25, Train loss: 2.038, Test loss: 2.070, Test accuracy: 41.36
Round  26, Train loss: 2.018, Test loss: 2.058, Test accuracy: 43.21
Round  27, Train loss: 2.065, Test loss: 2.049, Test accuracy: 45.52
Round  28, Train loss: 2.037, Test loss: 2.037, Test accuracy: 46.92
Round  29, Train loss: 2.035, Test loss: 2.031, Test accuracy: 47.80
Round  30, Train loss: 2.015, Test loss: 2.006, Test accuracy: 49.73
Round  31, Train loss: 1.997, Test loss: 2.009, Test accuracy: 49.80
Round  32, Train loss: 1.988, Test loss: 2.008, Test accuracy: 50.00
Round  33, Train loss: 1.942, Test loss: 1.990, Test accuracy: 51.05
Round  34, Train loss: 1.922, Test loss: 1.980, Test accuracy: 51.35
Round  35, Train loss: 1.966, Test loss: 1.960, Test accuracy: 53.97
Round  36, Train loss: 1.933, Test loss: 1.953, Test accuracy: 55.09
Round  37, Train loss: 1.991, Test loss: 1.948, Test accuracy: 55.68
Round  38, Train loss: 1.907, Test loss: 1.935, Test accuracy: 56.67
Round  39, Train loss: 1.979, Test loss: 1.938, Test accuracy: 57.14
Round  40, Train loss: 1.908, Test loss: 1.920, Test accuracy: 57.73
Round  41, Train loss: 1.910, Test loss: 1.918, Test accuracy: 57.98
Round  42, Train loss: 1.929, Test loss: 1.901, Test accuracy: 59.80
Round  43, Train loss: 1.883, Test loss: 1.901, Test accuracy: 60.20
Round  44, Train loss: 1.846, Test loss: 1.888, Test accuracy: 61.37
Round  45, Train loss: 1.851, Test loss: 1.873, Test accuracy: 63.11
Round  46, Train loss: 1.840, Test loss: 1.863, Test accuracy: 63.73
Round  47, Train loss: 1.815, Test loss: 1.857, Test accuracy: 64.86
Round  48, Train loss: 1.864, Test loss: 1.857, Test accuracy: 65.47
Round  49, Train loss: 1.827, Test loss: 1.842, Test accuracy: 66.27
Round  50, Train loss: 1.838, Test loss: 1.840, Test accuracy: 67.63
Round  51, Train loss: 1.882, Test loss: 1.842, Test accuracy: 67.56
Round  52, Train loss: 1.827, Test loss: 1.825, Test accuracy: 68.40
Round  53, Train loss: 1.814, Test loss: 1.820, Test accuracy: 68.72
Round  54, Train loss: 1.824, Test loss: 1.809, Test accuracy: 69.53
Round  55, Train loss: 1.778, Test loss: 1.799, Test accuracy: 70.33
Round  56, Train loss: 1.780, Test loss: 1.792, Test accuracy: 71.36
Round  57, Train loss: 1.737, Test loss: 1.787, Test accuracy: 71.67
Round  58, Train loss: 1.792, Test loss: 1.784, Test accuracy: 72.02
Round  59, Train loss: 1.788, Test loss: 1.778, Test accuracy: 73.10
Round  60, Train loss: 1.775, Test loss: 1.772, Test accuracy: 73.58
Round  61, Train loss: 1.726, Test loss: 1.763, Test accuracy: 74.06
Round  62, Train loss: 1.756, Test loss: 1.759, Test accuracy: 74.41
Round  63, Train loss: 1.731, Test loss: 1.761, Test accuracy: 74.55
Round  64, Train loss: 1.731, Test loss: 1.755, Test accuracy: 74.97
Round  65, Train loss: 1.745, Test loss: 1.749, Test accuracy: 75.49
Round  66, Train loss: 1.702, Test loss: 1.741, Test accuracy: 76.03
Round  67, Train loss: 1.742, Test loss: 1.739, Test accuracy: 76.38
Round  68, Train loss: 1.749, Test loss: 1.733, Test accuracy: 76.64
Round  69, Train loss: 1.723, Test loss: 1.733, Test accuracy: 76.67
Round  70, Train loss: 1.718, Test loss: 1.731, Test accuracy: 76.80
Round  71, Train loss: 1.731, Test loss: 1.732, Test accuracy: 76.58
Round  72, Train loss: 1.741, Test loss: 1.735, Test accuracy: 76.82
Round  73, Train loss: 1.703, Test loss: 1.731, Test accuracy: 76.88
Round  74, Train loss: 1.735, Test loss: 1.724, Test accuracy: 76.86
Round  75, Train loss: 1.711, Test loss: 1.719, Test accuracy: 77.63
Round  76, Train loss: 1.689, Test loss: 1.716, Test accuracy: 77.78
Round  77, Train loss: 1.687, Test loss: 1.713, Test accuracy: 78.06
Round  78, Train loss: 1.735, Test loss: 1.715, Test accuracy: 78.01
Round  79, Train loss: 1.706, Test loss: 1.713, Test accuracy: 78.24
Round  80, Train loss: 1.688, Test loss: 1.710, Test accuracy: 78.37
Round  81, Train loss: 1.692, Test loss: 1.708, Test accuracy: 78.38
Round  82, Train loss: 1.671, Test loss: 1.705, Test accuracy: 78.67
Round  83, Train loss: 1.688, Test loss: 1.703, Test accuracy: 79.06
Round  84, Train loss: 1.688, Test loss: 1.702, Test accuracy: 79.07
Round  85, Train loss: 1.677, Test loss: 1.704, Test accuracy: 79.03
Round  86, Train loss: 1.694, Test loss: 1.704, Test accuracy: 79.36
Round  87, Train loss: 1.684, Test loss: 1.694, Test accuracy: 79.74
Round  88, Train loss: 1.692, Test loss: 1.695, Test accuracy: 79.73
Round  89, Train loss: 1.645, Test loss: 1.689, Test accuracy: 80.25
Round  90, Train loss: 1.643, Test loss: 1.689, Test accuracy: 80.21
Round  91, Train loss: 1.682, Test loss: 1.691, Test accuracy: 80.27
Round  92, Train loss: 1.695, Test loss: 1.690, Test accuracy: 80.30
Round  93, Train loss: 1.657, Test loss: 1.687, Test accuracy: 80.39/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.655, Test loss: 1.684, Test accuracy: 80.53
Round  95, Train loss: 1.654, Test loss: 1.685, Test accuracy: 80.55
Round  96, Train loss: 1.672, Test loss: 1.686, Test accuracy: 80.53
Round  97, Train loss: 1.687, Test loss: 1.681, Test accuracy: 81.03
Round  98, Train loss: 1.654, Test loss: 1.680, Test accuracy: 81.28
Round  99, Train loss: 1.629, Test loss: 1.672, Test accuracy: 81.74
Final Round, Train loss: 1.624, Test loss: 1.653, Test accuracy: 83.28
Average accuracy final 10 rounds: 80.68166666666667
1582.318754196167
[1.5072615146636963, 3.0145230293273926, 4.438404560089111, 5.86228609085083, 7.234187126159668, 8.606088161468506, 9.975659132003784, 11.345230102539062, 12.765076398849487, 14.184922695159912, 15.606828689575195, 17.02873468399048, 18.437591314315796, 19.846447944641113, 21.184406280517578, 22.522364616394043, 23.882890701293945, 25.243416786193848, 26.677436351776123, 28.1114559173584, 29.522774696350098, 30.934093475341797, 32.272321701049805, 33.61054992675781, 34.98045349121094, 36.35035705566406, 37.73804497718811, 39.12573289871216, 40.567471504211426, 42.00921010971069, 43.42875552177429, 44.84830093383789, 46.260533571243286, 47.67276620864868, 49.059569358825684, 50.446372509002686, 51.82663893699646, 53.206905364990234, 54.57327365875244, 55.93964195251465, 57.35684776306152, 58.7740535736084, 60.16901969909668, 61.56398582458496, 62.93287920951843, 64.3017725944519, 65.55464601516724, 66.80751943588257, 68.12787652015686, 69.44823360443115, 70.761150598526, 72.07406759262085, 73.32701325416565, 74.57995891571045, 75.87844896316528, 77.17693901062012, 78.457839012146, 79.73873901367188, 81.03782343864441, 82.33690786361694, 83.6111843585968, 84.88546085357666, 86.13305735588074, 87.38065385818481, 88.70247197151184, 90.02429008483887, 91.28879737854004, 92.55330467224121, 93.818434715271, 95.08356475830078, 96.35593605041504, 97.6283073425293, 98.93401646614075, 100.2397255897522, 101.50909996032715, 102.7784743309021, 104.04320812225342, 105.30794191360474, 106.55478739738464, 107.80163288116455, 109.09637665748596, 110.39112043380737, 111.72683620452881, 113.06255197525024, 114.36643505096436, 115.67031812667847, 116.93368577957153, 118.1970534324646, 119.53577637672424, 120.87449932098389, 122.23167490959167, 123.58885049819946, 124.89304208755493, 126.1972336769104, 127.49387168884277, 128.79050970077515, 130.0530755519867, 131.31564140319824, 132.69798588752747, 134.0803303718567, 135.4960150718689, 136.9116997718811, 138.31999349594116, 139.72828722000122, 141.0180184841156, 142.30774974822998, 143.6274266242981, 144.9471035003662, 146.4133939743042, 147.8796844482422, 149.29017686843872, 150.70066928863525, 152.10499334335327, 153.5093173980713, 154.83470559120178, 156.16009378433228, 157.55036544799805, 158.94063711166382, 160.4129867553711, 161.88533639907837, 163.39233422279358, 164.8993320465088, 166.41007328033447, 167.92081451416016, 169.35422778129578, 170.7876410484314, 172.23001503944397, 173.67238903045654, 175.1641936302185, 176.65599822998047, 178.1500084400177, 179.64401865005493, 181.09921073913574, 182.55440282821655, 183.96309971809387, 185.3717966079712, 186.80437016487122, 188.23694372177124, 189.6866478919983, 191.13635206222534, 192.60568761825562, 194.0750231742859, 195.56131219863892, 197.04760122299194, 198.5231113433838, 199.99862146377563, 201.49141573905945, 202.98421001434326, 204.4290108680725, 205.87381172180176, 207.30454444885254, 208.73527717590332, 210.20408844947815, 211.67289972305298, 213.1223862171173, 214.57187271118164, 216.04847621917725, 217.52507972717285, 219.02175545692444, 220.51843118667603, 221.96552968025208, 223.41262817382812, 224.865975856781, 226.3193235397339, 227.77508759498596, 229.23085165023804, 230.70973563194275, 232.18861961364746, 233.67779231071472, 235.16696500778198, 236.6235113143921, 238.0800576210022, 239.5350947380066, 240.990131855011, 242.38494229316711, 243.77975273132324, 245.19216060638428, 246.6045684814453, 248.09261322021484, 249.58065795898438, 251.06062984466553, 252.54060173034668, 253.9716019630432, 255.40260219573975, 256.82532954216003, 258.2480568885803, 259.73044753074646, 261.2128381729126, 262.70222902297974, 264.1916198730469, 265.70211911201477, 267.21261835098267, 268.6412081718445, 270.0697979927063, 271.5267586708069, 272.98371934890747, 274.45185112953186, 275.91998291015625, 277.3332664966583, 278.7465500831604, 280.54414200782776, 282.3417339324951]
[9.975, 9.975, 9.883333333333333, 9.883333333333333, 10.066666666666666, 10.066666666666666, 10.041666666666666, 10.041666666666666, 11.083333333333334, 11.083333333333334, 11.491666666666667, 11.491666666666667, 12.416666666666666, 12.416666666666666, 13.116666666666667, 13.116666666666667, 13.85, 13.85, 15.875, 15.875, 18.166666666666668, 18.166666666666668, 18.216666666666665, 18.216666666666665, 19.608333333333334, 19.608333333333334, 20.983333333333334, 20.983333333333334, 21.75, 21.75, 23.508333333333333, 23.508333333333333, 25.966666666666665, 25.966666666666665, 27.366666666666667, 27.366666666666667, 28.683333333333334, 28.683333333333334, 30.691666666666666, 30.691666666666666, 33.075, 33.075, 35.075, 35.075, 35.28333333333333, 35.28333333333333, 37.1, 37.1, 39.15833333333333, 39.15833333333333, 41.358333333333334, 41.358333333333334, 43.208333333333336, 43.208333333333336, 45.516666666666666, 45.516666666666666, 46.925, 46.925, 47.8, 47.8, 49.733333333333334, 49.733333333333334, 49.8, 49.8, 50.0, 50.0, 51.05, 51.05, 51.35, 51.35, 53.96666666666667, 53.96666666666667, 55.09166666666667, 55.09166666666667, 55.68333333333333, 55.68333333333333, 56.675, 56.675, 57.141666666666666, 57.141666666666666, 57.725, 57.725, 57.975, 57.975, 59.8, 59.8, 60.2, 60.2, 61.36666666666667, 61.36666666666667, 63.108333333333334, 63.108333333333334, 63.733333333333334, 63.733333333333334, 64.85833333333333, 64.85833333333333, 65.475, 65.475, 66.26666666666667, 66.26666666666667, 67.63333333333334, 67.63333333333334, 67.55833333333334, 67.55833333333334, 68.4, 68.4, 68.71666666666667, 68.71666666666667, 69.53333333333333, 69.53333333333333, 70.33333333333333, 70.33333333333333, 71.35833333333333, 71.35833333333333, 71.66666666666667, 71.66666666666667, 72.01666666666667, 72.01666666666667, 73.1, 73.1, 73.58333333333333, 73.58333333333333, 74.05833333333334, 74.05833333333334, 74.40833333333333, 74.40833333333333, 74.55, 74.55, 74.96666666666667, 74.96666666666667, 75.49166666666666, 75.49166666666666, 76.03333333333333, 76.03333333333333, 76.375, 76.375, 76.64166666666667, 76.64166666666667, 76.66666666666667, 76.66666666666667, 76.8, 76.8, 76.58333333333333, 76.58333333333333, 76.81666666666666, 76.81666666666666, 76.88333333333334, 76.88333333333334, 76.85833333333333, 76.85833333333333, 77.63333333333334, 77.63333333333334, 77.775, 77.775, 78.05833333333334, 78.05833333333334, 78.00833333333334, 78.00833333333334, 78.24166666666666, 78.24166666666666, 78.36666666666666, 78.36666666666666, 78.38333333333334, 78.38333333333334, 78.675, 78.675, 79.05833333333334, 79.05833333333334, 79.06666666666666, 79.06666666666666, 79.03333333333333, 79.03333333333333, 79.35833333333333, 79.35833333333333, 79.74166666666666, 79.74166666666666, 79.73333333333333, 79.73333333333333, 80.25, 80.25, 80.20833333333333, 80.20833333333333, 80.26666666666667, 80.26666666666667, 80.3, 80.3, 80.39166666666667, 80.39166666666667, 80.525, 80.525, 80.55, 80.55, 80.525, 80.525, 81.03333333333333, 81.03333333333333, 81.275, 81.275, 81.74166666666666, 81.74166666666666, 83.28333333333333, 83.28333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.202, Test loss: 2.186, Test accuracy: 29.86
Round   0, Global train loss: 2.202, Global test loss: 2.285, Global test accuracy: 18.96
Round   1, Train loss: 1.829, Test loss: 2.050, Test accuracy: 44.35
Round   1, Global train loss: 1.829, Global test loss: 2.277, Global test accuracy: 21.15
Round   2, Train loss: 1.725, Test loss: 1.929, Test accuracy: 52.98
Round   2, Global train loss: 1.725, Global test loss: 2.298, Global test accuracy: 12.60
Round   3, Train loss: 1.690, Test loss: 1.856, Test accuracy: 62.10
Round   3, Global train loss: 1.690, Global test loss: 2.255, Global test accuracy: 20.22
Round   4, Train loss: 1.595, Test loss: 1.826, Test accuracy: 64.35
Round   4, Global train loss: 1.595, Global test loss: 2.279, Global test accuracy: 17.36
Round   5, Train loss: 1.760, Test loss: 1.752, Test accuracy: 73.63
Round   5, Global train loss: 1.760, Global test loss: 2.258, Global test accuracy: 18.63
Round   6, Train loss: 1.515, Test loss: 1.738, Test accuracy: 72.66
Round   6, Global train loss: 1.515, Global test loss: 2.267, Global test accuracy: 18.08
Round   7, Train loss: 1.600, Test loss: 1.680, Test accuracy: 77.68
Round   7, Global train loss: 1.600, Global test loss: 2.302, Global test accuracy: 11.45
Round   8, Train loss: 1.620, Test loss: 1.650, Test accuracy: 81.83
Round   8, Global train loss: 1.620, Global test loss: 2.268, Global test accuracy: 18.33
Round   9, Train loss: 1.659, Test loss: 1.587, Test accuracy: 88.22
Round   9, Global train loss: 1.659, Global test loss: 2.252, Global test accuracy: 19.20
Round  10, Train loss: 1.535, Test loss: 1.585, Test accuracy: 88.28
Round  10, Global train loss: 1.535, Global test loss: 2.296, Global test accuracy: 11.72
Round  11, Train loss: 1.588, Test loss: 1.583, Test accuracy: 88.38
Round  11, Global train loss: 1.588, Global test loss: 2.268, Global test accuracy: 16.95
Round  12, Train loss: 1.503, Test loss: 1.570, Test accuracy: 89.67
Round  12, Global train loss: 1.503, Global test loss: 2.297, Global test accuracy: 12.28
Round  13, Train loss: 1.530, Test loss: 1.570, Test accuracy: 89.64
Round  13, Global train loss: 1.530, Global test loss: 2.253, Global test accuracy: 20.90
Round  14, Train loss: 1.475, Test loss: 1.570, Test accuracy: 89.57
Round  14, Global train loss: 1.475, Global test loss: 2.272, Global test accuracy: 16.30
Round  15, Train loss: 1.527, Test loss: 1.569, Test accuracy: 89.58
Round  15, Global train loss: 1.527, Global test loss: 2.295, Global test accuracy: 15.00
Round  16, Train loss: 1.581, Test loss: 1.569, Test accuracy: 89.67
Round  16, Global train loss: 1.581, Global test loss: 2.244, Global test accuracy: 20.43
Round  17, Train loss: 1.524, Test loss: 1.568, Test accuracy: 89.70
Round  17, Global train loss: 1.524, Global test loss: 2.256, Global test accuracy: 20.07
Round  18, Train loss: 1.529, Test loss: 1.566, Test accuracy: 89.88
Round  18, Global train loss: 1.529, Global test loss: 2.256, Global test accuracy: 21.18
Round  19, Train loss: 1.471, Test loss: 1.566, Test accuracy: 89.82
Round  19, Global train loss: 1.471, Global test loss: 2.285, Global test accuracy: 15.74
Round  20, Train loss: 1.525, Test loss: 1.566, Test accuracy: 89.83
Round  20, Global train loss: 1.525, Global test loss: 2.282, Global test accuracy: 13.78
Round  21, Train loss: 1.523, Test loss: 1.566, Test accuracy: 89.83
Round  21, Global train loss: 1.523, Global test loss: 2.287, Global test accuracy: 15.00
Round  22, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.85
Round  22, Global train loss: 1.522, Global test loss: 2.246, Global test accuracy: 20.78
Round  23, Train loss: 1.575, Test loss: 1.565, Test accuracy: 89.85
Round  23, Global train loss: 1.575, Global test loss: 2.238, Global test accuracy: 18.28
Round  24, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.92
Round  24, Global train loss: 1.522, Global test loss: 2.257, Global test accuracy: 18.03
Round  25, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.92
Round  25, Global train loss: 1.522, Global test loss: 2.266, Global test accuracy: 18.14
Round  26, Train loss: 1.466, Test loss: 1.565, Test accuracy: 89.91
Round  26, Global train loss: 1.466, Global test loss: 2.293, Global test accuracy: 10.86
Round  27, Train loss: 1.523, Test loss: 1.564, Test accuracy: 89.94
Round  27, Global train loss: 1.523, Global test loss: 2.276, Global test accuracy: 17.33
Round  28, Train loss: 1.466, Test loss: 1.564, Test accuracy: 89.93
Round  28, Global train loss: 1.466, Global test loss: 2.255, Global test accuracy: 18.24
Round  29, Train loss: 1.576, Test loss: 1.564, Test accuracy: 89.92
Round  29, Global train loss: 1.576, Global test loss: 2.251, Global test accuracy: 19.61
Round  30, Train loss: 1.578, Test loss: 1.564, Test accuracy: 89.91
Round  30, Global train loss: 1.578, Global test loss: 2.239, Global test accuracy: 21.60
Round  31, Train loss: 1.467, Test loss: 1.564, Test accuracy: 89.91
Round  31, Global train loss: 1.467, Global test loss: 2.278, Global test accuracy: 12.69
Round  32, Train loss: 1.575, Test loss: 1.564, Test accuracy: 89.90
Round  32, Global train loss: 1.575, Global test loss: 2.257, Global test accuracy: 19.77
Round  33, Train loss: 1.518, Test loss: 1.564, Test accuracy: 89.90
Round  33, Global train loss: 1.518, Global test loss: 2.272, Global test accuracy: 17.21
Round  34, Train loss: 1.576, Test loss: 1.564, Test accuracy: 89.92
Round  34, Global train loss: 1.576, Global test loss: 2.253, Global test accuracy: 18.16
Round  35, Train loss: 1.575, Test loss: 1.564, Test accuracy: 89.91
Round  35, Global train loss: 1.575, Global test loss: 2.236, Global test accuracy: 21.46
Round  36, Train loss: 1.521, Test loss: 1.564, Test accuracy: 89.91
Round  36, Global train loss: 1.521, Global test loss: 2.286, Global test accuracy: 14.16
Round  37, Train loss: 1.573, Test loss: 1.564, Test accuracy: 89.89
Round  37, Global train loss: 1.573, Global test loss: 2.235, Global test accuracy: 20.63
Round  38, Train loss: 1.628, Test loss: 1.564, Test accuracy: 89.88
Round  38, Global train loss: 1.628, Global test loss: 2.284, Global test accuracy: 15.32
Round  39, Train loss: 1.574, Test loss: 1.564, Test accuracy: 89.90
Round  39, Global train loss: 1.574, Global test loss: 2.245, Global test accuracy: 21.67
Round  40, Train loss: 1.522, Test loss: 1.564, Test accuracy: 89.88
Round  40, Global train loss: 1.522, Global test loss: 2.277, Global test accuracy: 14.54
Round  41, Train loss: 1.466, Test loss: 1.564, Test accuracy: 89.91
Round  41, Global train loss: 1.466, Global test loss: 2.264, Global test accuracy: 18.34
Round  42, Train loss: 1.520, Test loss: 1.564, Test accuracy: 89.90
Round  42, Global train loss: 1.520, Global test loss: 2.258, Global test accuracy: 19.98
Round  43, Train loss: 1.522, Test loss: 1.564, Test accuracy: 89.88
Round  43, Global train loss: 1.522, Global test loss: 2.274, Global test accuracy: 14.21
Round  44, Train loss: 1.497, Test loss: 1.550, Test accuracy: 91.38
Round  44, Global train loss: 1.497, Global test loss: 2.254, Global test accuracy: 20.28
Round  45, Train loss: 1.577, Test loss: 1.549, Test accuracy: 91.42
Round  45, Global train loss: 1.577, Global test loss: 2.230, Global test accuracy: 21.69
Round  46, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.39
Round  46, Global train loss: 1.519, Global test loss: 2.288, Global test accuracy: 14.17
Round  47, Train loss: 1.574, Test loss: 1.549, Test accuracy: 91.38
Round  47, Global train loss: 1.574, Global test loss: 2.300, Global test accuracy: 15.02
Round  48, Train loss: 1.574, Test loss: 1.549, Test accuracy: 91.38
Round  48, Global train loss: 1.574, Global test loss: 2.259, Global test accuracy: 18.48
Round  49, Train loss: 1.574, Test loss: 1.549, Test accuracy: 91.38
Round  49, Global train loss: 1.574, Global test loss: 2.269, Global test accuracy: 14.96
Round  50, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.31
Round  50, Global train loss: 1.520, Global test loss: 2.264, Global test accuracy: 18.07
Round  51, Train loss: 1.466, Test loss: 1.549, Test accuracy: 91.32
Round  51, Global train loss: 1.466, Global test loss: 2.260, Global test accuracy: 17.98
Round  52, Train loss: 1.465, Test loss: 1.549, Test accuracy: 91.34
Round  52, Global train loss: 1.465, Global test loss: 2.272, Global test accuracy: 18.24
Round  53, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.34
Round  53, Global train loss: 1.518, Global test loss: 2.247, Global test accuracy: 20.16
Round  54, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.35
Round  54, Global train loss: 1.520, Global test loss: 2.269, Global test accuracy: 18.33
Round  55, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.31
Round  55, Global train loss: 1.518, Global test loss: 2.253, Global test accuracy: 20.01
Round  56, Train loss: 1.465, Test loss: 1.549, Test accuracy: 91.31
Round  56, Global train loss: 1.465, Global test loss: 2.247, Global test accuracy: 20.11
Round  57, Train loss: 1.575, Test loss: 1.549, Test accuracy: 91.31
Round  57, Global train loss: 1.575, Global test loss: 2.288, Global test accuracy: 11.92
Round  58, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.31
Round  58, Global train loss: 1.520, Global test loss: 2.263, Global test accuracy: 16.80
Round  59, Train loss: 1.467, Test loss: 1.549, Test accuracy: 91.31
Round  59, Global train loss: 1.467, Global test loss: 2.336, Global test accuracy: 9.62
Round  60, Train loss: 1.572, Test loss: 1.549, Test accuracy: 91.29
Round  60, Global train loss: 1.572, Global test loss: 2.265, Global test accuracy: 18.33
Round  61, Train loss: 1.464, Test loss: 1.549, Test accuracy: 91.30
Round  61, Global train loss: 1.464, Global test loss: 2.254, Global test accuracy: 18.50
Round  62, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.32
Round  62, Global train loss: 1.519, Global test loss: 2.268, Global test accuracy: 17.03
Round  63, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  63, Global train loss: 1.519, Global test loss: 2.251, Global test accuracy: 20.95
Round  64, Train loss: 1.464, Test loss: 1.549, Test accuracy: 91.33
Round  64, Global train loss: 1.464, Global test loss: 2.238, Global test accuracy: 20.61
Round  65, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.33
Round  65, Global train loss: 1.520, Global test loss: 2.257, Global test accuracy: 16.43
Round  66, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.34
Round  66, Global train loss: 1.520, Global test loss: 2.275, Global test accuracy: 18.33
Round  67, Train loss: 1.465, Test loss: 1.549, Test accuracy: 91.33
Round  67, Global train loss: 1.465, Global test loss: 2.275, Global test accuracy: 15.95
Round  68, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.34
Round  68, Global train loss: 1.520, Global test loss: 2.226, Global test accuracy: 22.01
Round  69, Train loss: 1.572, Test loss: 1.549, Test accuracy: 91.32
Round  69, Global train loss: 1.572, Global test loss: 2.269, Global test accuracy: 16.62
Round  70, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  70, Global train loss: 1.519, Global test loss: 2.245, Global test accuracy: 19.98
Round  71, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  71, Global train loss: 1.519, Global test loss: 2.278, Global test accuracy: 13.63
Round  72, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.33
Round  72, Global train loss: 1.519, Global test loss: 2.272, Global test accuracy: 14.30
Round  73, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.35
Round  73, Global train loss: 1.519, Global test loss: 2.317, Global test accuracy: 11.68
Round  74, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.37
Round  74, Global train loss: 1.518, Global test loss: 2.266, Global test accuracy: 18.40
Round  75, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.37
Round  75, Global train loss: 1.519, Global test loss: 2.256, Global test accuracy: 21.62
Round  76, Train loss: 1.466, Test loss: 1.549, Test accuracy: 91.38
Round  76, Global train loss: 1.466, Global test loss: 2.225, Global test accuracy: 23.02
Round  77, Train loss: 1.464, Test loss: 1.549, Test accuracy: 91.39
Round  77, Global train loss: 1.464, Global test loss: 2.264, Global test accuracy: 17.50
Round  78, Train loss: 1.519, Test loss: 1.549, Test accuracy: 91.38
Round  78, Global train loss: 1.519, Global test loss: 2.240, Global test accuracy: 21.10
Round  79, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.38
Round  79, Global train loss: 1.518, Global test loss: 2.300, Global test accuracy: 12.75
Round  80, Train loss: 1.520, Test loss: 1.549, Test accuracy: 91.39
Round  80, Global train loss: 1.520, Global test loss: 2.249, Global test accuracy: 18.10
Round  81, Train loss: 1.518, Test loss: 1.549, Test accuracy: 91.40
Round  81, Global train loss: 1.518, Global test loss: 2.298, Global test accuracy: 11.81
Round  82, Train loss: 1.465, Test loss: 1.548, Test accuracy: 91.41
Round  82, Global train loss: 1.465, Global test loss: 2.255, Global test accuracy: 18.09
Round  83, Train loss: 1.517, Test loss: 1.548, Test accuracy: 91.41
Round  83, Global train loss: 1.517, Global test loss: 2.236, Global test accuracy: 22.18
Round  84, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.40
Round  84, Global train loss: 1.464, Global test loss: 2.258, Global test accuracy: 17.77
Round  85, Train loss: 1.572, Test loss: 1.548, Test accuracy: 91.40
Round  85, Global train loss: 1.572, Global test loss: 2.244, Global test accuracy: 19.50
Round  86, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.39
Round  86, Global train loss: 1.519, Global test loss: 2.285, Global test accuracy: 12.95
Round  87, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.39
Round  87, Global train loss: 1.464, Global test loss: 2.243, Global test accuracy: 19.88
Round  88, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.39
Round  88, Global train loss: 1.464, Global test loss: 2.294, Global test accuracy: 12.58
Round  89, Train loss: 1.572, Test loss: 1.548, Test accuracy: 91.39
Round  89, Global train loss: 1.572, Global test loss: 2.299, Global test accuracy: 12.08
Round  90, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.39
Round  90, Global train loss: 1.519, Global test loss: 2.262, Global test accuracy: 17.61
Round  91, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.39
Round  91, Global train loss: 1.519, Global test loss: 2.280, Global test accuracy: 15.40
Round  92, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.38
Round  92, Global train loss: 1.519, Global test loss: 2.285, Global test accuracy: 12.82
Round  93, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.38
Round  93, Global train loss: 1.464, Global test loss: 2.260, Global test accuracy: 19.06
Round  94, Train loss: 1.464, Test loss: 1.548, Test accuracy: 91.38
Round  94, Global train loss: 1.464, Global test loss: 2.245, Global test accuracy: 20.23
Round  95, Train loss: 1.465, Test loss: 1.548, Test accuracy: 91.37
Round  95, Global train loss: 1.465, Global test loss: 2.269, Global test accuracy: 16.53/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.466, Test loss: 1.548, Test accuracy: 91.38
Round  96, Global train loss: 1.466, Global test loss: 2.220, Global test accuracy: 23.64
Round  97, Train loss: 1.465, Test loss: 1.548, Test accuracy: 91.38
Round  97, Global train loss: 1.465, Global test loss: 2.269, Global test accuracy: 16.41
Round  98, Train loss: 1.466, Test loss: 1.548, Test accuracy: 91.38
Round  98, Global train loss: 1.466, Global test loss: 2.298, Global test accuracy: 12.98
Round  99, Train loss: 1.573, Test loss: 1.548, Test accuracy: 91.38
Round  99, Global train loss: 1.573, Global test loss: 2.285, Global test accuracy: 13.40
Final Round, Train loss: 1.512, Test loss: 1.548, Test accuracy: 91.38
Final Round, Global train loss: 1.512, Global test loss: 2.285, Global test accuracy: 13.40
Average accuracy final 10 rounds: 91.38000000000001 

Average global accuracy final 10 rounds: 16.807500000000005 

1631.7130942344666
[1.2457003593444824, 2.491400718688965, 3.6576008796691895, 4.823801040649414, 6.010705471038818, 7.197609901428223, 8.359447956085205, 9.521286010742188, 10.663824319839478, 11.806362628936768, 12.956706762313843, 14.107050895690918, 15.31734848022461, 16.5276460647583, 17.75052046775818, 18.973394870758057, 20.012800216674805, 21.052205562591553, 22.042078256607056, 23.03195095062256, 24.01418972015381, 24.99642848968506, 25.93848967552185, 26.880550861358643, 27.835181713104248, 28.789812564849854, 29.81527280807495, 30.84073305130005, 31.818371295928955, 32.79600954055786, 33.80137228965759, 34.806735038757324, 35.82373833656311, 36.8407416343689, 37.841289043426514, 38.84183645248413, 39.84019422531128, 40.83855199813843, 41.83521127700806, 42.831870555877686, 43.749155044555664, 44.66643953323364, 45.660829305648804, 46.655219078063965, 47.626707315444946, 48.59819555282593, 49.57623052597046, 50.55426549911499, 51.538228034973145, 52.5221905708313, 53.47016382217407, 54.418137073516846, 55.366520404815674, 56.3149037361145, 57.31104397773743, 58.30718421936035, 59.29261636734009, 60.278048515319824, 61.21817660331726, 62.1583046913147, 63.11991548538208, 64.08152627944946, 65.00761485099792, 65.93370342254639, 66.87641525268555, 67.8191270828247, 68.72983026504517, 69.64053344726562, 70.58184599876404, 71.52315855026245, 72.51624608039856, 73.50933361053467, 74.502925157547, 75.49651670455933, 76.48390555381775, 77.47129440307617, 78.41578769683838, 79.36028099060059, 80.32955718040466, 81.29883337020874, 82.22613549232483, 83.15343761444092, 84.08962845802307, 85.02581930160522, 86.0851263999939, 87.14443349838257, 88.21786069869995, 89.29128789901733, 90.38836932182312, 91.4854507446289, 92.52872490882874, 93.57199907302856, 94.69011998176575, 95.80824089050293, 96.97237038612366, 98.13649988174438, 99.12418842315674, 100.11187696456909, 101.07344889640808, 102.03502082824707, 102.97938704490662, 103.92375326156616, 104.90758800506592, 105.89142274856567, 106.8117105960846, 107.73199844360352, 108.7142903804779, 109.6965823173523, 110.77203464508057, 111.84748697280884, 112.78571367263794, 113.72394037246704, 114.70144319534302, 115.678946018219, 116.63282799720764, 117.58670997619629, 118.55898404121399, 119.53125810623169, 120.52501726150513, 121.51877641677856, 122.45678472518921, 123.39479303359985, 124.28757667541504, 125.18036031723022, 126.17883086204529, 127.17730140686035, 128.1195478439331, 129.06179428100586, 129.9677517414093, 130.87370920181274, 131.80162167549133, 132.72953414916992, 133.71186542510986, 134.6941967010498, 135.61942672729492, 136.54465675354004, 137.4474229812622, 138.35018920898438, 139.33553791046143, 140.32088661193848, 141.27248978614807, 142.22409296035767, 143.2006540298462, 144.17721509933472, 145.13422274589539, 146.09123039245605, 147.0154104232788, 147.93959045410156, 148.8870701789856, 149.83454990386963, 150.80927968025208, 151.78400945663452, 152.74101400375366, 153.6980185508728, 154.67532587051392, 155.65263319015503, 156.5928156375885, 157.53299808502197, 158.50857424736023, 159.4841504096985, 160.46735000610352, 161.45054960250854, 162.36272072792053, 163.27489185333252, 164.25260210037231, 165.2303123474121, 166.1806640625, 167.1310157775879, 168.04899215698242, 168.96696853637695, 169.85783624649048, 170.748703956604, 171.7701563835144, 172.7916088104248, 173.77931189537048, 174.76701498031616, 175.6892068386078, 176.6113986968994, 177.6022083759308, 178.59301805496216, 179.57729506492615, 180.56157207489014, 181.5102505683899, 182.45892906188965, 183.4091100692749, 184.35929107666016, 185.32145762443542, 186.2836241722107, 187.24010682106018, 188.19658946990967, 189.18967413902283, 190.182758808136, 191.10171222686768, 192.02066564559937, 192.9783968925476, 193.93612813949585, 194.92531633377075, 195.91450452804565, 196.880211353302, 197.84591817855835, 199.50356101989746, 201.16120386123657]
[29.858333333333334, 29.858333333333334, 44.35, 44.35, 52.975, 52.975, 62.1, 62.1, 64.35, 64.35, 73.63333333333334, 73.63333333333334, 72.65833333333333, 72.65833333333333, 77.68333333333334, 77.68333333333334, 81.83333333333333, 81.83333333333333, 88.21666666666667, 88.21666666666667, 88.28333333333333, 88.28333333333333, 88.375, 88.375, 89.66666666666667, 89.66666666666667, 89.64166666666667, 89.64166666666667, 89.56666666666666, 89.56666666666666, 89.575, 89.575, 89.66666666666667, 89.66666666666667, 89.7, 89.7, 89.88333333333334, 89.88333333333334, 89.81666666666666, 89.81666666666666, 89.83333333333333, 89.83333333333333, 89.825, 89.825, 89.85, 89.85, 89.85, 89.85, 89.91666666666667, 89.91666666666667, 89.91666666666667, 89.91666666666667, 89.90833333333333, 89.90833333333333, 89.94166666666666, 89.94166666666666, 89.93333333333334, 89.93333333333334, 89.925, 89.925, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.9, 89.9, 89.9, 89.9, 89.91666666666667, 89.91666666666667, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.90833333333333, 89.89166666666667, 89.89166666666667, 89.88333333333334, 89.88333333333334, 89.9, 89.9, 89.88333333333334, 89.88333333333334, 89.90833333333333, 89.90833333333333, 89.9, 89.9, 89.875, 89.875, 91.375, 91.375, 91.41666666666667, 91.41666666666667, 91.39166666666667, 91.39166666666667, 91.375, 91.375, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.30833333333334, 91.30833333333334, 91.31666666666666, 91.31666666666666, 91.34166666666667, 91.34166666666667, 91.34166666666667, 91.34166666666667, 91.35, 91.35, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.30833333333334, 91.29166666666667, 91.29166666666667, 91.3, 91.3, 91.31666666666666, 91.31666666666666, 91.325, 91.325, 91.325, 91.325, 91.325, 91.325, 91.34166666666667, 91.34166666666667, 91.325, 91.325, 91.34166666666667, 91.34166666666667, 91.31666666666666, 91.31666666666666, 91.325, 91.325, 91.325, 91.325, 91.33333333333333, 91.33333333333333, 91.35, 91.35, 91.36666666666666, 91.36666666666666, 91.36666666666666, 91.36666666666666, 91.38333333333334, 91.38333333333334, 91.39166666666667, 91.39166666666667, 91.375, 91.375, 91.38333333333334, 91.38333333333334, 91.39166666666667, 91.39166666666667, 91.4, 91.4, 91.40833333333333, 91.40833333333333, 91.40833333333333, 91.40833333333333, 91.4, 91.4, 91.4, 91.4, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.39166666666667, 91.38333333333334, 91.38333333333334, 91.375, 91.375, 91.375, 91.375, 91.36666666666666, 91.36666666666666, 91.375, 91.375, 91.375, 91.375, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.273, Test loss: 2.263, Test accuracy: 18.37
Round   0, Global train loss: 2.273, Global test loss: 2.301, Global test accuracy: 11.79
Round   1, Train loss: 2.039, Test loss: 2.146, Test accuracy: 31.84
Round   1, Global train loss: 2.039, Global test loss: 2.300, Global test accuracy: 12.26
Round   2, Train loss: 1.952, Test loss: 2.069, Test accuracy: 39.28
Round   2, Global train loss: 1.952, Global test loss: 2.308, Global test accuracy: 11.91
Round   3, Train loss: 2.002, Test loss: 2.025, Test accuracy: 43.76
Round   3, Global train loss: 2.002, Global test loss: 2.313, Global test accuracy: 12.67
Round   4, Train loss: 1.955, Test loss: 1.980, Test accuracy: 48.29
Round   4, Global train loss: 1.955, Global test loss: 2.305, Global test accuracy: 12.95
Round   5, Train loss: 1.952, Test loss: 1.939, Test accuracy: 52.74
Round   5, Global train loss: 1.952, Global test loss: 2.300, Global test accuracy: 13.81
Round   6, Train loss: 2.024, Test loss: 1.944, Test accuracy: 52.08
Round   6, Global train loss: 2.024, Global test loss: 2.302, Global test accuracy: 13.26
Round   7, Train loss: 1.984, Test loss: 1.936, Test accuracy: 52.82
Round   7, Global train loss: 1.984, Global test loss: 2.307, Global test accuracy: 12.65
Round   8, Train loss: 1.951, Test loss: 1.932, Test accuracy: 53.16
Round   8, Global train loss: 1.951, Global test loss: 2.307, Global test accuracy: 13.00
Round   9, Train loss: 1.900, Test loss: 1.935, Test accuracy: 52.83
Round   9, Global train loss: 1.900, Global test loss: 2.315, Global test accuracy: 11.78
Round  10, Train loss: 1.961, Test loss: 1.928, Test accuracy: 53.02
Round  10, Global train loss: 1.961, Global test loss: 2.301, Global test accuracy: 12.91
Round  11, Train loss: 1.893, Test loss: 1.916, Test accuracy: 54.25
Round  11, Global train loss: 1.893, Global test loss: 2.292, Global test accuracy: 14.25
Round  12, Train loss: 1.895, Test loss: 1.899, Test accuracy: 55.94
Round  12, Global train loss: 1.895, Global test loss: 2.297, Global test accuracy: 13.94
Round  13, Train loss: 1.883, Test loss: 1.873, Test accuracy: 58.62
Round  13, Global train loss: 1.883, Global test loss: 2.295, Global test accuracy: 14.14
Round  14, Train loss: 1.855, Test loss: 1.879, Test accuracy: 57.91
Round  14, Global train loss: 1.855, Global test loss: 2.289, Global test accuracy: 14.71
Round  15, Train loss: 1.928, Test loss: 1.880, Test accuracy: 57.71
Round  15, Global train loss: 1.928, Global test loss: 2.292, Global test accuracy: 14.22
Round  16, Train loss: 1.852, Test loss: 1.854, Test accuracy: 60.55
Round  16, Global train loss: 1.852, Global test loss: 2.278, Global test accuracy: 16.20
Round  17, Train loss: 1.868, Test loss: 1.857, Test accuracy: 60.16
Round  17, Global train loss: 1.868, Global test loss: 2.291, Global test accuracy: 14.51
Round  18, Train loss: 1.832, Test loss: 1.851, Test accuracy: 60.80
Round  18, Global train loss: 1.832, Global test loss: 2.287, Global test accuracy: 15.43
Round  19, Train loss: 1.838, Test loss: 1.856, Test accuracy: 60.24
Round  19, Global train loss: 1.838, Global test loss: 2.285, Global test accuracy: 15.32
Round  20, Train loss: 1.909, Test loss: 1.845, Test accuracy: 61.30
Round  20, Global train loss: 1.909, Global test loss: 2.282, Global test accuracy: 15.44
Round  21, Train loss: 1.851, Test loss: 1.844, Test accuracy: 61.42
Round  21, Global train loss: 1.851, Global test loss: 2.285, Global test accuracy: 15.37
Round  22, Train loss: 1.832, Test loss: 1.848, Test accuracy: 60.93
Round  22, Global train loss: 1.832, Global test loss: 2.282, Global test accuracy: 15.68
Round  23, Train loss: 1.881, Test loss: 1.841, Test accuracy: 61.69
Round  23, Global train loss: 1.881, Global test loss: 2.285, Global test accuracy: 15.30
Round  24, Train loss: 1.907, Test loss: 1.847, Test accuracy: 60.99
Round  24, Global train loss: 1.907, Global test loss: 2.284, Global test accuracy: 15.22
Round  25, Train loss: 1.834, Test loss: 1.846, Test accuracy: 60.99
Round  25, Global train loss: 1.834, Global test loss: 2.284, Global test accuracy: 15.53
Round  26, Train loss: 1.846, Test loss: 1.842, Test accuracy: 61.41
Round  26, Global train loss: 1.846, Global test loss: 2.293, Global test accuracy: 13.88
Round  27, Train loss: 1.767, Test loss: 1.826, Test accuracy: 63.12
Round  27, Global train loss: 1.767, Global test loss: 2.278, Global test accuracy: 16.29
Round  28, Train loss: 1.836, Test loss: 1.822, Test accuracy: 63.53
Round  28, Global train loss: 1.836, Global test loss: 2.284, Global test accuracy: 15.61
Round  29, Train loss: 1.809, Test loss: 1.802, Test accuracy: 65.62
Round  29, Global train loss: 1.809, Global test loss: 2.281, Global test accuracy: 15.18
Round  30, Train loss: 1.800, Test loss: 1.796, Test accuracy: 66.27
Round  30, Global train loss: 1.800, Global test loss: 2.286, Global test accuracy: 15.29
Round  31, Train loss: 1.812, Test loss: 1.783, Test accuracy: 67.54
Round  31, Global train loss: 1.812, Global test loss: 2.283, Global test accuracy: 15.33
Round  32, Train loss: 1.873, Test loss: 1.794, Test accuracy: 66.47
Round  32, Global train loss: 1.873, Global test loss: 2.282, Global test accuracy: 15.62
Round  33, Train loss: 1.763, Test loss: 1.789, Test accuracy: 67.02
Round  33, Global train loss: 1.763, Global test loss: 2.286, Global test accuracy: 15.17
Round  34, Train loss: 1.722, Test loss: 1.776, Test accuracy: 68.29
Round  34, Global train loss: 1.722, Global test loss: 2.295, Global test accuracy: 14.73
Round  35, Train loss: 1.755, Test loss: 1.772, Test accuracy: 68.69
Round  35, Global train loss: 1.755, Global test loss: 2.294, Global test accuracy: 14.12
Round  36, Train loss: 1.739, Test loss: 1.769, Test accuracy: 69.02
Round  36, Global train loss: 1.739, Global test loss: 2.288, Global test accuracy: 14.57
Round  37, Train loss: 1.840, Test loss: 1.769, Test accuracy: 68.99
Round  37, Global train loss: 1.840, Global test loss: 2.294, Global test accuracy: 13.62
Round  38, Train loss: 1.752, Test loss: 1.768, Test accuracy: 69.13
Round  38, Global train loss: 1.752, Global test loss: 2.283, Global test accuracy: 14.92
Round  39, Train loss: 1.811, Test loss: 1.753, Test accuracy: 70.69
Round  39, Global train loss: 1.811, Global test loss: 2.284, Global test accuracy: 15.14
Round  40, Train loss: 1.777, Test loss: 1.753, Test accuracy: 70.72
Round  40, Global train loss: 1.777, Global test loss: 2.295, Global test accuracy: 14.31
Round  41, Train loss: 1.729, Test loss: 1.747, Test accuracy: 71.30
Round  41, Global train loss: 1.729, Global test loss: 2.285, Global test accuracy: 15.25
Round  42, Train loss: 1.751, Test loss: 1.748, Test accuracy: 71.20
Round  42, Global train loss: 1.751, Global test loss: 2.281, Global test accuracy: 15.64
Round  43, Train loss: 1.761, Test loss: 1.747, Test accuracy: 71.20
Round  43, Global train loss: 1.761, Global test loss: 2.278, Global test accuracy: 15.81
Round  44, Train loss: 1.678, Test loss: 1.747, Test accuracy: 71.17
Round  44, Global train loss: 1.678, Global test loss: 2.279, Global test accuracy: 15.82
Round  45, Train loss: 1.707, Test loss: 1.743, Test accuracy: 71.65
Round  45, Global train loss: 1.707, Global test loss: 2.290, Global test accuracy: 14.55
Round  46, Train loss: 1.715, Test loss: 1.743, Test accuracy: 71.58
Round  46, Global train loss: 1.715, Global test loss: 2.288, Global test accuracy: 15.15
Round  47, Train loss: 1.811, Test loss: 1.747, Test accuracy: 71.12
Round  47, Global train loss: 1.811, Global test loss: 2.277, Global test accuracy: 15.95
Round  48, Train loss: 1.812, Test loss: 1.751, Test accuracy: 70.69
Round  48, Global train loss: 1.812, Global test loss: 2.297, Global test accuracy: 13.28
Round  49, Train loss: 1.741, Test loss: 1.742, Test accuracy: 71.69
Round  49, Global train loss: 1.741, Global test loss: 2.282, Global test accuracy: 15.02
Round  50, Train loss: 1.708, Test loss: 1.746, Test accuracy: 71.22
Round  50, Global train loss: 1.708, Global test loss: 2.277, Global test accuracy: 16.09
Round  51, Train loss: 1.743, Test loss: 1.734, Test accuracy: 72.45
Round  51, Global train loss: 1.743, Global test loss: 2.290, Global test accuracy: 14.14
Round  52, Train loss: 1.727, Test loss: 1.727, Test accuracy: 73.23
Round  52, Global train loss: 1.727, Global test loss: 2.272, Global test accuracy: 16.62
Round  53, Train loss: 1.672, Test loss: 1.726, Test accuracy: 73.35
Round  53, Global train loss: 1.672, Global test loss: 2.283, Global test accuracy: 15.02
Round  54, Train loss: 1.721, Test loss: 1.725, Test accuracy: 73.39
Round  54, Global train loss: 1.721, Global test loss: 2.289, Global test accuracy: 14.46
Round  55, Train loss: 1.753, Test loss: 1.721, Test accuracy: 73.78
Round  55, Global train loss: 1.753, Global test loss: 2.289, Global test accuracy: 14.97
Round  56, Train loss: 1.714, Test loss: 1.719, Test accuracy: 73.96
Round  56, Global train loss: 1.714, Global test loss: 2.301, Global test accuracy: 13.82
Round  57, Train loss: 1.774, Test loss: 1.720, Test accuracy: 73.92
Round  57, Global train loss: 1.774, Global test loss: 2.280, Global test accuracy: 15.90
Round  58, Train loss: 1.773, Test loss: 1.718, Test accuracy: 74.04
Round  58, Global train loss: 1.773, Global test loss: 2.302, Global test accuracy: 12.33
Round  59, Train loss: 1.663, Test loss: 1.718, Test accuracy: 74.07
Round  59, Global train loss: 1.663, Global test loss: 2.290, Global test accuracy: 14.90
Round  60, Train loss: 1.755, Test loss: 1.719, Test accuracy: 74.02
Round  60, Global train loss: 1.755, Global test loss: 2.290, Global test accuracy: 14.37
Round  61, Train loss: 1.651, Test loss: 1.713, Test accuracy: 74.58
Round  61, Global train loss: 1.651, Global test loss: 2.277, Global test accuracy: 16.25
Round  62, Train loss: 1.710, Test loss: 1.715, Test accuracy: 74.39
Round  62, Global train loss: 1.710, Global test loss: 2.286, Global test accuracy: 15.72
Round  63, Train loss: 1.714, Test loss: 1.719, Test accuracy: 73.92
Round  63, Global train loss: 1.714, Global test loss: 2.288, Global test accuracy: 14.61
Round  64, Train loss: 1.698, Test loss: 1.716, Test accuracy: 74.30
Round  64, Global train loss: 1.698, Global test loss: 2.295, Global test accuracy: 14.24
Round  65, Train loss: 1.724, Test loss: 1.714, Test accuracy: 74.53
Round  65, Global train loss: 1.724, Global test loss: 2.284, Global test accuracy: 15.32
Round  66, Train loss: 1.769, Test loss: 1.710, Test accuracy: 74.97
Round  66, Global train loss: 1.769, Global test loss: 2.287, Global test accuracy: 15.00
Round  67, Train loss: 1.735, Test loss: 1.712, Test accuracy: 74.70
Round  67, Global train loss: 1.735, Global test loss: 2.286, Global test accuracy: 14.94
Round  68, Train loss: 1.741, Test loss: 1.709, Test accuracy: 75.11
Round  68, Global train loss: 1.741, Global test loss: 2.279, Global test accuracy: 15.63
Round  69, Train loss: 1.691, Test loss: 1.709, Test accuracy: 75.09
Round  69, Global train loss: 1.691, Global test loss: 2.291, Global test accuracy: 15.02
Round  70, Train loss: 1.665, Test loss: 1.713, Test accuracy: 74.69
Round  70, Global train loss: 1.665, Global test loss: 2.282, Global test accuracy: 15.81
Round  71, Train loss: 1.683, Test loss: 1.712, Test accuracy: 74.72
Round  71, Global train loss: 1.683, Global test loss: 2.296, Global test accuracy: 14.12
Round  72, Train loss: 1.728, Test loss: 1.720, Test accuracy: 73.88
Round  72, Global train loss: 1.728, Global test loss: 2.295, Global test accuracy: 13.94
Round  73, Train loss: 1.754, Test loss: 1.712, Test accuracy: 74.73
Round  73, Global train loss: 1.754, Global test loss: 2.293, Global test accuracy: 14.77
Round  74, Train loss: 1.687, Test loss: 1.712, Test accuracy: 74.73
Round  74, Global train loss: 1.687, Global test loss: 2.284, Global test accuracy: 14.99
Round  75, Train loss: 1.708, Test loss: 1.707, Test accuracy: 75.19
Round  75, Global train loss: 1.708, Global test loss: 2.276, Global test accuracy: 16.35
Round  76, Train loss: 1.690, Test loss: 1.707, Test accuracy: 75.22
Round  76, Global train loss: 1.690, Global test loss: 2.276, Global test accuracy: 16.52
Round  77, Train loss: 1.680, Test loss: 1.708, Test accuracy: 75.13
Round  77, Global train loss: 1.680, Global test loss: 2.274, Global test accuracy: 16.26
Round  78, Train loss: 1.658, Test loss: 1.703, Test accuracy: 75.64
Round  78, Global train loss: 1.658, Global test loss: 2.281, Global test accuracy: 15.74
Round  79, Train loss: 1.701, Test loss: 1.699, Test accuracy: 76.03
Round  79, Global train loss: 1.701, Global test loss: 2.284, Global test accuracy: 15.14
Round  80, Train loss: 1.642, Test loss: 1.704, Test accuracy: 75.55
Round  80, Global train loss: 1.642, Global test loss: 2.282, Global test accuracy: 15.79
Round  81, Train loss: 1.739, Test loss: 1.705, Test accuracy: 75.40
Round  81, Global train loss: 1.739, Global test loss: 2.287, Global test accuracy: 14.61
Round  82, Train loss: 1.692, Test loss: 1.697, Test accuracy: 76.33
Round  82, Global train loss: 1.692, Global test loss: 2.284, Global test accuracy: 15.53
Round  83, Train loss: 1.657, Test loss: 1.696, Test accuracy: 76.33
Round  83, Global train loss: 1.657, Global test loss: 2.288, Global test accuracy: 15.28
Round  84, Train loss: 1.693, Test loss: 1.696, Test accuracy: 76.34
Round  84, Global train loss: 1.693, Global test loss: 2.289, Global test accuracy: 15.16
Round  85, Train loss: 1.656, Test loss: 1.692, Test accuracy: 76.77
Round  85, Global train loss: 1.656, Global test loss: 2.297, Global test accuracy: 13.78
Round  86, Train loss: 1.673, Test loss: 1.687, Test accuracy: 77.28
Round  86, Global train loss: 1.673, Global test loss: 2.284, Global test accuracy: 15.03
Round  87, Train loss: 1.700, Test loss: 1.682, Test accuracy: 77.74
Round  87, Global train loss: 1.700, Global test loss: 2.285, Global test accuracy: 15.10
Round  88, Train loss: 1.675, Test loss: 1.683, Test accuracy: 77.67
Round  88, Global train loss: 1.675, Global test loss: 2.292, Global test accuracy: 14.81
Round  89, Train loss: 1.661, Test loss: 1.678, Test accuracy: 78.18
Round  89, Global train loss: 1.661, Global test loss: 2.295, Global test accuracy: 13.79
Round  90, Train loss: 1.736, Test loss: 1.683, Test accuracy: 77.70
Round  90, Global train loss: 1.736, Global test loss: 2.288, Global test accuracy: 14.53
Round  91, Train loss: 1.715, Test loss: 1.690, Test accuracy: 76.96
Round  91, Global train loss: 1.715, Global test loss: 2.275, Global test accuracy: 16.19
Round  92, Train loss: 1.675, Test loss: 1.687, Test accuracy: 77.28
Round  92, Global train loss: 1.675, Global test loss: 2.285, Global test accuracy: 15.48
Round  93, Train loss: 1.672, Test loss: 1.682, Test accuracy: 77.81
Round  93, Global train loss: 1.672, Global test loss: 2.298, Global test accuracy: 13.95
Round  94, Train loss: 1.658, Test loss: 1.680, Test accuracy: 77.95
Round  94, Global train loss: 1.658, Global test loss: 2.272, Global test accuracy: 17.29
Round  95, Train loss: 1.652, Test loss: 1.680, Test accuracy: 77.99
Round  95, Global train loss: 1.652, Global test loss: 2.288, Global test accuracy: 14.27/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.606, Test loss: 1.677, Test accuracy: 78.31
Round  96, Global train loss: 1.606, Global test loss: 2.278, Global test accuracy: 16.61
Round  97, Train loss: 1.681, Test loss: 1.672, Test accuracy: 78.83
Round  97, Global train loss: 1.681, Global test loss: 2.284, Global test accuracy: 15.18
Round  98, Train loss: 1.614, Test loss: 1.672, Test accuracy: 78.88
Round  98, Global train loss: 1.614, Global test loss: 2.278, Global test accuracy: 16.45
Round  99, Train loss: 1.643, Test loss: 1.671, Test accuracy: 78.89
Round  99, Global train loss: 1.643, Global test loss: 2.294, Global test accuracy: 14.40
Final Round, Train loss: 1.692, Test loss: 1.708, Test accuracy: 75.41
Final Round, Global train loss: 1.692, Global test loss: 2.294, Global test accuracy: 14.40
Average accuracy final 10 rounds: 78.0595 

Average global accuracy final 10 rounds: 15.434 

5280.736265897751
[3.379554033279419, 6.759108066558838, 9.956866025924683, 13.154623985290527, 16.46398949623108, 19.77335500717163, 23.042120456695557, 26.310885906219482, 29.52164125442505, 32.732396602630615, 35.88644242286682, 39.04048824310303, 42.04981732368469, 45.05914640426636, 48.19964098930359, 51.34013557434082, 54.52819085121155, 57.716246128082275, 60.921857595443726, 64.12746906280518, 67.41546273231506, 70.70345640182495, 73.99449014663696, 77.28552389144897, 80.46410322189331, 83.64268255233765, 86.86051368713379, 90.07834482192993, 93.25364708900452, 96.4289493560791, 99.58893752098083, 102.74892568588257, 105.93178582191467, 109.11464595794678, 112.36733102798462, 115.62001609802246, 118.82160973548889, 122.02320337295532, 125.2349865436554, 128.44676971435547, 131.59895753860474, 134.751145362854, 137.84916019439697, 140.94717502593994, 144.0362639427185, 147.12535285949707, 150.27386713027954, 153.422381401062, 156.63662123680115, 159.85086107254028, 162.93786692619324, 166.0248727798462, 169.1184365749359, 172.21200037002563, 175.38770651817322, 178.5634126663208, 182.03954601287842, 185.51567935943604, 188.53805661201477, 191.5604338645935, 194.56055784225464, 197.56068181991577, 200.78868508338928, 204.0166883468628, 207.21944642066956, 210.42220449447632, 213.63429617881775, 216.84638786315918, 219.965478181839, 223.0845685005188, 226.2642879486084, 229.444007396698, 232.66234135627747, 235.88067531585693, 239.03323006629944, 242.18578481674194, 245.4659788608551, 248.74617290496826, 251.9858365058899, 255.22550010681152, 258.53192591667175, 261.838351726532, 265.00220799446106, 268.16606426239014, 271.3700518608093, 274.5740394592285, 277.7466170787811, 280.91919469833374, 284.03585839271545, 287.15252208709717, 290.3977196216583, 293.6429171562195, 297.0166337490082, 300.3903503417969, 303.66904640197754, 306.9477424621582, 310.1475839614868, 313.34742546081543, 316.5855176448822, 319.823609828949, 322.99528551101685, 326.1669611930847, 329.4642038345337, 332.76144647598267, 335.94918394088745, 339.13692140579224, 342.3887619972229, 345.64060258865356, 348.8603444099426, 352.0800862312317, 355.4062077999115, 358.7323293685913, 362.07355546951294, 365.41478157043457, 368.6214406490326, 371.8280997276306, 375.14209032058716, 378.4560809135437, 381.700407743454, 384.94473457336426, 388.28464794158936, 391.62456130981445, 395.00930881500244, 398.39405632019043, 401.6150074005127, 404.83595848083496, 408.1430010795593, 411.4500436782837, 414.7085151672363, 417.96698665618896, 421.1640794277191, 424.36117219924927, 427.36729168891907, 430.37341117858887, 433.51077246665955, 436.6481337547302, 439.8861961364746, 443.124258518219, 446.4544954299927, 449.78473234176636, 453.0790047645569, 456.3732771873474, 459.6770405769348, 462.9808039665222, 466.37621688842773, 469.77162981033325, 473.1636281013489, 476.5556263923645, 479.88360619544983, 483.21158599853516, 486.40211939811707, 489.592652797699, 492.87881350517273, 496.1649742126465, 499.55057883262634, 502.9361834526062, 506.2622697353363, 509.5883560180664, 512.9905822277069, 516.3928084373474, 519.6438028812408, 522.8947973251343, 526.2298278808594, 529.5648584365845, 532.819057226181, 536.0732560157776, 539.253082036972, 542.4329080581665, 545.5707609653473, 548.7086138725281, 551.9216110706329, 555.1346082687378, 558.5173630714417, 561.9001178741455, 565.282187461853, 568.6642570495605, 572.0575437545776, 575.4508304595947, 578.6194536685944, 581.788076877594, 584.8449003696442, 587.9017238616943, 591.0693218708038, 594.2369198799133, 597.560384273529, 600.8838486671448, 604.1513695716858, 607.4188904762268, 610.6115589141846, 613.8042273521423, 616.9068574905396, 620.0094876289368, 623.1126611232758, 626.2158346176147, 629.3728709220886, 632.5299072265625, 635.8024344444275, 639.0749616622925, 642.3590395450592, 645.6431174278259, 647.3036625385284, 648.964207649231]
[18.3675, 18.3675, 31.845, 31.845, 39.2775, 39.2775, 43.76, 43.76, 48.29, 48.29, 52.74, 52.74, 52.0775, 52.0775, 52.82, 52.82, 53.1625, 53.1625, 52.83, 52.83, 53.025, 53.025, 54.2525, 54.2525, 55.94, 55.94, 58.615, 58.615, 57.915, 57.915, 57.71, 57.71, 60.5475, 60.5475, 60.165, 60.165, 60.7975, 60.7975, 60.2375, 60.2375, 61.3, 61.3, 61.4225, 61.4225, 60.9275, 60.9275, 61.69, 61.69, 60.9875, 60.9875, 60.9875, 60.9875, 61.415, 61.415, 63.115, 63.115, 63.5325, 63.5325, 65.625, 65.625, 66.27, 66.27, 67.5425, 67.5425, 66.47, 66.47, 67.015, 67.015, 68.2925, 68.2925, 68.6925, 68.6925, 69.02, 69.02, 68.9925, 68.9925, 69.1275, 69.1275, 70.6875, 70.6875, 70.715, 70.715, 71.3, 71.3, 71.205, 71.205, 71.2025, 71.2025, 71.1725, 71.1725, 71.6475, 71.6475, 71.585, 71.585, 71.1175, 71.1175, 70.6875, 70.6875, 71.6875, 71.6875, 71.215, 71.215, 72.4525, 72.4525, 73.2325, 73.2325, 73.3475, 73.3475, 73.39, 73.39, 73.7825, 73.7825, 73.96, 73.96, 73.92, 73.92, 74.0425, 74.0425, 74.0675, 74.0675, 74.0175, 74.0175, 74.5775, 74.5775, 74.39, 74.39, 73.9175, 73.9175, 74.295, 74.295, 74.5275, 74.5275, 74.9725, 74.9725, 74.705, 74.705, 75.1125, 75.1125, 75.09, 75.09, 74.6875, 74.6875, 74.72, 74.72, 73.875, 73.875, 74.7275, 74.7275, 74.735, 74.735, 75.1925, 75.1925, 75.2175, 75.2175, 75.1325, 75.1325, 75.6425, 75.6425, 76.025, 76.025, 75.5525, 75.5525, 75.4, 75.4, 76.3275, 76.3275, 76.3275, 76.3275, 76.3375, 76.3375, 76.765, 76.765, 77.28, 77.28, 77.7425, 77.7425, 77.675, 77.675, 78.1825, 78.1825, 77.6975, 77.6975, 76.9625, 76.9625, 77.285, 77.285, 77.81, 77.81, 77.9525, 77.9525, 77.9875, 77.9875, 78.3075, 78.3075, 78.8275, 78.8275, 78.875, 78.875, 78.89, 78.89, 75.405, 75.405]
