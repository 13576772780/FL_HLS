nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.294, Test accuracy: 34.15
Round   0, Global train loss: 2.299, Global test loss: 2.294, Global test accuracy: 34.14
Round   1, Train loss: 2.270, Test loss: 2.219, Test accuracy: 32.16
Round   1, Global train loss: 2.270, Global test loss: 2.205, Global test accuracy: 31.96
Round   2, Train loss: 2.065, Test loss: 2.015, Test accuracy: 51.69
Round   2, Global train loss: 2.065, Global test loss: 1.893, Global test accuracy: 64.41
Round   3, Train loss: 1.905, Test loss: 1.954, Test accuracy: 55.87
Round   3, Global train loss: 1.905, Global test loss: 1.807, Global test accuracy: 71.26
Round   4, Train loss: 1.859, Test loss: 1.902, Test accuracy: 59.81
Round   4, Global train loss: 1.859, Global test loss: 1.779, Global test accuracy: 71.69
Round   5, Train loss: 1.794, Test loss: 1.870, Test accuracy: 61.70
Round   5, Global train loss: 1.794, Global test loss: 1.759, Global test accuracy: 72.07
Round   6, Train loss: 1.864, Test loss: 1.847, Test accuracy: 63.67
Round   6, Global train loss: 1.864, Global test loss: 1.770, Global test accuracy: 72.80
Round   7, Train loss: 1.763, Test loss: 1.825, Test accuracy: 65.86
Round   7, Global train loss: 1.763, Global test loss: 1.739, Global test accuracy: 73.51
Round   8, Train loss: 1.708, Test loss: 1.820, Test accuracy: 66.27
Round   8, Global train loss: 1.708, Global test loss: 1.730, Global test accuracy: 73.92
Round   9, Train loss: 1.721, Test loss: 1.812, Test accuracy: 66.87
Round   9, Global train loss: 1.721, Global test loss: 1.736, Global test accuracy: 73.42
Round  10, Train loss: 1.748, Test loss: 1.789, Test accuracy: 69.15
Round  10, Global train loss: 1.748, Global test loss: 1.735, Global test accuracy: 73.56
Round  11, Train loss: 1.717, Test loss: 1.779, Test accuracy: 69.67
Round  11, Global train loss: 1.717, Global test loss: 1.735, Global test accuracy: 73.36
Round  12, Train loss: 1.703, Test loss: 1.773, Test accuracy: 70.02
Round  12, Global train loss: 1.703, Global test loss: 1.727, Global test accuracy: 73.99
Round  13, Train loss: 1.750, Test loss: 1.752, Test accuracy: 71.90
Round  13, Global train loss: 1.750, Global test loss: 1.737, Global test accuracy: 73.31
Round  14, Train loss: 1.705, Test loss: 1.748, Test accuracy: 72.19
Round  14, Global train loss: 1.705, Global test loss: 1.734, Global test accuracy: 73.46
Round  15, Train loss: 1.682, Test loss: 1.747, Test accuracy: 72.19
Round  15, Global train loss: 1.682, Global test loss: 1.724, Global test accuracy: 74.33
Round  16, Train loss: 1.687, Test loss: 1.741, Test accuracy: 72.69
Round  16, Global train loss: 1.687, Global test loss: 1.728, Global test accuracy: 73.66
Round  17, Train loss: 1.663, Test loss: 1.741, Test accuracy: 72.64
Round  17, Global train loss: 1.663, Global test loss: 1.721, Global test accuracy: 74.16
Round  18, Train loss: 1.658, Test loss: 1.737, Test accuracy: 73.05
Round  18, Global train loss: 1.658, Global test loss: 1.721, Global test accuracy: 74.19
Round  19, Train loss: 1.668, Test loss: 1.735, Test accuracy: 73.12
Round  19, Global train loss: 1.668, Global test loss: 1.722, Global test accuracy: 74.14
Round  20, Train loss: 1.665, Test loss: 1.735, Test accuracy: 73.09
Round  20, Global train loss: 1.665, Global test loss: 1.722, Global test accuracy: 74.11
Round  21, Train loss: 1.674, Test loss: 1.734, Test accuracy: 73.11
Round  21, Global train loss: 1.674, Global test loss: 1.719, Global test accuracy: 74.41
Round  22, Train loss: 1.664, Test loss: 1.734, Test accuracy: 73.12
Round  22, Global train loss: 1.664, Global test loss: 1.721, Global test accuracy: 74.19
Round  23, Train loss: 1.671, Test loss: 1.733, Test accuracy: 73.19
Round  23, Global train loss: 1.671, Global test loss: 1.726, Global test accuracy: 73.76
Round  24, Train loss: 1.664, Test loss: 1.732, Test accuracy: 73.19
Round  24, Global train loss: 1.664, Global test loss: 1.724, Global test accuracy: 73.82
Round  25, Train loss: 1.673, Test loss: 1.732, Test accuracy: 73.23
Round  25, Global train loss: 1.673, Global test loss: 1.719, Global test accuracy: 74.36
Round  26, Train loss: 1.673, Test loss: 1.731, Test accuracy: 73.29
Round  26, Global train loss: 1.673, Global test loss: 1.720, Global test accuracy: 74.41
Round  27, Train loss: 1.671, Test loss: 1.731, Test accuracy: 73.31
Round  27, Global train loss: 1.671, Global test loss: 1.720, Global test accuracy: 74.30
Round  28, Train loss: 1.656, Test loss: 1.730, Test accuracy: 73.36
Round  28, Global train loss: 1.656, Global test loss: 1.718, Global test accuracy: 74.47
Round  29, Train loss: 1.655, Test loss: 1.730, Test accuracy: 73.38
Round  29, Global train loss: 1.655, Global test loss: 1.715, Global test accuracy: 74.75
Round  30, Train loss: 1.669, Test loss: 1.731, Test accuracy: 73.34
Round  30, Global train loss: 1.669, Global test loss: 1.719, Global test accuracy: 74.34
Round  31, Train loss: 1.654, Test loss: 1.730, Test accuracy: 73.33
Round  31, Global train loss: 1.654, Global test loss: 1.717, Global test accuracy: 74.47
Round  32, Train loss: 1.657, Test loss: 1.730, Test accuracy: 73.34
Round  32, Global train loss: 1.657, Global test loss: 1.718, Global test accuracy: 74.41
Round  33, Train loss: 1.655, Test loss: 1.730, Test accuracy: 73.33
Round  33, Global train loss: 1.655, Global test loss: 1.718, Global test accuracy: 74.34
Round  34, Train loss: 1.670, Test loss: 1.730, Test accuracy: 73.33
Round  34, Global train loss: 1.670, Global test loss: 1.718, Global test accuracy: 74.56
Round  35, Train loss: 1.669, Test loss: 1.730, Test accuracy: 73.34
Round  35, Global train loss: 1.669, Global test loss: 1.717, Global test accuracy: 74.62
Round  36, Train loss: 1.672, Test loss: 1.730, Test accuracy: 73.37
Round  36, Global train loss: 1.672, Global test loss: 1.718, Global test accuracy: 74.35
Round  37, Train loss: 1.669, Test loss: 1.730, Test accuracy: 73.32
Round  37, Global train loss: 1.669, Global test loss: 1.718, Global test accuracy: 74.64
Round  38, Train loss: 1.652, Test loss: 1.729, Test accuracy: 73.36
Round  38, Global train loss: 1.652, Global test loss: 1.716, Global test accuracy: 74.61
Round  39, Train loss: 1.653, Test loss: 1.729, Test accuracy: 73.36
Round  39, Global train loss: 1.653, Global test loss: 1.717, Global test accuracy: 74.53
Round  40, Train loss: 1.636, Test loss: 1.729, Test accuracy: 73.31
Round  40, Global train loss: 1.636, Global test loss: 1.717, Global test accuracy: 74.52
Round  41, Train loss: 1.667, Test loss: 1.729, Test accuracy: 73.31
Round  41, Global train loss: 1.667, Global test loss: 1.716, Global test accuracy: 74.50
Round  42, Train loss: 1.666, Test loss: 1.729, Test accuracy: 73.33
Round  42, Global train loss: 1.666, Global test loss: 1.713, Global test accuracy: 74.93
Round  43, Train loss: 1.637, Test loss: 1.729, Test accuracy: 73.34
Round  43, Global train loss: 1.637, Global test loss: 1.716, Global test accuracy: 74.55
Round  44, Train loss: 1.653, Test loss: 1.729, Test accuracy: 73.33
Round  44, Global train loss: 1.653, Global test loss: 1.720, Global test accuracy: 74.28
Round  45, Train loss: 1.656, Test loss: 1.728, Test accuracy: 73.43
Round  45, Global train loss: 1.656, Global test loss: 1.720, Global test accuracy: 74.26
Round  46, Train loss: 1.668, Test loss: 1.728, Test accuracy: 73.43
Round  46, Global train loss: 1.668, Global test loss: 1.720, Global test accuracy: 74.17
Round  47, Train loss: 1.653, Test loss: 1.728, Test accuracy: 73.44
Round  47, Global train loss: 1.653, Global test loss: 1.719, Global test accuracy: 74.29
Round  48, Train loss: 1.666, Test loss: 1.728, Test accuracy: 73.42
Round  48, Global train loss: 1.666, Global test loss: 1.715, Global test accuracy: 74.78
Round  49, Train loss: 1.637, Test loss: 1.728, Test accuracy: 73.42
Round  49, Global train loss: 1.637, Global test loss: 1.717, Global test accuracy: 74.52
Round  50, Train loss: 1.636, Test loss: 1.728, Test accuracy: 73.41
Round  50, Global train loss: 1.636, Global test loss: 1.718, Global test accuracy: 74.38
Round  51, Train loss: 1.652, Test loss: 1.728, Test accuracy: 73.42
Round  51, Global train loss: 1.652, Global test loss: 1.714, Global test accuracy: 74.84
Round  52, Train loss: 1.665, Test loss: 1.728, Test accuracy: 73.45
Round  52, Global train loss: 1.665, Global test loss: 1.717, Global test accuracy: 74.45
Round  53, Train loss: 1.668, Test loss: 1.728, Test accuracy: 73.44
Round  53, Global train loss: 1.668, Global test loss: 1.717, Global test accuracy: 74.56
Round  54, Train loss: 1.652, Test loss: 1.728, Test accuracy: 73.42
Round  54, Global train loss: 1.652, Global test loss: 1.717, Global test accuracy: 74.41
Round  55, Train loss: 1.651, Test loss: 1.728, Test accuracy: 73.47
Round  55, Global train loss: 1.651, Global test loss: 1.713, Global test accuracy: 75.01
Round  56, Train loss: 1.666, Test loss: 1.728, Test accuracy: 73.47
Round  56, Global train loss: 1.666, Global test loss: 1.718, Global test accuracy: 74.36
Round  57, Train loss: 1.651, Test loss: 1.728, Test accuracy: 73.45
Round  57, Global train loss: 1.651, Global test loss: 1.717, Global test accuracy: 74.55
Round  58, Train loss: 1.667, Test loss: 1.728, Test accuracy: 73.44
Round  58, Global train loss: 1.667, Global test loss: 1.717, Global test accuracy: 74.54
Round  59, Train loss: 1.635, Test loss: 1.728, Test accuracy: 73.47
Round  59, Global train loss: 1.635, Global test loss: 1.716, Global test accuracy: 74.58
Round  60, Train loss: 1.664, Test loss: 1.728, Test accuracy: 73.44
Round  60, Global train loss: 1.664, Global test loss: 1.715, Global test accuracy: 74.75
Round  61, Train loss: 1.635, Test loss: 1.728, Test accuracy: 73.44
Round  61, Global train loss: 1.635, Global test loss: 1.716, Global test accuracy: 74.58
Round  62, Train loss: 1.650, Test loss: 1.728, Test accuracy: 73.44
Round  62, Global train loss: 1.650, Global test loss: 1.717, Global test accuracy: 74.48
Round  63, Train loss: 1.635, Test loss: 1.728, Test accuracy: 73.45
Round  63, Global train loss: 1.635, Global test loss: 1.719, Global test accuracy: 74.07
Round  64, Train loss: 1.664, Test loss: 1.727, Test accuracy: 73.47
Round  64, Global train loss: 1.664, Global test loss: 1.715, Global test accuracy: 74.49
Round  65, Train loss: 1.666, Test loss: 1.727, Test accuracy: 73.50
Round  65, Global train loss: 1.666, Global test loss: 1.714, Global test accuracy: 74.78
Round  66, Train loss: 1.635, Test loss: 1.727, Test accuracy: 73.47
Round  66, Global train loss: 1.635, Global test loss: 1.716, Global test accuracy: 74.52
Round  67, Train loss: 1.664, Test loss: 1.727, Test accuracy: 73.50
Round  67, Global train loss: 1.664, Global test loss: 1.717, Global test accuracy: 74.40
Round  68, Train loss: 1.663, Test loss: 1.727, Test accuracy: 73.52
Round  68, Global train loss: 1.663, Global test loss: 1.716, Global test accuracy: 74.66
Round  69, Train loss: 1.649, Test loss: 1.727, Test accuracy: 73.55
Round  69, Global train loss: 1.649, Global test loss: 1.713, Global test accuracy: 74.94
Round  70, Train loss: 1.650, Test loss: 1.727, Test accuracy: 73.50
Round  70, Global train loss: 1.650, Global test loss: 1.713, Global test accuracy: 74.72
Round  71, Train loss: 1.650, Test loss: 1.727, Test accuracy: 73.50
Round  71, Global train loss: 1.650, Global test loss: 1.716, Global test accuracy: 74.61
Round  72, Train loss: 1.664, Test loss: 1.727, Test accuracy: 73.48
Round  72, Global train loss: 1.664, Global test loss: 1.715, Global test accuracy: 74.81
Round  73, Train loss: 1.665, Test loss: 1.727, Test accuracy: 73.45
Round  73, Global train loss: 1.665, Global test loss: 1.716, Global test accuracy: 74.61
Round  74, Train loss: 1.664, Test loss: 1.727, Test accuracy: 73.46
Round  74, Global train loss: 1.664, Global test loss: 1.714, Global test accuracy: 74.84
Round  75, Train loss: 1.650, Test loss: 1.727, Test accuracy: 73.45
Round  75, Global train loss: 1.650, Global test loss: 1.719, Global test accuracy: 74.42
Round  76, Train loss: 1.633, Test loss: 1.727, Test accuracy: 73.44
Round  76, Global train loss: 1.633, Global test loss: 1.717, Global test accuracy: 74.45
Round  77, Train loss: 1.663, Test loss: 1.727, Test accuracy: 73.44
Round  77, Global train loss: 1.663, Global test loss: 1.713, Global test accuracy: 74.95
Round  78, Train loss: 1.649, Test loss: 1.727, Test accuracy: 73.45
Round  78, Global train loss: 1.649, Global test loss: 1.716, Global test accuracy: 74.62
Round  79, Train loss: 1.664, Test loss: 1.727, Test accuracy: 73.43
Round  79, Global train loss: 1.664, Global test loss: 1.715, Global test accuracy: 74.68
Round  80, Train loss: 1.634, Test loss: 1.727, Test accuracy: 73.42
Round  80, Global train loss: 1.634, Global test loss: 1.715, Global test accuracy: 74.54
Round  81, Train loss: 1.649, Test loss: 1.727, Test accuracy: 73.42
Round  81, Global train loss: 1.649, Global test loss: 1.716, Global test accuracy: 74.64
Round  82, Train loss: 1.648, Test loss: 1.727, Test accuracy: 73.44
Round  82, Global train loss: 1.648, Global test loss: 1.715, Global test accuracy: 74.47
Round  83, Train loss: 1.648, Test loss: 1.727, Test accuracy: 73.42
Round  83, Global train loss: 1.648, Global test loss: 1.713, Global test accuracy: 74.91
Round  84, Train loss: 1.648, Test loss: 1.727, Test accuracy: 73.45
Round  84, Global train loss: 1.648, Global test loss: 1.716, Global test accuracy: 74.67
Round  85, Train loss: 1.648, Test loss: 1.727, Test accuracy: 73.48
Round  85, Global train loss: 1.648, Global test loss: 1.715, Global test accuracy: 74.67
Round  86, Train loss: 1.664, Test loss: 1.727, Test accuracy: 73.48
Round  86, Global train loss: 1.664, Global test loss: 1.714, Global test accuracy: 74.71
Round  87, Train loss: 1.663, Test loss: 1.727, Test accuracy: 73.52
Round  87, Global train loss: 1.663, Global test loss: 1.714, Global test accuracy: 74.83
Round  88, Train loss: 1.649, Test loss: 1.727, Test accuracy: 73.52
Round  88, Global train loss: 1.649, Global test loss: 1.713, Global test accuracy: 74.94
Round  89, Train loss: 1.662, Test loss: 1.727, Test accuracy: 73.52
Round  89, Global train loss: 1.662, Global test loss: 1.714, Global test accuracy: 74.69
Round  90, Train loss: 1.648, Test loss: 1.727, Test accuracy: 73.52
Round  90, Global train loss: 1.648, Global test loss: 1.716, Global test accuracy: 74.61
Round  91, Train loss: 1.648, Test loss: 1.727, Test accuracy: 73.52
Round  91, Global train loss: 1.648, Global test loss: 1.713, Global test accuracy: 74.90
Round  92, Train loss: 1.649, Test loss: 1.727, Test accuracy: 73.53
Round  92, Global train loss: 1.649, Global test loss: 1.713, Global test accuracy: 75.05
Round  93, Train loss: 1.647, Test loss: 1.727, Test accuracy: 73.53
Round  93, Global train loss: 1.647, Global test loss: 1.716, Global test accuracy: 74.45
Round  94, Train loss: 1.663, Test loss: 1.727, Test accuracy: 73.55
Round  94, Global train loss: 1.663, Global test loss: 1.714, Global test accuracy: 74.68
Round  95, Train loss: 1.656, Test loss: 1.724, Test accuracy: 73.80
Round  95, Global train loss: 1.656, Global test loss: 1.716, Global test accuracy: 74.63/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.636, Test loss: 1.724, Test accuracy: 73.81
Round  96, Global train loss: 1.636, Global test loss: 1.717, Global test accuracy: 74.42
Round  97, Train loss: 1.663, Test loss: 1.724, Test accuracy: 73.80
Round  97, Global train loss: 1.663, Global test loss: 1.714, Global test accuracy: 74.69
Round  98, Train loss: 1.649, Test loss: 1.724, Test accuracy: 73.78
Round  98, Global train loss: 1.649, Global test loss: 1.714, Global test accuracy: 74.75
Round  99, Train loss: 1.662, Test loss: 1.724, Test accuracy: 73.77
Round  99, Global train loss: 1.662, Global test loss: 1.714, Global test accuracy: 74.61
Final Round, Train loss: 1.654, Test loss: 1.724, Test accuracy: 73.81
Final Round, Global train loss: 1.654, Global test loss: 1.714, Global test accuracy: 74.61
Average accuracy final 10 rounds: 73.6585 

Average global accuracy final 10 rounds: 74.6795 

2655.74756193161
[1.5432138442993164, 3.086427688598633, 4.623995780944824, 6.161563873291016, 7.893610000610352, 9.625656127929688, 11.35225534439087, 13.07885456085205, 14.841893434524536, 16.60493230819702, 18.141074180603027, 19.677216053009033, 21.19755244255066, 22.717888832092285, 24.266784191131592, 25.8156795501709, 27.385876178741455, 28.95607280731201, 30.544203519821167, 32.13233423233032, 33.700459480285645, 35.26858472824097, 36.83838129043579, 38.408177852630615, 40.24347472190857, 42.07877159118652, 43.65749979019165, 45.23622798919678, 46.84014868736267, 48.444069385528564, 49.991880893707275, 51.539692401885986, 53.100510358810425, 54.66132831573486, 56.242714405059814, 57.824100494384766, 59.37122178077698, 60.91834306716919, 62.507805824279785, 64.09726858139038, 65.65266108512878, 67.20805358886719, 68.78452324867249, 70.36099290847778, 71.88591861724854, 73.41084432601929, 74.96475005149841, 76.51865577697754, 78.07752227783203, 79.63638877868652, 81.22513103485107, 82.81387329101562, 84.41410183906555, 86.01433038711548, 87.56232690811157, 89.11032342910767, 90.67868208885193, 92.24704074859619, 93.80483627319336, 95.36263179779053, 96.92021155357361, 98.47779130935669, 100.08299326896667, 101.68819522857666, 103.33451175689697, 104.98082828521729, 106.56720733642578, 108.15358638763428, 109.77603697776794, 111.39848756790161, 113.01630902290344, 114.63413047790527, 116.203688621521, 117.77324676513672, 119.38368320465088, 120.99411964416504, 122.58314085006714, 124.17216205596924, 125.7115375995636, 127.25091314315796, 128.82860040664673, 130.4062876701355, 131.93938398361206, 133.47248029708862, 135.04931807518005, 136.62615585327148, 138.2417435646057, 139.85733127593994, 141.48027753829956, 143.10322380065918, 144.6453926563263, 146.1875615119934, 147.6898365020752, 149.19211149215698, 150.73932242393494, 152.2865333557129, 153.8765721321106, 155.4666109085083, 157.06078624725342, 158.65496158599854, 160.22986483573914, 161.80476808547974, 163.34051132202148, 164.87625455856323, 166.38694548606873, 167.89763641357422, 169.48064804077148, 171.06365966796875, 172.6801290512085, 174.29659843444824, 175.91764783859253, 177.53869724273682, 179.123206615448, 180.70771598815918, 182.25927233695984, 183.8108286857605, 185.30720162391663, 186.80357456207275, 188.40708804130554, 190.01060152053833, 191.65037322044373, 193.29014492034912, 194.91704487800598, 196.54394483566284, 198.10968828201294, 199.67543172836304, 201.19170212745667, 202.7079725265503, 204.2062475681305, 205.7045226097107, 207.30864548683167, 208.91276836395264, 210.53488111495972, 212.1569938659668, 213.80232286453247, 215.44765186309814, 217.05174016952515, 218.65582847595215, 220.1700189113617, 221.68420934677124, 223.2293107509613, 224.77441215515137, 226.3747844696045, 227.97515678405762, 229.66508722305298, 231.35501766204834, 232.98244404792786, 234.60987043380737, 236.1780824661255, 237.7462944984436, 239.29461479187012, 240.84293508529663, 242.43000078201294, 244.01706647872925, 245.68612694740295, 247.35518741607666, 249.04142832756042, 250.7276692390442, 252.3824806213379, 254.0372920036316, 255.66995406150818, 257.30261611938477, 258.8852894306183, 260.4679627418518, 262.0671350955963, 263.6663074493408, 265.3282651901245, 266.9902229309082, 268.6663703918457, 270.3425178527832, 272.00095772743225, 273.6593976020813, 275.3211591243744, 276.9829206466675, 278.5269434452057, 280.0709662437439, 281.6026430130005, 283.1343197822571, 284.7703676223755, 286.4064154624939, 288.1699995994568, 289.9335837364197, 291.824818611145, 293.71605348587036, 295.44039821624756, 297.16474294662476, 298.9781000614166, 300.7914571762085, 302.4312975406647, 304.07113790512085, 305.82470965385437, 307.5782814025879, 309.27450704574585, 310.9707326889038, 312.83493876457214, 314.6991448402405, 316.4255094528198, 318.15187406539917, 320.0932238101959, 322.0345735549927, 323.9285559654236, 325.8225383758545]
[34.15, 34.15, 32.155, 32.155, 51.69, 51.69, 55.865, 55.865, 59.81, 59.81, 61.705, 61.705, 63.675, 63.675, 65.855, 65.855, 66.27, 66.27, 66.87, 66.87, 69.15, 69.15, 69.67, 69.67, 70.02, 70.02, 71.9, 71.9, 72.195, 72.195, 72.195, 72.195, 72.685, 72.685, 72.645, 72.645, 73.05, 73.05, 73.125, 73.125, 73.09, 73.09, 73.105, 73.105, 73.12, 73.12, 73.185, 73.185, 73.195, 73.195, 73.23, 73.23, 73.29, 73.29, 73.315, 73.315, 73.36, 73.36, 73.375, 73.375, 73.34, 73.34, 73.33, 73.33, 73.345, 73.345, 73.335, 73.335, 73.33, 73.33, 73.34, 73.34, 73.37, 73.37, 73.32, 73.32, 73.36, 73.36, 73.36, 73.36, 73.31, 73.31, 73.315, 73.315, 73.325, 73.325, 73.34, 73.34, 73.33, 73.33, 73.43, 73.43, 73.43, 73.43, 73.435, 73.435, 73.415, 73.415, 73.42, 73.42, 73.405, 73.405, 73.42, 73.42, 73.455, 73.455, 73.44, 73.44, 73.425, 73.425, 73.47, 73.47, 73.465, 73.465, 73.455, 73.455, 73.445, 73.445, 73.465, 73.465, 73.445, 73.445, 73.435, 73.435, 73.44, 73.44, 73.455, 73.455, 73.465, 73.465, 73.495, 73.495, 73.47, 73.47, 73.505, 73.505, 73.52, 73.52, 73.55, 73.55, 73.5, 73.5, 73.505, 73.505, 73.48, 73.48, 73.455, 73.455, 73.46, 73.46, 73.455, 73.455, 73.44, 73.44, 73.435, 73.435, 73.45, 73.45, 73.43, 73.43, 73.42, 73.42, 73.425, 73.425, 73.445, 73.445, 73.42, 73.42, 73.45, 73.45, 73.485, 73.485, 73.48, 73.48, 73.52, 73.52, 73.515, 73.515, 73.52, 73.52, 73.515, 73.515, 73.515, 73.515, 73.525, 73.525, 73.53, 73.53, 73.545, 73.545, 73.8, 73.8, 73.81, 73.81, 73.8, 73.8, 73.775, 73.775, 73.77, 73.77, 73.81, 73.81]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.298, Test loss: 2.292, Test accuracy: 25.82
Round   0, Global train loss: 2.298, Global test loss: 2.292, Global test accuracy: 25.60
Round   1, Train loss: 2.256, Test loss: 2.223, Test accuracy: 36.64
Round   1, Global train loss: 2.256, Global test loss: 2.199, Global test accuracy: 40.18
Round   2, Train loss: 2.031, Test loss: 2.023, Test accuracy: 53.14
Round   2, Global train loss: 2.031, Global test loss: 1.844, Global test accuracy: 68.63
Round   3, Train loss: 1.705, Test loss: 1.910, Test accuracy: 63.02
Round   3, Global train loss: 1.705, Global test loss: 1.641, Global test accuracy: 85.89
Round   4, Train loss: 1.598, Test loss: 1.821, Test accuracy: 69.56
Round   4, Global train loss: 1.598, Global test loss: 1.599, Global test accuracy: 88.11
Round   5, Train loss: 1.568, Test loss: 1.779, Test accuracy: 72.72
Round   5, Global train loss: 1.568, Global test loss: 1.580, Global test accuracy: 89.38
Round   6, Train loss: 1.570, Test loss: 1.641, Test accuracy: 84.22
Round   6, Global train loss: 1.570, Global test loss: 1.571, Global test accuracy: 89.93
Round   7, Train loss: 1.548, Test loss: 1.607, Test accuracy: 87.26
Round   7, Global train loss: 1.548, Global test loss: 1.564, Global test accuracy: 90.46
Round   8, Train loss: 1.540, Test loss: 1.602, Test accuracy: 87.66
Round   8, Global train loss: 1.540, Global test loss: 1.560, Global test accuracy: 90.81
Round   9, Train loss: 1.530, Test loss: 1.580, Test accuracy: 89.04
Round   9, Global train loss: 1.530, Global test loss: 1.553, Global test accuracy: 91.71
Round  10, Train loss: 1.525, Test loss: 1.575, Test accuracy: 89.38
Round  10, Global train loss: 1.525, Global test loss: 1.550, Global test accuracy: 91.92
Round  11, Train loss: 1.531, Test loss: 1.570, Test accuracy: 89.84
Round  11, Global train loss: 1.531, Global test loss: 1.548, Global test accuracy: 91.98
Round  12, Train loss: 1.528, Test loss: 1.568, Test accuracy: 90.08
Round  12, Global train loss: 1.528, Global test loss: 1.546, Global test accuracy: 92.13
Round  13, Train loss: 1.517, Test loss: 1.563, Test accuracy: 90.46
Round  13, Global train loss: 1.517, Global test loss: 1.542, Global test accuracy: 92.53
Round  14, Train loss: 1.517, Test loss: 1.559, Test accuracy: 90.79
Round  14, Global train loss: 1.517, Global test loss: 1.542, Global test accuracy: 92.40
Round  15, Train loss: 1.511, Test loss: 1.558, Test accuracy: 90.91
Round  15, Global train loss: 1.511, Global test loss: 1.541, Global test accuracy: 92.45
Round  16, Train loss: 1.513, Test loss: 1.552, Test accuracy: 91.41
Round  16, Global train loss: 1.513, Global test loss: 1.537, Global test accuracy: 92.86
Round  17, Train loss: 1.513, Test loss: 1.547, Test accuracy: 91.78
Round  17, Global train loss: 1.513, Global test loss: 1.536, Global test accuracy: 92.90
Round  18, Train loss: 1.507, Test loss: 1.546, Test accuracy: 91.84
Round  18, Global train loss: 1.507, Global test loss: 1.536, Global test accuracy: 92.95
Round  19, Train loss: 1.503, Test loss: 1.544, Test accuracy: 91.98
Round  19, Global train loss: 1.503, Global test loss: 1.533, Global test accuracy: 93.01
Round  20, Train loss: 1.502, Test loss: 1.543, Test accuracy: 92.16
Round  20, Global train loss: 1.502, Global test loss: 1.534, Global test accuracy: 92.98
Round  21, Train loss: 1.498, Test loss: 1.542, Test accuracy: 92.30
Round  21, Global train loss: 1.498, Global test loss: 1.532, Global test accuracy: 93.20
Round  22, Train loss: 1.497, Test loss: 1.541, Test accuracy: 92.38
Round  22, Global train loss: 1.497, Global test loss: 1.532, Global test accuracy: 93.11
Round  23, Train loss: 1.498, Test loss: 1.540, Test accuracy: 92.44
Round  23, Global train loss: 1.498, Global test loss: 1.531, Global test accuracy: 93.23
Round  24, Train loss: 1.499, Test loss: 1.538, Test accuracy: 92.62
Round  24, Global train loss: 1.499, Global test loss: 1.528, Global test accuracy: 93.52
Round  25, Train loss: 1.501, Test loss: 1.537, Test accuracy: 92.74
Round  25, Global train loss: 1.501, Global test loss: 1.529, Global test accuracy: 93.31
Round  26, Train loss: 1.491, Test loss: 1.536, Test accuracy: 92.86
Round  26, Global train loss: 1.491, Global test loss: 1.527, Global test accuracy: 93.73
Round  27, Train loss: 1.493, Test loss: 1.535, Test accuracy: 92.96
Round  27, Global train loss: 1.493, Global test loss: 1.526, Global test accuracy: 93.77
Round  28, Train loss: 1.493, Test loss: 1.534, Test accuracy: 93.01
Round  28, Global train loss: 1.493, Global test loss: 1.524, Global test accuracy: 93.92
Round  29, Train loss: 1.490, Test loss: 1.532, Test accuracy: 93.22
Round  29, Global train loss: 1.490, Global test loss: 1.524, Global test accuracy: 93.89
Round  30, Train loss: 1.493, Test loss: 1.531, Test accuracy: 93.31
Round  30, Global train loss: 1.493, Global test loss: 1.525, Global test accuracy: 93.75
Round  31, Train loss: 1.492, Test loss: 1.530, Test accuracy: 93.38
Round  31, Global train loss: 1.492, Global test loss: 1.523, Global test accuracy: 94.02
Round  32, Train loss: 1.491, Test loss: 1.530, Test accuracy: 93.39
Round  32, Global train loss: 1.491, Global test loss: 1.522, Global test accuracy: 94.12
Round  33, Train loss: 1.490, Test loss: 1.529, Test accuracy: 93.50
Round  33, Global train loss: 1.490, Global test loss: 1.521, Global test accuracy: 94.18
Round  34, Train loss: 1.487, Test loss: 1.528, Test accuracy: 93.58
Round  34, Global train loss: 1.487, Global test loss: 1.522, Global test accuracy: 94.17
Round  35, Train loss: 1.488, Test loss: 1.528, Test accuracy: 93.59
Round  35, Global train loss: 1.488, Global test loss: 1.522, Global test accuracy: 94.06
Round  36, Train loss: 1.484, Test loss: 1.527, Test accuracy: 93.70
Round  36, Global train loss: 1.484, Global test loss: 1.521, Global test accuracy: 94.08
Round  37, Train loss: 1.489, Test loss: 1.526, Test accuracy: 93.75
Round  37, Global train loss: 1.489, Global test loss: 1.520, Global test accuracy: 94.27
Round  38, Train loss: 1.485, Test loss: 1.527, Test accuracy: 93.68
Round  38, Global train loss: 1.485, Global test loss: 1.520, Global test accuracy: 94.23
Round  39, Train loss: 1.484, Test loss: 1.526, Test accuracy: 93.73
Round  39, Global train loss: 1.484, Global test loss: 1.519, Global test accuracy: 94.48
Round  40, Train loss: 1.487, Test loss: 1.525, Test accuracy: 93.87
Round  40, Global train loss: 1.487, Global test loss: 1.519, Global test accuracy: 94.47
Round  41, Train loss: 1.484, Test loss: 1.524, Test accuracy: 93.95
Round  41, Global train loss: 1.484, Global test loss: 1.518, Global test accuracy: 94.61
Round  42, Train loss: 1.486, Test loss: 1.524, Test accuracy: 94.02
Round  42, Global train loss: 1.486, Global test loss: 1.519, Global test accuracy: 94.36
Round  43, Train loss: 1.483, Test loss: 1.523, Test accuracy: 94.03
Round  43, Global train loss: 1.483, Global test loss: 1.518, Global test accuracy: 94.43
Round  44, Train loss: 1.484, Test loss: 1.523, Test accuracy: 94.06
Round  44, Global train loss: 1.484, Global test loss: 1.517, Global test accuracy: 94.58
Round  45, Train loss: 1.482, Test loss: 1.522, Test accuracy: 94.13
Round  45, Global train loss: 1.482, Global test loss: 1.517, Global test accuracy: 94.50
Round  46, Train loss: 1.484, Test loss: 1.522, Test accuracy: 94.20
Round  46, Global train loss: 1.484, Global test loss: 1.516, Global test accuracy: 94.54
Round  47, Train loss: 1.482, Test loss: 1.521, Test accuracy: 94.28
Round  47, Global train loss: 1.482, Global test loss: 1.515, Global test accuracy: 94.84
Round  48, Train loss: 1.482, Test loss: 1.520, Test accuracy: 94.38
Round  48, Global train loss: 1.482, Global test loss: 1.515, Global test accuracy: 94.79
Round  49, Train loss: 1.481, Test loss: 1.520, Test accuracy: 94.36
Round  49, Global train loss: 1.481, Global test loss: 1.514, Global test accuracy: 94.84
Round  50, Train loss: 1.482, Test loss: 1.520, Test accuracy: 94.38
Round  50, Global train loss: 1.482, Global test loss: 1.514, Global test accuracy: 94.81
Round  51, Train loss: 1.482, Test loss: 1.520, Test accuracy: 94.37
Round  51, Global train loss: 1.482, Global test loss: 1.515, Global test accuracy: 94.83
Round  52, Train loss: 1.481, Test loss: 1.519, Test accuracy: 94.44
Round  52, Global train loss: 1.481, Global test loss: 1.515, Global test accuracy: 94.77
Round  53, Train loss: 1.480, Test loss: 1.519, Test accuracy: 94.47
Round  53, Global train loss: 1.480, Global test loss: 1.514, Global test accuracy: 94.72
Round  54, Train loss: 1.477, Test loss: 1.519, Test accuracy: 94.48
Round  54, Global train loss: 1.477, Global test loss: 1.513, Global test accuracy: 94.94
Round  55, Train loss: 1.479, Test loss: 1.518, Test accuracy: 94.50
Round  55, Global train loss: 1.479, Global test loss: 1.512, Global test accuracy: 95.02
Round  56, Train loss: 1.480, Test loss: 1.518, Test accuracy: 94.51
Round  56, Global train loss: 1.480, Global test loss: 1.513, Global test accuracy: 95.08
Round  57, Train loss: 1.479, Test loss: 1.518, Test accuracy: 94.55
Round  57, Global train loss: 1.479, Global test loss: 1.513, Global test accuracy: 94.96
Round  58, Train loss: 1.477, Test loss: 1.517, Test accuracy: 94.57
Round  58, Global train loss: 1.477, Global test loss: 1.512, Global test accuracy: 95.17
Round  59, Train loss: 1.480, Test loss: 1.517, Test accuracy: 94.57
Round  59, Global train loss: 1.480, Global test loss: 1.512, Global test accuracy: 95.03
Round  60, Train loss: 1.479, Test loss: 1.517, Test accuracy: 94.68
Round  60, Global train loss: 1.479, Global test loss: 1.512, Global test accuracy: 95.17
Round  61, Train loss: 1.478, Test loss: 1.517, Test accuracy: 94.69
Round  61, Global train loss: 1.478, Global test loss: 1.512, Global test accuracy: 95.08
Round  62, Train loss: 1.480, Test loss: 1.516, Test accuracy: 94.69
Round  62, Global train loss: 1.480, Global test loss: 1.512, Global test accuracy: 95.06
Round  63, Train loss: 1.477, Test loss: 1.516, Test accuracy: 94.71
Round  63, Global train loss: 1.477, Global test loss: 1.511, Global test accuracy: 95.12
Round  64, Train loss: 1.478, Test loss: 1.516, Test accuracy: 94.72
Round  64, Global train loss: 1.478, Global test loss: 1.511, Global test accuracy: 95.14
Round  65, Train loss: 1.479, Test loss: 1.516, Test accuracy: 94.73
Round  65, Global train loss: 1.479, Global test loss: 1.512, Global test accuracy: 95.25
Round  66, Train loss: 1.477, Test loss: 1.515, Test accuracy: 94.78
Round  66, Global train loss: 1.477, Global test loss: 1.511, Global test accuracy: 95.24
Round  67, Train loss: 1.481, Test loss: 1.515, Test accuracy: 94.81
Round  67, Global train loss: 1.481, Global test loss: 1.511, Global test accuracy: 95.20
Round  68, Train loss: 1.478, Test loss: 1.515, Test accuracy: 94.81
Round  68, Global train loss: 1.478, Global test loss: 1.511, Global test accuracy: 95.09
Round  69, Train loss: 1.478, Test loss: 1.515, Test accuracy: 94.81
Round  69, Global train loss: 1.478, Global test loss: 1.511, Global test accuracy: 95.19
Round  70, Train loss: 1.476, Test loss: 1.514, Test accuracy: 94.88
Round  70, Global train loss: 1.476, Global test loss: 1.511, Global test accuracy: 95.31
Round  71, Train loss: 1.478, Test loss: 1.514, Test accuracy: 94.90
Round  71, Global train loss: 1.478, Global test loss: 1.511, Global test accuracy: 95.17
Round  72, Train loss: 1.479, Test loss: 1.514, Test accuracy: 94.90
Round  72, Global train loss: 1.479, Global test loss: 1.512, Global test accuracy: 95.03
Round  73, Train loss: 1.479, Test loss: 1.514, Test accuracy: 94.89
Round  73, Global train loss: 1.479, Global test loss: 1.511, Global test accuracy: 95.17
Round  74, Train loss: 1.476, Test loss: 1.514, Test accuracy: 94.88
Round  74, Global train loss: 1.476, Global test loss: 1.511, Global test accuracy: 95.19
Round  75, Train loss: 1.474, Test loss: 1.514, Test accuracy: 94.86
Round  75, Global train loss: 1.474, Global test loss: 1.510, Global test accuracy: 95.27
Round  76, Train loss: 1.476, Test loss: 1.514, Test accuracy: 94.89
Round  76, Global train loss: 1.476, Global test loss: 1.510, Global test accuracy: 95.43
Round  77, Train loss: 1.474, Test loss: 1.513, Test accuracy: 94.97
Round  77, Global train loss: 1.474, Global test loss: 1.510, Global test accuracy: 95.43
Round  78, Train loss: 1.476, Test loss: 1.513, Test accuracy: 95.02
Round  78, Global train loss: 1.476, Global test loss: 1.510, Global test accuracy: 95.33
Round  79, Train loss: 1.475, Test loss: 1.513, Test accuracy: 95.02
Round  79, Global train loss: 1.475, Global test loss: 1.510, Global test accuracy: 95.23
Round  80, Train loss: 1.475, Test loss: 1.513, Test accuracy: 95.00
Round  80, Global train loss: 1.475, Global test loss: 1.510, Global test accuracy: 95.47
Round  81, Train loss: 1.475, Test loss: 1.513, Test accuracy: 95.03
Round  81, Global train loss: 1.475, Global test loss: 1.510, Global test accuracy: 95.36
Round  82, Train loss: 1.476, Test loss: 1.513, Test accuracy: 95.05
Round  82, Global train loss: 1.476, Global test loss: 1.510, Global test accuracy: 95.36
Round  83, Train loss: 1.477, Test loss: 1.513, Test accuracy: 95.06
Round  83, Global train loss: 1.477, Global test loss: 1.509, Global test accuracy: 95.39
Round  84, Train loss: 1.476, Test loss: 1.513, Test accuracy: 95.07
Round  84, Global train loss: 1.476, Global test loss: 1.509, Global test accuracy: 95.43
Round  85, Train loss: 1.475, Test loss: 1.513, Test accuracy: 95.08
Round  85, Global train loss: 1.475, Global test loss: 1.509, Global test accuracy: 95.33
Round  86, Train loss: 1.476, Test loss: 1.513, Test accuracy: 95.08
Round  86, Global train loss: 1.476, Global test loss: 1.509, Global test accuracy: 95.30
Round  87, Train loss: 1.475, Test loss: 1.512, Test accuracy: 95.14
Round  87, Global train loss: 1.475, Global test loss: 1.509, Global test accuracy: 95.42
Round  88, Train loss: 1.475, Test loss: 1.512, Test accuracy: 95.11
Round  88, Global train loss: 1.475, Global test loss: 1.509, Global test accuracy: 95.33
Round  89, Train loss: 1.474, Test loss: 1.512, Test accuracy: 95.11
Round  89, Global train loss: 1.474, Global test loss: 1.509, Global test accuracy: 95.37
Round  90, Train loss: 1.477, Test loss: 1.512, Test accuracy: 95.08
Round  90, Global train loss: 1.477, Global test loss: 1.508, Global test accuracy: 95.40
Round  91, Train loss: 1.475, Test loss: 1.512, Test accuracy: 95.08
Round  91, Global train loss: 1.475, Global test loss: 1.509, Global test accuracy: 95.39
Round  92, Train loss: 1.474, Test loss: 1.512, Test accuracy: 95.11
Round  92, Global train loss: 1.474, Global test loss: 1.509, Global test accuracy: 95.36
Round  93, Train loss: 1.474, Test loss: 1.512, Test accuracy: 95.14
Round  93, Global train loss: 1.474, Global test loss: 1.508, Global test accuracy: 95.36
Round  94, Train loss: 1.473, Test loss: 1.512, Test accuracy: 95.14
Round  94, Global train loss: 1.473, Global test loss: 1.508, Global test accuracy: 95.55
Round  95, Train loss: 1.475, Test loss: 1.511, Test accuracy: 95.19
Round  95, Global train loss: 1.475, Global test loss: 1.507, Global test accuracy: 95.53/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.473, Test loss: 1.511, Test accuracy: 95.22
Round  96, Global train loss: 1.473, Global test loss: 1.507, Global test accuracy: 95.55
Round  97, Train loss: 1.473, Test loss: 1.511, Test accuracy: 95.24
Round  97, Global train loss: 1.473, Global test loss: 1.508, Global test accuracy: 95.56
Round  98, Train loss: 1.475, Test loss: 1.511, Test accuracy: 95.28
Round  98, Global train loss: 1.475, Global test loss: 1.508, Global test accuracy: 95.45
Round  99, Train loss: 1.475, Test loss: 1.510, Test accuracy: 95.28
Round  99, Global train loss: 1.475, Global test loss: 1.507, Global test accuracy: 95.58
Final Round, Train loss: 1.472, Test loss: 1.509, Test accuracy: 95.40
Final Round, Global train loss: 1.472, Global test loss: 1.507, Global test accuracy: 95.58
Average accuracy final 10 rounds: 95.1775 

Average global accuracy final 10 rounds: 95.47200000000001 

3282.9413475990295
[2.065969467163086, 4.131938934326172, 6.12146258354187, 8.110986232757568, 9.944540739059448, 11.778095245361328, 13.626656293869019, 15.475217342376709, 17.20667552947998, 18.938133716583252, 21.153402090072632, 23.36867046356201, 25.356793642044067, 27.344916820526123, 29.593815565109253, 31.842714309692383, 33.94828915596008, 36.05386400222778, 38.08382058143616, 40.11377716064453, 42.04106283187866, 43.96834850311279, 46.07051396369934, 48.17267942428589, 49.91000556945801, 51.64733171463013, 53.628117084503174, 55.60890245437622, 57.59299159049988, 59.577080726623535, 61.42391777038574, 63.27075481414795, 65.43180823326111, 67.59286165237427, 69.56238889694214, 71.53191614151001, 73.64064359664917, 75.74937105178833, 77.67071747779846, 79.5920639038086, 81.54314589500427, 83.49422788619995, 85.42014217376709, 87.34605646133423, 89.29867911338806, 91.2513017654419, 93.06528377532959, 94.87926578521729, 96.92004442214966, 98.96082305908203, 100.77126598358154, 102.58170890808105, 104.55241322517395, 106.52311754226685, 108.52576470375061, 110.52841186523438, 112.34757995605469, 114.166748046875, 116.17180156707764, 118.17685508728027, 120.09401655197144, 122.0111780166626, 123.96513772010803, 125.91909742355347, 127.84635996818542, 129.77362251281738, 131.80327701568604, 133.8329315185547, 135.70402359962463, 137.57511568069458, 139.71624088287354, 141.8573660850525, 143.57000970840454, 145.2826533317566, 147.34629034996033, 149.40992736816406, 151.33151531219482, 153.2531032562256, 155.133855342865, 157.0146074295044, 158.99612426757812, 160.97764110565186, 163.19535851478577, 165.41307592391968, 167.36902832984924, 169.3249807357788, 171.2673258781433, 173.2096710205078, 175.23047471046448, 177.25127840042114, 179.13919067382812, 181.0271029472351, 183.279798746109, 185.5324945449829, 187.31343173980713, 189.09436893463135, 191.13030982017517, 193.166250705719, 194.91557836532593, 196.66490602493286, 198.5311942100525, 200.39748239517212, 202.30742621421814, 204.21737003326416, 206.13262224197388, 208.0478744506836, 209.9600579738617, 211.8722414970398, 213.97174978256226, 216.07125806808472, 218.02028393745422, 219.96930980682373, 222.01364159584045, 224.05797338485718, 226.08032083511353, 228.10266828536987, 229.9118103981018, 231.72095251083374, 233.97455263137817, 236.2281527519226, 238.0082757472992, 239.78839874267578, 241.8740565776825, 243.9597144126892, 245.88266849517822, 247.80562257766724, 249.79795908927917, 251.7902956008911, 253.67137742042542, 255.55245923995972, 257.5075116157532, 259.46256399154663, 261.3038823604584, 263.1452007293701, 265.1151854991913, 267.08517026901245, 268.9612720012665, 270.8373737335205, 272.693293094635, 274.5492124557495, 276.60866832733154, 278.6681241989136, 280.4474833011627, 282.22684240341187, 284.2588882446289, 286.29093408584595, 288.2254660129547, 290.1599979400635, 292.04976987838745, 293.9395418167114, 295.77865290641785, 297.61776399612427, 299.60253071784973, 301.5872974395752, 303.48851323127747, 305.38972902297974, 307.70216512680054, 310.01460123062134, 311.8683032989502, 313.72200536727905, 315.800270318985, 317.8785352706909, 319.8876144886017, 321.89669370651245, 323.7117533683777, 325.5268130302429, 327.55295181274414, 329.57909059524536, 331.49140906333923, 333.4037275314331, 335.5302906036377, 337.6568536758423, 339.6875870227814, 341.71832036972046, 343.6152102947235, 345.51210021972656, 347.2973129749298, 349.08252573013306, 351.09975385665894, 353.1169819831848, 354.8040051460266, 356.4910283088684, 358.4840831756592, 360.47713804244995, 362.35788106918335, 364.23862409591675, 366.17271852493286, 368.106812953949, 369.97962617874146, 371.85243940353394, 373.7432949542999, 375.6341505050659, 377.5313596725464, 379.42856884002686, 381.43860244750977, 383.4486360549927, 385.4007713794708, 387.352906703949, 389.15618348121643, 390.9594602584839, 393.0605368614197, 395.16161346435547]
[25.82, 25.82, 36.64, 36.64, 53.14, 53.14, 63.025, 63.025, 69.565, 69.565, 72.72, 72.72, 84.22, 84.22, 87.26, 87.26, 87.655, 87.655, 89.04, 89.04, 89.38, 89.38, 89.845, 89.845, 90.075, 90.075, 90.46, 90.46, 90.79, 90.79, 90.905, 90.905, 91.41, 91.41, 91.78, 91.78, 91.845, 91.845, 91.98, 91.98, 92.16, 92.16, 92.295, 92.295, 92.38, 92.38, 92.445, 92.445, 92.625, 92.625, 92.74, 92.74, 92.86, 92.86, 92.96, 92.96, 93.01, 93.01, 93.215, 93.215, 93.305, 93.305, 93.375, 93.375, 93.385, 93.385, 93.495, 93.495, 93.575, 93.575, 93.595, 93.595, 93.7, 93.7, 93.755, 93.755, 93.68, 93.68, 93.735, 93.735, 93.87, 93.87, 93.955, 93.955, 94.015, 94.015, 94.025, 94.025, 94.06, 94.06, 94.13, 94.13, 94.205, 94.205, 94.275, 94.275, 94.375, 94.375, 94.36, 94.36, 94.375, 94.375, 94.37, 94.37, 94.44, 94.44, 94.465, 94.465, 94.48, 94.48, 94.505, 94.505, 94.51, 94.51, 94.545, 94.545, 94.57, 94.57, 94.57, 94.57, 94.68, 94.68, 94.685, 94.685, 94.695, 94.695, 94.71, 94.71, 94.72, 94.72, 94.73, 94.73, 94.785, 94.785, 94.805, 94.805, 94.81, 94.81, 94.815, 94.815, 94.88, 94.88, 94.9, 94.9, 94.9, 94.9, 94.885, 94.885, 94.88, 94.88, 94.86, 94.86, 94.885, 94.885, 94.965, 94.965, 95.015, 95.015, 95.02, 95.02, 94.995, 94.995, 95.03, 95.03, 95.045, 95.045, 95.065, 95.065, 95.07, 95.07, 95.075, 95.075, 95.085, 95.085, 95.14, 95.14, 95.115, 95.115, 95.11, 95.11, 95.085, 95.085, 95.085, 95.085, 95.11, 95.11, 95.14, 95.14, 95.145, 95.145, 95.19, 95.19, 95.22, 95.22, 95.24, 95.24, 95.275, 95.275, 95.285, 95.285, 95.4, 95.4]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.298, Test accuracy: 18.29
Round   1, Train loss: 2.296, Test loss: 2.290, Test accuracy: 33.62
Round   2, Train loss: 2.277, Test loss: 2.237, Test accuracy: 27.73
Round   3, Train loss: 2.199, Test loss: 2.131, Test accuracy: 35.38
Round   4, Train loss: 2.076, Test loss: 2.010, Test accuracy: 47.75
Round   5, Train loss: 1.948, Test loss: 1.915, Test accuracy: 58.80
Round   6, Train loss: 1.838, Test loss: 1.838, Test accuracy: 64.71
Round   7, Train loss: 1.758, Test loss: 1.781, Test accuracy: 70.06
Round   8, Train loss: 1.679, Test loss: 1.718, Test accuracy: 76.31
Round   9, Train loss: 1.624, Test loss: 1.687, Test accuracy: 79.20
Round  10, Train loss: 1.613, Test loss: 1.654, Test accuracy: 82.38
Round  11, Train loss: 1.594, Test loss: 1.640, Test accuracy: 83.53
Round  12, Train loss: 1.580, Test loss: 1.621, Test accuracy: 85.23
Round  13, Train loss: 1.571, Test loss: 1.614, Test accuracy: 85.84
Round  14, Train loss: 1.562, Test loss: 1.601, Test accuracy: 86.99
Round  15, Train loss: 1.554, Test loss: 1.596, Test accuracy: 87.41
Round  16, Train loss: 1.549, Test loss: 1.582, Test accuracy: 88.86
Round  17, Train loss: 1.546, Test loss: 1.580, Test accuracy: 88.93
Round  18, Train loss: 1.543, Test loss: 1.578, Test accuracy: 89.08
Round  19, Train loss: 1.540, Test loss: 1.578, Test accuracy: 89.08
Round  20, Train loss: 1.539, Test loss: 1.562, Test accuracy: 90.79
Round  21, Train loss: 1.539, Test loss: 1.558, Test accuracy: 91.11
Round  22, Train loss: 1.535, Test loss: 1.559, Test accuracy: 90.96
Round  23, Train loss: 1.531, Test loss: 1.556, Test accuracy: 91.18
Round  24, Train loss: 1.528, Test loss: 1.555, Test accuracy: 91.26
Round  25, Train loss: 1.526, Test loss: 1.554, Test accuracy: 91.37
Round  26, Train loss: 1.522, Test loss: 1.551, Test accuracy: 91.74
Round  27, Train loss: 1.526, Test loss: 1.550, Test accuracy: 91.80
Round  28, Train loss: 1.521, Test loss: 1.549, Test accuracy: 91.78
Round  29, Train loss: 1.521, Test loss: 1.548, Test accuracy: 91.87
Round  30, Train loss: 1.517, Test loss: 1.549, Test accuracy: 91.83
Round  31, Train loss: 1.519, Test loss: 1.548, Test accuracy: 91.78
Round  32, Train loss: 1.516, Test loss: 1.547, Test accuracy: 91.82
Round  33, Train loss: 1.514, Test loss: 1.545, Test accuracy: 92.06
Round  34, Train loss: 1.510, Test loss: 1.544, Test accuracy: 92.19
Round  35, Train loss: 1.510, Test loss: 1.543, Test accuracy: 92.33
Round  36, Train loss: 1.509, Test loss: 1.541, Test accuracy: 92.45
Round  37, Train loss: 1.511, Test loss: 1.542, Test accuracy: 92.22
Round  38, Train loss: 1.509, Test loss: 1.542, Test accuracy: 92.32
Round  39, Train loss: 1.507, Test loss: 1.542, Test accuracy: 92.41
Round  40, Train loss: 1.508, Test loss: 1.541, Test accuracy: 92.36
Round  41, Train loss: 1.503, Test loss: 1.540, Test accuracy: 92.47
Round  42, Train loss: 1.508, Test loss: 1.541, Test accuracy: 92.36
Round  43, Train loss: 1.504, Test loss: 1.541, Test accuracy: 92.30
Round  44, Train loss: 1.503, Test loss: 1.540, Test accuracy: 92.45
Round  45, Train loss: 1.507, Test loss: 1.540, Test accuracy: 92.39
Round  46, Train loss: 1.503, Test loss: 1.538, Test accuracy: 92.61
Round  47, Train loss: 1.502, Test loss: 1.538, Test accuracy: 92.61
Round  48, Train loss: 1.506, Test loss: 1.538, Test accuracy: 92.62
Round  49, Train loss: 1.498, Test loss: 1.537, Test accuracy: 92.72
Round  50, Train loss: 1.501, Test loss: 1.536, Test accuracy: 92.83
Round  51, Train loss: 1.498, Test loss: 1.536, Test accuracy: 92.77
Round  52, Train loss: 1.494, Test loss: 1.536, Test accuracy: 92.73
Round  53, Train loss: 1.494, Test loss: 1.536, Test accuracy: 92.86
Round  54, Train loss: 1.492, Test loss: 1.536, Test accuracy: 92.79
Round  55, Train loss: 1.497, Test loss: 1.535, Test accuracy: 92.91
Round  56, Train loss: 1.498, Test loss: 1.535, Test accuracy: 92.90
Round  57, Train loss: 1.494, Test loss: 1.534, Test accuracy: 92.95
Round  58, Train loss: 1.496, Test loss: 1.533, Test accuracy: 93.08
Round  59, Train loss: 1.496, Test loss: 1.533, Test accuracy: 93.03
Round  60, Train loss: 1.497, Test loss: 1.534, Test accuracy: 93.05
Round  61, Train loss: 1.495, Test loss: 1.534, Test accuracy: 92.99
Round  62, Train loss: 1.493, Test loss: 1.534, Test accuracy: 93.05
Round  63, Train loss: 1.493, Test loss: 1.534, Test accuracy: 93.02
Round  64, Train loss: 1.491, Test loss: 1.534, Test accuracy: 93.06
Round  65, Train loss: 1.493, Test loss: 1.534, Test accuracy: 92.95
Round  66, Train loss: 1.490, Test loss: 1.534, Test accuracy: 93.00
Round  67, Train loss: 1.492, Test loss: 1.533, Test accuracy: 93.06
Round  68, Train loss: 1.489, Test loss: 1.533, Test accuracy: 93.12
Round  69, Train loss: 1.494, Test loss: 1.533, Test accuracy: 93.11
Round  70, Train loss: 1.495, Test loss: 1.533, Test accuracy: 93.12
Round  71, Train loss: 1.491, Test loss: 1.533, Test accuracy: 93.08
Round  72, Train loss: 1.488, Test loss: 1.533, Test accuracy: 92.98
Round  73, Train loss: 1.491, Test loss: 1.533, Test accuracy: 93.00
Round  74, Train loss: 1.492, Test loss: 1.533, Test accuracy: 93.08
Round  75, Train loss: 1.490, Test loss: 1.532, Test accuracy: 93.08
Round  76, Train loss: 1.487, Test loss: 1.532, Test accuracy: 93.11
Round  77, Train loss: 1.490, Test loss: 1.531, Test accuracy: 93.30
Round  78, Train loss: 1.489, Test loss: 1.531, Test accuracy: 93.31
Round  79, Train loss: 1.487, Test loss: 1.531, Test accuracy: 93.28
Round  80, Train loss: 1.488, Test loss: 1.531, Test accuracy: 93.36
Round  81, Train loss: 1.489, Test loss: 1.530, Test accuracy: 93.36
Round  82, Train loss: 1.489, Test loss: 1.530, Test accuracy: 93.34
Round  83, Train loss: 1.488, Test loss: 1.530, Test accuracy: 93.34
Round  84, Train loss: 1.486, Test loss: 1.530, Test accuracy: 93.35
Round  85, Train loss: 1.488, Test loss: 1.530, Test accuracy: 93.33
Round  86, Train loss: 1.490, Test loss: 1.530, Test accuracy: 93.28
Round  87, Train loss: 1.486, Test loss: 1.530, Test accuracy: 93.33
Round  88, Train loss: 1.485, Test loss: 1.529, Test accuracy: 93.38
Round  89, Train loss: 1.488, Test loss: 1.529, Test accuracy: 93.43
Round  90, Train loss: 1.487, Test loss: 1.529, Test accuracy: 93.40
Round  91, Train loss: 1.487, Test loss: 1.529, Test accuracy: 93.47
Round  92, Train loss: 1.486, Test loss: 1.528, Test accuracy: 93.52
Round  93, Train loss: 1.487, Test loss: 1.529, Test accuracy: 93.36
Round  94, Train loss: 1.487, Test loss: 1.529, Test accuracy: 93.35
Round  95, Train loss: 1.483, Test loss: 1.528, Test accuracy: 93.40
Round  96, Train loss: 1.486, Test loss: 1.528, Test accuracy: 93.43
Round  97, Train loss: 1.488, Test loss: 1.528, Test accuracy: 93.45
Round  98, Train loss: 1.485, Test loss: 1.528, Test accuracy: 93.53/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.486, Test loss: 1.528, Test accuracy: 93.58
Final Round, Train loss: 1.484, Test loss: 1.528, Test accuracy: 93.52
Average accuracy final 10 rounds: 93.4505 

2491.784824848175
[2.1048498153686523, 4.209699630737305, 6.215636491775513, 8.22157335281372, 9.970385789871216, 11.719198226928711, 13.939005851745605, 16.1588134765625, 18.289390087127686, 20.41996669769287, 22.796449422836304, 25.172932147979736, 27.200051069259644, 29.22716999053955, 31.342373847961426, 33.4575777053833, 35.29322123527527, 37.128864765167236, 39.203505516052246, 41.278146266937256, 43.225048542022705, 45.171950817108154, 47.16307878494263, 49.1542067527771, 51.080408573150635, 53.00661039352417, 55.01026153564453, 57.01391267776489, 59.115182399749756, 61.21645212173462, 62.90192914009094, 64.58740615844727, 66.5552887916565, 68.52317142486572, 70.34821963310242, 72.17326784133911, 73.99663186073303, 75.81999588012695, 77.68937754631042, 79.5587592124939, 81.42469096183777, 83.29062271118164, 85.31196618080139, 87.33330965042114, 89.03286671638489, 90.73242378234863, 92.74870228767395, 94.76498079299927, 96.7237901687622, 98.68259954452515, 100.33904027938843, 101.99548101425171, 103.83241391181946, 105.6693468093872, 107.4921202659607, 109.31489372253418, 111.28883838653564, 113.26278305053711, 114.97622108459473, 116.68965911865234, 118.67782425880432, 120.6659893989563, 122.65389442443848, 124.64179944992065, 126.30157732963562, 127.96135520935059, 129.82982921600342, 131.69830322265625, 133.59535479545593, 135.49240636825562, 137.48067426681519, 139.46894216537476, 141.3000512123108, 143.13116025924683, 144.92668271064758, 146.72220516204834, 148.6521611213684, 150.58211708068848, 152.2983386516571, 154.01456022262573, 155.83306288719177, 157.6515655517578, 159.45979952812195, 161.26803350448608, 163.1092517375946, 164.95046997070312, 166.77902698516846, 168.6075839996338, 170.3445897102356, 172.0815954208374, 174.1441695690155, 176.2067437171936, 177.8449637889862, 179.4831838607788, 181.29220604896545, 183.1012282371521, 185.13787627220154, 187.17452430725098, 189.01634216308594, 190.8581600189209, 192.6649079322815, 194.4716558456421, 196.22851133346558, 197.98536682128906, 199.9229815006256, 201.86059617996216, 203.63813042640686, 205.41566467285156, 207.1170425415039, 208.81842041015625, 210.6680405139923, 212.51766061782837, 214.27128553390503, 216.0249104499817, 217.84778833389282, 219.67066621780396, 221.36922812461853, 223.0677900314331, 224.81381464004517, 226.55983924865723, 228.58282589912415, 230.60581254959106, 232.277738571167, 233.94966459274292, 235.8965904712677, 237.84351634979248, 239.64682340621948, 241.45013046264648, 243.2093710899353, 244.96861171722412, 246.86742401123047, 248.76623630523682, 250.5603666305542, 252.35449695587158, 254.2718164920807, 256.1891360282898, 257.8631627559662, 259.5371894836426, 261.37450909614563, 263.2118287086487, 265.08118772506714, 266.9505467414856, 268.6075768470764, 270.26460695266724, 272.0965392589569, 273.9284715652466, 275.73687076568604, 277.5452699661255, 279.5513434410095, 281.55741691589355, 283.302264213562, 285.04711151123047, 286.8016471862793, 288.5561828613281, 290.4926242828369, 292.4290657043457, 294.09536480903625, 295.7616639137268, 297.55725932121277, 299.35285472869873, 301.09100699424744, 302.82915925979614, 304.7068486213684, 306.5845379829407, 308.36675572395325, 310.1489734649658, 311.86590027809143, 313.58282709121704, 315.5128798484802, 317.4429326057434, 319.08462476730347, 320.7263169288635, 322.4464821815491, 324.1666474342346, 326.13918900489807, 328.1117305755615, 329.875679731369, 331.6396288871765, 333.4271242618561, 335.21461963653564, 336.9525113105774, 338.69040298461914, 340.6351492404938, 342.5798954963684, 344.4148874282837, 346.249879360199, 347.8859236240387, 349.5219678878784, 351.40626072883606, 353.2905535697937, 355.0653851032257, 356.8402166366577, 358.60286235809326, 360.3655080795288, 362.2177245616913, 364.06994104385376, 365.85650968551636, 367.64307832717896, 369.5454795360565, 371.4478807449341, 373.2297146320343, 375.0115485191345]
[18.285, 18.285, 33.615, 33.615, 27.735, 27.735, 35.385, 35.385, 47.75, 47.75, 58.805, 58.805, 64.71, 64.71, 70.065, 70.065, 76.315, 76.315, 79.2, 79.2, 82.38, 82.38, 83.535, 83.535, 85.235, 85.235, 85.845, 85.845, 86.99, 86.99, 87.41, 87.41, 88.855, 88.855, 88.93, 88.93, 89.085, 89.085, 89.08, 89.08, 90.79, 90.79, 91.11, 91.11, 90.96, 90.96, 91.18, 91.18, 91.26, 91.26, 91.37, 91.37, 91.74, 91.74, 91.795, 91.795, 91.78, 91.78, 91.87, 91.87, 91.83, 91.83, 91.785, 91.785, 91.82, 91.82, 92.06, 92.06, 92.195, 92.195, 92.325, 92.325, 92.455, 92.455, 92.215, 92.215, 92.32, 92.32, 92.41, 92.41, 92.365, 92.365, 92.465, 92.465, 92.36, 92.36, 92.3, 92.3, 92.455, 92.455, 92.395, 92.395, 92.605, 92.605, 92.605, 92.605, 92.62, 92.62, 92.725, 92.725, 92.83, 92.83, 92.765, 92.765, 92.73, 92.73, 92.865, 92.865, 92.79, 92.79, 92.905, 92.905, 92.9, 92.9, 92.95, 92.95, 93.08, 93.08, 93.035, 93.035, 93.045, 93.045, 92.99, 92.99, 93.05, 93.05, 93.02, 93.02, 93.055, 93.055, 92.95, 92.95, 92.995, 92.995, 93.055, 93.055, 93.12, 93.12, 93.11, 93.11, 93.12, 93.12, 93.075, 93.075, 92.98, 92.98, 93.0, 93.0, 93.075, 93.075, 93.08, 93.08, 93.115, 93.115, 93.3, 93.3, 93.315, 93.315, 93.28, 93.28, 93.36, 93.36, 93.36, 93.36, 93.34, 93.34, 93.34, 93.34, 93.35, 93.35, 93.335, 93.335, 93.285, 93.285, 93.325, 93.325, 93.38, 93.38, 93.43, 93.43, 93.4, 93.4, 93.475, 93.475, 93.515, 93.515, 93.365, 93.365, 93.35, 93.35, 93.4, 93.4, 93.43, 93.43, 93.455, 93.455, 93.53, 93.53, 93.585, 93.585, 93.515, 93.515]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.298, Test loss: 2.293, Test accuracy: 38.27
Round   1, Train loss: 2.274, Test loss: 2.227, Test accuracy: 41.82
Round   2, Train loss: 2.008, Test loss: 1.861, Test accuracy: 69.44
Round   3, Train loss: 1.731, Test loss: 1.723, Test accuracy: 78.25
Round   4, Train loss: 1.635, Test loss: 1.654, Test accuracy: 83.71
Round   5, Train loss: 1.613, Test loss: 1.610, Test accuracy: 87.00
Round   6, Train loss: 1.567, Test loss: 1.597, Test accuracy: 87.90
Round   7, Train loss: 1.567, Test loss: 1.583, Test accuracy: 89.03
Round   8, Train loss: 1.553, Test loss: 1.578, Test accuracy: 89.22
Round   9, Train loss: 1.540, Test loss: 1.574, Test accuracy: 89.73
Round  10, Train loss: 1.536, Test loss: 1.571, Test accuracy: 89.92
Round  11, Train loss: 1.534, Test loss: 1.566, Test accuracy: 90.34
Round  12, Train loss: 1.523, Test loss: 1.564, Test accuracy: 90.57
Round  13, Train loss: 1.525, Test loss: 1.555, Test accuracy: 91.33
Round  14, Train loss: 1.519, Test loss: 1.553, Test accuracy: 91.38
Round  15, Train loss: 1.517, Test loss: 1.549, Test accuracy: 91.81
Round  16, Train loss: 1.516, Test loss: 1.549, Test accuracy: 91.69
Round  17, Train loss: 1.512, Test loss: 1.549, Test accuracy: 91.68
Round  18, Train loss: 1.513, Test loss: 1.547, Test accuracy: 91.94
Round  19, Train loss: 1.516, Test loss: 1.545, Test accuracy: 92.09
Round  20, Train loss: 1.510, Test loss: 1.545, Test accuracy: 92.06
Round  21, Train loss: 1.510, Test loss: 1.542, Test accuracy: 92.40
Round  22, Train loss: 1.505, Test loss: 1.540, Test accuracy: 92.42
Round  23, Train loss: 1.502, Test loss: 1.540, Test accuracy: 92.52
Round  24, Train loss: 1.501, Test loss: 1.540, Test accuracy: 92.56
Round  25, Train loss: 1.507, Test loss: 1.539, Test accuracy: 92.58
Round  26, Train loss: 1.502, Test loss: 1.538, Test accuracy: 92.69
Round  27, Train loss: 1.501, Test loss: 1.537, Test accuracy: 92.67
Round  28, Train loss: 1.496, Test loss: 1.537, Test accuracy: 92.83
Round  29, Train loss: 1.501, Test loss: 1.536, Test accuracy: 92.86
Round  30, Train loss: 1.494, Test loss: 1.535, Test accuracy: 92.94
Round  31, Train loss: 1.491, Test loss: 1.534, Test accuracy: 93.03
Round  32, Train loss: 1.491, Test loss: 1.534, Test accuracy: 93.03
Round  33, Train loss: 1.496, Test loss: 1.533, Test accuracy: 93.03
Round  34, Train loss: 1.492, Test loss: 1.532, Test accuracy: 93.14
Round  35, Train loss: 1.489, Test loss: 1.532, Test accuracy: 93.11
Round  36, Train loss: 1.488, Test loss: 1.532, Test accuracy: 93.11
Round  37, Train loss: 1.496, Test loss: 1.531, Test accuracy: 93.23
Round  38, Train loss: 1.493, Test loss: 1.532, Test accuracy: 93.20
Round  39, Train loss: 1.492, Test loss: 1.531, Test accuracy: 93.36
Round  40, Train loss: 1.489, Test loss: 1.530, Test accuracy: 93.47
Round  41, Train loss: 1.487, Test loss: 1.531, Test accuracy: 93.30
Round  42, Train loss: 1.486, Test loss: 1.530, Test accuracy: 93.33
Round  43, Train loss: 1.486, Test loss: 1.530, Test accuracy: 93.33
Round  44, Train loss: 1.484, Test loss: 1.529, Test accuracy: 93.50
Round  45, Train loss: 1.487, Test loss: 1.528, Test accuracy: 93.52
Round  46, Train loss: 1.488, Test loss: 1.527, Test accuracy: 93.61
Round  47, Train loss: 1.489, Test loss: 1.527, Test accuracy: 93.64
Round  48, Train loss: 1.483, Test loss: 1.527, Test accuracy: 93.70
Round  49, Train loss: 1.482, Test loss: 1.527, Test accuracy: 93.59
Round  50, Train loss: 1.482, Test loss: 1.527, Test accuracy: 93.78
Round  51, Train loss: 1.483, Test loss: 1.526, Test accuracy: 93.75
Round  52, Train loss: 1.482, Test loss: 1.526, Test accuracy: 93.80
Round  53, Train loss: 1.479, Test loss: 1.526, Test accuracy: 93.71
Round  54, Train loss: 1.482, Test loss: 1.525, Test accuracy: 93.81
Round  55, Train loss: 1.480, Test loss: 1.525, Test accuracy: 93.72
Round  56, Train loss: 1.481, Test loss: 1.524, Test accuracy: 93.86
Round  57, Train loss: 1.479, Test loss: 1.524, Test accuracy: 93.92
Round  58, Train loss: 1.480, Test loss: 1.524, Test accuracy: 93.86
Round  59, Train loss: 1.479, Test loss: 1.524, Test accuracy: 93.86
Round  60, Train loss: 1.483, Test loss: 1.524, Test accuracy: 93.94
Round  61, Train loss: 1.482, Test loss: 1.523, Test accuracy: 93.97
Round  62, Train loss: 1.478, Test loss: 1.523, Test accuracy: 94.00
Round  63, Train loss: 1.479, Test loss: 1.523, Test accuracy: 94.02
Round  64, Train loss: 1.480, Test loss: 1.522, Test accuracy: 94.06
Round  65, Train loss: 1.478, Test loss: 1.522, Test accuracy: 94.02
Round  66, Train loss: 1.482, Test loss: 1.522, Test accuracy: 94.05
Round  67, Train loss: 1.477, Test loss: 1.522, Test accuracy: 94.17
Round  68, Train loss: 1.480, Test loss: 1.521, Test accuracy: 94.24
Round  69, Train loss: 1.478, Test loss: 1.522, Test accuracy: 94.14
Round  70, Train loss: 1.479, Test loss: 1.521, Test accuracy: 94.21
Round  71, Train loss: 1.476, Test loss: 1.521, Test accuracy: 94.31
Round  72, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.34
Round  73, Train loss: 1.480, Test loss: 1.520, Test accuracy: 94.25
Round  74, Train loss: 1.476, Test loss: 1.520, Test accuracy: 94.30
Round  75, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.28
Round  76, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.17
Round  77, Train loss: 1.477, Test loss: 1.520, Test accuracy: 94.28
Round  78, Train loss: 1.477, Test loss: 1.520, Test accuracy: 94.31
Round  79, Train loss: 1.479, Test loss: 1.520, Test accuracy: 94.33
Round  80, Train loss: 1.477, Test loss: 1.519, Test accuracy: 94.39
Round  81, Train loss: 1.476, Test loss: 1.519, Test accuracy: 94.39
Round  82, Train loss: 1.476, Test loss: 1.519, Test accuracy: 94.38
Round  83, Train loss: 1.477, Test loss: 1.519, Test accuracy: 94.42
Round  84, Train loss: 1.477, Test loss: 1.519, Test accuracy: 94.41
Round  85, Train loss: 1.479, Test loss: 1.519, Test accuracy: 94.47
Round  86, Train loss: 1.478, Test loss: 1.519, Test accuracy: 94.42
Round  87, Train loss: 1.478, Test loss: 1.519, Test accuracy: 94.38
Round  88, Train loss: 1.477, Test loss: 1.519, Test accuracy: 94.39
Round  89, Train loss: 1.477, Test loss: 1.518, Test accuracy: 94.45
Round  90, Train loss: 1.477, Test loss: 1.519, Test accuracy: 94.44
Round  91, Train loss: 1.478, Test loss: 1.519, Test accuracy: 94.44
Round  92, Train loss: 1.476, Test loss: 1.519, Test accuracy: 94.41
Round  93, Train loss: 1.478, Test loss: 1.518, Test accuracy: 94.48
Round  94, Train loss: 1.476, Test loss: 1.518, Test accuracy: 94.52
Round  95, Train loss: 1.477, Test loss: 1.518, Test accuracy: 94.53
Round  96, Train loss: 1.475, Test loss: 1.518, Test accuracy: 94.52
Round  97, Train loss: 1.476, Test loss: 1.518, Test accuracy: 94.50
Round  98, Train loss: 1.477, Test loss: 1.518, Test accuracy: 94.48/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.474, Test loss: 1.518, Test accuracy: 94.48
Final Round, Train loss: 1.476, Test loss: 1.519, Test accuracy: 94.46
Average accuracy final 10 rounds: 94.48149999999998 

2826.974807024002
[1.8916518688201904, 3.783303737640381, 5.698327302932739, 7.613350868225098, 9.439844608306885, 11.266338348388672, 13.304095029830933, 15.341851711273193, 17.46089482307434, 19.57993793487549, 21.64838671684265, 23.716835498809814, 26.05982780456543, 28.402820110321045, 30.232916593551636, 32.06301307678223, 33.90611720085144, 35.749221324920654, 37.71508169174194, 39.68094205856323, 41.562938928604126, 43.44493579864502, 45.40342473983765, 47.36191368103027, 49.0776469707489, 50.79338026046753, 52.80183982849121, 54.81029939651489, 56.71754431724548, 58.624789237976074, 60.80221891403198, 62.97964859008789, 65.30114769935608, 67.62264680862427, 69.7151939868927, 71.80774116516113, 73.69626665115356, 75.584792137146, 77.56061124801636, 79.53643035888672, 81.63673210144043, 83.73703384399414, 85.70930218696594, 87.68157052993774, 89.43084216117859, 91.18011379241943, 93.19484734535217, 95.20958089828491, 97.04707789421082, 98.88457489013672, 100.76204776763916, 102.6395206451416, 104.36560797691345, 106.0916953086853, 108.19832348823547, 110.30495166778564, 112.72487282752991, 115.14479398727417, 117.44309830665588, 119.7414026260376, 122.25356006622314, 124.76571750640869, 127.10600733757019, 129.4462971687317, 131.57912731170654, 133.7119574546814, 135.9267213344574, 138.1414852142334, 140.59060955047607, 143.03973388671875, 145.4701578617096, 147.90058183670044, 150.5964436531067, 153.29230546951294, 155.5435540676117, 157.79480266571045, 160.21344447135925, 162.63208627700806, 164.9861946105957, 167.34030294418335, 169.96766066551208, 172.59501838684082, 175.32438850402832, 178.05375862121582, 180.1689100265503, 182.28406143188477, 184.77693629264832, 187.26981115341187, 189.82526469230652, 192.38071823120117, 194.87363076210022, 197.36654329299927, 199.1752269268036, 200.9839105606079, 202.94543409347534, 204.90695762634277, 207.00922632217407, 209.11149501800537, 210.83139371871948, 212.5512924194336, 214.62868690490723, 216.70608139038086, 218.56185102462769, 220.4176206588745, 222.40852904319763, 224.39943742752075, 226.1724009513855, 227.94536447525024, 230.00583338737488, 232.0663022994995, 234.16402411460876, 236.26174592971802, 238.06326413154602, 239.86478233337402, 241.87784075737, 243.89089918136597, 245.849200963974, 247.80750274658203, 249.7012324333191, 251.59496212005615, 253.48145723342896, 255.36795234680176, 257.36603450775146, 259.3641166687012, 261.4914665222168, 263.6188163757324, 265.49197697639465, 267.3651375770569, 269.6681089401245, 271.97108030319214, 274.112952709198, 276.25482511520386, 278.4964528083801, 280.7380805015564, 282.82976961135864, 284.9214587211609, 286.89308428764343, 288.864709854126, 290.85419487953186, 292.84367990493774, 294.81905341148376, 296.7944269180298, 299.1279911994934, 301.46155548095703, 303.6596806049347, 305.85780572891235, 308.1655008792877, 310.4731960296631, 312.62585616111755, 314.778516292572, 316.93802642822266, 319.0975365638733, 321.4407784938812, 323.78402042388916, 325.99610805511475, 328.20819568634033, 330.6162736415863, 333.0243515968323, 335.13009333610535, 337.2358350753784, 339.71428966522217, 342.1927442550659, 344.4005219936371, 346.60829973220825, 348.8925693035126, 351.1768388748169, 353.3716197013855, 355.5664005279541, 357.9853744506836, 360.4043483734131, 362.6471116542816, 364.88987493515015, 367.0786771774292, 369.26747941970825, 371.68105936050415, 374.09463930130005, 376.37411236763, 378.65358543395996, 380.8797285556793, 383.1058716773987, 385.27352690696716, 387.44118213653564, 389.79635787010193, 392.1515336036682, 394.20371437072754, 396.25589513778687, 398.60762071609497, 400.9593462944031, 403.33445930480957, 405.70957231521606, 407.5762209892273, 409.4428696632385, 411.39716625213623, 413.35146284103394, 415.2778363227844, 417.2042098045349, 419.34009981155396, 421.475989818573, 423.345162153244, 425.21433448791504, 427.17043900489807, 429.1265435218811]
[38.265, 38.265, 41.82, 41.82, 69.44, 69.44, 78.25, 78.25, 83.71, 83.71, 87.005, 87.005, 87.9, 87.9, 89.035, 89.035, 89.215, 89.215, 89.735, 89.735, 89.925, 89.925, 90.34, 90.34, 90.57, 90.57, 91.33, 91.33, 91.38, 91.38, 91.815, 91.815, 91.695, 91.695, 91.68, 91.68, 91.94, 91.94, 92.095, 92.095, 92.06, 92.06, 92.4, 92.4, 92.425, 92.425, 92.52, 92.52, 92.56, 92.56, 92.58, 92.58, 92.695, 92.695, 92.665, 92.665, 92.835, 92.835, 92.855, 92.855, 92.935, 92.935, 93.025, 93.025, 93.03, 93.03, 93.025, 93.025, 93.145, 93.145, 93.105, 93.105, 93.105, 93.105, 93.23, 93.23, 93.205, 93.205, 93.355, 93.355, 93.465, 93.465, 93.295, 93.295, 93.33, 93.33, 93.325, 93.325, 93.495, 93.495, 93.515, 93.515, 93.605, 93.605, 93.64, 93.64, 93.705, 93.705, 93.59, 93.59, 93.785, 93.785, 93.755, 93.755, 93.795, 93.795, 93.71, 93.71, 93.805, 93.805, 93.715, 93.715, 93.86, 93.86, 93.915, 93.915, 93.865, 93.865, 93.86, 93.86, 93.935, 93.935, 93.975, 93.975, 93.995, 93.995, 94.02, 94.02, 94.06, 94.06, 94.015, 94.015, 94.045, 94.045, 94.17, 94.17, 94.24, 94.24, 94.145, 94.145, 94.21, 94.21, 94.31, 94.31, 94.345, 94.345, 94.245, 94.245, 94.3, 94.3, 94.285, 94.285, 94.17, 94.17, 94.275, 94.275, 94.315, 94.315, 94.33, 94.33, 94.39, 94.39, 94.395, 94.395, 94.38, 94.38, 94.42, 94.42, 94.41, 94.41, 94.47, 94.47, 94.415, 94.415, 94.375, 94.375, 94.385, 94.385, 94.45, 94.45, 94.44, 94.44, 94.44, 94.44, 94.41, 94.41, 94.485, 94.485, 94.52, 94.52, 94.53, 94.53, 94.52, 94.52, 94.505, 94.505, 94.485, 94.485, 94.48, 94.48, 94.46, 94.46]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.293, Test accuracy: 23.34
Round   1, Train loss: 2.267, Test loss: 2.214, Test accuracy: 37.91
Round   2, Train loss: 2.030, Test loss: 1.950, Test accuracy: 66.24
Round   3, Train loss: 1.786, Test loss: 1.859, Test accuracy: 69.29
Round   4, Train loss: 1.755, Test loss: 1.783, Test accuracy: 73.24
Round   5, Train loss: 1.678, Test loss: 1.720, Test accuracy: 77.39
Round   6, Train loss: 1.621, Test loss: 1.711, Test accuracy: 77.62
Round   7, Train loss: 1.636, Test loss: 1.691, Test accuracy: 78.75
Round   8, Train loss: 1.620, Test loss: 1.683, Test accuracy: 79.32
Round   9, Train loss: 1.605, Test loss: 1.679, Test accuracy: 79.67
Round  10, Train loss: 1.599, Test loss: 1.676, Test accuracy: 79.66
Round  11, Train loss: 1.591, Test loss: 1.674, Test accuracy: 79.78
Round  12, Train loss: 1.597, Test loss: 1.662, Test accuracy: 80.45
Round  13, Train loss: 1.591, Test loss: 1.661, Test accuracy: 80.49
Round  14, Train loss: 1.548, Test loss: 1.633, Test accuracy: 84.19
Round  15, Train loss: 1.515, Test loss: 1.623, Test accuracy: 85.26
Round  16, Train loss: 1.508, Test loss: 1.614, Test accuracy: 85.88
Round  17, Train loss: 1.505, Test loss: 1.609, Test accuracy: 86.20
Round  18, Train loss: 1.500, Test loss: 1.607, Test accuracy: 86.33
Round  19, Train loss: 1.511, Test loss: 1.602, Test accuracy: 86.73
Round  20, Train loss: 1.495, Test loss: 1.598, Test accuracy: 86.97
Round  21, Train loss: 1.494, Test loss: 1.596, Test accuracy: 87.21
Round  22, Train loss: 1.485, Test loss: 1.595, Test accuracy: 87.17
Round  23, Train loss: 1.490, Test loss: 1.594, Test accuracy: 87.19
Round  24, Train loss: 1.485, Test loss: 1.593, Test accuracy: 87.37
Round  25, Train loss: 1.483, Test loss: 1.593, Test accuracy: 87.42
Round  26, Train loss: 1.487, Test loss: 1.593, Test accuracy: 87.39
Round  27, Train loss: 1.489, Test loss: 1.591, Test accuracy: 87.55
Round  28, Train loss: 1.486, Test loss: 1.591, Test accuracy: 87.58
Round  29, Train loss: 1.485, Test loss: 1.591, Test accuracy: 87.55
Round  30, Train loss: 1.484, Test loss: 1.591, Test accuracy: 87.56
Round  31, Train loss: 1.481, Test loss: 1.590, Test accuracy: 87.59
Round  32, Train loss: 1.484, Test loss: 1.590, Test accuracy: 87.60
Round  33, Train loss: 1.482, Test loss: 1.590, Test accuracy: 87.58
Round  34, Train loss: 1.483, Test loss: 1.590, Test accuracy: 87.59
Round  35, Train loss: 1.477, Test loss: 1.590, Test accuracy: 87.58
Round  36, Train loss: 1.481, Test loss: 1.590, Test accuracy: 87.61
Round  37, Train loss: 1.480, Test loss: 1.590, Test accuracy: 87.61
Round  38, Train loss: 1.481, Test loss: 1.589, Test accuracy: 87.61
Round  39, Train loss: 1.484, Test loss: 1.589, Test accuracy: 87.62
Round  40, Train loss: 1.482, Test loss: 1.589, Test accuracy: 87.63
Round  41, Train loss: 1.479, Test loss: 1.589, Test accuracy: 87.61
Round  42, Train loss: 1.482, Test loss: 1.589, Test accuracy: 87.66
Round  43, Train loss: 1.481, Test loss: 1.589, Test accuracy: 87.67
Round  44, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.67
Round  45, Train loss: 1.480, Test loss: 1.588, Test accuracy: 87.62
Round  46, Train loss: 1.481, Test loss: 1.588, Test accuracy: 87.65
Round  47, Train loss: 1.481, Test loss: 1.588, Test accuracy: 87.67
Round  48, Train loss: 1.483, Test loss: 1.588, Test accuracy: 87.64
Round  49, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.67
Round  50, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.63
Round  51, Train loss: 1.479, Test loss: 1.589, Test accuracy: 87.62
Round  52, Train loss: 1.480, Test loss: 1.588, Test accuracy: 87.67
Round  53, Train loss: 1.476, Test loss: 1.588, Test accuracy: 87.64
Round  54, Train loss: 1.480, Test loss: 1.588, Test accuracy: 87.64
Round  55, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.61
Round  56, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.62
Round  57, Train loss: 1.480, Test loss: 1.588, Test accuracy: 87.61
Round  58, Train loss: 1.481, Test loss: 1.588, Test accuracy: 87.60
Round  59, Train loss: 1.478, Test loss: 1.588, Test accuracy: 87.61
Round  60, Train loss: 1.477, Test loss: 1.588, Test accuracy: 87.61
Round  61, Train loss: 1.475, Test loss: 1.588, Test accuracy: 87.59
Round  62, Train loss: 1.475, Test loss: 1.588, Test accuracy: 87.61
Round  63, Train loss: 1.480, Test loss: 1.588, Test accuracy: 87.63
Round  64, Train loss: 1.477, Test loss: 1.588, Test accuracy: 87.64
Round  65, Train loss: 1.478, Test loss: 1.588, Test accuracy: 87.62
Round  66, Train loss: 1.476, Test loss: 1.588, Test accuracy: 87.64
Round  67, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.61
Round  68, Train loss: 1.477, Test loss: 1.588, Test accuracy: 87.61
Round  69, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.64
Round  70, Train loss: 1.478, Test loss: 1.588, Test accuracy: 87.63
Round  71, Train loss: 1.476, Test loss: 1.588, Test accuracy: 87.62
Round  72, Train loss: 1.477, Test loss: 1.588, Test accuracy: 87.64
Round  73, Train loss: 1.478, Test loss: 1.588, Test accuracy: 87.68
Round  74, Train loss: 1.479, Test loss: 1.588, Test accuracy: 87.66
Round  75, Train loss: 1.475, Test loss: 1.587, Test accuracy: 87.67
Round  76, Train loss: 1.478, Test loss: 1.587, Test accuracy: 87.68
Round  77, Train loss: 1.476, Test loss: 1.587, Test accuracy: 87.67
Round  78, Train loss: 1.476, Test loss: 1.587, Test accuracy: 87.66
Round  79, Train loss: 1.478, Test loss: 1.587, Test accuracy: 87.65
Round  80, Train loss: 1.477, Test loss: 1.587, Test accuracy: 87.65
Round  81, Train loss: 1.478, Test loss: 1.587, Test accuracy: 87.66
Round  82, Train loss: 1.479, Test loss: 1.587, Test accuracy: 87.66
Round  83, Train loss: 1.477, Test loss: 1.587, Test accuracy: 87.66
Round  84, Train loss: 1.478, Test loss: 1.587, Test accuracy: 87.66
Round  85, Train loss: 1.480, Test loss: 1.587, Test accuracy: 87.66
Round  86, Train loss: 1.479, Test loss: 1.587, Test accuracy: 87.64
Round  87, Train loss: 1.476, Test loss: 1.587, Test accuracy: 87.62
Round  88, Train loss: 1.478, Test loss: 1.587, Test accuracy: 87.65
Round  89, Train loss: 1.475, Test loss: 1.587, Test accuracy: 87.66
Round  90, Train loss: 1.476, Test loss: 1.587, Test accuracy: 87.71
Round  91, Train loss: 1.481, Test loss: 1.587, Test accuracy: 87.70
Round  92, Train loss: 1.478, Test loss: 1.587, Test accuracy: 87.69
Round  93, Train loss: 1.480, Test loss: 1.587, Test accuracy: 87.68
Round  94, Train loss: 1.477, Test loss: 1.587, Test accuracy: 87.68
Round  95, Train loss: 1.476, Test loss: 1.587, Test accuracy: 87.67
Round  96, Train loss: 1.475, Test loss: 1.587, Test accuracy: 87.69
Round  97, Train loss: 1.481, Test loss: 1.587, Test accuracy: 87.67
Round  98, Train loss: 1.478, Test loss: 1.587, Test accuracy: 87.64
Round  99, Train loss: 1.480, Test loss: 1.587, Test accuracy: 87.66/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.477, Test loss: 1.587, Test accuracy: 87.61
Average accuracy final 10 rounds: 87.681 

2611.118615627289
[1.9984333515167236, 3.9968667030334473, 5.974138498306274, 7.951410293579102, 9.832750082015991, 11.71408987045288, 13.79181694984436, 15.86954402923584, 17.656033515930176, 19.44252300262451, 21.57180404663086, 23.701085090637207, 25.776509284973145, 27.851933479309082, 29.71370553970337, 31.575477600097656, 33.5859956741333, 35.596513748168945, 37.52574801445007, 39.4549822807312, 41.62630581855774, 43.79762935638428, 45.63045835494995, 47.463287353515625, 49.46447825431824, 51.46566915512085, 53.46528768539429, 55.464906215667725, 57.41461539268494, 59.36432456970215, 61.38548231124878, 63.40664005279541, 65.387868642807, 67.3690972328186, 69.48589873313904, 71.60270023345947, 73.43522143363953, 75.26774263381958, 77.35991764068604, 79.45209264755249, 81.39641046524048, 83.34072828292847, 85.25048351287842, 87.16023874282837, 89.19893169403076, 91.23762464523315, 93.10378980636597, 94.96995496749878, 96.89927220344543, 98.82858943939209, 100.7094476222992, 102.5903058052063, 104.76661086082458, 106.94291591644287, 109.01552629470825, 111.08813667297363, 112.97379302978516, 114.85944938659668, 116.98955607414246, 119.11966276168823, 121.07576274871826, 123.03186273574829, 124.9365463256836, 126.8412299156189, 128.612731218338, 130.38423252105713, 132.30468678474426, 134.2251410484314, 136.2800829410553, 138.3350248336792, 140.13770294189453, 141.94038105010986, 144.05731773376465, 146.17425441741943, 148.15377688407898, 150.13329935073853, 152.12618851661682, 154.11907768249512, 155.96138453483582, 157.8036913871765, 159.75856614112854, 161.71344089508057, 163.66820335388184, 165.6229658126831, 167.4192817211151, 169.21559762954712, 171.29170298576355, 173.36780834197998, 175.29215717315674, 177.2165060043335, 179.23687887191772, 181.25725173950195, 183.24556016921997, 185.233868598938, 187.15033841133118, 189.06680822372437, 191.03075504302979, 192.9947018623352, 194.81166195869446, 196.6286220550537, 198.6279399394989, 200.6272578239441, 202.51613807678223, 204.40501832962036, 206.34740781784058, 208.2897973060608, 210.36178374290466, 212.43377017974854, 214.34026074409485, 216.24675130844116, 218.17668986320496, 220.10662841796875, 221.89788341522217, 223.6891384124756, 225.71627926826477, 227.74342012405396, 229.5667760372162, 231.39013195037842, 233.1070990562439, 234.82406616210938, 236.85804104804993, 238.89201593399048, 240.79729318618774, 242.702570438385, 244.64938044548035, 246.59619045257568, 248.4450879096985, 250.2939853668213, 252.31214666366577, 254.33030796051025, 256.3569803237915, 258.38365268707275, 260.23844051361084, 262.0932283401489, 264.16588258743286, 266.2385368347168, 268.294588804245, 270.3506407737732, 272.43174386024475, 274.5128469467163, 276.436222076416, 278.3595972061157, 280.5707702636719, 282.781943321228, 284.751140832901, 286.720338344574, 288.6178228855133, 290.51530742645264, 292.66145753860474, 294.80760765075684, 296.77162528038025, 298.73564291000366, 300.64503383636475, 302.55442476272583, 304.43752884864807, 306.3206329345703, 308.44900488853455, 310.5773768424988, 312.5602970123291, 314.5432171821594, 316.44188165664673, 318.34054613113403, 320.5991072654724, 322.8576683998108, 324.8819532394409, 326.90623807907104, 328.96725583076477, 331.0282735824585, 332.9446151256561, 334.86095666885376, 336.93341541290283, 339.0058741569519, 341.03592467308044, 343.065975189209, 345.0649516582489, 347.0639281272888, 349.287451505661, 351.5109748840332, 353.45433616638184, 355.39769744873047, 357.47631764411926, 359.55493783950806, 361.5496492385864, 363.5443606376648, 365.63452434539795, 367.7246880531311, 369.6897451877594, 371.6548023223877, 373.64775013923645, 375.6406979560852, 377.777090549469, 379.9134831428528, 381.72263741493225, 383.5317916870117, 385.4903836250305, 387.4489755630493, 389.39791440963745, 391.3468532562256, 393.34090089797974, 395.3349485397339, 397.43581533432007, 399.53668212890625]
[23.34, 23.34, 37.905, 37.905, 66.24, 66.24, 69.29, 69.29, 73.24, 73.24, 77.385, 77.385, 77.62, 77.62, 78.75, 78.75, 79.32, 79.32, 79.665, 79.665, 79.66, 79.66, 79.785, 79.785, 80.45, 80.45, 80.49, 80.49, 84.185, 84.185, 85.26, 85.26, 85.875, 85.875, 86.2, 86.2, 86.33, 86.33, 86.735, 86.735, 86.97, 86.97, 87.21, 87.21, 87.165, 87.165, 87.19, 87.19, 87.37, 87.37, 87.42, 87.42, 87.395, 87.395, 87.545, 87.545, 87.585, 87.585, 87.55, 87.55, 87.565, 87.565, 87.59, 87.59, 87.6, 87.6, 87.585, 87.585, 87.595, 87.595, 87.585, 87.585, 87.605, 87.605, 87.61, 87.61, 87.615, 87.615, 87.62, 87.62, 87.63, 87.63, 87.615, 87.615, 87.66, 87.66, 87.665, 87.665, 87.665, 87.665, 87.62, 87.62, 87.65, 87.65, 87.675, 87.675, 87.64, 87.64, 87.675, 87.675, 87.63, 87.63, 87.625, 87.625, 87.675, 87.675, 87.645, 87.645, 87.635, 87.635, 87.61, 87.61, 87.625, 87.625, 87.615, 87.615, 87.6, 87.6, 87.61, 87.61, 87.615, 87.615, 87.59, 87.59, 87.615, 87.615, 87.63, 87.63, 87.635, 87.635, 87.62, 87.62, 87.64, 87.64, 87.605, 87.605, 87.61, 87.61, 87.635, 87.635, 87.63, 87.63, 87.62, 87.62, 87.635, 87.635, 87.68, 87.68, 87.66, 87.66, 87.665, 87.665, 87.68, 87.68, 87.67, 87.67, 87.66, 87.66, 87.65, 87.65, 87.65, 87.65, 87.655, 87.655, 87.655, 87.655, 87.655, 87.655, 87.66, 87.66, 87.655, 87.655, 87.64, 87.64, 87.625, 87.625, 87.65, 87.65, 87.66, 87.66, 87.71, 87.71, 87.705, 87.705, 87.69, 87.69, 87.68, 87.68, 87.68, 87.68, 87.675, 87.675, 87.695, 87.695, 87.675, 87.675, 87.645, 87.645, 87.655, 87.655, 87.61, 87.61]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.651, Test loss: 2.256, Test accuracy: 50.33
Round   1, Train loss: 1.374, Test loss: 1.995, Test accuracy: 63.88
Round   2, Train loss: 1.289, Test loss: 1.897, Test accuracy: 68.47
Round   3, Train loss: 1.280, Test loss: 1.872, Test accuracy: 70.27
Round   4, Train loss: 1.276, Test loss: 1.863, Test accuracy: 70.81
Round   5, Train loss: 1.269, Test loss: 1.857, Test accuracy: 70.81
Round   6, Train loss: 1.273, Test loss: 1.850, Test accuracy: 71.73
Round   7, Train loss: 1.262, Test loss: 1.845, Test accuracy: 71.81
Round   8, Train loss: 1.268, Test loss: 1.844, Test accuracy: 71.74
Round   9, Train loss: 1.262, Test loss: 1.842, Test accuracy: 71.77
Round  10, Train loss: 1.261, Test loss: 1.836, Test accuracy: 72.31
Round  11, Train loss: 1.263, Test loss: 1.836, Test accuracy: 72.15
Round  12, Train loss: 1.263, Test loss: 1.836, Test accuracy: 72.07
Round  13, Train loss: 1.261, Test loss: 1.838, Test accuracy: 71.86
Round  14, Train loss: 1.260, Test loss: 1.837, Test accuracy: 72.00
Round  15, Train loss: 1.261, Test loss: 1.838, Test accuracy: 71.80
Round  16, Train loss: 1.258, Test loss: 1.839, Test accuracy: 71.64
Round  17, Train loss: 1.258, Test loss: 1.840, Test accuracy: 71.47
Round  18, Train loss: 1.257, Test loss: 1.840, Test accuracy: 71.47
Round  19, Train loss: 1.258, Test loss: 1.841, Test accuracy: 71.33
Round  20, Train loss: 1.256, Test loss: 1.841, Test accuracy: 71.17
Round  21, Train loss: 1.256, Test loss: 1.842, Test accuracy: 70.97
Round  22, Train loss: 1.256, Test loss: 1.842, Test accuracy: 70.87
Round  23, Train loss: 1.257, Test loss: 1.843, Test accuracy: 70.69
Round  24, Train loss: 1.255, Test loss: 1.844, Test accuracy: 70.62
Round  25, Train loss: 1.254, Test loss: 1.845, Test accuracy: 70.55
Round  26, Train loss: 1.254, Test loss: 1.846, Test accuracy: 70.45
Round  27, Train loss: 1.253, Test loss: 1.847, Test accuracy: 70.17
Round  28, Train loss: 1.254, Test loss: 1.849, Test accuracy: 70.06
Round  29, Train loss: 1.252, Test loss: 1.850, Test accuracy: 69.98
Round  30, Train loss: 1.254, Test loss: 1.851, Test accuracy: 69.82
Round  31, Train loss: 1.253, Test loss: 1.851, Test accuracy: 69.72
Round  32, Train loss: 1.251, Test loss: 1.852, Test accuracy: 69.69
Round  33, Train loss: 1.256, Test loss: 1.853, Test accuracy: 69.44
Round  34, Train loss: 1.254, Test loss: 1.855, Test accuracy: 69.28
Round  35, Train loss: 1.252, Test loss: 1.856, Test accuracy: 69.19
Round  36, Train loss: 1.252, Test loss: 1.857, Test accuracy: 68.99
Round  37, Train loss: 1.251, Test loss: 1.858, Test accuracy: 68.92
Round  38, Train loss: 1.251, Test loss: 1.859, Test accuracy: 68.70
Round  39, Train loss: 1.253, Test loss: 1.860, Test accuracy: 68.66
Round  40, Train loss: 1.251, Test loss: 1.860, Test accuracy: 68.55
Round  41, Train loss: 1.254, Test loss: 1.861, Test accuracy: 68.48
Round  42, Train loss: 1.252, Test loss: 1.863, Test accuracy: 68.20
Round  43, Train loss: 1.252, Test loss: 1.864, Test accuracy: 68.14
Round  44, Train loss: 1.250, Test loss: 1.865, Test accuracy: 68.11
Round  45, Train loss: 1.254, Test loss: 1.866, Test accuracy: 68.02
Round  46, Train loss: 1.252, Test loss: 1.867, Test accuracy: 67.87
Round  47, Train loss: 1.251, Test loss: 1.868, Test accuracy: 67.78
Round  48, Train loss: 1.250, Test loss: 1.868, Test accuracy: 67.78
Round  49, Train loss: 1.251, Test loss: 1.868, Test accuracy: 67.69
Round  50, Train loss: 1.250, Test loss: 1.868, Test accuracy: 67.73
Round  51, Train loss: 1.252, Test loss: 1.868, Test accuracy: 67.74
Round  52, Train loss: 1.251, Test loss: 1.869, Test accuracy: 67.70
Round  53, Train loss: 1.251, Test loss: 1.869, Test accuracy: 67.62
Round  54, Train loss: 1.251, Test loss: 1.870, Test accuracy: 67.52
Round  55, Train loss: 1.249, Test loss: 1.870, Test accuracy: 67.38
Round  56, Train loss: 1.251, Test loss: 1.872, Test accuracy: 67.25
Round  57, Train loss: 1.253, Test loss: 1.872, Test accuracy: 67.30
Round  58, Train loss: 1.251, Test loss: 1.873, Test accuracy: 67.21
Round  59, Train loss: 1.250, Test loss: 1.874, Test accuracy: 67.06
Round  60, Train loss: 1.250, Test loss: 1.875, Test accuracy: 67.00
Round  61, Train loss: 1.250, Test loss: 1.875, Test accuracy: 66.95
Round  62, Train loss: 1.250, Test loss: 1.875, Test accuracy: 66.91
Round  63, Train loss: 1.248, Test loss: 1.876, Test accuracy: 66.75
Round  64, Train loss: 1.251, Test loss: 1.877, Test accuracy: 66.64
Round  65, Train loss: 1.251, Test loss: 1.877, Test accuracy: 66.69
Round  66, Train loss: 1.250, Test loss: 1.878, Test accuracy: 66.78
Round  67, Train loss: 1.251, Test loss: 1.878, Test accuracy: 66.70
Round  68, Train loss: 1.249, Test loss: 1.879, Test accuracy: 66.64
Round  69, Train loss: 1.249, Test loss: 1.879, Test accuracy: 66.56
Round  70, Train loss: 1.250, Test loss: 1.881, Test accuracy: 66.39
Round  71, Train loss: 1.250, Test loss: 1.881, Test accuracy: 66.41
Round  72, Train loss: 1.249, Test loss: 1.882, Test accuracy: 66.36
Round  73, Train loss: 1.249, Test loss: 1.882, Test accuracy: 66.30
Round  74, Train loss: 1.247, Test loss: 1.885, Test accuracy: 66.67
Round  75, Train loss: 1.206, Test loss: 1.874, Test accuracy: 68.38
Round  76, Train loss: 1.191, Test loss: 1.868, Test accuracy: 69.59
Round  77, Train loss: 1.192, Test loss: 1.865, Test accuracy: 70.08
Round  78, Train loss: 1.198, Test loss: 1.855, Test accuracy: 71.28
Round  79, Train loss: 1.186, Test loss: 1.852, Test accuracy: 71.67
Round  80, Train loss: 1.187, Test loss: 1.848, Test accuracy: 72.33
Round  81, Train loss: 1.180, Test loss: 1.848, Test accuracy: 72.33
Round  82, Train loss: 1.187, Test loss: 1.849, Test accuracy: 72.50
Round  83, Train loss: 1.185, Test loss: 1.849, Test accuracy: 72.72
Round  84, Train loss: 1.180, Test loss: 1.848, Test accuracy: 72.77
Round  85, Train loss: 1.179, Test loss: 1.849, Test accuracy: 72.58
Round  86, Train loss: 1.179, Test loss: 1.848, Test accuracy: 72.78
Round  87, Train loss: 1.179, Test loss: 1.847, Test accuracy: 72.66
Round  88, Train loss: 1.178, Test loss: 1.848, Test accuracy: 72.53
Round  89, Train loss: 1.177, Test loss: 1.849, Test accuracy: 72.25
Round  90, Train loss: 1.179, Test loss: 1.848, Test accuracy: 72.12
Round  91, Train loss: 1.178, Test loss: 1.849, Test accuracy: 72.15
Round  92, Train loss: 1.178, Test loss: 1.849, Test accuracy: 72.09
Round  93, Train loss: 1.179, Test loss: 1.850, Test accuracy: 71.93
Round  94, Train loss: 1.178, Test loss: 1.850, Test accuracy: 71.93
Round  95, Train loss: 1.175, Test loss: 1.850, Test accuracy: 71.78
Round  96, Train loss: 1.176, Test loss: 1.850, Test accuracy: 71.78
Round  97, Train loss: 1.176, Test loss: 1.851, Test accuracy: 71.47
Round  98, Train loss: 1.176, Test loss: 1.851, Test accuracy: 71.36
Round  99, Train loss: 1.177, Test loss: 1.851, Test accuracy: 71.35
Final Round, Train loss: 1.176, Test loss: 1.854, Test accuracy: 70.92
Average accuracy final 10 rounds: 71.79650000000001
2946.8382012844086
[]
[50.325, 63.885, 68.465, 70.27, 70.805, 70.815, 71.73, 71.805, 71.74, 71.765, 72.315, 72.15, 72.07, 71.865, 72.005, 71.8, 71.64, 71.47, 71.47, 71.335, 71.175, 70.97, 70.87, 70.695, 70.62, 70.545, 70.45, 70.17, 70.055, 69.985, 69.82, 69.72, 69.685, 69.445, 69.285, 69.19, 68.99, 68.915, 68.7, 68.66, 68.55, 68.48, 68.2, 68.135, 68.115, 68.015, 67.87, 67.785, 67.775, 67.69, 67.735, 67.74, 67.7, 67.625, 67.515, 67.38, 67.25, 67.295, 67.21, 67.065, 66.995, 66.955, 66.905, 66.745, 66.645, 66.695, 66.785, 66.7, 66.64, 66.56, 66.39, 66.405, 66.365, 66.3, 66.67, 68.375, 69.595, 70.075, 71.275, 71.675, 72.33, 72.325, 72.5, 72.725, 72.765, 72.575, 72.775, 72.655, 72.525, 72.25, 72.12, 72.15, 72.095, 71.93, 71.93, 71.785, 71.775, 71.475, 71.355, 71.35, 70.915]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.298, Test loss: 2.299, Test accuracy: 18.55
Round   1, Train loss: 2.292, Test loss: 2.292, Test accuracy: 26.54
Round   2, Train loss: 2.259, Test loss: 2.268, Test accuracy: 30.74
Round   3, Train loss: 2.180, Test loss: 2.220, Test accuracy: 36.39
Round   4, Train loss: 2.245, Test loss: 2.228, Test accuracy: 34.95
Round   5, Train loss: 2.180, Test loss: 2.193, Test accuracy: 35.22
Round   6, Train loss: 2.072, Test loss: 2.179, Test accuracy: 33.23
Round   7, Train loss: 1.863, Test loss: 2.153, Test accuracy: 35.42
Round   8, Train loss: 1.457, Test loss: 2.023, Test accuracy: 45.62
Round   9, Train loss: 1.778, Test loss: 2.061, Test accuracy: 40.87
Round  10, Train loss: 1.632, Test loss: 2.058, Test accuracy: 40.69
Round  11, Train loss: 1.898, Test loss: 2.096, Test accuracy: 34.98
Round  12, Train loss: 1.395, Test loss: 2.044, Test accuracy: 40.70
Round  13, Train loss: 2.148, Test loss: 2.121, Test accuracy: 32.66
Round  14, Train loss: 1.478, Test loss: 2.082, Test accuracy: 35.94
Round  15, Train loss: 0.765, Test loss: 1.981, Test accuracy: 46.92
Round  16, Train loss: 1.966, Test loss: 2.000, Test accuracy: 45.93
Round  17, Train loss: 1.260, Test loss: 1.976, Test accuracy: 48.66
Round  18, Train loss: 0.936, Test loss: 1.920, Test accuracy: 55.20
Round  19, Train loss: 0.334, Test loss: 1.840, Test accuracy: 63.55
Round  20, Train loss: 1.038, Test loss: 1.818, Test accuracy: 65.34
Round  21, Train loss: 0.786, Test loss: 1.779, Test accuracy: 69.03
Round  22, Train loss: 1.692, Test loss: 1.797, Test accuracy: 67.80
Round  23, Train loss: 1.606, Test loss: 1.803, Test accuracy: 68.16
Round  24, Train loss: 1.169, Test loss: 1.774, Test accuracy: 70.19
Round  25, Train loss: 1.272, Test loss: 1.743, Test accuracy: 73.89
Round  26, Train loss: 1.289, Test loss: 1.728, Test accuracy: 74.80
Round  27, Train loss: 1.007, Test loss: 1.712, Test accuracy: 76.20
Round  28, Train loss: 1.350, Test loss: 1.717, Test accuracy: 76.00
Round  29, Train loss: 1.014, Test loss: 1.719, Test accuracy: 75.86
Round  30, Train loss: 1.363, Test loss: 1.715, Test accuracy: 76.61
Round  31, Train loss: 1.151, Test loss: 1.704, Test accuracy: 77.06
Round  32, Train loss: 1.006, Test loss: 1.683, Test accuracy: 78.73
Round  33, Train loss: 1.264, Test loss: 1.683, Test accuracy: 78.75
Round  34, Train loss: 1.250, Test loss: 1.680, Test accuracy: 79.16
Round  35, Train loss: 1.154, Test loss: 1.676, Test accuracy: 79.28
Round  36, Train loss: 0.932, Test loss: 1.672, Test accuracy: 79.50
Round  37, Train loss: 0.927, Test loss: 1.672, Test accuracy: 79.41
Round  38, Train loss: 1.097, Test loss: 1.670, Test accuracy: 79.68
Round  39, Train loss: 1.153, Test loss: 1.667, Test accuracy: 80.13
Round  40, Train loss: 0.837, Test loss: 1.661, Test accuracy: 80.70
Round  41, Train loss: 1.019, Test loss: 1.659, Test accuracy: 80.94
Round  42, Train loss: 0.879, Test loss: 1.661, Test accuracy: 80.67
Round  43, Train loss: 0.954, Test loss: 1.662, Test accuracy: 80.60
Round  44, Train loss: 0.982, Test loss: 1.662, Test accuracy: 80.58
Round  45, Train loss: 0.758, Test loss: 1.661, Test accuracy: 80.48
Round  46, Train loss: 0.835, Test loss: 1.664, Test accuracy: 80.03
Round  47, Train loss: 0.742, Test loss: 1.659, Test accuracy: 80.57
Round  48, Train loss: 0.704, Test loss: 1.659, Test accuracy: 80.45
Round  49, Train loss: 0.745, Test loss: 1.656, Test accuracy: 80.94
Round  50, Train loss: 0.678, Test loss: 1.653, Test accuracy: 81.19
Round  51, Train loss: 0.789, Test loss: 1.655, Test accuracy: 81.03
Round  52, Train loss: 0.582, Test loss: 1.650, Test accuracy: 81.43
Round  53, Train loss: 0.784, Test loss: 1.643, Test accuracy: 82.16
Round  54, Train loss: 0.774, Test loss: 1.643, Test accuracy: 82.09
Round  55, Train loss: 0.670, Test loss: 1.643, Test accuracy: 82.19
Round  56, Train loss: 0.761, Test loss: 1.638, Test accuracy: 82.69
Round  57, Train loss: 0.633, Test loss: 1.637, Test accuracy: 82.79
Round  58, Train loss: 0.543, Test loss: 1.631, Test accuracy: 83.44
Round  59, Train loss: 0.665, Test loss: 1.642, Test accuracy: 82.40
Round  60, Train loss: 0.817, Test loss: 1.644, Test accuracy: 82.19
Round  61, Train loss: 0.641, Test loss: 1.644, Test accuracy: 82.09
Round  62, Train loss: 0.580, Test loss: 1.639, Test accuracy: 82.64
Round  63, Train loss: 0.689, Test loss: 1.645, Test accuracy: 82.10
Round  64, Train loss: 0.722, Test loss: 1.654, Test accuracy: 81.13
Round  65, Train loss: 0.586, Test loss: 1.649, Test accuracy: 81.61
Round  66, Train loss: 0.557, Test loss: 1.643, Test accuracy: 82.19
Round  67, Train loss: 0.664, Test loss: 1.633, Test accuracy: 83.31
Round  68, Train loss: 0.614, Test loss: 1.646, Test accuracy: 81.97
Round  69, Train loss: 0.623, Test loss: 1.642, Test accuracy: 82.27
Round  70, Train loss: 0.242, Test loss: 1.625, Test accuracy: 84.09
Round  71, Train loss: 0.510, Test loss: 1.626, Test accuracy: 84.02
Round  72, Train loss: 0.227, Test loss: 1.622, Test accuracy: 84.34
Round  73, Train loss: 0.575, Test loss: 1.623, Test accuracy: 84.18
Round  74, Train loss: 0.703, Test loss: 1.627, Test accuracy: 83.77
Round  75, Train loss: 0.509, Test loss: 1.612, Test accuracy: 85.36
Round  76, Train loss: 0.473, Test loss: 1.617, Test accuracy: 84.88
Round  77, Train loss: 0.466, Test loss: 1.617, Test accuracy: 84.81
Round  78, Train loss: 0.630, Test loss: 1.619, Test accuracy: 84.78
Round  79, Train loss: 0.478, Test loss: 1.615, Test accuracy: 85.17
Round  80, Train loss: 0.413, Test loss: 1.612, Test accuracy: 85.32
Round  81, Train loss: 0.501, Test loss: 1.612, Test accuracy: 85.33
Round  82, Train loss: 0.133, Test loss: 1.608, Test accuracy: 85.64
Round  83, Train loss: 0.373, Test loss: 1.608, Test accuracy: 85.78
Round  84, Train loss: 0.484, Test loss: 1.604, Test accuracy: 86.19
Round  85, Train loss: 0.465, Test loss: 1.604, Test accuracy: 86.08
Round  86, Train loss: 0.455, Test loss: 1.598, Test accuracy: 86.70
Round  87, Train loss: 0.362, Test loss: 1.602, Test accuracy: 86.37
Round  88, Train loss: 0.448, Test loss: 1.602, Test accuracy: 86.34
Round  89, Train loss: 0.353, Test loss: 1.598, Test accuracy: 86.69
Round  90, Train loss: 0.439, Test loss: 1.597, Test accuracy: 86.80
Round  91, Train loss: 0.310, Test loss: 1.597, Test accuracy: 86.74
Round  92, Train loss: 0.420, Test loss: 1.598, Test accuracy: 86.69
Round  93, Train loss: 0.229, Test loss: 1.590, Test accuracy: 87.48
Round  94, Train loss: 0.286, Test loss: 1.593, Test accuracy: 87.23
Round  95, Train loss: 0.576, Test loss: 1.593, Test accuracy: 87.22
Round  96, Train loss: 0.567, Test loss: 1.590, Test accuracy: 87.70
Round  97, Train loss: 0.457, Test loss: 1.593, Test accuracy: 87.35
Round  98, Train loss: 0.485, Test loss: 1.592, Test accuracy: 87.39
Round  99, Train loss: 0.398, Test loss: 1.594, Test accuracy: 87.20
Final Round, Train loss: 1.525, Test loss: 1.570, Test accuracy: 89.97
Average accuracy final 10 rounds: 87.182
Average global accuracy final 10 rounds: 87.182
2367.1864700317383
[]
[18.555, 26.535, 30.74, 36.39, 34.955, 35.215, 33.235, 35.425, 45.615, 40.865, 40.69, 34.98, 40.7, 32.655, 35.935, 46.925, 45.93, 48.665, 55.195, 63.545, 65.345, 69.025, 67.8, 68.155, 70.185, 73.89, 74.795, 76.205, 75.995, 75.865, 76.61, 77.065, 78.73, 78.745, 79.16, 79.28, 79.495, 79.41, 79.68, 80.13, 80.7, 80.945, 80.675, 80.6, 80.585, 80.485, 80.025, 80.57, 80.45, 80.935, 81.195, 81.025, 81.43, 82.155, 82.095, 82.19, 82.695, 82.79, 83.435, 82.4, 82.195, 82.09, 82.64, 82.1, 81.13, 81.615, 82.185, 83.305, 81.97, 82.265, 84.09, 84.015, 84.345, 84.18, 83.77, 85.36, 84.88, 84.81, 84.785, 85.175, 85.32, 85.335, 85.645, 85.78, 86.185, 86.085, 86.705, 86.37, 86.34, 86.695, 86.8, 86.74, 86.685, 87.48, 87.235, 87.225, 87.705, 87.35, 87.395, 87.205, 89.975]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

prox
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.297, Test accuracy: 20.41
Round   0, Global train loss: 2.300, Global test loss: 2.297, Global test accuracy: 20.36
Round   1, Train loss: 2.291, Test loss: 2.286, Test accuracy: 21.32
Round   1, Global train loss: 2.291, Global test loss: 2.281, Global test accuracy: 22.20
Round   2, Train loss: 2.230, Test loss: 2.211, Test accuracy: 33.12
Round   2, Global train loss: 2.230, Global test loss: 2.158, Global test accuracy: 40.65
Round   3, Train loss: 2.000, Test loss: 2.052, Test accuracy: 48.05
Round   3, Global train loss: 2.000, Global test loss: 1.899, Global test accuracy: 63.03
Round   4, Train loss: 1.832, Test loss: 1.966, Test accuracy: 56.25
Round   4, Global train loss: 1.832, Global test loss: 1.778, Global test accuracy: 75.38
Round   5, Train loss: 1.729, Test loss: 1.878, Test accuracy: 63.80
Round   5, Global train loss: 1.729, Global test loss: 1.705, Global test accuracy: 79.12
Round   6, Train loss: 1.679, Test loss: 1.837, Test accuracy: 67.19
Round   6, Global train loss: 1.679, Global test loss: 1.679, Global test accuracy: 80.27
Round   7, Train loss: 1.658, Test loss: 1.796, Test accuracy: 70.69
Round   7, Global train loss: 1.658, Global test loss: 1.663, Global test accuracy: 81.16
Round   8, Train loss: 1.650, Test loss: 1.756, Test accuracy: 73.91
Round   8, Global train loss: 1.650, Global test loss: 1.655, Global test accuracy: 81.66
Round   9, Train loss: 1.648, Test loss: 1.679, Test accuracy: 79.97
Round   9, Global train loss: 1.648, Global test loss: 1.649, Global test accuracy: 82.20
Round  10, Train loss: 1.634, Test loss: 1.668, Test accuracy: 80.70
Round  10, Global train loss: 1.634, Global test loss: 1.644, Global test accuracy: 82.38
Round  11, Train loss: 1.623, Test loss: 1.666, Test accuracy: 80.81
Round  11, Global train loss: 1.623, Global test loss: 1.641, Global test accuracy: 82.64
Round  12, Train loss: 1.621, Test loss: 1.659, Test accuracy: 81.06
Round  12, Global train loss: 1.621, Global test loss: 1.640, Global test accuracy: 82.62
Round  13, Train loss: 1.628, Test loss: 1.653, Test accuracy: 81.48
Round  13, Global train loss: 1.628, Global test loss: 1.636, Global test accuracy: 82.96
Round  14, Train loss: 1.618, Test loss: 1.648, Test accuracy: 81.83
Round  14, Global train loss: 1.618, Global test loss: 1.635, Global test accuracy: 83.06
Round  15, Train loss: 1.623, Test loss: 1.643, Test accuracy: 82.25
Round  15, Global train loss: 1.623, Global test loss: 1.631, Global test accuracy: 83.44
Round  16, Train loss: 1.615, Test loss: 1.641, Test accuracy: 82.37
Round  16, Global train loss: 1.615, Global test loss: 1.630, Global test accuracy: 83.47
Round  17, Train loss: 1.614, Test loss: 1.640, Test accuracy: 82.44
Round  17, Global train loss: 1.614, Global test loss: 1.629, Global test accuracy: 83.56
Round  18, Train loss: 1.610, Test loss: 1.638, Test accuracy: 82.61
Round  18, Global train loss: 1.610, Global test loss: 1.627, Global test accuracy: 83.75
Round  19, Train loss: 1.607, Test loss: 1.637, Test accuracy: 82.80
Round  19, Global train loss: 1.607, Global test loss: 1.626, Global test accuracy: 83.88
Round  20, Train loss: 1.608, Test loss: 1.636, Test accuracy: 82.88
Round  20, Global train loss: 1.608, Global test loss: 1.624, Global test accuracy: 83.94
Round  21, Train loss: 1.595, Test loss: 1.628, Test accuracy: 83.72
Round  21, Global train loss: 1.595, Global test loss: 1.607, Global test accuracy: 85.67
Round  22, Train loss: 1.548, Test loss: 1.610, Test accuracy: 85.72
Round  22, Global train loss: 1.548, Global test loss: 1.558, Global test accuracy: 91.48
Round  23, Train loss: 1.529, Test loss: 1.595, Test accuracy: 87.36
Round  23, Global train loss: 1.529, Global test loss: 1.551, Global test accuracy: 91.88
Round  24, Train loss: 1.522, Test loss: 1.594, Test accuracy: 87.32
Round  24, Global train loss: 1.522, Global test loss: 1.548, Global test accuracy: 91.88
Round  25, Train loss: 1.522, Test loss: 1.584, Test accuracy: 88.28
Round  25, Global train loss: 1.522, Global test loss: 1.545, Global test accuracy: 92.28
Round  26, Train loss: 1.518, Test loss: 1.571, Test accuracy: 89.73
Round  26, Global train loss: 1.518, Global test loss: 1.541, Global test accuracy: 92.62
Round  27, Train loss: 1.512, Test loss: 1.564, Test accuracy: 90.25
Round  27, Global train loss: 1.512, Global test loss: 1.539, Global test accuracy: 92.77
Round  28, Train loss: 1.517, Test loss: 1.558, Test accuracy: 90.91
Round  28, Global train loss: 1.517, Global test loss: 1.537, Global test accuracy: 92.98
Round  29, Train loss: 1.508, Test loss: 1.557, Test accuracy: 91.00
Round  29, Global train loss: 1.508, Global test loss: 1.535, Global test accuracy: 93.12
Round  30, Train loss: 1.503, Test loss: 1.547, Test accuracy: 91.94
Round  30, Global train loss: 1.503, Global test loss: 1.532, Global test accuracy: 93.42
Round  31, Train loss: 1.501, Test loss: 1.541, Test accuracy: 92.52
Round  31, Global train loss: 1.501, Global test loss: 1.532, Global test accuracy: 93.31
Round  32, Train loss: 1.499, Test loss: 1.541, Test accuracy: 92.52
Round  32, Global train loss: 1.499, Global test loss: 1.532, Global test accuracy: 93.28
Round  33, Train loss: 1.510, Test loss: 1.538, Test accuracy: 92.74
Round  33, Global train loss: 1.510, Global test loss: 1.532, Global test accuracy: 93.25
Round  34, Train loss: 1.501, Test loss: 1.538, Test accuracy: 92.83
Round  34, Global train loss: 1.501, Global test loss: 1.530, Global test accuracy: 93.44
Round  35, Train loss: 1.494, Test loss: 1.537, Test accuracy: 92.81
Round  35, Global train loss: 1.494, Global test loss: 1.529, Global test accuracy: 93.58
Round  36, Train loss: 1.494, Test loss: 1.537, Test accuracy: 92.85
Round  36, Global train loss: 1.494, Global test loss: 1.528, Global test accuracy: 93.61
Round  37, Train loss: 1.498, Test loss: 1.536, Test accuracy: 92.98
Round  37, Global train loss: 1.498, Global test loss: 1.527, Global test accuracy: 93.66
Round  38, Train loss: 1.499, Test loss: 1.535, Test accuracy: 93.05
Round  38, Global train loss: 1.499, Global test loss: 1.527, Global test accuracy: 93.71
Round  39, Train loss: 1.500, Test loss: 1.534, Test accuracy: 93.19
Round  39, Global train loss: 1.500, Global test loss: 1.526, Global test accuracy: 93.71
Round  40, Train loss: 1.500, Test loss: 1.533, Test accuracy: 93.30
Round  40, Global train loss: 1.500, Global test loss: 1.526, Global test accuracy: 93.80
Round  41, Train loss: 1.495, Test loss: 1.532, Test accuracy: 93.33
Round  41, Global train loss: 1.495, Global test loss: 1.525, Global test accuracy: 94.04
Round  42, Train loss: 1.493, Test loss: 1.531, Test accuracy: 93.34
Round  42, Global train loss: 1.493, Global test loss: 1.525, Global test accuracy: 93.95
Round  43, Train loss: 1.491, Test loss: 1.531, Test accuracy: 93.33
Round  43, Global train loss: 1.491, Global test loss: 1.524, Global test accuracy: 94.11
Round  44, Train loss: 1.493, Test loss: 1.529, Test accuracy: 93.53
Round  44, Global train loss: 1.493, Global test loss: 1.523, Global test accuracy: 94.12
Round  45, Train loss: 1.493, Test loss: 1.529, Test accuracy: 93.55
Round  45, Global train loss: 1.493, Global test loss: 1.523, Global test accuracy: 94.11
Round  46, Train loss: 1.495, Test loss: 1.529, Test accuracy: 93.62
Round  46, Global train loss: 1.495, Global test loss: 1.523, Global test accuracy: 94.00
Round  47, Train loss: 1.490, Test loss: 1.528, Test accuracy: 93.69
Round  47, Global train loss: 1.490, Global test loss: 1.523, Global test accuracy: 94.03
Round  48, Train loss: 1.489, Test loss: 1.527, Test accuracy: 93.75
Round  48, Global train loss: 1.489, Global test loss: 1.522, Global test accuracy: 94.24
Round  49, Train loss: 1.489, Test loss: 1.527, Test accuracy: 93.75
Round  49, Global train loss: 1.489, Global test loss: 1.522, Global test accuracy: 94.25
Round  50, Train loss: 1.486, Test loss: 1.527, Test accuracy: 93.73
Round  50, Global train loss: 1.486, Global test loss: 1.522, Global test accuracy: 94.08
Round  51, Train loss: 1.491, Test loss: 1.527, Test accuracy: 93.74
Round  51, Global train loss: 1.491, Global test loss: 1.521, Global test accuracy: 94.25
Round  52, Train loss: 1.488, Test loss: 1.525, Test accuracy: 93.89
Round  52, Global train loss: 1.488, Global test loss: 1.520, Global test accuracy: 94.34
Round  53, Train loss: 1.490, Test loss: 1.525, Test accuracy: 93.95
Round  53, Global train loss: 1.490, Global test loss: 1.520, Global test accuracy: 94.41
Round  54, Train loss: 1.485, Test loss: 1.524, Test accuracy: 94.04
Round  54, Global train loss: 1.485, Global test loss: 1.520, Global test accuracy: 94.30
Round  55, Train loss: 1.488, Test loss: 1.523, Test accuracy: 94.11
Round  55, Global train loss: 1.488, Global test loss: 1.519, Global test accuracy: 94.47
Round  56, Train loss: 1.488, Test loss: 1.524, Test accuracy: 94.11
Round  56, Global train loss: 1.488, Global test loss: 1.520, Global test accuracy: 94.38
Round  57, Train loss: 1.485, Test loss: 1.523, Test accuracy: 94.15
Round  57, Global train loss: 1.485, Global test loss: 1.519, Global test accuracy: 94.44
Round  58, Train loss: 1.488, Test loss: 1.523, Test accuracy: 94.12
Round  58, Global train loss: 1.488, Global test loss: 1.519, Global test accuracy: 94.30
Round  59, Train loss: 1.484, Test loss: 1.522, Test accuracy: 94.16
Round  59, Global train loss: 1.484, Global test loss: 1.519, Global test accuracy: 94.45
Round  60, Train loss: 1.483, Test loss: 1.522, Test accuracy: 94.21
Round  60, Global train loss: 1.483, Global test loss: 1.518, Global test accuracy: 94.47
Round  61, Train loss: 1.482, Test loss: 1.522, Test accuracy: 94.19
Round  61, Global train loss: 1.482, Global test loss: 1.517, Global test accuracy: 94.56
Round  62, Train loss: 1.484, Test loss: 1.522, Test accuracy: 94.16
Round  62, Global train loss: 1.484, Global test loss: 1.517, Global test accuracy: 94.55
Round  63, Train loss: 1.488, Test loss: 1.521, Test accuracy: 94.19
Round  63, Global train loss: 1.488, Global test loss: 1.517, Global test accuracy: 94.59
Round  64, Train loss: 1.483, Test loss: 1.521, Test accuracy: 94.16
Round  64, Global train loss: 1.483, Global test loss: 1.516, Global test accuracy: 94.56
Round  65, Train loss: 1.482, Test loss: 1.520, Test accuracy: 94.23
Round  65, Global train loss: 1.482, Global test loss: 1.516, Global test accuracy: 94.70
Round  66, Train loss: 1.485, Test loss: 1.520, Test accuracy: 94.28
Round  66, Global train loss: 1.485, Global test loss: 1.516, Global test accuracy: 94.59
Round  67, Train loss: 1.483, Test loss: 1.519, Test accuracy: 94.33
Round  67, Global train loss: 1.483, Global test loss: 1.516, Global test accuracy: 94.66
Round  68, Train loss: 1.485, Test loss: 1.519, Test accuracy: 94.39
Round  68, Global train loss: 1.485, Global test loss: 1.516, Global test accuracy: 94.66
Round  69, Train loss: 1.483, Test loss: 1.519, Test accuracy: 94.39
Round  69, Global train loss: 1.483, Global test loss: 1.516, Global test accuracy: 94.72
Round  70, Train loss: 1.482, Test loss: 1.519, Test accuracy: 94.41
Round  70, Global train loss: 1.482, Global test loss: 1.515, Global test accuracy: 94.78
Round  71, Train loss: 1.483, Test loss: 1.519, Test accuracy: 94.41
Round  71, Global train loss: 1.483, Global test loss: 1.515, Global test accuracy: 94.78
Round  72, Train loss: 1.481, Test loss: 1.518, Test accuracy: 94.42
Round  72, Global train loss: 1.481, Global test loss: 1.515, Global test accuracy: 94.80
Round  73, Train loss: 1.482, Test loss: 1.518, Test accuracy: 94.44
Round  73, Global train loss: 1.482, Global test loss: 1.515, Global test accuracy: 94.78
Round  74, Train loss: 1.481, Test loss: 1.518, Test accuracy: 94.50
Round  74, Global train loss: 1.481, Global test loss: 1.515, Global test accuracy: 94.78
Round  75, Train loss: 1.481, Test loss: 1.518, Test accuracy: 94.47
Round  75, Global train loss: 1.481, Global test loss: 1.515, Global test accuracy: 94.85
Round  76, Train loss: 1.483, Test loss: 1.518, Test accuracy: 94.49
Round  76, Global train loss: 1.483, Global test loss: 1.515, Global test accuracy: 94.83
Round  77, Train loss: 1.480, Test loss: 1.518, Test accuracy: 94.57
Round  77, Global train loss: 1.480, Global test loss: 1.515, Global test accuracy: 94.90
Round  78, Train loss: 1.479, Test loss: 1.517, Test accuracy: 94.59
Round  78, Global train loss: 1.479, Global test loss: 1.514, Global test accuracy: 94.88
Round  79, Train loss: 1.481, Test loss: 1.517, Test accuracy: 94.61
Round  79, Global train loss: 1.481, Global test loss: 1.514, Global test accuracy: 94.88
Round  80, Train loss: 1.483, Test loss: 1.517, Test accuracy: 94.64
Round  80, Global train loss: 1.483, Global test loss: 1.514, Global test accuracy: 94.89
Round  81, Train loss: 1.480, Test loss: 1.517, Test accuracy: 94.61
Round  81, Global train loss: 1.480, Global test loss: 1.514, Global test accuracy: 94.86
Round  82, Train loss: 1.483, Test loss: 1.517, Test accuracy: 94.59
Round  82, Global train loss: 1.483, Global test loss: 1.514, Global test accuracy: 94.94
Round  83, Train loss: 1.479, Test loss: 1.517, Test accuracy: 94.59
Round  83, Global train loss: 1.479, Global test loss: 1.514, Global test accuracy: 94.89
Round  84, Train loss: 1.481, Test loss: 1.516, Test accuracy: 94.61
Round  84, Global train loss: 1.481, Global test loss: 1.514, Global test accuracy: 95.00
Round  85, Train loss: 1.481, Test loss: 1.517, Test accuracy: 94.64
Round  85, Global train loss: 1.481, Global test loss: 1.514, Global test accuracy: 94.88
Round  86, Train loss: 1.478, Test loss: 1.516, Test accuracy: 94.68
Round  86, Global train loss: 1.478, Global test loss: 1.514, Global test accuracy: 95.01
Round  87, Train loss: 1.480, Test loss: 1.516, Test accuracy: 94.69
Round  87, Global train loss: 1.480, Global test loss: 1.513, Global test accuracy: 94.92
Round  88, Train loss: 1.482, Test loss: 1.516, Test accuracy: 94.72
Round  88, Global train loss: 1.482, Global test loss: 1.513, Global test accuracy: 95.03
Round  89, Train loss: 1.479, Test loss: 1.516, Test accuracy: 94.69
Round  89, Global train loss: 1.479, Global test loss: 1.513, Global test accuracy: 95.05
Round  90, Train loss: 1.479, Test loss: 1.516, Test accuracy: 94.75
Round  90, Global train loss: 1.479, Global test loss: 1.513, Global test accuracy: 95.07
Round  91, Train loss: 1.480, Test loss: 1.515, Test accuracy: 94.78
Round  91, Global train loss: 1.480, Global test loss: 1.513, Global test accuracy: 95.00
Round  92, Train loss: 1.479, Test loss: 1.515, Test accuracy: 94.80
Round  92, Global train loss: 1.479, Global test loss: 1.512, Global test accuracy: 95.03
Round  93, Train loss: 1.478, Test loss: 1.515, Test accuracy: 94.78
Round  93, Global train loss: 1.478, Global test loss: 1.512, Global test accuracy: 95.07
Round  94, Train loss: 1.478, Test loss: 1.515, Test accuracy: 94.81
Round  94, Global train loss: 1.478, Global test loss: 1.512, Global test accuracy: 95.14
Round  95, Train loss: 1.479, Test loss: 1.515, Test accuracy: 94.81
Round  95, Global train loss: 1.479, Global test loss: 1.512, Global test accuracy: 95.07/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  96, Train loss: 1.475, Test loss: 1.514, Test accuracy: 94.86
Round  96, Global train loss: 1.475, Global test loss: 1.512, Global test accuracy: 95.12
Round  97, Train loss: 1.479, Test loss: 1.514, Test accuracy: 94.85
Round  97, Global train loss: 1.479, Global test loss: 1.511, Global test accuracy: 95.19
Round  98, Train loss: 1.479, Test loss: 1.514, Test accuracy: 94.92
Round  98, Global train loss: 1.479, Global test loss: 1.511, Global test accuracy: 95.11
Round  99, Train loss: 1.478, Test loss: 1.514, Test accuracy: 94.97
Round  99, Global train loss: 1.478, Global test loss: 1.511, Global test accuracy: 95.20
Final Round, Train loss: 1.477, Test loss: 1.514, Test accuracy: 94.86
Final Round, Global train loss: 1.477, Global test loss: 1.511, Global test accuracy: 95.20
Average accuracy final 10 rounds: 94.83200000000001 

Average global accuracy final 10 rounds: 95.09999999999998 

3475.8492131233215
[2.2104055881500244, 4.420811176300049, 6.592313051223755, 8.763814926147461, 10.902616024017334, 13.041417121887207, 15.14822769165039, 17.255038261413574, 19.64202904701233, 22.029019832611084, 24.107171058654785, 26.185322284698486, 28.517505168914795, 30.849688053131104, 33.02558255195618, 35.20147705078125, 37.3051962852478, 39.408915519714355, 41.68737435340881, 43.96583318710327, 45.999953746795654, 48.03407430648804, 50.293816566467285, 52.55355882644653, 54.70909571647644, 56.86463260650635, 58.97203803062439, 61.07944345474243, 63.16563129425049, 65.25181913375854, 67.35076451301575, 69.44970989227295, 71.62749123573303, 73.80527257919312, 75.95360589027405, 78.10193920135498, 80.19347643852234, 82.2850136756897, 84.29033470153809, 86.29565572738647, 88.4385597705841, 90.58146381378174, 92.64219284057617, 94.7029218673706, 97.05459403991699, 99.40626621246338, 101.51501703262329, 103.6237678527832, 105.87397289276123, 108.12417793273926, 110.24467444419861, 112.36517095565796, 114.43639659881592, 116.50762224197388, 118.70462608337402, 120.90162992477417, 122.82486534118652, 124.74810075759888, 127.0842056274414, 129.42031049728394, 131.5132131576538, 133.60611581802368, 135.62597608566284, 137.645836353302, 139.75117421150208, 141.85651206970215, 143.99641633033752, 146.1363205909729, 148.35916709899902, 150.58201360702515, 152.79022693634033, 154.99844026565552, 157.28373408317566, 159.5690279006958, 161.79883933067322, 164.02865076065063, 166.35433387756348, 168.68001699447632, 170.75111484527588, 172.82221269607544, 175.14058208465576, 177.45895147323608, 179.60719656944275, 181.7554416656494, 183.89505124092102, 186.03466081619263, 188.28324270248413, 190.53182458877563, 192.5352599620819, 194.53869533538818, 196.73918414115906, 198.93967294692993, 200.90912914276123, 202.87858533859253, 205.0232436656952, 207.16790199279785, 209.26047801971436, 211.35305404663086, 213.5620732307434, 215.77109241485596, 217.9013261795044, 220.03155994415283, 222.31474661827087, 224.59793329238892, 226.73993468284607, 228.88193607330322, 231.0785574913025, 233.27517890930176, 235.41106033325195, 237.54694175720215, 239.6982705593109, 241.84959936141968, 244.16801357269287, 246.48642778396606, 248.50251579284668, 250.5186038017273, 252.89858436584473, 255.27856492996216, 257.3416681289673, 259.4047713279724, 261.45463967323303, 263.50450801849365, 265.83947706222534, 268.17444610595703, 270.1165380477905, 272.058629989624, 274.3613831996918, 276.6641364097595, 278.7960453033447, 280.92795419692993, 283.0901560783386, 285.2523579597473, 287.4270875453949, 289.6018171310425, 291.75562620162964, 293.9094352722168, 296.1826527118683, 298.4558701515198, 300.7400960922241, 303.02432203292847, 305.2091212272644, 307.39392042160034, 309.5467338562012, 311.699547290802, 313.92401146888733, 316.14847564697266, 318.24424290657043, 320.3400101661682, 322.63513565063477, 324.9302611351013, 327.05520725250244, 329.18015336990356, 331.27256441116333, 333.3649754524231, 335.51225686073303, 337.65953826904297, 339.5829527378082, 341.5063672065735, 343.78517985343933, 346.0639925003052, 348.13066363334656, 350.19733476638794, 352.47007393836975, 354.74281311035156, 356.85119366645813, 358.9595742225647, 361.0580780506134, 363.1565818786621, 365.46504831314087, 367.77351474761963, 369.75170850753784, 371.72990226745605, 373.81352138519287, 375.8971405029297, 377.97420048713684, 380.051260471344, 382.1792457103729, 384.30723094940186, 386.3546578884125, 388.4020848274231, 390.74183082580566, 393.08157682418823, 395.1185703277588, 397.15556383132935, 399.3269248008728, 401.49828577041626, 403.6752276420593, 405.8521695137024, 407.96842885017395, 410.0846881866455, 412.35387778282166, 414.6230673789978, 416.54312229156494, 418.4631772041321, 420.6917977333069, 422.9204182624817, 424.8821897506714, 426.8439612388611, 429.0222599506378, 431.20055866241455, 433.42547273635864, 435.65038681030273]
[20.415, 20.415, 21.32, 21.32, 33.125, 33.125, 48.045, 48.045, 56.25, 56.25, 63.805, 63.805, 67.185, 67.185, 70.69, 70.69, 73.905, 73.905, 79.97, 79.97, 80.705, 80.705, 80.815, 80.815, 81.06, 81.06, 81.48, 81.48, 81.825, 81.825, 82.25, 82.25, 82.37, 82.37, 82.435, 82.435, 82.615, 82.615, 82.8, 82.8, 82.88, 82.88, 83.715, 83.715, 85.725, 85.725, 87.355, 87.355, 87.32, 87.32, 88.285, 88.285, 89.735, 89.735, 90.245, 90.245, 90.905, 90.905, 91.0, 91.0, 91.935, 91.935, 92.515, 92.515, 92.515, 92.515, 92.74, 92.74, 92.825, 92.825, 92.805, 92.805, 92.85, 92.85, 92.98, 92.98, 93.045, 93.045, 93.19, 93.19, 93.295, 93.295, 93.325, 93.325, 93.34, 93.34, 93.335, 93.335, 93.525, 93.525, 93.545, 93.545, 93.62, 93.62, 93.685, 93.685, 93.745, 93.745, 93.755, 93.755, 93.735, 93.735, 93.74, 93.74, 93.895, 93.895, 93.95, 93.95, 94.04, 94.04, 94.115, 94.115, 94.105, 94.105, 94.15, 94.15, 94.125, 94.125, 94.16, 94.16, 94.21, 94.21, 94.185, 94.185, 94.155, 94.155, 94.195, 94.195, 94.155, 94.155, 94.235, 94.235, 94.285, 94.285, 94.325, 94.325, 94.395, 94.395, 94.385, 94.385, 94.41, 94.41, 94.405, 94.405, 94.42, 94.42, 94.44, 94.44, 94.495, 94.495, 94.47, 94.47, 94.49, 94.49, 94.57, 94.57, 94.595, 94.595, 94.605, 94.605, 94.64, 94.64, 94.605, 94.605, 94.59, 94.59, 94.595, 94.595, 94.61, 94.61, 94.635, 94.635, 94.68, 94.68, 94.685, 94.685, 94.72, 94.72, 94.695, 94.695, 94.75, 94.75, 94.775, 94.775, 94.8, 94.8, 94.78, 94.78, 94.805, 94.805, 94.815, 94.815, 94.855, 94.855, 94.85, 94.85, 94.915, 94.915, 94.975, 94.975, 94.855, 94.855]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.301, Test loss: 2.296, Test accuracy: 30.59
Round   1, Train loss: 2.294, Test loss: 2.252, Test accuracy: 32.10
Round   2, Train loss: 2.221, Test loss: 1.953, Test accuracy: 59.41
Round   3, Train loss: 1.953, Test loss: 1.753, Test accuracy: 76.28
Round   4, Train loss: 1.734, Test loss: 1.614, Test accuracy: 87.65
Round   5, Train loss: 1.661, Test loss: 1.586, Test accuracy: 89.05
Round   6, Train loss: 1.596, Test loss: 1.573, Test accuracy: 90.00
Round   7, Train loss: 1.595, Test loss: 1.565, Test accuracy: 90.41
Round   8, Train loss: 1.576, Test loss: 1.559, Test accuracy: 90.89
Round   9, Train loss: 1.555, Test loss: 1.556, Test accuracy: 91.08
Round  10, Train loss: 1.534, Test loss: 1.554, Test accuracy: 91.30
Round  11, Train loss: 1.550, Test loss: 1.550, Test accuracy: 91.58
Round  12, Train loss: 1.538, Test loss: 1.548, Test accuracy: 91.75
Round  13, Train loss: 1.551, Test loss: 1.545, Test accuracy: 91.93
Round  14, Train loss: 1.530, Test loss: 1.543, Test accuracy: 92.17
Round  15, Train loss: 1.524, Test loss: 1.540, Test accuracy: 92.44
Round  16, Train loss: 1.519, Test loss: 1.539, Test accuracy: 92.45
Round  17, Train loss: 1.520, Test loss: 1.537, Test accuracy: 92.72
Round  18, Train loss: 1.515, Test loss: 1.534, Test accuracy: 93.09
Round  19, Train loss: 1.516, Test loss: 1.534, Test accuracy: 93.15
Round  20, Train loss: 1.508, Test loss: 1.533, Test accuracy: 93.20
Round  21, Train loss: 1.510, Test loss: 1.531, Test accuracy: 93.16
Round  22, Train loss: 1.508, Test loss: 1.531, Test accuracy: 93.37
Round  23, Train loss: 1.506, Test loss: 1.529, Test accuracy: 93.39
Round  24, Train loss: 1.503, Test loss: 1.528, Test accuracy: 93.42
Round  25, Train loss: 1.502, Test loss: 1.527, Test accuracy: 93.72
Round  26, Train loss: 1.500, Test loss: 1.525, Test accuracy: 93.91
Round  27, Train loss: 1.495, Test loss: 1.526, Test accuracy: 93.91
Round  28, Train loss: 1.497, Test loss: 1.524, Test accuracy: 94.03
Round  29, Train loss: 1.493, Test loss: 1.524, Test accuracy: 94.04
Round  30, Train loss: 1.497, Test loss: 1.523, Test accuracy: 94.02
Round  31, Train loss: 1.493, Test loss: 1.522, Test accuracy: 94.14
Round  32, Train loss: 1.495, Test loss: 1.522, Test accuracy: 94.22
Round  33, Train loss: 1.494, Test loss: 1.521, Test accuracy: 94.29
Round  34, Train loss: 1.490, Test loss: 1.520, Test accuracy: 94.42
Round  35, Train loss: 1.487, Test loss: 1.520, Test accuracy: 94.34
Round  36, Train loss: 1.491, Test loss: 1.519, Test accuracy: 94.54
Round  37, Train loss: 1.487, Test loss: 1.519, Test accuracy: 94.36
Round  38, Train loss: 1.487, Test loss: 1.518, Test accuracy: 94.52
Round  39, Train loss: 1.485, Test loss: 1.517, Test accuracy: 94.69
Round  40, Train loss: 1.486, Test loss: 1.518, Test accuracy: 94.53
Round  41, Train loss: 1.487, Test loss: 1.518, Test accuracy: 94.56
Round  42, Train loss: 1.486, Test loss: 1.518, Test accuracy: 94.62
Round  43, Train loss: 1.484, Test loss: 1.517, Test accuracy: 94.74
Round  44, Train loss: 1.485, Test loss: 1.517, Test accuracy: 94.67
Round  45, Train loss: 1.484, Test loss: 1.517, Test accuracy: 94.67
Round  46, Train loss: 1.484, Test loss: 1.518, Test accuracy: 94.63
Round  47, Train loss: 1.486, Test loss: 1.516, Test accuracy: 94.78
Round  48, Train loss: 1.481, Test loss: 1.515, Test accuracy: 94.83
Round  49, Train loss: 1.480, Test loss: 1.515, Test accuracy: 94.84
Round  50, Train loss: 1.481, Test loss: 1.515, Test accuracy: 94.80
Round  51, Train loss: 1.480, Test loss: 1.515, Test accuracy: 94.94
Round  52, Train loss: 1.480, Test loss: 1.515, Test accuracy: 94.89
Round  53, Train loss: 1.482, Test loss: 1.514, Test accuracy: 94.92
Round  54, Train loss: 1.481, Test loss: 1.513, Test accuracy: 95.01
Round  55, Train loss: 1.479, Test loss: 1.514, Test accuracy: 94.97
Round  56, Train loss: 1.482, Test loss: 1.513, Test accuracy: 94.97
Round  57, Train loss: 1.481, Test loss: 1.515, Test accuracy: 94.86
Round  58, Train loss: 1.477, Test loss: 1.514, Test accuracy: 94.92
Round  59, Train loss: 1.482, Test loss: 1.514, Test accuracy: 94.94
Round  60, Train loss: 1.479, Test loss: 1.512, Test accuracy: 95.06
Round  61, Train loss: 1.477, Test loss: 1.513, Test accuracy: 94.95
Round  62, Train loss: 1.479, Test loss: 1.512, Test accuracy: 95.11
Round  63, Train loss: 1.480, Test loss: 1.512, Test accuracy: 95.10
Round  64, Train loss: 1.481, Test loss: 1.511, Test accuracy: 95.20
Round  65, Train loss: 1.478, Test loss: 1.512, Test accuracy: 95.19
Round  66, Train loss: 1.479, Test loss: 1.512, Test accuracy: 95.10
Round  67, Train loss: 1.478, Test loss: 1.511, Test accuracy: 95.27
Round  68, Train loss: 1.478, Test loss: 1.511, Test accuracy: 95.17
Round  69, Train loss: 1.478, Test loss: 1.511, Test accuracy: 95.23
Round  70, Train loss: 1.475, Test loss: 1.511, Test accuracy: 95.17
Round  71, Train loss: 1.477, Test loss: 1.510, Test accuracy: 95.20
Round  72, Train loss: 1.477, Test loss: 1.510, Test accuracy: 95.17
Round  73, Train loss: 1.475, Test loss: 1.510, Test accuracy: 95.34
Round  74, Train loss: 1.477, Test loss: 1.510, Test accuracy: 95.30
Round  75, Train loss: 1.475, Test loss: 1.510, Test accuracy: 95.20
Round  76, Train loss: 1.475, Test loss: 1.510, Test accuracy: 95.23
Round  77, Train loss: 1.476, Test loss: 1.510, Test accuracy: 95.12
Round  78, Train loss: 1.477, Test loss: 1.510, Test accuracy: 95.16
Round  79, Train loss: 1.475, Test loss: 1.511, Test accuracy: 95.05
Round  80, Train loss: 1.476, Test loss: 1.510, Test accuracy: 95.16
Round  81, Train loss: 1.477, Test loss: 1.510, Test accuracy: 95.16
Round  82, Train loss: 1.474, Test loss: 1.510, Test accuracy: 95.19
Round  83, Train loss: 1.476, Test loss: 1.510, Test accuracy: 95.25
Round  84, Train loss: 1.476, Test loss: 1.509, Test accuracy: 95.21
Round  85, Train loss: 1.475, Test loss: 1.509, Test accuracy: 95.20
Round  86, Train loss: 1.476, Test loss: 1.509, Test accuracy: 95.30
Round  87, Train loss: 1.473, Test loss: 1.509, Test accuracy: 95.36
Round  88, Train loss: 1.474, Test loss: 1.509, Test accuracy: 95.22
Round  89, Train loss: 1.479, Test loss: 1.509, Test accuracy: 95.35
Round  90, Train loss: 1.473, Test loss: 1.508, Test accuracy: 95.49
Round  91, Train loss: 1.474, Test loss: 1.508, Test accuracy: 95.56
Round  92, Train loss: 1.475, Test loss: 1.508, Test accuracy: 95.52
Round  93, Train loss: 1.475, Test loss: 1.508, Test accuracy: 95.42
Round  94, Train loss: 1.473, Test loss: 1.508, Test accuracy: 95.51
Round  95, Train loss: 1.473, Test loss: 1.508, Test accuracy: 95.46
Round  96, Train loss: 1.475, Test loss: 1.508, Test accuracy: 95.41
Round  97, Train loss: 1.475, Test loss: 1.508, Test accuracy: 95.39
Round  98, Train loss: 1.475, Test loss: 1.508, Test accuracy: 95.41
Round  99, Train loss: 1.474, Test loss: 1.508, Test accuracy: 95.48
Final Round, Train loss: 1.474, Test loss: 1.507, Test accuracy: 95.50
Average accuracy final 10 rounds: 95.4645
3696.859053373337
[5.156160354614258, 10.015505075454712, 15.036368370056152, 20.003087520599365, 24.94113278388977, 30.196549892425537, 34.97135376930237, 40.21116232872009, 45.207138776779175, 50.15269112586975, 55.385504722595215, 60.14151668548584, 65.2489550113678, 70.24717879295349, 75.18477368354797, 80.35395073890686, 85.11750555038452, 90.41433095932007, 95.32880449295044, 100.26990342140198, 105.50329041481018, 110.39086174964905, 115.54758262634277, 120.5808219909668, 125.66231894493103, 130.81934547424316, 135.74095702171326, 140.97994995117188, 145.93204832077026, 151.2706480026245, 156.31905961036682, 161.2722029685974, 166.5647678375244, 171.5870499610901, 176.6370837688446, 181.5599205493927, 186.4763309955597, 191.80080890655518, 196.90517497062683, 202.04616618156433, 207.00132966041565, 211.84702944755554, 216.91827607154846, 222.10873889923096, 227.2847557067871, 232.39369297027588, 237.431889295578, 242.46759605407715, 247.5987958908081, 252.83074760437012, 258.0731484889984, 263.01326513290405, 267.96103954315186, 273.142858505249, 278.1211884021759, 283.29536724090576, 288.37600803375244, 293.38310408592224, 298.7319989204407, 303.5215582847595, 308.6395597457886, 313.7154347896576, 318.92991876602173, 324.063236951828, 328.92448019981384, 334.31869435310364, 339.329448223114, 344.44857835769653, 349.6452839374542, 354.5018906593323, 359.6643054485321, 364.55816316604614, 369.85101675987244, 374.87346625328064, 379.93192291259766, 385.15251302719116, 390.21294474601746, 395.2498927116394, 400.226603269577, 405.1052758693695, 410.2708842754364, 415.50483894348145, 420.5755398273468, 425.62185287475586, 430.53742957115173, 435.68619894981384, 440.7264530658722, 445.8592176437378, 450.88657903671265, 455.91672539711, 460.9967956542969, 466.16861367225647, 471.5236611366272, 476.71667528152466, 481.7768750190735, 486.7602570056915, 491.7982246875763, 497.01942348480225, 502.1902012825012, 507.2125120162964, 509.73431062698364]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[30.59, 32.1, 59.41, 76.275, 87.65, 89.045, 90.0, 90.41, 90.895, 91.075, 91.3, 91.585, 91.75, 91.93, 92.175, 92.435, 92.45, 92.725, 93.095, 93.15, 93.2, 93.16, 93.37, 93.39, 93.415, 93.725, 93.905, 93.905, 94.03, 94.04, 94.015, 94.14, 94.215, 94.29, 94.42, 94.34, 94.54, 94.365, 94.515, 94.695, 94.535, 94.565, 94.625, 94.74, 94.665, 94.665, 94.63, 94.775, 94.835, 94.845, 94.795, 94.94, 94.89, 94.925, 95.01, 94.97, 94.97, 94.86, 94.915, 94.94, 95.055, 94.95, 95.11, 95.1, 95.205, 95.195, 95.1, 95.265, 95.175, 95.23, 95.175, 95.205, 95.17, 95.345, 95.295, 95.2, 95.235, 95.12, 95.155, 95.05, 95.16, 95.155, 95.195, 95.245, 95.21, 95.2, 95.295, 95.36, 95.22, 95.35, 95.49, 95.565, 95.515, 95.415, 95.51, 95.46, 95.41, 95.395, 95.405, 95.48, 95.495]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.316, Test loss: 2.294, Test accuracy: 20.69
Round   1, Train loss: 2.288, Test loss: 2.265, Test accuracy: 27.59
Round   2, Train loss: 2.239, Test loss: 2.195, Test accuracy: 40.92
Round   3, Train loss: 2.139, Test loss: 2.043, Test accuracy: 50.12
Round   4, Train loss: 1.983, Test loss: 1.933, Test accuracy: 60.85
Round   5, Train loss: 1.877, Test loss: 1.814, Test accuracy: 74.62
Round   6, Train loss: 1.779, Test loss: 1.743, Test accuracy: 78.92
Round   7, Train loss: 1.724, Test loss: 1.710, Test accuracy: 80.42
Round   8, Train loss: 1.700, Test loss: 1.692, Test accuracy: 81.64
Round   9, Train loss: 1.676, Test loss: 1.679, Test accuracy: 82.22
Round  10, Train loss: 1.679, Test loss: 1.653, Test accuracy: 84.65
Round  11, Train loss: 1.640, Test loss: 1.633, Test accuracy: 86.90
Round  12, Train loss: 1.608, Test loss: 1.621, Test accuracy: 87.70
Round  13, Train loss: 1.601, Test loss: 1.606, Test accuracy: 89.30
Round  14, Train loss: 1.600, Test loss: 1.588, Test accuracy: 91.12
Round  15, Train loss: 1.578, Test loss: 1.582, Test accuracy: 91.51
Round  16, Train loss: 1.574, Test loss: 1.574, Test accuracy: 92.12
Round  17, Train loss: 1.572, Test loss: 1.566, Test accuracy: 92.95
Round  18, Train loss: 1.559, Test loss: 1.562, Test accuracy: 93.12
Round  19, Train loss: 1.551, Test loss: 1.559, Test accuracy: 93.27
Round  20, Train loss: 1.552, Test loss: 1.556, Test accuracy: 93.48
Round  21, Train loss: 1.554, Test loss: 1.552, Test accuracy: 93.61
Round  22, Train loss: 1.539, Test loss: 1.548, Test accuracy: 93.93
Round  23, Train loss: 1.541, Test loss: 1.546, Test accuracy: 93.97
Round  24, Train loss: 1.537, Test loss: 1.542, Test accuracy: 94.28
Round  25, Train loss: 1.529, Test loss: 1.541, Test accuracy: 94.35
Round  26, Train loss: 1.529, Test loss: 1.539, Test accuracy: 94.38
Round  27, Train loss: 1.533, Test loss: 1.537, Test accuracy: 94.43
Round  28, Train loss: 1.524, Test loss: 1.536, Test accuracy: 94.57
Round  29, Train loss: 1.526, Test loss: 1.534, Test accuracy: 94.69
Round  30, Train loss: 1.523, Test loss: 1.532, Test accuracy: 94.79
Round  31, Train loss: 1.518, Test loss: 1.531, Test accuracy: 94.92
Round  32, Train loss: 1.511, Test loss: 1.529, Test accuracy: 95.02
Round  33, Train loss: 1.513, Test loss: 1.529, Test accuracy: 95.12
Round  34, Train loss: 1.510, Test loss: 1.528, Test accuracy: 95.13
Round  35, Train loss: 1.511, Test loss: 1.527, Test accuracy: 95.17
Round  36, Train loss: 1.512, Test loss: 1.526, Test accuracy: 95.24
Round  37, Train loss: 1.504, Test loss: 1.525, Test accuracy: 95.38
Round  38, Train loss: 1.506, Test loss: 1.524, Test accuracy: 95.47
Round  39, Train loss: 1.503, Test loss: 1.523, Test accuracy: 95.47
Round  40, Train loss: 1.505, Test loss: 1.521, Test accuracy: 95.52
Round  41, Train loss: 1.504, Test loss: 1.520, Test accuracy: 95.58
Round  42, Train loss: 1.497, Test loss: 1.519, Test accuracy: 95.66
Round  43, Train loss: 1.496, Test loss: 1.519, Test accuracy: 95.72
Round  44, Train loss: 1.498, Test loss: 1.518, Test accuracy: 95.84
Round  45, Train loss: 1.494, Test loss: 1.518, Test accuracy: 95.87
Round  46, Train loss: 1.492, Test loss: 1.518, Test accuracy: 95.81
Round  47, Train loss: 1.494, Test loss: 1.517, Test accuracy: 95.86
Round  48, Train loss: 1.498, Test loss: 1.516, Test accuracy: 95.86
Round  49, Train loss: 1.494, Test loss: 1.516, Test accuracy: 95.94
Round  50, Train loss: 1.490, Test loss: 1.515, Test accuracy: 96.05
Round  51, Train loss: 1.490, Test loss: 1.515, Test accuracy: 96.06
Round  52, Train loss: 1.490, Test loss: 1.514, Test accuracy: 96.06
Round  53, Train loss: 1.491, Test loss: 1.514, Test accuracy: 96.06
Round  54, Train loss: 1.491, Test loss: 1.513, Test accuracy: 96.04
Round  55, Train loss: 1.489, Test loss: 1.513, Test accuracy: 96.19
Round  56, Train loss: 1.492, Test loss: 1.512, Test accuracy: 96.13
Round  57, Train loss: 1.486, Test loss: 1.512, Test accuracy: 96.19
Round  58, Train loss: 1.484, Test loss: 1.511, Test accuracy: 96.28
Round  59, Train loss: 1.487, Test loss: 1.512, Test accuracy: 96.22
Round  60, Train loss: 1.486, Test loss: 1.511, Test accuracy: 96.29
Round  61, Train loss: 1.488, Test loss: 1.510, Test accuracy: 96.36
Round  62, Train loss: 1.484, Test loss: 1.510, Test accuracy: 96.30
Round  63, Train loss: 1.485, Test loss: 1.510, Test accuracy: 96.36
Round  64, Train loss: 1.485, Test loss: 1.509, Test accuracy: 96.42
Round  65, Train loss: 1.483, Test loss: 1.509, Test accuracy: 96.47
Round  66, Train loss: 1.484, Test loss: 1.509, Test accuracy: 96.49
Round  67, Train loss: 1.486, Test loss: 1.508, Test accuracy: 96.52
Round  68, Train loss: 1.482, Test loss: 1.508, Test accuracy: 96.53
Round  69, Train loss: 1.482, Test loss: 1.507, Test accuracy: 96.64
Round  70, Train loss: 1.483, Test loss: 1.508, Test accuracy: 96.54
Round  71, Train loss: 1.483, Test loss: 1.507, Test accuracy: 96.61
Round  72, Train loss: 1.481, Test loss: 1.507, Test accuracy: 96.56
Round  73, Train loss: 1.482, Test loss: 1.507, Test accuracy: 96.53
Round  74, Train loss: 1.481, Test loss: 1.506, Test accuracy: 96.56
Round  75, Train loss: 1.480, Test loss: 1.506, Test accuracy: 96.64
Round  76, Train loss: 1.479, Test loss: 1.506, Test accuracy: 96.72
Round  77, Train loss: 1.480, Test loss: 1.506, Test accuracy: 96.64
Round  78, Train loss: 1.478, Test loss: 1.505, Test accuracy: 96.73
Round  79, Train loss: 1.479, Test loss: 1.505, Test accuracy: 96.79
Round  80, Train loss: 1.478, Test loss: 1.505, Test accuracy: 96.80
Round  81, Train loss: 1.477, Test loss: 1.505, Test accuracy: 96.80
Round  82, Train loss: 1.478, Test loss: 1.505, Test accuracy: 96.81
Round  83, Train loss: 1.477, Test loss: 1.505, Test accuracy: 96.72
Round  84, Train loss: 1.476, Test loss: 1.504, Test accuracy: 96.85
Round  85, Train loss: 1.477, Test loss: 1.504, Test accuracy: 96.87
Round  86, Train loss: 1.475, Test loss: 1.504, Test accuracy: 96.84
Round  87, Train loss: 1.477, Test loss: 1.504, Test accuracy: 96.83
Round  88, Train loss: 1.477, Test loss: 1.504, Test accuracy: 96.94
Round  89, Train loss: 1.476, Test loss: 1.504, Test accuracy: 96.88
Round  90, Train loss: 1.476, Test loss: 1.503, Test accuracy: 96.91
Round  91, Train loss: 1.475, Test loss: 1.503, Test accuracy: 96.86
Round  92, Train loss: 1.474, Test loss: 1.503, Test accuracy: 96.92
Round  93, Train loss: 1.478, Test loss: 1.503, Test accuracy: 96.92
Round  94, Train loss: 1.475, Test loss: 1.503, Test accuracy: 96.95/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.476, Test loss: 1.502, Test accuracy: 96.93
Round  96, Train loss: 1.475, Test loss: 1.502, Test accuracy: 97.02
Round  97, Train loss: 1.475, Test loss: 1.502, Test accuracy: 97.05
Round  98, Train loss: 1.475, Test loss: 1.502, Test accuracy: 96.95
Round  99, Train loss: 1.474, Test loss: 1.502, Test accuracy: 97.02
Final Round, Train loss: 1.470, Test loss: 1.502, Test accuracy: 96.97
Average accuracy final 10 rounds: 96.953
2307.3915016651154
[2.861377239227295, 5.462383031845093, 7.93256950378418, 10.726809024810791, 13.235785007476807, 15.786519765853882, 18.387820959091187, 20.890220880508423, 23.538537979125977, 26.05698299407959, 28.597699403762817, 31.317254781723022, 34.12181115150452, 36.38107657432556, 38.978127241134644, 41.57986330986023, 44.2287483215332, 47.33126211166382, 49.7905170917511, 52.43908739089966, 55.22570061683655, 57.57235765457153, 60.396697998046875, 63.01831316947937, 65.61015582084656, 68.27704167366028, 70.77003717422485, 73.38775992393494, 76.15142107009888, 78.5346372127533, 81.26613759994507, 83.91330218315125, 86.32159233093262, 88.95149087905884, 91.55366563796997, 94.11371374130249, 96.89346742630005, 99.27863049507141, 102.0204758644104, 104.78973650932312, 107.20258450508118, 109.74141001701355, 112.55733585357666, 115.07870078086853, 117.77121162414551, 120.34807658195496, 122.88763165473938, 125.70529055595398, 128.06257390975952, 130.44980907440186, 133.17133450508118, 135.66718363761902, 138.10759043693542, 140.61049032211304, 143.16779279708862, 145.99172043800354, 148.58679485321045, 150.95488119125366, 153.80470418930054, 156.6966414451599, 159.69667267799377, 162.3599774837494, 165.34264492988586, 168.21306490898132, 171.23759150505066, 174.1263289451599, 177.48889994621277, 180.49423551559448, 183.50468611717224, 186.62395691871643, 189.6583957672119, 192.77574467658997, 195.21548056602478, 197.93175506591797, 200.92309880256653, 203.3349289894104, 205.9152009487152, 208.77717852592468, 211.26461839675903, 213.8471221923828, 216.26084280014038, 218.9852545261383, 222.52158999443054, 224.88437676429749, 227.4310336112976, 230.27091646194458, 233.06151247024536, 235.7279634475708, 238.27680802345276, 240.87223434448242, 243.64761209487915, 246.21954154968262, 248.689759016037, 251.42821645736694, 254.02236461639404, 257.1774697303772, 259.87816047668457, 262.9157569408417, 265.91974115371704, 268.7264790534973, 271.1327106952667]
[20.69, 27.59, 40.92, 50.125, 60.85, 74.62, 78.925, 80.425, 81.64, 82.215, 84.65, 86.9, 87.705, 89.295, 91.125, 91.51, 92.12, 92.955, 93.12, 93.265, 93.48, 93.605, 93.93, 93.965, 94.285, 94.35, 94.375, 94.43, 94.57, 94.695, 94.79, 94.915, 95.02, 95.125, 95.13, 95.165, 95.24, 95.375, 95.475, 95.475, 95.515, 95.585, 95.655, 95.725, 95.845, 95.87, 95.805, 95.86, 95.865, 95.945, 96.045, 96.06, 96.065, 96.06, 96.04, 96.19, 96.13, 96.19, 96.275, 96.215, 96.29, 96.36, 96.295, 96.355, 96.415, 96.47, 96.49, 96.52, 96.535, 96.635, 96.54, 96.605, 96.56, 96.535, 96.56, 96.64, 96.715, 96.635, 96.735, 96.79, 96.8, 96.8, 96.81, 96.72, 96.85, 96.87, 96.84, 96.83, 96.94, 96.875, 96.91, 96.855, 96.915, 96.925, 96.955, 96.93, 97.02, 97.05, 96.95, 97.02, 96.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.318, Test loss: 2.296, Test accuracy: 23.30
Round   1, Train loss: 2.301, Test loss: 2.274, Test accuracy: 33.24
Round   2, Train loss: 2.254, Test loss: 2.178, Test accuracy: 52.35
Round   3, Train loss: 2.126, Test loss: 2.010, Test accuracy: 66.62
Round   4, Train loss: 1.953, Test loss: 1.860, Test accuracy: 76.69
Round   5, Train loss: 1.824, Test loss: 1.809, Test accuracy: 80.36
Round   6, Train loss: 1.821, Test loss: 1.756, Test accuracy: 81.44
Round   7, Train loss: 1.775, Test loss: 1.731, Test accuracy: 82.09
Round   8, Train loss: 1.751, Test loss: 1.715, Test accuracy: 82.69
Round   9, Train loss: 1.727, Test loss: 1.708, Test accuracy: 83.00
Round  10, Train loss: 1.717, Test loss: 1.701, Test accuracy: 83.27
Round  11, Train loss: 1.709, Test loss: 1.691, Test accuracy: 83.59
Round  12, Train loss: 1.702, Test loss: 1.683, Test accuracy: 83.64
Round  13, Train loss: 1.709, Test loss: 1.674, Test accuracy: 83.95
Round  14, Train loss: 1.691, Test loss: 1.671, Test accuracy: 84.22
Round  15, Train loss: 1.679, Test loss: 1.669, Test accuracy: 84.42
Round  16, Train loss: 1.676, Test loss: 1.665, Test accuracy: 84.61
Round  17, Train loss: 1.686, Test loss: 1.651, Test accuracy: 84.82
Round  18, Train loss: 1.656, Test loss: 1.657, Test accuracy: 84.89
Round  19, Train loss: 1.669, Test loss: 1.646, Test accuracy: 85.00
Round  20, Train loss: 1.654, Test loss: 1.646, Test accuracy: 85.17
Round  21, Train loss: 1.655, Test loss: 1.642, Test accuracy: 85.75
Round  22, Train loss: 1.640, Test loss: 1.627, Test accuracy: 89.31
Round  23, Train loss: 1.633, Test loss: 1.602, Test accuracy: 91.83
Round  24, Train loss: 1.601, Test loss: 1.593, Test accuracy: 92.72
Round  25, Train loss: 1.590, Test loss: 1.586, Test accuracy: 93.09
Round  26, Train loss: 1.572, Test loss: 1.583, Test accuracy: 93.44
Round  27, Train loss: 1.578, Test loss: 1.578, Test accuracy: 93.75
Round  28, Train loss: 1.579, Test loss: 1.567, Test accuracy: 94.06
Round  29, Train loss: 1.572, Test loss: 1.561, Test accuracy: 94.45
Round  30, Train loss: 1.567, Test loss: 1.557, Test accuracy: 94.56
Round  31, Train loss: 1.563, Test loss: 1.554, Test accuracy: 94.90
Round  32, Train loss: 1.556, Test loss: 1.551, Test accuracy: 95.13
Round  33, Train loss: 1.552, Test loss: 1.551, Test accuracy: 95.03
Round  34, Train loss: 1.548, Test loss: 1.550, Test accuracy: 95.20
Round  35, Train loss: 1.545, Test loss: 1.550, Test accuracy: 95.38
Round  36, Train loss: 1.550, Test loss: 1.545, Test accuracy: 95.55
Round  37, Train loss: 1.545, Test loss: 1.544, Test accuracy: 95.53
Round  38, Train loss: 1.543, Test loss: 1.541, Test accuracy: 95.74
Round  39, Train loss: 1.541, Test loss: 1.540, Test accuracy: 95.83
Round  40, Train loss: 1.538, Test loss: 1.541, Test accuracy: 95.91
Round  41, Train loss: 1.533, Test loss: 1.542, Test accuracy: 95.81
Round  42, Train loss: 1.541, Test loss: 1.537, Test accuracy: 95.84
Round  43, Train loss: 1.535, Test loss: 1.537, Test accuracy: 95.86
Round  44, Train loss: 1.536, Test loss: 1.534, Test accuracy: 96.02
Round  45, Train loss: 1.527, Test loss: 1.536, Test accuracy: 96.08
Round  46, Train loss: 1.533, Test loss: 1.533, Test accuracy: 96.19
Round  47, Train loss: 1.527, Test loss: 1.533, Test accuracy: 96.20
Round  48, Train loss: 1.522, Test loss: 1.533, Test accuracy: 96.20
Round  49, Train loss: 1.527, Test loss: 1.531, Test accuracy: 96.32
Round  50, Train loss: 1.522, Test loss: 1.532, Test accuracy: 96.33
Round  51, Train loss: 1.521, Test loss: 1.531, Test accuracy: 96.36
Round  52, Train loss: 1.522, Test loss: 1.529, Test accuracy: 96.28
Round  53, Train loss: 1.518, Test loss: 1.530, Test accuracy: 96.28
Round  54, Train loss: 1.521, Test loss: 1.527, Test accuracy: 96.42
Round  55, Train loss: 1.513, Test loss: 1.529, Test accuracy: 96.43
Round  56, Train loss: 1.522, Test loss: 1.526, Test accuracy: 96.53
Round  57, Train loss: 1.522, Test loss: 1.525, Test accuracy: 96.52
Round  58, Train loss: 1.513, Test loss: 1.527, Test accuracy: 96.50
Round  59, Train loss: 1.515, Test loss: 1.526, Test accuracy: 96.53
Round  60, Train loss: 1.521, Test loss: 1.522, Test accuracy: 96.52
Round  61, Train loss: 1.514, Test loss: 1.524, Test accuracy: 96.65
Round  62, Train loss: 1.516, Test loss: 1.523, Test accuracy: 96.57
Round  63, Train loss: 1.514, Test loss: 1.523, Test accuracy: 96.61
Round  64, Train loss: 1.514, Test loss: 1.522, Test accuracy: 96.69
Round  65, Train loss: 1.512, Test loss: 1.522, Test accuracy: 96.65
Round  66, Train loss: 1.509, Test loss: 1.522, Test accuracy: 96.70
Round  67, Train loss: 1.515, Test loss: 1.520, Test accuracy: 96.61
Round  68, Train loss: 1.510, Test loss: 1.521, Test accuracy: 96.64
Round  69, Train loss: 1.510, Test loss: 1.519, Test accuracy: 96.73
Round  70, Train loss: 1.508, Test loss: 1.520, Test accuracy: 96.72
Round  71, Train loss: 1.509, Test loss: 1.519, Test accuracy: 96.74
Round  72, Train loss: 1.507, Test loss: 1.520, Test accuracy: 96.72
Round  73, Train loss: 1.506, Test loss: 1.520, Test accuracy: 96.69
Round  74, Train loss: 1.506, Test loss: 1.520, Test accuracy: 96.75
Round  75, Train loss: 1.505, Test loss: 1.519, Test accuracy: 96.75
Round  76, Train loss: 1.506, Test loss: 1.518, Test accuracy: 96.78
Round  77, Train loss: 1.504, Test loss: 1.519, Test accuracy: 96.69
Round  78, Train loss: 1.506, Test loss: 1.517, Test accuracy: 96.76
Round  79, Train loss: 1.505, Test loss: 1.517, Test accuracy: 96.81
Round  80, Train loss: 1.502, Test loss: 1.518, Test accuracy: 96.85
Round  81, Train loss: 1.503, Test loss: 1.517, Test accuracy: 96.78
Round  82, Train loss: 1.500, Test loss: 1.517, Test accuracy: 96.88
Round  83, Train loss: 1.504, Test loss: 1.516, Test accuracy: 96.85
Round  84, Train loss: 1.500, Test loss: 1.517, Test accuracy: 96.88
Round  85, Train loss: 1.502, Test loss: 1.517, Test accuracy: 96.83
Round  86, Train loss: 1.501, Test loss: 1.517, Test accuracy: 96.84
Round  87, Train loss: 1.500, Test loss: 1.517, Test accuracy: 96.89
Round  88, Train loss: 1.502, Test loss: 1.516, Test accuracy: 96.87
Round  89, Train loss: 1.501, Test loss: 1.515, Test accuracy: 96.87
Round  90, Train loss: 1.500, Test loss: 1.516, Test accuracy: 96.97
Round  91, Train loss: 1.499, Test loss: 1.516, Test accuracy: 96.91
Round  92, Train loss: 1.497, Test loss: 1.516, Test accuracy: 96.94
Round  93, Train loss: 1.502, Test loss: 1.514, Test accuracy: 96.85/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.499, Test loss: 1.515, Test accuracy: 96.91
Round  95, Train loss: 1.498, Test loss: 1.515, Test accuracy: 96.94
Round  96, Train loss: 1.498, Test loss: 1.515, Test accuracy: 96.92
Round  97, Train loss: 1.499, Test loss: 1.514, Test accuracy: 96.86
Round  98, Train loss: 1.498, Test loss: 1.514, Test accuracy: 96.88
Round  99, Train loss: 1.497, Test loss: 1.514, Test accuracy: 96.93
Final Round, Train loss: 1.480, Test loss: 1.513, Test accuracy: 96.95
Average accuracy final 10 rounds: 96.91099999999999
3410.69878077507
[2.7343389987945557, 5.468677997589111, 8.578579187393188, 11.688480377197266, 14.860614776611328, 18.03274917602539, 21.20735764503479, 24.38196611404419, 27.15518832206726, 29.928410530090332, 33.32388663291931, 36.71936273574829, 39.39888906478882, 42.078415393829346, 44.908130168914795, 47.737844944000244, 50.760239124298096, 53.78263330459595, 56.827598094940186, 59.872562885284424, 62.799659967422485, 65.72675704956055, 68.80423378944397, 71.88171052932739, 74.70046496391296, 77.51921939849854, 80.6340012550354, 83.74878311157227, 86.46219396591187, 89.17560482025146, 92.31848335266113, 95.4613618850708, 98.53835010528564, 101.61533832550049, 104.55817723274231, 107.50101613998413, 110.45637130737305, 113.41172647476196, 116.65885853767395, 119.90599060058594, 122.92956113815308, 125.95313167572021, 128.98499965667725, 132.01686763763428, 135.1914463043213, 138.3660249710083, 141.31909918785095, 144.2721734046936, 147.41279554367065, 150.5534176826477, 153.5846164226532, 156.6158151626587, 159.704425573349, 162.7930359840393, 165.86862564086914, 168.94421529769897, 171.76545977592468, 174.5867042541504, 177.29889845848083, 180.01109266281128, 183.2434856891632, 186.47587871551514, 189.1082649230957, 191.74065113067627, 194.76296091079712, 197.78527069091797, 200.84010910987854, 203.8949475288391, 207.04150772094727, 210.18806791305542, 212.8874490261078, 215.58683013916016, 218.9682035446167, 222.34957695007324, 225.0768325328827, 227.80408811569214, 230.80412435531616, 233.80416059494019, 236.86140894889832, 239.91865730285645, 242.952374458313, 245.98609161376953, 248.9581606388092, 251.93022966384888, 255.06731176376343, 258.204393863678, 260.9299762248993, 263.6555585861206, 266.8767623901367, 270.09796619415283, 272.7474455833435, 275.3969249725342, 278.51521039009094, 281.6334958076477, 284.6631257534027, 287.6927556991577, 290.6086356639862, 293.5245156288147, 296.2761652469635, 299.0278148651123, 302.34966564178467, 305.67151641845703, 308.37792801856995, 311.08433961868286, 314.09232544898987, 317.1003112792969, 320.2427453994751, 323.3851795196533, 326.26951456069946, 329.1538496017456, 332.26136350631714, 335.3688774108887, 338.600382566452, 341.8318877220154, 344.91987323760986, 348.00785875320435, 351.1955244541168, 354.3831901550293, 357.1700325012207, 359.9568748474121, 362.9192819595337, 365.8816890716553, 368.9393265247345, 371.9969639778137, 374.657466173172, 377.3179683685303, 380.20900321006775, 383.1000380516052, 386.27175307273865, 389.44346809387207, 392.29717350006104, 395.15087890625, 397.91085934638977, 400.67083978652954, 404.02006483078003, 407.3692898750305, 410.19826006889343, 413.02723026275635, 416.08406352996826, 419.1408967971802, 422.4551863670349, 425.76947593688965, 428.7735867500305, 431.7776975631714, 434.547322511673, 437.31694746017456, 440.4233889579773, 443.52983045578003, 446.3222107887268, 449.1145911216736, 452.1466886997223, 455.178786277771, 457.6961362361908, 460.2134861946106, 463.24055576324463, 466.26762533187866, 469.22926568984985, 472.19090604782104, 474.97553181648254, 477.76015758514404, 480.3964567184448, 483.0327558517456, 486.34770679473877, 489.66265773773193, 492.26995062828064, 494.87724351882935, 498.0220925807953, 501.16694164276123, 504.19292759895325, 507.21891355514526, 510.1379773616791, 513.0570411682129, 515.8414869308472, 518.6259326934814, 521.8901302814484, 525.1543278694153, 527.8491711616516, 530.5440144538879, 533.6661818027496, 536.7883491516113, 539.7140221595764, 542.6396951675415, 545.6367523670197, 548.6338095664978, 551.4578995704651, 554.2819895744324, 557.6346523761749, 560.9873151779175, 563.6732144355774, 566.3591136932373, 569.5982587337494, 572.8374037742615, 575.3083102703094, 577.7792167663574, 580.796511888504, 583.8138070106506, 586.789922952652, 589.7660388946533, 592.7012004852295, 595.6363620758057, 598.0756964683533, 600.5150308609009]
[23.3, 23.3, 33.24, 33.24, 52.355, 52.355, 66.62, 66.62, 76.69, 76.69, 80.36, 80.36, 81.445, 81.445, 82.09, 82.09, 82.685, 82.685, 82.995, 82.995, 83.27, 83.27, 83.59, 83.59, 83.64, 83.64, 83.955, 83.955, 84.215, 84.215, 84.415, 84.415, 84.605, 84.605, 84.82, 84.82, 84.885, 84.885, 85.005, 85.005, 85.17, 85.17, 85.755, 85.755, 89.315, 89.315, 91.835, 91.835, 92.725, 92.725, 93.09, 93.09, 93.435, 93.435, 93.745, 93.745, 94.06, 94.06, 94.455, 94.455, 94.56, 94.56, 94.9, 94.9, 95.13, 95.13, 95.025, 95.025, 95.2, 95.2, 95.375, 95.375, 95.55, 95.55, 95.535, 95.535, 95.74, 95.74, 95.825, 95.825, 95.91, 95.91, 95.81, 95.81, 95.84, 95.84, 95.865, 95.865, 96.015, 96.015, 96.085, 96.085, 96.185, 96.185, 96.2, 96.2, 96.2, 96.2, 96.32, 96.32, 96.325, 96.325, 96.36, 96.36, 96.28, 96.28, 96.28, 96.28, 96.425, 96.425, 96.43, 96.43, 96.53, 96.53, 96.515, 96.515, 96.5, 96.5, 96.53, 96.53, 96.52, 96.52, 96.65, 96.65, 96.57, 96.57, 96.605, 96.605, 96.695, 96.695, 96.65, 96.65, 96.7, 96.7, 96.61, 96.61, 96.635, 96.635, 96.73, 96.73, 96.72, 96.72, 96.74, 96.74, 96.715, 96.715, 96.695, 96.695, 96.75, 96.75, 96.75, 96.75, 96.78, 96.78, 96.69, 96.69, 96.76, 96.76, 96.815, 96.815, 96.85, 96.85, 96.775, 96.775, 96.88, 96.88, 96.85, 96.85, 96.875, 96.875, 96.825, 96.825, 96.84, 96.84, 96.89, 96.89, 96.87, 96.87, 96.87, 96.87, 96.97, 96.97, 96.905, 96.905, 96.945, 96.945, 96.85, 96.85, 96.91, 96.91, 96.94, 96.94, 96.92, 96.92, 96.865, 96.865, 96.875, 96.875, 96.93, 96.93, 96.95, 96.95]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.281, Test loss: 2.285, Test accuracy: 18.60
Round   0, Global train loss: 2.281, Global test loss: 2.298, Global test accuracy: 10.38
Round   1, Train loss: 2.247, Test loss: 2.232, Test accuracy: 41.03
Round   1, Global train loss: 2.247, Global test loss: 2.280, Global test accuracy: 27.80
Round   2, Train loss: 2.064, Test loss: 2.120, Test accuracy: 40.03
Round   2, Global train loss: 2.064, Global test loss: 2.263, Global test accuracy: 11.75
Round   3, Train loss: 1.821, Test loss: 2.007, Test accuracy: 53.70
Round   3, Global train loss: 1.821, Global test loss: 2.155, Global test accuracy: 30.92
Round   4, Train loss: 1.727, Test loss: 1.951, Test accuracy: 59.10
Round   4, Global train loss: 1.727, Global test loss: 2.142, Global test accuracy: 31.00
Round   5, Train loss: 1.808, Test loss: 1.881, Test accuracy: 61.40
Round   5, Global train loss: 1.808, Global test loss: 2.238, Global test accuracy: 16.37
Round   6, Train loss: 1.719, Test loss: 1.731, Test accuracy: 78.57
Round   6, Global train loss: 1.719, Global test loss: 2.233, Global test accuracy: 15.55
Round   7, Train loss: 1.579, Test loss: 1.706, Test accuracy: 79.42
Round   7, Global train loss: 1.579, Global test loss: 2.259, Global test accuracy: 11.82
Round   8, Train loss: 1.489, Test loss: 1.684, Test accuracy: 82.38
Round   8, Global train loss: 1.489, Global test loss: 2.136, Global test accuracy: 35.05
Round   9, Train loss: 1.514, Test loss: 1.644, Test accuracy: 86.47
Round   9, Global train loss: 1.514, Global test loss: 2.134, Global test accuracy: 42.18
Round  10, Train loss: 1.620, Test loss: 1.589, Test accuracy: 90.70
Round  10, Global train loss: 1.620, Global test loss: 2.107, Global test accuracy: 40.73
Round  11, Train loss: 1.511, Test loss: 1.577, Test accuracy: 92.00
Round  11, Global train loss: 1.511, Global test loss: 2.197, Global test accuracy: 26.03
Round  12, Train loss: 1.492, Test loss: 1.571, Test accuracy: 92.20
Round  12, Global train loss: 1.492, Global test loss: 2.136, Global test accuracy: 32.95
Round  13, Train loss: 1.489, Test loss: 1.568, Test accuracy: 92.28
Round  13, Global train loss: 1.489, Global test loss: 2.149, Global test accuracy: 32.22
Round  14, Train loss: 1.478, Test loss: 1.567, Test accuracy: 92.32
Round  14, Global train loss: 1.478, Global test loss: 2.268, Global test accuracy: 16.80
Round  15, Train loss: 1.473, Test loss: 1.566, Test accuracy: 92.35
Round  15, Global train loss: 1.473, Global test loss: 2.172, Global test accuracy: 23.05
Round  16, Train loss: 1.561, Test loss: 1.542, Test accuracy: 94.38
Round  16, Global train loss: 1.561, Global test loss: 2.135, Global test accuracy: 35.95
Round  17, Train loss: 1.473, Test loss: 1.542, Test accuracy: 94.35
Round  17, Global train loss: 1.473, Global test loss: 2.048, Global test accuracy: 44.07
Round  18, Train loss: 1.468, Test loss: 1.541, Test accuracy: 94.33
Round  18, Global train loss: 1.468, Global test loss: 2.097, Global test accuracy: 47.67
Round  19, Train loss: 1.482, Test loss: 1.532, Test accuracy: 94.42
Round  19, Global train loss: 1.482, Global test loss: 2.117, Global test accuracy: 32.03
Round  20, Train loss: 1.471, Test loss: 1.532, Test accuracy: 94.48
Round  20, Global train loss: 1.471, Global test loss: 2.323, Global test accuracy: 11.67
Round  21, Train loss: 1.487, Test loss: 1.519, Test accuracy: 95.63
Round  21, Global train loss: 1.487, Global test loss: 2.103, Global test accuracy: 52.82
Round  22, Train loss: 1.471, Test loss: 1.518, Test accuracy: 95.62
Round  22, Global train loss: 1.471, Global test loss: 2.109, Global test accuracy: 34.55
Round  23, Train loss: 1.472, Test loss: 1.517, Test accuracy: 95.68
Round  23, Global train loss: 1.472, Global test loss: 2.041, Global test accuracy: 54.03
Round  24, Train loss: 1.467, Test loss: 1.517, Test accuracy: 95.63
Round  24, Global train loss: 1.467, Global test loss: 2.028, Global test accuracy: 47.95
Round  25, Train loss: 1.471, Test loss: 1.516, Test accuracy: 95.62
Round  25, Global train loss: 1.471, Global test loss: 2.157, Global test accuracy: 29.73
Round  26, Train loss: 1.467, Test loss: 1.516, Test accuracy: 95.63
Round  26, Global train loss: 1.467, Global test loss: 2.079, Global test accuracy: 40.88
Round  27, Train loss: 1.467, Test loss: 1.516, Test accuracy: 95.65
Round  27, Global train loss: 1.467, Global test loss: 2.136, Global test accuracy: 30.32
Round  28, Train loss: 1.469, Test loss: 1.515, Test accuracy: 95.65
Round  28, Global train loss: 1.469, Global test loss: 2.093, Global test accuracy: 34.05
Round  29, Train loss: 1.469, Test loss: 1.515, Test accuracy: 95.63
Round  29, Global train loss: 1.469, Global test loss: 2.178, Global test accuracy: 22.77
Round  30, Train loss: 1.468, Test loss: 1.515, Test accuracy: 95.67
Round  30, Global train loss: 1.468, Global test loss: 2.100, Global test accuracy: 37.37
Round  31, Train loss: 1.476, Test loss: 1.510, Test accuracy: 95.70
Round  31, Global train loss: 1.476, Global test loss: 2.075, Global test accuracy: 43.72
Round  32, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.72
Round  32, Global train loss: 1.466, Global test loss: 2.066, Global test accuracy: 46.07
Round  33, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.70
Round  33, Global train loss: 1.465, Global test loss: 2.106, Global test accuracy: 39.08
Round  34, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.72
Round  34, Global train loss: 1.464, Global test loss: 2.101, Global test accuracy: 49.18
Round  35, Train loss: 1.470, Test loss: 1.509, Test accuracy: 95.75
Round  35, Global train loss: 1.470, Global test loss: 2.186, Global test accuracy: 24.43
Round  36, Train loss: 1.468, Test loss: 1.509, Test accuracy: 95.73
Round  36, Global train loss: 1.468, Global test loss: 2.141, Global test accuracy: 28.38
Round  37, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.73
Round  37, Global train loss: 1.464, Global test loss: 2.123, Global test accuracy: 34.15
Round  38, Train loss: 1.467, Test loss: 1.509, Test accuracy: 95.73
Round  38, Global train loss: 1.467, Global test loss: 2.099, Global test accuracy: 46.07
Round  39, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.75
Round  39, Global train loss: 1.466, Global test loss: 2.156, Global test accuracy: 29.43
Round  40, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.73
Round  40, Global train loss: 1.467, Global test loss: 2.115, Global test accuracy: 42.93
Round  41, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.77
Round  41, Global train loss: 1.467, Global test loss: 2.092, Global test accuracy: 40.58
Round  42, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.80
Round  42, Global train loss: 1.467, Global test loss: 2.117, Global test accuracy: 39.55
Round  43, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.75
Round  43, Global train loss: 1.467, Global test loss: 2.115, Global test accuracy: 35.42
Round  44, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.75
Round  44, Global train loss: 1.464, Global test loss: 2.094, Global test accuracy: 34.90
Round  45, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.77
Round  45, Global train loss: 1.465, Global test loss: 2.095, Global test accuracy: 41.67
Round  46, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.77
Round  46, Global train loss: 1.464, Global test loss: 2.131, Global test accuracy: 27.15
Round  47, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.75
Round  47, Global train loss: 1.466, Global test loss: 2.109, Global test accuracy: 35.93
Round  48, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.80
Round  48, Global train loss: 1.466, Global test loss: 2.106, Global test accuracy: 33.10
Round  49, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.80
Round  49, Global train loss: 1.465, Global test loss: 2.056, Global test accuracy: 56.07
Round  50, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.78
Round  50, Global train loss: 1.466, Global test loss: 2.082, Global test accuracy: 47.33
Round  51, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.78
Round  51, Global train loss: 1.465, Global test loss: 2.120, Global test accuracy: 32.33
Round  52, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.80
Round  52, Global train loss: 1.467, Global test loss: 2.181, Global test accuracy: 23.57
Round  53, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.80
Round  53, Global train loss: 1.467, Global test loss: 2.136, Global test accuracy: 31.90
Round  54, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.80
Round  54, Global train loss: 1.467, Global test loss: 2.092, Global test accuracy: 42.05
Round  55, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.80
Round  55, Global train loss: 1.466, Global test loss: 2.091, Global test accuracy: 44.42
Round  56, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.80
Round  56, Global train loss: 1.466, Global test loss: 2.057, Global test accuracy: 42.58
Round  57, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.80
Round  57, Global train loss: 1.464, Global test loss: 2.048, Global test accuracy: 46.28
Round  58, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.82
Round  58, Global train loss: 1.465, Global test loss: 2.141, Global test accuracy: 28.92
Round  59, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.83
Round  59, Global train loss: 1.466, Global test loss: 2.052, Global test accuracy: 44.93
Round  60, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.83
Round  60, Global train loss: 1.466, Global test loss: 2.088, Global test accuracy: 36.28
Round  61, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.83
Round  61, Global train loss: 1.464, Global test loss: 2.099, Global test accuracy: 47.65
Round  62, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.83
Round  62, Global train loss: 1.465, Global test loss: 2.043, Global test accuracy: 44.93
Round  63, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.83
Round  63, Global train loss: 1.466, Global test loss: 2.212, Global test accuracy: 22.37
Round  64, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.82
Round  64, Global train loss: 1.466, Global test loss: 2.181, Global test accuracy: 33.57
Round  65, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.80
Round  65, Global train loss: 1.466, Global test loss: 2.160, Global test accuracy: 28.97
Round  66, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.85
Round  66, Global train loss: 1.464, Global test loss: 2.037, Global test accuracy: 51.22
Round  67, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.85
Round  67, Global train loss: 1.465, Global test loss: 2.051, Global test accuracy: 39.82
Round  68, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.85
Round  68, Global train loss: 1.464, Global test loss: 2.110, Global test accuracy: 38.38
Round  69, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.83
Round  69, Global train loss: 1.465, Global test loss: 2.138, Global test accuracy: 27.18
Round  70, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.82
Round  70, Global train loss: 1.464, Global test loss: 2.132, Global test accuracy: 25.75
Round  71, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.82
Round  71, Global train loss: 1.465, Global test loss: 2.086, Global test accuracy: 38.70
Round  72, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.82
Round  72, Global train loss: 1.464, Global test loss: 2.071, Global test accuracy: 47.27
Round  73, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.82
Round  73, Global train loss: 1.465, Global test loss: 2.084, Global test accuracy: 43.00
Round  74, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.82
Round  74, Global train loss: 1.465, Global test loss: 2.158, Global test accuracy: 29.15
Round  75, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.82
Round  75, Global train loss: 1.466, Global test loss: 2.135, Global test accuracy: 28.25
Round  76, Train loss: 1.466, Test loss: 1.506, Test accuracy: 95.78
Round  76, Global train loss: 1.466, Global test loss: 2.133, Global test accuracy: 34.07
Round  77, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.78
Round  77, Global train loss: 1.464, Global test loss: 2.165, Global test accuracy: 28.95
Round  78, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.78
Round  78, Global train loss: 1.464, Global test loss: 2.134, Global test accuracy: 29.38
Round  79, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.77
Round  79, Global train loss: 1.464, Global test loss: 2.293, Global test accuracy: 12.12
Round  80, Train loss: 1.463, Test loss: 1.506, Test accuracy: 95.75
Round  80, Global train loss: 1.463, Global test loss: 2.178, Global test accuracy: 26.58
Round  81, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.75
Round  81, Global train loss: 1.465, Global test loss: 2.147, Global test accuracy: 32.18
Round  82, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.75
Round  82, Global train loss: 1.464, Global test loss: 2.172, Global test accuracy: 23.70
Round  83, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.75
Round  83, Global train loss: 1.465, Global test loss: 2.073, Global test accuracy: 45.22
Round  84, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.77
Round  84, Global train loss: 1.464, Global test loss: 2.267, Global test accuracy: 17.30
Round  85, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.75
Round  85, Global train loss: 1.465, Global test loss: 2.094, Global test accuracy: 34.52
Round  86, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.78
Round  86, Global train loss: 1.464, Global test loss: 2.091, Global test accuracy: 40.55
Round  87, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.78
Round  87, Global train loss: 1.465, Global test loss: 2.118, Global test accuracy: 31.82
Round  88, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.78
Round  88, Global train loss: 1.464, Global test loss: 2.162, Global test accuracy: 25.17
Round  89, Train loss: 1.463, Test loss: 1.506, Test accuracy: 95.78
Round  89, Global train loss: 1.463, Global test loss: 2.140, Global test accuracy: 32.60
Round  90, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.78
Round  90, Global train loss: 1.464, Global test loss: 2.076, Global test accuracy: 43.13
Round  91, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.73
Round  91, Global train loss: 1.465, Global test loss: 2.133, Global test accuracy: 29.78
Round  92, Train loss: 1.463, Test loss: 1.506, Test accuracy: 95.72
Round  92, Global train loss: 1.463, Global test loss: 2.259, Global test accuracy: 14.68
Round  93, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.73
Round  93, Global train loss: 1.464, Global test loss: 2.083, Global test accuracy: 39.05
Round  94, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.75
Round  94, Global train loss: 1.464, Global test loss: 2.088, Global test accuracy: 40.32
Round  95, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.68
Round  95, Global train loss: 1.464, Global test loss: 2.086, Global test accuracy: 38.38/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.68
Round  96, Global train loss: 1.464, Global test loss: 2.126, Global test accuracy: 30.42
Round  97, Train loss: 1.463, Test loss: 1.506, Test accuracy: 95.68
Round  97, Global train loss: 1.463, Global test loss: 2.121, Global test accuracy: 48.25
Round  98, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.70
Round  98, Global train loss: 1.464, Global test loss: 2.083, Global test accuracy: 45.95
Round  99, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.70
Round  99, Global train loss: 1.464, Global test loss: 2.057, Global test accuracy: 40.87
Final Round, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.77
Final Round, Global train loss: 1.464, Global test loss: 2.057, Global test accuracy: 40.87
Average accuracy final 10 rounds: 95.71666666666667 

Average global accuracy final 10 rounds: 37.083333333333336 

1134.9519517421722
[0.674598217010498, 1.349196434020996, 1.9827890396118164, 2.6163816452026367, 3.229362726211548, 3.842343807220459, 4.526665210723877, 5.210986614227295, 5.912387132644653, 6.613787651062012, 7.260249853134155, 7.906712055206299, 8.581399202346802, 9.256086349487305, 9.92271089553833, 10.589335441589355, 11.139718055725098, 11.69010066986084, 12.392665147781372, 13.095229625701904, 13.802366018295288, 14.509502410888672, 15.142085075378418, 15.774667739868164, 16.45904278755188, 17.143417835235596, 17.769530773162842, 18.395643711090088, 19.106351613998413, 19.81705951690674, 20.613871335983276, 21.410683155059814, 22.157960653305054, 22.905238151550293, 23.480157613754272, 24.055077075958252, 24.653531789779663, 25.251986503601074, 25.851367473602295, 26.450748443603516, 27.18464946746826, 27.918550491333008, 28.660683631896973, 29.402816772460938, 30.18528652191162, 30.967756271362305, 31.606382608413696, 32.24500894546509, 32.89568305015564, 33.54635715484619, 34.10253095626831, 34.65870475769043, 35.34179139137268, 36.02487802505493, 36.69920086860657, 37.3735237121582, 38.06438994407654, 38.75525617599487, 39.38928771018982, 40.023319244384766, 40.65584993362427, 41.28838062286377, 41.94449186325073, 42.600603103637695, 43.37900424003601, 44.157405376434326, 44.83322048187256, 45.50903558731079, 46.18306756019592, 46.857099533081055, 47.41883587837219, 47.98057222366333, 48.623074531555176, 49.26557683944702, 49.91722774505615, 50.56887865066528, 51.28387808799744, 51.99887752532959, 52.73059010505676, 53.462302684783936, 54.25409960746765, 55.04589653015137, 55.69366002082825, 56.34142351150513, 56.94257187843323, 57.54372024536133, 58.14009642601013, 58.736472606658936, 59.4773805141449, 60.21828842163086, 60.959134578704834, 61.69998073577881, 62.34984374046326, 62.999706745147705, 63.62254548072815, 64.2453842163086, 64.98559403419495, 65.7258038520813, 66.40614128112793, 67.08647871017456, 67.81601285934448, 68.5455470085144, 69.19922184944153, 69.85289669036865, 70.51250076293945, 71.17210483551025, 71.81117367744446, 72.45024251937866, 73.08518314361572, 73.72012376785278, 74.36451005935669, 75.0088963508606, 75.7602801322937, 76.5116639137268, 77.282705783844, 78.05374765396118, 78.74496579170227, 79.43618392944336, 79.973308801651, 80.51043367385864, 81.10735702514648, 81.70428037643433, 82.31628060340881, 82.9282808303833, 83.63131761550903, 84.33435440063477, 84.97345280647278, 85.61255121231079, 86.26131677627563, 86.91008234024048, 87.5858678817749, 88.26165342330933, 88.94957280158997, 89.6374921798706, 90.30314588546753, 90.96879959106445, 91.60221219062805, 92.23562479019165, 92.97110915184021, 93.70659351348877, 94.31670594215393, 94.92681837081909, 95.52532291412354, 96.12382745742798, 96.7668104171753, 97.40979337692261, 98.1745240688324, 98.93925476074219, 99.69008564949036, 100.44091653823853, 101.19748568534851, 101.9540548324585, 102.52468204498291, 103.09530925750732, 103.6749176979065, 104.25452613830566, 104.87580704689026, 105.49708795547485, 106.13762378692627, 106.77815961837769, 107.38010859489441, 107.98205757141113, 108.67341542243958, 109.36477327346802, 110.06210327148438, 110.75943326950073, 111.42569947242737, 112.091965675354, 112.75546956062317, 113.41897344589233, 114.09413957595825, 114.76930570602417, 115.43814516067505, 116.10698461532593, 116.78399443626404, 117.46100425720215, 118.11844754219055, 118.77589082717896, 119.43272280693054, 120.08955478668213, 120.78437781333923, 121.47920083999634, 122.27386140823364, 123.06852197647095, 123.84199070930481, 124.61545944213867, 125.3085823059082, 126.00170516967773, 126.63351082801819, 127.26531648635864, 127.88716125488281, 128.50900602340698, 129.1964271068573, 129.88384819030762, 130.53736782073975, 131.19088745117188, 131.87677907943726, 132.56267070770264, 133.3331127166748, 134.10355472564697, 135.4250192642212, 136.7464838027954]
[18.6, 18.6, 41.03333333333333, 41.03333333333333, 40.03333333333333, 40.03333333333333, 53.7, 53.7, 59.1, 59.1, 61.4, 61.4, 78.56666666666666, 78.56666666666666, 79.41666666666667, 79.41666666666667, 82.38333333333334, 82.38333333333334, 86.46666666666667, 86.46666666666667, 90.7, 90.7, 92.0, 92.0, 92.2, 92.2, 92.28333333333333, 92.28333333333333, 92.31666666666666, 92.31666666666666, 92.35, 92.35, 94.38333333333334, 94.38333333333334, 94.35, 94.35, 94.33333333333333, 94.33333333333333, 94.41666666666667, 94.41666666666667, 94.48333333333333, 94.48333333333333, 95.63333333333334, 95.63333333333334, 95.61666666666666, 95.61666666666666, 95.68333333333334, 95.68333333333334, 95.63333333333334, 95.63333333333334, 95.61666666666666, 95.61666666666666, 95.63333333333334, 95.63333333333334, 95.65, 95.65, 95.65, 95.65, 95.63333333333334, 95.63333333333334, 95.66666666666667, 95.66666666666667, 95.7, 95.7, 95.71666666666667, 95.71666666666667, 95.7, 95.7, 95.71666666666667, 95.71666666666667, 95.75, 95.75, 95.73333333333333, 95.73333333333333, 95.73333333333333, 95.73333333333333, 95.73333333333333, 95.73333333333333, 95.75, 95.75, 95.73333333333333, 95.73333333333333, 95.76666666666667, 95.76666666666667, 95.8, 95.8, 95.75, 95.75, 95.75, 95.75, 95.76666666666667, 95.76666666666667, 95.76666666666667, 95.76666666666667, 95.75, 95.75, 95.8, 95.8, 95.8, 95.8, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.81666666666666, 95.81666666666666, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.81666666666666, 95.81666666666666, 95.8, 95.8, 95.85, 95.85, 95.85, 95.85, 95.85, 95.85, 95.83333333333333, 95.83333333333333, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.76666666666667, 95.76666666666667, 95.75, 95.75, 95.75, 95.75, 95.75, 95.75, 95.75, 95.75, 95.76666666666667, 95.76666666666667, 95.75, 95.75, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.73333333333333, 95.73333333333333, 95.71666666666667, 95.71666666666667, 95.73333333333333, 95.73333333333333, 95.75, 95.75, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.68333333333334, 95.7, 95.7, 95.7, 95.7, 95.76666666666667, 95.76666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.271, Test loss: 2.269, Test accuracy: 15.03
Round   0, Global train loss: 2.271, Global test loss: 2.295, Global test accuracy: 10.00
Round   1, Train loss: 2.104, Test loss: 2.132, Test accuracy: 34.22
Round   1, Global train loss: 2.104, Global test loss: 2.227, Global test accuracy: 26.07
Round   2, Train loss: 1.936, Test loss: 1.973, Test accuracy: 55.72
Round   2, Global train loss: 1.936, Global test loss: 2.153, Global test accuracy: 43.88
Round   3, Train loss: 1.785, Test loss: 1.821, Test accuracy: 71.25
Round   3, Global train loss: 1.785, Global test loss: 2.082, Global test accuracy: 52.02
Round   4, Train loss: 1.647, Test loss: 1.707, Test accuracy: 80.40
Round   4, Global train loss: 1.647, Global test loss: 2.023, Global test accuracy: 45.60
Round   5, Train loss: 1.584, Test loss: 1.676, Test accuracy: 84.07
Round   5, Global train loss: 1.584, Global test loss: 1.869, Global test accuracy: 71.67
Round   6, Train loss: 1.625, Test loss: 1.664, Test accuracy: 84.35
Round   6, Global train loss: 1.625, Global test loss: 1.869, Global test accuracy: 59.63
Round   7, Train loss: 1.671, Test loss: 1.644, Test accuracy: 84.80
Round   7, Global train loss: 1.671, Global test loss: 1.813, Global test accuracy: 69.22
Round   8, Train loss: 1.577, Test loss: 1.635, Test accuracy: 84.95
Round   8, Global train loss: 1.577, Global test loss: 1.825, Global test accuracy: 67.75
Round   9, Train loss: 1.621, Test loss: 1.623, Test accuracy: 85.40
Round   9, Global train loss: 1.621, Global test loss: 1.768, Global test accuracy: 73.78
Round  10, Train loss: 1.619, Test loss: 1.609, Test accuracy: 86.00
Round  10, Global train loss: 1.619, Global test loss: 1.758, Global test accuracy: 74.12
Round  11, Train loss: 1.651, Test loss: 1.593, Test accuracy: 87.72
Round  11, Global train loss: 1.651, Global test loss: 1.761, Global test accuracy: 73.18
Round  12, Train loss: 1.520, Test loss: 1.576, Test accuracy: 89.38
Round  12, Global train loss: 1.520, Global test loss: 1.766, Global test accuracy: 73.67
Round  13, Train loss: 1.497, Test loss: 1.575, Test accuracy: 89.45
Round  13, Global train loss: 1.497, Global test loss: 1.741, Global test accuracy: 76.28
Round  14, Train loss: 1.543, Test loss: 1.560, Test accuracy: 90.97
Round  14, Global train loss: 1.543, Global test loss: 1.727, Global test accuracy: 75.77
Round  15, Train loss: 1.553, Test loss: 1.561, Test accuracy: 90.87
Round  15, Global train loss: 1.553, Global test loss: 1.748, Global test accuracy: 73.20
Round  16, Train loss: 1.520, Test loss: 1.543, Test accuracy: 92.65
Round  16, Global train loss: 1.520, Global test loss: 1.691, Global test accuracy: 81.27
Round  17, Train loss: 1.512, Test loss: 1.541, Test accuracy: 92.67
Round  17, Global train loss: 1.512, Global test loss: 1.675, Global test accuracy: 81.73
Round  18, Train loss: 1.543, Test loss: 1.539, Test accuracy: 92.68
Round  18, Global train loss: 1.543, Global test loss: 1.680, Global test accuracy: 79.90
Round  19, Train loss: 1.501, Test loss: 1.525, Test accuracy: 94.40
Round  19, Global train loss: 1.501, Global test loss: 1.654, Global test accuracy: 85.50
Round  20, Train loss: 1.497, Test loss: 1.525, Test accuracy: 94.33
Round  20, Global train loss: 1.497, Global test loss: 1.672, Global test accuracy: 81.20
Round  21, Train loss: 1.498, Test loss: 1.524, Test accuracy: 94.35
Round  21, Global train loss: 1.498, Global test loss: 1.679, Global test accuracy: 80.17
Round  22, Train loss: 1.495, Test loss: 1.526, Test accuracy: 94.18
Round  22, Global train loss: 1.495, Global test loss: 1.685, Global test accuracy: 79.85
Round  23, Train loss: 1.547, Test loss: 1.526, Test accuracy: 94.08
Round  23, Global train loss: 1.547, Global test loss: 1.656, Global test accuracy: 82.92
Round  24, Train loss: 1.543, Test loss: 1.526, Test accuracy: 94.12
Round  24, Global train loss: 1.543, Global test loss: 1.642, Global test accuracy: 85.35
Round  25, Train loss: 1.540, Test loss: 1.527, Test accuracy: 93.87
Round  25, Global train loss: 1.540, Global test loss: 1.637, Global test accuracy: 84.63
Round  26, Train loss: 1.532, Test loss: 1.526, Test accuracy: 93.93
Round  26, Global train loss: 1.532, Global test loss: 1.683, Global test accuracy: 78.73
Round  27, Train loss: 1.485, Test loss: 1.525, Test accuracy: 94.08
Round  27, Global train loss: 1.485, Global test loss: 1.692, Global test accuracy: 77.40
Round  28, Train loss: 1.482, Test loss: 1.525, Test accuracy: 94.07
Round  28, Global train loss: 1.482, Global test loss: 1.642, Global test accuracy: 84.03
Round  29, Train loss: 1.485, Test loss: 1.525, Test accuracy: 94.03
Round  29, Global train loss: 1.485, Global test loss: 1.628, Global test accuracy: 86.12
Round  30, Train loss: 1.487, Test loss: 1.525, Test accuracy: 94.03
Round  30, Global train loss: 1.487, Global test loss: 1.660, Global test accuracy: 82.02
Round  31, Train loss: 1.486, Test loss: 1.523, Test accuracy: 94.17
Round  31, Global train loss: 1.486, Global test loss: 1.619, Global test accuracy: 86.73
Round  32, Train loss: 1.488, Test loss: 1.521, Test accuracy: 94.35
Round  32, Global train loss: 1.488, Global test loss: 1.638, Global test accuracy: 83.43
Round  33, Train loss: 1.531, Test loss: 1.521, Test accuracy: 94.33
Round  33, Global train loss: 1.531, Global test loss: 1.615, Global test accuracy: 86.98
Round  34, Train loss: 1.481, Test loss: 1.521, Test accuracy: 94.38
Round  34, Global train loss: 1.481, Global test loss: 1.646, Global test accuracy: 82.92
Round  35, Train loss: 1.489, Test loss: 1.521, Test accuracy: 94.43
Round  35, Global train loss: 1.489, Global test loss: 1.624, Global test accuracy: 85.32
Round  36, Train loss: 1.482, Test loss: 1.520, Test accuracy: 94.55
Round  36, Global train loss: 1.482, Global test loss: 1.625, Global test accuracy: 84.93
Round  37, Train loss: 1.481, Test loss: 1.520, Test accuracy: 94.48
Round  37, Global train loss: 1.481, Global test loss: 1.644, Global test accuracy: 82.73
Round  38, Train loss: 1.481, Test loss: 1.520, Test accuracy: 94.47
Round  38, Global train loss: 1.481, Global test loss: 1.643, Global test accuracy: 83.32
Round  39, Train loss: 1.477, Test loss: 1.520, Test accuracy: 94.53
Round  39, Global train loss: 1.477, Global test loss: 1.646, Global test accuracy: 82.13
Round  40, Train loss: 1.485, Test loss: 1.520, Test accuracy: 94.50
Round  40, Global train loss: 1.485, Global test loss: 1.634, Global test accuracy: 84.53
Round  41, Train loss: 1.486, Test loss: 1.521, Test accuracy: 94.50
Round  41, Global train loss: 1.486, Global test loss: 1.637, Global test accuracy: 83.43
Round  42, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.48
Round  42, Global train loss: 1.478, Global test loss: 1.613, Global test accuracy: 86.48
Round  43, Train loss: 1.477, Test loss: 1.520, Test accuracy: 94.52
Round  43, Global train loss: 1.477, Global test loss: 1.597, Global test accuracy: 87.60
Round  44, Train loss: 1.478, Test loss: 1.520, Test accuracy: 94.53
Round  44, Global train loss: 1.478, Global test loss: 1.605, Global test accuracy: 86.65
Round  45, Train loss: 1.483, Test loss: 1.519, Test accuracy: 94.67
Round  45, Global train loss: 1.483, Global test loss: 1.627, Global test accuracy: 84.22
Round  46, Train loss: 1.478, Test loss: 1.519, Test accuracy: 94.57
Round  46, Global train loss: 1.478, Global test loss: 1.607, Global test accuracy: 86.88
Round  47, Train loss: 1.478, Test loss: 1.519, Test accuracy: 94.50
Round  47, Global train loss: 1.478, Global test loss: 1.622, Global test accuracy: 84.72
Round  48, Train loss: 1.479, Test loss: 1.519, Test accuracy: 94.52
Round  48, Global train loss: 1.479, Global test loss: 1.630, Global test accuracy: 83.88
Round  49, Train loss: 1.475, Test loss: 1.519, Test accuracy: 94.50
Round  49, Global train loss: 1.475, Global test loss: 1.626, Global test accuracy: 84.52
Round  50, Train loss: 1.475, Test loss: 1.519, Test accuracy: 94.55
Round  50, Global train loss: 1.475, Global test loss: 1.620, Global test accuracy: 85.23
Round  51, Train loss: 1.475, Test loss: 1.519, Test accuracy: 94.63
Round  51, Global train loss: 1.475, Global test loss: 1.605, Global test accuracy: 86.62
Round  52, Train loss: 1.479, Test loss: 1.519, Test accuracy: 94.68
Round  52, Global train loss: 1.479, Global test loss: 1.594, Global test accuracy: 87.73
Round  53, Train loss: 1.528, Test loss: 1.518, Test accuracy: 94.78
Round  53, Global train loss: 1.528, Global test loss: 1.616, Global test accuracy: 85.42
Round  54, Train loss: 1.531, Test loss: 1.518, Test accuracy: 94.77
Round  54, Global train loss: 1.531, Global test loss: 1.617, Global test accuracy: 85.27
Round  55, Train loss: 1.476, Test loss: 1.518, Test accuracy: 94.78
Round  55, Global train loss: 1.476, Global test loss: 1.611, Global test accuracy: 85.92
Round  56, Train loss: 1.475, Test loss: 1.519, Test accuracy: 94.72
Round  56, Global train loss: 1.475, Global test loss: 1.593, Global test accuracy: 87.85
Round  57, Train loss: 1.474, Test loss: 1.519, Test accuracy: 94.70
Round  57, Global train loss: 1.474, Global test loss: 1.593, Global test accuracy: 88.00
Round  58, Train loss: 1.477, Test loss: 1.519, Test accuracy: 94.70
Round  58, Global train loss: 1.477, Global test loss: 1.596, Global test accuracy: 87.28
Round  59, Train loss: 1.474, Test loss: 1.519, Test accuracy: 94.63
Round  59, Global train loss: 1.474, Global test loss: 1.589, Global test accuracy: 88.28
Round  60, Train loss: 1.477, Test loss: 1.518, Test accuracy: 94.68
Round  60, Global train loss: 1.477, Global test loss: 1.629, Global test accuracy: 84.38
Round  61, Train loss: 1.478, Test loss: 1.518, Test accuracy: 94.70
Round  61, Global train loss: 1.478, Global test loss: 1.633, Global test accuracy: 83.03
Round  62, Train loss: 1.476, Test loss: 1.518, Test accuracy: 94.77
Round  62, Global train loss: 1.476, Global test loss: 1.597, Global test accuracy: 87.17
Round  63, Train loss: 1.475, Test loss: 1.518, Test accuracy: 94.72
Round  63, Global train loss: 1.475, Global test loss: 1.604, Global test accuracy: 86.33
Round  64, Train loss: 1.529, Test loss: 1.518, Test accuracy: 94.68
Round  64, Global train loss: 1.529, Global test loss: 1.627, Global test accuracy: 84.40
Round  65, Train loss: 1.473, Test loss: 1.519, Test accuracy: 94.67
Round  65, Global train loss: 1.473, Global test loss: 1.603, Global test accuracy: 86.58
Round  66, Train loss: 1.528, Test loss: 1.519, Test accuracy: 94.60
Round  66, Global train loss: 1.528, Global test loss: 1.590, Global test accuracy: 87.95
Round  67, Train loss: 1.475, Test loss: 1.518, Test accuracy: 94.57
Round  67, Global train loss: 1.475, Global test loss: 1.612, Global test accuracy: 86.08
Round  68, Train loss: 1.475, Test loss: 1.518, Test accuracy: 94.63
Round  68, Global train loss: 1.475, Global test loss: 1.591, Global test accuracy: 88.15
Round  69, Train loss: 1.473, Test loss: 1.518, Test accuracy: 94.65
Round  69, Global train loss: 1.473, Global test loss: 1.590, Global test accuracy: 88.15
Round  70, Train loss: 1.472, Test loss: 1.518, Test accuracy: 94.70
Round  70, Global train loss: 1.472, Global test loss: 1.601, Global test accuracy: 87.03
Round  71, Train loss: 1.472, Test loss: 1.518, Test accuracy: 94.75
Round  71, Global train loss: 1.472, Global test loss: 1.646, Global test accuracy: 81.62
Round  72, Train loss: 1.473, Test loss: 1.518, Test accuracy: 94.72
Round  72, Global train loss: 1.473, Global test loss: 1.585, Global test accuracy: 88.40
Round  73, Train loss: 1.474, Test loss: 1.518, Test accuracy: 94.70
Round  73, Global train loss: 1.474, Global test loss: 1.589, Global test accuracy: 87.90
Round  74, Train loss: 1.523, Test loss: 1.518, Test accuracy: 94.70
Round  74, Global train loss: 1.523, Global test loss: 1.601, Global test accuracy: 87.15
Round  75, Train loss: 1.473, Test loss: 1.518, Test accuracy: 94.83
Round  75, Global train loss: 1.473, Global test loss: 1.590, Global test accuracy: 87.87
Round  76, Train loss: 1.470, Test loss: 1.518, Test accuracy: 94.80
Round  76, Global train loss: 1.470, Global test loss: 1.586, Global test accuracy: 88.52
Round  77, Train loss: 1.471, Test loss: 1.518, Test accuracy: 94.72
Round  77, Global train loss: 1.471, Global test loss: 1.586, Global test accuracy: 88.43
Round  78, Train loss: 1.471, Test loss: 1.518, Test accuracy: 94.68
Round  78, Global train loss: 1.471, Global test loss: 1.592, Global test accuracy: 87.92
Round  79, Train loss: 1.472, Test loss: 1.518, Test accuracy: 94.67
Round  79, Global train loss: 1.472, Global test loss: 1.590, Global test accuracy: 87.98
Round  80, Train loss: 1.472, Test loss: 1.518, Test accuracy: 94.62
Round  80, Global train loss: 1.472, Global test loss: 1.587, Global test accuracy: 88.13
Round  81, Train loss: 1.472, Test loss: 1.517, Test accuracy: 94.75
Round  81, Global train loss: 1.472, Global test loss: 1.610, Global test accuracy: 85.48
Round  82, Train loss: 1.525, Test loss: 1.517, Test accuracy: 94.75
Round  82, Global train loss: 1.525, Global test loss: 1.590, Global test accuracy: 88.03
Round  83, Train loss: 1.471, Test loss: 1.518, Test accuracy: 94.70
Round  83, Global train loss: 1.471, Global test loss: 1.594, Global test accuracy: 87.37
Round  84, Train loss: 1.471, Test loss: 1.518, Test accuracy: 94.72
Round  84, Global train loss: 1.471, Global test loss: 1.611, Global test accuracy: 85.57
Round  85, Train loss: 1.472, Test loss: 1.517, Test accuracy: 94.75
Round  85, Global train loss: 1.472, Global test loss: 1.582, Global test accuracy: 88.95
Round  86, Train loss: 1.524, Test loss: 1.517, Test accuracy: 94.73
Round  86, Global train loss: 1.524, Global test loss: 1.592, Global test accuracy: 88.03
Round  87, Train loss: 1.469, Test loss: 1.518, Test accuracy: 94.60
Round  87, Global train loss: 1.469, Global test loss: 1.585, Global test accuracy: 88.50
Round  88, Train loss: 1.524, Test loss: 1.518, Test accuracy: 94.60
Round  88, Global train loss: 1.524, Global test loss: 1.593, Global test accuracy: 87.60
Round  89, Train loss: 1.472, Test loss: 1.519, Test accuracy: 94.60
Round  89, Global train loss: 1.472, Global test loss: 1.603, Global test accuracy: 86.57
Round  90, Train loss: 1.471, Test loss: 1.519, Test accuracy: 94.62
Round  90, Global train loss: 1.471, Global test loss: 1.617, Global test accuracy: 85.07
Round  91, Train loss: 1.472, Test loss: 1.519, Test accuracy: 94.57
Round  91, Global train loss: 1.472, Global test loss: 1.613, Global test accuracy: 85.32
Round  92, Train loss: 1.469, Test loss: 1.519, Test accuracy: 94.53
Round  92, Global train loss: 1.469, Global test loss: 1.596, Global test accuracy: 87.10
Round  93, Train loss: 1.470, Test loss: 1.519, Test accuracy: 94.53
Round  93, Global train loss: 1.470, Global test loss: 1.582, Global test accuracy: 88.43
Round  94, Train loss: 1.523, Test loss: 1.519, Test accuracy: 94.58
Round  94, Global train loss: 1.523, Global test loss: 1.589, Global test accuracy: 88.17
Round  95, Train loss: 1.471, Test loss: 1.519, Test accuracy: 94.57
Round  95, Global train loss: 1.471, Global test loss: 1.584, Global test accuracy: 88.32/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.468, Test loss: 1.518, Test accuracy: 94.63
Round  96, Global train loss: 1.468, Global test loss: 1.591, Global test accuracy: 87.60
Round  97, Train loss: 1.469, Test loss: 1.518, Test accuracy: 94.58
Round  97, Global train loss: 1.469, Global test loss: 1.583, Global test accuracy: 88.48
Round  98, Train loss: 1.471, Test loss: 1.519, Test accuracy: 94.48
Round  98, Global train loss: 1.471, Global test loss: 1.578, Global test accuracy: 88.57
Round  99, Train loss: 1.470, Test loss: 1.518, Test accuracy: 94.48
Round  99, Global train loss: 1.470, Global test loss: 1.588, Global test accuracy: 87.77
Final Round, Train loss: 1.484, Test loss: 1.517, Test accuracy: 94.75
Final Round, Global train loss: 1.484, Global test loss: 1.588, Global test accuracy: 87.77
Average accuracy final 10 rounds: 94.55833333333334 

Average global accuracy final 10 rounds: 87.48166666666667 

961.9693629741669
[0.7179117202758789, 1.4358234405517578, 2.143594741821289, 2.8513660430908203, 3.557452917098999, 4.263539791107178, 4.917004346847534, 5.570468902587891, 6.212937355041504, 6.855405807495117, 7.498398065567017, 8.141390323638916, 8.82182765007019, 9.502264976501465, 10.029176473617554, 10.556087970733643, 11.118589401245117, 11.681090831756592, 12.296750545501709, 12.912410259246826, 13.50945520401001, 14.106500148773193, 14.743292331695557, 15.38008451461792, 16.013397216796875, 16.64670991897583, 17.258443355560303, 17.870176792144775, 18.517752647399902, 19.16532850265503, 19.799363136291504, 20.43339776992798, 20.971097707748413, 21.508797645568848, 22.049236297607422, 22.589674949645996, 23.19944190979004, 23.809208869934082, 24.419161558151245, 25.029114246368408, 25.61938190460205, 26.209649562835693, 26.839747190475464, 27.469844818115234, 28.138990879058838, 28.80813694000244, 29.44978404045105, 30.091431140899658, 30.742085695266724, 31.39274024963379, 31.912925720214844, 32.4331111907959, 32.995405435562134, 33.55769968032837, 34.16455698013306, 34.771414279937744, 35.35718846321106, 35.942962646484375, 36.53456234931946, 37.12616205215454, 37.75647830963135, 38.386794567108154, 39.02628970146179, 39.66578483581543, 40.330381870269775, 40.99497890472412, 41.60431361198425, 42.213648319244385, 42.75551223754883, 43.29737615585327, 43.868916034698486, 44.4404559135437, 45.031734466552734, 45.62301301956177, 46.21558237075806, 46.808151721954346, 47.395567893981934, 47.98298406600952, 48.59862661361694, 49.214269161224365, 49.89816331863403, 50.5820574760437, 51.21482229232788, 51.84758710861206, 52.45460653305054, 53.061625957489014, 53.61022090911865, 54.15881586074829, 54.74281167984009, 55.326807498931885, 55.897679805755615, 56.468552112579346, 57.05385208129883, 57.63915205001831, 58.23754358291626, 58.83593511581421, 59.45198178291321, 60.06802845001221, 60.73821139335632, 61.40839433670044, 62.06462216377258, 62.72084999084473, 63.3011953830719, 63.88154077529907, 64.43041777610779, 64.9792947769165, 65.57436847686768, 66.16944217681885, 66.79418206214905, 67.41892194747925, 67.99719095230103, 68.5754599571228, 69.15670919418335, 69.7379584312439, 70.32294368743896, 70.90792894363403, 71.46426844596863, 72.02060794830322, 72.57821226119995, 73.13581657409668, 73.70527362823486, 74.27473068237305, 74.80312824249268, 75.3315258026123, 75.83522415161133, 76.33892250061035, 76.88874363899231, 77.43856477737427, 77.92242646217346, 78.40628814697266, 78.90429353713989, 79.40229892730713, 79.97387027740479, 80.54544162750244, 81.13655591011047, 81.7276701927185, 82.31227803230286, 82.8968858718872, 83.48087430000305, 84.0648627281189, 84.63533997535706, 85.20581722259521, 85.7078845500946, 86.209951877594, 86.69453167915344, 87.17911148071289, 87.6916241645813, 88.2041368484497, 88.73102569580078, 89.25791454315186, 89.85990524291992, 90.46189594268799, 91.02144169807434, 91.5809874534607, 92.2039487361908, 92.8269100189209, 93.38116502761841, 93.93542003631592, 94.45979166030884, 94.98416328430176, 95.51542520523071, 96.04668712615967, 96.56421184539795, 97.08173656463623, 97.59491419792175, 98.10809183120728, 98.61270713806152, 99.11732244491577, 99.67474126815796, 100.23216009140015, 100.81049537658691, 101.38883066177368, 101.91381597518921, 102.43880128860474, 103.02361965179443, 103.60843801498413, 104.19105958938599, 104.77368116378784, 105.31254506111145, 105.85140895843506, 106.36813473701477, 106.88486051559448, 107.41226887702942, 107.93967723846436, 108.4769639968872, 109.01425075531006, 109.49996423721313, 109.98567771911621, 110.57066535949707, 111.15565299987793, 111.73242092132568, 112.30918884277344, 112.8867437839508, 113.46429872512817, 114.0347273349762, 114.60515594482422, 115.17988395690918, 115.75461196899414, 116.2820177078247, 116.80942344665527, 117.91614580154419, 119.0228681564331]
[15.033333333333333, 15.033333333333333, 34.21666666666667, 34.21666666666667, 55.71666666666667, 55.71666666666667, 71.25, 71.25, 80.4, 80.4, 84.06666666666666, 84.06666666666666, 84.35, 84.35, 84.8, 84.8, 84.95, 84.95, 85.4, 85.4, 86.0, 86.0, 87.71666666666667, 87.71666666666667, 89.38333333333334, 89.38333333333334, 89.45, 89.45, 90.96666666666667, 90.96666666666667, 90.86666666666666, 90.86666666666666, 92.65, 92.65, 92.66666666666667, 92.66666666666667, 92.68333333333334, 92.68333333333334, 94.4, 94.4, 94.33333333333333, 94.33333333333333, 94.35, 94.35, 94.18333333333334, 94.18333333333334, 94.08333333333333, 94.08333333333333, 94.11666666666666, 94.11666666666666, 93.86666666666666, 93.86666666666666, 93.93333333333334, 93.93333333333334, 94.08333333333333, 94.08333333333333, 94.06666666666666, 94.06666666666666, 94.03333333333333, 94.03333333333333, 94.03333333333333, 94.03333333333333, 94.16666666666667, 94.16666666666667, 94.35, 94.35, 94.33333333333333, 94.33333333333333, 94.38333333333334, 94.38333333333334, 94.43333333333334, 94.43333333333334, 94.55, 94.55, 94.48333333333333, 94.48333333333333, 94.46666666666667, 94.46666666666667, 94.53333333333333, 94.53333333333333, 94.5, 94.5, 94.5, 94.5, 94.48333333333333, 94.48333333333333, 94.51666666666667, 94.51666666666667, 94.53333333333333, 94.53333333333333, 94.66666666666667, 94.66666666666667, 94.56666666666666, 94.56666666666666, 94.5, 94.5, 94.51666666666667, 94.51666666666667, 94.5, 94.5, 94.55, 94.55, 94.63333333333334, 94.63333333333334, 94.68333333333334, 94.68333333333334, 94.78333333333333, 94.78333333333333, 94.76666666666667, 94.76666666666667, 94.78333333333333, 94.78333333333333, 94.71666666666667, 94.71666666666667, 94.7, 94.7, 94.7, 94.7, 94.63333333333334, 94.63333333333334, 94.68333333333334, 94.68333333333334, 94.7, 94.7, 94.76666666666667, 94.76666666666667, 94.71666666666667, 94.71666666666667, 94.68333333333334, 94.68333333333334, 94.66666666666667, 94.66666666666667, 94.6, 94.6, 94.56666666666666, 94.56666666666666, 94.63333333333334, 94.63333333333334, 94.65, 94.65, 94.7, 94.7, 94.75, 94.75, 94.71666666666667, 94.71666666666667, 94.7, 94.7, 94.7, 94.7, 94.83333333333333, 94.83333333333333, 94.8, 94.8, 94.71666666666667, 94.71666666666667, 94.68333333333334, 94.68333333333334, 94.66666666666667, 94.66666666666667, 94.61666666666666, 94.61666666666666, 94.75, 94.75, 94.75, 94.75, 94.7, 94.7, 94.71666666666667, 94.71666666666667, 94.75, 94.75, 94.73333333333333, 94.73333333333333, 94.6, 94.6, 94.6, 94.6, 94.6, 94.6, 94.61666666666666, 94.61666666666666, 94.56666666666666, 94.56666666666666, 94.53333333333333, 94.53333333333333, 94.53333333333333, 94.53333333333333, 94.58333333333333, 94.58333333333333, 94.56666666666666, 94.56666666666666, 94.63333333333334, 94.63333333333334, 94.58333333333333, 94.58333333333333, 94.48333333333333, 94.48333333333333, 94.48333333333333, 94.48333333333333, 94.75, 94.75]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.296, Test loss: 2.300, Test accuracy: 21.72
Round   1, Train loss: 2.293, Test loss: 2.296, Test accuracy: 25.17
Round   2, Train loss: 2.282, Test loss: 2.289, Test accuracy: 28.65
Round   3, Train loss: 2.222, Test loss: 2.263, Test accuracy: 14.50
Round   4, Train loss: 2.186, Test loss: 2.215, Test accuracy: 26.67
Round   5, Train loss: 2.053, Test loss: 2.121, Test accuracy: 45.37
Round   6, Train loss: 1.899, Test loss: 2.037, Test accuracy: 48.68
Round   7, Train loss: 1.802, Test loss: 1.899, Test accuracy: 63.62
Round   8, Train loss: 1.662, Test loss: 1.820, Test accuracy: 69.68
Round   9, Train loss: 1.657, Test loss: 1.753, Test accuracy: 75.73
Round  10, Train loss: 1.639, Test loss: 1.705, Test accuracy: 79.98
Round  11, Train loss: 1.600, Test loss: 1.662, Test accuracy: 83.78
Round  12, Train loss: 1.583, Test loss: 1.620, Test accuracy: 87.63
Round  13, Train loss: 1.540, Test loss: 1.608, Test accuracy: 88.43
Round  14, Train loss: 1.544, Test loss: 1.600, Test accuracy: 88.50
Round  15, Train loss: 1.531, Test loss: 1.574, Test accuracy: 90.98
Round  16, Train loss: 1.567, Test loss: 1.571, Test accuracy: 91.07
Round  17, Train loss: 1.532, Test loss: 1.561, Test accuracy: 92.12
Round  18, Train loss: 1.525, Test loss: 1.558, Test accuracy: 91.95
Round  19, Train loss: 1.514, Test loss: 1.552, Test accuracy: 92.43
Round  20, Train loss: 1.497, Test loss: 1.551, Test accuracy: 92.40
Round  21, Train loss: 1.514, Test loss: 1.547, Test accuracy: 92.82
Round  22, Train loss: 1.522, Test loss: 1.543, Test accuracy: 93.17
Round  23, Train loss: 1.510, Test loss: 1.543, Test accuracy: 93.05
Round  24, Train loss: 1.552, Test loss: 1.541, Test accuracy: 93.13
Round  25, Train loss: 1.505, Test loss: 1.540, Test accuracy: 93.25
Round  26, Train loss: 1.496, Test loss: 1.538, Test accuracy: 93.30
Round  27, Train loss: 1.545, Test loss: 1.538, Test accuracy: 93.33
Round  28, Train loss: 1.505, Test loss: 1.537, Test accuracy: 93.33
Round  29, Train loss: 1.491, Test loss: 1.537, Test accuracy: 93.27
Round  30, Train loss: 1.486, Test loss: 1.536, Test accuracy: 93.40
Round  31, Train loss: 1.492, Test loss: 1.536, Test accuracy: 93.37
Round  32, Train loss: 1.494, Test loss: 1.536, Test accuracy: 93.28
Round  33, Train loss: 1.494, Test loss: 1.534, Test accuracy: 93.50
Round  34, Train loss: 1.493, Test loss: 1.535, Test accuracy: 93.30
Round  35, Train loss: 1.555, Test loss: 1.532, Test accuracy: 93.57
Round  36, Train loss: 1.497, Test loss: 1.533, Test accuracy: 93.45
Round  37, Train loss: 1.497, Test loss: 1.532, Test accuracy: 93.60
Round  38, Train loss: 1.541, Test loss: 1.531, Test accuracy: 93.62
Round  39, Train loss: 1.506, Test loss: 1.530, Test accuracy: 93.70
Round  40, Train loss: 1.548, Test loss: 1.530, Test accuracy: 93.55
Round  41, Train loss: 1.485, Test loss: 1.530, Test accuracy: 93.68
Round  42, Train loss: 1.485, Test loss: 1.529, Test accuracy: 93.70
Round  43, Train loss: 1.533, Test loss: 1.529, Test accuracy: 93.70
Round  44, Train loss: 1.548, Test loss: 1.528, Test accuracy: 93.82
Round  45, Train loss: 1.483, Test loss: 1.528, Test accuracy: 93.82
Round  46, Train loss: 1.525, Test loss: 1.529, Test accuracy: 93.72
Round  47, Train loss: 1.495, Test loss: 1.514, Test accuracy: 95.33
Round  48, Train loss: 1.488, Test loss: 1.513, Test accuracy: 95.35
Round  49, Train loss: 1.488, Test loss: 1.512, Test accuracy: 95.32
Round  50, Train loss: 1.484, Test loss: 1.512, Test accuracy: 95.23
Round  51, Train loss: 1.478, Test loss: 1.513, Test accuracy: 95.20
Round  52, Train loss: 1.493, Test loss: 1.511, Test accuracy: 95.30
Round  53, Train loss: 1.479, Test loss: 1.511, Test accuracy: 95.45
Round  54, Train loss: 1.482, Test loss: 1.512, Test accuracy: 95.25
Round  55, Train loss: 1.478, Test loss: 1.511, Test accuracy: 95.25
Round  56, Train loss: 1.483, Test loss: 1.512, Test accuracy: 95.18
Round  57, Train loss: 1.478, Test loss: 1.512, Test accuracy: 95.15
Round  58, Train loss: 1.483, Test loss: 1.511, Test accuracy: 95.45
Round  59, Train loss: 1.476, Test loss: 1.511, Test accuracy: 95.47
Round  60, Train loss: 1.480, Test loss: 1.510, Test accuracy: 95.48
Round  61, Train loss: 1.488, Test loss: 1.510, Test accuracy: 95.43
Round  62, Train loss: 1.477, Test loss: 1.510, Test accuracy: 95.45
Round  63, Train loss: 1.476, Test loss: 1.510, Test accuracy: 95.50
Round  64, Train loss: 1.477, Test loss: 1.510, Test accuracy: 95.52
Round  65, Train loss: 1.484, Test loss: 1.509, Test accuracy: 95.52
Round  66, Train loss: 1.483, Test loss: 1.509, Test accuracy: 95.52
Round  67, Train loss: 1.482, Test loss: 1.509, Test accuracy: 95.63
Round  68, Train loss: 1.481, Test loss: 1.509, Test accuracy: 95.58
Round  69, Train loss: 1.480, Test loss: 1.509, Test accuracy: 95.57
Round  70, Train loss: 1.472, Test loss: 1.509, Test accuracy: 95.55
Round  71, Train loss: 1.480, Test loss: 1.509, Test accuracy: 95.47
Round  72, Train loss: 1.479, Test loss: 1.509, Test accuracy: 95.45
Round  73, Train loss: 1.475, Test loss: 1.509, Test accuracy: 95.55
Round  74, Train loss: 1.477, Test loss: 1.508, Test accuracy: 95.58
Round  75, Train loss: 1.475, Test loss: 1.508, Test accuracy: 95.67
Round  76, Train loss: 1.473, Test loss: 1.508, Test accuracy: 95.65
Round  77, Train loss: 1.481, Test loss: 1.508, Test accuracy: 95.63
Round  78, Train loss: 1.479, Test loss: 1.508, Test accuracy: 95.70
Round  79, Train loss: 1.480, Test loss: 1.508, Test accuracy: 95.67
Round  80, Train loss: 1.477, Test loss: 1.508, Test accuracy: 95.72
Round  81, Train loss: 1.475, Test loss: 1.508, Test accuracy: 95.72
Round  82, Train loss: 1.474, Test loss: 1.508, Test accuracy: 95.60
Round  83, Train loss: 1.473, Test loss: 1.508, Test accuracy: 95.68
Round  84, Train loss: 1.476, Test loss: 1.507, Test accuracy: 95.70
Round  85, Train loss: 1.476, Test loss: 1.507, Test accuracy: 95.65
Round  86, Train loss: 1.472, Test loss: 1.507, Test accuracy: 95.62
Round  87, Train loss: 1.475, Test loss: 1.507, Test accuracy: 95.58
Round  88, Train loss: 1.476, Test loss: 1.507, Test accuracy: 95.68
Round  89, Train loss: 1.477, Test loss: 1.507, Test accuracy: 95.67
Round  90, Train loss: 1.475, Test loss: 1.507, Test accuracy: 95.63
Round  91, Train loss: 1.472, Test loss: 1.507, Test accuracy: 95.58
Round  92, Train loss: 1.475, Test loss: 1.507, Test accuracy: 95.60
Round  93, Train loss: 1.476, Test loss: 1.507, Test accuracy: 95.58
Round  94, Train loss: 1.472, Test loss: 1.507, Test accuracy: 95.60
Round  95, Train loss: 1.474, Test loss: 1.507, Test accuracy: 95.67
Round  96, Train loss: 1.472, Test loss: 1.507, Test accuracy: 95.65
Round  97, Train loss: 1.472, Test loss: 1.506, Test accuracy: 95.68
Round  98, Train loss: 1.472, Test loss: 1.506, Test accuracy: 95.70/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.468, Test loss: 1.506, Test accuracy: 95.70
Final Round, Train loss: 1.473, Test loss: 1.506, Test accuracy: 95.63
Average accuracy final 10 rounds: 95.63999999999999 

680.6116404533386
[0.5661096572875977, 1.1322193145751953, 1.6871654987335205, 2.2421116828918457, 2.7897090911865234, 3.337306499481201, 3.909876585006714, 4.482446670532227, 5.051344394683838, 5.620242118835449, 6.137262582778931, 6.654283046722412, 7.152368783950806, 7.650454521179199, 8.148996829986572, 8.647539138793945, 9.122594356536865, 9.597649574279785, 10.10006046295166, 10.602471351623535, 11.112959146499634, 11.623446941375732, 12.122215270996094, 12.620983600616455, 13.19103717803955, 13.761090755462646, 14.295393466949463, 14.82969617843628, 15.358892917633057, 15.888089656829834, 16.46065640449524, 17.033223152160645, 17.597399711608887, 18.16157627105713, 18.673905849456787, 19.186235427856445, 19.678208351135254, 20.170181274414062, 20.67957615852356, 21.188971042633057, 21.677358865737915, 22.165746688842773, 22.662587642669678, 23.159428596496582, 23.636967658996582, 24.114506721496582, 24.595075607299805, 25.075644493103027, 25.646569967269897, 26.217495441436768, 26.784669637680054, 27.35184383392334, 27.87524652481079, 28.398649215698242, 28.949601888656616, 29.50055456161499, 30.05712604522705, 30.61369752883911, 31.15280246734619, 31.69190740585327, 32.2046959400177, 32.71748447418213, 33.19458532333374, 33.67168617248535, 34.15410399436951, 34.63652181625366, 35.14157509803772, 35.64662837982178, 36.146262884140015, 36.64589738845825, 37.13516545295715, 37.624433517456055, 38.10921573638916, 38.593997955322266, 39.17031812667847, 39.74663829803467, 40.311466455459595, 40.87629461288452, 41.43341612815857, 41.99053764343262, 42.533820152282715, 43.07710266113281, 43.609896659851074, 44.142690658569336, 44.6878080368042, 45.23292541503906, 45.72123098373413, 46.2095365524292, 46.67509746551514, 47.140658378601074, 47.62197160720825, 48.10328483581543, 48.60069680213928, 49.098108768463135, 49.6083607673645, 50.11861276626587, 50.60090208053589, 51.08319139480591, 51.61731481552124, 52.15143823623657, 52.692431688308716, 53.23342514038086, 53.811920166015625, 54.39041519165039, 54.91618824005127, 55.44196128845215, 56.00421404838562, 56.56646680831909, 57.09240412712097, 57.61834144592285, 58.11360287666321, 58.608864307403564, 59.12015962600708, 59.631454944610596, 60.14134216308594, 60.65122938156128, 61.136706590652466, 61.62218379974365, 62.113422870635986, 62.60466194152832, 63.127848386764526, 63.65103483200073, 64.20376801490784, 64.75650119781494, 65.30979752540588, 65.86309385299683, 66.42384457588196, 66.98459529876709, 67.5412962436676, 68.09799718856812, 68.68206524848938, 69.26613330841064, 69.78744387626648, 70.30875444412231, 70.80444931983948, 71.30014419555664, 71.80349922180176, 72.30685424804688, 72.82317352294922, 73.33949279785156, 73.83040070533752, 74.32130861282349, 74.81290888786316, 75.30450916290283, 75.82448983192444, 76.34447050094604, 76.81944584846497, 77.29442119598389, 77.8794150352478, 78.46440887451172, 79.0174310207367, 79.57045316696167, 80.10010266304016, 80.62975215911865, 81.17948627471924, 81.72922039031982, 82.31317400932312, 82.89712762832642, 83.40428686141968, 83.91144609451294, 84.37281727790833, 84.83418846130371, 85.31836557388306, 85.8025426864624, 86.32050800323486, 86.83847332000732, 87.36285448074341, 87.88723564147949, 88.38986372947693, 88.89249181747437, 89.36673617362976, 89.84098052978516, 90.38925194740295, 90.93752336502075, 91.51477122306824, 92.09201908111572, 92.66195774078369, 93.23189640045166, 93.77713680267334, 94.32237720489502, 94.86900162696838, 95.41562604904175, 95.93523836135864, 96.45485067367554, 96.95965933799744, 97.46446800231934, 97.95595145225525, 98.44743490219116, 98.92709994316101, 99.40676498413086, 99.89974284172058, 100.3927206993103, 100.91218376159668, 101.43164682388306, 101.94173693656921, 102.45182704925537, 102.96596217155457, 103.48009729385376, 104.01824140548706, 104.55638551712036, 105.60527610778809, 106.65416669845581]
[21.716666666666665, 21.716666666666665, 25.166666666666668, 25.166666666666668, 28.65, 28.65, 14.5, 14.5, 26.666666666666668, 26.666666666666668, 45.36666666666667, 45.36666666666667, 48.68333333333333, 48.68333333333333, 63.61666666666667, 63.61666666666667, 69.68333333333334, 69.68333333333334, 75.73333333333333, 75.73333333333333, 79.98333333333333, 79.98333333333333, 83.78333333333333, 83.78333333333333, 87.63333333333334, 87.63333333333334, 88.43333333333334, 88.43333333333334, 88.5, 88.5, 90.98333333333333, 90.98333333333333, 91.06666666666666, 91.06666666666666, 92.11666666666666, 92.11666666666666, 91.95, 91.95, 92.43333333333334, 92.43333333333334, 92.4, 92.4, 92.81666666666666, 92.81666666666666, 93.16666666666667, 93.16666666666667, 93.05, 93.05, 93.13333333333334, 93.13333333333334, 93.25, 93.25, 93.3, 93.3, 93.33333333333333, 93.33333333333333, 93.33333333333333, 93.33333333333333, 93.26666666666667, 93.26666666666667, 93.4, 93.4, 93.36666666666666, 93.36666666666666, 93.28333333333333, 93.28333333333333, 93.5, 93.5, 93.3, 93.3, 93.56666666666666, 93.56666666666666, 93.45, 93.45, 93.6, 93.6, 93.61666666666666, 93.61666666666666, 93.7, 93.7, 93.55, 93.55, 93.68333333333334, 93.68333333333334, 93.7, 93.7, 93.7, 93.7, 93.81666666666666, 93.81666666666666, 93.81666666666666, 93.81666666666666, 93.71666666666667, 93.71666666666667, 95.33333333333333, 95.33333333333333, 95.35, 95.35, 95.31666666666666, 95.31666666666666, 95.23333333333333, 95.23333333333333, 95.2, 95.2, 95.3, 95.3, 95.45, 95.45, 95.25, 95.25, 95.25, 95.25, 95.18333333333334, 95.18333333333334, 95.15, 95.15, 95.45, 95.45, 95.46666666666667, 95.46666666666667, 95.48333333333333, 95.48333333333333, 95.43333333333334, 95.43333333333334, 95.45, 95.45, 95.5, 95.5, 95.51666666666667, 95.51666666666667, 95.51666666666667, 95.51666666666667, 95.51666666666667, 95.51666666666667, 95.63333333333334, 95.63333333333334, 95.58333333333333, 95.58333333333333, 95.56666666666666, 95.56666666666666, 95.55, 95.55, 95.46666666666667, 95.46666666666667, 95.45, 95.45, 95.55, 95.55, 95.58333333333333, 95.58333333333333, 95.66666666666667, 95.66666666666667, 95.65, 95.65, 95.63333333333334, 95.63333333333334, 95.7, 95.7, 95.66666666666667, 95.66666666666667, 95.71666666666667, 95.71666666666667, 95.71666666666667, 95.71666666666667, 95.6, 95.6, 95.68333333333334, 95.68333333333334, 95.7, 95.7, 95.65, 95.65, 95.61666666666666, 95.61666666666666, 95.58333333333333, 95.58333333333333, 95.68333333333334, 95.68333333333334, 95.66666666666667, 95.66666666666667, 95.63333333333334, 95.63333333333334, 95.58333333333333, 95.58333333333333, 95.6, 95.6, 95.58333333333333, 95.58333333333333, 95.6, 95.6, 95.66666666666667, 95.66666666666667, 95.65, 95.65, 95.68333333333334, 95.68333333333334, 95.7, 95.7, 95.7, 95.7, 95.63333333333334, 95.63333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.280, Test loss: 2.289, Test accuracy: 27.15
Round   1, Train loss: 2.134, Test loss: 2.193, Test accuracy: 39.53
Round   2, Train loss: 1.882, Test loss: 2.068, Test accuracy: 48.32
Round   3, Train loss: 1.710, Test loss: 1.925, Test accuracy: 64.90
Round   4, Train loss: 1.718, Test loss: 1.801, Test accuracy: 73.32
Round   5, Train loss: 1.579, Test loss: 1.748, Test accuracy: 74.87
Round   6, Train loss: 1.571, Test loss: 1.687, Test accuracy: 80.33
Round   7, Train loss: 1.570, Test loss: 1.680, Test accuracy: 80.73
Round   8, Train loss: 1.565, Test loss: 1.641, Test accuracy: 84.03
Round   9, Train loss: 1.515, Test loss: 1.645, Test accuracy: 82.97
Round  10, Train loss: 1.604, Test loss: 1.626, Test accuracy: 85.25
Round  11, Train loss: 1.659, Test loss: 1.608, Test accuracy: 86.67
Round  12, Train loss: 1.544, Test loss: 1.604, Test accuracy: 87.12
Round  13, Train loss: 1.603, Test loss: 1.599, Test accuracy: 87.63
Round  14, Train loss: 1.596, Test loss: 1.595, Test accuracy: 87.65
Round  15, Train loss: 1.591, Test loss: 1.594, Test accuracy: 87.75
Round  16, Train loss: 1.588, Test loss: 1.592, Test accuracy: 87.68
Round  17, Train loss: 1.593, Test loss: 1.591, Test accuracy: 87.75
Round  18, Train loss: 1.590, Test loss: 1.590, Test accuracy: 87.75
Round  19, Train loss: 1.541, Test loss: 1.590, Test accuracy: 87.60
Round  20, Train loss: 1.483, Test loss: 1.588, Test accuracy: 87.80
Round  21, Train loss: 1.535, Test loss: 1.589, Test accuracy: 87.75
Round  22, Train loss: 1.586, Test loss: 1.587, Test accuracy: 87.78
Round  23, Train loss: 1.583, Test loss: 1.587, Test accuracy: 87.67
Round  24, Train loss: 1.477, Test loss: 1.587, Test accuracy: 87.73
Round  25, Train loss: 1.531, Test loss: 1.588, Test accuracy: 87.60
Round  26, Train loss: 1.579, Test loss: 1.588, Test accuracy: 87.58
Round  27, Train loss: 1.482, Test loss: 1.586, Test accuracy: 87.97
Round  28, Train loss: 1.535, Test loss: 1.584, Test accuracy: 88.03
Round  29, Train loss: 1.635, Test loss: 1.584, Test accuracy: 88.07
Round  30, Train loss: 1.482, Test loss: 1.571, Test accuracy: 89.43
Round  31, Train loss: 1.531, Test loss: 1.571, Test accuracy: 89.30
Round  32, Train loss: 1.583, Test loss: 1.570, Test accuracy: 89.32
Round  33, Train loss: 1.478, Test loss: 1.569, Test accuracy: 89.52
Round  34, Train loss: 1.529, Test loss: 1.569, Test accuracy: 89.50
Round  35, Train loss: 1.526, Test loss: 1.569, Test accuracy: 89.58
Round  36, Train loss: 1.527, Test loss: 1.568, Test accuracy: 89.68
Round  37, Train loss: 1.582, Test loss: 1.568, Test accuracy: 89.62
Round  38, Train loss: 1.476, Test loss: 1.567, Test accuracy: 89.68
Round  39, Train loss: 1.475, Test loss: 1.568, Test accuracy: 89.60
Round  40, Train loss: 1.527, Test loss: 1.568, Test accuracy: 89.50
Round  41, Train loss: 1.473, Test loss: 1.568, Test accuracy: 89.53
Round  42, Train loss: 1.529, Test loss: 1.567, Test accuracy: 89.57
Round  43, Train loss: 1.477, Test loss: 1.567, Test accuracy: 89.60
Round  44, Train loss: 1.526, Test loss: 1.567, Test accuracy: 89.67
Round  45, Train loss: 1.527, Test loss: 1.567, Test accuracy: 89.57
Round  46, Train loss: 1.524, Test loss: 1.566, Test accuracy: 89.58
Round  47, Train loss: 1.579, Test loss: 1.566, Test accuracy: 89.67
Round  48, Train loss: 1.524, Test loss: 1.566, Test accuracy: 89.62
Round  49, Train loss: 1.524, Test loss: 1.566, Test accuracy: 89.52
Round  50, Train loss: 1.524, Test loss: 1.566, Test accuracy: 89.57
Round  51, Train loss: 1.525, Test loss: 1.566, Test accuracy: 89.60
Round  52, Train loss: 1.474, Test loss: 1.566, Test accuracy: 89.67
Round  53, Train loss: 1.578, Test loss: 1.566, Test accuracy: 89.68
Round  54, Train loss: 1.472, Test loss: 1.566, Test accuracy: 89.60
Round  55, Train loss: 1.580, Test loss: 1.566, Test accuracy: 89.65
Round  56, Train loss: 1.469, Test loss: 1.566, Test accuracy: 89.58
Round  57, Train loss: 1.526, Test loss: 1.565, Test accuracy: 89.68
Round  58, Train loss: 1.524, Test loss: 1.565, Test accuracy: 89.62
Round  59, Train loss: 1.578, Test loss: 1.565, Test accuracy: 89.62
Round  60, Train loss: 1.523, Test loss: 1.565, Test accuracy: 89.57
Round  61, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.55
Round  62, Train loss: 1.578, Test loss: 1.566, Test accuracy: 89.57
Round  63, Train loss: 1.474, Test loss: 1.565, Test accuracy: 89.57
Round  64, Train loss: 1.580, Test loss: 1.565, Test accuracy: 89.72
Round  65, Train loss: 1.521, Test loss: 1.565, Test accuracy: 89.68
Round  66, Train loss: 1.576, Test loss: 1.565, Test accuracy: 89.62
Round  67, Train loss: 1.521, Test loss: 1.565, Test accuracy: 89.68
Round  68, Train loss: 1.577, Test loss: 1.565, Test accuracy: 89.70
Round  69, Train loss: 1.578, Test loss: 1.565, Test accuracy: 89.67
Round  70, Train loss: 1.577, Test loss: 1.565, Test accuracy: 89.73
Round  71, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.68
Round  72, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.68
Round  73, Train loss: 1.523, Test loss: 1.565, Test accuracy: 89.62
Round  74, Train loss: 1.469, Test loss: 1.565, Test accuracy: 89.58
Round  75, Train loss: 1.630, Test loss: 1.565, Test accuracy: 89.63
Round  76, Train loss: 1.577, Test loss: 1.565, Test accuracy: 89.63
Round  77, Train loss: 1.521, Test loss: 1.566, Test accuracy: 89.60
Round  78, Train loss: 1.579, Test loss: 1.566, Test accuracy: 89.57
Round  79, Train loss: 1.469, Test loss: 1.565, Test accuracy: 89.53
Round  80, Train loss: 1.520, Test loss: 1.565, Test accuracy: 89.55
Round  81, Train loss: 1.575, Test loss: 1.566, Test accuracy: 89.53
Round  82, Train loss: 1.525, Test loss: 1.565, Test accuracy: 89.60
Round  83, Train loss: 1.522, Test loss: 1.565, Test accuracy: 89.60
Round  84, Train loss: 1.467, Test loss: 1.565, Test accuracy: 89.58
Round  85, Train loss: 1.575, Test loss: 1.564, Test accuracy: 89.63
Round  86, Train loss: 1.578, Test loss: 1.565, Test accuracy: 89.65
Round  87, Train loss: 1.467, Test loss: 1.565, Test accuracy: 89.58
Round  88, Train loss: 1.574, Test loss: 1.564, Test accuracy: 89.58
Round  89, Train loss: 1.524, Test loss: 1.564, Test accuracy: 89.62
Round  90, Train loss: 1.631, Test loss: 1.565, Test accuracy: 89.57
Round  91, Train loss: 1.577, Test loss: 1.565, Test accuracy: 89.58
Round  92, Train loss: 1.521, Test loss: 1.565, Test accuracy: 89.60
Round  93, Train loss: 1.522, Test loss: 1.564, Test accuracy: 89.58
Round  94, Train loss: 1.576, Test loss: 1.565, Test accuracy: 89.62
Round  95, Train loss: 1.469, Test loss: 1.564, Test accuracy: 89.60
Round  96, Train loss: 1.522, Test loss: 1.564, Test accuracy: 89.60
Round  97, Train loss: 1.468, Test loss: 1.564, Test accuracy: 89.67
Round  98, Train loss: 1.524, Test loss: 1.564, Test accuracy: 89.63/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.470, Test loss: 1.564, Test accuracy: 89.60
Final Round, Train loss: 1.524, Test loss: 1.550, Test accuracy: 91.17
Average accuracy final 10 rounds: 89.60499999999999 

695.271121263504
[0.6417577266693115, 1.283515453338623, 1.7872235774993896, 2.2909317016601562, 2.818997383117676, 3.3470630645751953, 3.859647035598755, 4.3722310066223145, 4.879838228225708, 5.387445449829102, 5.885281085968018, 6.383116722106934, 6.910301923751831, 7.4374871253967285, 8.015037775039673, 8.592588424682617, 9.124562740325928, 9.656537055969238, 10.210100889205933, 10.763664722442627, 11.365704536437988, 11.96774435043335, 12.538821458816528, 13.109898567199707, 13.633384704589844, 14.15687084197998, 14.670456886291504, 15.184042930603027, 15.732980966567993, 16.28191900253296, 16.819994688034058, 17.358070373535156, 17.893492221832275, 18.428914070129395, 18.93434429168701, 19.43977451324463, 19.968190908432007, 20.496607303619385, 21.059142589569092, 21.6216778755188, 22.186890125274658, 22.752102375030518, 23.316824913024902, 23.881547451019287, 24.440348863601685, 24.999150276184082, 25.567020654678345, 26.134891033172607, 26.701910257339478, 27.268929481506348, 27.813803672790527, 28.358677864074707, 28.85647988319397, 29.354281902313232, 29.890487670898438, 30.426693439483643, 30.96596097946167, 31.505228519439697, 32.01829671859741, 32.53136491775513, 33.03111529350281, 33.53086566925049, 34.08462142944336, 34.63837718963623, 35.24480748176575, 35.851237773895264, 36.38850283622742, 36.92576789855957, 37.48677110671997, 38.04777431488037, 38.639761209487915, 39.23174810409546, 39.79288721084595, 40.354026317596436, 40.89203095436096, 41.43003559112549, 41.94233703613281, 42.45463848114014, 42.94699287414551, 43.43934726715088, 43.979936838150024, 44.52052640914917, 45.04214525222778, 45.5637640953064, 46.085166454315186, 46.606568813323975, 47.112534523010254, 47.61850023269653, 48.16321921348572, 48.7079381942749, 49.247639179229736, 49.78734016418457, 50.376086950302124, 50.96483373641968, 51.534483432769775, 52.10413312911987, 52.66879916191101, 53.23346519470215, 53.79279899597168, 54.35213279724121, 54.87197017669678, 55.391807556152344, 55.9384229183197, 56.48503828048706, 56.99213480949402, 57.49923133850098, 57.986780405044556, 58.474329471588135, 58.98840093612671, 59.50247240066528, 60.048760414123535, 60.59504842758179, 61.136205434799194, 61.6773624420166, 62.24110007286072, 62.804837703704834, 63.396849632263184, 63.98886156082153, 64.57766366004944, 65.16646575927734, 65.75684142112732, 66.3472170829773, 66.85707592964172, 67.36693477630615, 67.87138962745667, 68.37584447860718, 68.90727066993713, 69.43869686126709, 69.96260046958923, 70.48650407791138, 70.98622250556946, 71.48594093322754, 71.98401880264282, 72.4820966720581, 73.047034740448, 73.61197280883789, 74.20217895507812, 74.79238510131836, 75.34296083450317, 75.89353656768799, 76.44265532493591, 76.99177408218384, 77.59080696105957, 78.1898398399353, 78.79854011535645, 79.40724039077759, 79.9441089630127, 80.4809775352478, 80.96422100067139, 81.44746446609497, 81.95958280563354, 82.47170114517212, 83.01828336715698, 83.56486558914185, 84.09554481506348, 84.62622404098511, 85.13142800331116, 85.6366319656372, 86.16751146316528, 86.69839096069336, 87.2962954044342, 87.89419984817505, 88.454416513443, 89.01463317871094, 89.57294702529907, 90.1312608718872, 90.6868884563446, 91.242516040802, 91.7648675441742, 92.28721904754639, 92.83760976791382, 93.38800048828125, 93.88869261741638, 94.38938474655151, 94.89567041397095, 95.40195608139038, 95.93502736091614, 96.4680986404419, 96.99633240699768, 97.52456617355347, 98.04109263420105, 98.55761909484863, 99.07675647735596, 99.59589385986328, 100.15690302848816, 100.71791219711304, 101.28106355667114, 101.84421491622925, 102.43554711341858, 103.02687931060791, 103.60739922523499, 104.18791913986206, 104.73708891868591, 105.28625869750977, 105.86434817314148, 106.4424376487732, 106.98765468597412, 107.53287172317505, 108.05380511283875, 108.57473850250244, 109.523108959198, 110.47147941589355]
[27.15, 27.15, 39.53333333333333, 39.53333333333333, 48.31666666666667, 48.31666666666667, 64.9, 64.9, 73.31666666666666, 73.31666666666666, 74.86666666666666, 74.86666666666666, 80.33333333333333, 80.33333333333333, 80.73333333333333, 80.73333333333333, 84.03333333333333, 84.03333333333333, 82.96666666666667, 82.96666666666667, 85.25, 85.25, 86.66666666666667, 86.66666666666667, 87.11666666666666, 87.11666666666666, 87.63333333333334, 87.63333333333334, 87.65, 87.65, 87.75, 87.75, 87.68333333333334, 87.68333333333334, 87.75, 87.75, 87.75, 87.75, 87.6, 87.6, 87.8, 87.8, 87.75, 87.75, 87.78333333333333, 87.78333333333333, 87.66666666666667, 87.66666666666667, 87.73333333333333, 87.73333333333333, 87.6, 87.6, 87.58333333333333, 87.58333333333333, 87.96666666666667, 87.96666666666667, 88.03333333333333, 88.03333333333333, 88.06666666666666, 88.06666666666666, 89.43333333333334, 89.43333333333334, 89.3, 89.3, 89.31666666666666, 89.31666666666666, 89.51666666666667, 89.51666666666667, 89.5, 89.5, 89.58333333333333, 89.58333333333333, 89.68333333333334, 89.68333333333334, 89.61666666666666, 89.61666666666666, 89.68333333333334, 89.68333333333334, 89.6, 89.6, 89.5, 89.5, 89.53333333333333, 89.53333333333333, 89.56666666666666, 89.56666666666666, 89.6, 89.6, 89.66666666666667, 89.66666666666667, 89.56666666666666, 89.56666666666666, 89.58333333333333, 89.58333333333333, 89.66666666666667, 89.66666666666667, 89.61666666666666, 89.61666666666666, 89.51666666666667, 89.51666666666667, 89.56666666666666, 89.56666666666666, 89.6, 89.6, 89.66666666666667, 89.66666666666667, 89.68333333333334, 89.68333333333334, 89.6, 89.6, 89.65, 89.65, 89.58333333333333, 89.58333333333333, 89.68333333333334, 89.68333333333334, 89.61666666666666, 89.61666666666666, 89.61666666666666, 89.61666666666666, 89.56666666666666, 89.56666666666666, 89.55, 89.55, 89.56666666666666, 89.56666666666666, 89.56666666666666, 89.56666666666666, 89.71666666666667, 89.71666666666667, 89.68333333333334, 89.68333333333334, 89.61666666666666, 89.61666666666666, 89.68333333333334, 89.68333333333334, 89.7, 89.7, 89.66666666666667, 89.66666666666667, 89.73333333333333, 89.73333333333333, 89.68333333333334, 89.68333333333334, 89.68333333333334, 89.68333333333334, 89.61666666666666, 89.61666666666666, 89.58333333333333, 89.58333333333333, 89.63333333333334, 89.63333333333334, 89.63333333333334, 89.63333333333334, 89.6, 89.6, 89.56666666666666, 89.56666666666666, 89.53333333333333, 89.53333333333333, 89.55, 89.55, 89.53333333333333, 89.53333333333333, 89.6, 89.6, 89.6, 89.6, 89.58333333333333, 89.58333333333333, 89.63333333333334, 89.63333333333334, 89.65, 89.65, 89.58333333333333, 89.58333333333333, 89.58333333333333, 89.58333333333333, 89.61666666666666, 89.61666666666666, 89.56666666666666, 89.56666666666666, 89.58333333333333, 89.58333333333333, 89.6, 89.6, 89.58333333333333, 89.58333333333333, 89.61666666666666, 89.61666666666666, 89.6, 89.6, 89.6, 89.6, 89.66666666666667, 89.66666666666667, 89.63333333333334, 89.63333333333334, 89.6, 89.6, 91.16666666666667, 91.16666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.285, Test loss: 2.295, Test accuracy: 19.88
Round   1, Train loss: 2.213, Test loss: 2.251, Test accuracy: 28.57
Round   2, Train loss: 2.034, Test loss: 2.183, Test accuracy: 38.82
Round   3, Train loss: 1.860, Test loss: 2.056, Test accuracy: 53.83
Round   4, Train loss: 1.621, Test loss: 1.987, Test accuracy: 57.75
Round   5, Train loss: 1.591, Test loss: 1.883, Test accuracy: 66.70
Round   6, Train loss: 1.569, Test loss: 1.856, Test accuracy: 67.22
Round   7, Train loss: 1.559, Test loss: 1.778, Test accuracy: 74.32
Round   8, Train loss: 1.606, Test loss: 1.656, Test accuracy: 85.65
Round   9, Train loss: 1.570, Test loss: 1.627, Test accuracy: 88.23
Round  10, Train loss: 1.533, Test loss: 1.611, Test accuracy: 89.60
Round  11, Train loss: 1.492, Test loss: 1.597, Test accuracy: 90.52
Round  12, Train loss: 1.494, Test loss: 1.569, Test accuracy: 92.05
Round  13, Train loss: 1.537, Test loss: 1.562, Test accuracy: 92.35
Round  14, Train loss: 1.475, Test loss: 1.560, Test accuracy: 92.18
Round  15, Train loss: 1.582, Test loss: 1.559, Test accuracy: 92.18
Round  16, Train loss: 1.530, Test loss: 1.556, Test accuracy: 92.27
Round  17, Train loss: 1.527, Test loss: 1.554, Test accuracy: 92.27
Round  18, Train loss: 1.477, Test loss: 1.553, Test accuracy: 92.22
Round  19, Train loss: 1.474, Test loss: 1.549, Test accuracy: 92.25
Round  20, Train loss: 1.528, Test loss: 1.545, Test accuracy: 92.70
Round  21, Train loss: 1.523, Test loss: 1.545, Test accuracy: 92.63
Round  22, Train loss: 1.525, Test loss: 1.543, Test accuracy: 92.65
Round  23, Train loss: 1.519, Test loss: 1.542, Test accuracy: 92.67
Round  24, Train loss: 1.470, Test loss: 1.541, Test accuracy: 92.67
Round  25, Train loss: 1.519, Test loss: 1.541, Test accuracy: 92.67
Round  26, Train loss: 1.474, Test loss: 1.533, Test accuracy: 93.77
Round  27, Train loss: 1.524, Test loss: 1.533, Test accuracy: 93.73
Round  28, Train loss: 1.468, Test loss: 1.533, Test accuracy: 93.80
Round  29, Train loss: 1.471, Test loss: 1.528, Test accuracy: 94.03
Round  30, Train loss: 1.469, Test loss: 1.526, Test accuracy: 94.20
Round  31, Train loss: 1.469, Test loss: 1.526, Test accuracy: 94.17
Round  32, Train loss: 1.521, Test loss: 1.526, Test accuracy: 94.17
Round  33, Train loss: 1.467, Test loss: 1.526, Test accuracy: 94.13
Round  34, Train loss: 1.518, Test loss: 1.525, Test accuracy: 94.23
Round  35, Train loss: 1.467, Test loss: 1.525, Test accuracy: 94.18
Round  36, Train loss: 1.517, Test loss: 1.524, Test accuracy: 94.18
Round  37, Train loss: 1.465, Test loss: 1.524, Test accuracy: 94.27
Round  38, Train loss: 1.466, Test loss: 1.524, Test accuracy: 94.25
Round  39, Train loss: 1.516, Test loss: 1.524, Test accuracy: 94.15
Round  40, Train loss: 1.518, Test loss: 1.524, Test accuracy: 94.20
Round  41, Train loss: 1.467, Test loss: 1.524, Test accuracy: 94.12
Round  42, Train loss: 1.464, Test loss: 1.523, Test accuracy: 94.25
Round  43, Train loss: 1.468, Test loss: 1.523, Test accuracy: 94.25
Round  44, Train loss: 1.465, Test loss: 1.523, Test accuracy: 94.22
Round  45, Train loss: 1.465, Test loss: 1.523, Test accuracy: 94.22
Round  46, Train loss: 1.465, Test loss: 1.523, Test accuracy: 94.20
Round  47, Train loss: 1.499, Test loss: 1.523, Test accuracy: 94.25
Round  48, Train loss: 1.466, Test loss: 1.523, Test accuracy: 94.25
Round  49, Train loss: 1.471, Test loss: 1.521, Test accuracy: 94.33
Round  50, Train loss: 1.464, Test loss: 1.521, Test accuracy: 94.40
Round  51, Train loss: 1.469, Test loss: 1.517, Test accuracy: 95.12
Round  52, Train loss: 1.467, Test loss: 1.517, Test accuracy: 95.08
Round  53, Train loss: 1.468, Test loss: 1.517, Test accuracy: 95.12
Round  54, Train loss: 1.466, Test loss: 1.517, Test accuracy: 95.02
Round  55, Train loss: 1.465, Test loss: 1.513, Test accuracy: 95.47
Round  56, Train loss: 1.463, Test loss: 1.513, Test accuracy: 95.47
Round  57, Train loss: 1.466, Test loss: 1.513, Test accuracy: 95.52
Round  58, Train loss: 1.467, Test loss: 1.510, Test accuracy: 95.72
Round  59, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.68
Round  60, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.65
Round  61, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.65
Round  62, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.65
Round  63, Train loss: 1.463, Test loss: 1.510, Test accuracy: 95.65
Round  64, Train loss: 1.463, Test loss: 1.509, Test accuracy: 95.70
Round  65, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.73
Round  66, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.75
Round  67, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.78
Round  68, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.78
Round  69, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.75
Round  70, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.78
Round  71, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.83
Round  72, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.83
Round  73, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.82
Round  74, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.85
Round  75, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.85
Round  76, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.85
Round  77, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.85
Round  78, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.85
Round  79, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.87
Round  80, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.83
Round  81, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.82
Round  82, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.83
Round  83, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.78
Round  84, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.85
Round  85, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.85
Round  86, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.83
Round  87, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.85
Round  88, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.82
Round  89, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.88
Round  90, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.92
Round  91, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.92
Round  92, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.93
Round  93, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.92
Round  94, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.88
Round  95, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.90
Round  96, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.90
Round  97, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.87
Round  98, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.87
Round  99, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.90/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.465, Test loss: 1.506, Test accuracy: 95.87
Average accuracy final 10 rounds: 95.9 

690.0355470180511
[0.6784107685089111, 1.3568215370178223, 1.9437308311462402, 2.530640125274658, 3.073089122772217, 3.6155381202697754, 4.180110692977905, 4.744683265686035, 5.335163354873657, 5.925643444061279, 6.462295293807983, 6.9989471435546875, 7.524068832397461, 8.049190521240234, 8.557600736618042, 9.06601095199585, 9.600108861923218, 10.134206771850586, 10.652596235275269, 11.170985698699951, 11.687337875366211, 12.20369005203247, 12.69796109199524, 13.192232131958008, 13.72443437576294, 14.256636619567871, 14.8280770778656, 15.39951753616333, 15.93772268295288, 16.47592782974243, 17.03432273864746, 17.59271764755249, 18.179386854171753, 18.766056060791016, 19.32475996017456, 19.883463859558105, 20.417785167694092, 20.952106475830078, 21.446112155914307, 21.940117835998535, 22.466015338897705, 22.991912841796875, 23.53066921234131, 24.069425582885742, 24.60880184173584, 25.148178100585938, 25.63904905319214, 26.12992000579834, 26.644777059555054, 27.159634113311768, 27.72888970375061, 28.298145294189453, 28.87015461921692, 29.442163944244385, 29.96572208404541, 30.489280223846436, 31.056572198867798, 31.62386417388916, 32.222086906433105, 32.82030963897705, 33.384599924087524, 33.948890209198, 34.43716025352478, 34.92543029785156, 35.42289853096008, 35.9203667640686, 36.438392639160156, 36.95641851425171, 37.48967003822327, 38.022921562194824, 38.56723928451538, 39.11155700683594, 39.64391875267029, 40.17628049850464, 40.75154638290405, 41.32681226730347, 41.904419898986816, 42.482027530670166, 43.05898308753967, 43.63593864440918, 44.202162742614746, 44.76838684082031, 45.33696794509888, 45.90554904937744, 46.442105770111084, 46.97866249084473, 47.47974109649658, 47.98081970214844, 48.5199179649353, 49.05901622772217, 49.58668637275696, 50.11435651779175, 50.643566846847534, 51.17277717590332, 51.68508791923523, 52.19739866256714, 52.71459364891052, 53.231788635253906, 53.8112690448761, 54.39074945449829, 54.91263508796692, 55.43452072143555, 55.99487042427063, 56.55522012710571, 57.11132574081421, 57.667431354522705, 58.181264877319336, 58.69509840011597, 59.24930930137634, 59.80352020263672, 60.302001953125, 60.80048370361328, 61.30291795730591, 61.805352210998535, 62.334792137145996, 62.86423206329346, 63.35463047027588, 63.8450288772583, 64.3329689502716, 64.82090902328491, 65.32015442848206, 65.8193998336792, 66.38812518119812, 66.95685052871704, 67.50006985664368, 68.04328918457031, 68.58452367782593, 69.12575817108154, 69.68580770492554, 70.24585723876953, 70.82669496536255, 71.40753269195557, 71.95864486694336, 72.50975704193115, 72.99584078788757, 73.481924533844, 73.95438146591187, 74.42683839797974, 74.95927906036377, 75.4917197227478, 76.01128673553467, 76.53085374832153, 77.02218794822693, 77.51352214813232, 77.9898157119751, 78.46610927581787, 79.03213810920715, 79.59816694259644, 80.17732214927673, 80.75647735595703, 81.30259108543396, 81.84870481491089, 82.41205954551697, 82.97541427612305, 83.52814388275146, 84.08087348937988, 84.61619853973389, 85.15152359008789, 85.68961215019226, 86.22770071029663, 86.71638369560242, 87.2050666809082, 87.69431519508362, 88.18356370925903, 88.69289183616638, 89.20221996307373, 89.7213180065155, 90.24041604995728, 90.74983716011047, 91.25925827026367, 91.83343958854675, 92.40762090682983, 92.98315072059631, 93.5586805343628, 94.11642098426819, 94.67416143417358, 95.22982382774353, 95.78548622131348, 96.35568118095398, 96.92587614059448, 97.51058006286621, 98.09528398513794, 98.63725185394287, 99.1792197227478, 99.66461634635925, 100.1500129699707, 100.66084337234497, 101.17167377471924, 101.68729543685913, 102.20291709899902, 102.70978808403015, 103.21665906906128, 103.69742369651794, 104.17818832397461, 104.67872929573059, 105.17927026748657, 105.71560263633728, 106.25193500518799, 106.8058717250824, 107.3598084449768, 108.41943049430847, 109.47905254364014]
[19.883333333333333, 19.883333333333333, 28.566666666666666, 28.566666666666666, 38.81666666666667, 38.81666666666667, 53.833333333333336, 53.833333333333336, 57.75, 57.75, 66.7, 66.7, 67.21666666666667, 67.21666666666667, 74.31666666666666, 74.31666666666666, 85.65, 85.65, 88.23333333333333, 88.23333333333333, 89.6, 89.6, 90.51666666666667, 90.51666666666667, 92.05, 92.05, 92.35, 92.35, 92.18333333333334, 92.18333333333334, 92.18333333333334, 92.18333333333334, 92.26666666666667, 92.26666666666667, 92.26666666666667, 92.26666666666667, 92.21666666666667, 92.21666666666667, 92.25, 92.25, 92.7, 92.7, 92.63333333333334, 92.63333333333334, 92.65, 92.65, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 93.76666666666667, 93.76666666666667, 93.73333333333333, 93.73333333333333, 93.8, 93.8, 94.03333333333333, 94.03333333333333, 94.2, 94.2, 94.16666666666667, 94.16666666666667, 94.16666666666667, 94.16666666666667, 94.13333333333334, 94.13333333333334, 94.23333333333333, 94.23333333333333, 94.18333333333334, 94.18333333333334, 94.18333333333334, 94.18333333333334, 94.26666666666667, 94.26666666666667, 94.25, 94.25, 94.15, 94.15, 94.2, 94.2, 94.11666666666666, 94.11666666666666, 94.25, 94.25, 94.25, 94.25, 94.21666666666667, 94.21666666666667, 94.21666666666667, 94.21666666666667, 94.2, 94.2, 94.25, 94.25, 94.25, 94.25, 94.33333333333333, 94.33333333333333, 94.4, 94.4, 95.11666666666666, 95.11666666666666, 95.08333333333333, 95.08333333333333, 95.11666666666666, 95.11666666666666, 95.01666666666667, 95.01666666666667, 95.46666666666667, 95.46666666666667, 95.46666666666667, 95.46666666666667, 95.51666666666667, 95.51666666666667, 95.71666666666667, 95.71666666666667, 95.68333333333334, 95.68333333333334, 95.65, 95.65, 95.65, 95.65, 95.65, 95.65, 95.65, 95.65, 95.7, 95.7, 95.73333333333333, 95.73333333333333, 95.75, 95.75, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.75, 95.75, 95.78333333333333, 95.78333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.83333333333333, 95.81666666666666, 95.81666666666666, 95.85, 95.85, 95.85, 95.85, 95.85, 95.85, 95.85, 95.85, 95.85, 95.85, 95.86666666666666, 95.86666666666666, 95.83333333333333, 95.83333333333333, 95.81666666666666, 95.81666666666666, 95.83333333333333, 95.83333333333333, 95.78333333333333, 95.78333333333333, 95.85, 95.85, 95.85, 95.85, 95.83333333333333, 95.83333333333333, 95.85, 95.85, 95.81666666666666, 95.81666666666666, 95.88333333333334, 95.88333333333334, 95.91666666666667, 95.91666666666667, 95.91666666666667, 95.91666666666667, 95.93333333333334, 95.93333333333334, 95.91666666666667, 95.91666666666667, 95.88333333333334, 95.88333333333334, 95.9, 95.9, 95.9, 95.9, 95.86666666666666, 95.86666666666666, 95.86666666666666, 95.86666666666666, 95.9, 95.9, 95.86666666666666, 95.86666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.521, Test loss: 2.240, Test accuracy: 47.97
Round   1, Train loss: 1.350, Test loss: 2.126, Test accuracy: 54.37
Round   2, Train loss: 1.295, Test loss: 1.986, Test accuracy: 80.00
Round   3, Train loss: 1.176, Test loss: 1.901, Test accuracy: 77.85
Round   4, Train loss: 1.134, Test loss: 1.795, Test accuracy: 86.02
Round   5, Train loss: 1.119, Test loss: 1.726, Test accuracy: 88.55
Round   6, Train loss: 1.130, Test loss: 1.698, Test accuracy: 90.55
Round   7, Train loss: 1.151, Test loss: 1.677, Test accuracy: 90.58
Round   8, Train loss: 1.157, Test loss: 1.655, Test accuracy: 93.10
Round   9, Train loss: 1.155, Test loss: 1.649, Test accuracy: 92.37
Round  10, Train loss: 1.108, Test loss: 1.626, Test accuracy: 93.25
Round  11, Train loss: 1.143, Test loss: 1.619, Test accuracy: 93.18
Round  12, Train loss: 1.107, Test loss: 1.611, Test accuracy: 93.40
Round  13, Train loss: 1.147, Test loss: 1.601, Test accuracy: 93.58
Round  14, Train loss: 1.108, Test loss: 1.594, Test accuracy: 93.62
Round  15, Train loss: 1.106, Test loss: 1.587, Test accuracy: 93.55
Round  16, Train loss: 1.103, Test loss: 1.584, Test accuracy: 93.28
Round  17, Train loss: 1.105, Test loss: 1.578, Test accuracy: 93.50
Round  18, Train loss: 1.101, Test loss: 1.578, Test accuracy: 93.32
Round  19, Train loss: 1.143, Test loss: 1.575, Test accuracy: 93.37
Round  20, Train loss: 1.143, Test loss: 1.574, Test accuracy: 93.40
Round  21, Train loss: 1.104, Test loss: 1.574, Test accuracy: 93.15
Round  22, Train loss: 1.103, Test loss: 1.572, Test accuracy: 93.13
Round  23, Train loss: 1.101, Test loss: 1.574, Test accuracy: 92.95
Round  24, Train loss: 1.141, Test loss: 1.571, Test accuracy: 93.08
Round  25, Train loss: 1.144, Test loss: 1.572, Test accuracy: 93.05
Round  26, Train loss: 1.103, Test loss: 1.570, Test accuracy: 93.00
Round  27, Train loss: 1.100, Test loss: 1.568, Test accuracy: 93.15
Round  28, Train loss: 1.140, Test loss: 1.567, Test accuracy: 93.13
Round  29, Train loss: 1.103, Test loss: 1.566, Test accuracy: 93.03
Round  30, Train loss: 1.099, Test loss: 1.566, Test accuracy: 92.97
Round  31, Train loss: 1.104, Test loss: 1.566, Test accuracy: 93.07
Round  32, Train loss: 1.102, Test loss: 1.567, Test accuracy: 92.95
Round  33, Train loss: 1.102, Test loss: 1.564, Test accuracy: 93.05
Round  34, Train loss: 1.141, Test loss: 1.565, Test accuracy: 92.87
Round  35, Train loss: 1.140, Test loss: 1.566, Test accuracy: 92.80
Round  36, Train loss: 1.141, Test loss: 1.566, Test accuracy: 92.78
Round  37, Train loss: 1.103, Test loss: 1.568, Test accuracy: 92.37
Round  38, Train loss: 1.103, Test loss: 1.566, Test accuracy: 92.55
Round  39, Train loss: 1.143, Test loss: 1.567, Test accuracy: 92.37
Round  40, Train loss: 1.100, Test loss: 1.566, Test accuracy: 92.48
Round  41, Train loss: 1.099, Test loss: 1.566, Test accuracy: 92.32
Round  42, Train loss: 1.099, Test loss: 1.568, Test accuracy: 92.35
Round  43, Train loss: 1.142, Test loss: 1.568, Test accuracy: 92.27
Round  44, Train loss: 1.101, Test loss: 1.568, Test accuracy: 92.25
Round  45, Train loss: 1.141, Test loss: 1.568, Test accuracy: 92.25
Round  46, Train loss: 1.143, Test loss: 1.567, Test accuracy: 92.22
Round  47, Train loss: 1.101, Test loss: 1.569, Test accuracy: 92.13
Round  48, Train loss: 1.142, Test loss: 1.568, Test accuracy: 92.20
Round  49, Train loss: 1.101, Test loss: 1.568, Test accuracy: 92.08
Round  50, Train loss: 1.101, Test loss: 1.569, Test accuracy: 91.97
Round  51, Train loss: 1.141, Test loss: 1.568, Test accuracy: 91.95
Round  52, Train loss: 1.099, Test loss: 1.570, Test accuracy: 91.85
Round  53, Train loss: 1.100, Test loss: 1.568, Test accuracy: 91.97
Round  54, Train loss: 1.100, Test loss: 1.567, Test accuracy: 92.10
Round  55, Train loss: 1.142, Test loss: 1.566, Test accuracy: 92.00
Round  56, Train loss: 1.101, Test loss: 1.565, Test accuracy: 91.97
Round  57, Train loss: 1.101, Test loss: 1.568, Test accuracy: 91.85
Round  58, Train loss: 1.100, Test loss: 1.571, Test accuracy: 91.43
Round  59, Train loss: 1.140, Test loss: 1.571, Test accuracy: 91.60
Round  60, Train loss: 1.140, Test loss: 1.570, Test accuracy: 91.77
Round  61, Train loss: 1.100, Test loss: 1.572, Test accuracy: 91.32
Round  62, Train loss: 1.141, Test loss: 1.570, Test accuracy: 91.67
Round  63, Train loss: 1.099, Test loss: 1.571, Test accuracy: 91.27
Round  64, Train loss: 1.141, Test loss: 1.571, Test accuracy: 91.25
Round  65, Train loss: 1.100, Test loss: 1.570, Test accuracy: 91.38
Round  66, Train loss: 1.101, Test loss: 1.571, Test accuracy: 91.28
Round  67, Train loss: 1.100, Test loss: 1.573, Test accuracy: 91.02
Round  68, Train loss: 1.100, Test loss: 1.571, Test accuracy: 91.22
Round  69, Train loss: 1.099, Test loss: 1.571, Test accuracy: 91.28
Round  70, Train loss: 1.099, Test loss: 1.573, Test accuracy: 91.03
Round  71, Train loss: 1.099, Test loss: 1.572, Test accuracy: 91.17
Round  72, Train loss: 1.141, Test loss: 1.573, Test accuracy: 91.12
Round  73, Train loss: 1.099, Test loss: 1.574, Test accuracy: 90.82
Round  74, Train loss: 1.099, Test loss: 1.573, Test accuracy: 90.90
Round  75, Train loss: 1.140, Test loss: 1.574, Test accuracy: 90.73
Round  76, Train loss: 1.101, Test loss: 1.573, Test accuracy: 90.82
Round  77, Train loss: 1.140, Test loss: 1.574, Test accuracy: 90.77
Round  78, Train loss: 1.100, Test loss: 1.574, Test accuracy: 90.88
Round  79, Train loss: 1.099, Test loss: 1.574, Test accuracy: 90.92
Round  80, Train loss: 1.101, Test loss: 1.574, Test accuracy: 90.90
Round  81, Train loss: 1.098, Test loss: 1.573, Test accuracy: 91.03
Round  82, Train loss: 1.100, Test loss: 1.574, Test accuracy: 90.80
Round  83, Train loss: 1.099, Test loss: 1.574, Test accuracy: 90.75
Round  84, Train loss: 1.099, Test loss: 1.575, Test accuracy: 90.67
Round  85, Train loss: 1.100, Test loss: 1.575, Test accuracy: 90.52
Round  86, Train loss: 1.100, Test loss: 1.575, Test accuracy: 90.68
Round  87, Train loss: 1.100, Test loss: 1.577, Test accuracy: 90.45
Round  88, Train loss: 1.099, Test loss: 1.577, Test accuracy: 90.42
Round  89, Train loss: 1.099, Test loss: 1.577, Test accuracy: 90.28
Round  90, Train loss: 1.099, Test loss: 1.579, Test accuracy: 90.10
Round  91, Train loss: 1.100, Test loss: 1.578, Test accuracy: 89.93
Round  92, Train loss: 1.099, Test loss: 1.578, Test accuracy: 89.98
Round  93, Train loss: 1.098, Test loss: 1.580, Test accuracy: 89.92
Round  94, Train loss: 1.100, Test loss: 1.581, Test accuracy: 89.72
Round  95, Train loss: 1.141, Test loss: 1.581, Test accuracy: 89.67
Round  96, Train loss: 1.100, Test loss: 1.583, Test accuracy: 89.50
Round  97, Train loss: 1.099, Test loss: 1.580, Test accuracy: 89.72
Round  98, Train loss: 1.098, Test loss: 1.580, Test accuracy: 89.67
Round  99, Train loss: 1.140, Test loss: 1.582, Test accuracy: 89.58
Final Round, Train loss: 1.111, Test loss: 1.583, Test accuracy: 89.50
Average accuracy final 10 rounds: 89.77833333333334
787.9392611980438
[]
[47.96666666666667, 54.36666666666667, 80.0, 77.85, 86.01666666666667, 88.55, 90.55, 90.58333333333333, 93.1, 92.36666666666666, 93.25, 93.18333333333334, 93.4, 93.58333333333333, 93.61666666666666, 93.55, 93.28333333333333, 93.5, 93.31666666666666, 93.36666666666666, 93.4, 93.15, 93.13333333333334, 92.95, 93.08333333333333, 93.05, 93.0, 93.15, 93.13333333333334, 93.03333333333333, 92.96666666666667, 93.06666666666666, 92.95, 93.05, 92.86666666666666, 92.8, 92.78333333333333, 92.36666666666666, 92.55, 92.36666666666666, 92.48333333333333, 92.31666666666666, 92.35, 92.26666666666667, 92.25, 92.25, 92.21666666666667, 92.13333333333334, 92.2, 92.08333333333333, 91.96666666666667, 91.95, 91.85, 91.96666666666667, 92.1, 92.0, 91.96666666666667, 91.85, 91.43333333333334, 91.6, 91.76666666666667, 91.31666666666666, 91.66666666666667, 91.26666666666667, 91.25, 91.38333333333334, 91.28333333333333, 91.01666666666667, 91.21666666666667, 91.28333333333333, 91.03333333333333, 91.16666666666667, 91.11666666666666, 90.81666666666666, 90.9, 90.73333333333333, 90.81666666666666, 90.76666666666667, 90.88333333333334, 90.91666666666667, 90.9, 91.03333333333333, 90.8, 90.75, 90.66666666666667, 90.51666666666667, 90.68333333333334, 90.45, 90.41666666666667, 90.28333333333333, 90.1, 89.93333333333334, 89.98333333333333, 89.91666666666667, 89.71666666666667, 89.66666666666667, 89.5, 89.71666666666667, 89.66666666666667, 89.58333333333333, 89.5]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.282, Test loss: 2.284, Test accuracy: 22.40
Round   1, Train loss: 2.251, Test loss: 2.261, Test accuracy: 21.80
Round   2, Train loss: 2.160, Test loss: 2.218, Test accuracy: 26.20
Round   3, Train loss: 2.162, Test loss: 2.192, Test accuracy: 28.23
Round   4, Train loss: 1.977, Test loss: 2.160, Test accuracy: 30.33
Round   5, Train loss: 1.913, Test loss: 2.123, Test accuracy: 32.33
Round   6, Train loss: 2.068, Test loss: 2.169, Test accuracy: 24.25
Round   7, Train loss: 1.604, Test loss: 2.090, Test accuracy: 32.08
Round   8, Train loss: 1.754, Test loss: 2.037, Test accuracy: 41.67
Round   9, Train loss: 1.710, Test loss: 2.043, Test accuracy: 39.93
Round  10, Train loss: 1.550, Test loss: 2.021, Test accuracy: 41.92
Round  11, Train loss: 1.763, Test loss: 2.084, Test accuracy: 36.73
Round  12, Train loss: 1.205, Test loss: 2.017, Test accuracy: 47.40
Round  13, Train loss: 1.160, Test loss: 2.024, Test accuracy: 52.95
Round  14, Train loss: 0.585, Test loss: 1.933, Test accuracy: 58.83
Round  15, Train loss: 1.107, Test loss: 1.964, Test accuracy: 53.17
Round  16, Train loss: 0.621, Test loss: 1.863, Test accuracy: 66.60
Round  17, Train loss: 0.212, Test loss: 1.880, Test accuracy: 66.43
Round  18, Train loss: 0.988, Test loss: 2.010, Test accuracy: 56.52
Round  19, Train loss: 0.407, Test loss: 1.989, Test accuracy: 58.32
Round  20, Train loss: -0.129, Test loss: 1.909, Test accuracy: 63.92
Round  21, Train loss: -0.037, Test loss: 1.880, Test accuracy: 64.42
Round  22, Train loss: -0.038, Test loss: 1.883, Test accuracy: 63.25
Round  23, Train loss: -0.985, Test loss: 1.758, Test accuracy: 72.27
Round  24, Train loss: -0.259, Test loss: 1.816, Test accuracy: 71.42
Round  25, Train loss: -0.323, Test loss: 1.798, Test accuracy: 70.95
Round  26, Train loss: -1.641, Test loss: 1.739, Test accuracy: 75.65
Round  27, Train loss: -0.955, Test loss: 1.781, Test accuracy: 72.27
Round  28, Train loss: -0.399, Test loss: 1.802, Test accuracy: 70.30
Round  29, Train loss: -2.107, Test loss: 1.720, Test accuracy: 78.50
Round  30, Train loss: -1.253, Test loss: 1.683, Test accuracy: 79.93
Round  31, Train loss: -2.359, Test loss: 1.618, Test accuracy: 87.05
Round  32, Train loss: -2.408, Test loss: 1.602, Test accuracy: 87.25
Round  33, Train loss: -0.032, Test loss: 1.651, Test accuracy: 83.63
Round  34, Train loss: -0.115, Test loss: 1.674, Test accuracy: 80.52
Round  35, Train loss: -1.695, Test loss: 1.594, Test accuracy: 87.80
Round  36, Train loss: -0.940, Test loss: 1.578, Test accuracy: 89.42
Round  37, Train loss: -0.729, Test loss: 1.596, Test accuracy: 86.97
Round  38, Train loss: -0.189, Test loss: 1.631, Test accuracy: 83.77
Round  39, Train loss: -2.086, Test loss: 1.615, Test accuracy: 85.52
Round  40, Train loss: -0.379, Test loss: 1.613, Test accuracy: 85.92
Round  41, Train loss: 0.360, Test loss: 1.649, Test accuracy: 83.45
Round  42, Train loss: 0.108, Test loss: 1.556, Test accuracy: 92.32
Round  43, Train loss: -1.329, Test loss: 1.572, Test accuracy: 90.52
Round  44, Train loss: -0.160, Test loss: 1.594, Test accuracy: 88.50
Round  45, Train loss: -1.382, Test loss: 1.576, Test accuracy: 90.27
Round  46, Train loss: -0.612, Test loss: 1.603, Test accuracy: 87.22
Round  47, Train loss: -0.203, Test loss: 1.585, Test accuracy: 88.92
Round  48, Train loss: -0.552, Test loss: 1.598, Test accuracy: 87.58
Round  49, Train loss: -1.024, Test loss: 1.576, Test accuracy: 88.58
Round  50, Train loss: -0.412, Test loss: 1.603, Test accuracy: 85.88
Round  51, Train loss: -0.744, Test loss: 1.559, Test accuracy: 90.32
Round  52, Train loss: -0.919, Test loss: 1.556, Test accuracy: 90.57
Round  53, Train loss: -0.580, Test loss: 1.573, Test accuracy: 88.73
Round  54, Train loss: -1.611, Test loss: 1.560, Test accuracy: 90.08
Round  55, Train loss: -1.277, Test loss: 1.557, Test accuracy: 90.43
Round  56, Train loss: -0.323, Test loss: 1.550, Test accuracy: 91.37
Round  57, Train loss: -0.381, Test loss: 1.578, Test accuracy: 88.47
Round  58, Train loss: -0.638, Test loss: 1.577, Test accuracy: 88.67
Round  59, Train loss: -0.968, Test loss: 1.548, Test accuracy: 91.53
Round  60, Train loss: -0.737, Test loss: 1.572, Test accuracy: 88.92
Round  61, Train loss: -0.615, Test loss: 1.560, Test accuracy: 90.38
Round  62, Train loss: -0.589, Test loss: 1.556, Test accuracy: 90.70
Round  63, Train loss: -0.807, Test loss: 1.544, Test accuracy: 91.87
Round  64, Train loss: -0.136, Test loss: 1.545, Test accuracy: 91.80
Round  65, Train loss: -0.188, Test loss: 1.547, Test accuracy: 91.70
Round  66, Train loss: -0.779, Test loss: 1.548, Test accuracy: 91.60
Round  67, Train loss: -0.103, Test loss: 1.535, Test accuracy: 92.95
Round  68, Train loss: -0.212, Test loss: 1.543, Test accuracy: 92.00
Round  69, Train loss: -0.624, Test loss: 1.531, Test accuracy: 93.15
Round  70, Train loss: -0.089, Test loss: 1.532, Test accuracy: 93.03
Round  71, Train loss: -0.832, Test loss: 1.546, Test accuracy: 91.57
Round  72, Train loss: -0.455, Test loss: 1.549, Test accuracy: 91.43
Round  73, Train loss: -0.169, Test loss: 1.532, Test accuracy: 93.12
Round  74, Train loss: -0.486, Test loss: 1.517, Test accuracy: 94.58
Round  75, Train loss: -0.249, Test loss: 1.548, Test accuracy: 91.45
Round  76, Train loss: -0.393, Test loss: 1.545, Test accuracy: 91.80
Round  77, Train loss: -0.639, Test loss: 1.526, Test accuracy: 93.60
Round  78, Train loss: -0.262, Test loss: 1.529, Test accuracy: 93.27
Round  79, Train loss: -0.639, Test loss: 1.528, Test accuracy: 93.30
Round  80, Train loss: -0.392, Test loss: 1.528, Test accuracy: 93.35
Round  81, Train loss: -0.396, Test loss: 1.542, Test accuracy: 91.87
Round  82, Train loss: -0.587, Test loss: 1.526, Test accuracy: 93.52
Round  83, Train loss: -0.441, Test loss: 1.527, Test accuracy: 93.50
Round  84, Train loss: -0.589, Test loss: 1.526, Test accuracy: 93.65
Round  85, Train loss: -0.561, Test loss: 1.526, Test accuracy: 93.57
Round  86, Train loss: -0.532, Test loss: 1.528, Test accuracy: 93.33
Round  87, Train loss: -0.486, Test loss: 1.514, Test accuracy: 94.85
Round  88, Train loss: -0.072, Test loss: 1.529, Test accuracy: 93.37
Round  89, Train loss: -0.718, Test loss: 1.527, Test accuracy: 93.52
Round  90, Train loss: -0.394, Test loss: 1.527, Test accuracy: 93.65
Round  91, Train loss: -0.623, Test loss: 1.526, Test accuracy: 93.67
Round  92, Train loss: -0.259, Test loss: 1.527, Test accuracy: 93.55
Round  93, Train loss: -0.346, Test loss: 1.528, Test accuracy: 93.40
Round  94, Train loss: -0.164, Test loss: 1.527, Test accuracy: 93.53
Round  95, Train loss: -0.384, Test loss: 1.527, Test accuracy: 93.50
Round  96, Train loss: -0.511, Test loss: 1.527, Test accuracy: 93.50
Round  97, Train loss: -0.289, Test loss: 1.528, Test accuracy: 93.52
Round  98, Train loss: -0.540, Test loss: 1.528, Test accuracy: 93.40
Round  99, Train loss: -0.515, Test loss: 1.529, Test accuracy: 93.43
Final Round, Train loss: 1.543, Test loss: 1.530, Test accuracy: 94.23
Average accuracy final 10 rounds: 93.51500000000001
Average global accuracy final 10 rounds: 93.51500000000001
614.3448646068573
[]
[22.4, 21.8, 26.2, 28.233333333333334, 30.333333333333332, 32.333333333333336, 24.25, 32.083333333333336, 41.666666666666664, 39.93333333333333, 41.916666666666664, 36.733333333333334, 47.4, 52.95, 58.833333333333336, 53.166666666666664, 66.6, 66.43333333333334, 56.516666666666666, 58.31666666666667, 63.916666666666664, 64.41666666666667, 63.25, 72.26666666666667, 71.41666666666667, 70.95, 75.65, 72.26666666666667, 70.3, 78.5, 79.93333333333334, 87.05, 87.25, 83.63333333333334, 80.51666666666667, 87.8, 89.41666666666667, 86.96666666666667, 83.76666666666667, 85.51666666666667, 85.91666666666667, 83.45, 92.31666666666666, 90.51666666666667, 88.5, 90.26666666666667, 87.21666666666667, 88.91666666666667, 87.58333333333333, 88.58333333333333, 85.88333333333334, 90.31666666666666, 90.56666666666666, 88.73333333333333, 90.08333333333333, 90.43333333333334, 91.36666666666666, 88.46666666666667, 88.66666666666667, 91.53333333333333, 88.91666666666667, 90.38333333333334, 90.7, 91.86666666666666, 91.8, 91.7, 91.6, 92.95, 92.0, 93.15, 93.03333333333333, 91.56666666666666, 91.43333333333334, 93.11666666666666, 94.58333333333333, 91.45, 91.8, 93.6, 93.26666666666667, 93.3, 93.35, 91.86666666666666, 93.51666666666667, 93.5, 93.65, 93.56666666666666, 93.33333333333333, 94.85, 93.36666666666666, 93.51666666666667, 93.65, 93.66666666666667, 93.55, 93.4, 93.53333333333333, 93.5, 93.5, 93.51666666666667, 93.4, 93.43333333333334, 94.23333333333333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

prox
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.282, Test loss: 2.280, Test accuracy: 18.17
Round   0, Global train loss: 2.282, Global test loss: 2.294, Global test accuracy: 11.75
Round   1, Train loss: 2.234, Test loss: 2.218, Test accuracy: 34.90
Round   1, Global train loss: 2.234, Global test loss: 2.273, Global test accuracy: 25.20
Round   2, Train loss: 2.039, Test loss: 2.104, Test accuracy: 45.22
Round   2, Global train loss: 2.039, Global test loss: 2.195, Global test accuracy: 36.90
Round   3, Train loss: 1.884, Test loss: 1.976, Test accuracy: 55.37
Round   3, Global train loss: 1.884, Global test loss: 2.120, Global test accuracy: 35.82
Round   4, Train loss: 1.775, Test loss: 1.839, Test accuracy: 68.38
Round   4, Global train loss: 1.775, Global test loss: 2.039, Global test accuracy: 58.35
Round   5, Train loss: 1.739, Test loss: 1.775, Test accuracy: 73.08
Round   5, Global train loss: 1.739, Global test loss: 1.971, Global test accuracy: 60.60
Round   6, Train loss: 1.745, Test loss: 1.753, Test accuracy: 74.73
Round   6, Global train loss: 1.745, Global test loss: 1.946, Global test accuracy: 58.08
Round   7, Train loss: 1.738, Test loss: 1.734, Test accuracy: 76.02
Round   7, Global train loss: 1.738, Global test loss: 1.909, Global test accuracy: 61.60
Round   8, Train loss: 1.774, Test loss: 1.701, Test accuracy: 77.82
Round   8, Global train loss: 1.774, Global test loss: 1.863, Global test accuracy: 66.60
Round   9, Train loss: 1.566, Test loss: 1.698, Test accuracy: 77.72
Round   9, Global train loss: 1.566, Global test loss: 1.855, Global test accuracy: 63.82
Round  10, Train loss: 1.562, Test loss: 1.695, Test accuracy: 78.05
Round  10, Global train loss: 1.562, Global test loss: 1.855, Global test accuracy: 64.22
Round  11, Train loss: 1.708, Test loss: 1.698, Test accuracy: 77.37
Round  11, Global train loss: 1.708, Global test loss: 1.858, Global test accuracy: 61.95
Round  12, Train loss: 1.626, Test loss: 1.676, Test accuracy: 79.73
Round  12, Global train loss: 1.626, Global test loss: 1.808, Global test accuracy: 69.47
Round  13, Train loss: 1.752, Test loss: 1.675, Test accuracy: 79.75
Round  13, Global train loss: 1.752, Global test loss: 1.812, Global test accuracy: 66.75
Round  14, Train loss: 1.703, Test loss: 1.674, Test accuracy: 79.80
Round  14, Global train loss: 1.703, Global test loss: 1.834, Global test accuracy: 62.13
Round  15, Train loss: 1.699, Test loss: 1.674, Test accuracy: 79.83
Round  15, Global train loss: 1.699, Global test loss: 1.838, Global test accuracy: 62.78
Round  16, Train loss: 1.721, Test loss: 1.665, Test accuracy: 80.33
Round  16, Global train loss: 1.721, Global test loss: 1.813, Global test accuracy: 68.07
Round  17, Train loss: 1.713, Test loss: 1.664, Test accuracy: 80.45
Round  17, Global train loss: 1.713, Global test loss: 1.801, Global test accuracy: 68.07
Round  18, Train loss: 1.654, Test loss: 1.663, Test accuracy: 80.52
Round  18, Global train loss: 1.654, Global test loss: 1.845, Global test accuracy: 62.35
Round  19, Train loss: 1.765, Test loss: 1.662, Test accuracy: 80.58
Round  19, Global train loss: 1.765, Global test loss: 1.773, Global test accuracy: 71.07
Round  20, Train loss: 1.646, Test loss: 1.661, Test accuracy: 80.58
Round  20, Global train loss: 1.646, Global test loss: 1.781, Global test accuracy: 68.93
Round  21, Train loss: 1.701, Test loss: 1.661, Test accuracy: 80.58
Round  21, Global train loss: 1.701, Global test loss: 1.753, Global test accuracy: 73.90
Round  22, Train loss: 1.650, Test loss: 1.661, Test accuracy: 80.55
Round  22, Global train loss: 1.650, Global test loss: 1.789, Global test accuracy: 67.80
Round  23, Train loss: 1.743, Test loss: 1.661, Test accuracy: 80.57
Round  23, Global train loss: 1.743, Global test loss: 1.777, Global test accuracy: 69.32
Round  24, Train loss: 1.648, Test loss: 1.661, Test accuracy: 80.52
Round  24, Global train loss: 1.648, Global test loss: 1.794, Global test accuracy: 67.40
Round  25, Train loss: 1.599, Test loss: 1.660, Test accuracy: 80.53
Round  25, Global train loss: 1.599, Global test loss: 1.799, Global test accuracy: 66.48
Round  26, Train loss: 1.641, Test loss: 1.660, Test accuracy: 80.57
Round  26, Global train loss: 1.641, Global test loss: 1.776, Global test accuracy: 68.62
Round  27, Train loss: 1.693, Test loss: 1.660, Test accuracy: 80.57
Round  27, Global train loss: 1.693, Global test loss: 1.811, Global test accuracy: 65.88
Round  28, Train loss: 1.538, Test loss: 1.660, Test accuracy: 80.57
Round  28, Global train loss: 1.538, Global test loss: 1.760, Global test accuracy: 72.50
Round  29, Train loss: 1.645, Test loss: 1.659, Test accuracy: 80.58
Round  29, Global train loss: 1.645, Global test loss: 1.757, Global test accuracy: 73.55
Round  30, Train loss: 1.754, Test loss: 1.659, Test accuracy: 80.63
Round  30, Global train loss: 1.754, Global test loss: 1.743, Global test accuracy: 73.53
Round  31, Train loss: 1.588, Test loss: 1.658, Test accuracy: 80.63
Round  31, Global train loss: 1.588, Global test loss: 1.742, Global test accuracy: 74.48
Round  32, Train loss: 1.541, Test loss: 1.660, Test accuracy: 80.47
Round  32, Global train loss: 1.541, Global test loss: 1.737, Global test accuracy: 74.67
Round  33, Train loss: 1.694, Test loss: 1.659, Test accuracy: 80.43
Round  33, Global train loss: 1.694, Global test loss: 1.733, Global test accuracy: 74.73
Round  34, Train loss: 1.640, Test loss: 1.660, Test accuracy: 80.40
Round  34, Global train loss: 1.640, Global test loss: 1.738, Global test accuracy: 74.07
Round  35, Train loss: 1.538, Test loss: 1.659, Test accuracy: 80.47
Round  35, Global train loss: 1.538, Global test loss: 1.730, Global test accuracy: 75.02
Round  36, Train loss: 1.746, Test loss: 1.659, Test accuracy: 80.52
Round  36, Global train loss: 1.746, Global test loss: 1.724, Global test accuracy: 75.30
Round  37, Train loss: 1.635, Test loss: 1.659, Test accuracy: 80.55
Round  37, Global train loss: 1.635, Global test loss: 1.726, Global test accuracy: 74.82
Round  38, Train loss: 1.644, Test loss: 1.658, Test accuracy: 80.50
Round  38, Global train loss: 1.644, Global test loss: 1.738, Global test accuracy: 73.22
Round  39, Train loss: 1.643, Test loss: 1.658, Test accuracy: 80.68
Round  39, Global train loss: 1.643, Global test loss: 1.727, Global test accuracy: 74.88
Round  40, Train loss: 1.534, Test loss: 1.658, Test accuracy: 80.55
Round  40, Global train loss: 1.534, Global test loss: 1.726, Global test accuracy: 75.08
Round  41, Train loss: 1.688, Test loss: 1.658, Test accuracy: 80.48
Round  41, Global train loss: 1.688, Global test loss: 1.723, Global test accuracy: 75.53
Round  42, Train loss: 1.535, Test loss: 1.659, Test accuracy: 80.40
Round  42, Global train loss: 1.535, Global test loss: 1.726, Global test accuracy: 75.13
Round  43, Train loss: 1.582, Test loss: 1.661, Test accuracy: 80.20
Round  43, Global train loss: 1.582, Global test loss: 1.727, Global test accuracy: 74.87
Round  44, Train loss: 1.690, Test loss: 1.660, Test accuracy: 80.32
Round  44, Global train loss: 1.690, Global test loss: 1.724, Global test accuracy: 75.33
Round  45, Train loss: 1.638, Test loss: 1.660, Test accuracy: 80.30
Round  45, Global train loss: 1.638, Global test loss: 1.725, Global test accuracy: 75.10
Round  46, Train loss: 1.637, Test loss: 1.660, Test accuracy: 80.33
Round  46, Global train loss: 1.637, Global test loss: 1.719, Global test accuracy: 75.50
Round  47, Train loss: 1.690, Test loss: 1.659, Test accuracy: 80.33
Round  47, Global train loss: 1.690, Global test loss: 1.748, Global test accuracy: 71.87
Round  48, Train loss: 1.531, Test loss: 1.659, Test accuracy: 80.35
Round  48, Global train loss: 1.531, Global test loss: 1.716, Global test accuracy: 75.72
Round  49, Train loss: 1.638, Test loss: 1.659, Test accuracy: 80.38
Round  49, Global train loss: 1.638, Global test loss: 1.714, Global test accuracy: 76.05
Round  50, Train loss: 1.637, Test loss: 1.658, Test accuracy: 80.50
Round  50, Global train loss: 1.637, Global test loss: 1.724, Global test accuracy: 74.80
Round  51, Train loss: 1.584, Test loss: 1.658, Test accuracy: 80.47
Round  51, Global train loss: 1.584, Global test loss: 1.710, Global test accuracy: 76.10
Round  52, Train loss: 1.639, Test loss: 1.658, Test accuracy: 80.55
Round  52, Global train loss: 1.639, Global test loss: 1.724, Global test accuracy: 74.65
Round  53, Train loss: 1.745, Test loss: 1.658, Test accuracy: 80.53
Round  53, Global train loss: 1.745, Global test loss: 1.736, Global test accuracy: 73.65
Round  54, Train loss: 1.687, Test loss: 1.658, Test accuracy: 80.33
Round  54, Global train loss: 1.687, Global test loss: 1.713, Global test accuracy: 75.72
Round  55, Train loss: 1.686, Test loss: 1.658, Test accuracy: 80.35
Round  55, Global train loss: 1.686, Global test loss: 1.719, Global test accuracy: 74.95
Round  56, Train loss: 1.743, Test loss: 1.658, Test accuracy: 80.30
Round  56, Global train loss: 1.743, Global test loss: 1.712, Global test accuracy: 76.03
Round  57, Train loss: 1.687, Test loss: 1.659, Test accuracy: 80.25
Round  57, Global train loss: 1.687, Global test loss: 1.719, Global test accuracy: 75.43
Round  58, Train loss: 1.636, Test loss: 1.658, Test accuracy: 80.42
Round  58, Global train loss: 1.636, Global test loss: 1.713, Global test accuracy: 75.82
Round  59, Train loss: 1.687, Test loss: 1.658, Test accuracy: 80.40
Round  59, Global train loss: 1.687, Global test loss: 1.716, Global test accuracy: 75.52
Round  60, Train loss: 1.635, Test loss: 1.658, Test accuracy: 80.37
Round  60, Global train loss: 1.635, Global test loss: 1.724, Global test accuracy: 74.80
Round  61, Train loss: 1.528, Test loss: 1.658, Test accuracy: 80.35
Round  61, Global train loss: 1.528, Global test loss: 1.728, Global test accuracy: 74.20
Round  62, Train loss: 1.688, Test loss: 1.658, Test accuracy: 80.38
Round  62, Global train loss: 1.688, Global test loss: 1.710, Global test accuracy: 76.07
Round  63, Train loss: 1.686, Test loss: 1.658, Test accuracy: 80.47
Round  63, Global train loss: 1.686, Global test loss: 1.720, Global test accuracy: 74.77
Round  64, Train loss: 1.634, Test loss: 1.658, Test accuracy: 80.47
Round  64, Global train loss: 1.634, Global test loss: 1.714, Global test accuracy: 75.70
Round  65, Train loss: 1.634, Test loss: 1.657, Test accuracy: 80.48
Round  65, Global train loss: 1.634, Global test loss: 1.706, Global test accuracy: 76.50
Round  66, Train loss: 1.579, Test loss: 1.658, Test accuracy: 80.45
Round  66, Global train loss: 1.579, Global test loss: 1.706, Global test accuracy: 76.37
Round  67, Train loss: 1.581, Test loss: 1.658, Test accuracy: 80.33
Round  67, Global train loss: 1.581, Global test loss: 1.715, Global test accuracy: 75.62
Round  68, Train loss: 1.581, Test loss: 1.658, Test accuracy: 80.33
Round  68, Global train loss: 1.581, Global test loss: 1.710, Global test accuracy: 75.90
Round  69, Train loss: 1.632, Test loss: 1.658, Test accuracy: 80.30
Round  69, Global train loss: 1.632, Global test loss: 1.709, Global test accuracy: 75.88
Round  70, Train loss: 1.634, Test loss: 1.658, Test accuracy: 80.27
Round  70, Global train loss: 1.634, Global test loss: 1.711, Global test accuracy: 75.95
Round  71, Train loss: 1.579, Test loss: 1.657, Test accuracy: 80.27
Round  71, Global train loss: 1.579, Global test loss: 1.710, Global test accuracy: 76.10
Round  72, Train loss: 1.687, Test loss: 1.657, Test accuracy: 80.25
Round  72, Global train loss: 1.687, Global test loss: 1.713, Global test accuracy: 75.32
Round  73, Train loss: 1.739, Test loss: 1.657, Test accuracy: 80.22
Round  73, Global train loss: 1.739, Global test loss: 1.709, Global test accuracy: 76.10
Round  74, Train loss: 1.633, Test loss: 1.658, Test accuracy: 80.20
Round  74, Global train loss: 1.633, Global test loss: 1.705, Global test accuracy: 76.30
Round  75, Train loss: 1.580, Test loss: 1.658, Test accuracy: 80.20
Round  75, Global train loss: 1.580, Global test loss: 1.714, Global test accuracy: 75.70
Round  76, Train loss: 1.473, Test loss: 1.657, Test accuracy: 80.37
Round  76, Global train loss: 1.473, Global test loss: 1.709, Global test accuracy: 76.03
Round  77, Train loss: 1.578, Test loss: 1.657, Test accuracy: 80.37
Round  77, Global train loss: 1.578, Global test loss: 1.734, Global test accuracy: 72.83
Round  78, Train loss: 1.634, Test loss: 1.657, Test accuracy: 80.33
Round  78, Global train loss: 1.634, Global test loss: 1.707, Global test accuracy: 76.23
Round  79, Train loss: 1.633, Test loss: 1.657, Test accuracy: 80.33
Round  79, Global train loss: 1.633, Global test loss: 1.705, Global test accuracy: 76.45
Round  80, Train loss: 1.633, Test loss: 1.657, Test accuracy: 80.30
Round  80, Global train loss: 1.633, Global test loss: 1.722, Global test accuracy: 74.58
Round  81, Train loss: 1.634, Test loss: 1.657, Test accuracy: 80.30
Round  81, Global train loss: 1.634, Global test loss: 1.702, Global test accuracy: 76.50
Round  82, Train loss: 1.580, Test loss: 1.657, Test accuracy: 80.22
Round  82, Global train loss: 1.580, Global test loss: 1.707, Global test accuracy: 75.98
Round  83, Train loss: 1.578, Test loss: 1.657, Test accuracy: 80.30
Round  83, Global train loss: 1.578, Global test loss: 1.702, Global test accuracy: 76.48
Round  84, Train loss: 1.685, Test loss: 1.657, Test accuracy: 80.38
Round  84, Global train loss: 1.685, Global test loss: 1.704, Global test accuracy: 76.43
Round  85, Train loss: 1.737, Test loss: 1.657, Test accuracy: 80.40
Round  85, Global train loss: 1.737, Global test loss: 1.714, Global test accuracy: 75.62
Round  86, Train loss: 1.633, Test loss: 1.657, Test accuracy: 80.45
Round  86, Global train loss: 1.633, Global test loss: 1.703, Global test accuracy: 76.35
Round  87, Train loss: 1.629, Test loss: 1.658, Test accuracy: 80.30
Round  87, Global train loss: 1.629, Global test loss: 1.732, Global test accuracy: 73.42
Round  88, Train loss: 1.737, Test loss: 1.658, Test accuracy: 80.27
Round  88, Global train loss: 1.737, Global test loss: 1.713, Global test accuracy: 75.78
Round  89, Train loss: 1.523, Test loss: 1.657, Test accuracy: 80.35
Round  89, Global train loss: 1.523, Global test loss: 1.711, Global test accuracy: 75.68
Round  90, Train loss: 1.631, Test loss: 1.657, Test accuracy: 80.33
Round  90, Global train loss: 1.631, Global test loss: 1.717, Global test accuracy: 74.95
Round  91, Train loss: 1.687, Test loss: 1.658, Test accuracy: 80.22
Round  91, Global train loss: 1.687, Global test loss: 1.700, Global test accuracy: 76.70
Round  92, Train loss: 1.736, Test loss: 1.658, Test accuracy: 80.22
Round  92, Global train loss: 1.736, Global test loss: 1.714, Global test accuracy: 75.37
Round  93, Train loss: 1.683, Test loss: 1.658, Test accuracy: 80.23
Round  93, Global train loss: 1.683, Global test loss: 1.706, Global test accuracy: 76.33
Round  94, Train loss: 1.684, Test loss: 1.658, Test accuracy: 80.27
Round  94, Global train loss: 1.684, Global test loss: 1.708, Global test accuracy: 76.22
Round  95, Train loss: 1.682, Test loss: 1.658, Test accuracy: 80.27
Round  95, Global train loss: 1.682, Global test loss: 1.708, Global test accuracy: 75.72/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  96, Train loss: 1.575, Test loss: 1.658, Test accuracy: 80.28
Round  96, Global train loss: 1.575, Global test loss: 1.704, Global test accuracy: 75.93
Round  97, Train loss: 1.630, Test loss: 1.658, Test accuracy: 80.28
Round  97, Global train loss: 1.630, Global test loss: 1.706, Global test accuracy: 76.08
Round  98, Train loss: 1.686, Test loss: 1.658, Test accuracy: 80.30
Round  98, Global train loss: 1.686, Global test loss: 1.700, Global test accuracy: 76.92
Round  99, Train loss: 1.525, Test loss: 1.658, Test accuracy: 80.30
Round  99, Global train loss: 1.525, Global test loss: 1.713, Global test accuracy: 75.37
Final Round, Train loss: 1.629, Test loss: 1.657, Test accuracy: 80.42
Final Round, Global train loss: 1.629, Global test loss: 1.713, Global test accuracy: 75.37
Average accuracy final 10 rounds: 80.27000000000001 

Average global accuracy final 10 rounds: 75.95833333333333 

914.7699542045593
[0.6746702194213867, 1.3493404388427734, 1.9294617176055908, 2.509582996368408, 3.0911037921905518, 3.6726245880126953, 4.262677192687988, 4.852729797363281, 5.432157039642334, 6.011584281921387, 6.636462211608887, 7.261340141296387, 7.856665134429932, 8.451990127563477, 9.028062343597412, 9.604134559631348, 10.152150392532349, 10.70016622543335, 11.260797023773193, 11.821427822113037, 12.366842269897461, 12.912256717681885, 13.481781721115112, 14.05130672454834, 14.623089790344238, 15.194872856140137, 15.819190979003906, 16.443509101867676, 17.043449878692627, 17.643390655517578, 18.251163005828857, 18.858935356140137, 19.441468477249146, 20.024001598358154, 20.585841417312622, 21.14768123626709, 21.6857590675354, 22.22383689880371, 22.78792381286621, 23.35201072692871, 23.899747610092163, 24.447484493255615, 25.026516914367676, 25.605549335479736, 26.200105905532837, 26.794662475585938, 27.403995037078857, 28.013327598571777, 28.63650894165039, 29.259690284729004, 29.84840965270996, 30.437129020690918, 30.987412691116333, 31.537696361541748, 32.09971833229065, 32.66174030303955, 33.20162558555603, 33.74151086807251, 34.29383563995361, 34.84616041183472, 35.4378936290741, 36.02962684631348, 36.60718655586243, 37.18474626541138, 37.797284841537476, 38.409823417663574, 39.03536415100098, 39.66090488433838, 40.29524564743042, 40.92958641052246, 41.430062770843506, 41.93053913116455, 42.47311782836914, 43.01569652557373, 43.56278204917908, 44.109867572784424, 44.64629554748535, 45.18272352218628, 45.71385884284973, 46.244994163513184, 46.87565469741821, 47.50631523132324, 48.111441135406494, 48.716567039489746, 49.3610680103302, 50.005568981170654, 50.62013244628906, 51.23469591140747, 51.80608248710632, 52.377469062805176, 52.92510509490967, 53.47274112701416, 54.039857149124146, 54.60697317123413, 55.1152560710907, 55.623538970947266, 56.190643310546875, 56.757747650146484, 57.39231324195862, 58.02687883377075, 58.61581230163574, 59.20474576950073, 59.79791069030762, 60.3910756111145, 61.01797556877136, 61.64487552642822, 62.22313833236694, 62.801401138305664, 63.337465047836304, 63.87352895736694, 64.39843583106995, 64.92334270477295, 65.44761037826538, 65.97187805175781, 66.5383996963501, 67.10492134094238, 67.7224805355072, 68.34003973007202, 68.95270586013794, 69.56537199020386, 70.20076894760132, 70.83616590499878, 71.45570397377014, 72.0752420425415, 72.65178918838501, 73.22833633422852, 73.74777388572693, 74.26721143722534, 74.8327305316925, 75.39824962615967, 75.9561996459961, 76.51414966583252, 77.04235529899597, 77.57056093215942, 78.13099431991577, 78.69142770767212, 79.36406326293945, 80.03669881820679, 80.6431896686554, 81.249680519104, 81.86658692359924, 82.48349332809448, 83.08186793327332, 83.68024253845215, 84.21863269805908, 84.75702285766602, 85.28665566444397, 85.81628847122192, 86.34497690200806, 86.87366533279419, 87.42268586158752, 87.97170639038086, 88.54139184951782, 89.11107730865479, 89.72875118255615, 90.34642505645752, 90.91169762611389, 91.47697019577026, 92.09708976745605, 92.71720933914185, 93.30814385414124, 93.89907836914062, 94.47004270553589, 95.04100704193115, 95.56439757347107, 96.08778810501099, 96.64465093612671, 97.20151376724243, 97.72959089279175, 98.25766801834106, 98.82397437095642, 99.39028072357178, 100.00890588760376, 100.62753105163574, 101.22197103500366, 101.81641101837158, 102.40833902359009, 103.0002670288086, 103.60272073745728, 104.20517444610596, 104.76171970367432, 105.31826496124268, 105.86538553237915, 106.41250610351562, 106.9856927394867, 107.55887937545776, 108.0787353515625, 108.59859132766724, 109.12257385253906, 109.64655637741089, 110.29876971244812, 110.95098304748535, 111.52861833572388, 112.1062536239624, 112.68939733505249, 113.27254104614258, 113.89705204963684, 114.5215630531311, 115.10591006278992, 115.69025707244873, 116.80029511451721, 117.9103331565857]
[18.166666666666668, 18.166666666666668, 34.9, 34.9, 45.21666666666667, 45.21666666666667, 55.36666666666667, 55.36666666666667, 68.38333333333334, 68.38333333333334, 73.08333333333333, 73.08333333333333, 74.73333333333333, 74.73333333333333, 76.01666666666667, 76.01666666666667, 77.81666666666666, 77.81666666666666, 77.71666666666667, 77.71666666666667, 78.05, 78.05, 77.36666666666666, 77.36666666666666, 79.73333333333333, 79.73333333333333, 79.75, 79.75, 79.8, 79.8, 79.83333333333333, 79.83333333333333, 80.33333333333333, 80.33333333333333, 80.45, 80.45, 80.51666666666667, 80.51666666666667, 80.58333333333333, 80.58333333333333, 80.58333333333333, 80.58333333333333, 80.58333333333333, 80.58333333333333, 80.55, 80.55, 80.56666666666666, 80.56666666666666, 80.51666666666667, 80.51666666666667, 80.53333333333333, 80.53333333333333, 80.56666666666666, 80.56666666666666, 80.56666666666666, 80.56666666666666, 80.56666666666666, 80.56666666666666, 80.58333333333333, 80.58333333333333, 80.63333333333334, 80.63333333333334, 80.63333333333334, 80.63333333333334, 80.46666666666667, 80.46666666666667, 80.43333333333334, 80.43333333333334, 80.4, 80.4, 80.46666666666667, 80.46666666666667, 80.51666666666667, 80.51666666666667, 80.55, 80.55, 80.5, 80.5, 80.68333333333334, 80.68333333333334, 80.55, 80.55, 80.48333333333333, 80.48333333333333, 80.4, 80.4, 80.2, 80.2, 80.31666666666666, 80.31666666666666, 80.3, 80.3, 80.33333333333333, 80.33333333333333, 80.33333333333333, 80.33333333333333, 80.35, 80.35, 80.38333333333334, 80.38333333333334, 80.5, 80.5, 80.46666666666667, 80.46666666666667, 80.55, 80.55, 80.53333333333333, 80.53333333333333, 80.33333333333333, 80.33333333333333, 80.35, 80.35, 80.3, 80.3, 80.25, 80.25, 80.41666666666667, 80.41666666666667, 80.4, 80.4, 80.36666666666666, 80.36666666666666, 80.35, 80.35, 80.38333333333334, 80.38333333333334, 80.46666666666667, 80.46666666666667, 80.46666666666667, 80.46666666666667, 80.48333333333333, 80.48333333333333, 80.45, 80.45, 80.33333333333333, 80.33333333333333, 80.33333333333333, 80.33333333333333, 80.3, 80.3, 80.26666666666667, 80.26666666666667, 80.26666666666667, 80.26666666666667, 80.25, 80.25, 80.21666666666667, 80.21666666666667, 80.2, 80.2, 80.2, 80.2, 80.36666666666666, 80.36666666666666, 80.36666666666666, 80.36666666666666, 80.33333333333333, 80.33333333333333, 80.33333333333333, 80.33333333333333, 80.3, 80.3, 80.3, 80.3, 80.21666666666667, 80.21666666666667, 80.3, 80.3, 80.38333333333334, 80.38333333333334, 80.4, 80.4, 80.45, 80.45, 80.3, 80.3, 80.26666666666667, 80.26666666666667, 80.35, 80.35, 80.33333333333333, 80.33333333333333, 80.21666666666667, 80.21666666666667, 80.21666666666667, 80.21666666666667, 80.23333333333333, 80.23333333333333, 80.26666666666667, 80.26666666666667, 80.26666666666667, 80.26666666666667, 80.28333333333333, 80.28333333333333, 80.28333333333333, 80.28333333333333, 80.3, 80.3, 80.3, 80.3, 80.41666666666667, 80.41666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.293, Test loss: 2.299, Test accuracy: 13.33
Round   1, Train loss: 2.266, Test loss: 2.273, Test accuracy: 13.33
Round   2, Train loss: 2.092, Test loss: 2.204, Test accuracy: 23.62
Round   3, Train loss: 2.040, Test loss: 2.106, Test accuracy: 37.65
Round   4, Train loss: 1.869, Test loss: 2.082, Test accuracy: 44.17
Round   5, Train loss: 1.822, Test loss: 2.075, Test accuracy: 37.37
Round   6, Train loss: 1.712, Test loss: 1.965, Test accuracy: 57.92
Round   7, Train loss: 1.569, Test loss: 1.942, Test accuracy: 54.78
Round   8, Train loss: 1.664, Test loss: 1.937, Test accuracy: 56.93
Round   9, Train loss: 1.631, Test loss: 1.882, Test accuracy: 63.78
Round  10, Train loss: 1.593, Test loss: 1.823, Test accuracy: 69.47
Round  11, Train loss: 1.551, Test loss: 1.852, Test accuracy: 64.07
Round  12, Train loss: 1.515, Test loss: 1.821, Test accuracy: 66.38
Round  13, Train loss: 1.533, Test loss: 1.748, Test accuracy: 76.35
Round  14, Train loss: 1.511, Test loss: 1.748, Test accuracy: 73.17
Round  15, Train loss: 1.510, Test loss: 1.741, Test accuracy: 74.27
Round  16, Train loss: 1.495, Test loss: 1.707, Test accuracy: 81.25
Round  17, Train loss: 1.550, Test loss: 1.759, Test accuracy: 71.80
Round  18, Train loss: 1.498, Test loss: 1.729, Test accuracy: 76.75
Round  19, Train loss: 1.494, Test loss: 1.715, Test accuracy: 78.37
Round  20, Train loss: 1.501, Test loss: 1.748, Test accuracy: 72.43
Round  21, Train loss: 1.496, Test loss: 1.709, Test accuracy: 75.85
Round  22, Train loss: 1.490, Test loss: 1.670, Test accuracy: 83.50
Round  23, Train loss: 1.491, Test loss: 1.707, Test accuracy: 76.40
Round  24, Train loss: 1.491, Test loss: 1.705, Test accuracy: 76.80
Round  25, Train loss: 1.483, Test loss: 1.673, Test accuracy: 81.52
Round  26, Train loss: 1.486, Test loss: 1.692, Test accuracy: 77.23
Round  27, Train loss: 1.483, Test loss: 1.706, Test accuracy: 78.00
Round  28, Train loss: 1.483, Test loss: 1.693, Test accuracy: 76.97
Round  29, Train loss: 1.489, Test loss: 1.663, Test accuracy: 81.95
Round  30, Train loss: 1.484, Test loss: 1.671, Test accuracy: 80.37
Round  31, Train loss: 1.490, Test loss: 1.659, Test accuracy: 83.30
Round  32, Train loss: 1.483, Test loss: 1.654, Test accuracy: 83.42
Round  33, Train loss: 1.482, Test loss: 1.678, Test accuracy: 80.67
Round  34, Train loss: 1.484, Test loss: 1.643, Test accuracy: 84.00
Round  35, Train loss: 1.488, Test loss: 1.637, Test accuracy: 84.25
Round  36, Train loss: 1.484, Test loss: 1.642, Test accuracy: 83.88
Round  37, Train loss: 1.477, Test loss: 1.683, Test accuracy: 77.70
Round  38, Train loss: 1.481, Test loss: 1.641, Test accuracy: 83.67
Round  39, Train loss: 1.476, Test loss: 1.644, Test accuracy: 84.07
Round  40, Train loss: 1.483, Test loss: 1.657, Test accuracy: 82.33
Round  41, Train loss: 1.474, Test loss: 1.682, Test accuracy: 77.72
Round  42, Train loss: 1.475, Test loss: 1.669, Test accuracy: 79.00
Round  43, Train loss: 1.480, Test loss: 1.620, Test accuracy: 85.87
Round  44, Train loss: 1.477, Test loss: 1.627, Test accuracy: 84.68
Round  45, Train loss: 1.475, Test loss: 1.633, Test accuracy: 84.10
Round  46, Train loss: 1.478, Test loss: 1.652, Test accuracy: 82.00
Round  47, Train loss: 1.480, Test loss: 1.653, Test accuracy: 81.77
Round  48, Train loss: 1.480, Test loss: 1.663, Test accuracy: 81.02
Round  49, Train loss: 1.476, Test loss: 1.607, Test accuracy: 87.18
Round  50, Train loss: 1.480, Test loss: 1.625, Test accuracy: 85.23
Round  51, Train loss: 1.474, Test loss: 1.628, Test accuracy: 85.13
Round  52, Train loss: 1.478, Test loss: 1.641, Test accuracy: 83.88
Round  53, Train loss: 1.478, Test loss: 1.640, Test accuracy: 82.68
Round  54, Train loss: 1.474, Test loss: 1.640, Test accuracy: 83.18
Round  55, Train loss: 1.475, Test loss: 1.599, Test accuracy: 87.93
Round  56, Train loss: 1.475, Test loss: 1.623, Test accuracy: 85.45
Round  57, Train loss: 1.475, Test loss: 1.622, Test accuracy: 85.30
Round  58, Train loss: 1.473, Test loss: 1.610, Test accuracy: 86.52
Round  59, Train loss: 1.477, Test loss: 1.613, Test accuracy: 85.65
Round  60, Train loss: 1.473, Test loss: 1.610, Test accuracy: 86.38
Round  61, Train loss: 1.478, Test loss: 1.602, Test accuracy: 87.12
Round  62, Train loss: 1.471, Test loss: 1.593, Test accuracy: 88.07
Round  63, Train loss: 1.476, Test loss: 1.599, Test accuracy: 87.17
Round  64, Train loss: 1.472, Test loss: 1.595, Test accuracy: 87.67
Round  65, Train loss: 1.473, Test loss: 1.605, Test accuracy: 87.15
Round  66, Train loss: 1.475, Test loss: 1.600, Test accuracy: 87.18
Round  67, Train loss: 1.471, Test loss: 1.613, Test accuracy: 85.42
Round  68, Train loss: 1.475, Test loss: 1.622, Test accuracy: 85.08
Round  69, Train loss: 1.474, Test loss: 1.604, Test accuracy: 87.37
Round  70, Train loss: 1.470, Test loss: 1.642, Test accuracy: 83.08
Round  71, Train loss: 1.472, Test loss: 1.595, Test accuracy: 88.22
Round  72, Train loss: 1.471, Test loss: 1.606, Test accuracy: 86.75
Round  73, Train loss: 1.478, Test loss: 1.613, Test accuracy: 85.48
Round  74, Train loss: 1.472, Test loss: 1.621, Test accuracy: 84.70
Round  75, Train loss: 1.472, Test loss: 1.597, Test accuracy: 87.70
Round  76, Train loss: 1.468, Test loss: 1.613, Test accuracy: 85.65
Round  77, Train loss: 1.470, Test loss: 1.617, Test accuracy: 85.22
Round  78, Train loss: 1.471, Test loss: 1.588, Test accuracy: 88.45
Round  79, Train loss: 1.471, Test loss: 1.599, Test accuracy: 87.02
Round  80, Train loss: 1.469, Test loss: 1.591, Test accuracy: 88.50
Round  81, Train loss: 1.470, Test loss: 1.582, Test accuracy: 89.10
Round  82, Train loss: 1.475, Test loss: 1.617, Test accuracy: 85.43
Round  83, Train loss: 1.475, Test loss: 1.587, Test accuracy: 88.53
Round  84, Train loss: 1.469, Test loss: 1.601, Test accuracy: 87.38
Round  85, Train loss: 1.471, Test loss: 1.596, Test accuracy: 87.85
Round  86, Train loss: 1.472, Test loss: 1.618, Test accuracy: 85.23
Round  87, Train loss: 1.473, Test loss: 1.593, Test accuracy: 88.00
Round  88, Train loss: 1.469, Test loss: 1.598, Test accuracy: 87.55
Round  89, Train loss: 1.471, Test loss: 1.583, Test accuracy: 89.02
Round  90, Train loss: 1.473, Test loss: 1.587, Test accuracy: 88.63
Round  91, Train loss: 1.474, Test loss: 1.591, Test accuracy: 88.18
Round  92, Train loss: 1.472, Test loss: 1.626, Test accuracy: 84.25
Round  93, Train loss: 1.473, Test loss: 1.588, Test accuracy: 88.33
Round  94, Train loss: 1.471, Test loss: 1.604, Test accuracy: 86.40
Round  95, Train loss: 1.473, Test loss: 1.609, Test accuracy: 86.30
Round  96, Train loss: 1.469, Test loss: 1.606, Test accuracy: 86.43
Round  97, Train loss: 1.468, Test loss: 1.591, Test accuracy: 88.02
Round  98, Train loss: 1.472, Test loss: 1.576, Test accuracy: 89.80
Round  99, Train loss: 1.471, Test loss: 1.585, Test accuracy: 88.80
Final Round, Train loss: 1.470, Test loss: 1.577, Test accuracy: 89.43
Average accuracy final 10 rounds: 87.51499999999999
1037.535940170288
[1.4633634090423584, 2.9226465225219727, 4.35961127281189, 5.8139567375183105, 7.27910852432251, 8.635416984558105, 10.005928039550781, 11.349721908569336, 12.744977474212646, 14.178905963897705, 15.584686517715454, 17.040706396102905, 18.542450666427612, 19.892417907714844, 21.26575207710266, 22.621390104293823, 23.97770357131958, 25.428544282913208, 26.839405059814453, 28.277464151382446, 29.744283199310303, 31.100590229034424, 32.46732759475708, 33.83662462234497, 35.21150588989258, 36.67831254005432, 38.074421882629395, 39.500513553619385, 40.9971706867218, 42.429600954055786, 43.78254055976868, 45.15198087692261, 46.51331543922424, 47.950976848602295, 49.379032611846924, 50.820629358291626, 52.244831562042236, 53.65414595603943, 55.02664065361023, 56.396076917648315, 57.73144721984863, 59.17333102226257, 60.6212158203125, 62.06542181968689, 63.546905279159546, 64.95683574676514, 66.32890248298645, 67.69333934783936, 69.0203812122345, 70.4289448261261, 71.8426673412323, 73.24280309677124, 74.70239973068237, 76.14534878730774, 77.51214981079102, 78.90904903411865, 80.25156140327454, 81.60619592666626, 83.06492853164673, 84.49472212791443, 85.94527626037598, 87.39596009254456, 88.75496459007263, 90.13149189949036, 91.47250461578369, 92.85864305496216, 94.31453466415405, 95.7318868637085, 97.19636392593384, 98.6770088672638, 100.03785967826843, 101.3992612361908, 102.75740170478821, 104.1094343662262, 105.52039074897766, 106.92424869537354, 108.35815453529358, 109.83966326713562, 111.2181646823883, 112.59371399879456, 113.99141025543213, 115.34355092048645, 116.80604982376099, 118.26724815368652, 119.68491721153259, 121.13702583312988, 122.5410258769989, 123.91311573982239, 125.26410269737244, 126.61430048942566, 128.03833961486816, 129.46020984649658, 130.94211888313293, 132.42002081871033, 133.83505487442017, 135.2140085697174, 136.57421565055847, 137.93277549743652, 139.33856201171875, 140.7537305355072, 142.22267079353333]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[13.333333333333334, 13.333333333333334, 23.616666666666667, 37.65, 44.166666666666664, 37.36666666666667, 57.916666666666664, 54.78333333333333, 56.93333333333333, 63.78333333333333, 69.46666666666667, 64.06666666666666, 66.38333333333334, 76.35, 73.16666666666667, 74.26666666666667, 81.25, 71.8, 76.75, 78.36666666666666, 72.43333333333334, 75.85, 83.5, 76.4, 76.8, 81.51666666666667, 77.23333333333333, 78.0, 76.96666666666667, 81.95, 80.36666666666666, 83.3, 83.41666666666667, 80.66666666666667, 84.0, 84.25, 83.88333333333334, 77.7, 83.66666666666667, 84.06666666666666, 82.33333333333333, 77.71666666666667, 79.0, 85.86666666666666, 84.68333333333334, 84.1, 82.0, 81.76666666666667, 81.01666666666667, 87.18333333333334, 85.23333333333333, 85.13333333333334, 83.88333333333334, 82.68333333333334, 83.18333333333334, 87.93333333333334, 85.45, 85.3, 86.51666666666667, 85.65, 86.38333333333334, 87.11666666666666, 88.06666666666666, 87.16666666666667, 87.66666666666667, 87.15, 87.18333333333334, 85.41666666666667, 85.08333333333333, 87.36666666666666, 83.08333333333333, 88.21666666666667, 86.75, 85.48333333333333, 84.7, 87.7, 85.65, 85.21666666666667, 88.45, 87.01666666666667, 88.5, 89.1, 85.43333333333334, 88.53333333333333, 87.38333333333334, 87.85, 85.23333333333333, 88.0, 87.55, 89.01666666666667, 88.63333333333334, 88.18333333333334, 84.25, 88.33333333333333, 86.4, 86.3, 86.43333333333334, 88.01666666666667, 89.8, 88.8, 89.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.303, Test accuracy: 6.25
Round   1, Train loss: 2.295, Test loss: 2.300, Test accuracy: 10.07
Round   2, Train loss: 2.293, Test loss: 2.295, Test accuracy: 23.62
Round   3, Train loss: 2.281, Test loss: 2.284, Test accuracy: 31.13
Round   4, Train loss: 2.233, Test loss: 2.243, Test accuracy: 24.28
Round   5, Train loss: 2.111, Test loss: 2.156, Test accuracy: 42.60
Round   6, Train loss: 1.918, Test loss: 2.060, Test accuracy: 54.57
Round   7, Train loss: 1.850, Test loss: 1.950, Test accuracy: 64.58
Round   8, Train loss: 1.751, Test loss: 1.843, Test accuracy: 74.03
Round   9, Train loss: 1.732, Test loss: 1.778, Test accuracy: 78.05
Round  10, Train loss: 1.727, Test loss: 1.707, Test accuracy: 85.10
Round  11, Train loss: 1.634, Test loss: 1.681, Test accuracy: 85.70
Round  12, Train loss: 1.576, Test loss: 1.654, Test accuracy: 86.78
Round  13, Train loss: 1.608, Test loss: 1.630, Test accuracy: 89.88
Round  14, Train loss: 1.560, Test loss: 1.619, Test accuracy: 89.87
Round  15, Train loss: 1.544, Test loss: 1.612, Test accuracy: 89.62
Round  16, Train loss: 1.572, Test loss: 1.591, Test accuracy: 91.77
Round  17, Train loss: 1.531, Test loss: 1.585, Test accuracy: 92.15
Round  18, Train loss: 1.611, Test loss: 1.577, Test accuracy: 92.67
Round  19, Train loss: 1.581, Test loss: 1.572, Test accuracy: 92.93
Round  20, Train loss: 1.530, Test loss: 1.567, Test accuracy: 93.12
Round  21, Train loss: 1.531, Test loss: 1.563, Test accuracy: 93.17
Round  22, Train loss: 1.541, Test loss: 1.559, Test accuracy: 93.37
Round  23, Train loss: 1.537, Test loss: 1.555, Test accuracy: 93.45
Round  24, Train loss: 1.522, Test loss: 1.553, Test accuracy: 93.57
Round  25, Train loss: 1.520, Test loss: 1.552, Test accuracy: 93.65
Round  26, Train loss: 1.516, Test loss: 1.551, Test accuracy: 93.80
Round  27, Train loss: 1.527, Test loss: 1.546, Test accuracy: 93.70
Round  28, Train loss: 1.521, Test loss: 1.544, Test accuracy: 93.88
Round  29, Train loss: 1.512, Test loss: 1.542, Test accuracy: 93.97
Round  30, Train loss: 1.520, Test loss: 1.541, Test accuracy: 93.88
Round  31, Train loss: 1.551, Test loss: 1.538, Test accuracy: 94.02
Round  32, Train loss: 1.505, Test loss: 1.538, Test accuracy: 94.07
Round  33, Train loss: 1.504, Test loss: 1.536, Test accuracy: 94.02
Round  34, Train loss: 1.522, Test loss: 1.525, Test accuracy: 95.28
Round  35, Train loss: 1.506, Test loss: 1.523, Test accuracy: 95.58
Round  36, Train loss: 1.500, Test loss: 1.523, Test accuracy: 95.77
Round  37, Train loss: 1.497, Test loss: 1.523, Test accuracy: 95.67
Round  38, Train loss: 1.509, Test loss: 1.523, Test accuracy: 95.78
Round  39, Train loss: 1.498, Test loss: 1.520, Test accuracy: 95.65
Round  40, Train loss: 1.494, Test loss: 1.520, Test accuracy: 95.82
Round  41, Train loss: 1.499, Test loss: 1.521, Test accuracy: 95.68
Round  42, Train loss: 1.497, Test loss: 1.520, Test accuracy: 95.68
Round  43, Train loss: 1.492, Test loss: 1.519, Test accuracy: 95.78
Round  44, Train loss: 1.491, Test loss: 1.518, Test accuracy: 95.93
Round  45, Train loss: 1.491, Test loss: 1.518, Test accuracy: 95.97
Round  46, Train loss: 1.490, Test loss: 1.518, Test accuracy: 96.07
Round  47, Train loss: 1.495, Test loss: 1.518, Test accuracy: 96.07
Round  48, Train loss: 1.492, Test loss: 1.517, Test accuracy: 95.97
Round  49, Train loss: 1.489, Test loss: 1.516, Test accuracy: 95.97
Round  50, Train loss: 1.500, Test loss: 1.515, Test accuracy: 96.15
Round  51, Train loss: 1.491, Test loss: 1.515, Test accuracy: 96.13
Round  52, Train loss: 1.491, Test loss: 1.515, Test accuracy: 96.10
Round  53, Train loss: 1.487, Test loss: 1.514, Test accuracy: 96.12
Round  54, Train loss: 1.496, Test loss: 1.514, Test accuracy: 96.10
Round  55, Train loss: 1.487, Test loss: 1.514, Test accuracy: 96.12
Round  56, Train loss: 1.492, Test loss: 1.514, Test accuracy: 96.15
Round  57, Train loss: 1.489, Test loss: 1.514, Test accuracy: 96.13
Round  58, Train loss: 1.491, Test loss: 1.513, Test accuracy: 96.12
Round  59, Train loss: 1.490, Test loss: 1.512, Test accuracy: 96.17
Round  60, Train loss: 1.489, Test loss: 1.512, Test accuracy: 96.13
Round  61, Train loss: 1.483, Test loss: 1.513, Test accuracy: 95.95
Round  62, Train loss: 1.491, Test loss: 1.512, Test accuracy: 96.22
Round  63, Train loss: 1.484, Test loss: 1.513, Test accuracy: 96.07
Round  64, Train loss: 1.493, Test loss: 1.511, Test accuracy: 96.18
Round  65, Train loss: 1.482, Test loss: 1.512, Test accuracy: 96.18
Round  66, Train loss: 1.481, Test loss: 1.512, Test accuracy: 96.13
Round  67, Train loss: 1.488, Test loss: 1.511, Test accuracy: 96.15
Round  68, Train loss: 1.489, Test loss: 1.510, Test accuracy: 96.33
Round  69, Train loss: 1.483, Test loss: 1.510, Test accuracy: 96.38
Round  70, Train loss: 1.482, Test loss: 1.510, Test accuracy: 96.32
Round  71, Train loss: 1.482, Test loss: 1.509, Test accuracy: 96.25
Round  72, Train loss: 1.482, Test loss: 1.509, Test accuracy: 96.32
Round  73, Train loss: 1.481, Test loss: 1.508, Test accuracy: 96.32
Round  74, Train loss: 1.479, Test loss: 1.509, Test accuracy: 96.23
Round  75, Train loss: 1.481, Test loss: 1.509, Test accuracy: 96.32
Round  76, Train loss: 1.483, Test loss: 1.509, Test accuracy: 96.28
Round  77, Train loss: 1.486, Test loss: 1.508, Test accuracy: 96.40
Round  78, Train loss: 1.481, Test loss: 1.507, Test accuracy: 96.38
Round  79, Train loss: 1.482, Test loss: 1.508, Test accuracy: 96.48
Round  80, Train loss: 1.479, Test loss: 1.508, Test accuracy: 96.37
Round  81, Train loss: 1.481, Test loss: 1.506, Test accuracy: 96.57
Round  82, Train loss: 1.477, Test loss: 1.507, Test accuracy: 96.47
Round  83, Train loss: 1.476, Test loss: 1.507, Test accuracy: 96.50
Round  84, Train loss: 1.482, Test loss: 1.507, Test accuracy: 96.50
Round  85, Train loss: 1.478, Test loss: 1.507, Test accuracy: 96.57
Round  86, Train loss: 1.478, Test loss: 1.507, Test accuracy: 96.47
Round  87, Train loss: 1.478, Test loss: 1.507, Test accuracy: 96.58
Round  88, Train loss: 1.476, Test loss: 1.506, Test accuracy: 96.55
Round  89, Train loss: 1.477, Test loss: 1.506, Test accuracy: 96.52
Round  90, Train loss: 1.477, Test loss: 1.506, Test accuracy: 96.53
Round  91, Train loss: 1.478, Test loss: 1.506, Test accuracy: 96.57
Round  92, Train loss: 1.481, Test loss: 1.506, Test accuracy: 96.58
Round  93, Train loss: 1.476, Test loss: 1.505, Test accuracy: 96.53
Round  94, Train loss: 1.476, Test loss: 1.506, Test accuracy: 96.62/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.482, Test loss: 1.506, Test accuracy: 96.43
Round  96, Train loss: 1.476, Test loss: 1.505, Test accuracy: 96.57
Round  97, Train loss: 1.475, Test loss: 1.505, Test accuracy: 96.55
Round  98, Train loss: 1.475, Test loss: 1.505, Test accuracy: 96.55
Round  99, Train loss: 1.480, Test loss: 1.505, Test accuracy: 96.45
Final Round, Train loss: 1.472, Test loss: 1.504, Test accuracy: 96.63
Average accuracy final 10 rounds: 96.53833333333333
609.2540640830994
[0.8323185443878174, 1.503922939300537, 2.173457622528076, 2.8222975730895996, 3.4793026447296143, 4.154475212097168, 4.920496225357056, 5.7001121044158936, 6.42978048324585, 7.179179906845093, 7.9436163902282715, 8.714329242706299, 9.432174444198608, 10.105563879013062, 10.762021541595459, 11.444994688034058, 12.126430034637451, 12.793349266052246, 13.458391427993774, 14.145071983337402, 14.916492938995361, 15.676057577133179, 16.416881322860718, 17.207319498062134, 17.980610370635986, 18.76724863052368, 19.487961292266846, 20.13155770301819, 20.782094478607178, 21.467459678649902, 22.174598455429077, 22.834269523620605, 23.520787239074707, 24.18526339530945, 24.93467092514038, 25.699917554855347, 26.471930503845215, 27.260088682174683, 28.057608127593994, 28.788654088974, 29.490330457687378, 30.147557973861694, 30.842578649520874, 31.513863563537598, 32.17686986923218, 32.83730745315552, 33.5100154876709, 34.189958572387695, 34.94997692108154, 35.70553994178772, 36.41759443283081, 37.1949257850647, 37.98053765296936, 38.724485635757446, 39.4226188659668, 40.08559441566467, 40.774521350860596, 41.452048778533936, 42.09180450439453, 42.754485845565796, 43.441524267196655, 44.18689513206482, 44.956809282302856, 45.668461322784424, 46.392650842666626, 47.15605354309082, 47.936094999313354, 48.65670609474182, 49.35298275947571, 50.02584528923035, 50.69870352745056, 51.387047290802, 52.06545352935791, 52.72919464111328, 53.40114951133728, 54.181732177734375, 54.95343899726868, 55.69649624824524, 56.411792039871216, 57.195627212524414, 57.92909646034241, 58.659223318099976, 59.365328788757324, 60.02824950218201, 60.7187602519989, 61.39432764053345, 62.08209776878357, 62.76005172729492, 63.464250564575195, 64.22080254554749, 65.01386260986328, 65.75611090660095, 66.41235566139221, 67.17672562599182, 67.97651290893555, 68.72702312469482, 69.38003349304199, 70.04206395149231, 70.71542978286743, 71.40549612045288, 72.52039170265198]
[6.25, 10.066666666666666, 23.616666666666667, 31.133333333333333, 24.283333333333335, 42.6, 54.56666666666667, 64.58333333333333, 74.03333333333333, 78.05, 85.1, 85.7, 86.78333333333333, 89.88333333333334, 89.86666666666666, 89.61666666666666, 91.76666666666667, 92.15, 92.66666666666667, 92.93333333333334, 93.11666666666666, 93.16666666666667, 93.36666666666666, 93.45, 93.56666666666666, 93.65, 93.8, 93.7, 93.88333333333334, 93.96666666666667, 93.88333333333334, 94.01666666666667, 94.06666666666666, 94.01666666666667, 95.28333333333333, 95.58333333333333, 95.76666666666667, 95.66666666666667, 95.78333333333333, 95.65, 95.81666666666666, 95.68333333333334, 95.68333333333334, 95.78333333333333, 95.93333333333334, 95.96666666666667, 96.06666666666666, 96.06666666666666, 95.96666666666667, 95.96666666666667, 96.15, 96.13333333333334, 96.1, 96.11666666666666, 96.1, 96.11666666666666, 96.15, 96.13333333333334, 96.11666666666666, 96.16666666666667, 96.13333333333334, 95.95, 96.21666666666667, 96.06666666666666, 96.18333333333334, 96.18333333333334, 96.13333333333334, 96.15, 96.33333333333333, 96.38333333333334, 96.31666666666666, 96.25, 96.31666666666666, 96.31666666666666, 96.23333333333333, 96.31666666666666, 96.28333333333333, 96.4, 96.38333333333334, 96.48333333333333, 96.36666666666666, 96.56666666666666, 96.46666666666667, 96.5, 96.5, 96.56666666666666, 96.46666666666667, 96.58333333333333, 96.55, 96.51666666666667, 96.53333333333333, 96.56666666666666, 96.58333333333333, 96.53333333333333, 96.61666666666666, 96.43333333333334, 96.56666666666666, 96.55, 96.55, 96.45, 96.63333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.314, Test loss: 2.299, Test accuracy: 15.17
Round   1, Train loss: 2.306, Test loss: 2.293, Test accuracy: 28.75
Round   2, Train loss: 2.291, Test loss: 2.280, Test accuracy: 49.80
Round   3, Train loss: 2.215, Test loss: 2.231, Test accuracy: 27.80
Round   4, Train loss: 2.073, Test loss: 2.138, Test accuracy: 50.18
Round   5, Train loss: 1.973, Test loss: 2.048, Test accuracy: 61.63
Round   6, Train loss: 1.817, Test loss: 1.935, Test accuracy: 70.75
Round   7, Train loss: 1.762, Test loss: 1.852, Test accuracy: 76.32
Round   8, Train loss: 1.788, Test loss: 1.795, Test accuracy: 80.38
Round   9, Train loss: 1.685, Test loss: 1.729, Test accuracy: 84.22
Round  10, Train loss: 1.719, Test loss: 1.681, Test accuracy: 88.75
Round  11, Train loss: 1.644, Test loss: 1.650, Test accuracy: 90.80
Round  12, Train loss: 1.609, Test loss: 1.638, Test accuracy: 91.33
Round  13, Train loss: 1.639, Test loss: 1.611, Test accuracy: 93.65
Round  14, Train loss: 1.607, Test loss: 1.595, Test accuracy: 94.70
Round  15, Train loss: 1.581, Test loss: 1.593, Test accuracy: 94.88
Round  16, Train loss: 1.590, Test loss: 1.581, Test accuracy: 95.23
Round  17, Train loss: 1.581, Test loss: 1.574, Test accuracy: 95.45
Round  18, Train loss: 1.551, Test loss: 1.573, Test accuracy: 95.45
Round  19, Train loss: 1.561, Test loss: 1.569, Test accuracy: 95.87
Round  20, Train loss: 1.570, Test loss: 1.564, Test accuracy: 95.85
Round  21, Train loss: 1.554, Test loss: 1.561, Test accuracy: 96.02
Round  22, Train loss: 1.565, Test loss: 1.558, Test accuracy: 96.02
Round  23, Train loss: 1.543, Test loss: 1.558, Test accuracy: 96.15
Round  24, Train loss: 1.548, Test loss: 1.554, Test accuracy: 96.10
Round  25, Train loss: 1.549, Test loss: 1.552, Test accuracy: 96.15
Round  26, Train loss: 1.539, Test loss: 1.552, Test accuracy: 96.30
Round  27, Train loss: 1.544, Test loss: 1.549, Test accuracy: 96.23
Round  28, Train loss: 1.535, Test loss: 1.548, Test accuracy: 96.32
Round  29, Train loss: 1.539, Test loss: 1.545, Test accuracy: 96.32
Round  30, Train loss: 1.545, Test loss: 1.544, Test accuracy: 96.45
Round  31, Train loss: 1.548, Test loss: 1.540, Test accuracy: 96.52
Round  32, Train loss: 1.533, Test loss: 1.539, Test accuracy: 96.53
Round  33, Train loss: 1.543, Test loss: 1.536, Test accuracy: 96.62
Round  34, Train loss: 1.524, Test loss: 1.537, Test accuracy: 96.65
Round  35, Train loss: 1.531, Test loss: 1.535, Test accuracy: 96.73
Round  36, Train loss: 1.537, Test loss: 1.532, Test accuracy: 96.67
Round  37, Train loss: 1.531, Test loss: 1.532, Test accuracy: 96.70
Round  38, Train loss: 1.518, Test loss: 1.533, Test accuracy: 96.75
Round  39, Train loss: 1.522, Test loss: 1.532, Test accuracy: 96.73
Round  40, Train loss: 1.536, Test loss: 1.529, Test accuracy: 96.82
Round  41, Train loss: 1.526, Test loss: 1.529, Test accuracy: 96.88
Round  42, Train loss: 1.515, Test loss: 1.529, Test accuracy: 96.85
Round  43, Train loss: 1.513, Test loss: 1.529, Test accuracy: 96.82
Round  44, Train loss: 1.522, Test loss: 1.528, Test accuracy: 96.83
Round  45, Train loss: 1.518, Test loss: 1.527, Test accuracy: 96.97
Round  46, Train loss: 1.516, Test loss: 1.527, Test accuracy: 96.93
Round  47, Train loss: 1.513, Test loss: 1.526, Test accuracy: 96.80
Round  48, Train loss: 1.519, Test loss: 1.525, Test accuracy: 97.02
Round  49, Train loss: 1.515, Test loss: 1.524, Test accuracy: 96.92
Round  50, Train loss: 1.507, Test loss: 1.524, Test accuracy: 96.92
Round  51, Train loss: 1.514, Test loss: 1.524, Test accuracy: 96.87
Round  52, Train loss: 1.509, Test loss: 1.524, Test accuracy: 96.98
Round  53, Train loss: 1.510, Test loss: 1.523, Test accuracy: 97.02
Round  54, Train loss: 1.514, Test loss: 1.522, Test accuracy: 97.12
Round  55, Train loss: 1.508, Test loss: 1.523, Test accuracy: 97.08
Round  56, Train loss: 1.509, Test loss: 1.522, Test accuracy: 97.17
Round  57, Train loss: 1.514, Test loss: 1.520, Test accuracy: 97.23
Round  58, Train loss: 1.502, Test loss: 1.522, Test accuracy: 97.08
Round  59, Train loss: 1.507, Test loss: 1.520, Test accuracy: 97.18
Round  60, Train loss: 1.505, Test loss: 1.520, Test accuracy: 97.15
Round  61, Train loss: 1.502, Test loss: 1.520, Test accuracy: 97.08
Round  62, Train loss: 1.512, Test loss: 1.518, Test accuracy: 97.25
Round  63, Train loss: 1.502, Test loss: 1.518, Test accuracy: 97.02
Round  64, Train loss: 1.499, Test loss: 1.518, Test accuracy: 97.20
Round  65, Train loss: 1.501, Test loss: 1.518, Test accuracy: 97.22
Round  66, Train loss: 1.505, Test loss: 1.517, Test accuracy: 97.23
Round  67, Train loss: 1.507, Test loss: 1.517, Test accuracy: 97.08
Round  68, Train loss: 1.497, Test loss: 1.518, Test accuracy: 97.10
Round  69, Train loss: 1.502, Test loss: 1.516, Test accuracy: 97.10
Round  70, Train loss: 1.504, Test loss: 1.516, Test accuracy: 97.08
Round  71, Train loss: 1.502, Test loss: 1.516, Test accuracy: 97.13
Round  72, Train loss: 1.495, Test loss: 1.516, Test accuracy: 97.23
Round  73, Train loss: 1.501, Test loss: 1.516, Test accuracy: 97.17
Round  74, Train loss: 1.496, Test loss: 1.515, Test accuracy: 97.15
Round  75, Train loss: 1.497, Test loss: 1.517, Test accuracy: 96.97
Round  76, Train loss: 1.495, Test loss: 1.516, Test accuracy: 97.18
Round  77, Train loss: 1.499, Test loss: 1.516, Test accuracy: 97.12
Round  78, Train loss: 1.497, Test loss: 1.514, Test accuracy: 97.18
Round  79, Train loss: 1.494, Test loss: 1.515, Test accuracy: 97.08
Round  80, Train loss: 1.500, Test loss: 1.514, Test accuracy: 97.32
Round  81, Train loss: 1.495, Test loss: 1.514, Test accuracy: 97.18
Round  82, Train loss: 1.501, Test loss: 1.513, Test accuracy: 97.22
Round  83, Train loss: 1.498, Test loss: 1.513, Test accuracy: 97.15
Round  84, Train loss: 1.495, Test loss: 1.514, Test accuracy: 97.13
Round  85, Train loss: 1.493, Test loss: 1.514, Test accuracy: 97.17
Round  86, Train loss: 1.496, Test loss: 1.514, Test accuracy: 97.17
Round  87, Train loss: 1.492, Test loss: 1.514, Test accuracy: 97.08
Round  88, Train loss: 1.493, Test loss: 1.514, Test accuracy: 97.17
Round  89, Train loss: 1.498, Test loss: 1.512, Test accuracy: 97.20
Round  90, Train loss: 1.496, Test loss: 1.511, Test accuracy: 97.28
Round  91, Train loss: 1.492, Test loss: 1.511, Test accuracy: 97.22
Round  92, Train loss: 1.492, Test loss: 1.512, Test accuracy: 97.15
Round  93, Train loss: 1.490, Test loss: 1.512, Test accuracy: 97.18/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.493, Test loss: 1.511, Test accuracy: 97.18
Round  95, Train loss: 1.494, Test loss: 1.513, Test accuracy: 97.13
Round  96, Train loss: 1.491, Test loss: 1.512, Test accuracy: 97.22
Round  97, Train loss: 1.496, Test loss: 1.510, Test accuracy: 97.35
Round  98, Train loss: 1.491, Test loss: 1.510, Test accuracy: 97.30
Round  99, Train loss: 1.491, Test loss: 1.511, Test accuracy: 97.15
Final Round, Train loss: 1.477, Test loss: 1.508, Test accuracy: 97.22
Average accuracy final 10 rounds: 97.21666666666668
798.177820444107
[0.8947162628173828, 1.7894325256347656, 2.539457321166992, 3.2894821166992188, 4.081718921661377, 4.873955726623535, 5.607690095901489, 6.341424465179443, 6.934947967529297, 7.52847146987915, 8.167077541351318, 8.805683612823486, 9.491703510284424, 10.177723407745361, 10.82221531867981, 11.466707229614258, 12.117663860321045, 12.768620491027832, 13.544274091720581, 14.31992769241333, 15.140252113342285, 15.96057653427124, 16.759031057357788, 17.557485580444336, 18.312747478485107, 19.06800937652588, 19.82358455657959, 20.5791597366333, 21.32629895210266, 22.07343816757202, 22.74602246284485, 23.418606758117676, 24.104799032211304, 24.79099130630493, 25.4456889629364, 26.10038661956787, 26.767497301101685, 27.434607982635498, 28.152928113937378, 28.871248245239258, 29.63288974761963, 30.39453125, 31.147765398025513, 31.900999546051025, 32.62496018409729, 33.348920822143555, 34.153406858444214, 34.95789289474487, 35.69143867492676, 36.42498445510864, 37.07545757293701, 37.72593069076538, 38.38166570663452, 39.03740072250366, 39.7411630153656, 40.44492530822754, 41.11048078536987, 41.77603626251221, 42.41980290412903, 43.06356954574585, 43.805984020233154, 44.54839849472046, 45.301026344299316, 46.053654193878174, 46.800294160842896, 47.54693412780762, 48.28557586669922, 49.02421760559082, 49.762205362319946, 50.50019311904907, 51.224783420562744, 51.949373722076416, 52.60784697532654, 53.26632022857666, 53.94189405441284, 54.61746788024902, 55.31197190284729, 56.00647592544556, 56.66400384902954, 57.321531772613525, 58.037447452545166, 58.75336313247681, 59.54725384712219, 60.34114456176758, 61.0990104675293, 61.856876373291016, 62.61056852340698, 63.36426067352295, 64.14454650878906, 64.92483234405518, 65.69189095497131, 66.45894956588745, 67.1112072467804, 67.76346492767334, 68.4612045288086, 69.15894412994385, 69.8622043132782, 70.56546449661255, 71.22212624549866, 71.87878799438477, 72.52905654907227, 73.17932510375977, 73.94038105010986, 74.70143699645996, 75.48434972763062, 76.26726245880127, 76.96146655082703, 77.65567064285278, 78.431396484375, 79.20712232589722, 79.93866801261902, 80.67021369934082, 81.3080575466156, 81.94590139389038, 82.58527874946594, 83.2246561050415, 83.91886711120605, 84.6130781173706, 85.31857800483704, 86.02407789230347, 86.7028865814209, 87.38169527053833, 88.17173218727112, 88.9617691040039, 89.72973942756653, 90.49770975112915, 91.23621106147766, 91.97471237182617, 92.75529432296753, 93.53587627410889, 94.30382990837097, 95.07178354263306, 95.78748631477356, 96.50318908691406, 97.15242838859558, 97.8016676902771, 98.4988043308258, 99.19594097137451, 99.86673045158386, 100.53751993179321, 101.19230937957764, 101.84709882736206, 102.50386691093445, 103.16063499450684, 103.95010232925415, 104.73956966400146, 105.51917362213135, 106.29877758026123, 107.0172872543335, 107.73579692840576, 108.48871994018555, 109.24164295196533, 109.9835159778595, 110.72538900375366, 111.37193632125854, 112.01848363876343, 112.66822385787964, 113.31796407699585, 113.99418187141418, 114.67039966583252, 115.36000514030457, 116.04961061477661, 116.70213580131531, 117.354660987854, 118.04990792274475, 118.7451548576355, 119.50442409515381, 120.26369333267212, 121.00605869293213, 121.74842405319214, 122.52426195144653, 123.30009984970093, 124.05315446853638, 124.80620908737183, 125.53796482086182, 126.2697205543518, 126.93245482444763, 127.59518909454346, 128.28989386558533, 128.9845986366272, 129.66201853752136, 130.33943843841553, 131.02797269821167, 131.7165069580078, 132.39578986167908, 133.07507276535034, 133.80414509773254, 134.53321743011475, 135.29569172859192, 136.0581660270691, 136.7747836112976, 137.49140119552612, 138.2419195175171, 138.99243783950806, 139.75752520561218, 140.5226125717163, 141.20147943496704, 141.88034629821777, 142.4986593723297, 143.11697244644165, 144.16948103904724, 145.22198963165283]
[15.166666666666666, 15.166666666666666, 28.75, 28.75, 49.8, 49.8, 27.8, 27.8, 50.18333333333333, 50.18333333333333, 61.63333333333333, 61.63333333333333, 70.75, 70.75, 76.31666666666666, 76.31666666666666, 80.38333333333334, 80.38333333333334, 84.21666666666667, 84.21666666666667, 88.75, 88.75, 90.8, 90.8, 91.33333333333333, 91.33333333333333, 93.65, 93.65, 94.7, 94.7, 94.88333333333334, 94.88333333333334, 95.23333333333333, 95.23333333333333, 95.45, 95.45, 95.45, 95.45, 95.86666666666666, 95.86666666666666, 95.85, 95.85, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.01666666666667, 96.15, 96.15, 96.1, 96.1, 96.15, 96.15, 96.3, 96.3, 96.23333333333333, 96.23333333333333, 96.31666666666666, 96.31666666666666, 96.31666666666666, 96.31666666666666, 96.45, 96.45, 96.51666666666667, 96.51666666666667, 96.53333333333333, 96.53333333333333, 96.61666666666666, 96.61666666666666, 96.65, 96.65, 96.73333333333333, 96.73333333333333, 96.66666666666667, 96.66666666666667, 96.7, 96.7, 96.75, 96.75, 96.73333333333333, 96.73333333333333, 96.81666666666666, 96.81666666666666, 96.88333333333334, 96.88333333333334, 96.85, 96.85, 96.81666666666666, 96.81666666666666, 96.83333333333333, 96.83333333333333, 96.96666666666667, 96.96666666666667, 96.93333333333334, 96.93333333333334, 96.8, 96.8, 97.01666666666667, 97.01666666666667, 96.91666666666667, 96.91666666666667, 96.91666666666667, 96.91666666666667, 96.86666666666666, 96.86666666666666, 96.98333333333333, 96.98333333333333, 97.01666666666667, 97.01666666666667, 97.11666666666666, 97.11666666666666, 97.08333333333333, 97.08333333333333, 97.16666666666667, 97.16666666666667, 97.23333333333333, 97.23333333333333, 97.08333333333333, 97.08333333333333, 97.18333333333334, 97.18333333333334, 97.15, 97.15, 97.08333333333333, 97.08333333333333, 97.25, 97.25, 97.01666666666667, 97.01666666666667, 97.2, 97.2, 97.21666666666667, 97.21666666666667, 97.23333333333333, 97.23333333333333, 97.08333333333333, 97.08333333333333, 97.1, 97.1, 97.1, 97.1, 97.08333333333333, 97.08333333333333, 97.13333333333334, 97.13333333333334, 97.23333333333333, 97.23333333333333, 97.16666666666667, 97.16666666666667, 97.15, 97.15, 96.96666666666667, 96.96666666666667, 97.18333333333334, 97.18333333333334, 97.11666666666666, 97.11666666666666, 97.18333333333334, 97.18333333333334, 97.08333333333333, 97.08333333333333, 97.31666666666666, 97.31666666666666, 97.18333333333334, 97.18333333333334, 97.21666666666667, 97.21666666666667, 97.15, 97.15, 97.13333333333334, 97.13333333333334, 97.16666666666667, 97.16666666666667, 97.16666666666667, 97.16666666666667, 97.08333333333333, 97.08333333333333, 97.16666666666667, 97.16666666666667, 97.2, 97.2, 97.28333333333333, 97.28333333333333, 97.21666666666667, 97.21666666666667, 97.15, 97.15, 97.18333333333334, 97.18333333333334, 97.18333333333334, 97.18333333333334, 97.13333333333334, 97.13333333333334, 97.21666666666667, 97.21666666666667, 97.35, 97.35, 97.3, 97.3, 97.15, 97.15, 97.21666666666667, 97.21666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.279, Test loss: 2.282, Test accuracy: 24.85
Round   0, Global train loss: 2.279, Global test loss: 2.300, Global test accuracy: 17.35
Round   1, Train loss: 2.184, Test loss: 2.206, Test accuracy: 31.97
Round   1, Global train loss: 2.184, Global test loss: 2.290, Global test accuracy: 18.33
Round   2, Train loss: 2.016, Test loss: 2.092, Test accuracy: 40.22
Round   2, Global train loss: 2.016, Global test loss: 2.297, Global test accuracy: 11.80
Round   3, Train loss: 1.885, Test loss: 1.975, Test accuracy: 53.75
Round   3, Global train loss: 1.885, Global test loss: 2.284, Global test accuracy: 15.83
Round   4, Train loss: 1.842, Test loss: 1.844, Test accuracy: 66.82
Round   4, Global train loss: 1.842, Global test loss: 2.265, Global test accuracy: 18.33
Round   5, Train loss: 1.622, Test loss: 1.802, Test accuracy: 69.98
Round   5, Global train loss: 1.622, Global test loss: 2.264, Global test accuracy: 18.33
Round   6, Train loss: 1.679, Test loss: 1.758, Test accuracy: 73.07
Round   6, Global train loss: 1.679, Global test loss: 2.264, Global test accuracy: 18.57
Round   7, Train loss: 1.593, Test loss: 1.750, Test accuracy: 73.65
Round   7, Global train loss: 1.593, Global test loss: 2.261, Global test accuracy: 18.33
Round   8, Train loss: 1.671, Test loss: 1.702, Test accuracy: 78.37
Round   8, Global train loss: 1.671, Global test loss: 2.273, Global test accuracy: 18.33
Round   9, Train loss: 1.586, Test loss: 1.707, Test accuracy: 77.40
Round   9, Global train loss: 1.586, Global test loss: 2.262, Global test accuracy: 17.23
Round  10, Train loss: 1.524, Test loss: 1.689, Test accuracy: 79.83
Round  10, Global train loss: 1.524, Global test loss: 2.278, Global test accuracy: 17.15
Round  11, Train loss: 1.617, Test loss: 1.675, Test accuracy: 81.30
Round  11, Global train loss: 1.617, Global test loss: 2.264, Global test accuracy: 17.03
Round  12, Train loss: 1.645, Test loss: 1.677, Test accuracy: 79.92
Round  12, Global train loss: 1.645, Global test loss: 2.313, Global test accuracy: 9.03
Round  13, Train loss: 1.531, Test loss: 1.654, Test accuracy: 81.98
Round  13, Global train loss: 1.531, Global test loss: 2.293, Global test accuracy: 12.07
Round  14, Train loss: 1.588, Test loss: 1.632, Test accuracy: 85.03
Round  14, Global train loss: 1.588, Global test loss: 2.276, Global test accuracy: 16.68
Round  15, Train loss: 1.488, Test loss: 1.627, Test accuracy: 84.43
Round  15, Global train loss: 1.488, Global test loss: 2.276, Global test accuracy: 16.13
Round  16, Train loss: 1.471, Test loss: 1.621, Test accuracy: 85.98
Round  16, Global train loss: 1.471, Global test loss: 2.264, Global test accuracy: 18.43
Round  17, Train loss: 1.631, Test loss: 1.619, Test accuracy: 85.97
Round  17, Global train loss: 1.631, Global test loss: 2.270, Global test accuracy: 18.35
Round  18, Train loss: 1.580, Test loss: 1.631, Test accuracy: 84.33
Round  18, Global train loss: 1.580, Global test loss: 2.285, Global test accuracy: 16.53
Round  19, Train loss: 1.521, Test loss: 1.624, Test accuracy: 84.73
Round  19, Global train loss: 1.521, Global test loss: 2.262, Global test accuracy: 16.25
Round  20, Train loss: 1.533, Test loss: 1.615, Test accuracy: 86.15
Round  20, Global train loss: 1.533, Global test loss: 2.259, Global test accuracy: 18.32
Round  21, Train loss: 1.605, Test loss: 1.601, Test accuracy: 87.30
Round  21, Global train loss: 1.605, Global test loss: 2.269, Global test accuracy: 18.33
Round  22, Train loss: 1.634, Test loss: 1.599, Test accuracy: 87.28
Round  22, Global train loss: 1.634, Global test loss: 2.264, Global test accuracy: 18.12
Round  23, Train loss: 1.646, Test loss: 1.596, Test accuracy: 87.35
Round  23, Global train loss: 1.646, Global test loss: 2.319, Global test accuracy: 11.67
Round  24, Train loss: 1.472, Test loss: 1.596, Test accuracy: 87.42
Round  24, Global train loss: 1.472, Global test loss: 2.267, Global test accuracy: 18.33
Round  25, Train loss: 1.580, Test loss: 1.596, Test accuracy: 87.37
Round  25, Global train loss: 1.580, Global test loss: 2.297, Global test accuracy: 9.90
Round  26, Train loss: 1.579, Test loss: 1.595, Test accuracy: 87.37
Round  26, Global train loss: 1.579, Global test loss: 2.297, Global test accuracy: 10.32
Round  27, Train loss: 1.519, Test loss: 1.595, Test accuracy: 87.37
Round  27, Global train loss: 1.519, Global test loss: 2.275, Global test accuracy: 15.60
Round  28, Train loss: 1.575, Test loss: 1.595, Test accuracy: 87.38
Round  28, Global train loss: 1.575, Global test loss: 2.267, Global test accuracy: 17.77
Round  29, Train loss: 1.589, Test loss: 1.589, Test accuracy: 87.73
Round  29, Global train loss: 1.589, Global test loss: 2.283, Global test accuracy: 17.25
Round  30, Train loss: 1.573, Test loss: 1.589, Test accuracy: 87.80
Round  30, Global train loss: 1.573, Global test loss: 2.280, Global test accuracy: 16.40
Round  31, Train loss: 1.637, Test loss: 1.586, Test accuracy: 87.98
Round  31, Global train loss: 1.637, Global test loss: 2.283, Global test accuracy: 13.88
Round  32, Train loss: 1.577, Test loss: 1.585, Test accuracy: 88.00
Round  32, Global train loss: 1.577, Global test loss: 2.297, Global test accuracy: 12.55
Round  33, Train loss: 1.574, Test loss: 1.585, Test accuracy: 88.02
Round  33, Global train loss: 1.574, Global test loss: 2.260, Global test accuracy: 17.87
Round  34, Train loss: 1.519, Test loss: 1.585, Test accuracy: 88.08
Round  34, Global train loss: 1.519, Global test loss: 2.280, Global test accuracy: 16.32
Round  35, Train loss: 1.518, Test loss: 1.585, Test accuracy: 88.08
Round  35, Global train loss: 1.518, Global test loss: 2.266, Global test accuracy: 18.33
Round  36, Train loss: 1.519, Test loss: 1.585, Test accuracy: 88.08
Round  36, Global train loss: 1.519, Global test loss: 2.268, Global test accuracy: 18.33
Round  37, Train loss: 1.582, Test loss: 1.555, Test accuracy: 91.32
Round  37, Global train loss: 1.582, Global test loss: 2.275, Global test accuracy: 16.82
Round  38, Train loss: 1.522, Test loss: 1.555, Test accuracy: 91.33
Round  38, Global train loss: 1.522, Global test loss: 2.310, Global test accuracy: 11.77
Round  39, Train loss: 1.472, Test loss: 1.553, Test accuracy: 91.38
Round  39, Global train loss: 1.472, Global test loss: 2.264, Global test accuracy: 17.88
Round  40, Train loss: 1.520, Test loss: 1.553, Test accuracy: 91.37
Round  40, Global train loss: 1.520, Global test loss: 2.263, Global test accuracy: 18.68
Round  41, Train loss: 1.469, Test loss: 1.553, Test accuracy: 91.38
Round  41, Global train loss: 1.469, Global test loss: 2.287, Global test accuracy: 14.70
Round  42, Train loss: 1.520, Test loss: 1.553, Test accuracy: 91.38
Round  42, Global train loss: 1.520, Global test loss: 2.259, Global test accuracy: 18.70
Round  43, Train loss: 1.522, Test loss: 1.552, Test accuracy: 91.43
Round  43, Global train loss: 1.522, Global test loss: 2.265, Global test accuracy: 16.95
Round  44, Train loss: 1.520, Test loss: 1.552, Test accuracy: 91.43
Round  44, Global train loss: 1.520, Global test loss: 2.303, Global test accuracy: 10.00
Round  45, Train loss: 1.554, Test loss: 1.542, Test accuracy: 92.58
Round  45, Global train loss: 1.554, Global test loss: 2.257, Global test accuracy: 18.73
Round  46, Train loss: 1.474, Test loss: 1.540, Test accuracy: 92.70
Round  46, Global train loss: 1.474, Global test loss: 2.269, Global test accuracy: 16.65
Round  47, Train loss: 1.467, Test loss: 1.540, Test accuracy: 92.68
Round  47, Global train loss: 1.467, Global test loss: 2.314, Global test accuracy: 11.78
Round  48, Train loss: 1.469, Test loss: 1.539, Test accuracy: 92.70
Round  48, Global train loss: 1.469, Global test loss: 2.272, Global test accuracy: 16.52
Round  49, Train loss: 1.465, Test loss: 1.539, Test accuracy: 92.70
Round  49, Global train loss: 1.465, Global test loss: 2.304, Global test accuracy: 12.33
Round  50, Train loss: 1.519, Test loss: 1.539, Test accuracy: 92.75
Round  50, Global train loss: 1.519, Global test loss: 2.274, Global test accuracy: 18.33
Round  51, Train loss: 1.572, Test loss: 1.539, Test accuracy: 92.75
Round  51, Global train loss: 1.572, Global test loss: 2.265, Global test accuracy: 17.23
Round  52, Train loss: 1.517, Test loss: 1.539, Test accuracy: 92.75
Round  52, Global train loss: 1.517, Global test loss: 2.270, Global test accuracy: 18.33
Round  53, Train loss: 1.466, Test loss: 1.539, Test accuracy: 92.72
Round  53, Global train loss: 1.466, Global test loss: 2.317, Global test accuracy: 11.67
Round  54, Train loss: 1.467, Test loss: 1.539, Test accuracy: 92.70
Round  54, Global train loss: 1.467, Global test loss: 2.278, Global test accuracy: 16.52
Round  55, Train loss: 1.466, Test loss: 1.539, Test accuracy: 92.70
Round  55, Global train loss: 1.466, Global test loss: 2.259, Global test accuracy: 18.58
Round  56, Train loss: 1.520, Test loss: 1.539, Test accuracy: 92.72
Round  56, Global train loss: 1.520, Global test loss: 2.307, Global test accuracy: 12.58
Round  57, Train loss: 1.466, Test loss: 1.539, Test accuracy: 92.72
Round  57, Global train loss: 1.466, Global test loss: 2.268, Global test accuracy: 18.33
Round  58, Train loss: 1.519, Test loss: 1.539, Test accuracy: 92.70
Round  58, Global train loss: 1.519, Global test loss: 2.267, Global test accuracy: 18.35
Round  59, Train loss: 1.465, Test loss: 1.539, Test accuracy: 92.70
Round  59, Global train loss: 1.465, Global test loss: 2.268, Global test accuracy: 17.27
Round  60, Train loss: 1.572, Test loss: 1.539, Test accuracy: 92.68
Round  60, Global train loss: 1.572, Global test loss: 2.317, Global test accuracy: 11.90
Round  61, Train loss: 1.467, Test loss: 1.538, Test accuracy: 92.70
Round  61, Global train loss: 1.467, Global test loss: 2.280, Global test accuracy: 15.47
Round  62, Train loss: 1.468, Test loss: 1.538, Test accuracy: 92.70
Round  62, Global train loss: 1.468, Global test loss: 2.301, Global test accuracy: 12.08
Round  63, Train loss: 1.518, Test loss: 1.538, Test accuracy: 92.72
Round  63, Global train loss: 1.518, Global test loss: 2.267, Global test accuracy: 18.32
Round  64, Train loss: 1.467, Test loss: 1.538, Test accuracy: 92.72
Round  64, Global train loss: 1.467, Global test loss: 2.267, Global test accuracy: 18.32
Round  65, Train loss: 1.519, Test loss: 1.538, Test accuracy: 92.70
Round  65, Global train loss: 1.519, Global test loss: 2.264, Global test accuracy: 18.28
Round  66, Train loss: 1.517, Test loss: 1.538, Test accuracy: 92.70
Round  66, Global train loss: 1.517, Global test loss: 2.276, Global test accuracy: 19.60
Round  67, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.67
Round  67, Global train loss: 1.464, Global test loss: 2.302, Global test accuracy: 11.37
Round  68, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.67
Round  68, Global train loss: 1.465, Global test loss: 2.296, Global test accuracy: 15.48
Round  69, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.67
Round  69, Global train loss: 1.464, Global test loss: 2.268, Global test accuracy: 16.37
Round  70, Train loss: 1.572, Test loss: 1.538, Test accuracy: 92.65
Round  70, Global train loss: 1.572, Global test loss: 2.305, Global test accuracy: 12.23
Round  71, Train loss: 1.518, Test loss: 1.538, Test accuracy: 92.63
Round  71, Global train loss: 1.518, Global test loss: 2.263, Global test accuracy: 17.48
Round  72, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.60
Round  72, Global train loss: 1.464, Global test loss: 2.314, Global test accuracy: 10.00
Round  73, Train loss: 1.519, Test loss: 1.538, Test accuracy: 92.58
Round  73, Global train loss: 1.519, Global test loss: 2.258, Global test accuracy: 18.53
Round  74, Train loss: 1.467, Test loss: 1.538, Test accuracy: 92.58
Round  74, Global train loss: 1.467, Global test loss: 2.326, Global test accuracy: 10.35
Round  75, Train loss: 1.466, Test loss: 1.538, Test accuracy: 92.57
Round  75, Global train loss: 1.466, Global test loss: 2.260, Global test accuracy: 18.07
Round  76, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.57
Round  76, Global train loss: 1.464, Global test loss: 2.292, Global test accuracy: 13.07
Round  77, Train loss: 1.466, Test loss: 1.538, Test accuracy: 92.57
Round  77, Global train loss: 1.466, Global test loss: 2.266, Global test accuracy: 18.57
Round  78, Train loss: 1.519, Test loss: 1.538, Test accuracy: 92.58
Round  78, Global train loss: 1.519, Global test loss: 2.284, Global test accuracy: 15.98
Round  79, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.62
Round  79, Global train loss: 1.465, Global test loss: 2.303, Global test accuracy: 12.43
Round  80, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.62
Round  80, Global train loss: 1.464, Global test loss: 2.286, Global test accuracy: 14.27
Round  81, Train loss: 1.517, Test loss: 1.538, Test accuracy: 92.60
Round  81, Global train loss: 1.517, Global test loss: 2.306, Global test accuracy: 11.28
Round  82, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.62
Round  82, Global train loss: 1.464, Global test loss: 2.262, Global test accuracy: 18.32
Round  83, Train loss: 1.519, Test loss: 1.538, Test accuracy: 92.62
Round  83, Global train loss: 1.519, Global test loss: 2.260, Global test accuracy: 17.00
Round  84, Train loss: 1.463, Test loss: 1.538, Test accuracy: 92.62
Round  84, Global train loss: 1.463, Global test loss: 2.271, Global test accuracy: 18.33
Round  85, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.62
Round  85, Global train loss: 1.464, Global test loss: 2.283, Global test accuracy: 16.00
Round  86, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.60
Round  86, Global train loss: 1.465, Global test loss: 2.266, Global test accuracy: 16.73
Round  87, Train loss: 1.465, Test loss: 1.537, Test accuracy: 92.58
Round  87, Global train loss: 1.465, Global test loss: 2.278, Global test accuracy: 17.38
Round  88, Train loss: 1.465, Test loss: 1.537, Test accuracy: 92.58
Round  88, Global train loss: 1.465, Global test loss: 2.278, Global test accuracy: 14.73
Round  89, Train loss: 1.517, Test loss: 1.537, Test accuracy: 92.58
Round  89, Global train loss: 1.517, Global test loss: 2.270, Global test accuracy: 18.33
Round  90, Train loss: 1.518, Test loss: 1.537, Test accuracy: 92.58
Round  90, Global train loss: 1.518, Global test loss: 2.287, Global test accuracy: 13.15
Round  91, Train loss: 1.518, Test loss: 1.537, Test accuracy: 92.57
Round  91, Global train loss: 1.518, Global test loss: 2.300, Global test accuracy: 11.70
Round  92, Train loss: 1.519, Test loss: 1.537, Test accuracy: 92.60
Round  92, Global train loss: 1.519, Global test loss: 2.288, Global test accuracy: 15.28
Round  93, Train loss: 1.465, Test loss: 1.537, Test accuracy: 92.60
Round  93, Global train loss: 1.465, Global test loss: 2.267, Global test accuracy: 18.33
Round  94, Train loss: 1.464, Test loss: 1.537, Test accuracy: 92.60
Round  94, Global train loss: 1.464, Global test loss: 2.265, Global test accuracy: 16.43
Round  95, Train loss: 1.518, Test loss: 1.537, Test accuracy: 92.58
Round  95, Global train loss: 1.518, Global test loss: 2.266, Global test accuracy: 16.87/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.463, Test loss: 1.537, Test accuracy: 92.58
Round  96, Global train loss: 1.463, Global test loss: 2.300, Global test accuracy: 12.47
Round  97, Train loss: 1.516, Test loss: 1.537, Test accuracy: 92.57
Round  97, Global train loss: 1.516, Global test loss: 2.259, Global test accuracy: 19.10
Round  98, Train loss: 1.516, Test loss: 1.537, Test accuracy: 92.57
Round  98, Global train loss: 1.516, Global test loss: 2.276, Global test accuracy: 18.33
Round  99, Train loss: 1.519, Test loss: 1.537, Test accuracy: 92.58
Round  99, Global train loss: 1.519, Global test loss: 2.291, Global test accuracy: 14.38
Final Round, Train loss: 1.497, Test loss: 1.537, Test accuracy: 92.58
Final Round, Global train loss: 1.497, Global test loss: 2.291, Global test accuracy: 14.38
Average accuracy final 10 rounds: 92.58333333333331 

Average global accuracy final 10 rounds: 15.605 

911.9816455841064
[0.6483592987060547, 1.2967185974121094, 1.8915104866027832, 2.486302375793457, 3.05433988571167, 3.622377395629883, 4.201698541641235, 4.781019687652588, 5.335315942764282, 5.889612197875977, 6.369670867919922, 6.849729537963867, 7.399146318435669, 7.948563098907471, 8.48806118965149, 9.027559280395508, 9.534571647644043, 10.041584014892578, 10.582987070083618, 11.124390125274658, 11.683374404907227, 12.242358684539795, 12.773515701293945, 13.304672718048096, 13.905323266983032, 14.505973815917969, 15.072979927062988, 15.639986038208008, 16.15988254547119, 16.679779052734375, 17.186196327209473, 17.69261360168457, 18.250983476638794, 18.809353351593018, 19.35218048095703, 19.895007610321045, 20.401654481887817, 20.90830135345459, 21.50482726097107, 22.10135316848755, 22.661250352859497, 23.221147537231445, 23.812150716781616, 24.403153896331787, 25.009509325027466, 25.615864753723145, 26.154361486434937, 26.69285821914673, 27.21420121192932, 27.735544204711914, 28.28326678276062, 28.830989360809326, 29.341508388519287, 29.852027416229248, 30.38475203514099, 30.917476654052734, 31.5219566822052, 32.126436710357666, 32.68174624443054, 33.23705577850342, 33.83536195755005, 34.43366813659668, 34.99116826057434, 35.548668384552, 36.12797403335571, 36.707279682159424, 37.231969118118286, 37.75665855407715, 38.26915431022644, 38.78165006637573, 39.340656757354736, 39.89966344833374, 40.45066022872925, 41.001657009124756, 41.521650314331055, 42.04164361953735, 42.63830351829529, 43.23496341705322, 43.82670879364014, 44.41845417022705, 44.9953031539917, 45.57215213775635, 46.20937395095825, 46.846595764160156, 47.406187772750854, 47.96577978134155, 48.48967170715332, 49.01356363296509, 49.56788086891174, 50.1221981048584, 50.6362669467926, 51.15033578872681, 51.683584690093994, 52.21683359146118, 52.81522583961487, 53.413618087768555, 53.99264407157898, 54.571670055389404, 55.16557264328003, 55.759475231170654, 56.371896505355835, 56.984317779541016, 57.53449487686157, 58.08467197418213, 58.6304190158844, 59.17616605758667, 59.715410470962524, 60.25465488433838, 60.79295802116394, 61.3312611579895, 61.876633644104004, 62.422006130218506, 62.96103382110596, 63.50006151199341, 64.0602765083313, 64.62049150466919, 65.19422745704651, 65.76796340942383, 66.36867070198059, 66.96937799453735, 67.53304529190063, 68.09671258926392, 68.60842895507812, 69.12014532089233, 69.6667366027832, 70.21332788467407, 70.77217888832092, 71.33102989196777, 71.8549268245697, 72.37882375717163, 72.95218324661255, 73.52554273605347, 74.1397602558136, 74.75397777557373, 75.2827353477478, 75.81149291992188, 76.404128074646, 76.99676322937012, 77.57014679908752, 78.14353036880493, 78.69581604003906, 79.2481017112732, 79.77701258659363, 80.30592346191406, 80.82570719718933, 81.3454909324646, 81.86950039863586, 82.39350986480713, 82.93243074417114, 83.47135162353516, 84.03300428390503, 84.5946569442749, 85.14166069030762, 85.68866443634033, 86.26962876319885, 86.85059309005737, 87.44226050376892, 88.03392791748047, 88.59069633483887, 89.14746475219727, 89.67222571372986, 90.19698667526245, 90.73989462852478, 91.28280258178711, 91.79227066040039, 92.30173873901367, 92.79811000823975, 93.29448127746582, 93.84248638153076, 94.3904914855957, 94.97827291488647, 95.56605434417725, 96.12148451805115, 96.67691469192505, 97.26318883895874, 97.84946298599243, 98.44762992858887, 99.0457968711853, 99.58931422233582, 100.13283157348633, 100.67300772666931, 101.2131838798523, 101.73707246780396, 102.26096105575562, 102.78187823295593, 103.30279541015625, 103.85302495956421, 104.40325450897217, 104.98103547096252, 105.55881643295288, 106.12861251831055, 106.69840860366821, 107.27033472061157, 107.84226083755493, 108.41822528839111, 108.9941897392273, 109.56829166412354, 110.14239358901978, 110.66935110092163, 111.19630861282349, 112.27934312820435, 113.3623776435852]
[24.85, 24.85, 31.966666666666665, 31.966666666666665, 40.21666666666667, 40.21666666666667, 53.75, 53.75, 66.81666666666666, 66.81666666666666, 69.98333333333333, 69.98333333333333, 73.06666666666666, 73.06666666666666, 73.65, 73.65, 78.36666666666666, 78.36666666666666, 77.4, 77.4, 79.83333333333333, 79.83333333333333, 81.3, 81.3, 79.91666666666667, 79.91666666666667, 81.98333333333333, 81.98333333333333, 85.03333333333333, 85.03333333333333, 84.43333333333334, 84.43333333333334, 85.98333333333333, 85.98333333333333, 85.96666666666667, 85.96666666666667, 84.33333333333333, 84.33333333333333, 84.73333333333333, 84.73333333333333, 86.15, 86.15, 87.3, 87.3, 87.28333333333333, 87.28333333333333, 87.35, 87.35, 87.41666666666667, 87.41666666666667, 87.36666666666666, 87.36666666666666, 87.36666666666666, 87.36666666666666, 87.36666666666666, 87.36666666666666, 87.38333333333334, 87.38333333333334, 87.73333333333333, 87.73333333333333, 87.8, 87.8, 87.98333333333333, 87.98333333333333, 88.0, 88.0, 88.01666666666667, 88.01666666666667, 88.08333333333333, 88.08333333333333, 88.08333333333333, 88.08333333333333, 88.08333333333333, 88.08333333333333, 91.31666666666666, 91.31666666666666, 91.33333333333333, 91.33333333333333, 91.38333333333334, 91.38333333333334, 91.36666666666666, 91.36666666666666, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.38333333333334, 91.43333333333334, 91.43333333333334, 91.43333333333334, 91.43333333333334, 92.58333333333333, 92.58333333333333, 92.7, 92.7, 92.68333333333334, 92.68333333333334, 92.7, 92.7, 92.7, 92.7, 92.75, 92.75, 92.75, 92.75, 92.75, 92.75, 92.71666666666667, 92.71666666666667, 92.7, 92.7, 92.7, 92.7, 92.71666666666667, 92.71666666666667, 92.71666666666667, 92.71666666666667, 92.7, 92.7, 92.7, 92.7, 92.68333333333334, 92.68333333333334, 92.7, 92.7, 92.7, 92.7, 92.71666666666667, 92.71666666666667, 92.71666666666667, 92.71666666666667, 92.7, 92.7, 92.7, 92.7, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.66666666666667, 92.65, 92.65, 92.63333333333334, 92.63333333333334, 92.6, 92.6, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.58333333333333, 92.58333333333333, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.6, 92.6, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.6, 92.6, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.56666666666666, 92.56666666666666, 92.6, 92.6, 92.6, 92.6, 92.6, 92.6, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.288, Test loss: 2.292, Test accuracy: 19.83
Round   0, Global train loss: 2.288, Global test loss: 2.301, Global test accuracy: 11.62
Round   1, Train loss: 2.272, Test loss: 2.258, Test accuracy: 30.22
Round   1, Global train loss: 2.272, Global test loss: 2.296, Global test accuracy: 15.07
Round   2, Train loss: 2.182, Test loss: 2.175, Test accuracy: 38.45
Round   2, Global train loss: 2.182, Global test loss: 2.284, Global test accuracy: 15.00
Round   3, Train loss: 1.943, Test loss: 2.007, Test accuracy: 53.58
Round   3, Global train loss: 1.943, Global test loss: 2.274, Global test accuracy: 13.88
Round   4, Train loss: 1.867, Test loss: 1.896, Test accuracy: 64.58
Round   4, Global train loss: 1.867, Global test loss: 2.266, Global test accuracy: 17.92
Round   5, Train loss: 1.812, Test loss: 1.804, Test accuracy: 70.97
Round   5, Global train loss: 1.812, Global test loss: 2.248, Global test accuracy: 20.43
Round   6, Train loss: 1.819, Test loss: 1.785, Test accuracy: 71.67
Round   6, Global train loss: 1.819, Global test loss: 2.276, Global test accuracy: 16.72
Round   7, Train loss: 1.756, Test loss: 1.741, Test accuracy: 74.62
Round   7, Global train loss: 1.756, Global test loss: 2.252, Global test accuracy: 19.28
Round   8, Train loss: 1.750, Test loss: 1.681, Test accuracy: 80.22
Round   8, Global train loss: 1.750, Global test loss: 2.247, Global test accuracy: 18.38
Round   9, Train loss: 1.770, Test loss: 1.679, Test accuracy: 80.20
Round   9, Global train loss: 1.770, Global test loss: 2.248, Global test accuracy: 17.87
Round  10, Train loss: 1.747, Test loss: 1.692, Test accuracy: 78.67
Round  10, Global train loss: 1.747, Global test loss: 2.240, Global test accuracy: 20.13
Round  11, Train loss: 1.696, Test loss: 1.696, Test accuracy: 78.22
Round  11, Global train loss: 1.696, Global test loss: 2.248, Global test accuracy: 19.37
Round  12, Train loss: 1.701, Test loss: 1.698, Test accuracy: 77.37
Round  12, Global train loss: 1.701, Global test loss: 2.239, Global test accuracy: 21.40
Round  13, Train loss: 1.764, Test loss: 1.696, Test accuracy: 77.50
Round  13, Global train loss: 1.764, Global test loss: 2.279, Global test accuracy: 17.10
Round  14, Train loss: 1.750, Test loss: 1.696, Test accuracy: 77.40
Round  14, Global train loss: 1.750, Global test loss: 2.267, Global test accuracy: 15.55
Round  15, Train loss: 1.782, Test loss: 1.696, Test accuracy: 77.30
Round  15, Global train loss: 1.782, Global test loss: 2.231, Global test accuracy: 21.88
Round  16, Train loss: 1.685, Test loss: 1.704, Test accuracy: 76.40
Round  16, Global train loss: 1.685, Global test loss: 2.229, Global test accuracy: 22.27
Round  17, Train loss: 1.737, Test loss: 1.705, Test accuracy: 76.00
Round  17, Global train loss: 1.737, Global test loss: 2.260, Global test accuracy: 18.25
Round  18, Train loss: 1.765, Test loss: 1.717, Test accuracy: 74.62
Round  18, Global train loss: 1.765, Global test loss: 2.236, Global test accuracy: 21.38
Round  19, Train loss: 1.732, Test loss: 1.715, Test accuracy: 74.88
Round  19, Global train loss: 1.732, Global test loss: 2.238, Global test accuracy: 19.60
Round  20, Train loss: 1.629, Test loss: 1.715, Test accuracy: 74.95
Round  20, Global train loss: 1.629, Global test loss: 2.263, Global test accuracy: 18.65
Round  21, Train loss: 1.823, Test loss: 1.732, Test accuracy: 73.27
Round  21, Global train loss: 1.823, Global test loss: 2.256, Global test accuracy: 19.30
Round  22, Train loss: 1.787, Test loss: 1.747, Test accuracy: 71.62
Round  22, Global train loss: 1.787, Global test loss: 2.250, Global test accuracy: 19.65
Round  23, Train loss: 1.740, Test loss: 1.746, Test accuracy: 71.67
Round  23, Global train loss: 1.740, Global test loss: 2.225, Global test accuracy: 22.57
Round  24, Train loss: 1.890, Test loss: 1.760, Test accuracy: 70.35
Round  24, Global train loss: 1.890, Global test loss: 2.231, Global test accuracy: 21.68
Round  25, Train loss: 1.568, Test loss: 1.760, Test accuracy: 70.47
Round  25, Global train loss: 1.568, Global test loss: 2.229, Global test accuracy: 21.53
Round  26, Train loss: 1.740, Test loss: 1.723, Test accuracy: 74.37
Round  26, Global train loss: 1.740, Global test loss: 2.215, Global test accuracy: 23.55
Round  27, Train loss: 1.692, Test loss: 1.708, Test accuracy: 75.70
Round  27, Global train loss: 1.692, Global test loss: 2.230, Global test accuracy: 22.12
Round  28, Train loss: 1.678, Test loss: 1.709, Test accuracy: 75.57
Round  28, Global train loss: 1.678, Global test loss: 2.239, Global test accuracy: 19.78
Round  29, Train loss: 1.651, Test loss: 1.706, Test accuracy: 75.80
Round  29, Global train loss: 1.651, Global test loss: 2.223, Global test accuracy: 22.37
Round  30, Train loss: 1.626, Test loss: 1.705, Test accuracy: 75.93
Round  30, Global train loss: 1.626, Global test loss: 2.241, Global test accuracy: 19.65
Round  31, Train loss: 1.674, Test loss: 1.706, Test accuracy: 75.93
Round  31, Global train loss: 1.674, Global test loss: 2.241, Global test accuracy: 19.82
Round  32, Train loss: 1.816, Test loss: 1.718, Test accuracy: 74.48
Round  32, Global train loss: 1.816, Global test loss: 2.231, Global test accuracy: 22.33
Round  33, Train loss: 1.728, Test loss: 1.732, Test accuracy: 73.07
Round  33, Global train loss: 1.728, Global test loss: 2.220, Global test accuracy: 23.03
Round  34, Train loss: 1.731, Test loss: 1.733, Test accuracy: 72.92
Round  34, Global train loss: 1.731, Global test loss: 2.218, Global test accuracy: 23.13
Round  35, Train loss: 1.731, Test loss: 1.732, Test accuracy: 73.03
Round  35, Global train loss: 1.731, Global test loss: 2.231, Global test accuracy: 21.52
Round  36, Train loss: 1.714, Test loss: 1.716, Test accuracy: 74.58
Round  36, Global train loss: 1.714, Global test loss: 2.243, Global test accuracy: 19.43
Round  37, Train loss: 1.687, Test loss: 1.701, Test accuracy: 76.18
Round  37, Global train loss: 1.687, Global test loss: 2.235, Global test accuracy: 21.72
Round  38, Train loss: 1.663, Test loss: 1.701, Test accuracy: 76.12
Round  38, Global train loss: 1.663, Global test loss: 2.237, Global test accuracy: 20.10
Round  39, Train loss: 1.702, Test loss: 1.685, Test accuracy: 77.80
Round  39, Global train loss: 1.702, Global test loss: 2.220, Global test accuracy: 22.38
Round  40, Train loss: 1.624, Test loss: 1.685, Test accuracy: 77.88
Round  40, Global train loss: 1.624, Global test loss: 2.235, Global test accuracy: 20.53
Round  41, Train loss: 1.633, Test loss: 1.685, Test accuracy: 77.77
Round  41, Global train loss: 1.633, Global test loss: 2.217, Global test accuracy: 21.98
Round  42, Train loss: 1.707, Test loss: 1.685, Test accuracy: 77.83
Round  42, Global train loss: 1.707, Global test loss: 2.219, Global test accuracy: 22.67
Round  43, Train loss: 1.673, Test loss: 1.684, Test accuracy: 77.92
Round  43, Global train loss: 1.673, Global test loss: 2.234, Global test accuracy: 21.15
Round  44, Train loss: 1.656, Test loss: 1.685, Test accuracy: 77.80
Round  44, Global train loss: 1.656, Global test loss: 2.227, Global test accuracy: 21.50
Round  45, Train loss: 1.709, Test loss: 1.685, Test accuracy: 77.78
Round  45, Global train loss: 1.709, Global test loss: 2.202, Global test accuracy: 25.05
Round  46, Train loss: 1.600, Test loss: 1.684, Test accuracy: 77.88
Round  46, Global train loss: 1.600, Global test loss: 2.246, Global test accuracy: 20.10
Round  47, Train loss: 1.696, Test loss: 1.684, Test accuracy: 77.90
Round  47, Global train loss: 1.696, Global test loss: 2.211, Global test accuracy: 23.42
Round  48, Train loss: 1.712, Test loss: 1.700, Test accuracy: 76.20
Round  48, Global train loss: 1.712, Global test loss: 2.231, Global test accuracy: 20.83
Round  49, Train loss: 1.633, Test loss: 1.700, Test accuracy: 76.20
Round  49, Global train loss: 1.633, Global test loss: 2.231, Global test accuracy: 20.92
Round  50, Train loss: 1.706, Test loss: 1.713, Test accuracy: 74.90
Round  50, Global train loss: 1.706, Global test loss: 2.227, Global test accuracy: 22.13
Round  51, Train loss: 1.711, Test loss: 1.712, Test accuracy: 74.97
Round  51, Global train loss: 1.711, Global test loss: 2.222, Global test accuracy: 22.63
Round  52, Train loss: 1.767, Test loss: 1.711, Test accuracy: 74.98
Round  52, Global train loss: 1.767, Global test loss: 2.201, Global test accuracy: 24.98
Round  53, Train loss: 1.648, Test loss: 1.696, Test accuracy: 76.53
Round  53, Global train loss: 1.648, Global test loss: 2.221, Global test accuracy: 22.98
Round  54, Train loss: 1.734, Test loss: 1.699, Test accuracy: 76.38
Round  54, Global train loss: 1.734, Global test loss: 2.202, Global test accuracy: 24.92
Round  55, Train loss: 1.727, Test loss: 1.683, Test accuracy: 77.98
Round  55, Global train loss: 1.727, Global test loss: 2.201, Global test accuracy: 24.53
Round  56, Train loss: 1.701, Test loss: 1.673, Test accuracy: 79.00
Round  56, Global train loss: 1.701, Global test loss: 2.233, Global test accuracy: 20.83
Round  57, Train loss: 1.711, Test loss: 1.674, Test accuracy: 78.83
Round  57, Global train loss: 1.711, Global test loss: 2.239, Global test accuracy: 21.07
Round  58, Train loss: 1.755, Test loss: 1.675, Test accuracy: 78.73
Round  58, Global train loss: 1.755, Global test loss: 2.234, Global test accuracy: 20.68
Round  59, Train loss: 1.784, Test loss: 1.687, Test accuracy: 77.35
Round  59, Global train loss: 1.784, Global test loss: 2.208, Global test accuracy: 23.53
Round  60, Train loss: 1.642, Test loss: 1.675, Test accuracy: 78.63
Round  60, Global train loss: 1.642, Global test loss: 2.217, Global test accuracy: 23.07
Round  61, Train loss: 1.654, Test loss: 1.689, Test accuracy: 77.13
Round  61, Global train loss: 1.654, Global test loss: 2.224, Global test accuracy: 21.63
Round  62, Train loss: 1.626, Test loss: 1.672, Test accuracy: 79.08
Round  62, Global train loss: 1.626, Global test loss: 2.251, Global test accuracy: 19.47
Round  63, Train loss: 1.727, Test loss: 1.671, Test accuracy: 79.18
Round  63, Global train loss: 1.727, Global test loss: 2.234, Global test accuracy: 21.13
Round  64, Train loss: 1.759, Test loss: 1.687, Test accuracy: 77.60
Round  64, Global train loss: 1.759, Global test loss: 2.234, Global test accuracy: 21.42
Round  65, Train loss: 1.659, Test loss: 1.711, Test accuracy: 75.03
Round  65, Global train loss: 1.659, Global test loss: 2.222, Global test accuracy: 22.03
Round  66, Train loss: 1.636, Test loss: 1.697, Test accuracy: 76.42
Round  66, Global train loss: 1.636, Global test loss: 2.196, Global test accuracy: 25.43
Round  67, Train loss: 1.553, Test loss: 1.685, Test accuracy: 77.83
Round  67, Global train loss: 1.553, Global test loss: 2.218, Global test accuracy: 22.85
Round  68, Train loss: 1.652, Test loss: 1.684, Test accuracy: 77.90
Round  68, Global train loss: 1.652, Global test loss: 2.222, Global test accuracy: 22.70
Round  69, Train loss: 1.646, Test loss: 1.695, Test accuracy: 76.73
Round  69, Global train loss: 1.646, Global test loss: 2.209, Global test accuracy: 23.87
Round  70, Train loss: 1.655, Test loss: 1.695, Test accuracy: 76.72
Round  70, Global train loss: 1.655, Global test loss: 2.201, Global test accuracy: 24.72
Round  71, Train loss: 1.604, Test loss: 1.679, Test accuracy: 78.25
Round  71, Global train loss: 1.604, Global test loss: 2.215, Global test accuracy: 22.72
Round  72, Train loss: 1.712, Test loss: 1.680, Test accuracy: 78.13
Round  72, Global train loss: 1.712, Global test loss: 2.204, Global test accuracy: 24.12
Round  73, Train loss: 1.549, Test loss: 1.680, Test accuracy: 78.17
Round  73, Global train loss: 1.549, Global test loss: 2.229, Global test accuracy: 22.12
Round  74, Train loss: 1.611, Test loss: 1.680, Test accuracy: 78.30
Round  74, Global train loss: 1.611, Global test loss: 2.204, Global test accuracy: 24.32
Round  75, Train loss: 1.756, Test loss: 1.679, Test accuracy: 78.40
Round  75, Global train loss: 1.756, Global test loss: 2.206, Global test accuracy: 24.77
Round  76, Train loss: 1.602, Test loss: 1.678, Test accuracy: 78.48
Round  76, Global train loss: 1.602, Global test loss: 2.221, Global test accuracy: 22.67
Round  77, Train loss: 1.773, Test loss: 1.681, Test accuracy: 78.05
Round  77, Global train loss: 1.773, Global test loss: 2.204, Global test accuracy: 24.45
Round  78, Train loss: 1.708, Test loss: 1.682, Test accuracy: 78.05
Round  78, Global train loss: 1.708, Global test loss: 2.205, Global test accuracy: 24.57
Round  79, Train loss: 1.809, Test loss: 1.697, Test accuracy: 76.42
Round  79, Global train loss: 1.809, Global test loss: 2.210, Global test accuracy: 23.75
Round  80, Train loss: 1.646, Test loss: 1.698, Test accuracy: 76.35
Round  80, Global train loss: 1.646, Global test loss: 2.217, Global test accuracy: 22.95
Round  81, Train loss: 1.746, Test loss: 1.699, Test accuracy: 76.22
Round  81, Global train loss: 1.746, Global test loss: 2.210, Global test accuracy: 23.43
Round  82, Train loss: 1.745, Test loss: 1.683, Test accuracy: 77.78
Round  82, Global train loss: 1.745, Global test loss: 2.211, Global test accuracy: 23.05
Round  83, Train loss: 1.644, Test loss: 1.683, Test accuracy: 77.83
Round  83, Global train loss: 1.644, Global test loss: 2.222, Global test accuracy: 22.42
Round  84, Train loss: 1.620, Test loss: 1.671, Test accuracy: 79.17
Round  84, Global train loss: 1.620, Global test loss: 2.194, Global test accuracy: 25.72
Round  85, Train loss: 1.716, Test loss: 1.670, Test accuracy: 79.23
Round  85, Global train loss: 1.716, Global test loss: 2.209, Global test accuracy: 24.15
Round  86, Train loss: 1.598, Test loss: 1.685, Test accuracy: 77.70
Round  86, Global train loss: 1.598, Global test loss: 2.213, Global test accuracy: 23.22
Round  87, Train loss: 1.685, Test loss: 1.683, Test accuracy: 77.85
Round  87, Global train loss: 1.685, Global test loss: 2.216, Global test accuracy: 23.23
Round  88, Train loss: 1.656, Test loss: 1.681, Test accuracy: 77.98
Round  88, Global train loss: 1.656, Global test loss: 2.211, Global test accuracy: 23.07
Round  89, Train loss: 1.678, Test loss: 1.667, Test accuracy: 79.53
Round  89, Global train loss: 1.678, Global test loss: 2.197, Global test accuracy: 24.88
Round  90, Train loss: 1.646, Test loss: 1.667, Test accuracy: 79.65
Round  90, Global train loss: 1.646, Global test loss: 2.190, Global test accuracy: 25.90
Round  91, Train loss: 1.599, Test loss: 1.666, Test accuracy: 79.65
Round  91, Global train loss: 1.599, Global test loss: 2.219, Global test accuracy: 23.40
Round  92, Train loss: 1.662, Test loss: 1.667, Test accuracy: 79.58
Round  92, Global train loss: 1.662, Global test loss: 2.204, Global test accuracy: 24.92
Round  93, Train loss: 1.657, Test loss: 1.667, Test accuracy: 79.60
Round  93, Global train loss: 1.657, Global test loss: 2.211, Global test accuracy: 23.18
Round  94, Train loss: 1.717, Test loss: 1.667, Test accuracy: 79.52
Round  94, Global train loss: 1.717, Global test loss: 2.201, Global test accuracy: 24.93
Round  95, Train loss: 1.650, Test loss: 1.667, Test accuracy: 79.45
Round  95, Global train loss: 1.650, Global test loss: 2.222, Global test accuracy: 22.47/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.623, Test loss: 1.668, Test accuracy: 79.38
Round  96, Global train loss: 1.623, Global test loss: 2.197, Global test accuracy: 25.55
Round  97, Train loss: 1.716, Test loss: 1.668, Test accuracy: 79.45
Round  97, Global train loss: 1.716, Global test loss: 2.226, Global test accuracy: 21.73
Round  98, Train loss: 1.660, Test loss: 1.668, Test accuracy: 79.40
Round  98, Global train loss: 1.660, Global test loss: 2.213, Global test accuracy: 23.82
Round  99, Train loss: 1.651, Test loss: 1.668, Test accuracy: 79.40
Round  99, Global train loss: 1.651, Global test loss: 2.216, Global test accuracy: 22.90
Final Round, Train loss: 1.657, Test loss: 1.677, Test accuracy: 78.37
Final Round, Global train loss: 1.657, Global test loss: 2.216, Global test accuracy: 22.90
Average accuracy final 10 rounds: 79.50833333333334 

Average global accuracy final 10 rounds: 23.88 

919.7583577632904
[0.6585397720336914, 1.3170795440673828, 1.8944387435913086, 2.4717979431152344, 3.079491138458252, 3.6871843338012695, 4.321675062179565, 4.956165790557861, 5.512558221817017, 6.068950653076172, 6.627550363540649, 7.186150074005127, 7.7188451290130615, 8.251540184020996, 8.785386085510254, 9.319231986999512, 9.872447729110718, 10.425663471221924, 10.990492105484009, 11.555320739746094, 12.133891344070435, 12.712461948394775, 13.325555086135864, 13.938648223876953, 14.552612781524658, 15.166577339172363, 15.720282554626465, 16.273987770080566, 16.85907816886902, 17.44416856765747, 17.995296478271484, 18.546424388885498, 19.10834050178528, 19.67025661468506, 20.209424018859863, 20.748591423034668, 21.281887531280518, 21.815183639526367, 22.361647367477417, 22.908111095428467, 23.47671866416931, 24.045326232910156, 24.651243209838867, 25.257160186767578, 25.8175106048584, 26.37786102294922, 26.942453622817993, 27.507046222686768, 28.040764093399048, 28.574481964111328, 29.083370685577393, 29.592259407043457, 30.114004135131836, 30.635748863220215, 31.15311884880066, 31.670488834381104, 32.23250102996826, 32.79451322555542, 33.381998777389526, 33.96948432922363, 34.54388427734375, 35.11828422546387, 35.705875396728516, 36.293466567993164, 36.85222911834717, 37.41099166870117, 37.94182872772217, 38.472665786743164, 39.005120038986206, 39.53757429122925, 40.07565927505493, 40.613744258880615, 41.166202783584595, 41.718661308288574, 42.270517110824585, 42.822372913360596, 43.339563846588135, 43.856754779815674, 44.46284008026123, 45.06892538070679, 45.688193559646606, 46.307461738586426, 46.88481402397156, 47.46216630935669, 48.04447293281555, 48.626779556274414, 49.18017101287842, 49.73356246948242, 50.23574423789978, 50.73792600631714, 51.28775334358215, 51.83758068084717, 52.373151540756226, 52.90872240066528, 53.47829604148865, 54.04786968231201, 54.645076751708984, 55.24228382110596, 55.83987069129944, 56.43745756149292, 57.02857995033264, 57.61970233917236, 58.19300127029419, 58.766300201416016, 59.282780170440674, 59.79926013946533, 60.341277837753296, 60.88329553604126, 61.40564799308777, 61.92800045013428, 62.45155382156372, 62.975107192993164, 63.55649209022522, 64.13787698745728, 64.69715118408203, 65.25642538070679, 65.82276439666748, 66.38910341262817, 66.97353386878967, 67.55796432495117, 68.12657308578491, 68.69518184661865, 69.24586725234985, 69.79655265808105, 70.31845569610596, 70.84035873413086, 71.34437966346741, 71.84840059280396, 72.41019415855408, 72.9719877243042, 73.52282619476318, 74.07366466522217, 74.64254999160767, 75.21143531799316, 75.83882856369019, 76.4662218093872, 77.07533240318298, 77.68444299697876, 78.24722194671631, 78.81000089645386, 79.36226606369019, 79.91453123092651, 80.44337177276611, 80.97221231460571, 81.5133945941925, 82.0545768737793, 82.60553622245789, 83.15649557113647, 83.68558764457703, 84.21467971801758, 84.79248785972595, 85.37029600143433, 85.94167399406433, 86.51305198669434, 87.0924334526062, 87.67181491851807, 88.2325427532196, 88.79327058792114, 89.34254622459412, 89.89182186126709, 90.4413001537323, 90.99077844619751, 91.5555408000946, 92.1203031539917, 92.62714219093323, 93.13398122787476, 93.64759469032288, 94.161208152771, 94.76084494590759, 95.36048173904419, 95.94452667236328, 96.52857160568237, 97.09587740898132, 97.66318321228027, 98.22878170013428, 98.79438018798828, 99.38812112808228, 99.98186206817627, 100.52127861976624, 101.0606951713562, 101.56876921653748, 102.07684326171875, 102.64630556106567, 103.2157678604126, 103.77772355079651, 104.33967924118042, 104.85578083992004, 105.37188243865967, 105.9461121559143, 106.52034187316895, 107.0739197731018, 107.62749767303467, 108.20615553855896, 108.78481340408325, 109.346923828125, 109.90903425216675, 110.4324278831482, 110.95582151412964, 111.46437525749207, 111.97292900085449, 113.0348334312439, 114.0967378616333]
[19.833333333333332, 19.833333333333332, 30.216666666666665, 30.216666666666665, 38.45, 38.45, 53.583333333333336, 53.583333333333336, 64.58333333333333, 64.58333333333333, 70.96666666666667, 70.96666666666667, 71.66666666666667, 71.66666666666667, 74.61666666666666, 74.61666666666666, 80.21666666666667, 80.21666666666667, 80.2, 80.2, 78.66666666666667, 78.66666666666667, 78.21666666666667, 78.21666666666667, 77.36666666666666, 77.36666666666666, 77.5, 77.5, 77.4, 77.4, 77.3, 77.3, 76.4, 76.4, 76.0, 76.0, 74.61666666666666, 74.61666666666666, 74.88333333333334, 74.88333333333334, 74.95, 74.95, 73.26666666666667, 73.26666666666667, 71.61666666666666, 71.61666666666666, 71.66666666666667, 71.66666666666667, 70.35, 70.35, 70.46666666666667, 70.46666666666667, 74.36666666666666, 74.36666666666666, 75.7, 75.7, 75.56666666666666, 75.56666666666666, 75.8, 75.8, 75.93333333333334, 75.93333333333334, 75.93333333333334, 75.93333333333334, 74.48333333333333, 74.48333333333333, 73.06666666666666, 73.06666666666666, 72.91666666666667, 72.91666666666667, 73.03333333333333, 73.03333333333333, 74.58333333333333, 74.58333333333333, 76.18333333333334, 76.18333333333334, 76.11666666666666, 76.11666666666666, 77.8, 77.8, 77.88333333333334, 77.88333333333334, 77.76666666666667, 77.76666666666667, 77.83333333333333, 77.83333333333333, 77.91666666666667, 77.91666666666667, 77.8, 77.8, 77.78333333333333, 77.78333333333333, 77.88333333333334, 77.88333333333334, 77.9, 77.9, 76.2, 76.2, 76.2, 76.2, 74.9, 74.9, 74.96666666666667, 74.96666666666667, 74.98333333333333, 74.98333333333333, 76.53333333333333, 76.53333333333333, 76.38333333333334, 76.38333333333334, 77.98333333333333, 77.98333333333333, 79.0, 79.0, 78.83333333333333, 78.83333333333333, 78.73333333333333, 78.73333333333333, 77.35, 77.35, 78.63333333333334, 78.63333333333334, 77.13333333333334, 77.13333333333334, 79.08333333333333, 79.08333333333333, 79.18333333333334, 79.18333333333334, 77.6, 77.6, 75.03333333333333, 75.03333333333333, 76.41666666666667, 76.41666666666667, 77.83333333333333, 77.83333333333333, 77.9, 77.9, 76.73333333333333, 76.73333333333333, 76.71666666666667, 76.71666666666667, 78.25, 78.25, 78.13333333333334, 78.13333333333334, 78.16666666666667, 78.16666666666667, 78.3, 78.3, 78.4, 78.4, 78.48333333333333, 78.48333333333333, 78.05, 78.05, 78.05, 78.05, 76.41666666666667, 76.41666666666667, 76.35, 76.35, 76.21666666666667, 76.21666666666667, 77.78333333333333, 77.78333333333333, 77.83333333333333, 77.83333333333333, 79.16666666666667, 79.16666666666667, 79.23333333333333, 79.23333333333333, 77.7, 77.7, 77.85, 77.85, 77.98333333333333, 77.98333333333333, 79.53333333333333, 79.53333333333333, 79.65, 79.65, 79.65, 79.65, 79.58333333333333, 79.58333333333333, 79.6, 79.6, 79.51666666666667, 79.51666666666667, 79.45, 79.45, 79.38333333333334, 79.38333333333334, 79.45, 79.45, 79.4, 79.4, 79.4, 79.4, 78.36666666666666, 78.36666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.297, Test loss: 2.301, Test accuracy: 14.87
Round   1, Train loss: 2.293, Test loss: 2.299, Test accuracy: 21.35
Round   2, Train loss: 2.283, Test loss: 2.295, Test accuracy: 26.18
Round   3, Train loss: 2.253, Test loss: 2.281, Test accuracy: 14.40
Round   4, Train loss: 2.162, Test loss: 2.227, Test accuracy: 23.48
Round   5, Train loss: 2.114, Test loss: 2.175, Test accuracy: 27.55
Round   6, Train loss: 2.033, Test loss: 2.131, Test accuracy: 34.17
Round   7, Train loss: 1.931, Test loss: 2.086, Test accuracy: 36.07
Round   8, Train loss: 1.962, Test loss: 2.063, Test accuracy: 39.97
Round   9, Train loss: 1.888, Test loss: 2.009, Test accuracy: 45.42
Round  10, Train loss: 1.898, Test loss: 1.960, Test accuracy: 52.38
Round  11, Train loss: 1.819, Test loss: 1.928, Test accuracy: 55.52
Round  12, Train loss: 1.828, Test loss: 1.904, Test accuracy: 57.20
Round  13, Train loss: 1.875, Test loss: 1.880, Test accuracy: 59.20
Round  14, Train loss: 1.805, Test loss: 1.854, Test accuracy: 62.48
Round  15, Train loss: 1.782, Test loss: 1.844, Test accuracy: 63.42
Round  16, Train loss: 1.788, Test loss: 1.826, Test accuracy: 64.77
Round  17, Train loss: 1.761, Test loss: 1.810, Test accuracy: 66.27
Round  18, Train loss: 1.690, Test loss: 1.801, Test accuracy: 67.55
Round  19, Train loss: 1.778, Test loss: 1.793, Test accuracy: 68.18
Round  20, Train loss: 1.713, Test loss: 1.792, Test accuracy: 68.35
Round  21, Train loss: 1.731, Test loss: 1.789, Test accuracy: 68.18
Round  22, Train loss: 1.773, Test loss: 1.790, Test accuracy: 68.17
Round  23, Train loss: 1.744, Test loss: 1.789, Test accuracy: 67.92
Round  24, Train loss: 1.671, Test loss: 1.777, Test accuracy: 69.27
Round  25, Train loss: 1.719, Test loss: 1.773, Test accuracy: 69.67
Round  26, Train loss: 1.720, Test loss: 1.770, Test accuracy: 69.85
Round  27, Train loss: 1.761, Test loss: 1.767, Test accuracy: 70.17
Round  28, Train loss: 1.768, Test loss: 1.764, Test accuracy: 70.72
Round  29, Train loss: 1.666, Test loss: 1.761, Test accuracy: 70.80
Round  30, Train loss: 1.756, Test loss: 1.762, Test accuracy: 70.82
Round  31, Train loss: 1.670, Test loss: 1.759, Test accuracy: 71.07
Round  32, Train loss: 1.662, Test loss: 1.756, Test accuracy: 71.28
Round  33, Train loss: 1.703, Test loss: 1.755, Test accuracy: 71.33
Round  34, Train loss: 1.758, Test loss: 1.755, Test accuracy: 71.38
Round  35, Train loss: 1.606, Test loss: 1.754, Test accuracy: 71.45
Round  36, Train loss: 1.702, Test loss: 1.753, Test accuracy: 71.37
Round  37, Train loss: 1.802, Test loss: 1.753, Test accuracy: 71.40
Round  38, Train loss: 1.676, Test loss: 1.750, Test accuracy: 71.70
Round  39, Train loss: 1.752, Test loss: 1.749, Test accuracy: 71.75
Round  40, Train loss: 1.711, Test loss: 1.748, Test accuracy: 71.90
Round  41, Train loss: 1.764, Test loss: 1.746, Test accuracy: 71.92
Round  42, Train loss: 1.661, Test loss: 1.740, Test accuracy: 72.75
Round  43, Train loss: 1.759, Test loss: 1.740, Test accuracy: 72.78
Round  44, Train loss: 1.708, Test loss: 1.741, Test accuracy: 72.40
Round  45, Train loss: 1.649, Test loss: 1.738, Test accuracy: 72.52
Round  46, Train loss: 1.752, Test loss: 1.736, Test accuracy: 73.00
Round  47, Train loss: 1.660, Test loss: 1.723, Test accuracy: 74.47
Round  48, Train loss: 1.703, Test loss: 1.722, Test accuracy: 74.57
Round  49, Train loss: 1.648, Test loss: 1.720, Test accuracy: 74.67
Round  50, Train loss: 1.644, Test loss: 1.720, Test accuracy: 74.68
Round  51, Train loss: 1.704, Test loss: 1.719, Test accuracy: 74.83
Round  52, Train loss: 1.702, Test loss: 1.718, Test accuracy: 74.80
Round  53, Train loss: 1.750, Test loss: 1.718, Test accuracy: 74.82
Round  54, Train loss: 1.705, Test loss: 1.717, Test accuracy: 74.88
Round  55, Train loss: 1.706, Test loss: 1.717, Test accuracy: 74.92
Round  56, Train loss: 1.861, Test loss: 1.717, Test accuracy: 74.93
Round  57, Train loss: 1.752, Test loss: 1.716, Test accuracy: 74.98
Round  58, Train loss: 1.694, Test loss: 1.716, Test accuracy: 74.97
Round  59, Train loss: 1.649, Test loss: 1.715, Test accuracy: 74.98
Round  60, Train loss: 1.706, Test loss: 1.715, Test accuracy: 74.93
Round  61, Train loss: 1.640, Test loss: 1.714, Test accuracy: 74.92
Round  62, Train loss: 1.807, Test loss: 1.715, Test accuracy: 74.92
Round  63, Train loss: 1.698, Test loss: 1.714, Test accuracy: 75.03
Round  64, Train loss: 1.700, Test loss: 1.714, Test accuracy: 75.12
Round  65, Train loss: 1.696, Test loss: 1.715, Test accuracy: 75.03
Round  66, Train loss: 1.650, Test loss: 1.713, Test accuracy: 75.05
Round  67, Train loss: 1.595, Test loss: 1.698, Test accuracy: 76.63
Round  68, Train loss: 1.695, Test loss: 1.697, Test accuracy: 76.73
Round  69, Train loss: 1.696, Test loss: 1.696, Test accuracy: 76.75
Round  70, Train loss: 1.753, Test loss: 1.697, Test accuracy: 76.68
Round  71, Train loss: 1.533, Test loss: 1.697, Test accuracy: 76.72
Round  72, Train loss: 1.590, Test loss: 1.696, Test accuracy: 76.73
Round  73, Train loss: 1.750, Test loss: 1.696, Test accuracy: 76.75
Round  74, Train loss: 1.534, Test loss: 1.696, Test accuracy: 76.80
Round  75, Train loss: 1.694, Test loss: 1.695, Test accuracy: 76.77
Round  76, Train loss: 1.750, Test loss: 1.695, Test accuracy: 76.77
Round  77, Train loss: 1.803, Test loss: 1.695, Test accuracy: 76.80
Round  78, Train loss: 1.743, Test loss: 1.695, Test accuracy: 76.78
Round  79, Train loss: 1.689, Test loss: 1.694, Test accuracy: 76.77
Round  80, Train loss: 1.635, Test loss: 1.694, Test accuracy: 76.85
Round  81, Train loss: 1.743, Test loss: 1.694, Test accuracy: 76.78
Round  82, Train loss: 1.694, Test loss: 1.694, Test accuracy: 76.92
Round  83, Train loss: 1.683, Test loss: 1.697, Test accuracy: 76.57
Round  84, Train loss: 1.746, Test loss: 1.696, Test accuracy: 76.65
Round  85, Train loss: 1.697, Test loss: 1.695, Test accuracy: 76.75
Round  86, Train loss: 1.641, Test loss: 1.694, Test accuracy: 76.85
Round  87, Train loss: 1.592, Test loss: 1.693, Test accuracy: 77.03
Round  88, Train loss: 1.693, Test loss: 1.693, Test accuracy: 77.05
Round  89, Train loss: 1.692, Test loss: 1.693, Test accuracy: 76.97
Round  90, Train loss: 1.693, Test loss: 1.694, Test accuracy: 76.97
Round  91, Train loss: 1.603, Test loss: 1.681, Test accuracy: 78.17
Round  92, Train loss: 1.757, Test loss: 1.680, Test accuracy: 78.28
Round  93, Train loss: 1.583, Test loss: 1.679, Test accuracy: 78.40
Round  94, Train loss: 1.695, Test loss: 1.679, Test accuracy: 78.38
Round  95, Train loss: 1.586, Test loss: 1.679, Test accuracy: 78.42
Round  96, Train loss: 1.692, Test loss: 1.679, Test accuracy: 78.35
Round  97, Train loss: 1.642, Test loss: 1.678, Test accuracy: 78.35
Round  98, Train loss: 1.639, Test loss: 1.678, Test accuracy: 78.50/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.575, Test loss: 1.678, Test accuracy: 78.40
Final Round, Train loss: 1.629, Test loss: 1.647, Test accuracy: 81.60
Average accuracy final 10 rounds: 78.22166666666668 

692.7406108379364
[0.5683274269104004, 1.1366548538208008, 1.683988332748413, 2.2313218116760254, 2.8257455825805664, 3.4201693534851074, 4.01202392578125, 4.603878498077393, 5.1345906257629395, 5.665302753448486, 6.171915769577026, 6.678528785705566, 7.190816640853882, 7.703104496002197, 8.204888820648193, 8.70667314529419, 9.214962720870972, 9.723252296447754, 10.221490621566772, 10.719728946685791, 11.257253646850586, 11.79477834701538, 12.35292911529541, 12.91107988357544, 13.47616457939148, 14.04124927520752, 14.640403270721436, 15.239557266235352, 15.788112878799438, 16.336668491363525, 16.85123085975647, 17.365793228149414, 17.94003415107727, 18.514275074005127, 19.013257265090942, 19.512239456176758, 19.984386682510376, 20.456533908843994, 20.984526872634888, 21.51251983642578, 22.01196265220642, 22.51140546798706, 23.013871431350708, 23.516337394714355, 24.015462398529053, 24.51458740234375, 25.056117296218872, 25.597647190093994, 26.18273091316223, 26.76781463623047, 27.334734439849854, 27.90165424346924, 28.46557879447937, 29.029503345489502, 29.587252140045166, 30.14500093460083, 30.676459312438965, 31.2079176902771, 31.730286836624146, 32.25265598297119, 32.759479999542236, 33.26630401611328, 33.759618520736694, 34.25293302536011, 34.73906946182251, 35.22520589828491, 35.7527334690094, 36.28026103973389, 36.795886516571045, 37.3115119934082, 37.844868659973145, 38.378225326538086, 38.94790196418762, 39.51757860183716, 40.067866563797, 40.618154525756836, 41.19340205192566, 41.76864957809448, 42.335057497024536, 42.90146541595459, 43.44531011581421, 43.98915481567383, 44.54107451438904, 45.09299421310425, 45.597371339797974, 46.1017484664917, 46.58040428161621, 47.05906009674072, 47.586002826690674, 48.112945556640625, 48.63437271118164, 49.155799865722656, 49.64945983886719, 50.14311981201172, 50.65364050865173, 51.16416120529175, 51.74683451652527, 52.32950782775879, 52.88021159172058, 53.43091535568237, 53.999642848968506, 54.56837034225464, 55.157689809799194, 55.74700927734375, 56.30401277542114, 56.861016273498535, 57.38417029380798, 57.90732431411743, 58.42129588127136, 58.93526744842529, 59.42302465438843, 59.91078186035156, 60.40773415565491, 60.90468645095825, 61.4194769859314, 61.93426752090454, 62.43655705451965, 62.938846588134766, 63.42955160140991, 63.92025661468506, 64.44875741004944, 64.97725820541382, 65.54504752159119, 66.11283683776855, 66.71005201339722, 67.30726718902588, 67.8668885231018, 68.42650985717773, 68.97269988059998, 69.51888990402222, 70.08468389511108, 70.65047788619995, 71.14886474609375, 71.64725160598755, 72.11625862121582, 72.58526563644409, 73.09571409225464, 73.60616254806519, 74.10264730453491, 74.59913206100464, 75.07987093925476, 75.56060981750488, 76.06933975219727, 76.57806968688965, 77.1279296875, 77.67778968811035, 78.29415225982666, 78.91051483154297, 79.45972466468811, 80.00893449783325, 80.53880381584167, 81.0686731338501, 81.6179404258728, 82.16720771789551, 82.69625091552734, 83.22529411315918, 83.70001244544983, 84.17473077774048, 84.66522669792175, 85.15572261810303, 85.66732668876648, 86.17893075942993, 86.65579915046692, 87.1326675415039, 87.66700983047485, 88.2013521194458, 88.71603202819824, 89.23071193695068, 89.74982190132141, 90.26893186569214, 90.80904936790466, 91.34916687011719, 91.90807175636292, 92.46697664260864, 93.03058886528015, 93.59420108795166, 94.17521142959595, 94.75622177124023, 95.28384351730347, 95.8114652633667, 96.2822368144989, 96.7530083656311, 97.22786808013916, 97.70272779464722, 98.18061590194702, 98.65850400924683, 99.17588520050049, 99.69326639175415, 100.21106123924255, 100.72885608673096, 101.21317863464355, 101.69750118255615, 102.26285243034363, 102.8282036781311, 103.43295550346375, 104.03770732879639, 104.57601737976074, 105.1143274307251, 105.66153025627136, 106.20873308181763, 107.28674697875977, 108.3647608757019]
[14.866666666666667, 14.866666666666667, 21.35, 21.35, 26.183333333333334, 26.183333333333334, 14.4, 14.4, 23.483333333333334, 23.483333333333334, 27.55, 27.55, 34.166666666666664, 34.166666666666664, 36.06666666666667, 36.06666666666667, 39.96666666666667, 39.96666666666667, 45.416666666666664, 45.416666666666664, 52.38333333333333, 52.38333333333333, 55.516666666666666, 55.516666666666666, 57.2, 57.2, 59.2, 59.2, 62.483333333333334, 62.483333333333334, 63.416666666666664, 63.416666666666664, 64.76666666666667, 64.76666666666667, 66.26666666666667, 66.26666666666667, 67.55, 67.55, 68.18333333333334, 68.18333333333334, 68.35, 68.35, 68.18333333333334, 68.18333333333334, 68.16666666666667, 68.16666666666667, 67.91666666666667, 67.91666666666667, 69.26666666666667, 69.26666666666667, 69.66666666666667, 69.66666666666667, 69.85, 69.85, 70.16666666666667, 70.16666666666667, 70.71666666666667, 70.71666666666667, 70.8, 70.8, 70.81666666666666, 70.81666666666666, 71.06666666666666, 71.06666666666666, 71.28333333333333, 71.28333333333333, 71.33333333333333, 71.33333333333333, 71.38333333333334, 71.38333333333334, 71.45, 71.45, 71.36666666666666, 71.36666666666666, 71.4, 71.4, 71.7, 71.7, 71.75, 71.75, 71.9, 71.9, 71.91666666666667, 71.91666666666667, 72.75, 72.75, 72.78333333333333, 72.78333333333333, 72.4, 72.4, 72.51666666666667, 72.51666666666667, 73.0, 73.0, 74.46666666666667, 74.46666666666667, 74.56666666666666, 74.56666666666666, 74.66666666666667, 74.66666666666667, 74.68333333333334, 74.68333333333334, 74.83333333333333, 74.83333333333333, 74.8, 74.8, 74.81666666666666, 74.81666666666666, 74.88333333333334, 74.88333333333334, 74.91666666666667, 74.91666666666667, 74.93333333333334, 74.93333333333334, 74.98333333333333, 74.98333333333333, 74.96666666666667, 74.96666666666667, 74.98333333333333, 74.98333333333333, 74.93333333333334, 74.93333333333334, 74.91666666666667, 74.91666666666667, 74.91666666666667, 74.91666666666667, 75.03333333333333, 75.03333333333333, 75.11666666666666, 75.11666666666666, 75.03333333333333, 75.03333333333333, 75.05, 75.05, 76.63333333333334, 76.63333333333334, 76.73333333333333, 76.73333333333333, 76.75, 76.75, 76.68333333333334, 76.68333333333334, 76.71666666666667, 76.71666666666667, 76.73333333333333, 76.73333333333333, 76.75, 76.75, 76.8, 76.8, 76.76666666666667, 76.76666666666667, 76.76666666666667, 76.76666666666667, 76.8, 76.8, 76.78333333333333, 76.78333333333333, 76.76666666666667, 76.76666666666667, 76.85, 76.85, 76.78333333333333, 76.78333333333333, 76.91666666666667, 76.91666666666667, 76.56666666666666, 76.56666666666666, 76.65, 76.65, 76.75, 76.75, 76.85, 76.85, 77.03333333333333, 77.03333333333333, 77.05, 77.05, 76.96666666666667, 76.96666666666667, 76.96666666666667, 76.96666666666667, 78.16666666666667, 78.16666666666667, 78.28333333333333, 78.28333333333333, 78.4, 78.4, 78.38333333333334, 78.38333333333334, 78.41666666666667, 78.41666666666667, 78.35, 78.35, 78.35, 78.35, 78.5, 78.5, 78.4, 78.4, 81.6, 81.6]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.284, Test loss: 2.296, Test accuracy: 15.98
Round   1, Train loss: 2.226, Test loss: 2.272, Test accuracy: 17.97
Round   2, Train loss: 2.093, Test loss: 2.234, Test accuracy: 21.70
Round   3, Train loss: 1.894, Test loss: 2.149, Test accuracy: 34.60
Round   4, Train loss: 1.927, Test loss: 2.036, Test accuracy: 50.15
Round   5, Train loss: 1.639, Test loss: 1.956, Test accuracy: 53.48
Round   6, Train loss: 1.710, Test loss: 1.900, Test accuracy: 58.18
Round   7, Train loss: 1.659, Test loss: 1.854, Test accuracy: 62.72
Round   8, Train loss: 1.625, Test loss: 1.836, Test accuracy: 64.67
Round   9, Train loss: 1.679, Test loss: 1.805, Test accuracy: 67.63
Round  10, Train loss: 1.666, Test loss: 1.795, Test accuracy: 68.32
Round  11, Train loss: 1.655, Test loss: 1.800, Test accuracy: 67.40
Round  12, Train loss: 1.763, Test loss: 1.776, Test accuracy: 69.68
Round  13, Train loss: 1.614, Test loss: 1.753, Test accuracy: 72.15
Round  14, Train loss: 1.654, Test loss: 1.750, Test accuracy: 72.48
Round  15, Train loss: 1.552, Test loss: 1.736, Test accuracy: 74.00
Round  16, Train loss: 1.652, Test loss: 1.733, Test accuracy: 73.82
Round  17, Train loss: 1.652, Test loss: 1.729, Test accuracy: 74.27
Round  18, Train loss: 1.696, Test loss: 1.729, Test accuracy: 74.20
Round  19, Train loss: 1.655, Test loss: 1.721, Test accuracy: 74.97
Round  20, Train loss: 1.650, Test loss: 1.717, Test accuracy: 75.30
Round  21, Train loss: 1.499, Test loss: 1.707, Test accuracy: 76.18
Round  22, Train loss: 1.703, Test loss: 1.703, Test accuracy: 76.42
Round  23, Train loss: 1.644, Test loss: 1.703, Test accuracy: 76.43
Round  24, Train loss: 1.696, Test loss: 1.690, Test accuracy: 77.72
Round  25, Train loss: 1.754, Test loss: 1.691, Test accuracy: 77.65
Round  26, Train loss: 1.674, Test loss: 1.690, Test accuracy: 77.53
Round  27, Train loss: 1.644, Test loss: 1.681, Test accuracy: 78.52
Round  28, Train loss: 1.710, Test loss: 1.670, Test accuracy: 79.75
Round  29, Train loss: 1.701, Test loss: 1.668, Test accuracy: 79.77
Round  30, Train loss: 1.802, Test loss: 1.666, Test accuracy: 80.00
Round  31, Train loss: 1.593, Test loss: 1.664, Test accuracy: 80.30
Round  32, Train loss: 1.648, Test loss: 1.662, Test accuracy: 80.38
Round  33, Train loss: 1.481, Test loss: 1.663, Test accuracy: 80.28
Round  34, Train loss: 1.646, Test loss: 1.662, Test accuracy: 80.18
Round  35, Train loss: 1.587, Test loss: 1.662, Test accuracy: 80.32
Round  36, Train loss: 1.641, Test loss: 1.662, Test accuracy: 80.27
Round  37, Train loss: 1.642, Test loss: 1.661, Test accuracy: 80.47
Round  38, Train loss: 1.586, Test loss: 1.661, Test accuracy: 80.27
Round  39, Train loss: 1.533, Test loss: 1.661, Test accuracy: 80.38
Round  40, Train loss: 1.745, Test loss: 1.659, Test accuracy: 80.43
Round  41, Train loss: 1.798, Test loss: 1.659, Test accuracy: 80.40
Round  42, Train loss: 1.696, Test loss: 1.659, Test accuracy: 80.38
Round  43, Train loss: 1.694, Test loss: 1.659, Test accuracy: 80.45
Round  44, Train loss: 1.583, Test loss: 1.659, Test accuracy: 80.50
Round  45, Train loss: 1.585, Test loss: 1.658, Test accuracy: 80.53
Round  46, Train loss: 1.579, Test loss: 1.658, Test accuracy: 80.53
Round  47, Train loss: 1.582, Test loss: 1.658, Test accuracy: 80.42
Round  48, Train loss: 1.473, Test loss: 1.657, Test accuracy: 80.60
Round  49, Train loss: 1.473, Test loss: 1.656, Test accuracy: 80.65
Round  50, Train loss: 1.776, Test loss: 1.644, Test accuracy: 82.03
Round  51, Train loss: 1.691, Test loss: 1.643, Test accuracy: 82.07
Round  52, Train loss: 1.584, Test loss: 1.643, Test accuracy: 82.18
Round  53, Train loss: 1.582, Test loss: 1.642, Test accuracy: 82.18
Round  54, Train loss: 1.636, Test loss: 1.642, Test accuracy: 82.20
Round  55, Train loss: 1.524, Test loss: 1.641, Test accuracy: 82.27
Round  56, Train loss: 1.665, Test loss: 1.634, Test accuracy: 83.08
Round  57, Train loss: 1.747, Test loss: 1.633, Test accuracy: 83.12
Round  58, Train loss: 1.582, Test loss: 1.630, Test accuracy: 83.32
Round  59, Train loss: 1.583, Test loss: 1.631, Test accuracy: 83.17
Round  60, Train loss: 1.526, Test loss: 1.628, Test accuracy: 83.58
Round  61, Train loss: 1.580, Test loss: 1.627, Test accuracy: 83.55
Round  62, Train loss: 1.636, Test loss: 1.626, Test accuracy: 83.63
Round  63, Train loss: 1.634, Test loss: 1.626, Test accuracy: 83.68
Round  64, Train loss: 1.583, Test loss: 1.625, Test accuracy: 83.78
Round  65, Train loss: 1.636, Test loss: 1.624, Test accuracy: 83.83
Round  66, Train loss: 1.633, Test loss: 1.624, Test accuracy: 83.88
Round  67, Train loss: 1.580, Test loss: 1.624, Test accuracy: 83.80
Round  68, Train loss: 1.525, Test loss: 1.624, Test accuracy: 83.93
Round  69, Train loss: 1.629, Test loss: 1.624, Test accuracy: 83.80
Round  70, Train loss: 1.473, Test loss: 1.623, Test accuracy: 83.78
Round  71, Train loss: 1.580, Test loss: 1.623, Test accuracy: 83.93
Round  72, Train loss: 1.632, Test loss: 1.623, Test accuracy: 83.88
Round  73, Train loss: 1.526, Test loss: 1.623, Test accuracy: 83.92
Round  74, Train loss: 1.581, Test loss: 1.623, Test accuracy: 83.92
Round  75, Train loss: 1.631, Test loss: 1.623, Test accuracy: 83.87
Round  76, Train loss: 1.579, Test loss: 1.623, Test accuracy: 83.92
Round  77, Train loss: 1.524, Test loss: 1.623, Test accuracy: 83.88
Round  78, Train loss: 1.686, Test loss: 1.622, Test accuracy: 83.90
Round  79, Train loss: 1.577, Test loss: 1.622, Test accuracy: 83.83
Round  80, Train loss: 1.578, Test loss: 1.622, Test accuracy: 83.95
Round  81, Train loss: 1.580, Test loss: 1.622, Test accuracy: 83.93
Round  82, Train loss: 1.578, Test loss: 1.622, Test accuracy: 83.95
Round  83, Train loss: 1.525, Test loss: 1.622, Test accuracy: 83.88
Round  84, Train loss: 1.524, Test loss: 1.622, Test accuracy: 84.00
Round  85, Train loss: 1.633, Test loss: 1.622, Test accuracy: 83.98
Round  86, Train loss: 1.576, Test loss: 1.621, Test accuracy: 83.97
Round  87, Train loss: 1.631, Test loss: 1.621, Test accuracy: 84.07
Round  88, Train loss: 1.684, Test loss: 1.621, Test accuracy: 84.03
Round  89, Train loss: 1.524, Test loss: 1.621, Test accuracy: 84.08
Round  90, Train loss: 1.582, Test loss: 1.621, Test accuracy: 84.12
Round  91, Train loss: 1.577, Test loss: 1.621, Test accuracy: 84.07
Round  92, Train loss: 1.525, Test loss: 1.621, Test accuracy: 84.10
Round  93, Train loss: 1.579, Test loss: 1.621, Test accuracy: 84.08
Round  94, Train loss: 1.526, Test loss: 1.621, Test accuracy: 83.88
Round  95, Train loss: 1.578, Test loss: 1.621, Test accuracy: 83.90
Round  96, Train loss: 1.635, Test loss: 1.621, Test accuracy: 83.88
Round  97, Train loss: 1.689, Test loss: 1.621, Test accuracy: 83.97
Round  98, Train loss: 1.577, Test loss: 1.621, Test accuracy: 84.03/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.578, Test loss: 1.621, Test accuracy: 84.00
Final Round, Train loss: 1.584, Test loss: 1.621, Test accuracy: 84.08
Average accuracy final 10 rounds: 84.00333333333334 

708.9454426765442
[0.6121687889099121, 1.2243375778198242, 1.7875416278839111, 2.350745677947998, 2.871427297592163, 3.392108917236328, 3.901989698410034, 4.41187047958374, 4.993074893951416, 5.574279308319092, 6.131600379943848, 6.6889214515686035, 7.236648321151733, 7.784375190734863, 8.371379852294922, 8.95838451385498, 9.504961013793945, 10.05153751373291, 10.584970712661743, 11.118403911590576, 11.670192003250122, 12.221980094909668, 12.736607074737549, 13.25123405456543, 13.747415542602539, 14.243597030639648, 14.782660961151123, 15.321724891662598, 15.869191884994507, 16.416658878326416, 16.934043169021606, 17.451427459716797, 18.008947372436523, 18.56646728515625, 19.11662983894348, 19.666792392730713, 20.250972747802734, 20.835153102874756, 21.397068738937378, 21.958984375, 22.535010814666748, 23.111037254333496, 23.65012001991272, 24.189202785491943, 24.71092414855957, 25.232645511627197, 25.69488501548767, 26.157124519348145, 26.685412406921387, 27.21370029449463, 27.76480484008789, 28.315909385681152, 28.82352614402771, 29.331142902374268, 29.849835872650146, 30.368528842926025, 30.987217664718628, 31.60590648651123, 32.16762924194336, 32.72935199737549, 33.28645658493042, 33.84356117248535, 34.43959307670593, 35.035624980926514, 35.608388900756836, 36.18115282058716, 36.71721577644348, 37.253278732299805, 37.79712414741516, 38.34096956253052, 38.86144018173218, 39.38191080093384, 39.87636947631836, 40.37082815170288, 40.89013624191284, 41.4094443321228, 41.945168256759644, 42.480892181396484, 42.99499011039734, 43.50908803939819, 44.03615617752075, 44.56322431564331, 45.120519161224365, 45.67781400680542, 46.2315034866333, 46.78519296646118, 47.33411407470703, 47.88303518295288, 48.433276653289795, 48.98351812362671, 49.539483070373535, 50.09544801712036, 50.66221213340759, 51.228976249694824, 51.73988318443298, 52.25079011917114, 52.77742552757263, 53.30406093597412, 53.83799743652344, 54.371933937072754, 54.89173412322998, 55.41153430938721, 55.92229461669922, 56.43305492401123, 57.0114209651947, 57.589787006378174, 58.177643060684204, 58.765499114990234, 59.35638952255249, 59.947279930114746, 60.54730010032654, 61.14732027053833, 61.73932337760925, 62.331326484680176, 62.88054442405701, 63.42976236343384, 63.9386420249939, 64.44752168655396, 64.97619724273682, 65.50487279891968, 66.04809951782227, 66.59132623672485, 67.10453820228577, 67.61775016784668, 68.14421439170837, 68.67067861557007, 69.21552276611328, 69.7603669166565, 70.34778761863708, 70.93520832061768, 71.52936053276062, 72.12351274490356, 72.744455575943, 73.36539840698242, 73.96192836761475, 74.55845832824707, 75.1306881904602, 75.70291805267334, 76.28182196617126, 76.86072587966919, 77.39072871208191, 77.92073154449463, 78.45366740226746, 78.98660326004028, 79.51763272285461, 80.04866218566895, 80.58296966552734, 81.11727714538574, 81.60790681838989, 82.09853649139404, 82.61432814598083, 83.13011980056763, 83.68962812423706, 84.2491364479065, 84.82907032966614, 85.40900421142578, 85.97092747688293, 86.53285074234009, 87.12097477912903, 87.70909881591797, 88.2999906539917, 88.89088249206543, 89.46063661575317, 90.03039073944092, 90.55080103874207, 91.07121133804321, 91.58217239379883, 92.09313344955444, 92.61125206947327, 93.12937068939209, 93.64200973510742, 94.15464878082275, 94.71950197219849, 95.28435516357422, 95.84349417686462, 96.40263319015503, 96.985848903656, 97.56906461715698, 98.12340974807739, 98.6777548789978, 99.27726173400879, 99.87676858901978, 100.42663311958313, 100.97649765014648, 101.54834389686584, 102.1201901435852, 102.69267320632935, 103.26515626907349, 103.80481910705566, 104.34448194503784, 104.85957431793213, 105.37466669082642, 105.91575956344604, 106.45685243606567, 107.00783658027649, 107.5588207244873, 108.11832356452942, 108.67782640457153, 109.2166178226471, 109.75540924072266, 110.80926966667175, 111.86313009262085]
[15.983333333333333, 15.983333333333333, 17.966666666666665, 17.966666666666665, 21.7, 21.7, 34.6, 34.6, 50.15, 50.15, 53.483333333333334, 53.483333333333334, 58.18333333333333, 58.18333333333333, 62.71666666666667, 62.71666666666667, 64.66666666666667, 64.66666666666667, 67.63333333333334, 67.63333333333334, 68.31666666666666, 68.31666666666666, 67.4, 67.4, 69.68333333333334, 69.68333333333334, 72.15, 72.15, 72.48333333333333, 72.48333333333333, 74.0, 74.0, 73.81666666666666, 73.81666666666666, 74.26666666666667, 74.26666666666667, 74.2, 74.2, 74.96666666666667, 74.96666666666667, 75.3, 75.3, 76.18333333333334, 76.18333333333334, 76.41666666666667, 76.41666666666667, 76.43333333333334, 76.43333333333334, 77.71666666666667, 77.71666666666667, 77.65, 77.65, 77.53333333333333, 77.53333333333333, 78.51666666666667, 78.51666666666667, 79.75, 79.75, 79.76666666666667, 79.76666666666667, 80.0, 80.0, 80.3, 80.3, 80.38333333333334, 80.38333333333334, 80.28333333333333, 80.28333333333333, 80.18333333333334, 80.18333333333334, 80.31666666666666, 80.31666666666666, 80.26666666666667, 80.26666666666667, 80.46666666666667, 80.46666666666667, 80.26666666666667, 80.26666666666667, 80.38333333333334, 80.38333333333334, 80.43333333333334, 80.43333333333334, 80.4, 80.4, 80.38333333333334, 80.38333333333334, 80.45, 80.45, 80.5, 80.5, 80.53333333333333, 80.53333333333333, 80.53333333333333, 80.53333333333333, 80.41666666666667, 80.41666666666667, 80.6, 80.6, 80.65, 80.65, 82.03333333333333, 82.03333333333333, 82.06666666666666, 82.06666666666666, 82.18333333333334, 82.18333333333334, 82.18333333333334, 82.18333333333334, 82.2, 82.2, 82.26666666666667, 82.26666666666667, 83.08333333333333, 83.08333333333333, 83.11666666666666, 83.11666666666666, 83.31666666666666, 83.31666666666666, 83.16666666666667, 83.16666666666667, 83.58333333333333, 83.58333333333333, 83.55, 83.55, 83.63333333333334, 83.63333333333334, 83.68333333333334, 83.68333333333334, 83.78333333333333, 83.78333333333333, 83.83333333333333, 83.83333333333333, 83.88333333333334, 83.88333333333334, 83.8, 83.8, 83.93333333333334, 83.93333333333334, 83.8, 83.8, 83.78333333333333, 83.78333333333333, 83.93333333333334, 83.93333333333334, 83.88333333333334, 83.88333333333334, 83.91666666666667, 83.91666666666667, 83.91666666666667, 83.91666666666667, 83.86666666666666, 83.86666666666666, 83.91666666666667, 83.91666666666667, 83.88333333333334, 83.88333333333334, 83.9, 83.9, 83.83333333333333, 83.83333333333333, 83.95, 83.95, 83.93333333333334, 83.93333333333334, 83.95, 83.95, 83.88333333333334, 83.88333333333334, 84.0, 84.0, 83.98333333333333, 83.98333333333333, 83.96666666666667, 83.96666666666667, 84.06666666666666, 84.06666666666666, 84.03333333333333, 84.03333333333333, 84.08333333333333, 84.08333333333333, 84.11666666666666, 84.11666666666666, 84.06666666666666, 84.06666666666666, 84.1, 84.1, 84.08333333333333, 84.08333333333333, 83.88333333333334, 83.88333333333334, 83.9, 83.9, 83.88333333333334, 83.88333333333334, 83.96666666666667, 83.96666666666667, 84.03333333333333, 84.03333333333333, 84.0, 84.0, 84.08333333333333, 84.08333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.284, Test loss: 2.295, Test accuracy: 17.50
Round   1, Train loss: 2.191, Test loss: 2.256, Test accuracy: 26.07
Round   2, Train loss: 1.949, Test loss: 2.155, Test accuracy: 36.97
Round   3, Train loss: 1.918, Test loss: 2.055, Test accuracy: 44.30
Round   4, Train loss: 1.864, Test loss: 2.000, Test accuracy: 50.67
Round   5, Train loss: 1.741, Test loss: 1.907, Test accuracy: 60.70
Round   6, Train loss: 1.715, Test loss: 1.887, Test accuracy: 59.78
Round   7, Train loss: 1.626, Test loss: 1.854, Test accuracy: 64.95
Round   8, Train loss: 1.626, Test loss: 1.814, Test accuracy: 69.43
Round   9, Train loss: 1.651, Test loss: 1.783, Test accuracy: 72.50
Round  10, Train loss: 1.601, Test loss: 1.709, Test accuracy: 80.15
Round  11, Train loss: 1.598, Test loss: 1.701, Test accuracy: 80.18
Round  12, Train loss: 1.615, Test loss: 1.686, Test accuracy: 80.38
Round  13, Train loss: 1.520, Test loss: 1.657, Test accuracy: 83.05
Round  14, Train loss: 1.596, Test loss: 1.638, Test accuracy: 85.07
Round  15, Train loss: 1.561, Test loss: 1.609, Test accuracy: 87.33
Round  16, Train loss: 1.489, Test loss: 1.595, Test accuracy: 88.90
Round  17, Train loss: 1.480, Test loss: 1.585, Test accuracy: 89.58
Round  18, Train loss: 1.528, Test loss: 1.584, Test accuracy: 89.32
Round  19, Train loss: 1.528, Test loss: 1.576, Test accuracy: 90.12
Round  20, Train loss: 1.478, Test loss: 1.571, Test accuracy: 90.37
Round  21, Train loss: 1.581, Test loss: 1.570, Test accuracy: 90.45
Round  22, Train loss: 1.525, Test loss: 1.567, Test accuracy: 90.48
Round  23, Train loss: 1.490, Test loss: 1.562, Test accuracy: 90.87
Round  24, Train loss: 1.474, Test loss: 1.560, Test accuracy: 91.02
Round  25, Train loss: 1.471, Test loss: 1.559, Test accuracy: 91.05
Round  26, Train loss: 1.468, Test loss: 1.560, Test accuracy: 90.95
Round  27, Train loss: 1.563, Test loss: 1.544, Test accuracy: 92.73
Round  28, Train loss: 1.473, Test loss: 1.537, Test accuracy: 93.23
Round  29, Train loss: 1.468, Test loss: 1.536, Test accuracy: 93.28
Round  30, Train loss: 1.467, Test loss: 1.536, Test accuracy: 93.12
Round  31, Train loss: 1.476, Test loss: 1.522, Test accuracy: 94.83
Round  32, Train loss: 1.468, Test loss: 1.518, Test accuracy: 95.10
Round  33, Train loss: 1.468, Test loss: 1.517, Test accuracy: 95.22
Round  34, Train loss: 1.468, Test loss: 1.516, Test accuracy: 95.30
Round  35, Train loss: 1.467, Test loss: 1.516, Test accuracy: 95.37
Round  36, Train loss: 1.465, Test loss: 1.515, Test accuracy: 95.40
Round  37, Train loss: 1.465, Test loss: 1.515, Test accuracy: 95.45
Round  38, Train loss: 1.469, Test loss: 1.515, Test accuracy: 95.32
Round  39, Train loss: 1.465, Test loss: 1.515, Test accuracy: 95.30
Round  40, Train loss: 1.470, Test loss: 1.513, Test accuracy: 95.52
Round  41, Train loss: 1.468, Test loss: 1.512, Test accuracy: 95.58
Round  42, Train loss: 1.467, Test loss: 1.511, Test accuracy: 95.58
Round  43, Train loss: 1.468, Test loss: 1.511, Test accuracy: 95.65
Round  44, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.67
Round  45, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.73
Round  46, Train loss: 1.467, Test loss: 1.509, Test accuracy: 95.72
Round  47, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.82
Round  48, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.75
Round  49, Train loss: 1.463, Test loss: 1.509, Test accuracy: 95.78
Round  50, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.75
Round  51, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.77
Round  52, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.70
Round  53, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.72
Round  54, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.72
Round  55, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.75
Round  56, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.73
Round  57, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.82
Round  58, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.78
Round  59, Train loss: 1.467, Test loss: 1.508, Test accuracy: 95.77
Round  60, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.78
Round  61, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.80
Round  62, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.82
Round  63, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.82
Round  64, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.82
Round  65, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.83
Round  66, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.80
Round  67, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.78
Round  68, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.82
Round  69, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.80
Round  70, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.82
Round  71, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.82
Round  72, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.83
Round  73, Train loss: 1.462, Test loss: 1.507, Test accuracy: 95.80
Round  74, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.78
Round  75, Train loss: 1.462, Test loss: 1.507, Test accuracy: 95.78
Round  76, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.80
Round  77, Train loss: 1.467, Test loss: 1.507, Test accuracy: 95.80
Round  78, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.83
Round  79, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.82
Round  80, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.82
Round  81, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.78
Round  82, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.82
Round  83, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.78
Round  84, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.78
Round  85, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.80
Round  86, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.78
Round  87, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.77
Round  88, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.78
Round  89, Train loss: 1.465, Test loss: 1.507, Test accuracy: 95.77
Round  90, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.78
Round  91, Train loss: 1.466, Test loss: 1.507, Test accuracy: 95.80
Round  92, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.78
Round  93, Train loss: 1.463, Test loss: 1.507, Test accuracy: 95.80
Round  94, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.80
Round  95, Train loss: 1.464, Test loss: 1.507, Test accuracy: 95.80
Round  96, Train loss: 1.463, Test loss: 1.506, Test accuracy: 95.78
Round  97, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.80
Round  98, Train loss: 1.463, Test loss: 1.506, Test accuracy: 95.78
Round  99, Train loss: 1.463, Test loss: 1.506, Test accuracy: 95.78/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.464, Test loss: 1.506, Test accuracy: 95.80
Average accuracy final 10 rounds: 95.79166666666666 

721.9787411689758
[0.6744399070739746, 1.3488798141479492, 1.9026658535003662, 2.456451892852783, 2.958479881286621, 3.460507869720459, 4.002214193344116, 4.543920516967773, 5.077193975448608, 5.610467433929443, 6.14076042175293, 6.671053409576416, 7.235166788101196, 7.799280166625977, 8.337762594223022, 8.876245021820068, 9.431299209594727, 9.986353397369385, 10.565274000167847, 11.144194602966309, 11.766281127929688, 12.388367652893066, 12.99767518043518, 13.606982707977295, 14.190437078475952, 14.77389144897461, 15.336133241653442, 15.898375034332275, 16.42033338546753, 16.942291736602783, 17.436084985733032, 17.92987823486328, 18.487152099609375, 19.04442596435547, 19.581098079681396, 20.117770195007324, 20.642340898513794, 21.166911602020264, 21.706867694854736, 22.24682378768921, 22.84516668319702, 23.443509578704834, 24.01772165298462, 24.591933727264404, 25.174943208694458, 25.75795269012451, 26.3347909450531, 26.91162919998169, 27.468167066574097, 28.024704933166504, 28.580342292785645, 29.135979652404785, 29.65883731842041, 30.181694984436035, 30.73124361038208, 31.280792236328125, 31.821425199508667, 32.36205816268921, 32.887295722961426, 33.41253328323364, 33.943148374557495, 34.47376346588135, 35.033472776412964, 35.59318208694458, 36.144938945770264, 36.69669580459595, 37.26070308685303, 37.82471036911011, 38.4260356426239, 39.027360916137695, 39.63109874725342, 40.23483657836914, 40.772310972213745, 41.30978536605835, 41.83989596366882, 42.3700065612793, 42.91039538383484, 43.45078420639038, 43.98039674758911, 44.51000928878784, 45.02239775657654, 45.534786224365234, 46.06520342826843, 46.59562063217163, 47.12605834007263, 47.65649604797363, 48.22384190559387, 48.79118776321411, 49.413246154785156, 50.0353045463562, 50.64471364021301, 51.254122734069824, 51.78412628173828, 52.31412982940674, 52.886852502822876, 53.459575176239014, 54.04758882522583, 54.63560247421265, 55.152392864227295, 55.66918325424194, 56.187384366989136, 56.70558547973633, 57.246055603027344, 57.78652572631836, 58.327019929885864, 58.86751413345337, 59.35791039466858, 59.84830665588379, 60.35631704330444, 60.8643274307251, 61.46135067939758, 62.05837392807007, 62.679439067840576, 63.300504207611084, 63.88606357574463, 64.47162294387817, 65.05387806892395, 65.63613319396973, 66.23068737983704, 66.82524156570435, 67.37043905258179, 67.91563653945923, 68.45082020759583, 68.98600387573242, 69.53304982185364, 70.08009576797485, 70.59474444389343, 71.10939311981201, 71.65984177589417, 72.21029043197632, 72.7667248249054, 73.32315921783447, 73.88001561164856, 74.43687200546265, 75.01015901565552, 75.58344602584839, 76.1715977191925, 76.75974941253662, 77.33796000480652, 77.91617059707642, 78.4606065750122, 79.005042552948, 79.56676697731018, 80.12849140167236, 80.7063262462616, 81.28416109085083, 81.80642366409302, 82.3286862373352, 82.84628820419312, 83.36389017105103, 83.88726544380188, 84.41064071655273, 84.9587631225586, 85.50688552856445, 86.02657389640808, 86.54626226425171, 87.09436464309692, 87.64246702194214, 88.2813630104065, 88.92025899887085, 89.5133147239685, 90.10637044906616, 90.69657707214355, 91.28678369522095, 91.87464380264282, 92.4625039100647, 93.04928612709045, 93.63606834411621, 94.19542932510376, 94.75479030609131, 95.29626965522766, 95.83774900436401, 96.38180422782898, 96.92585945129395, 97.4589216709137, 97.99198389053345, 98.53751468658447, 99.0830454826355, 99.64700031280518, 100.21095514297485, 100.80019664764404, 101.38943815231323, 101.96601891517639, 102.54259967803955, 103.1486566066742, 103.75471353530884, 104.37194776535034, 104.98918199539185, 105.59080815315247, 106.19243431091309, 106.77918100357056, 107.36592769622803, 107.91948342323303, 108.47303915023804, 109.0049192905426, 109.53679943084717, 110.06787467002869, 110.5989499092102, 111.14403414726257, 111.68911838531494, 112.80841636657715, 113.92771434783936]
[17.5, 17.5, 26.066666666666666, 26.066666666666666, 36.96666666666667, 36.96666666666667, 44.3, 44.3, 50.666666666666664, 50.666666666666664, 60.7, 60.7, 59.78333333333333, 59.78333333333333, 64.95, 64.95, 69.43333333333334, 69.43333333333334, 72.5, 72.5, 80.15, 80.15, 80.18333333333334, 80.18333333333334, 80.38333333333334, 80.38333333333334, 83.05, 83.05, 85.06666666666666, 85.06666666666666, 87.33333333333333, 87.33333333333333, 88.9, 88.9, 89.58333333333333, 89.58333333333333, 89.31666666666666, 89.31666666666666, 90.11666666666666, 90.11666666666666, 90.36666666666666, 90.36666666666666, 90.45, 90.45, 90.48333333333333, 90.48333333333333, 90.86666666666666, 90.86666666666666, 91.01666666666667, 91.01666666666667, 91.05, 91.05, 90.95, 90.95, 92.73333333333333, 92.73333333333333, 93.23333333333333, 93.23333333333333, 93.28333333333333, 93.28333333333333, 93.11666666666666, 93.11666666666666, 94.83333333333333, 94.83333333333333, 95.1, 95.1, 95.21666666666667, 95.21666666666667, 95.3, 95.3, 95.36666666666666, 95.36666666666666, 95.4, 95.4, 95.45, 95.45, 95.31666666666666, 95.31666666666666, 95.3, 95.3, 95.51666666666667, 95.51666666666667, 95.58333333333333, 95.58333333333333, 95.58333333333333, 95.58333333333333, 95.65, 95.65, 95.66666666666667, 95.66666666666667, 95.73333333333333, 95.73333333333333, 95.71666666666667, 95.71666666666667, 95.81666666666666, 95.81666666666666, 95.75, 95.75, 95.78333333333333, 95.78333333333333, 95.75, 95.75, 95.76666666666667, 95.76666666666667, 95.7, 95.7, 95.71666666666667, 95.71666666666667, 95.71666666666667, 95.71666666666667, 95.75, 95.75, 95.73333333333333, 95.73333333333333, 95.81666666666666, 95.81666666666666, 95.78333333333333, 95.78333333333333, 95.76666666666667, 95.76666666666667, 95.78333333333333, 95.78333333333333, 95.8, 95.8, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.83333333333333, 95.83333333333333, 95.8, 95.8, 95.78333333333333, 95.78333333333333, 95.81666666666666, 95.81666666666666, 95.8, 95.8, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.83333333333333, 95.83333333333333, 95.8, 95.8, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.8, 95.8, 95.8, 95.8, 95.83333333333333, 95.83333333333333, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.81666666666666, 95.78333333333333, 95.78333333333333, 95.81666666666666, 95.81666666666666, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.8, 95.8, 95.78333333333333, 95.78333333333333, 95.76666666666667, 95.76666666666667, 95.78333333333333, 95.78333333333333, 95.76666666666667, 95.76666666666667, 95.78333333333333, 95.78333333333333, 95.8, 95.8, 95.78333333333333, 95.78333333333333, 95.8, 95.8, 95.8, 95.8, 95.8, 95.8, 95.78333333333333, 95.78333333333333, 95.8, 95.8, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.78333333333333, 95.8, 95.8]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.555, Test loss: 2.262, Test accuracy: 32.93
Round   1, Train loss: 1.300, Test loss: 2.166, Test accuracy: 41.53
Round   2, Train loss: 1.305, Test loss: 2.103, Test accuracy: 44.52
Round   3, Train loss: 1.322, Test loss: 2.022, Test accuracy: 53.42
Round   4, Train loss: 1.382, Test loss: 1.973, Test accuracy: 57.17
Round   5, Train loss: 1.316, Test loss: 1.942, Test accuracy: 60.00
Round   6, Train loss: 1.379, Test loss: 1.922, Test accuracy: 61.40
Round   7, Train loss: 1.149, Test loss: 1.905, Test accuracy: 60.40
Round   8, Train loss: 1.268, Test loss: 1.899, Test accuracy: 59.90
Round   9, Train loss: 1.330, Test loss: 1.868, Test accuracy: 63.22
Round  10, Train loss: 1.399, Test loss: 1.855, Test accuracy: 63.48
Round  11, Train loss: 1.279, Test loss: 1.825, Test accuracy: 66.30
Round  12, Train loss: 1.364, Test loss: 1.809, Test accuracy: 68.08
Round  13, Train loss: 1.434, Test loss: 1.803, Test accuracy: 68.17
Round  14, Train loss: 1.314, Test loss: 1.799, Test accuracy: 67.77
Round  15, Train loss: 1.332, Test loss: 1.795, Test accuracy: 67.90
Round  16, Train loss: 1.229, Test loss: 1.796, Test accuracy: 67.40
Round  17, Train loss: 1.191, Test loss: 1.790, Test accuracy: 67.75
Round  18, Train loss: 1.188, Test loss: 1.787, Test accuracy: 67.88
Round  19, Train loss: 1.278, Test loss: 1.777, Test accuracy: 69.32
Round  20, Train loss: 1.308, Test loss: 1.779, Test accuracy: 69.15
Round  21, Train loss: 1.308, Test loss: 1.780, Test accuracy: 68.87
Round  22, Train loss: 1.266, Test loss: 1.781, Test accuracy: 68.63
Round  23, Train loss: 1.308, Test loss: 1.776, Test accuracy: 69.20
Round  24, Train loss: 1.390, Test loss: 1.776, Test accuracy: 69.37
Round  25, Train loss: 1.350, Test loss: 1.777, Test accuracy: 69.28
Round  26, Train loss: 1.224, Test loss: 1.778, Test accuracy: 68.97
Round  27, Train loss: 1.267, Test loss: 1.778, Test accuracy: 69.03
Round  28, Train loss: 1.288, Test loss: 1.769, Test accuracy: 70.25
Round  29, Train loss: 1.267, Test loss: 1.771, Test accuracy: 69.92
Round  30, Train loss: 1.308, Test loss: 1.772, Test accuracy: 69.82
Round  31, Train loss: 1.349, Test loss: 1.770, Test accuracy: 69.97
Round  32, Train loss: 1.265, Test loss: 1.774, Test accuracy: 69.43
Round  33, Train loss: 1.310, Test loss: 1.773, Test accuracy: 69.65
Round  34, Train loss: 1.236, Test loss: 1.772, Test accuracy: 69.28
Round  35, Train loss: 1.268, Test loss: 1.763, Test accuracy: 70.45
Round  36, Train loss: 1.390, Test loss: 1.762, Test accuracy: 70.80
Round  37, Train loss: 1.230, Test loss: 1.762, Test accuracy: 70.60
Round  38, Train loss: 1.225, Test loss: 1.761, Test accuracy: 70.93
Round  39, Train loss: 1.185, Test loss: 1.762, Test accuracy: 70.68
Round  40, Train loss: 1.226, Test loss: 1.762, Test accuracy: 70.80
Round  41, Train loss: 1.268, Test loss: 1.759, Test accuracy: 70.82
Round  42, Train loss: 1.349, Test loss: 1.759, Test accuracy: 70.85
Round  43, Train loss: 1.225, Test loss: 1.758, Test accuracy: 71.13
Round  44, Train loss: 1.228, Test loss: 1.761, Test accuracy: 70.70
Round  45, Train loss: 1.224, Test loss: 1.764, Test accuracy: 69.78
Round  46, Train loss: 1.348, Test loss: 1.764, Test accuracy: 69.78
Round  47, Train loss: 1.306, Test loss: 1.763, Test accuracy: 69.82
Round  48, Train loss: 1.144, Test loss: 1.768, Test accuracy: 69.53
Round  49, Train loss: 1.225, Test loss: 1.769, Test accuracy: 69.48
Round  50, Train loss: 1.349, Test loss: 1.770, Test accuracy: 69.62
Round  51, Train loss: 1.306, Test loss: 1.771, Test accuracy: 69.48
Round  52, Train loss: 1.226, Test loss: 1.768, Test accuracy: 69.73
Round  53, Train loss: 1.225, Test loss: 1.768, Test accuracy: 69.73
Round  54, Train loss: 1.183, Test loss: 1.768, Test accuracy: 69.72
Round  55, Train loss: 1.389, Test loss: 1.771, Test accuracy: 69.47
Round  56, Train loss: 1.141, Test loss: 1.776, Test accuracy: 69.13
Round  57, Train loss: 1.246, Test loss: 1.768, Test accuracy: 69.90
Round  58, Train loss: 1.141, Test loss: 1.766, Test accuracy: 70.08
Round  59, Train loss: 1.348, Test loss: 1.764, Test accuracy: 70.23
Round  60, Train loss: 1.266, Test loss: 1.767, Test accuracy: 70.05
Round  61, Train loss: 1.264, Test loss: 1.771, Test accuracy: 69.67
Round  62, Train loss: 1.225, Test loss: 1.768, Test accuracy: 70.03
Round  63, Train loss: 1.306, Test loss: 1.770, Test accuracy: 69.80
Round  64, Train loss: 1.140, Test loss: 1.767, Test accuracy: 70.15
Round  65, Train loss: 1.224, Test loss: 1.771, Test accuracy: 69.67
Round  66, Train loss: 1.265, Test loss: 1.775, Test accuracy: 69.17
Round  67, Train loss: 1.306, Test loss: 1.772, Test accuracy: 69.53
Round  68, Train loss: 1.264, Test loss: 1.775, Test accuracy: 69.32
Round  69, Train loss: 1.263, Test loss: 1.773, Test accuracy: 69.42
Round  70, Train loss: 1.267, Test loss: 1.774, Test accuracy: 69.15
Round  71, Train loss: 1.180, Test loss: 1.773, Test accuracy: 69.35
Round  72, Train loss: 1.388, Test loss: 1.775, Test accuracy: 69.20
Round  73, Train loss: 1.266, Test loss: 1.774, Test accuracy: 69.55
Round  74, Train loss: 1.367, Test loss: 1.771, Test accuracy: 69.62
Round  75, Train loss: 1.270, Test loss: 1.772, Test accuracy: 69.30
Round  76, Train loss: 1.228, Test loss: 1.773, Test accuracy: 69.30
Round  77, Train loss: 1.099, Test loss: 1.786, Test accuracy: 67.87
Round  78, Train loss: 1.182, Test loss: 1.782, Test accuracy: 68.45
Round  79, Train loss: 1.143, Test loss: 1.786, Test accuracy: 67.90
Round  80, Train loss: 1.224, Test loss: 1.789, Test accuracy: 67.75
Round  81, Train loss: 1.225, Test loss: 1.790, Test accuracy: 67.28
Round  82, Train loss: 1.348, Test loss: 1.791, Test accuracy: 67.43
Round  83, Train loss: 1.309, Test loss: 1.795, Test accuracy: 66.97
Round  84, Train loss: 1.184, Test loss: 1.790, Test accuracy: 67.38
Round  85, Train loss: 1.223, Test loss: 1.790, Test accuracy: 67.42
Round  86, Train loss: 1.223, Test loss: 1.788, Test accuracy: 67.72
Round  87, Train loss: 1.350, Test loss: 1.786, Test accuracy: 67.75
Round  88, Train loss: 1.307, Test loss: 1.792, Test accuracy: 66.95
Round  89, Train loss: 1.306, Test loss: 1.797, Test accuracy: 66.48
Round  90, Train loss: 1.350, Test loss: 1.793, Test accuracy: 66.77
Round  91, Train loss: 1.266, Test loss: 1.804, Test accuracy: 65.57
Round  92, Train loss: 1.182, Test loss: 1.802, Test accuracy: 65.60
Round  93, Train loss: 1.265, Test loss: 1.805, Test accuracy: 65.70
Round  94, Train loss: 1.265, Test loss: 1.800, Test accuracy: 66.35
Round  95, Train loss: 1.306, Test loss: 1.802, Test accuracy: 66.07
Round  96, Train loss: 1.224, Test loss: 1.809, Test accuracy: 65.13
Round  97, Train loss: 1.225, Test loss: 1.808, Test accuracy: 65.25
Round  98, Train loss: 1.223, Test loss: 1.815, Test accuracy: 64.20
Round  99, Train loss: 1.225, Test loss: 1.816, Test accuracy: 63.87
Final Round, Train loss: 1.236, Test loss: 1.818, Test accuracy: 63.80
Average accuracy final 10 rounds: 65.45
816.8694093227386
[]
[32.93333333333333, 41.53333333333333, 44.516666666666666, 53.416666666666664, 57.166666666666664, 60.0, 61.4, 60.4, 59.9, 63.21666666666667, 63.483333333333334, 66.3, 68.08333333333333, 68.16666666666667, 67.76666666666667, 67.9, 67.4, 67.75, 67.88333333333334, 69.31666666666666, 69.15, 68.86666666666666, 68.63333333333334, 69.2, 69.36666666666666, 69.28333333333333, 68.96666666666667, 69.03333333333333, 70.25, 69.91666666666667, 69.81666666666666, 69.96666666666667, 69.43333333333334, 69.65, 69.28333333333333, 70.45, 70.8, 70.6, 70.93333333333334, 70.68333333333334, 70.8, 70.81666666666666, 70.85, 71.13333333333334, 70.7, 69.78333333333333, 69.78333333333333, 69.81666666666666, 69.53333333333333, 69.48333333333333, 69.61666666666666, 69.48333333333333, 69.73333333333333, 69.73333333333333, 69.71666666666667, 69.46666666666667, 69.13333333333334, 69.9, 70.08333333333333, 70.23333333333333, 70.05, 69.66666666666667, 70.03333333333333, 69.8, 70.15, 69.66666666666667, 69.16666666666667, 69.53333333333333, 69.31666666666666, 69.41666666666667, 69.15, 69.35, 69.2, 69.55, 69.61666666666666, 69.3, 69.3, 67.86666666666666, 68.45, 67.9, 67.75, 67.28333333333333, 67.43333333333334, 66.96666666666667, 67.38333333333334, 67.41666666666667, 67.71666666666667, 67.75, 66.95, 66.48333333333333, 66.76666666666667, 65.56666666666666, 65.6, 65.7, 66.35, 66.06666666666666, 65.13333333333334, 65.25, 64.2, 63.86666666666667, 63.8]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.285, Test loss: 2.290, Test accuracy: 19.67
Round   1, Train loss: 2.265, Test loss: 2.272, Test accuracy: 22.33
Round   2, Train loss: 2.221, Test loss: 2.268, Test accuracy: 14.57
Round   3, Train loss: 2.248, Test loss: 2.264, Test accuracy: 15.98
Round   4, Train loss: 2.067, Test loss: 2.196, Test accuracy: 26.80
Round   5, Train loss: 2.122, Test loss: 2.172, Test accuracy: 27.78
Round   6, Train loss: 2.064, Test loss: 2.179, Test accuracy: 29.53
Round   7, Train loss: 1.944, Test loss: 2.084, Test accuracy: 43.75
Round   8, Train loss: 2.015, Test loss: 2.127, Test accuracy: 36.50
Round   9, Train loss: 1.983, Test loss: 2.148, Test accuracy: 29.87
Round  10, Train loss: 1.817, Test loss: 2.169, Test accuracy: 24.90
Round  11, Train loss: 1.492, Test loss: 2.032, Test accuracy: 42.82
Round  12, Train loss: 1.221, Test loss: 1.938, Test accuracy: 57.35
Round  13, Train loss: 1.707, Test loss: 2.011, Test accuracy: 52.48
Round  14, Train loss: 1.033, Test loss: 1.916, Test accuracy: 59.28
Round  15, Train loss: 0.305, Test loss: 1.838, Test accuracy: 70.20
Round  16, Train loss: 1.012, Test loss: 1.900, Test accuracy: 67.28
Round  17, Train loss: 0.425, Test loss: 1.878, Test accuracy: 65.92
Round  18, Train loss: -0.152, Test loss: 1.810, Test accuracy: 69.97
Round  19, Train loss: 1.085, Test loss: 1.983, Test accuracy: 52.45
Round  20, Train loss: 0.414, Test loss: 1.952, Test accuracy: 54.07
Round  21, Train loss: 0.709, Test loss: 2.027, Test accuracy: 47.78
Round  22, Train loss: 0.298, Test loss: 1.989, Test accuracy: 54.25
Round  23, Train loss: 0.226, Test loss: 1.927, Test accuracy: 57.70
Round  24, Train loss: 0.150, Test loss: 1.902, Test accuracy: 56.92
Round  25, Train loss: -1.144, Test loss: 1.844, Test accuracy: 61.13
Round  26, Train loss: -0.699, Test loss: 1.838, Test accuracy: 61.17
Round  27, Train loss: -1.248, Test loss: 1.748, Test accuracy: 71.48
Round  28, Train loss: -0.298, Test loss: 1.810, Test accuracy: 69.37
Round  29, Train loss: -0.750, Test loss: 1.785, Test accuracy: 70.40
Round  30, Train loss: -0.679, Test loss: 1.831, Test accuracy: 68.15
Round  31, Train loss: -2.097, Test loss: 1.802, Test accuracy: 70.58
Round  32, Train loss: -1.421, Test loss: 1.641, Test accuracy: 83.87
Round  33, Train loss: -2.074, Test loss: 1.671, Test accuracy: 80.90
Round  34, Train loss: -1.548, Test loss: 1.712, Test accuracy: 77.42
Round  35, Train loss: -2.340, Test loss: 1.703, Test accuracy: 77.08
Round  36, Train loss: -2.251, Test loss: 1.669, Test accuracy: 80.65
Round  37, Train loss: -2.403, Test loss: 1.665, Test accuracy: 81.20
Round  38, Train loss: -2.599, Test loss: 1.603, Test accuracy: 87.00
Round  39, Train loss: -2.555, Test loss: 1.590, Test accuracy: 89.13
Round  40, Train loss: -2.167, Test loss: 1.625, Test accuracy: 86.58
Round  41, Train loss: -3.403, Test loss: 1.620, Test accuracy: 87.07
Round  42, Train loss: -1.919, Test loss: 1.640, Test accuracy: 84.82
Round  43, Train loss: -3.083, Test loss: 1.618, Test accuracy: 86.75
Round  44, Train loss: -2.777, Test loss: 1.650, Test accuracy: 83.90
Round  45, Train loss: -2.916, Test loss: 1.650, Test accuracy: 83.50
Round  46, Train loss: -3.066, Test loss: 1.600, Test accuracy: 86.92
Round  47, Train loss: -4.141, Test loss: 1.580, Test accuracy: 88.58
Round  48, Train loss: -2.845, Test loss: 1.593, Test accuracy: 87.12
Round  49, Train loss: -3.563, Test loss: 1.564, Test accuracy: 89.85
Round  50, Train loss: -2.848, Test loss: 1.567, Test accuracy: 89.75
Round  51, Train loss: -4.279, Test loss: 1.556, Test accuracy: 91.15
Round  52, Train loss: -2.603, Test loss: 1.569, Test accuracy: 89.78
Round  53, Train loss: -4.105, Test loss: 1.563, Test accuracy: 90.20
Round  54, Train loss: -3.889, Test loss: 1.559, Test accuracy: 90.35
Round  55, Train loss: -3.289, Test loss: 1.548, Test accuracy: 91.95
Round  56, Train loss: -5.304, Test loss: 1.548, Test accuracy: 91.98
Round  57, Train loss: -4.619, Test loss: 1.548, Test accuracy: 91.93
Round  58, Train loss: -4.676, Test loss: 1.534, Test accuracy: 93.25
Round  59, Train loss: -4.322, Test loss: 1.530, Test accuracy: 93.20
Round  60, Train loss: -4.142, Test loss: 1.545, Test accuracy: 91.78
Round  61, Train loss: -4.735, Test loss: 1.542, Test accuracy: 92.02
Round  62, Train loss: -4.861, Test loss: 1.528, Test accuracy: 93.43
Round  63, Train loss: -5.098, Test loss: 1.546, Test accuracy: 91.72
Round  64, Train loss: -4.234, Test loss: 1.545, Test accuracy: 91.78
Round  65, Train loss: -4.786, Test loss: 1.546, Test accuracy: 91.67
Round  66, Train loss: -4.802, Test loss: 1.531, Test accuracy: 93.13
Round  67, Train loss: -5.821, Test loss: 1.548, Test accuracy: 91.38
Round  68, Train loss: -5.392, Test loss: 1.535, Test accuracy: 92.63
Round  69, Train loss: -5.071, Test loss: 1.548, Test accuracy: 91.38
Round  70, Train loss: -4.965, Test loss: 1.549, Test accuracy: 91.18
Round  71, Train loss: -4.526, Test loss: 1.534, Test accuracy: 92.68
Round  72, Train loss: -4.667, Test loss: 1.530, Test accuracy: 93.02
Round  73, Train loss: -4.839, Test loss: 1.531, Test accuracy: 92.98
Round  74, Train loss: -6.183, Test loss: 1.544, Test accuracy: 91.75
Round  75, Train loss: -3.982, Test loss: 1.557, Test accuracy: 90.48
Round  76, Train loss: -5.366, Test loss: 1.545, Test accuracy: 91.58
Round  77, Train loss: -5.411, Test loss: 1.560, Test accuracy: 90.12
Round  78, Train loss: -5.430, Test loss: 1.548, Test accuracy: 91.23
Round  79, Train loss: -4.801, Test loss: 1.529, Test accuracy: 93.10
Round  80, Train loss: -4.234, Test loss: 1.542, Test accuracy: 91.90
Round  81, Train loss: -4.253, Test loss: 1.559, Test accuracy: 90.18
Round  82, Train loss: -5.083, Test loss: 1.558, Test accuracy: 90.23
Round  83, Train loss: -5.138, Test loss: 1.557, Test accuracy: 90.32
Round  84, Train loss: -4.665, Test loss: 1.556, Test accuracy: 90.45
Round  85, Train loss: -4.863, Test loss: 1.540, Test accuracy: 92.05
Round  86, Train loss: -4.591, Test loss: 1.572, Test accuracy: 88.85
Round  87, Train loss: -5.604, Test loss: 1.560, Test accuracy: 90.13
Round  88, Train loss: -4.894, Test loss: 1.557, Test accuracy: 90.47
Round  89, Train loss: -4.980, Test loss: 1.541, Test accuracy: 91.97
Round  90, Train loss: -5.005, Test loss: 1.529, Test accuracy: 93.27
Round  91, Train loss: -4.641, Test loss: 1.542, Test accuracy: 91.93
Round  92, Train loss: -4.378, Test loss: 1.541, Test accuracy: 92.07
Round  93, Train loss: -4.338, Test loss: 1.542, Test accuracy: 91.90
Round  94, Train loss: -4.949, Test loss: 1.572, Test accuracy: 88.97
Round  95, Train loss: -4.867, Test loss: 1.544, Test accuracy: 91.82
Round  96, Train loss: -5.004, Test loss: 1.528, Test accuracy: 93.30
Round  97, Train loss: -4.507, Test loss: 1.543, Test accuracy: 91.83
Round  98, Train loss: -4.471, Test loss: 1.543, Test accuracy: 91.85
Round  99, Train loss: -4.992, Test loss: 1.543, Test accuracy: 91.83
Final Round, Train loss: 1.720, Test loss: 1.651, Test accuracy: 81.90
Average accuracy final 10 rounds: 91.87666666666668
Average global accuracy final 10 rounds: 91.87666666666668
619.8457319736481
[]
[19.666666666666668, 22.333333333333332, 14.566666666666666, 15.983333333333333, 26.8, 27.783333333333335, 29.533333333333335, 43.75, 36.5, 29.866666666666667, 24.9, 42.81666666666667, 57.35, 52.483333333333334, 59.28333333333333, 70.2, 67.28333333333333, 65.91666666666667, 69.96666666666667, 52.45, 54.06666666666667, 47.78333333333333, 54.25, 57.7, 56.916666666666664, 61.13333333333333, 61.166666666666664, 71.48333333333333, 69.36666666666666, 70.4, 68.15, 70.58333333333333, 83.86666666666666, 80.9, 77.41666666666667, 77.08333333333333, 80.65, 81.2, 87.0, 89.13333333333334, 86.58333333333333, 87.06666666666666, 84.81666666666666, 86.75, 83.9, 83.5, 86.91666666666667, 88.58333333333333, 87.11666666666666, 89.85, 89.75, 91.15, 89.78333333333333, 90.2, 90.35, 91.95, 91.98333333333333, 91.93333333333334, 93.25, 93.2, 91.78333333333333, 92.01666666666667, 93.43333333333334, 91.71666666666667, 91.78333333333333, 91.66666666666667, 93.13333333333334, 91.38333333333334, 92.63333333333334, 91.38333333333334, 91.18333333333334, 92.68333333333334, 93.01666666666667, 92.98333333333333, 91.75, 90.48333333333333, 91.58333333333333, 90.11666666666666, 91.23333333333333, 93.1, 91.9, 90.18333333333334, 90.23333333333333, 90.31666666666666, 90.45, 92.05, 88.85, 90.13333333333334, 90.46666666666667, 91.96666666666667, 93.26666666666667, 91.93333333333334, 92.06666666666666, 91.9, 88.96666666666667, 91.81666666666666, 93.3, 91.83333333333333, 91.85, 91.83333333333333, 81.9]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

prox
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.283, Test loss: 2.283, Test accuracy: 19.80
Round   0, Global train loss: 2.283, Global test loss: 2.302, Global test accuracy: 13.02
Round   1, Train loss: 2.207, Test loss: 2.211, Test accuracy: 27.32
Round   1, Global train loss: 2.207, Global test loss: 2.293, Global test accuracy: 12.25
Round   2, Train loss: 2.058, Test loss: 2.121, Test accuracy: 36.88
Round   2, Global train loss: 2.058, Global test loss: 2.269, Global test accuracy: 18.15
Round   3, Train loss: 2.013, Test loss: 2.078, Test accuracy: 41.15
Round   3, Global train loss: 2.013, Global test loss: 2.278, Global test accuracy: 16.67
Round   4, Train loss: 1.965, Test loss: 2.005, Test accuracy: 50.60
Round   4, Global train loss: 1.965, Global test loss: 2.251, Global test accuracy: 21.60
Round   5, Train loss: 1.895, Test loss: 1.964, Test accuracy: 54.72
Round   5, Global train loss: 1.895, Global test loss: 2.253, Global test accuracy: 21.68
Round   6, Train loss: 1.925, Test loss: 1.849, Test accuracy: 64.97
Round   6, Global train loss: 1.925, Global test loss: 2.251, Global test accuracy: 17.18
Round   7, Train loss: 1.892, Test loss: 1.846, Test accuracy: 65.10
Round   7, Global train loss: 1.892, Global test loss: 2.248, Global test accuracy: 18.45
Round   8, Train loss: 1.770, Test loss: 1.801, Test accuracy: 69.52
Round   8, Global train loss: 1.770, Global test loss: 2.231, Global test accuracy: 20.87
Round   9, Train loss: 1.929, Test loss: 1.810, Test accuracy: 67.75
Round   9, Global train loss: 1.929, Global test loss: 2.219, Global test accuracy: 23.85
Round  10, Train loss: 1.778, Test loss: 1.794, Test accuracy: 69.38
Round  10, Global train loss: 1.778, Global test loss: 2.237, Global test accuracy: 20.88
Round  11, Train loss: 1.928, Test loss: 1.770, Test accuracy: 72.05
Round  11, Global train loss: 1.928, Global test loss: 2.228, Global test accuracy: 21.38
Round  12, Train loss: 1.767, Test loss: 1.768, Test accuracy: 70.97
Round  12, Global train loss: 1.767, Global test loss: 2.226, Global test accuracy: 20.30
Round  13, Train loss: 1.690, Test loss: 1.762, Test accuracy: 71.10
Round  13, Global train loss: 1.690, Global test loss: 2.217, Global test accuracy: 22.43
Round  14, Train loss: 1.808, Test loss: 1.759, Test accuracy: 71.17
Round  14, Global train loss: 1.808, Global test loss: 2.223, Global test accuracy: 22.77
Round  15, Train loss: 1.795, Test loss: 1.778, Test accuracy: 69.12
Round  15, Global train loss: 1.795, Global test loss: 2.224, Global test accuracy: 22.55
Round  16, Train loss: 1.736, Test loss: 1.762, Test accuracy: 70.70
Round  16, Global train loss: 1.736, Global test loss: 2.211, Global test accuracy: 22.67
Round  17, Train loss: 1.682, Test loss: 1.762, Test accuracy: 70.77
Round  17, Global train loss: 1.682, Global test loss: 2.209, Global test accuracy: 23.92
Round  18, Train loss: 1.814, Test loss: 1.761, Test accuracy: 70.55
Round  18, Global train loss: 1.814, Global test loss: 2.217, Global test accuracy: 21.98
Round  19, Train loss: 1.704, Test loss: 1.765, Test accuracy: 70.27
Round  19, Global train loss: 1.704, Global test loss: 2.218, Global test accuracy: 22.52
Round  20, Train loss: 1.776, Test loss: 1.765, Test accuracy: 70.10
Round  20, Global train loss: 1.776, Global test loss: 2.225, Global test accuracy: 22.00
Round  21, Train loss: 1.737, Test loss: 1.764, Test accuracy: 70.22
Round  21, Global train loss: 1.737, Global test loss: 2.215, Global test accuracy: 22.82
Round  22, Train loss: 1.779, Test loss: 1.748, Test accuracy: 71.80
Round  22, Global train loss: 1.779, Global test loss: 2.216, Global test accuracy: 22.43
Round  23, Train loss: 1.753, Test loss: 1.747, Test accuracy: 71.95
Round  23, Global train loss: 1.753, Global test loss: 2.238, Global test accuracy: 20.00
Round  24, Train loss: 1.677, Test loss: 1.734, Test accuracy: 73.38
Round  24, Global train loss: 1.677, Global test loss: 2.230, Global test accuracy: 20.33
Round  25, Train loss: 1.756, Test loss: 1.734, Test accuracy: 73.30
Round  25, Global train loss: 1.756, Global test loss: 2.226, Global test accuracy: 20.90
Round  26, Train loss: 1.678, Test loss: 1.736, Test accuracy: 73.10
Round  26, Global train loss: 1.678, Global test loss: 2.226, Global test accuracy: 21.60
Round  27, Train loss: 1.691, Test loss: 1.748, Test accuracy: 71.53
Round  27, Global train loss: 1.691, Global test loss: 2.224, Global test accuracy: 22.43
Round  28, Train loss: 1.875, Test loss: 1.746, Test accuracy: 71.70
Round  28, Global train loss: 1.875, Global test loss: 2.224, Global test accuracy: 22.55
Round  29, Train loss: 1.813, Test loss: 1.745, Test accuracy: 71.85
Round  29, Global train loss: 1.813, Global test loss: 2.217, Global test accuracy: 23.73
Round  30, Train loss: 1.797, Test loss: 1.746, Test accuracy: 71.78
Round  30, Global train loss: 1.797, Global test loss: 2.216, Global test accuracy: 22.88
Round  31, Train loss: 1.883, Test loss: 1.746, Test accuracy: 71.70
Round  31, Global train loss: 1.883, Global test loss: 2.225, Global test accuracy: 21.95
Round  32, Train loss: 1.727, Test loss: 1.747, Test accuracy: 71.60
Round  32, Global train loss: 1.727, Global test loss: 2.231, Global test accuracy: 21.07
Round  33, Train loss: 1.834, Test loss: 1.746, Test accuracy: 71.70
Round  33, Global train loss: 1.834, Global test loss: 2.215, Global test accuracy: 23.45
Round  34, Train loss: 1.815, Test loss: 1.731, Test accuracy: 73.28
Round  34, Global train loss: 1.815, Global test loss: 2.222, Global test accuracy: 22.08
Round  35, Train loss: 1.731, Test loss: 1.730, Test accuracy: 73.40
Round  35, Global train loss: 1.731, Global test loss: 2.228, Global test accuracy: 20.93
Round  36, Train loss: 1.656, Test loss: 1.730, Test accuracy: 73.37
Round  36, Global train loss: 1.656, Global test loss: 2.204, Global test accuracy: 24.82
Round  37, Train loss: 1.710, Test loss: 1.731, Test accuracy: 73.23
Round  37, Global train loss: 1.710, Global test loss: 2.218, Global test accuracy: 22.55
Round  38, Train loss: 1.818, Test loss: 1.746, Test accuracy: 71.58
Round  38, Global train loss: 1.818, Global test loss: 2.203, Global test accuracy: 24.15
Round  39, Train loss: 1.673, Test loss: 1.717, Test accuracy: 74.67
Round  39, Global train loss: 1.673, Global test loss: 2.216, Global test accuracy: 23.07
Round  40, Train loss: 1.712, Test loss: 1.717, Test accuracy: 74.58
Round  40, Global train loss: 1.712, Global test loss: 2.222, Global test accuracy: 21.80
Round  41, Train loss: 1.655, Test loss: 1.731, Test accuracy: 73.28
Round  41, Global train loss: 1.655, Global test loss: 2.207, Global test accuracy: 23.62
Round  42, Train loss: 1.702, Test loss: 1.730, Test accuracy: 73.37
Round  42, Global train loss: 1.702, Global test loss: 2.214, Global test accuracy: 23.02
Round  43, Train loss: 1.823, Test loss: 1.729, Test accuracy: 73.37
Round  43, Global train loss: 1.823, Global test loss: 2.209, Global test accuracy: 23.53
Round  44, Train loss: 1.813, Test loss: 1.729, Test accuracy: 73.30
Round  44, Global train loss: 1.813, Global test loss: 2.208, Global test accuracy: 23.45
Round  45, Train loss: 1.629, Test loss: 1.715, Test accuracy: 74.77
Round  45, Global train loss: 1.629, Global test loss: 2.211, Global test accuracy: 23.92
Round  46, Train loss: 1.686, Test loss: 1.714, Test accuracy: 74.83
Round  46, Global train loss: 1.686, Global test loss: 2.212, Global test accuracy: 23.13
Round  47, Train loss: 1.709, Test loss: 1.714, Test accuracy: 74.92
Round  47, Global train loss: 1.709, Global test loss: 2.212, Global test accuracy: 23.15
Round  48, Train loss: 1.664, Test loss: 1.714, Test accuracy: 75.00
Round  48, Global train loss: 1.664, Global test loss: 2.198, Global test accuracy: 24.98
Round  49, Train loss: 1.763, Test loss: 1.713, Test accuracy: 74.98
Round  49, Global train loss: 1.763, Global test loss: 2.190, Global test accuracy: 25.92
Round  50, Train loss: 1.731, Test loss: 1.713, Test accuracy: 74.92
Round  50, Global train loss: 1.731, Global test loss: 2.204, Global test accuracy: 24.00
Round  51, Train loss: 1.707, Test loss: 1.714, Test accuracy: 74.83
Round  51, Global train loss: 1.707, Global test loss: 2.208, Global test accuracy: 23.82
Round  52, Train loss: 1.716, Test loss: 1.729, Test accuracy: 73.30
Round  52, Global train loss: 1.716, Global test loss: 2.196, Global test accuracy: 25.30
Round  53, Train loss: 1.815, Test loss: 1.729, Test accuracy: 73.28
Round  53, Global train loss: 1.815, Global test loss: 2.205, Global test accuracy: 24.43
Round  54, Train loss: 1.758, Test loss: 1.728, Test accuracy: 73.37
Round  54, Global train loss: 1.758, Global test loss: 2.191, Global test accuracy: 25.57
Round  55, Train loss: 1.763, Test loss: 1.728, Test accuracy: 73.28
Round  55, Global train loss: 1.763, Global test loss: 2.198, Global test accuracy: 24.88
Round  56, Train loss: 1.654, Test loss: 1.728, Test accuracy: 73.30
Round  56, Global train loss: 1.654, Global test loss: 2.206, Global test accuracy: 24.38
Round  57, Train loss: 1.905, Test loss: 1.727, Test accuracy: 73.48
Round  57, Global train loss: 1.905, Global test loss: 2.206, Global test accuracy: 24.13
Round  58, Train loss: 1.757, Test loss: 1.727, Test accuracy: 73.55
Round  58, Global train loss: 1.757, Global test loss: 2.202, Global test accuracy: 24.70
Round  59, Train loss: 1.688, Test loss: 1.712, Test accuracy: 75.12
Round  59, Global train loss: 1.688, Global test loss: 2.224, Global test accuracy: 21.55
Round  60, Train loss: 1.752, Test loss: 1.712, Test accuracy: 75.05
Round  60, Global train loss: 1.752, Global test loss: 2.207, Global test accuracy: 24.13
Round  61, Train loss: 1.669, Test loss: 1.712, Test accuracy: 75.10
Round  61, Global train loss: 1.669, Global test loss: 2.203, Global test accuracy: 24.40
Round  62, Train loss: 1.659, Test loss: 1.712, Test accuracy: 75.08
Round  62, Global train loss: 1.659, Global test loss: 2.206, Global test accuracy: 23.70
Round  63, Train loss: 1.706, Test loss: 1.712, Test accuracy: 75.13
Round  63, Global train loss: 1.706, Global test loss: 2.210, Global test accuracy: 23.72
Round  64, Train loss: 1.761, Test loss: 1.726, Test accuracy: 73.60
Round  64, Global train loss: 1.761, Global test loss: 2.224, Global test accuracy: 22.08
Round  65, Train loss: 1.824, Test loss: 1.715, Test accuracy: 74.68
Round  65, Global train loss: 1.824, Global test loss: 2.196, Global test accuracy: 25.45
Round  66, Train loss: 1.714, Test loss: 1.715, Test accuracy: 74.68
Round  66, Global train loss: 1.714, Global test loss: 2.220, Global test accuracy: 21.47
Round  67, Train loss: 1.654, Test loss: 1.715, Test accuracy: 74.70
Round  67, Global train loss: 1.654, Global test loss: 2.197, Global test accuracy: 25.78
Round  68, Train loss: 1.591, Test loss: 1.715, Test accuracy: 74.72
Round  68, Global train loss: 1.591, Global test loss: 2.189, Global test accuracy: 26.20
Round  69, Train loss: 1.712, Test loss: 1.713, Test accuracy: 74.93
Round  69, Global train loss: 1.712, Global test loss: 2.203, Global test accuracy: 24.87
Round  70, Train loss: 1.636, Test loss: 1.713, Test accuracy: 74.93
Round  70, Global train loss: 1.636, Global test loss: 2.222, Global test accuracy: 22.38
Round  71, Train loss: 1.711, Test loss: 1.714, Test accuracy: 74.88
Round  71, Global train loss: 1.711, Global test loss: 2.215, Global test accuracy: 23.52
Round  72, Train loss: 1.705, Test loss: 1.713, Test accuracy: 74.92
Round  72, Global train loss: 1.705, Global test loss: 2.200, Global test accuracy: 24.40
Round  73, Train loss: 1.704, Test loss: 1.714, Test accuracy: 74.82
Round  73, Global train loss: 1.704, Global test loss: 2.215, Global test accuracy: 22.70
Round  74, Train loss: 1.750, Test loss: 1.714, Test accuracy: 74.78
Round  74, Global train loss: 1.750, Global test loss: 2.182, Global test accuracy: 26.65
Round  75, Train loss: 1.640, Test loss: 1.714, Test accuracy: 74.73
Round  75, Global train loss: 1.640, Global test loss: 2.197, Global test accuracy: 24.95
Round  76, Train loss: 1.714, Test loss: 1.713, Test accuracy: 74.88
Round  76, Global train loss: 1.714, Global test loss: 2.207, Global test accuracy: 24.00
Round  77, Train loss: 1.776, Test loss: 1.713, Test accuracy: 74.73
Round  77, Global train loss: 1.776, Global test loss: 2.201, Global test accuracy: 24.65
Round  78, Train loss: 1.877, Test loss: 1.728, Test accuracy: 73.15
Round  78, Global train loss: 1.877, Global test loss: 2.223, Global test accuracy: 22.57
Round  79, Train loss: 1.672, Test loss: 1.713, Test accuracy: 74.78
Round  79, Global train loss: 1.672, Global test loss: 2.190, Global test accuracy: 25.22
Round  80, Train loss: 1.646, Test loss: 1.713, Test accuracy: 74.77
Round  80, Global train loss: 1.646, Global test loss: 2.198, Global test accuracy: 24.27
Round  81, Train loss: 1.700, Test loss: 1.713, Test accuracy: 74.78
Round  81, Global train loss: 1.700, Global test loss: 2.233, Global test accuracy: 20.52
Round  82, Train loss: 1.706, Test loss: 1.719, Test accuracy: 74.25
Round  82, Global train loss: 1.706, Global test loss: 2.221, Global test accuracy: 22.28
Round  83, Train loss: 1.754, Test loss: 1.719, Test accuracy: 74.33
Round  83, Global train loss: 1.754, Global test loss: 2.210, Global test accuracy: 23.83
Round  84, Train loss: 1.753, Test loss: 1.719, Test accuracy: 74.30
Round  84, Global train loss: 1.753, Global test loss: 2.184, Global test accuracy: 26.42
Round  85, Train loss: 1.706, Test loss: 1.716, Test accuracy: 74.55
Round  85, Global train loss: 1.706, Global test loss: 2.184, Global test accuracy: 26.52
Round  86, Train loss: 1.634, Test loss: 1.695, Test accuracy: 76.73
Round  86, Global train loss: 1.634, Global test loss: 2.217, Global test accuracy: 23.02
Round  87, Train loss: 1.727, Test loss: 1.694, Test accuracy: 76.73
Round  87, Global train loss: 1.727, Global test loss: 2.220, Global test accuracy: 22.15
Round  88, Train loss: 1.655, Test loss: 1.694, Test accuracy: 76.85
Round  88, Global train loss: 1.655, Global test loss: 2.216, Global test accuracy: 22.72
Round  89, Train loss: 1.770, Test loss: 1.694, Test accuracy: 76.65
Round  89, Global train loss: 1.770, Global test loss: 2.212, Global test accuracy: 23.32
Round  90, Train loss: 1.681, Test loss: 1.680, Test accuracy: 78.22
Round  90, Global train loss: 1.681, Global test loss: 2.201, Global test accuracy: 24.17
Round  91, Train loss: 1.723, Test loss: 1.682, Test accuracy: 78.08
Round  91, Global train loss: 1.723, Global test loss: 2.188, Global test accuracy: 26.12
Round  92, Train loss: 1.657, Test loss: 1.681, Test accuracy: 78.15
Round  92, Global train loss: 1.657, Global test loss: 2.215, Global test accuracy: 23.10
Round  93, Train loss: 1.744, Test loss: 1.681, Test accuracy: 78.12
Round  93, Global train loss: 1.744, Global test loss: 2.209, Global test accuracy: 24.10
Round  94, Train loss: 1.717, Test loss: 1.681, Test accuracy: 78.17
Round  94, Global train loss: 1.717, Global test loss: 2.201, Global test accuracy: 24.53
Round  95, Train loss: 1.727, Test loss: 1.681, Test accuracy: 78.17
Round  95, Global train loss: 1.727, Global test loss: 2.192, Global test accuracy: 25.95/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  96, Train loss: 1.703, Test loss: 1.681, Test accuracy: 78.23
Round  96, Global train loss: 1.703, Global test loss: 2.202, Global test accuracy: 24.75
Round  97, Train loss: 1.624, Test loss: 1.680, Test accuracy: 78.37
Round  97, Global train loss: 1.624, Global test loss: 2.226, Global test accuracy: 21.90
Round  98, Train loss: 1.760, Test loss: 1.681, Test accuracy: 78.23
Round  98, Global train loss: 1.760, Global test loss: 2.182, Global test accuracy: 26.73
Round  99, Train loss: 1.663, Test loss: 1.683, Test accuracy: 78.13
Round  99, Global train loss: 1.663, Global test loss: 2.229, Global test accuracy: 20.97
Final Round, Train loss: 1.670, Test loss: 1.679, Test accuracy: 78.37
Final Round, Global train loss: 1.670, Global test loss: 2.229, Global test accuracy: 20.97
Average accuracy final 10 rounds: 78.18666666666667 

Average global accuracy final 10 rounds: 24.23166666666667 

880.9573888778687
[0.6616432666778564, 1.323286533355713, 1.8931989669799805, 2.463111400604248, 3.003300428390503, 3.543489456176758, 4.109550714492798, 4.675611972808838, 5.218036651611328, 5.760461330413818, 6.3197181224823, 6.878974914550781, 7.434175252914429, 7.989375591278076, 8.54067087173462, 9.091966152191162, 9.655054569244385, 10.218142986297607, 10.788516283035278, 11.35888957977295, 11.907200813293457, 12.455512046813965, 13.023784399032593, 13.59205675125122, 14.130432844161987, 14.668808937072754, 15.213998556137085, 15.759188175201416, 16.315072059631348, 16.87095594406128, 17.4074809551239, 17.944005966186523, 18.50612711906433, 19.06824827194214, 19.633848190307617, 20.199448108673096, 20.74117660522461, 21.282905101776123, 21.84715247154236, 22.411399841308594, 22.967695951461792, 23.52399206161499, 24.09962773323059, 24.67526340484619, 25.2202730178833, 25.76528263092041, 26.292368173599243, 26.819453716278076, 27.378821849822998, 27.93818998336792, 28.505059003829956, 29.071928024291992, 29.613187313079834, 30.154446601867676, 30.71584701538086, 31.277247428894043, 31.83618426322937, 32.3951210975647, 32.96753811836243, 33.539955139160156, 34.102959871292114, 34.66596460342407, 35.19960618019104, 35.73324775695801, 36.27661108970642, 36.819974422454834, 37.37590050697327, 37.9318265914917, 38.48406481742859, 39.03630304336548, 39.59738802909851, 40.15847301483154, 40.70645785331726, 41.25444269180298, 41.82158899307251, 42.38873529434204, 42.95744466781616, 43.52615404129028, 44.06265950202942, 44.599164962768555, 45.14639759063721, 45.69363021850586, 46.230679750442505, 46.76772928237915, 47.320576906204224, 47.8734245300293, 48.43579888343811, 48.998173236846924, 49.55180811882019, 50.10544300079346, 50.66603755950928, 51.2266321182251, 51.793646574020386, 52.360661029815674, 52.911890268325806, 53.46311950683594, 54.00933885574341, 54.55555820465088, 55.097413301467896, 55.63926839828491, 56.17486095428467, 56.710453510284424, 57.27447175979614, 57.83849000930786, 58.4037983417511, 58.969106674194336, 59.49959659576416, 60.030086517333984, 60.59188985824585, 61.153693199157715, 61.70852065086365, 62.26334810256958, 62.82599472999573, 63.388641357421875, 63.93305587768555, 64.47747039794922, 64.98417901992798, 65.49088764190674, 66.0410988330841, 66.59131002426147, 67.1449818611145, 67.69865369796753, 68.23620629310608, 68.77375888824463, 69.32291221618652, 69.87206554412842, 70.38524293899536, 70.8984203338623, 71.43514728546143, 71.97187423706055, 72.52364301681519, 73.07541179656982, 73.6174201965332, 74.15942859649658, 74.70323300361633, 75.24703741073608, 75.81007885932922, 76.37312030792236, 76.9252724647522, 77.47742462158203, 78.02183198928833, 78.56623935699463, 79.11088371276855, 79.65552806854248, 80.18996620178223, 80.72440433502197, 81.27050805091858, 81.81661176681519, 82.38791704177856, 82.95922231674194, 83.52027344703674, 84.08132457733154, 84.6220543384552, 85.16278409957886, 85.72376680374146, 86.28474950790405, 86.84368681907654, 87.40262413024902, 87.94142484664917, 88.48022556304932, 88.98913598060608, 89.49804639816284, 90.03981041908264, 90.58157444000244, 91.13677644729614, 91.69197845458984, 92.24549674987793, 92.79901504516602, 93.36253690719604, 93.92605876922607, 94.46906733512878, 95.0120759010315, 95.58690738677979, 96.16173887252808, 96.73565983772278, 97.30958080291748, 97.82369780540466, 98.33781480789185, 98.88041710853577, 99.42301940917969, 99.99839997291565, 100.57378053665161, 101.12609505653381, 101.67840957641602, 102.23895907402039, 102.79950857162476, 103.34508109092712, 103.89065361022949, 104.45923566818237, 105.02781772613525, 105.59552764892578, 106.16323757171631, 106.6749849319458, 107.1867322921753, 107.73822927474976, 108.28972625732422, 108.85291647911072, 109.41610670089722, 109.95262145996094, 110.48913621902466, 111.57562923431396, 112.66212224960327]
[19.8, 19.8, 27.316666666666666, 27.316666666666666, 36.88333333333333, 36.88333333333333, 41.15, 41.15, 50.6, 50.6, 54.71666666666667, 54.71666666666667, 64.96666666666667, 64.96666666666667, 65.1, 65.1, 69.51666666666667, 69.51666666666667, 67.75, 67.75, 69.38333333333334, 69.38333333333334, 72.05, 72.05, 70.96666666666667, 70.96666666666667, 71.1, 71.1, 71.16666666666667, 71.16666666666667, 69.11666666666666, 69.11666666666666, 70.7, 70.7, 70.76666666666667, 70.76666666666667, 70.55, 70.55, 70.26666666666667, 70.26666666666667, 70.1, 70.1, 70.21666666666667, 70.21666666666667, 71.8, 71.8, 71.95, 71.95, 73.38333333333334, 73.38333333333334, 73.3, 73.3, 73.1, 73.1, 71.53333333333333, 71.53333333333333, 71.7, 71.7, 71.85, 71.85, 71.78333333333333, 71.78333333333333, 71.7, 71.7, 71.6, 71.6, 71.7, 71.7, 73.28333333333333, 73.28333333333333, 73.4, 73.4, 73.36666666666666, 73.36666666666666, 73.23333333333333, 73.23333333333333, 71.58333333333333, 71.58333333333333, 74.66666666666667, 74.66666666666667, 74.58333333333333, 74.58333333333333, 73.28333333333333, 73.28333333333333, 73.36666666666666, 73.36666666666666, 73.36666666666666, 73.36666666666666, 73.3, 73.3, 74.76666666666667, 74.76666666666667, 74.83333333333333, 74.83333333333333, 74.91666666666667, 74.91666666666667, 75.0, 75.0, 74.98333333333333, 74.98333333333333, 74.91666666666667, 74.91666666666667, 74.83333333333333, 74.83333333333333, 73.3, 73.3, 73.28333333333333, 73.28333333333333, 73.36666666666666, 73.36666666666666, 73.28333333333333, 73.28333333333333, 73.3, 73.3, 73.48333333333333, 73.48333333333333, 73.55, 73.55, 75.11666666666666, 75.11666666666666, 75.05, 75.05, 75.1, 75.1, 75.08333333333333, 75.08333333333333, 75.13333333333334, 75.13333333333334, 73.6, 73.6, 74.68333333333334, 74.68333333333334, 74.68333333333334, 74.68333333333334, 74.7, 74.7, 74.71666666666667, 74.71666666666667, 74.93333333333334, 74.93333333333334, 74.93333333333334, 74.93333333333334, 74.88333333333334, 74.88333333333334, 74.91666666666667, 74.91666666666667, 74.81666666666666, 74.81666666666666, 74.78333333333333, 74.78333333333333, 74.73333333333333, 74.73333333333333, 74.88333333333334, 74.88333333333334, 74.73333333333333, 74.73333333333333, 73.15, 73.15, 74.78333333333333, 74.78333333333333, 74.76666666666667, 74.76666666666667, 74.78333333333333, 74.78333333333333, 74.25, 74.25, 74.33333333333333, 74.33333333333333, 74.3, 74.3, 74.55, 74.55, 76.73333333333333, 76.73333333333333, 76.73333333333333, 76.73333333333333, 76.85, 76.85, 76.65, 76.65, 78.21666666666667, 78.21666666666667, 78.08333333333333, 78.08333333333333, 78.15, 78.15, 78.11666666666666, 78.11666666666666, 78.16666666666667, 78.16666666666667, 78.16666666666667, 78.16666666666667, 78.23333333333333, 78.23333333333333, 78.36666666666666, 78.36666666666666, 78.23333333333333, 78.23333333333333, 78.13333333333334, 78.13333333333334, 78.36666666666666, 78.36666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.289, Test loss: 2.301, Test accuracy: 10.25
Round   1, Train loss: 2.246, Test loss: 2.290, Test accuracy: 19.78
Round   2, Train loss: 2.058, Test loss: 2.268, Test accuracy: 20.12
Round   3, Train loss: 1.876, Test loss: 2.280, Test accuracy: 14.93
Round   4, Train loss: 1.840, Test loss: 2.283, Test accuracy: 12.53
Round   5, Train loss: 1.787, Test loss: 2.239, Test accuracy: 21.20
Round   6, Train loss: 1.617, Test loss: 2.249, Test accuracy: 18.72
Round   7, Train loss: 1.716, Test loss: 2.282, Test accuracy: 14.30
Round   8, Train loss: 1.643, Test loss: 2.234, Test accuracy: 21.57
Round   9, Train loss: 1.646, Test loss: 2.232, Test accuracy: 21.35
Round  10, Train loss: 1.678, Test loss: 2.223, Test accuracy: 22.17
Round  11, Train loss: 1.578, Test loss: 2.236, Test accuracy: 20.77
Round  12, Train loss: 1.708, Test loss: 2.221, Test accuracy: 22.55
Round  13, Train loss: 1.676, Test loss: 2.231, Test accuracy: 21.05
Round  14, Train loss: 1.665, Test loss: 2.228, Test accuracy: 21.52
Round  15, Train loss: 1.602, Test loss: 2.236, Test accuracy: 20.45
Round  16, Train loss: 1.570, Test loss: 2.222, Test accuracy: 22.13
Round  17, Train loss: 1.581, Test loss: 2.228, Test accuracy: 21.67
Round  18, Train loss: 1.621, Test loss: 2.227, Test accuracy: 22.60
Round  19, Train loss: 1.587, Test loss: 2.222, Test accuracy: 21.68
Round  20, Train loss: 1.571, Test loss: 2.238, Test accuracy: 20.87
Round  21, Train loss: 1.617, Test loss: 2.210, Test accuracy: 23.07
Round  22, Train loss: 1.572, Test loss: 2.225, Test accuracy: 22.37
Round  23, Train loss: 1.657, Test loss: 2.215, Test accuracy: 23.60
Round  24, Train loss: 1.533, Test loss: 2.218, Test accuracy: 22.45
Round  25, Train loss: 1.610, Test loss: 2.226, Test accuracy: 22.22
Round  26, Train loss: 1.624, Test loss: 2.227, Test accuracy: 21.42
Round  27, Train loss: 1.567, Test loss: 2.223, Test accuracy: 21.87
Round  28, Train loss: 1.602, Test loss: 2.219, Test accuracy: 22.55
Round  29, Train loss: 1.510, Test loss: 2.230, Test accuracy: 20.62
Round  30, Train loss: 1.564, Test loss: 2.206, Test accuracy: 24.30
Round  31, Train loss: 1.561, Test loss: 2.204, Test accuracy: 24.72
Round  32, Train loss: 1.607, Test loss: 2.238, Test accuracy: 20.27
Round  33, Train loss: 1.562, Test loss: 2.213, Test accuracy: 23.90
Round  34, Train loss: 1.554, Test loss: 2.227, Test accuracy: 21.55
Round  35, Train loss: 1.511, Test loss: 2.231, Test accuracy: 21.23
Round  36, Train loss: 1.549, Test loss: 2.213, Test accuracy: 22.93
Round  37, Train loss: 1.642, Test loss: 2.223, Test accuracy: 21.87
Round  38, Train loss: 1.550, Test loss: 2.231, Test accuracy: 20.93
Round  39, Train loss: 1.542, Test loss: 2.232, Test accuracy: 20.07
Round  40, Train loss: 1.598, Test loss: 2.235, Test accuracy: 19.03
Round  41, Train loss: 1.602, Test loss: 2.218, Test accuracy: 21.83
Round  42, Train loss: 1.547, Test loss: 2.233, Test accuracy: 20.25
Round  43, Train loss: 1.557, Test loss: 2.227, Test accuracy: 21.95
Round  44, Train loss: 1.495, Test loss: 2.223, Test accuracy: 21.68
Round  45, Train loss: 1.594, Test loss: 2.215, Test accuracy: 23.77
Round  46, Train loss: 1.596, Test loss: 2.202, Test accuracy: 24.28
Round  47, Train loss: 1.554, Test loss: 2.203, Test accuracy: 24.38
Round  48, Train loss: 1.549, Test loss: 2.235, Test accuracy: 20.95
Round  49, Train loss: 1.591, Test loss: 2.198, Test accuracy: 25.55
Round  50, Train loss: 1.538, Test loss: 2.228, Test accuracy: 21.27
Round  51, Train loss: 1.643, Test loss: 2.204, Test accuracy: 24.55
Round  52, Train loss: 1.645, Test loss: 2.202, Test accuracy: 24.38
Round  53, Train loss: 1.485, Test loss: 2.237, Test accuracy: 20.15
Round  54, Train loss: 1.590, Test loss: 2.211, Test accuracy: 23.37
Round  55, Train loss: 1.547, Test loss: 2.214, Test accuracy: 22.58
Round  56, Train loss: 1.536, Test loss: 2.221, Test accuracy: 22.97
Round  57, Train loss: 1.544, Test loss: 2.215, Test accuracy: 23.88
Round  58, Train loss: 1.544, Test loss: 2.212, Test accuracy: 23.32
Round  59, Train loss: 1.596, Test loss: 2.205, Test accuracy: 24.12
Round  60, Train loss: 1.642, Test loss: 2.196, Test accuracy: 25.52
Round  61, Train loss: 1.590, Test loss: 2.206, Test accuracy: 23.85
Round  62, Train loss: 1.540, Test loss: 2.200, Test accuracy: 24.90
Round  63, Train loss: 1.588, Test loss: 2.215, Test accuracy: 23.05
Round  64, Train loss: 1.544, Test loss: 2.220, Test accuracy: 22.25
Round  65, Train loss: 1.641, Test loss: 2.202, Test accuracy: 24.42
Round  66, Train loss: 1.540, Test loss: 2.210, Test accuracy: 23.92
Round  67, Train loss: 1.538, Test loss: 2.202, Test accuracy: 24.78
Round  68, Train loss: 1.487, Test loss: 2.225, Test accuracy: 21.12
Round  69, Train loss: 1.541, Test loss: 2.213, Test accuracy: 22.62
Round  70, Train loss: 1.540, Test loss: 2.225, Test accuracy: 21.67
Round  71, Train loss: 1.639, Test loss: 2.206, Test accuracy: 24.62
Round  72, Train loss: 1.535, Test loss: 2.206, Test accuracy: 23.83
Round  73, Train loss: 1.639, Test loss: 2.207, Test accuracy: 23.77
Round  74, Train loss: 1.488, Test loss: 2.212, Test accuracy: 22.93
Round  75, Train loss: 1.540, Test loss: 2.204, Test accuracy: 23.82
Round  76, Train loss: 1.640, Test loss: 2.204, Test accuracy: 24.33
Round  77, Train loss: 1.583, Test loss: 2.211, Test accuracy: 24.37
Round  78, Train loss: 1.535, Test loss: 2.199, Test accuracy: 25.47
Round  79, Train loss: 1.641, Test loss: 2.206, Test accuracy: 24.37
Round  80, Train loss: 1.641, Test loss: 2.212, Test accuracy: 23.77
Round  81, Train loss: 1.582, Test loss: 2.194, Test accuracy: 25.33
Round  82, Train loss: 1.534, Test loss: 2.205, Test accuracy: 24.63
Round  83, Train loss: 1.581, Test loss: 2.217, Test accuracy: 22.67
Round  84, Train loss: 1.486, Test loss: 2.206, Test accuracy: 23.50
Round  85, Train loss: 1.589, Test loss: 2.212, Test accuracy: 23.18
Round  86, Train loss: 1.594, Test loss: 2.188, Test accuracy: 26.12
Round  87, Train loss: 1.483, Test loss: 2.206, Test accuracy: 23.43
Round  88, Train loss: 1.534, Test loss: 2.197, Test accuracy: 24.90
Round  89, Train loss: 1.584, Test loss: 2.197, Test accuracy: 24.82
Round  90, Train loss: 1.532, Test loss: 2.192, Test accuracy: 25.65
Round  91, Train loss: 1.483, Test loss: 2.221, Test accuracy: 22.35
Round  92, Train loss: 1.585, Test loss: 2.225, Test accuracy: 21.20
Round  93, Train loss: 1.532, Test loss: 2.201, Test accuracy: 24.33
Round  94, Train loss: 1.583, Test loss: 2.194, Test accuracy: 25.07
Round  95, Train loss: 1.584, Test loss: 2.202, Test accuracy: 24.47
Round  96, Train loss: 1.473, Test loss: 2.221, Test accuracy: 21.72
Round  97, Train loss: 1.482, Test loss: 2.205, Test accuracy: 24.65
Round  98, Train loss: 1.529, Test loss: 2.209, Test accuracy: 23.97
Round  99, Train loss: 1.586, Test loss: 2.188, Test accuracy: 25.85
Final Round, Train loss: 1.557, Test loss: 2.187, Test accuracy: 26.17
Average accuracy final 10 rounds: 23.924999999999997
995.5492520332336
[1.480236291885376, 2.839704751968384, 4.200567960739136, 5.578165292739868, 6.924426317214966, 8.267948150634766, 9.669999361038208, 11.024171590805054, 12.389973640441895, 13.767651319503784, 15.130134582519531, 16.513400554656982, 17.861023902893066, 19.222821712493896, 20.582508087158203, 21.94164752960205, 23.324182271957397, 24.662792921066284, 26.040797472000122, 27.378283262252808, 28.7321035861969, 30.086666584014893, 31.41971516609192, 32.77613568305969, 34.11971712112427, 35.49449682235718, 36.84228181838989, 38.22099733352661, 39.6111958026886, 40.94750380516052, 42.32227945327759, 43.64250636100769, 45.03750467300415, 46.39934492111206, 47.76243042945862, 49.121938943862915, 50.48063588142395, 51.8651602268219, 53.21261715888977, 54.56571555137634, 55.93961477279663, 57.273093938827515, 58.64269709587097, 59.976954221725464, 61.36148738861084, 62.74041700363159, 64.11750817298889, 65.51038718223572, 66.8475239276886, 68.2005844116211, 69.55053091049194, 70.91441679000854, 72.28241777420044, 73.63320994377136, 74.97990655899048, 76.33052635192871, 77.70880222320557, 79.05198907852173, 80.41917943954468, 81.78577423095703, 83.1188337802887, 84.49767661094666, 85.80757665634155, 87.177001953125, 88.53585195541382, 89.90414071083069, 91.26158285140991, 92.63972735404968, 94.00469207763672, 95.32991409301758, 96.68217992782593, 98.01250553131104, 99.35311913490295, 100.69416904449463, 102.01673221588135, 103.34522223472595, 104.68028211593628, 106.02334332466125, 107.35636615753174, 108.70074510574341, 110.04826307296753, 111.3926739692688, 112.6959617137909, 114.01677417755127, 115.35075306892395, 116.67071318626404, 117.989022731781, 119.32513785362244, 120.6439859867096, 121.97031593322754, 123.3060553073883, 124.63880324363708, 125.95831871032715, 127.27533745765686, 128.60116052627563, 129.91055393218994, 131.25221014022827, 132.56674456596375, 133.8974199295044, 135.21417474746704, 136.5585560798645]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[10.25, 19.783333333333335, 20.116666666666667, 14.933333333333334, 12.533333333333333, 21.2, 18.716666666666665, 14.3, 21.566666666666666, 21.35, 22.166666666666668, 20.766666666666666, 22.55, 21.05, 21.516666666666666, 20.45, 22.133333333333333, 21.666666666666668, 22.6, 21.683333333333334, 20.866666666666667, 23.066666666666666, 22.366666666666667, 23.6, 22.45, 22.216666666666665, 21.416666666666668, 21.866666666666667, 22.55, 20.616666666666667, 24.3, 24.716666666666665, 20.266666666666666, 23.9, 21.55, 21.233333333333334, 22.933333333333334, 21.866666666666667, 20.933333333333334, 20.066666666666666, 19.033333333333335, 21.833333333333332, 20.25, 21.95, 21.683333333333334, 23.766666666666666, 24.283333333333335, 24.383333333333333, 20.95, 25.55, 21.266666666666666, 24.55, 24.383333333333333, 20.15, 23.366666666666667, 22.583333333333332, 22.966666666666665, 23.883333333333333, 23.316666666666666, 24.116666666666667, 25.516666666666666, 23.85, 24.9, 23.05, 22.25, 24.416666666666668, 23.916666666666668, 24.783333333333335, 21.116666666666667, 22.616666666666667, 21.666666666666668, 24.616666666666667, 23.833333333333332, 23.766666666666666, 22.933333333333334, 23.816666666666666, 24.333333333333332, 24.366666666666667, 25.466666666666665, 24.366666666666667, 23.766666666666666, 25.333333333333332, 24.633333333333333, 22.666666666666668, 23.5, 23.183333333333334, 26.116666666666667, 23.433333333333334, 24.9, 24.816666666666666, 25.65, 22.35, 21.2, 24.333333333333332, 25.066666666666666, 24.466666666666665, 21.716666666666665, 24.65, 23.966666666666665, 25.85, 26.166666666666668]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.324, Test loss: 2.302, Test accuracy: 12.98
Round   1, Train loss: 2.296, Test loss: 2.300, Test accuracy: 20.13
Round   2, Train loss: 2.290, Test loss: 2.296, Test accuracy: 21.38
Round   3, Train loss: 2.277, Test loss: 2.286, Test accuracy: 21.03
Round   4, Train loss: 2.212, Test loss: 2.252, Test accuracy: 18.47
Round   5, Train loss: 2.129, Test loss: 2.218, Test accuracy: 20.53
Round   6, Train loss: 2.124, Test loss: 2.173, Test accuracy: 28.60
Round   7, Train loss: 2.042, Test loss: 2.120, Test accuracy: 33.50
Round   8, Train loss: 1.910, Test loss: 2.082, Test accuracy: 37.75
Round   9, Train loss: 2.009, Test loss: 2.064, Test accuracy: 41.57
Round  10, Train loss: 1.988, Test loss: 2.035, Test accuracy: 43.80
Round  11, Train loss: 1.883, Test loss: 1.995, Test accuracy: 47.93
Round  12, Train loss: 1.914, Test loss: 1.970, Test accuracy: 52.88
Round  13, Train loss: 1.844, Test loss: 1.939, Test accuracy: 56.82
Round  14, Train loss: 1.825, Test loss: 1.929, Test accuracy: 58.05
Round  15, Train loss: 1.810, Test loss: 1.894, Test accuracy: 62.30
Round  16, Train loss: 1.773, Test loss: 1.866, Test accuracy: 65.98
Round  17, Train loss: 1.684, Test loss: 1.841, Test accuracy: 68.25
Round  18, Train loss: 1.761, Test loss: 1.822, Test accuracy: 69.53
Round  19, Train loss: 1.777, Test loss: 1.806, Test accuracy: 71.42
Round  20, Train loss: 1.735, Test loss: 1.801, Test accuracy: 70.83
Round  21, Train loss: 1.658, Test loss: 1.772, Test accuracy: 74.15
Round  22, Train loss: 1.794, Test loss: 1.764, Test accuracy: 75.68
Round  23, Train loss: 1.677, Test loss: 1.759, Test accuracy: 75.98
Round  24, Train loss: 1.656, Test loss: 1.748, Test accuracy: 76.43
Round  25, Train loss: 1.695, Test loss: 1.727, Test accuracy: 78.88
Round  26, Train loss: 1.591, Test loss: 1.709, Test accuracy: 79.93
Round  27, Train loss: 1.727, Test loss: 1.696, Test accuracy: 81.03
Round  28, Train loss: 1.655, Test loss: 1.692, Test accuracy: 81.13
Round  29, Train loss: 1.650, Test loss: 1.694, Test accuracy: 80.33
Round  30, Train loss: 1.771, Test loss: 1.690, Test accuracy: 81.52
Round  31, Train loss: 1.801, Test loss: 1.689, Test accuracy: 81.47
Round  32, Train loss: 1.708, Test loss: 1.688, Test accuracy: 81.65
Round  33, Train loss: 1.698, Test loss: 1.683, Test accuracy: 81.87
Round  34, Train loss: 1.699, Test loss: 1.675, Test accuracy: 82.92
Round  35, Train loss: 1.677, Test loss: 1.678, Test accuracy: 82.23
Round  36, Train loss: 1.651, Test loss: 1.672, Test accuracy: 82.58
Round  37, Train loss: 1.656, Test loss: 1.668, Test accuracy: 83.22
Round  38, Train loss: 1.646, Test loss: 1.659, Test accuracy: 83.85
Round  39, Train loss: 1.692, Test loss: 1.658, Test accuracy: 84.12
Round  40, Train loss: 1.626, Test loss: 1.657, Test accuracy: 84.23
Round  41, Train loss: 1.566, Test loss: 1.657, Test accuracy: 84.00
Round  42, Train loss: 1.687, Test loss: 1.658, Test accuracy: 83.92
Round  43, Train loss: 1.566, Test loss: 1.654, Test accuracy: 84.15
Round  44, Train loss: 1.565, Test loss: 1.663, Test accuracy: 83.43
Round  45, Train loss: 1.518, Test loss: 1.654, Test accuracy: 84.00
Round  46, Train loss: 1.670, Test loss: 1.658, Test accuracy: 83.87
Round  47, Train loss: 1.680, Test loss: 1.656, Test accuracy: 83.77
Round  48, Train loss: 1.564, Test loss: 1.654, Test accuracy: 84.00
Round  49, Train loss: 1.718, Test loss: 1.662, Test accuracy: 83.03
Round  50, Train loss: 1.683, Test loss: 1.653, Test accuracy: 84.20
Round  51, Train loss: 1.581, Test loss: 1.648, Test accuracy: 84.25
Round  52, Train loss: 1.606, Test loss: 1.648, Test accuracy: 84.18
Round  53, Train loss: 1.564, Test loss: 1.643, Test accuracy: 84.55
Round  54, Train loss: 1.559, Test loss: 1.645, Test accuracy: 84.45
Round  55, Train loss: 1.562, Test loss: 1.649, Test accuracy: 83.92
Round  56, Train loss: 1.510, Test loss: 1.646, Test accuracy: 84.05
Round  57, Train loss: 1.616, Test loss: 1.645, Test accuracy: 84.15
Round  58, Train loss: 1.718, Test loss: 1.651, Test accuracy: 84.05
Round  59, Train loss: 1.660, Test loss: 1.654, Test accuracy: 83.88
Round  60, Train loss: 1.640, Test loss: 1.647, Test accuracy: 85.12
Round  61, Train loss: 1.590, Test loss: 1.642, Test accuracy: 85.13
Round  62, Train loss: 1.612, Test loss: 1.639, Test accuracy: 85.12
Round  63, Train loss: 1.521, Test loss: 1.634, Test accuracy: 85.70
Round  64, Train loss: 1.601, Test loss: 1.639, Test accuracy: 85.17
Round  65, Train loss: 1.632, Test loss: 1.639, Test accuracy: 85.07
Round  66, Train loss: 1.512, Test loss: 1.628, Test accuracy: 85.83
Round  67, Train loss: 1.685, Test loss: 1.623, Test accuracy: 87.77
Round  68, Train loss: 1.571, Test loss: 1.611, Test accuracy: 88.17
Round  69, Train loss: 1.524, Test loss: 1.603, Test accuracy: 88.62
Round  70, Train loss: 1.662, Test loss: 1.614, Test accuracy: 88.03
Round  71, Train loss: 1.513, Test loss: 1.605, Test accuracy: 88.98
Round  72, Train loss: 1.496, Test loss: 1.605, Test accuracy: 88.62
Round  73, Train loss: 1.668, Test loss: 1.606, Test accuracy: 88.72
Round  74, Train loss: 1.545, Test loss: 1.607, Test accuracy: 88.93
Round  75, Train loss: 1.509, Test loss: 1.596, Test accuracy: 90.40
Round  76, Train loss: 1.520, Test loss: 1.598, Test accuracy: 89.92
Round  77, Train loss: 1.524, Test loss: 1.587, Test accuracy: 90.62
Round  78, Train loss: 1.555, Test loss: 1.585, Test accuracy: 90.82
Round  79, Train loss: 1.556, Test loss: 1.584, Test accuracy: 91.15
Round  80, Train loss: 1.497, Test loss: 1.588, Test accuracy: 90.53
Round  81, Train loss: 1.502, Test loss: 1.583, Test accuracy: 90.95
Round  82, Train loss: 1.604, Test loss: 1.585, Test accuracy: 91.02
Round  83, Train loss: 1.508, Test loss: 1.589, Test accuracy: 90.80
Round  84, Train loss: 1.488, Test loss: 1.583, Test accuracy: 91.03
Round  85, Train loss: 1.598, Test loss: 1.586, Test accuracy: 90.55
Round  86, Train loss: 1.492, Test loss: 1.585, Test accuracy: 90.88
Round  87, Train loss: 1.487, Test loss: 1.584, Test accuracy: 90.52
Round  88, Train loss: 1.671, Test loss: 1.591, Test accuracy: 90.27
Round  89, Train loss: 1.494, Test loss: 1.587, Test accuracy: 90.28
Round  90, Train loss: 1.545, Test loss: 1.588, Test accuracy: 90.28
Round  91, Train loss: 1.492, Test loss: 1.585, Test accuracy: 90.35
Round  92, Train loss: 1.486, Test loss: 1.591, Test accuracy: 89.92
Round  93, Train loss: 1.595, Test loss: 1.596, Test accuracy: 89.38
Round  94, Train loss: 1.654, Test loss: 1.597, Test accuracy: 89.47/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.498, Test loss: 1.589, Test accuracy: 89.93
Round  96, Train loss: 1.609, Test loss: 1.584, Test accuracy: 90.98
Round  97, Train loss: 1.608, Test loss: 1.581, Test accuracy: 91.17
Round  98, Train loss: 1.552, Test loss: 1.579, Test accuracy: 91.17
Round  99, Train loss: 1.488, Test loss: 1.577, Test accuracy: 91.20
Final Round, Train loss: 1.532, Test loss: 1.569, Test accuracy: 91.42
Average accuracy final 10 rounds: 90.38500000000002
551.7304215431213
[0.7240488529205322, 1.3707504272460938, 2.010780096054077, 2.643423080444336, 3.280517101287842, 3.883556604385376, 4.497660160064697, 5.095729112625122, 5.743197917938232, 6.348169326782227, 6.971457242965698, 7.577725172042847, 8.212133884429932, 8.853088855743408, 9.467390298843384, 10.097857236862183, 10.687860012054443, 11.310577392578125, 11.909394264221191, 12.561287641525269, 13.182456016540527, 13.816158533096313, 14.420725107192993, 15.017536878585815, 15.679874420166016, 16.290404558181763, 16.94163417816162, 17.55408263206482, 18.174717903137207, 18.808102130889893, 19.43261742591858, 20.088112354278564, 20.699275732040405, 21.31591510772705, 21.918387413024902, 22.544204473495483, 23.17643404006958, 23.826443672180176, 24.424402475357056, 25.03345036506653, 25.65472674369812, 26.282552480697632, 26.922346591949463, 27.53772521018982, 28.15799903869629, 28.7678644657135, 29.382557153701782, 30.01611828804016, 30.64857292175293, 31.269813299179077, 31.877774238586426, 32.49581837654114, 33.10922884941101, 33.74313950538635, 34.35737657546997, 34.98126816749573, 35.588889598846436, 36.20371961593628, 36.816323041915894, 37.44588851928711, 38.070375204086304, 38.695772886276245, 39.282766580581665, 39.898032426834106, 40.52425003051758, 41.15123915672302, 41.77204084396362, 42.37490630149841, 42.99502992630005, 43.62021565437317, 44.214906215667725, 44.832292795181274, 45.453590869903564, 46.056429386138916, 46.66747832298279, 47.287638902664185, 47.90261650085449, 48.532886266708374, 49.14075446128845, 49.7573618888855, 50.38262128829956, 50.997243881225586, 51.619158029556274, 52.25082516670227, 52.87947368621826, 53.50908613204956, 54.11365485191345, 54.74199438095093, 55.38064169883728, 56.00323033332825, 56.61995840072632, 57.24281096458435, 57.856361627578735, 58.47256302833557, 59.059218645095825, 59.6646249294281, 60.31443548202515, 60.96449160575867, 61.60354280471802, 62.20947527885437, 63.224159479141235]
[12.983333333333333, 20.133333333333333, 21.383333333333333, 21.033333333333335, 18.466666666666665, 20.533333333333335, 28.6, 33.5, 37.75, 41.56666666666667, 43.8, 47.93333333333333, 52.88333333333333, 56.81666666666667, 58.05, 62.3, 65.98333333333333, 68.25, 69.53333333333333, 71.41666666666667, 70.83333333333333, 74.15, 75.68333333333334, 75.98333333333333, 76.43333333333334, 78.88333333333334, 79.93333333333334, 81.03333333333333, 81.13333333333334, 80.33333333333333, 81.51666666666667, 81.46666666666667, 81.65, 81.86666666666666, 82.91666666666667, 82.23333333333333, 82.58333333333333, 83.21666666666667, 83.85, 84.11666666666666, 84.23333333333333, 84.0, 83.91666666666667, 84.15, 83.43333333333334, 84.0, 83.86666666666666, 83.76666666666667, 84.0, 83.03333333333333, 84.2, 84.25, 84.18333333333334, 84.55, 84.45, 83.91666666666667, 84.05, 84.15, 84.05, 83.88333333333334, 85.11666666666666, 85.13333333333334, 85.11666666666666, 85.7, 85.16666666666667, 85.06666666666666, 85.83333333333333, 87.76666666666667, 88.16666666666667, 88.61666666666666, 88.03333333333333, 88.98333333333333, 88.61666666666666, 88.71666666666667, 88.93333333333334, 90.4, 89.91666666666667, 90.61666666666666, 90.81666666666666, 91.15, 90.53333333333333, 90.95, 91.01666666666667, 90.8, 91.03333333333333, 90.55, 90.88333333333334, 90.51666666666667, 90.26666666666667, 90.28333333333333, 90.28333333333333, 90.35, 89.91666666666667, 89.38333333333334, 89.46666666666667, 89.93333333333334, 90.98333333333333, 91.16666666666667, 91.16666666666667, 91.2, 91.41666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.313, Test loss: 2.298, Test accuracy: 14.50
Round   1, Train loss: 2.307, Test loss: 2.294, Test accuracy: 19.80
Round   2, Train loss: 2.281, Test loss: 2.282, Test accuracy: 16.33
Round   3, Train loss: 2.206, Test loss: 2.247, Test accuracy: 28.90
Round   4, Train loss: 2.140, Test loss: 2.200, Test accuracy: 29.77
Round   5, Train loss: 2.088, Test loss: 2.159, Test accuracy: 30.50
Round   6, Train loss: 2.025, Test loss: 2.135, Test accuracy: 33.45
Round   7, Train loss: 1.930, Test loss: 2.099, Test accuracy: 41.60
Round   8, Train loss: 1.846, Test loss: 2.039, Test accuracy: 46.23
Round   9, Train loss: 1.828, Test loss: 1.999, Test accuracy: 51.42
Round  10, Train loss: 1.861, Test loss: 1.948, Test accuracy: 55.78
Round  11, Train loss: 1.814, Test loss: 1.909, Test accuracy: 60.10
Round  12, Train loss: 1.797, Test loss: 1.879, Test accuracy: 64.00
Round  13, Train loss: 1.779, Test loss: 1.847, Test accuracy: 65.53
Round  14, Train loss: 1.730, Test loss: 1.826, Test accuracy: 67.85
Round  15, Train loss: 1.710, Test loss: 1.810, Test accuracy: 68.97
Round  16, Train loss: 1.808, Test loss: 1.817, Test accuracy: 69.42
Round  17, Train loss: 1.717, Test loss: 1.778, Test accuracy: 73.55
Round  18, Train loss: 1.732, Test loss: 1.762, Test accuracy: 75.55
Round  19, Train loss: 1.667, Test loss: 1.761, Test accuracy: 74.75
Round  20, Train loss: 1.680, Test loss: 1.723, Test accuracy: 78.37
Round  21, Train loss: 1.649, Test loss: 1.708, Test accuracy: 80.02
Round  22, Train loss: 1.650, Test loss: 1.711, Test accuracy: 80.15
Round  23, Train loss: 1.605, Test loss: 1.698, Test accuracy: 80.60
Round  24, Train loss: 1.665, Test loss: 1.689, Test accuracy: 81.57
Round  25, Train loss: 1.735, Test loss: 1.669, Test accuracy: 84.78
Round  26, Train loss: 1.637, Test loss: 1.650, Test accuracy: 86.45
Round  27, Train loss: 1.549, Test loss: 1.647, Test accuracy: 86.90
Round  28, Train loss: 1.608, Test loss: 1.642, Test accuracy: 87.02
Round  29, Train loss: 1.572, Test loss: 1.633, Test accuracy: 87.72
Round  30, Train loss: 1.620, Test loss: 1.626, Test accuracy: 88.95
Round  31, Train loss: 1.561, Test loss: 1.617, Test accuracy: 89.65
Round  32, Train loss: 1.586, Test loss: 1.606, Test accuracy: 89.65
Round  33, Train loss: 1.608, Test loss: 1.602, Test accuracy: 90.35
Round  34, Train loss: 1.587, Test loss: 1.599, Test accuracy: 90.63
Round  35, Train loss: 1.559, Test loss: 1.593, Test accuracy: 91.53
Round  36, Train loss: 1.601, Test loss: 1.594, Test accuracy: 91.18
Round  37, Train loss: 1.536, Test loss: 1.590, Test accuracy: 91.57
Round  38, Train loss: 1.535, Test loss: 1.586, Test accuracy: 91.82
Round  39, Train loss: 1.523, Test loss: 1.588, Test accuracy: 91.62
Round  40, Train loss: 1.524, Test loss: 1.586, Test accuracy: 91.68
Round  41, Train loss: 1.573, Test loss: 1.586, Test accuracy: 91.83
Round  42, Train loss: 1.537, Test loss: 1.574, Test accuracy: 93.28
Round  43, Train loss: 1.544, Test loss: 1.569, Test accuracy: 93.28
Round  44, Train loss: 1.543, Test loss: 1.557, Test accuracy: 94.30
Round  45, Train loss: 1.516, Test loss: 1.560, Test accuracy: 94.32
Round  46, Train loss: 1.577, Test loss: 1.559, Test accuracy: 94.45
Round  47, Train loss: 1.579, Test loss: 1.556, Test accuracy: 94.25
Round  48, Train loss: 1.515, Test loss: 1.554, Test accuracy: 94.45
Round  49, Train loss: 1.542, Test loss: 1.547, Test accuracy: 95.23
Round  50, Train loss: 1.529, Test loss: 1.545, Test accuracy: 95.35
Round  51, Train loss: 1.536, Test loss: 1.539, Test accuracy: 96.02
Round  52, Train loss: 1.515, Test loss: 1.539, Test accuracy: 96.10
Round  53, Train loss: 1.522, Test loss: 1.535, Test accuracy: 96.12
Round  54, Train loss: 1.510, Test loss: 1.537, Test accuracy: 96.07
Round  55, Train loss: 1.512, Test loss: 1.537, Test accuracy: 96.03
Round  56, Train loss: 1.515, Test loss: 1.534, Test accuracy: 96.15
Round  57, Train loss: 1.512, Test loss: 1.533, Test accuracy: 96.28
Round  58, Train loss: 1.505, Test loss: 1.534, Test accuracy: 96.42
Round  59, Train loss: 1.526, Test loss: 1.532, Test accuracy: 96.70
Round  60, Train loss: 1.529, Test loss: 1.530, Test accuracy: 96.52
Round  61, Train loss: 1.520, Test loss: 1.530, Test accuracy: 96.52
Round  62, Train loss: 1.510, Test loss: 1.531, Test accuracy: 96.35
Round  63, Train loss: 1.509, Test loss: 1.530, Test accuracy: 96.32
Round  64, Train loss: 1.503, Test loss: 1.529, Test accuracy: 96.35
Round  65, Train loss: 1.511, Test loss: 1.528, Test accuracy: 96.28
Round  66, Train loss: 1.505, Test loss: 1.526, Test accuracy: 96.35
Round  67, Train loss: 1.503, Test loss: 1.526, Test accuracy: 96.55
Round  68, Train loss: 1.503, Test loss: 1.527, Test accuracy: 96.10
Round  69, Train loss: 1.496, Test loss: 1.528, Test accuracy: 96.43
Round  70, Train loss: 1.508, Test loss: 1.526, Test accuracy: 96.63
Round  71, Train loss: 1.502, Test loss: 1.526, Test accuracy: 96.35
Round  72, Train loss: 1.499, Test loss: 1.527, Test accuracy: 96.42
Round  73, Train loss: 1.506, Test loss: 1.525, Test accuracy: 96.62
Round  74, Train loss: 1.497, Test loss: 1.526, Test accuracy: 96.43
Round  75, Train loss: 1.496, Test loss: 1.525, Test accuracy: 96.55
Round  76, Train loss: 1.510, Test loss: 1.524, Test accuracy: 96.75
Round  77, Train loss: 1.499, Test loss: 1.527, Test accuracy: 96.15
Round  78, Train loss: 1.495, Test loss: 1.525, Test accuracy: 96.17
Round  79, Train loss: 1.497, Test loss: 1.524, Test accuracy: 96.92
Round  80, Train loss: 1.500, Test loss: 1.523, Test accuracy: 96.95
Round  81, Train loss: 1.503, Test loss: 1.523, Test accuracy: 96.80
Round  82, Train loss: 1.503, Test loss: 1.522, Test accuracy: 96.85
Round  83, Train loss: 1.494, Test loss: 1.524, Test accuracy: 96.17
Round  84, Train loss: 1.497, Test loss: 1.523, Test accuracy: 96.43
Round  85, Train loss: 1.500, Test loss: 1.520, Test accuracy: 96.68
Round  86, Train loss: 1.494, Test loss: 1.521, Test accuracy: 96.75
Round  87, Train loss: 1.496, Test loss: 1.519, Test accuracy: 96.83
Round  88, Train loss: 1.493, Test loss: 1.521, Test accuracy: 96.70
Round  89, Train loss: 1.497, Test loss: 1.523, Test accuracy: 96.38
Round  90, Train loss: 1.497, Test loss: 1.520, Test accuracy: 96.92
Round  91, Train loss: 1.493, Test loss: 1.519, Test accuracy: 96.88
Round  92, Train loss: 1.499, Test loss: 1.517, Test accuracy: 97.02
Round  93, Train loss: 1.492, Test loss: 1.520, Test accuracy: 96.63/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.491, Test loss: 1.519, Test accuracy: 96.47
Round  95, Train loss: 1.492, Test loss: 1.520, Test accuracy: 96.62
Round  96, Train loss: 1.492, Test loss: 1.519, Test accuracy: 96.88
Round  97, Train loss: 1.492, Test loss: 1.519, Test accuracy: 96.92
Round  98, Train loss: 1.492, Test loss: 1.520, Test accuracy: 96.90
Round  99, Train loss: 1.494, Test loss: 1.520, Test accuracy: 96.63
Final Round, Train loss: 1.479, Test loss: 1.515, Test accuracy: 96.83
Average accuracy final 10 rounds: 96.78666666666666
743.4210867881775
[0.8045370578765869, 1.6090741157531738, 2.2505545616149902, 2.8920350074768066, 3.556985378265381, 4.221935749053955, 4.868997573852539, 5.516059398651123, 6.127800703048706, 6.739542007446289, 7.3853232860565186, 8.031104564666748, 8.703067541122437, 9.375030517578125, 10.019695281982422, 10.664360046386719, 11.29136848449707, 11.918376922607422, 12.565677642822266, 13.21297836303711, 13.873159170150757, 14.533339977264404, 15.182228326797485, 15.831116676330566, 16.42543935775757, 17.01976203918457, 17.649869680404663, 18.279977321624756, 18.947929620742798, 19.61588191986084, 20.26735210418701, 20.918822288513184, 21.495456218719482, 22.07209014892578, 22.706533908843994, 23.340977668762207, 23.975481510162354, 24.6099853515625, 25.270695686340332, 25.931406021118164, 26.5698082447052, 27.208210468292236, 27.84415030479431, 28.480090141296387, 29.1340229511261, 29.78795576095581, 30.441282749176025, 31.09460973739624, 31.741618871688843, 32.388628005981445, 33.03295016288757, 33.6772723197937, 34.319971799850464, 34.96267127990723, 35.625561475753784, 36.28845167160034, 36.935145139694214, 37.581838607788086, 38.21565008163452, 38.84946155548096, 39.50303053855896, 40.15659952163696, 40.793766260147095, 41.43093299865723, 42.08624529838562, 42.741557598114014, 43.374526500701904, 44.007495403289795, 44.65212798118591, 45.29676055908203, 45.95006513595581, 46.60336971282959, 47.251389265060425, 47.89940881729126, 48.550166606903076, 49.20092439651489, 49.84319734573364, 50.48547029495239, 51.12035131454468, 51.75523233413696, 52.409454345703125, 53.06367635726929, 53.71140670776367, 54.35913705825806, 54.99713063240051, 55.63512420654297, 56.28133940696716, 56.92755460739136, 57.56339502334595, 58.19923543930054, 58.83920121192932, 59.479166984558105, 60.11333227157593, 60.74749755859375, 61.386131048202515, 62.02476453781128, 62.65208411216736, 63.27940368652344, 63.90592002868652, 64.53243637084961, 65.18464922904968, 65.83686208724976, 66.48775839805603, 67.1386547088623, 67.78057885169983, 68.42250299453735, 69.05199027061462, 69.6814775466919, 70.33015632629395, 70.978835105896, 71.63021874427795, 72.28160238265991, 72.92746019363403, 73.57331800460815, 74.19846391677856, 74.82360982894897, 75.48177242279053, 76.13993501663208, 76.79143476486206, 77.44293451309204, 78.07986497879028, 78.71679544448853, 79.34335803985596, 79.96992063522339, 80.62467527389526, 81.27942991256714, 81.95051908493042, 82.6216082572937, 83.25319504737854, 83.88478183746338, 84.49940252304077, 85.11402320861816, 85.76453065872192, 86.41503810882568, 87.0685920715332, 87.72214603424072, 88.36804270744324, 89.01393938064575, 89.63851833343506, 90.26309728622437, 90.91382455825806, 91.56455183029175, 92.23199462890625, 92.89943742752075, 93.5186870098114, 94.13793659210205, 94.75750660896301, 95.37707662582397, 96.01087737083435, 96.64467811584473, 97.30600500106812, 97.9673318862915, 98.59198236465454, 99.21663284301758, 99.82211399078369, 100.4275951385498, 101.0641279220581, 101.7006607055664, 102.3722414970398, 103.04382228851318, 103.6758942604065, 104.3079662322998, 104.9092047214508, 105.5104432106018, 106.14855670928955, 106.7866702079773, 107.44119596481323, 108.09572172164917, 108.74360990524292, 109.39149808883667, 110.01581859588623, 110.64013910293579, 111.27019882202148, 111.90025854110718, 112.54467129707336, 113.18908405303955, 113.83089280128479, 114.47270154953003, 115.11259770393372, 115.7524938583374, 116.39448285102844, 117.03647184371948, 117.686527967453, 118.33658409118652, 118.97866654396057, 119.62074899673462, 120.2627341747284, 120.90471935272217, 121.55760669708252, 122.21049404144287, 122.83296775817871, 123.45544147491455, 124.0909059047699, 124.72637033462524, 125.35401487350464, 125.98165941238403, 126.63245368003845, 127.28324794769287, 127.9330244064331, 128.58280086517334, 129.5874092578888, 130.59201765060425]
[14.5, 14.5, 19.8, 19.8, 16.333333333333332, 16.333333333333332, 28.9, 28.9, 29.766666666666666, 29.766666666666666, 30.5, 30.5, 33.45, 33.45, 41.6, 41.6, 46.233333333333334, 46.233333333333334, 51.416666666666664, 51.416666666666664, 55.78333333333333, 55.78333333333333, 60.1, 60.1, 64.0, 64.0, 65.53333333333333, 65.53333333333333, 67.85, 67.85, 68.96666666666667, 68.96666666666667, 69.41666666666667, 69.41666666666667, 73.55, 73.55, 75.55, 75.55, 74.75, 74.75, 78.36666666666666, 78.36666666666666, 80.01666666666667, 80.01666666666667, 80.15, 80.15, 80.6, 80.6, 81.56666666666666, 81.56666666666666, 84.78333333333333, 84.78333333333333, 86.45, 86.45, 86.9, 86.9, 87.01666666666667, 87.01666666666667, 87.71666666666667, 87.71666666666667, 88.95, 88.95, 89.65, 89.65, 89.65, 89.65, 90.35, 90.35, 90.63333333333334, 90.63333333333334, 91.53333333333333, 91.53333333333333, 91.18333333333334, 91.18333333333334, 91.56666666666666, 91.56666666666666, 91.81666666666666, 91.81666666666666, 91.61666666666666, 91.61666666666666, 91.68333333333334, 91.68333333333334, 91.83333333333333, 91.83333333333333, 93.28333333333333, 93.28333333333333, 93.28333333333333, 93.28333333333333, 94.3, 94.3, 94.31666666666666, 94.31666666666666, 94.45, 94.45, 94.25, 94.25, 94.45, 94.45, 95.23333333333333, 95.23333333333333, 95.35, 95.35, 96.01666666666667, 96.01666666666667, 96.1, 96.1, 96.11666666666666, 96.11666666666666, 96.06666666666666, 96.06666666666666, 96.03333333333333, 96.03333333333333, 96.15, 96.15, 96.28333333333333, 96.28333333333333, 96.41666666666667, 96.41666666666667, 96.7, 96.7, 96.51666666666667, 96.51666666666667, 96.51666666666667, 96.51666666666667, 96.35, 96.35, 96.31666666666666, 96.31666666666666, 96.35, 96.35, 96.28333333333333, 96.28333333333333, 96.35, 96.35, 96.55, 96.55, 96.1, 96.1, 96.43333333333334, 96.43333333333334, 96.63333333333334, 96.63333333333334, 96.35, 96.35, 96.41666666666667, 96.41666666666667, 96.61666666666666, 96.61666666666666, 96.43333333333334, 96.43333333333334, 96.55, 96.55, 96.75, 96.75, 96.15, 96.15, 96.16666666666667, 96.16666666666667, 96.91666666666667, 96.91666666666667, 96.95, 96.95, 96.8, 96.8, 96.85, 96.85, 96.16666666666667, 96.16666666666667, 96.43333333333334, 96.43333333333334, 96.68333333333334, 96.68333333333334, 96.75, 96.75, 96.83333333333333, 96.83333333333333, 96.7, 96.7, 96.38333333333334, 96.38333333333334, 96.91666666666667, 96.91666666666667, 96.88333333333334, 96.88333333333334, 97.01666666666667, 97.01666666666667, 96.63333333333334, 96.63333333333334, 96.46666666666667, 96.46666666666667, 96.61666666666666, 96.61666666666666, 96.88333333333334, 96.88333333333334, 96.91666666666667, 96.91666666666667, 96.9, 96.9, 96.63333333333334, 96.63333333333334, 96.83333333333333, 96.83333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.290, Test loss: 2.291, Test accuracy: 20.72
Round   0, Global train loss: 2.290, Global test loss: 2.302, Global test accuracy: 14.77
Round   1, Train loss: 2.248, Test loss: 2.238, Test accuracy: 28.62
Round   1, Global train loss: 2.248, Global test loss: 2.294, Global test accuracy: 13.33
Round   2, Train loss: 2.018, Test loss: 2.141, Test accuracy: 41.78
Round   2, Global train loss: 2.018, Global test loss: 2.273, Global test accuracy: 16.30
Round   3, Train loss: 1.870, Test loss: 2.018, Test accuracy: 54.20
Round   3, Global train loss: 1.870, Global test loss: 2.277, Global test accuracy: 16.58
Round   4, Train loss: 1.757, Test loss: 1.942, Test accuracy: 61.23
Round   4, Global train loss: 1.757, Global test loss: 2.277, Global test accuracy: 16.67
Round   5, Train loss: 1.879, Test loss: 1.854, Test accuracy: 65.52
Round   5, Global train loss: 1.879, Global test loss: 2.267, Global test accuracy: 15.58
Round   6, Train loss: 1.556, Test loss: 1.807, Test accuracy: 68.13
Round   6, Global train loss: 1.556, Global test loss: 2.269, Global test accuracy: 17.28
Round   7, Train loss: 1.863, Test loss: 1.757, Test accuracy: 74.25
Round   7, Global train loss: 1.863, Global test loss: 2.275, Global test accuracy: 18.38
Round   8, Train loss: 1.720, Test loss: 1.706, Test accuracy: 79.60
Round   8, Global train loss: 1.720, Global test loss: 2.281, Global test accuracy: 16.18
Round   9, Train loss: 1.598, Test loss: 1.699, Test accuracy: 79.83
Round   9, Global train loss: 1.598, Global test loss: 2.262, Global test accuracy: 19.38
Round  10, Train loss: 1.685, Test loss: 1.671, Test accuracy: 81.93
Round  10, Global train loss: 1.685, Global test loss: 2.273, Global test accuracy: 17.58
Round  11, Train loss: 1.565, Test loss: 1.657, Test accuracy: 83.38
Round  11, Global train loss: 1.565, Global test loss: 2.276, Global test accuracy: 18.58
Round  12, Train loss: 1.516, Test loss: 1.631, Test accuracy: 85.75
Round  12, Global train loss: 1.516, Global test loss: 2.293, Global test accuracy: 11.95
Round  13, Train loss: 1.563, Test loss: 1.589, Test accuracy: 89.35
Round  13, Global train loss: 1.563, Global test loss: 2.311, Global test accuracy: 13.33
Round  14, Train loss: 1.551, Test loss: 1.580, Test accuracy: 89.58
Round  14, Global train loss: 1.551, Global test loss: 2.286, Global test accuracy: 15.22
Round  15, Train loss: 1.550, Test loss: 1.567, Test accuracy: 90.75
Round  15, Global train loss: 1.550, Global test loss: 2.258, Global test accuracy: 17.57
Round  16, Train loss: 1.532, Test loss: 1.565, Test accuracy: 90.77
Round  16, Global train loss: 1.532, Global test loss: 2.283, Global test accuracy: 15.00
Round  17, Train loss: 1.530, Test loss: 1.563, Test accuracy: 90.83
Round  17, Global train loss: 1.530, Global test loss: 2.272, Global test accuracy: 16.58
Round  18, Train loss: 1.477, Test loss: 1.562, Test accuracy: 90.82
Round  18, Global train loss: 1.477, Global test loss: 2.319, Global test accuracy: 10.00
Round  19, Train loss: 1.524, Test loss: 1.561, Test accuracy: 90.83
Round  19, Global train loss: 1.524, Global test loss: 2.287, Global test accuracy: 11.35
Round  20, Train loss: 1.469, Test loss: 1.561, Test accuracy: 90.85
Round  20, Global train loss: 1.469, Global test loss: 2.288, Global test accuracy: 12.95
Round  21, Train loss: 1.469, Test loss: 1.560, Test accuracy: 90.85
Round  21, Global train loss: 1.469, Global test loss: 2.284, Global test accuracy: 13.18
Round  22, Train loss: 1.583, Test loss: 1.559, Test accuracy: 90.90
Round  22, Global train loss: 1.583, Global test loss: 2.279, Global test accuracy: 16.65
Round  23, Train loss: 1.525, Test loss: 1.559, Test accuracy: 90.90
Round  23, Global train loss: 1.525, Global test loss: 2.291, Global test accuracy: 14.47
Round  24, Train loss: 1.520, Test loss: 1.558, Test accuracy: 90.93
Round  24, Global train loss: 1.520, Global test loss: 2.273, Global test accuracy: 16.65
Round  25, Train loss: 1.468, Test loss: 1.558, Test accuracy: 90.92
Round  25, Global train loss: 1.468, Global test loss: 2.262, Global test accuracy: 17.67
Round  26, Train loss: 1.578, Test loss: 1.558, Test accuracy: 90.93
Round  26, Global train loss: 1.578, Global test loss: 2.279, Global test accuracy: 15.12
Round  27, Train loss: 1.520, Test loss: 1.558, Test accuracy: 90.90
Round  27, Global train loss: 1.520, Global test loss: 2.268, Global test accuracy: 15.50
Round  28, Train loss: 1.467, Test loss: 1.557, Test accuracy: 90.93
Round  28, Global train loss: 1.467, Global test loss: 2.285, Global test accuracy: 13.87
Round  29, Train loss: 1.467, Test loss: 1.557, Test accuracy: 90.95
Round  29, Global train loss: 1.467, Global test loss: 2.278, Global test accuracy: 16.80
Round  30, Train loss: 1.629, Test loss: 1.556, Test accuracy: 90.95
Round  30, Global train loss: 1.629, Global test loss: 2.258, Global test accuracy: 17.52
Round  31, Train loss: 1.521, Test loss: 1.556, Test accuracy: 90.93
Round  31, Global train loss: 1.521, Global test loss: 2.275, Global test accuracy: 14.58
Round  32, Train loss: 1.465, Test loss: 1.556, Test accuracy: 90.97
Round  32, Global train loss: 1.465, Global test loss: 2.280, Global test accuracy: 16.20
Round  33, Train loss: 1.521, Test loss: 1.556, Test accuracy: 90.95
Round  33, Global train loss: 1.521, Global test loss: 2.274, Global test accuracy: 17.40
Round  34, Train loss: 1.520, Test loss: 1.556, Test accuracy: 90.97
Round  34, Global train loss: 1.520, Global test loss: 2.259, Global test accuracy: 17.23
Round  35, Train loss: 1.518, Test loss: 1.547, Test accuracy: 92.50
Round  35, Global train loss: 1.518, Global test loss: 2.273, Global test accuracy: 14.20
Round  36, Train loss: 1.518, Test loss: 1.546, Test accuracy: 92.53
Round  36, Global train loss: 1.518, Global test loss: 2.269, Global test accuracy: 15.57
Round  37, Train loss: 1.522, Test loss: 1.546, Test accuracy: 92.50
Round  37, Global train loss: 1.522, Global test loss: 2.284, Global test accuracy: 16.67
Round  38, Train loss: 1.574, Test loss: 1.546, Test accuracy: 92.50
Round  38, Global train loss: 1.574, Global test loss: 2.278, Global test accuracy: 16.68
Round  39, Train loss: 1.473, Test loss: 1.540, Test accuracy: 92.53
Round  39, Global train loss: 1.473, Global test loss: 2.287, Global test accuracy: 13.93
Round  40, Train loss: 1.464, Test loss: 1.540, Test accuracy: 92.53
Round  40, Global train loss: 1.464, Global test loss: 2.283, Global test accuracy: 15.67
Round  41, Train loss: 1.522, Test loss: 1.540, Test accuracy: 92.55
Round  41, Global train loss: 1.522, Global test loss: 2.281, Global test accuracy: 16.00
Round  42, Train loss: 1.465, Test loss: 1.540, Test accuracy: 92.55
Round  42, Global train loss: 1.465, Global test loss: 2.308, Global test accuracy: 11.65
Round  43, Train loss: 1.465, Test loss: 1.540, Test accuracy: 92.55
Round  43, Global train loss: 1.465, Global test loss: 2.333, Global test accuracy: 8.65
Round  44, Train loss: 1.574, Test loss: 1.540, Test accuracy: 92.57
Round  44, Global train loss: 1.574, Global test loss: 2.262, Global test accuracy: 15.85
Round  45, Train loss: 1.468, Test loss: 1.539, Test accuracy: 92.60
Round  45, Global train loss: 1.468, Global test loss: 2.289, Global test accuracy: 13.25
Round  46, Train loss: 1.466, Test loss: 1.539, Test accuracy: 92.62
Round  46, Global train loss: 1.466, Global test loss: 2.285, Global test accuracy: 13.82
Round  47, Train loss: 1.465, Test loss: 1.539, Test accuracy: 92.58
Round  47, Global train loss: 1.465, Global test loss: 2.293, Global test accuracy: 13.70
Round  48, Train loss: 1.519, Test loss: 1.539, Test accuracy: 92.58
Round  48, Global train loss: 1.519, Global test loss: 2.274, Global test accuracy: 13.95
Round  49, Train loss: 1.573, Test loss: 1.539, Test accuracy: 92.58
Round  49, Global train loss: 1.573, Global test loss: 2.254, Global test accuracy: 18.80
Round  50, Train loss: 1.467, Test loss: 1.539, Test accuracy: 92.62
Round  50, Global train loss: 1.467, Global test loss: 2.312, Global test accuracy: 12.82
Round  51, Train loss: 1.518, Test loss: 1.540, Test accuracy: 92.53
Round  51, Global train loss: 1.518, Global test loss: 2.269, Global test accuracy: 18.97
Round  52, Train loss: 1.464, Test loss: 1.540, Test accuracy: 92.55
Round  52, Global train loss: 1.464, Global test loss: 2.290, Global test accuracy: 13.37
Round  53, Train loss: 1.522, Test loss: 1.539, Test accuracy: 92.55
Round  53, Global train loss: 1.522, Global test loss: 2.268, Global test accuracy: 16.18
Round  54, Train loss: 1.464, Test loss: 1.539, Test accuracy: 92.57
Round  54, Global train loss: 1.464, Global test loss: 2.269, Global test accuracy: 16.88
Round  55, Train loss: 1.466, Test loss: 1.539, Test accuracy: 92.57
Round  55, Global train loss: 1.466, Global test loss: 2.269, Global test accuracy: 16.27
Round  56, Train loss: 1.573, Test loss: 1.539, Test accuracy: 92.57
Round  56, Global train loss: 1.573, Global test loss: 2.260, Global test accuracy: 17.12
Round  57, Train loss: 1.520, Test loss: 1.539, Test accuracy: 92.58
Round  57, Global train loss: 1.520, Global test loss: 2.302, Global test accuracy: 9.60
Round  58, Train loss: 1.464, Test loss: 1.539, Test accuracy: 92.58
Round  58, Global train loss: 1.464, Global test loss: 2.275, Global test accuracy: 15.63
Round  59, Train loss: 1.467, Test loss: 1.539, Test accuracy: 92.58
Round  59, Global train loss: 1.467, Global test loss: 2.287, Global test accuracy: 13.02
Round  60, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.57
Round  60, Global train loss: 1.465, Global test loss: 2.295, Global test accuracy: 13.33
Round  61, Train loss: 1.463, Test loss: 1.538, Test accuracy: 92.58
Round  61, Global train loss: 1.463, Global test loss: 2.277, Global test accuracy: 16.55
Round  62, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.60
Round  62, Global train loss: 1.465, Global test loss: 2.301, Global test accuracy: 13.02
Round  63, Train loss: 1.463, Test loss: 1.538, Test accuracy: 92.60
Round  63, Global train loss: 1.463, Global test loss: 2.280, Global test accuracy: 15.17
Round  64, Train loss: 1.466, Test loss: 1.538, Test accuracy: 92.60
Round  64, Global train loss: 1.466, Global test loss: 2.276, Global test accuracy: 17.88
Round  65, Train loss: 1.519, Test loss: 1.538, Test accuracy: 92.62
Round  65, Global train loss: 1.519, Global test loss: 2.274, Global test accuracy: 16.45
Round  66, Train loss: 1.573, Test loss: 1.538, Test accuracy: 92.60
Round  66, Global train loss: 1.573, Global test loss: 2.272, Global test accuracy: 17.52
Round  67, Train loss: 1.572, Test loss: 1.538, Test accuracy: 92.62
Round  67, Global train loss: 1.572, Global test loss: 2.271, Global test accuracy: 16.87
Round  68, Train loss: 1.520, Test loss: 1.538, Test accuracy: 92.62
Round  68, Global train loss: 1.520, Global test loss: 2.272, Global test accuracy: 15.10
Round  69, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.62
Round  69, Global train loss: 1.465, Global test loss: 2.307, Global test accuracy: 13.33
Round  70, Train loss: 1.467, Test loss: 1.538, Test accuracy: 92.62
Round  70, Global train loss: 1.467, Global test loss: 2.313, Global test accuracy: 13.33
Round  71, Train loss: 1.517, Test loss: 1.538, Test accuracy: 92.63
Round  71, Global train loss: 1.517, Global test loss: 2.267, Global test accuracy: 16.45
Round  72, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.63
Round  72, Global train loss: 1.464, Global test loss: 2.279, Global test accuracy: 16.15
Round  73, Train loss: 1.518, Test loss: 1.538, Test accuracy: 92.63
Round  73, Global train loss: 1.518, Global test loss: 2.294, Global test accuracy: 15.02
Round  74, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.60
Round  74, Global train loss: 1.465, Global test loss: 2.295, Global test accuracy: 12.97
Round  75, Train loss: 1.516, Test loss: 1.538, Test accuracy: 92.60
Round  75, Global train loss: 1.516, Global test loss: 2.285, Global test accuracy: 16.67
Round  76, Train loss: 1.463, Test loss: 1.538, Test accuracy: 92.60
Round  76, Global train loss: 1.463, Global test loss: 2.293, Global test accuracy: 12.73
Round  77, Train loss: 1.465, Test loss: 1.538, Test accuracy: 92.60
Round  77, Global train loss: 1.465, Global test loss: 2.289, Global test accuracy: 12.92
Round  78, Train loss: 1.517, Test loss: 1.538, Test accuracy: 92.60
Round  78, Global train loss: 1.517, Global test loss: 2.271, Global test accuracy: 14.28
Round  79, Train loss: 1.518, Test loss: 1.538, Test accuracy: 92.62
Round  79, Global train loss: 1.518, Global test loss: 2.281, Global test accuracy: 14.17
Round  80, Train loss: 1.518, Test loss: 1.538, Test accuracy: 92.63
Round  80, Global train loss: 1.518, Global test loss: 2.258, Global test accuracy: 18.93
Round  81, Train loss: 1.519, Test loss: 1.538, Test accuracy: 92.63
Round  81, Global train loss: 1.519, Global test loss: 2.256, Global test accuracy: 19.42
Round  82, Train loss: 1.464, Test loss: 1.538, Test accuracy: 92.63
Round  82, Global train loss: 1.464, Global test loss: 2.302, Global test accuracy: 13.33
Round  83, Train loss: 1.521, Test loss: 1.538, Test accuracy: 92.63
Round  83, Global train loss: 1.521, Global test loss: 2.273, Global test accuracy: 15.37
Round  84, Train loss: 1.517, Test loss: 1.538, Test accuracy: 92.63
Round  84, Global train loss: 1.517, Global test loss: 2.282, Global test accuracy: 15.93
Round  85, Train loss: 1.571, Test loss: 1.538, Test accuracy: 92.63
Round  85, Global train loss: 1.571, Global test loss: 2.278, Global test accuracy: 16.67
Round  86, Train loss: 1.518, Test loss: 1.537, Test accuracy: 92.63
Round  86, Global train loss: 1.518, Global test loss: 2.269, Global test accuracy: 16.63
Round  87, Train loss: 1.518, Test loss: 1.537, Test accuracy: 92.63
Round  87, Global train loss: 1.518, Global test loss: 2.278, Global test accuracy: 16.67
Round  88, Train loss: 1.516, Test loss: 1.537, Test accuracy: 92.63
Round  88, Global train loss: 1.516, Global test loss: 2.273, Global test accuracy: 17.42
Round  89, Train loss: 1.517, Test loss: 1.537, Test accuracy: 92.63
Round  89, Global train loss: 1.517, Global test loss: 2.306, Global test accuracy: 12.70
Round  90, Train loss: 1.462, Test loss: 1.537, Test accuracy: 92.63
Round  90, Global train loss: 1.462, Global test loss: 2.276, Global test accuracy: 17.23
Round  91, Train loss: 1.466, Test loss: 1.537, Test accuracy: 92.63
Round  91, Global train loss: 1.466, Global test loss: 2.295, Global test accuracy: 13.15
Round  92, Train loss: 1.463, Test loss: 1.537, Test accuracy: 92.63
Round  92, Global train loss: 1.463, Global test loss: 2.269, Global test accuracy: 18.50
Round  93, Train loss: 1.571, Test loss: 1.537, Test accuracy: 92.63
Round  93, Global train loss: 1.571, Global test loss: 2.270, Global test accuracy: 16.82
Round  94, Train loss: 1.571, Test loss: 1.537, Test accuracy: 92.63
Round  94, Global train loss: 1.571, Global test loss: 2.278, Global test accuracy: 16.73
Round  95, Train loss: 1.518, Test loss: 1.537, Test accuracy: 92.63
Round  95, Global train loss: 1.518, Global test loss: 2.282, Global test accuracy: 15.78/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.572, Test loss: 1.537, Test accuracy: 92.63
Round  96, Global train loss: 1.572, Global test loss: 2.260, Global test accuracy: 19.22
Round  97, Train loss: 1.517, Test loss: 1.537, Test accuracy: 92.62
Round  97, Global train loss: 1.517, Global test loss: 2.276, Global test accuracy: 16.60
Round  98, Train loss: 1.487, Test loss: 1.524, Test accuracy: 93.98
Round  98, Global train loss: 1.487, Global test loss: 2.270, Global test accuracy: 18.12
Round  99, Train loss: 1.469, Test loss: 1.522, Test accuracy: 94.13
Round  99, Global train loss: 1.469, Global test loss: 2.271, Global test accuracy: 16.33
Final Round, Train loss: 1.480, Test loss: 1.515, Test accuracy: 95.33
Final Round, Global train loss: 1.480, Global test loss: 2.271, Global test accuracy: 16.33
Average accuracy final 10 rounds: 92.91666666666666 

Average global accuracy final 10 rounds: 16.848333333333336 

817.3751244544983
[0.6138284206390381, 1.2276568412780762, 1.7064814567565918, 2.1853060722351074, 2.6776998043060303, 3.170093536376953, 3.651358127593994, 4.132622718811035, 4.612827777862549, 5.0930328369140625, 5.5942277908325195, 6.095422744750977, 6.598551511764526, 7.101680278778076, 7.59561014175415, 8.089540004730225, 8.569640636444092, 9.049741268157959, 9.560532808303833, 10.071324348449707, 10.57693362236023, 11.082542896270752, 11.561825275421143, 12.041107654571533, 12.529250144958496, 13.017392635345459, 13.50963568687439, 14.00187873840332, 14.476816177368164, 14.951753616333008, 15.436251163482666, 15.920748710632324, 16.432180166244507, 16.94361162185669, 17.429887294769287, 17.916162967681885, 18.40236735343933, 18.888571739196777, 19.377689361572266, 19.866806983947754, 20.35294198989868, 20.83907699584961, 21.31895089149475, 21.798824787139893, 22.267536163330078, 22.736247539520264, 23.221095085144043, 23.705942630767822, 24.205405712127686, 24.70486879348755, 25.192595958709717, 25.680323123931885, 26.16182041168213, 26.643317699432373, 27.12641429901123, 27.609510898590088, 28.099560499191284, 28.58961009979248, 29.072455167770386, 29.55530023574829, 30.063671350479126, 30.57204246520996, 31.079123973846436, 31.58620548248291, 32.06509780883789, 32.54399013519287, 33.02804613113403, 33.512102127075195, 34.02273869514465, 34.53337526321411, 35.035467863082886, 35.53756046295166, 36.00936317443848, 36.48116588592529, 36.97697591781616, 37.47278594970703, 37.97314715385437, 38.47350835800171, 38.952946186065674, 39.43238401412964, 39.88673663139343, 40.34108924865723, 40.84509015083313, 41.34909105300903, 41.855360984802246, 42.36163091659546, 42.838388204574585, 43.31514549255371, 43.78528165817261, 44.255417823791504, 44.76050519943237, 45.26559257507324, 45.76504707336426, 46.26450157165527, 46.735689640045166, 47.20687770843506, 47.69599652290344, 48.185115337371826, 48.693570375442505, 49.202025413513184, 49.69501805305481, 50.188010692596436, 50.656410217285156, 51.12480974197388, 51.631688594818115, 52.13856744766235, 52.64970421791077, 53.16084098815918, 53.63019299507141, 54.09954500198364, 54.56568217277527, 55.031819343566895, 55.53300762176514, 56.03419589996338, 56.541850328445435, 57.04950475692749, 57.52533960342407, 58.001174449920654, 58.4868586063385, 58.97254276275635, 59.473999977111816, 59.975457191467285, 60.459094285964966, 60.94273138046265, 61.39734768867493, 61.85196399688721, 62.34122014045715, 62.8304762840271, 63.33663892745972, 63.842801570892334, 64.3088698387146, 64.77493810653687, 65.26815485954285, 65.76137161254883, 66.26445579528809, 66.76753997802734, 67.2733039855957, 67.77906799316406, 68.26760530471802, 68.75614261627197, 69.2577772140503, 69.75941181182861, 70.26221036911011, 70.7650089263916, 71.2602550983429, 71.75550127029419, 72.24901556968689, 72.74252986907959, 73.24671816825867, 73.75090646743774, 74.25780034065247, 74.76469421386719, 75.24173331260681, 75.71877241134644, 76.22370648384094, 76.72864055633545, 77.22465443611145, 77.72066831588745, 78.1904969215393, 78.66032552719116, 79.13800859451294, 79.61569166183472, 80.1024878025055, 80.58928394317627, 81.07390856742859, 81.55853319168091, 82.0296220779419, 82.50071096420288, 82.97788572311401, 83.45506048202515, 83.9479124546051, 84.44076442718506, 84.91923785209656, 85.39771127700806, 85.85220980644226, 86.30670833587646, 86.78533554077148, 87.2639627456665, 87.77038049697876, 88.27679824829102, 88.75653266906738, 89.23626708984375, 89.72113513946533, 90.20600318908691, 90.704092502594, 91.20218181610107, 91.68849444389343, 92.17480707168579, 92.65307760238647, 93.13134813308716, 93.63277125358582, 94.13419437408447, 94.62999510765076, 95.12579584121704, 95.6049268245697, 96.08405780792236, 96.54043793678284, 96.99681806564331, 97.47358989715576, 97.95036172866821, 98.93790745735168, 99.92545318603516]
[20.716666666666665, 20.716666666666665, 28.616666666666667, 28.616666666666667, 41.78333333333333, 41.78333333333333, 54.2, 54.2, 61.233333333333334, 61.233333333333334, 65.51666666666667, 65.51666666666667, 68.13333333333334, 68.13333333333334, 74.25, 74.25, 79.6, 79.6, 79.83333333333333, 79.83333333333333, 81.93333333333334, 81.93333333333334, 83.38333333333334, 83.38333333333334, 85.75, 85.75, 89.35, 89.35, 89.58333333333333, 89.58333333333333, 90.75, 90.75, 90.76666666666667, 90.76666666666667, 90.83333333333333, 90.83333333333333, 90.81666666666666, 90.81666666666666, 90.83333333333333, 90.83333333333333, 90.85, 90.85, 90.85, 90.85, 90.9, 90.9, 90.9, 90.9, 90.93333333333334, 90.93333333333334, 90.91666666666667, 90.91666666666667, 90.93333333333334, 90.93333333333334, 90.9, 90.9, 90.93333333333334, 90.93333333333334, 90.95, 90.95, 90.95, 90.95, 90.93333333333334, 90.93333333333334, 90.96666666666667, 90.96666666666667, 90.95, 90.95, 90.96666666666667, 90.96666666666667, 92.5, 92.5, 92.53333333333333, 92.53333333333333, 92.5, 92.5, 92.5, 92.5, 92.53333333333333, 92.53333333333333, 92.53333333333333, 92.53333333333333, 92.55, 92.55, 92.55, 92.55, 92.55, 92.55, 92.56666666666666, 92.56666666666666, 92.6, 92.6, 92.61666666666666, 92.61666666666666, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.61666666666666, 92.61666666666666, 92.53333333333333, 92.53333333333333, 92.55, 92.55, 92.55, 92.55, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.56666666666666, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.58333333333333, 92.56666666666666, 92.56666666666666, 92.58333333333333, 92.58333333333333, 92.6, 92.6, 92.6, 92.6, 92.6, 92.6, 92.61666666666666, 92.61666666666666, 92.6, 92.6, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.61666666666666, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.6, 92.6, 92.6, 92.6, 92.6, 92.6, 92.6, 92.6, 92.6, 92.6, 92.61666666666666, 92.61666666666666, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.63333333333334, 92.61666666666666, 92.61666666666666, 93.98333333333333, 93.98333333333333, 94.13333333333334, 94.13333333333334, 95.33333333333333, 95.33333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.288, Test loss: 2.289, Test accuracy: 23.47
Round   0, Global train loss: 2.288, Global test loss: 2.301, Global test accuracy: 15.75
Round   1, Train loss: 2.259, Test loss: 2.246, Test accuracy: 27.55
Round   1, Global train loss: 2.259, Global test loss: 2.294, Global test accuracy: 11.62
Round   2, Train loss: 2.135, Test loss: 2.167, Test accuracy: 34.95
Round   2, Global train loss: 2.135, Global test loss: 2.280, Global test accuracy: 13.30
Round   3, Train loss: 2.001, Test loss: 2.095, Test accuracy: 38.95
Round   3, Global train loss: 2.001, Global test loss: 2.270, Global test accuracy: 15.53
Round   4, Train loss: 1.948, Test loss: 2.019, Test accuracy: 48.32
Round   4, Global train loss: 1.948, Global test loss: 2.255, Global test accuracy: 20.30
Round   5, Train loss: 1.859, Test loss: 1.955, Test accuracy: 53.78
Round   5, Global train loss: 1.859, Global test loss: 2.237, Global test accuracy: 20.55
Round   6, Train loss: 1.863, Test loss: 1.897, Test accuracy: 58.43
Round   6, Global train loss: 1.863, Global test loss: 2.263, Global test accuracy: 17.97
Round   7, Train loss: 1.815, Test loss: 1.887, Test accuracy: 58.50
Round   7, Global train loss: 1.815, Global test loss: 2.257, Global test accuracy: 18.53
Round   8, Train loss: 1.896, Test loss: 1.843, Test accuracy: 63.07
Round   8, Global train loss: 1.896, Global test loss: 2.254, Global test accuracy: 18.15
Round   9, Train loss: 1.828, Test loss: 1.817, Test accuracy: 66.07
Round   9, Global train loss: 1.828, Global test loss: 2.236, Global test accuracy: 19.97
Round  10, Train loss: 1.812, Test loss: 1.829, Test accuracy: 64.65
Round  10, Global train loss: 1.812, Global test loss: 2.223, Global test accuracy: 21.53
Round  11, Train loss: 1.722, Test loss: 1.808, Test accuracy: 66.35
Round  11, Global train loss: 1.722, Global test loss: 2.217, Global test accuracy: 23.17
Round  12, Train loss: 1.777, Test loss: 1.816, Test accuracy: 65.60
Round  12, Global train loss: 1.777, Global test loss: 2.225, Global test accuracy: 22.23
Round  13, Train loss: 1.953, Test loss: 1.803, Test accuracy: 66.93
Round  13, Global train loss: 1.953, Global test loss: 2.216, Global test accuracy: 22.85
Round  14, Train loss: 1.863, Test loss: 1.797, Test accuracy: 66.98
Round  14, Global train loss: 1.863, Global test loss: 2.221, Global test accuracy: 22.25
Round  15, Train loss: 1.907, Test loss: 1.798, Test accuracy: 66.92
Round  15, Global train loss: 1.907, Global test loss: 2.231, Global test accuracy: 21.42
Round  16, Train loss: 1.867, Test loss: 1.787, Test accuracy: 68.20
Round  16, Global train loss: 1.867, Global test loss: 2.215, Global test accuracy: 22.70
Round  17, Train loss: 1.724, Test loss: 1.787, Test accuracy: 68.27
Round  17, Global train loss: 1.724, Global test loss: 2.208, Global test accuracy: 24.42
Round  18, Train loss: 1.811, Test loss: 1.784, Test accuracy: 68.33
Round  18, Global train loss: 1.811, Global test loss: 2.204, Global test accuracy: 24.72
Round  19, Train loss: 1.822, Test loss: 1.785, Test accuracy: 68.25
Round  19, Global train loss: 1.822, Global test loss: 2.201, Global test accuracy: 25.30
Round  20, Train loss: 1.936, Test loss: 1.785, Test accuracy: 68.23
Round  20, Global train loss: 1.936, Global test loss: 2.225, Global test accuracy: 22.57
Round  21, Train loss: 1.994, Test loss: 1.796, Test accuracy: 66.72
Round  21, Global train loss: 1.994, Global test loss: 2.211, Global test accuracy: 23.95
Round  22, Train loss: 1.866, Test loss: 1.802, Test accuracy: 65.80
Round  22, Global train loss: 1.866, Global test loss: 2.210, Global test accuracy: 23.07
Round  23, Train loss: 1.717, Test loss: 1.775, Test accuracy: 68.83
Round  23, Global train loss: 1.717, Global test loss: 2.207, Global test accuracy: 23.87
Round  24, Train loss: 1.849, Test loss: 1.774, Test accuracy: 68.90
Round  24, Global train loss: 1.849, Global test loss: 2.208, Global test accuracy: 23.63
Round  25, Train loss: 1.763, Test loss: 1.770, Test accuracy: 69.30
Round  25, Global train loss: 1.763, Global test loss: 2.226, Global test accuracy: 22.02
Round  26, Train loss: 1.676, Test loss: 1.771, Test accuracy: 69.23
Round  26, Global train loss: 1.676, Global test loss: 2.212, Global test accuracy: 23.00
Round  27, Train loss: 1.825, Test loss: 1.771, Test accuracy: 69.23
Round  27, Global train loss: 1.825, Global test loss: 2.215, Global test accuracy: 23.22
Round  28, Train loss: 1.821, Test loss: 1.768, Test accuracy: 69.53
Round  28, Global train loss: 1.821, Global test loss: 2.205, Global test accuracy: 23.92
Round  29, Train loss: 1.754, Test loss: 1.769, Test accuracy: 69.28
Round  29, Global train loss: 1.754, Global test loss: 2.203, Global test accuracy: 24.62
Round  30, Train loss: 1.718, Test loss: 1.768, Test accuracy: 69.30
Round  30, Global train loss: 1.718, Global test loss: 2.196, Global test accuracy: 25.40
Round  31, Train loss: 1.812, Test loss: 1.770, Test accuracy: 69.10
Round  31, Global train loss: 1.812, Global test loss: 2.198, Global test accuracy: 24.58
Round  32, Train loss: 1.839, Test loss: 1.773, Test accuracy: 68.85
Round  32, Global train loss: 1.839, Global test loss: 2.202, Global test accuracy: 24.30
Round  33, Train loss: 1.931, Test loss: 1.758, Test accuracy: 70.45
Round  33, Global train loss: 1.931, Global test loss: 2.216, Global test accuracy: 22.88
Round  34, Train loss: 1.685, Test loss: 1.753, Test accuracy: 70.87
Round  34, Global train loss: 1.685, Global test loss: 2.211, Global test accuracy: 23.10
Round  35, Train loss: 1.767, Test loss: 1.753, Test accuracy: 70.92
Round  35, Global train loss: 1.767, Global test loss: 2.196, Global test accuracy: 25.37
Round  36, Train loss: 1.847, Test loss: 1.753, Test accuracy: 70.92
Round  36, Global train loss: 1.847, Global test loss: 2.223, Global test accuracy: 21.45
Round  37, Train loss: 1.685, Test loss: 1.752, Test accuracy: 70.95
Round  37, Global train loss: 1.685, Global test loss: 2.203, Global test accuracy: 23.58
Round  38, Train loss: 1.776, Test loss: 1.754, Test accuracy: 70.78
Round  38, Global train loss: 1.776, Global test loss: 2.199, Global test accuracy: 24.53
Round  39, Train loss: 1.791, Test loss: 1.753, Test accuracy: 70.80
Round  39, Global train loss: 1.791, Global test loss: 2.213, Global test accuracy: 22.95
Round  40, Train loss: 1.765, Test loss: 1.739, Test accuracy: 72.27
Round  40, Global train loss: 1.765, Global test loss: 2.215, Global test accuracy: 22.98
Round  41, Train loss: 1.774, Test loss: 1.738, Test accuracy: 72.22
Round  41, Global train loss: 1.774, Global test loss: 2.210, Global test accuracy: 23.08
Round  42, Train loss: 1.773, Test loss: 1.724, Test accuracy: 73.70
Round  42, Global train loss: 1.773, Global test loss: 2.210, Global test accuracy: 23.55
Round  43, Train loss: 1.918, Test loss: 1.741, Test accuracy: 71.97
Round  43, Global train loss: 1.918, Global test loss: 2.236, Global test accuracy: 21.17
Round  44, Train loss: 1.773, Test loss: 1.740, Test accuracy: 71.93
Round  44, Global train loss: 1.773, Global test loss: 2.200, Global test accuracy: 25.30
Round  45, Train loss: 1.661, Test loss: 1.740, Test accuracy: 71.98
Round  45, Global train loss: 1.661, Global test loss: 2.210, Global test accuracy: 23.95
Round  46, Train loss: 1.815, Test loss: 1.740, Test accuracy: 72.03
Round  46, Global train loss: 1.815, Global test loss: 2.209, Global test accuracy: 23.60
Round  47, Train loss: 1.694, Test loss: 1.738, Test accuracy: 72.20
Round  47, Global train loss: 1.694, Global test loss: 2.203, Global test accuracy: 24.83
Round  48, Train loss: 1.667, Test loss: 1.739, Test accuracy: 72.08
Round  48, Global train loss: 1.667, Global test loss: 2.201, Global test accuracy: 24.83
Round  49, Train loss: 1.731, Test loss: 1.738, Test accuracy: 72.13
Round  49, Global train loss: 1.731, Global test loss: 2.203, Global test accuracy: 24.62
Round  50, Train loss: 1.658, Test loss: 1.738, Test accuracy: 72.23
Round  50, Global train loss: 1.658, Global test loss: 2.199, Global test accuracy: 25.13
Round  51, Train loss: 1.768, Test loss: 1.737, Test accuracy: 72.25
Round  51, Global train loss: 1.768, Global test loss: 2.189, Global test accuracy: 25.97
Round  52, Train loss: 1.607, Test loss: 1.737, Test accuracy: 72.30
Round  52, Global train loss: 1.607, Global test loss: 2.216, Global test accuracy: 23.55
Round  53, Train loss: 1.662, Test loss: 1.737, Test accuracy: 72.28
Round  53, Global train loss: 1.662, Global test loss: 2.202, Global test accuracy: 24.62
Round  54, Train loss: 1.717, Test loss: 1.734, Test accuracy: 72.62
Round  54, Global train loss: 1.717, Global test loss: 2.204, Global test accuracy: 23.70
Round  55, Train loss: 1.706, Test loss: 1.707, Test accuracy: 75.33
Round  55, Global train loss: 1.706, Global test loss: 2.187, Global test accuracy: 25.88
Round  56, Train loss: 1.673, Test loss: 1.694, Test accuracy: 76.77
Round  56, Global train loss: 1.673, Global test loss: 2.198, Global test accuracy: 24.88
Round  57, Train loss: 1.703, Test loss: 1.695, Test accuracy: 76.70
Round  57, Global train loss: 1.703, Global test loss: 2.209, Global test accuracy: 23.48
Round  58, Train loss: 1.779, Test loss: 1.696, Test accuracy: 76.55
Round  58, Global train loss: 1.779, Global test loss: 2.223, Global test accuracy: 22.10
Round  59, Train loss: 1.659, Test loss: 1.693, Test accuracy: 76.85
Round  59, Global train loss: 1.659, Global test loss: 2.218, Global test accuracy: 22.90
Round  60, Train loss: 1.721, Test loss: 1.709, Test accuracy: 75.25
Round  60, Global train loss: 1.721, Global test loss: 2.205, Global test accuracy: 24.20
Round  61, Train loss: 1.608, Test loss: 1.709, Test accuracy: 75.13
Round  61, Global train loss: 1.608, Global test loss: 2.212, Global test accuracy: 22.67
Round  62, Train loss: 1.711, Test loss: 1.711, Test accuracy: 75.00
Round  62, Global train loss: 1.711, Global test loss: 2.239, Global test accuracy: 20.63
Round  63, Train loss: 1.770, Test loss: 1.728, Test accuracy: 73.37
Round  63, Global train loss: 1.770, Global test loss: 2.201, Global test accuracy: 23.85
Round  64, Train loss: 1.640, Test loss: 1.715, Test accuracy: 74.77
Round  64, Global train loss: 1.640, Global test loss: 2.204, Global test accuracy: 23.57
Round  65, Train loss: 1.677, Test loss: 1.696, Test accuracy: 76.57
Round  65, Global train loss: 1.677, Global test loss: 2.221, Global test accuracy: 22.62
Round  66, Train loss: 1.745, Test loss: 1.698, Test accuracy: 76.45
Round  66, Global train loss: 1.745, Global test loss: 2.230, Global test accuracy: 20.60
Round  67, Train loss: 1.776, Test loss: 1.714, Test accuracy: 74.67
Round  67, Global train loss: 1.776, Global test loss: 2.187, Global test accuracy: 26.67
Round  68, Train loss: 1.706, Test loss: 1.698, Test accuracy: 76.32
Round  68, Global train loss: 1.706, Global test loss: 2.186, Global test accuracy: 26.57
Round  69, Train loss: 1.712, Test loss: 1.696, Test accuracy: 76.37
Round  69, Global train loss: 1.712, Global test loss: 2.195, Global test accuracy: 25.40
Round  70, Train loss: 1.659, Test loss: 1.695, Test accuracy: 76.47
Round  70, Global train loss: 1.659, Global test loss: 2.193, Global test accuracy: 24.92
Round  71, Train loss: 1.800, Test loss: 1.694, Test accuracy: 76.52
Round  71, Global train loss: 1.800, Global test loss: 2.180, Global test accuracy: 26.88
Round  72, Train loss: 1.747, Test loss: 1.694, Test accuracy: 76.53
Round  72, Global train loss: 1.747, Global test loss: 2.193, Global test accuracy: 24.98
Round  73, Train loss: 1.792, Test loss: 1.696, Test accuracy: 76.30
Round  73, Global train loss: 1.792, Global test loss: 2.212, Global test accuracy: 22.78
Round  74, Train loss: 1.817, Test loss: 1.695, Test accuracy: 76.33
Round  74, Global train loss: 1.817, Global test loss: 2.225, Global test accuracy: 21.07
Round  75, Train loss: 1.729, Test loss: 1.680, Test accuracy: 78.03
Round  75, Global train loss: 1.729, Global test loss: 2.205, Global test accuracy: 23.95
Round  76, Train loss: 1.711, Test loss: 1.680, Test accuracy: 78.08
Round  76, Global train loss: 1.711, Global test loss: 2.188, Global test accuracy: 26.32
Round  77, Train loss: 1.677, Test loss: 1.680, Test accuracy: 78.03
Round  77, Global train loss: 1.677, Global test loss: 2.175, Global test accuracy: 27.68
Round  78, Train loss: 1.647, Test loss: 1.680, Test accuracy: 78.07
Round  78, Global train loss: 1.647, Global test loss: 2.188, Global test accuracy: 26.58
Round  79, Train loss: 1.913, Test loss: 1.710, Test accuracy: 74.80
Round  79, Global train loss: 1.913, Global test loss: 2.200, Global test accuracy: 24.62
Round  80, Train loss: 1.680, Test loss: 1.695, Test accuracy: 76.57
Round  80, Global train loss: 1.680, Global test loss: 2.218, Global test accuracy: 21.58
Round  81, Train loss: 1.665, Test loss: 1.695, Test accuracy: 76.65
Round  81, Global train loss: 1.665, Global test loss: 2.196, Global test accuracy: 24.63
Round  82, Train loss: 1.597, Test loss: 1.695, Test accuracy: 76.68
Round  82, Global train loss: 1.597, Global test loss: 2.210, Global test accuracy: 22.82
Round  83, Train loss: 1.634, Test loss: 1.694, Test accuracy: 76.60
Round  83, Global train loss: 1.634, Global test loss: 2.241, Global test accuracy: 19.72
Round  84, Train loss: 1.766, Test loss: 1.694, Test accuracy: 76.72
Round  84, Global train loss: 1.766, Global test loss: 2.217, Global test accuracy: 22.62
Round  85, Train loss: 1.658, Test loss: 1.695, Test accuracy: 76.70
Round  85, Global train loss: 1.658, Global test loss: 2.220, Global test accuracy: 21.78
Round  86, Train loss: 1.842, Test loss: 1.680, Test accuracy: 78.28
Round  86, Global train loss: 1.842, Global test loss: 2.206, Global test accuracy: 23.77
Round  87, Train loss: 1.647, Test loss: 1.678, Test accuracy: 78.50
Round  87, Global train loss: 1.647, Global test loss: 2.212, Global test accuracy: 22.83
Round  88, Train loss: 1.709, Test loss: 1.693, Test accuracy: 76.80
Round  88, Global train loss: 1.709, Global test loss: 2.234, Global test accuracy: 20.48
Round  89, Train loss: 1.806, Test loss: 1.693, Test accuracy: 76.75
Round  89, Global train loss: 1.806, Global test loss: 2.232, Global test accuracy: 21.30
Round  90, Train loss: 1.785, Test loss: 1.692, Test accuracy: 76.92
Round  90, Global train loss: 1.785, Global test loss: 2.202, Global test accuracy: 23.77
Round  91, Train loss: 1.706, Test loss: 1.692, Test accuracy: 76.92
Round  91, Global train loss: 1.706, Global test loss: 2.219, Global test accuracy: 21.75
Round  92, Train loss: 1.709, Test loss: 1.677, Test accuracy: 78.42
Round  92, Global train loss: 1.709, Global test loss: 2.188, Global test accuracy: 25.90
Round  93, Train loss: 1.599, Test loss: 1.678, Test accuracy: 78.25
Round  93, Global train loss: 1.599, Global test loss: 2.177, Global test accuracy: 27.15
Round  94, Train loss: 1.705, Test loss: 1.678, Test accuracy: 78.25
Round  94, Global train loss: 1.705, Global test loss: 2.211, Global test accuracy: 22.97
Round  95, Train loss: 1.661, Test loss: 1.679, Test accuracy: 78.32
Round  95, Global train loss: 1.661, Global test loss: 2.209, Global test accuracy: 23.50/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.731, Test loss: 1.680, Test accuracy: 78.23
Round  96, Global train loss: 1.731, Global test loss: 2.201, Global test accuracy: 23.57
Round  97, Train loss: 1.604, Test loss: 1.679, Test accuracy: 78.08
Round  97, Global train loss: 1.604, Global test loss: 2.200, Global test accuracy: 24.62
Round  98, Train loss: 1.649, Test loss: 1.679, Test accuracy: 78.12
Round  98, Global train loss: 1.649, Global test loss: 2.204, Global test accuracy: 24.27
Round  99, Train loss: 1.682, Test loss: 1.680, Test accuracy: 78.10
Round  99, Global train loss: 1.682, Global test loss: 2.193, Global test accuracy: 24.73
Final Round, Train loss: 1.655, Test loss: 1.676, Test accuracy: 78.45
Final Round, Global train loss: 1.655, Global test loss: 2.193, Global test accuracy: 24.73
Average accuracy final 10 rounds: 77.96 

Average global accuracy final 10 rounds: 24.221666666666664 

820.3058934211731
[0.6167964935302734, 1.2335929870605469, 1.7300827503204346, 2.2265725135803223, 2.68865704536438, 3.1507415771484375, 3.6444454193115234, 4.138149261474609, 4.650973081588745, 5.163796901702881, 5.663764476776123, 6.163732051849365, 6.652815818786621, 7.141899585723877, 7.628270387649536, 8.114641189575195, 8.607944965362549, 9.101248741149902, 9.574775695800781, 10.04830265045166, 10.511776208877563, 10.975249767303467, 11.474617719650269, 11.97398567199707, 12.474647998809814, 12.975310325622559, 13.477705717086792, 13.980101108551025, 14.471786260604858, 14.963471412658691, 15.434319257736206, 15.90516710281372, 16.381237268447876, 16.85730743408203, 17.35780167579651, 17.858295917510986, 18.36538815498352, 18.872480392456055, 19.34527277946472, 19.81806516647339, 20.2988440990448, 20.77962303161621, 21.293455123901367, 21.807287216186523, 22.312706232070923, 22.818125247955322, 23.282211780548096, 23.74629831314087, 24.24715542793274, 24.74801254272461, 25.256230354309082, 25.764448165893555, 26.261784315109253, 26.75912046432495, 27.249905586242676, 27.7406907081604, 28.25101351737976, 28.76133632659912, 29.25923180580139, 29.757127285003662, 30.241872549057007, 30.72661781311035, 31.23850154876709, 31.750385284423828, 32.2591814994812, 32.767977714538574, 33.27714562416077, 33.78631353378296, 34.267539501190186, 34.74876546859741, 35.26210355758667, 35.77544164657593, 36.30167198181152, 36.82790231704712, 37.33034348487854, 37.83278465270996, 38.30637764930725, 38.77997064590454, 39.27410268783569, 39.768234729766846, 40.274585485458374, 40.7809362411499, 41.23147773742676, 41.68201923370361, 42.16956186294556, 42.6571044921875, 43.153828382492065, 43.65055227279663, 44.133105993270874, 44.61565971374512, 45.09899854660034, 45.582337379455566, 46.08451724052429, 46.58669710159302, 47.11612343788147, 47.64554977416992, 48.1467399597168, 48.64793014526367, 49.150784492492676, 49.65363883972168, 50.16442918777466, 50.67521953582764, 51.16972780227661, 51.664236068725586, 52.150025844573975, 52.63581562042236, 53.134071826934814, 53.632328033447266, 54.11344504356384, 54.59456205368042, 55.0664267539978, 55.538291454315186, 56.0434467792511, 56.54860210418701, 57.03994369506836, 57.53128528594971, 57.99219465255737, 58.45310401916504, 58.94601535797119, 59.438926696777344, 59.96124529838562, 60.4835638999939, 60.987115144729614, 61.49066638946533, 61.961432695388794, 62.432199001312256, 62.94475865364075, 63.45731830596924, 63.983184814453125, 64.50905132293701, 65.00312757492065, 65.4972038269043, 65.97171592712402, 66.44622802734375, 66.96483063697815, 67.48343324661255, 67.98755502700806, 68.49167680740356, 68.97352981567383, 69.45538282394409, 69.95601916313171, 70.45665550231934, 70.97887897491455, 71.50110244750977, 71.97186827659607, 72.44263410568237, 72.92653942108154, 73.41044473648071, 73.89573955535889, 74.38103437423706, 74.88406014442444, 75.38708591461182, 75.84651970863342, 76.30595350265503, 76.81711339950562, 77.3282732963562, 77.8322503566742, 78.33622741699219, 78.84406876564026, 79.35191011428833, 79.83350157737732, 80.31509304046631, 80.82276773452759, 81.33044242858887, 81.84654664993286, 82.36265087127686, 82.86026358604431, 83.35787630081177, 83.87297677993774, 84.38807725906372, 84.90224504470825, 85.41641283035278, 85.88539171218872, 86.35437059402466, 86.84570598602295, 87.33704137802124, 87.82410216331482, 88.3111629486084, 88.7870876789093, 89.2630124092102, 89.73103356361389, 90.19905471801758, 90.69749546051025, 91.19593620300293, 91.70260000228882, 92.2092638015747, 92.69403743743896, 93.17881107330322, 93.6329653263092, 94.08711957931519, 94.59834551811218, 95.10957145690918, 95.61715245246887, 96.12473344802856, 96.6012794971466, 97.07782554626465, 97.58564400672913, 98.0934624671936, 98.60666584968567, 99.11986923217773, 100.14097857475281, 101.16208791732788]
[23.466666666666665, 23.466666666666665, 27.55, 27.55, 34.95, 34.95, 38.95, 38.95, 48.31666666666667, 48.31666666666667, 53.78333333333333, 53.78333333333333, 58.43333333333333, 58.43333333333333, 58.5, 58.5, 63.06666666666667, 63.06666666666667, 66.06666666666666, 66.06666666666666, 64.65, 64.65, 66.35, 66.35, 65.6, 65.6, 66.93333333333334, 66.93333333333334, 66.98333333333333, 66.98333333333333, 66.91666666666667, 66.91666666666667, 68.2, 68.2, 68.26666666666667, 68.26666666666667, 68.33333333333333, 68.33333333333333, 68.25, 68.25, 68.23333333333333, 68.23333333333333, 66.71666666666667, 66.71666666666667, 65.8, 65.8, 68.83333333333333, 68.83333333333333, 68.9, 68.9, 69.3, 69.3, 69.23333333333333, 69.23333333333333, 69.23333333333333, 69.23333333333333, 69.53333333333333, 69.53333333333333, 69.28333333333333, 69.28333333333333, 69.3, 69.3, 69.1, 69.1, 68.85, 68.85, 70.45, 70.45, 70.86666666666666, 70.86666666666666, 70.91666666666667, 70.91666666666667, 70.91666666666667, 70.91666666666667, 70.95, 70.95, 70.78333333333333, 70.78333333333333, 70.8, 70.8, 72.26666666666667, 72.26666666666667, 72.21666666666667, 72.21666666666667, 73.7, 73.7, 71.96666666666667, 71.96666666666667, 71.93333333333334, 71.93333333333334, 71.98333333333333, 71.98333333333333, 72.03333333333333, 72.03333333333333, 72.2, 72.2, 72.08333333333333, 72.08333333333333, 72.13333333333334, 72.13333333333334, 72.23333333333333, 72.23333333333333, 72.25, 72.25, 72.3, 72.3, 72.28333333333333, 72.28333333333333, 72.61666666666666, 72.61666666666666, 75.33333333333333, 75.33333333333333, 76.76666666666667, 76.76666666666667, 76.7, 76.7, 76.55, 76.55, 76.85, 76.85, 75.25, 75.25, 75.13333333333334, 75.13333333333334, 75.0, 75.0, 73.36666666666666, 73.36666666666666, 74.76666666666667, 74.76666666666667, 76.56666666666666, 76.56666666666666, 76.45, 76.45, 74.66666666666667, 74.66666666666667, 76.31666666666666, 76.31666666666666, 76.36666666666666, 76.36666666666666, 76.46666666666667, 76.46666666666667, 76.51666666666667, 76.51666666666667, 76.53333333333333, 76.53333333333333, 76.3, 76.3, 76.33333333333333, 76.33333333333333, 78.03333333333333, 78.03333333333333, 78.08333333333333, 78.08333333333333, 78.03333333333333, 78.03333333333333, 78.06666666666666, 78.06666666666666, 74.8, 74.8, 76.56666666666666, 76.56666666666666, 76.65, 76.65, 76.68333333333334, 76.68333333333334, 76.6, 76.6, 76.71666666666667, 76.71666666666667, 76.7, 76.7, 78.28333333333333, 78.28333333333333, 78.5, 78.5, 76.8, 76.8, 76.75, 76.75, 76.91666666666667, 76.91666666666667, 76.91666666666667, 76.91666666666667, 78.41666666666667, 78.41666666666667, 78.25, 78.25, 78.25, 78.25, 78.31666666666666, 78.31666666666666, 78.23333333333333, 78.23333333333333, 78.08333333333333, 78.08333333333333, 78.11666666666666, 78.11666666666666, 78.1, 78.1, 78.45, 78.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.294, Test loss: 2.300, Test accuracy: 15.37
Round   1, Train loss: 2.294, Test loss: 2.298, Test accuracy: 17.90
Round   2, Train loss: 2.287, Test loss: 2.296, Test accuracy: 17.58
Round   3, Train loss: 2.284, Test loss: 2.291, Test accuracy: 16.58
Round   4, Train loss: 2.269, Test loss: 2.279, Test accuracy: 19.83
Round   5, Train loss: 2.168, Test loss: 2.241, Test accuracy: 22.02
Round   6, Train loss: 2.101, Test loss: 2.200, Test accuracy: 25.10
Round   7, Train loss: 2.084, Test loss: 2.143, Test accuracy: 30.97
Round   8, Train loss: 2.012, Test loss: 2.102, Test accuracy: 39.12
Round   9, Train loss: 1.889, Test loss: 2.022, Test accuracy: 45.43
Round  10, Train loss: 1.909, Test loss: 1.962, Test accuracy: 53.47
Round  11, Train loss: 1.785, Test loss: 1.908, Test accuracy: 58.70
Round  12, Train loss: 1.700, Test loss: 1.839, Test accuracy: 64.92
Round  13, Train loss: 1.782, Test loss: 1.819, Test accuracy: 65.30
Round  14, Train loss: 1.713, Test loss: 1.793, Test accuracy: 68.52
Round  15, Train loss: 1.596, Test loss: 1.766, Test accuracy: 71.07
Round  16, Train loss: 1.733, Test loss: 1.755, Test accuracy: 72.02
Round  17, Train loss: 1.693, Test loss: 1.744, Test accuracy: 73.15
Round  18, Train loss: 1.796, Test loss: 1.732, Test accuracy: 74.23
Round  19, Train loss: 1.753, Test loss: 1.727, Test accuracy: 74.85
Round  20, Train loss: 1.620, Test loss: 1.724, Test accuracy: 74.82
Round  21, Train loss: 1.670, Test loss: 1.720, Test accuracy: 75.15
Round  22, Train loss: 1.594, Test loss: 1.716, Test accuracy: 75.77
Round  23, Train loss: 1.711, Test loss: 1.713, Test accuracy: 76.07
Round  24, Train loss: 1.778, Test loss: 1.711, Test accuracy: 76.15
Round  25, Train loss: 1.814, Test loss: 1.710, Test accuracy: 76.32
Round  26, Train loss: 1.732, Test loss: 1.709, Test accuracy: 76.33
Round  27, Train loss: 1.712, Test loss: 1.703, Test accuracy: 76.75
Round  28, Train loss: 1.723, Test loss: 1.703, Test accuracy: 76.72
Round  29, Train loss: 1.658, Test loss: 1.701, Test accuracy: 76.85
Round  30, Train loss: 1.761, Test loss: 1.699, Test accuracy: 76.97
Round  31, Train loss: 1.607, Test loss: 1.697, Test accuracy: 77.30
Round  32, Train loss: 1.767, Test loss: 1.696, Test accuracy: 77.27
Round  33, Train loss: 1.610, Test loss: 1.695, Test accuracy: 77.15
Round  34, Train loss: 1.758, Test loss: 1.696, Test accuracy: 77.02
Round  35, Train loss: 1.609, Test loss: 1.694, Test accuracy: 77.18
Round  36, Train loss: 1.711, Test loss: 1.693, Test accuracy: 77.23
Round  37, Train loss: 1.709, Test loss: 1.692, Test accuracy: 77.40
Round  38, Train loss: 1.558, Test loss: 1.690, Test accuracy: 77.72
Round  39, Train loss: 1.651, Test loss: 1.689, Test accuracy: 77.72
Round  40, Train loss: 1.706, Test loss: 1.688, Test accuracy: 77.77
Round  41, Train loss: 1.544, Test loss: 1.688, Test accuracy: 77.85
Round  42, Train loss: 1.752, Test loss: 1.688, Test accuracy: 77.87
Round  43, Train loss: 1.642, Test loss: 1.688, Test accuracy: 77.83
Round  44, Train loss: 1.752, Test loss: 1.687, Test accuracy: 77.83
Round  45, Train loss: 1.694, Test loss: 1.687, Test accuracy: 77.75
Round  46, Train loss: 1.706, Test loss: 1.687, Test accuracy: 77.78
Round  47, Train loss: 1.598, Test loss: 1.686, Test accuracy: 77.83
Round  48, Train loss: 1.703, Test loss: 1.686, Test accuracy: 77.87
Round  49, Train loss: 1.749, Test loss: 1.687, Test accuracy: 77.58
Round  50, Train loss: 1.710, Test loss: 1.673, Test accuracy: 79.32
Round  51, Train loss: 1.551, Test loss: 1.671, Test accuracy: 79.52
Round  52, Train loss: 1.649, Test loss: 1.671, Test accuracy: 79.38
Round  53, Train loss: 1.594, Test loss: 1.671, Test accuracy: 79.45
Round  54, Train loss: 1.539, Test loss: 1.670, Test accuracy: 79.52
Round  55, Train loss: 1.596, Test loss: 1.671, Test accuracy: 79.47
Round  56, Train loss: 1.651, Test loss: 1.670, Test accuracy: 79.52
Round  57, Train loss: 1.601, Test loss: 1.670, Test accuracy: 79.53
Round  58, Train loss: 1.594, Test loss: 1.670, Test accuracy: 79.52
Round  59, Train loss: 1.584, Test loss: 1.669, Test accuracy: 79.55
Round  60, Train loss: 1.696, Test loss: 1.669, Test accuracy: 79.57
Round  61, Train loss: 1.637, Test loss: 1.669, Test accuracy: 79.57
Round  62, Train loss: 1.802, Test loss: 1.669, Test accuracy: 79.55
Round  63, Train loss: 1.641, Test loss: 1.669, Test accuracy: 79.57
Round  64, Train loss: 1.754, Test loss: 1.670, Test accuracy: 79.50
Round  65, Train loss: 1.589, Test loss: 1.670, Test accuracy: 79.48
Round  66, Train loss: 1.644, Test loss: 1.669, Test accuracy: 79.40
Round  67, Train loss: 1.645, Test loss: 1.668, Test accuracy: 79.47
Round  68, Train loss: 1.588, Test loss: 1.668, Test accuracy: 79.52
Round  69, Train loss: 1.697, Test loss: 1.668, Test accuracy: 79.52
Round  70, Train loss: 1.530, Test loss: 1.668, Test accuracy: 79.62
Round  71, Train loss: 1.587, Test loss: 1.668, Test accuracy: 79.45
Round  72, Train loss: 1.535, Test loss: 1.668, Test accuracy: 79.45
Round  73, Train loss: 1.531, Test loss: 1.667, Test accuracy: 79.43
Round  74, Train loss: 1.644, Test loss: 1.667, Test accuracy: 79.58
Round  75, Train loss: 1.581, Test loss: 1.667, Test accuracy: 79.58
Round  76, Train loss: 1.801, Test loss: 1.667, Test accuracy: 79.62
Round  77, Train loss: 1.635, Test loss: 1.667, Test accuracy: 79.57
Round  78, Train loss: 1.586, Test loss: 1.667, Test accuracy: 79.60
Round  79, Train loss: 1.701, Test loss: 1.667, Test accuracy: 79.62
Round  80, Train loss: 1.527, Test loss: 1.667, Test accuracy: 79.63
Round  81, Train loss: 1.641, Test loss: 1.666, Test accuracy: 79.65
Round  82, Train loss: 1.746, Test loss: 1.666, Test accuracy: 79.62
Round  83, Train loss: 1.750, Test loss: 1.666, Test accuracy: 79.67
Round  84, Train loss: 1.699, Test loss: 1.667, Test accuracy: 79.62
Round  85, Train loss: 1.587, Test loss: 1.666, Test accuracy: 79.65
Round  86, Train loss: 1.643, Test loss: 1.666, Test accuracy: 79.65
Round  87, Train loss: 1.644, Test loss: 1.666, Test accuracy: 79.58
Round  88, Train loss: 1.693, Test loss: 1.667, Test accuracy: 79.53
Round  89, Train loss: 1.638, Test loss: 1.667, Test accuracy: 79.57
Round  90, Train loss: 1.640, Test loss: 1.666, Test accuracy: 79.55
Round  91, Train loss: 1.635, Test loss: 1.666, Test accuracy: 79.62
Round  92, Train loss: 1.581, Test loss: 1.667, Test accuracy: 79.57
Round  93, Train loss: 1.797, Test loss: 1.667, Test accuracy: 79.65
Round  94, Train loss: 1.634, Test loss: 1.666, Test accuracy: 79.58
Round  95, Train loss: 1.580, Test loss: 1.666, Test accuracy: 79.53
Round  96, Train loss: 1.640, Test loss: 1.666, Test accuracy: 79.53
Round  97, Train loss: 1.799, Test loss: 1.666, Test accuracy: 79.67
Round  98, Train loss: 1.536, Test loss: 1.666, Test accuracy: 79.58/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.686, Test loss: 1.665, Test accuracy: 79.60
Final Round, Train loss: 1.630, Test loss: 1.651, Test accuracy: 81.05
Average accuracy final 10 rounds: 79.58833333333332 

649.1614351272583
[0.613447904586792, 1.226895809173584, 1.6867222785949707, 2.1465487480163574, 2.6097564697265625, 3.0729641914367676, 3.576486349105835, 4.080008506774902, 4.5699052810668945, 5.059802055358887, 5.557373285293579, 6.0549445152282715, 6.5247719287872314, 6.994599342346191, 7.469573736190796, 7.9445481300354, 8.438491821289062, 8.932435512542725, 9.42354154586792, 9.914647579193115, 10.406340837478638, 10.89803409576416, 11.394094467163086, 11.890154838562012, 12.400367259979248, 12.910579681396484, 13.404735565185547, 13.89889144897461, 14.395390748977661, 14.891890048980713, 15.376450300216675, 15.861010551452637, 16.3598735332489, 16.858736515045166, 17.359763383865356, 17.860790252685547, 18.365408182144165, 18.870026111602783, 19.367528915405273, 19.865031719207764, 20.34096097946167, 20.816890239715576, 21.33225727081299, 21.8476243019104, 22.35785174369812, 22.86807918548584, 23.370232343673706, 23.872385501861572, 24.356656789779663, 24.840928077697754, 25.328413486480713, 25.815898895263672, 26.31322979927063, 26.810560703277588, 27.31678318977356, 27.82300567626953, 28.28544044494629, 28.747875213623047, 29.23671317100525, 29.72555112838745, 30.232635498046875, 30.7397198677063, 31.20683217048645, 31.6739444732666, 32.16877627372742, 32.66360807418823, 33.14160418510437, 33.61960029602051, 34.11414408683777, 34.60868787765503, 35.09907603263855, 35.58946418762207, 36.07628893852234, 36.56311368942261, 37.02364492416382, 37.48417615890503, 37.9588360786438, 38.43349599838257, 38.907256841659546, 39.38101768493652, 39.881290912628174, 40.381564140319824, 40.888798236846924, 41.39603233337402, 41.86689591407776, 42.337759494781494, 42.82236385345459, 43.306968212127686, 43.80768346786499, 44.308398723602295, 44.8215434551239, 45.33468818664551, 45.83285188674927, 46.33101558685303, 46.80004692077637, 47.26907825469971, 47.77506637573242, 48.28105449676514, 48.78872561454773, 49.29639673233032, 49.76951479911804, 50.24263286590576, 50.70253086090088, 51.162428855895996, 51.654590129852295, 52.146751403808594, 52.632477045059204, 53.118202686309814, 53.588008880615234, 54.057815074920654, 54.51422953605652, 54.97064399719238, 55.44619655609131, 55.921749114990234, 56.4177041053772, 56.91365909576416, 57.38166117668152, 57.84966325759888, 58.324344635009766, 58.799026012420654, 59.26547598838806, 59.73192596435547, 60.19305062294006, 60.65417528152466, 61.14870810508728, 61.6432409286499, 62.146809577941895, 62.65037822723389, 63.09723997116089, 63.54410171508789, 64.02933883666992, 64.51457595825195, 65.01373553276062, 65.51289510726929, 65.97123622894287, 66.42957735061646, 66.9205687046051, 67.41156005859375, 67.87668561935425, 68.34181118011475, 68.79954791069031, 69.25728464126587, 69.74220776557922, 70.22713088989258, 70.70934295654297, 71.19155502319336, 71.69420075416565, 72.19684648513794, 72.67734837532043, 73.15785026550293, 73.62554931640625, 74.09324836730957, 74.58465075492859, 75.07605314254761, 75.58350563049316, 76.09095811843872, 76.53723502159119, 76.98351192474365, 77.44967317581177, 77.91583442687988, 78.40876126289368, 78.90168809890747, 79.37793254852295, 79.85417699813843, 80.33726930618286, 80.8203616142273, 81.29391312599182, 81.76746463775635, 82.24172019958496, 82.71597576141357, 83.22093915939331, 83.72590255737305, 84.20666193962097, 84.6874213218689, 85.14847588539124, 85.60953044891357, 86.11063885688782, 86.61174726486206, 87.10157990455627, 87.59141254425049, 88.06967830657959, 88.54794406890869, 89.05709171295166, 89.56623935699463, 90.0197741985321, 90.47330904006958, 90.9447615146637, 91.41621398925781, 91.91407585144043, 92.41193771362305, 92.89884924888611, 93.38576078414917, 93.87043309211731, 94.35510540008545, 94.82242679595947, 95.2897481918335, 95.75324964523315, 96.21675109863281, 96.7140862941742, 97.21142148971558, 98.15675616264343, 99.10209083557129]
[15.366666666666667, 15.366666666666667, 17.9, 17.9, 17.583333333333332, 17.583333333333332, 16.583333333333332, 16.583333333333332, 19.833333333333332, 19.833333333333332, 22.016666666666666, 22.016666666666666, 25.1, 25.1, 30.966666666666665, 30.966666666666665, 39.11666666666667, 39.11666666666667, 45.43333333333333, 45.43333333333333, 53.46666666666667, 53.46666666666667, 58.7, 58.7, 64.91666666666667, 64.91666666666667, 65.3, 65.3, 68.51666666666667, 68.51666666666667, 71.06666666666666, 71.06666666666666, 72.01666666666667, 72.01666666666667, 73.15, 73.15, 74.23333333333333, 74.23333333333333, 74.85, 74.85, 74.81666666666666, 74.81666666666666, 75.15, 75.15, 75.76666666666667, 75.76666666666667, 76.06666666666666, 76.06666666666666, 76.15, 76.15, 76.31666666666666, 76.31666666666666, 76.33333333333333, 76.33333333333333, 76.75, 76.75, 76.71666666666667, 76.71666666666667, 76.85, 76.85, 76.96666666666667, 76.96666666666667, 77.3, 77.3, 77.26666666666667, 77.26666666666667, 77.15, 77.15, 77.01666666666667, 77.01666666666667, 77.18333333333334, 77.18333333333334, 77.23333333333333, 77.23333333333333, 77.4, 77.4, 77.71666666666667, 77.71666666666667, 77.71666666666667, 77.71666666666667, 77.76666666666667, 77.76666666666667, 77.85, 77.85, 77.86666666666666, 77.86666666666666, 77.83333333333333, 77.83333333333333, 77.83333333333333, 77.83333333333333, 77.75, 77.75, 77.78333333333333, 77.78333333333333, 77.83333333333333, 77.83333333333333, 77.86666666666666, 77.86666666666666, 77.58333333333333, 77.58333333333333, 79.31666666666666, 79.31666666666666, 79.51666666666667, 79.51666666666667, 79.38333333333334, 79.38333333333334, 79.45, 79.45, 79.51666666666667, 79.51666666666667, 79.46666666666667, 79.46666666666667, 79.51666666666667, 79.51666666666667, 79.53333333333333, 79.53333333333333, 79.51666666666667, 79.51666666666667, 79.55, 79.55, 79.56666666666666, 79.56666666666666, 79.56666666666666, 79.56666666666666, 79.55, 79.55, 79.56666666666666, 79.56666666666666, 79.5, 79.5, 79.48333333333333, 79.48333333333333, 79.4, 79.4, 79.46666666666667, 79.46666666666667, 79.51666666666667, 79.51666666666667, 79.51666666666667, 79.51666666666667, 79.61666666666666, 79.61666666666666, 79.45, 79.45, 79.45, 79.45, 79.43333333333334, 79.43333333333334, 79.58333333333333, 79.58333333333333, 79.58333333333333, 79.58333333333333, 79.61666666666666, 79.61666666666666, 79.56666666666666, 79.56666666666666, 79.6, 79.6, 79.61666666666666, 79.61666666666666, 79.63333333333334, 79.63333333333334, 79.65, 79.65, 79.61666666666666, 79.61666666666666, 79.66666666666667, 79.66666666666667, 79.61666666666666, 79.61666666666666, 79.65, 79.65, 79.65, 79.65, 79.58333333333333, 79.58333333333333, 79.53333333333333, 79.53333333333333, 79.56666666666666, 79.56666666666666, 79.55, 79.55, 79.61666666666666, 79.61666666666666, 79.56666666666666, 79.56666666666666, 79.65, 79.65, 79.58333333333333, 79.58333333333333, 79.53333333333333, 79.53333333333333, 79.53333333333333, 79.53333333333333, 79.66666666666667, 79.66666666666667, 79.58333333333333, 79.58333333333333, 79.6, 79.6, 81.05, 81.05]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.285, Test loss: 2.295, Test accuracy: 19.35
Round   1, Train loss: 2.226, Test loss: 2.258, Test accuracy: 29.23
Round   2, Train loss: 2.096, Test loss: 2.198, Test accuracy: 31.33
Round   3, Train loss: 1.925, Test loss: 2.132, Test accuracy: 35.83
Round   4, Train loss: 1.798, Test loss: 2.095, Test accuracy: 34.98
Round   5, Train loss: 1.705, Test loss: 2.033, Test accuracy: 41.97
Round   6, Train loss: 1.752, Test loss: 1.981, Test accuracy: 50.13
Round   7, Train loss: 1.776, Test loss: 1.926, Test accuracy: 56.52
Round   8, Train loss: 1.630, Test loss: 1.871, Test accuracy: 62.42
Round   9, Train loss: 1.684, Test loss: 1.858, Test accuracy: 62.12
Round  10, Train loss: 1.604, Test loss: 1.824, Test accuracy: 66.52
Round  11, Train loss: 1.760, Test loss: 1.807, Test accuracy: 67.32
Round  12, Train loss: 1.621, Test loss: 1.788, Test accuracy: 69.20
Round  13, Train loss: 1.609, Test loss: 1.776, Test accuracy: 70.00
Round  14, Train loss: 1.624, Test loss: 1.758, Test accuracy: 72.10
Round  15, Train loss: 1.765, Test loss: 1.750, Test accuracy: 73.15
Round  16, Train loss: 1.557, Test loss: 1.752, Test accuracy: 72.65
Round  17, Train loss: 1.701, Test loss: 1.740, Test accuracy: 73.43
Round  18, Train loss: 1.753, Test loss: 1.744, Test accuracy: 72.52
Round  19, Train loss: 1.653, Test loss: 1.740, Test accuracy: 73.22
Round  20, Train loss: 1.703, Test loss: 1.734, Test accuracy: 73.58
Round  21, Train loss: 1.595, Test loss: 1.727, Test accuracy: 74.40
Round  22, Train loss: 1.646, Test loss: 1.727, Test accuracy: 74.37
Round  23, Train loss: 1.597, Test loss: 1.706, Test accuracy: 76.52
Round  24, Train loss: 1.546, Test loss: 1.701, Test accuracy: 77.17
Round  25, Train loss: 1.647, Test loss: 1.699, Test accuracy: 77.28
Round  26, Train loss: 1.747, Test loss: 1.695, Test accuracy: 77.63
Round  27, Train loss: 1.543, Test loss: 1.693, Test accuracy: 77.67
Round  28, Train loss: 1.644, Test loss: 1.695, Test accuracy: 77.30
Round  29, Train loss: 1.592, Test loss: 1.688, Test accuracy: 78.00
Round  30, Train loss: 1.566, Test loss: 1.678, Test accuracy: 79.13
Round  31, Train loss: 1.639, Test loss: 1.677, Test accuracy: 79.23
Round  32, Train loss: 1.699, Test loss: 1.675, Test accuracy: 79.52
Round  33, Train loss: 1.638, Test loss: 1.675, Test accuracy: 79.35
Round  34, Train loss: 1.692, Test loss: 1.675, Test accuracy: 79.42
Round  35, Train loss: 1.693, Test loss: 1.674, Test accuracy: 79.52
Round  36, Train loss: 1.541, Test loss: 1.674, Test accuracy: 79.42
Round  37, Train loss: 1.546, Test loss: 1.670, Test accuracy: 79.82
Round  38, Train loss: 1.753, Test loss: 1.668, Test accuracy: 79.88
Round  39, Train loss: 1.638, Test loss: 1.667, Test accuracy: 79.92
Round  40, Train loss: 1.750, Test loss: 1.668, Test accuracy: 79.95
Round  41, Train loss: 1.744, Test loss: 1.667, Test accuracy: 80.13
Round  42, Train loss: 1.640, Test loss: 1.665, Test accuracy: 80.25
Round  43, Train loss: 1.745, Test loss: 1.664, Test accuracy: 80.33
Round  44, Train loss: 1.590, Test loss: 1.663, Test accuracy: 80.50
Round  45, Train loss: 1.475, Test loss: 1.662, Test accuracy: 80.62
Round  46, Train loss: 1.581, Test loss: 1.661, Test accuracy: 80.53
Round  47, Train loss: 1.698, Test loss: 1.661, Test accuracy: 80.53
Round  48, Train loss: 1.693, Test loss: 1.660, Test accuracy: 80.52
Round  49, Train loss: 1.752, Test loss: 1.660, Test accuracy: 80.40
Round  50, Train loss: 1.691, Test loss: 1.660, Test accuracy: 80.53
Round  51, Train loss: 1.586, Test loss: 1.660, Test accuracy: 80.55
Round  52, Train loss: 1.748, Test loss: 1.659, Test accuracy: 80.70
Round  53, Train loss: 1.532, Test loss: 1.658, Test accuracy: 80.70
Round  54, Train loss: 1.635, Test loss: 1.658, Test accuracy: 80.78
Round  55, Train loss: 1.692, Test loss: 1.658, Test accuracy: 80.78
Round  56, Train loss: 1.582, Test loss: 1.658, Test accuracy: 80.78
Round  57, Train loss: 1.691, Test loss: 1.658, Test accuracy: 80.68
Round  58, Train loss: 1.635, Test loss: 1.658, Test accuracy: 80.68
Round  59, Train loss: 1.639, Test loss: 1.657, Test accuracy: 80.63
Round  60, Train loss: 1.578, Test loss: 1.657, Test accuracy: 80.77
Round  61, Train loss: 1.579, Test loss: 1.657, Test accuracy: 80.77
Round  62, Train loss: 1.639, Test loss: 1.656, Test accuracy: 80.85
Round  63, Train loss: 1.529, Test loss: 1.656, Test accuracy: 80.77
Round  64, Train loss: 1.633, Test loss: 1.656, Test accuracy: 80.90
Round  65, Train loss: 1.468, Test loss: 1.655, Test accuracy: 80.93
Round  66, Train loss: 1.580, Test loss: 1.655, Test accuracy: 80.90
Round  67, Train loss: 1.582, Test loss: 1.655, Test accuracy: 81.03
Round  68, Train loss: 1.636, Test loss: 1.655, Test accuracy: 81.03
Round  69, Train loss: 1.582, Test loss: 1.655, Test accuracy: 80.93
Round  70, Train loss: 1.581, Test loss: 1.654, Test accuracy: 81.00
Round  71, Train loss: 1.582, Test loss: 1.654, Test accuracy: 81.02
Round  72, Train loss: 1.797, Test loss: 1.654, Test accuracy: 80.93
Round  73, Train loss: 1.580, Test loss: 1.654, Test accuracy: 81.08
Round  74, Train loss: 1.577, Test loss: 1.654, Test accuracy: 81.05
Round  75, Train loss: 1.796, Test loss: 1.654, Test accuracy: 80.97
Round  76, Train loss: 1.631, Test loss: 1.653, Test accuracy: 81.02
Round  77, Train loss: 1.686, Test loss: 1.653, Test accuracy: 81.03
Round  78, Train loss: 1.580, Test loss: 1.653, Test accuracy: 81.05
Round  79, Train loss: 1.577, Test loss: 1.653, Test accuracy: 81.13
Round  80, Train loss: 1.522, Test loss: 1.652, Test accuracy: 81.15
Round  81, Train loss: 1.580, Test loss: 1.652, Test accuracy: 81.17
Round  82, Train loss: 1.526, Test loss: 1.653, Test accuracy: 81.12
Round  83, Train loss: 1.633, Test loss: 1.653, Test accuracy: 81.22
Round  84, Train loss: 1.634, Test loss: 1.652, Test accuracy: 81.18
Round  85, Train loss: 1.688, Test loss: 1.652, Test accuracy: 81.20
Round  86, Train loss: 1.470, Test loss: 1.652, Test accuracy: 81.20
Round  87, Train loss: 1.466, Test loss: 1.652, Test accuracy: 81.13
Round  88, Train loss: 1.577, Test loss: 1.652, Test accuracy: 81.08
Round  89, Train loss: 1.469, Test loss: 1.652, Test accuracy: 81.12
Round  90, Train loss: 1.687, Test loss: 1.652, Test accuracy: 81.15
Round  91, Train loss: 1.630, Test loss: 1.652, Test accuracy: 81.22
Round  92, Train loss: 1.576, Test loss: 1.651, Test accuracy: 81.20
Round  93, Train loss: 1.524, Test loss: 1.652, Test accuracy: 81.20
Round  94, Train loss: 1.742, Test loss: 1.652, Test accuracy: 81.18
Round  95, Train loss: 1.637, Test loss: 1.652, Test accuracy: 81.23
Round  96, Train loss: 1.523, Test loss: 1.651, Test accuracy: 81.23
Round  97, Train loss: 1.632, Test loss: 1.651, Test accuracy: 81.15
Round  98, Train loss: 1.689, Test loss: 1.652, Test accuracy: 81.20/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.632, Test loss: 1.651, Test accuracy: 81.17
Final Round, Train loss: 1.617, Test loss: 1.652, Test accuracy: 81.10
Average accuracy final 10 rounds: 81.19333333333333 

641.3769967556
[0.6099379062652588, 1.2198758125305176, 1.7264468669891357, 2.233017921447754, 2.6961758136749268, 3.1593337059020996, 3.650285243988037, 4.141236782073975, 4.644367694854736, 5.147498607635498, 5.626692295074463, 6.105885982513428, 6.560015678405762, 7.014145374298096, 7.495811939239502, 7.977478504180908, 8.486295938491821, 8.995113372802734, 9.494234561920166, 9.993355751037598, 10.464910745620728, 10.936465740203857, 11.415733337402344, 11.89500093460083, 12.399023532867432, 12.903046131134033, 13.403043985366821, 13.90304183959961, 14.390947818756104, 14.878853797912598, 15.366678237915039, 15.85450267791748, 16.32426118850708, 16.79401969909668, 17.265351057052612, 17.736682415008545, 18.252469062805176, 18.768255710601807, 19.284146070480347, 19.800036430358887, 20.302118062973022, 20.804199695587158, 21.294676780700684, 21.78515386581421, 22.268537998199463, 22.751922130584717, 23.26151418685913, 23.771106243133545, 24.266575813293457, 24.76204538345337, 25.237038135528564, 25.71203088760376, 26.216742277145386, 26.72145366668701, 27.21392321586609, 27.706392765045166, 28.18717098236084, 28.667949199676514, 29.15585732460022, 29.643765449523926, 30.139069080352783, 30.63437271118164, 31.143450498580933, 31.652528285980225, 32.131202697753906, 32.60987710952759, 33.10165786743164, 33.59343862533569, 34.07657217979431, 34.55970573425293, 35.07314610481262, 35.586586475372314, 36.093860387802124, 36.601134300231934, 37.08787202835083, 37.57460975646973, 38.0695321559906, 38.564454555511475, 39.060333490371704, 39.556212425231934, 40.049434423446655, 40.54265642166138, 41.01727342605591, 41.49189043045044, 41.968382120132446, 42.44487380981445, 42.94806218147278, 43.4512505531311, 43.943044900894165, 44.43483924865723, 44.899771213531494, 45.36470317840576, 45.852081298828125, 46.33945941925049, 46.84108352661133, 47.34270763397217, 47.83547925949097, 48.328250885009766, 48.81958603858948, 49.31092119216919, 49.78297662734985, 50.25503206253052, 50.744030475616455, 51.23302888870239, 51.721083879470825, 52.20913887023926, 52.70101881027222, 53.192898750305176, 53.66573143005371, 54.138564109802246, 54.620649337768555, 55.10273456573486, 55.573014974594116, 56.04329538345337, 56.4963481426239, 56.949400901794434, 57.454468965530396, 57.95953702926636, 58.45498466491699, 58.95043230056763, 59.41580033302307, 59.881168365478516, 60.35532760620117, 60.82948684692383, 61.313496112823486, 61.797505378723145, 62.30981683731079, 62.82212829589844, 63.32353377342224, 63.824939250946045, 64.2668182849884, 64.70869731903076, 65.19068503379822, 65.67267274856567, 66.15093541145325, 66.62919807434082, 67.11195182800293, 67.59470558166504, 68.08327746391296, 68.57184934616089, 69.05626440048218, 69.54067945480347, 70.00354027748108, 70.46640110015869, 70.93628430366516, 71.40616750717163, 71.88763117790222, 72.36909484863281, 72.86764860153198, 73.36620235443115, 73.84478521347046, 74.32336807250977, 74.80105638504028, 75.2787446975708, 75.75980997085571, 76.24087524414062, 76.73402619361877, 77.22717714309692, 77.70182085037231, 78.1764645576477, 78.64526224136353, 79.11405992507935, 79.59245371818542, 80.0708475112915, 80.5452048778534, 81.01956224441528, 81.49759244918823, 81.97562265396118, 82.44494462013245, 82.91426658630371, 83.41346430778503, 83.91266202926636, 84.40118861198425, 84.88971519470215, 85.35605049133301, 85.82238578796387, 86.30711317062378, 86.79184055328369, 87.28454637527466, 87.77725219726562, 88.23574161529541, 88.6942310333252, 89.15792512893677, 89.62161922454834, 90.08463764190674, 90.54765605926514, 91.03525710105896, 91.52285814285278, 92.00368666648865, 92.48451519012451, 92.94765877723694, 93.41080236434937, 93.8941581249237, 94.37751388549805, 94.88263010978699, 95.38774633407593, 95.87760925292969, 96.36747217178345, 96.85855078697205, 97.34962940216064, 98.28930568695068, 99.22898197174072]
[19.35, 19.35, 29.233333333333334, 29.233333333333334, 31.333333333333332, 31.333333333333332, 35.833333333333336, 35.833333333333336, 34.983333333333334, 34.983333333333334, 41.96666666666667, 41.96666666666667, 50.13333333333333, 50.13333333333333, 56.516666666666666, 56.516666666666666, 62.416666666666664, 62.416666666666664, 62.11666666666667, 62.11666666666667, 66.51666666666667, 66.51666666666667, 67.31666666666666, 67.31666666666666, 69.2, 69.2, 70.0, 70.0, 72.1, 72.1, 73.15, 73.15, 72.65, 72.65, 73.43333333333334, 73.43333333333334, 72.51666666666667, 72.51666666666667, 73.21666666666667, 73.21666666666667, 73.58333333333333, 73.58333333333333, 74.4, 74.4, 74.36666666666666, 74.36666666666666, 76.51666666666667, 76.51666666666667, 77.16666666666667, 77.16666666666667, 77.28333333333333, 77.28333333333333, 77.63333333333334, 77.63333333333334, 77.66666666666667, 77.66666666666667, 77.3, 77.3, 78.0, 78.0, 79.13333333333334, 79.13333333333334, 79.23333333333333, 79.23333333333333, 79.51666666666667, 79.51666666666667, 79.35, 79.35, 79.41666666666667, 79.41666666666667, 79.51666666666667, 79.51666666666667, 79.41666666666667, 79.41666666666667, 79.81666666666666, 79.81666666666666, 79.88333333333334, 79.88333333333334, 79.91666666666667, 79.91666666666667, 79.95, 79.95, 80.13333333333334, 80.13333333333334, 80.25, 80.25, 80.33333333333333, 80.33333333333333, 80.5, 80.5, 80.61666666666666, 80.61666666666666, 80.53333333333333, 80.53333333333333, 80.53333333333333, 80.53333333333333, 80.51666666666667, 80.51666666666667, 80.4, 80.4, 80.53333333333333, 80.53333333333333, 80.55, 80.55, 80.7, 80.7, 80.7, 80.7, 80.78333333333333, 80.78333333333333, 80.78333333333333, 80.78333333333333, 80.78333333333333, 80.78333333333333, 80.68333333333334, 80.68333333333334, 80.68333333333334, 80.68333333333334, 80.63333333333334, 80.63333333333334, 80.76666666666667, 80.76666666666667, 80.76666666666667, 80.76666666666667, 80.85, 80.85, 80.76666666666667, 80.76666666666667, 80.9, 80.9, 80.93333333333334, 80.93333333333334, 80.9, 80.9, 81.03333333333333, 81.03333333333333, 81.03333333333333, 81.03333333333333, 80.93333333333334, 80.93333333333334, 81.0, 81.0, 81.01666666666667, 81.01666666666667, 80.93333333333334, 80.93333333333334, 81.08333333333333, 81.08333333333333, 81.05, 81.05, 80.96666666666667, 80.96666666666667, 81.01666666666667, 81.01666666666667, 81.03333333333333, 81.03333333333333, 81.05, 81.05, 81.13333333333334, 81.13333333333334, 81.15, 81.15, 81.16666666666667, 81.16666666666667, 81.11666666666666, 81.11666666666666, 81.21666666666667, 81.21666666666667, 81.18333333333334, 81.18333333333334, 81.2, 81.2, 81.2, 81.2, 81.13333333333334, 81.13333333333334, 81.08333333333333, 81.08333333333333, 81.11666666666666, 81.11666666666666, 81.15, 81.15, 81.21666666666667, 81.21666666666667, 81.2, 81.2, 81.2, 81.2, 81.18333333333334, 81.18333333333334, 81.23333333333333, 81.23333333333333, 81.23333333333333, 81.23333333333333, 81.15, 81.15, 81.2, 81.2, 81.16666666666667, 81.16666666666667, 81.1, 81.1]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.287, Test loss: 2.296, Test accuracy: 24.20
Round   1, Train loss: 2.235, Test loss: 2.274, Test accuracy: 32.85
Round   2, Train loss: 2.094, Test loss: 2.196, Test accuracy: 42.25
Round   3, Train loss: 1.866, Test loss: 2.124, Test accuracy: 37.20
Round   4, Train loss: 1.853, Test loss: 2.080, Test accuracy: 43.12
Round   5, Train loss: 1.865, Test loss: 1.954, Test accuracy: 61.38
Round   6, Train loss: 1.729, Test loss: 1.871, Test accuracy: 66.17
Round   7, Train loss: 1.832, Test loss: 1.814, Test accuracy: 71.63
Round   8, Train loss: 1.645, Test loss: 1.800, Test accuracy: 70.67
Round   9, Train loss: 1.640, Test loss: 1.764, Test accuracy: 72.23
Round  10, Train loss: 1.587, Test loss: 1.725, Test accuracy: 78.73
Round  11, Train loss: 1.587, Test loss: 1.679, Test accuracy: 82.98
Round  12, Train loss: 1.576, Test loss: 1.646, Test accuracy: 86.05
Round  13, Train loss: 1.561, Test loss: 1.623, Test accuracy: 87.78
Round  14, Train loss: 1.597, Test loss: 1.614, Test accuracy: 87.92
Round  15, Train loss: 1.483, Test loss: 1.605, Test accuracy: 88.60
Round  16, Train loss: 1.527, Test loss: 1.602, Test accuracy: 88.87
Round  17, Train loss: 1.479, Test loss: 1.598, Test accuracy: 88.90
Round  18, Train loss: 1.521, Test loss: 1.585, Test accuracy: 89.43
Round  19, Train loss: 1.540, Test loss: 1.571, Test accuracy: 90.30
Round  20, Train loss: 1.480, Test loss: 1.560, Test accuracy: 91.62
Round  21, Train loss: 1.505, Test loss: 1.543, Test accuracy: 93.43
Round  22, Train loss: 1.474, Test loss: 1.535, Test accuracy: 93.90
Round  23, Train loss: 1.484, Test loss: 1.525, Test accuracy: 95.03
Round  24, Train loss: 1.477, Test loss: 1.522, Test accuracy: 95.15
Round  25, Train loss: 1.469, Test loss: 1.521, Test accuracy: 95.22
Round  26, Train loss: 1.468, Test loss: 1.520, Test accuracy: 95.32
Round  27, Train loss: 1.467, Test loss: 1.519, Test accuracy: 95.30
Round  28, Train loss: 1.467, Test loss: 1.518, Test accuracy: 95.45
Round  29, Train loss: 1.466, Test loss: 1.518, Test accuracy: 95.43
Round  30, Train loss: 1.470, Test loss: 1.516, Test accuracy: 95.53
Round  31, Train loss: 1.467, Test loss: 1.515, Test accuracy: 95.55
Round  32, Train loss: 1.474, Test loss: 1.514, Test accuracy: 95.60
Round  33, Train loss: 1.466, Test loss: 1.514, Test accuracy: 95.63
Round  34, Train loss: 1.469, Test loss: 1.513, Test accuracy: 95.72
Round  35, Train loss: 1.467, Test loss: 1.513, Test accuracy: 95.68
Round  36, Train loss: 1.468, Test loss: 1.512, Test accuracy: 95.65
Round  37, Train loss: 1.467, Test loss: 1.512, Test accuracy: 95.63
Round  38, Train loss: 1.468, Test loss: 1.511, Test accuracy: 95.65
Round  39, Train loss: 1.468, Test loss: 1.511, Test accuracy: 95.62
Round  40, Train loss: 1.467, Test loss: 1.511, Test accuracy: 95.62
Round  41, Train loss: 1.468, Test loss: 1.511, Test accuracy: 95.62
Round  42, Train loss: 1.468, Test loss: 1.511, Test accuracy: 95.63
Round  43, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.63
Round  44, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.63
Round  45, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.65
Round  46, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.63
Round  47, Train loss: 1.467, Test loss: 1.510, Test accuracy: 95.62
Round  48, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.60
Round  49, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.63
Round  50, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.67
Round  51, Train loss: 1.465, Test loss: 1.510, Test accuracy: 95.65
Round  52, Train loss: 1.464, Test loss: 1.510, Test accuracy: 95.63
Round  53, Train loss: 1.467, Test loss: 1.510, Test accuracy: 95.62
Round  54, Train loss: 1.466, Test loss: 1.510, Test accuracy: 95.63
Round  55, Train loss: 1.467, Test loss: 1.509, Test accuracy: 95.58
Round  56, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.62
Round  57, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.62
Round  58, Train loss: 1.463, Test loss: 1.509, Test accuracy: 95.62
Round  59, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.65
Round  60, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.62
Round  61, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.62
Round  62, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.65
Round  63, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.63
Round  64, Train loss: 1.463, Test loss: 1.509, Test accuracy: 95.67
Round  65, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.65
Round  66, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.63
Round  67, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.62
Round  68, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.62
Round  69, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.60
Round  70, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.62
Round  71, Train loss: 1.466, Test loss: 1.509, Test accuracy: 95.62
Round  72, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.63
Round  73, Train loss: 1.465, Test loss: 1.509, Test accuracy: 95.63
Round  74, Train loss: 1.464, Test loss: 1.509, Test accuracy: 95.63
Round  75, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.63
Round  76, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Round  77, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.65
Round  78, Train loss: 1.466, Test loss: 1.508, Test accuracy: 95.70
Round  79, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.70
Round  80, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.72
Round  81, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.68
Round  82, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.70
Round  83, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.68
Round  84, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.70
Round  85, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.67
Round  86, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Round  87, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.65
Round  88, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.67
Round  89, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.67
Round  90, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Round  91, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Round  92, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.67
Round  93, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.67
Round  94, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Round  95, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Round  96, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.67
Round  97, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Round  98, Train loss: 1.465, Test loss: 1.508, Test accuracy: 95.67
Round  99, Train loss: 1.463, Test loss: 1.508, Test accuracy: 95.67/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.464, Test loss: 1.508, Test accuracy: 95.67
Average accuracy final 10 rounds: 95.66666666666664 

637.6352026462555
[0.6666390895843506, 1.3332781791687012, 1.8083393573760986, 2.283400535583496, 2.768366575241089, 3.2533326148986816, 3.7564942836761475, 4.259655952453613, 4.770135402679443, 5.280614852905273, 5.783591032028198, 6.286567211151123, 6.811530113220215, 7.336493015289307, 7.801371097564697, 8.266249179840088, 8.7466139793396, 9.226978778839111, 9.71995234489441, 10.212925910949707, 10.714040517807007, 11.215155124664307, 11.699200630187988, 12.18324613571167, 12.662367820739746, 13.141489505767822, 13.632140636444092, 14.122791767120361, 14.615548849105835, 15.108305931091309, 15.598615407943726, 16.088924884796143, 16.57109832763672, 17.053271770477295, 17.54848027229309, 18.043688774108887, 18.536526203155518, 19.02936363220215, 19.51548194885254, 20.00160026550293, 20.472728490829468, 20.943856716156006, 21.418978929519653, 21.8941011428833, 22.366512060165405, 22.83892297744751, 23.341859340667725, 23.84479570388794, 24.323336362838745, 24.80187702178955, 25.29399609565735, 25.786115169525146, 26.28268074989319, 26.77924633026123, 27.263484716415405, 27.74772310256958, 28.266166925430298, 28.784610748291016, 29.2866153717041, 29.788619995117188, 30.27626919746399, 30.76391839981079, 31.236193418502808, 31.708468437194824, 32.21173119544983, 32.714993953704834, 33.218682050704956, 33.72237014770508, 34.2269492149353, 34.73152828216553, 35.18865990638733, 35.64579153060913, 36.14861345291138, 36.65143537521362, 37.159141302108765, 37.666847229003906, 38.157869815826416, 38.648892402648926, 39.130326986312866, 39.61176156997681, 40.089163064956665, 40.56656455993652, 41.036827087402344, 41.507089614868164, 41.99403667449951, 42.48098373413086, 42.976956844329834, 43.47292995452881, 43.96035361289978, 44.44777727127075, 44.92792749404907, 45.40807771682739, 45.88842177391052, 46.36876583099365, 46.85024404525757, 47.331722259521484, 47.84854221343994, 48.3653621673584, 48.85480189323425, 49.34424161911011, 49.82583022117615, 50.30741882324219, 50.775593280792236, 51.243767738342285, 51.750858545303345, 52.257949352264404, 52.73906993865967, 53.22019052505493, 53.712939500808716, 54.2056884765625, 54.696794509887695, 55.18790054321289, 55.668922424316406, 56.14994430541992, 56.651395082473755, 57.15284585952759, 57.64275121688843, 58.13265657424927, 58.63399648666382, 59.13533639907837, 59.63641119003296, 60.13748598098755, 60.609410762786865, 61.08133554458618, 61.568650007247925, 62.05596446990967, 62.53823113441467, 63.02049779891968, 63.49717473983765, 63.973851680755615, 64.45880770683289, 64.94376373291016, 65.43551540374756, 65.92726707458496, 66.41534638404846, 66.90342569351196, 67.39726495742798, 67.891104221344, 68.36443448066711, 68.83776473999023, 69.32983589172363, 69.82190704345703, 70.32579827308655, 70.82968950271606, 71.31314539909363, 71.79660129547119, 72.28863859176636, 72.78067588806152, 73.25842237472534, 73.73616886138916, 74.20859050750732, 74.68101215362549, 75.18472814559937, 75.68844413757324, 76.19684410095215, 76.70524406433105, 77.19258093833923, 77.67991781234741, 78.1747887134552, 78.66965961456299, 79.12531590461731, 79.58097219467163, 80.0532283782959, 80.52548456192017, 81.02903652191162, 81.53258848190308, 82.00390124320984, 82.4752140045166, 82.94906282424927, 83.42291164398193, 83.88256049156189, 84.34220933914185, 84.82680702209473, 85.31140470504761, 85.7990791797638, 86.28675365447998, 86.76009011268616, 87.23342657089233, 87.70862865447998, 88.18383073806763, 88.65316271781921, 89.1224946975708, 89.5994553565979, 90.076416015625, 90.55597233772278, 91.03552865982056, 91.50310397148132, 91.97067928314209, 92.4675076007843, 92.96433591842651, 93.44619011878967, 93.92804431915283, 94.40747952461243, 94.88691473007202, 95.37347984313965, 95.86004495620728, 96.34858107566833, 96.8371171951294, 97.31137418746948, 97.78563117980957, 98.72414493560791, 99.66265869140625]
[24.2, 24.2, 32.85, 32.85, 42.25, 42.25, 37.2, 37.2, 43.11666666666667, 43.11666666666667, 61.38333333333333, 61.38333333333333, 66.16666666666667, 66.16666666666667, 71.63333333333334, 71.63333333333334, 70.66666666666667, 70.66666666666667, 72.23333333333333, 72.23333333333333, 78.73333333333333, 78.73333333333333, 82.98333333333333, 82.98333333333333, 86.05, 86.05, 87.78333333333333, 87.78333333333333, 87.91666666666667, 87.91666666666667, 88.6, 88.6, 88.86666666666666, 88.86666666666666, 88.9, 88.9, 89.43333333333334, 89.43333333333334, 90.3, 90.3, 91.61666666666666, 91.61666666666666, 93.43333333333334, 93.43333333333334, 93.9, 93.9, 95.03333333333333, 95.03333333333333, 95.15, 95.15, 95.21666666666667, 95.21666666666667, 95.31666666666666, 95.31666666666666, 95.3, 95.3, 95.45, 95.45, 95.43333333333334, 95.43333333333334, 95.53333333333333, 95.53333333333333, 95.55, 95.55, 95.6, 95.6, 95.63333333333334, 95.63333333333334, 95.71666666666667, 95.71666666666667, 95.68333333333334, 95.68333333333334, 95.65, 95.65, 95.63333333333334, 95.63333333333334, 95.65, 95.65, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.65, 95.65, 95.63333333333334, 95.63333333333334, 95.61666666666666, 95.61666666666666, 95.6, 95.6, 95.63333333333334, 95.63333333333334, 95.66666666666667, 95.66666666666667, 95.65, 95.65, 95.63333333333334, 95.63333333333334, 95.61666666666666, 95.61666666666666, 95.63333333333334, 95.63333333333334, 95.58333333333333, 95.58333333333333, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.65, 95.65, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.65, 95.65, 95.63333333333334, 95.63333333333334, 95.66666666666667, 95.66666666666667, 95.65, 95.65, 95.63333333333334, 95.63333333333334, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.6, 95.6, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.61666666666666, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.63333333333334, 95.66666666666667, 95.66666666666667, 95.65, 95.65, 95.7, 95.7, 95.7, 95.7, 95.71666666666667, 95.71666666666667, 95.68333333333334, 95.68333333333334, 95.7, 95.7, 95.68333333333334, 95.68333333333334, 95.7, 95.7, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.65, 95.65, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667, 95.66666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.578, Test loss: 2.267, Test accuracy: 25.58
Round   1, Train loss: 1.385, Test loss: 2.177, Test accuracy: 39.47
Round   2, Train loss: 1.296, Test loss: 2.079, Test accuracy: 52.18
Round   3, Train loss: 1.397, Test loss: 2.001, Test accuracy: 64.65
Round   4, Train loss: 1.355, Test loss: 1.936, Test accuracy: 70.42
Round   5, Train loss: 1.222, Test loss: 1.882, Test accuracy: 74.57
Round   6, Train loss: 1.274, Test loss: 1.846, Test accuracy: 75.80
Round   7, Train loss: 1.345, Test loss: 1.804, Test accuracy: 77.85
Round   8, Train loss: 1.292, Test loss: 1.767, Test accuracy: 79.65
Round   9, Train loss: 1.196, Test loss: 1.734, Test accuracy: 81.15
Round  10, Train loss: 1.233, Test loss: 1.718, Test accuracy: 81.48
Round  11, Train loss: 1.232, Test loss: 1.706, Test accuracy: 81.68
Round  12, Train loss: 1.234, Test loss: 1.695, Test accuracy: 81.80
Round  13, Train loss: 1.190, Test loss: 1.690, Test accuracy: 81.75
Round  14, Train loss: 1.260, Test loss: 1.678, Test accuracy: 82.98
Round  15, Train loss: 1.151, Test loss: 1.671, Test accuracy: 83.05
Round  16, Train loss: 1.228, Test loss: 1.664, Test accuracy: 82.92
Round  17, Train loss: 1.185, Test loss: 1.659, Test accuracy: 83.23
Round  18, Train loss: 1.157, Test loss: 1.654, Test accuracy: 84.35
Round  19, Train loss: 1.190, Test loss: 1.650, Test accuracy: 84.25
Round  20, Train loss: 1.186, Test loss: 1.647, Test accuracy: 84.37
Round  21, Train loss: 1.171, Test loss: 1.639, Test accuracy: 85.00
Round  22, Train loss: 1.172, Test loss: 1.638, Test accuracy: 85.05
Round  23, Train loss: 1.145, Test loss: 1.636, Test accuracy: 85.10
Round  24, Train loss: 1.184, Test loss: 1.637, Test accuracy: 84.85
Round  25, Train loss: 1.197, Test loss: 1.628, Test accuracy: 86.18
Round  26, Train loss: 1.108, Test loss: 1.628, Test accuracy: 85.85
Round  27, Train loss: 1.146, Test loss: 1.630, Test accuracy: 85.72
Round  28, Train loss: 1.107, Test loss: 1.623, Test accuracy: 86.38
Round  29, Train loss: 1.186, Test loss: 1.620, Test accuracy: 86.40
Round  30, Train loss: 1.185, Test loss: 1.621, Test accuracy: 86.40
Round  31, Train loss: 1.145, Test loss: 1.620, Test accuracy: 86.33
Round  32, Train loss: 1.144, Test loss: 1.620, Test accuracy: 86.25
Round  33, Train loss: 1.185, Test loss: 1.616, Test accuracy: 86.50
Round  34, Train loss: 1.184, Test loss: 1.617, Test accuracy: 86.50
Round  35, Train loss: 1.226, Test loss: 1.617, Test accuracy: 86.43
Round  36, Train loss: 1.143, Test loss: 1.618, Test accuracy: 86.10
Round  37, Train loss: 1.146, Test loss: 1.618, Test accuracy: 86.08
Round  38, Train loss: 1.100, Test loss: 1.619, Test accuracy: 85.88
Round  39, Train loss: 1.224, Test loss: 1.618, Test accuracy: 85.97
Round  40, Train loss: 1.141, Test loss: 1.619, Test accuracy: 85.75
Round  41, Train loss: 1.184, Test loss: 1.617, Test accuracy: 85.97
Round  42, Train loss: 1.145, Test loss: 1.619, Test accuracy: 85.67
Round  43, Train loss: 1.225, Test loss: 1.620, Test accuracy: 85.58
Round  44, Train loss: 1.183, Test loss: 1.623, Test accuracy: 85.25
Round  45, Train loss: 1.144, Test loss: 1.616, Test accuracy: 86.02
Round  46, Train loss: 1.224, Test loss: 1.616, Test accuracy: 86.00
Round  47, Train loss: 1.100, Test loss: 1.616, Test accuracy: 85.82
Round  48, Train loss: 1.144, Test loss: 1.616, Test accuracy: 85.68
Round  49, Train loss: 1.265, Test loss: 1.617, Test accuracy: 85.60
Round  50, Train loss: 1.140, Test loss: 1.615, Test accuracy: 85.73
Round  51, Train loss: 1.223, Test loss: 1.616, Test accuracy: 85.78
Round  52, Train loss: 1.183, Test loss: 1.619, Test accuracy: 85.47
Round  53, Train loss: 1.142, Test loss: 1.622, Test accuracy: 84.83
Round  54, Train loss: 1.141, Test loss: 1.623, Test accuracy: 84.63
Round  55, Train loss: 1.102, Test loss: 1.621, Test accuracy: 85.03
Round  56, Train loss: 1.103, Test loss: 1.620, Test accuracy: 84.93
Round  57, Train loss: 1.224, Test loss: 1.621, Test accuracy: 84.82
Round  58, Train loss: 1.185, Test loss: 1.621, Test accuracy: 84.93
Round  59, Train loss: 1.143, Test loss: 1.621, Test accuracy: 84.77
Round  60, Train loss: 1.184, Test loss: 1.621, Test accuracy: 84.60
Round  61, Train loss: 1.103, Test loss: 1.621, Test accuracy: 84.50
Round  62, Train loss: 1.184, Test loss: 1.627, Test accuracy: 83.75
Round  63, Train loss: 1.141, Test loss: 1.627, Test accuracy: 83.83
Round  64, Train loss: 1.185, Test loss: 1.628, Test accuracy: 83.95
Round  65, Train loss: 1.183, Test loss: 1.628, Test accuracy: 83.90
Round  66, Train loss: 1.183, Test loss: 1.629, Test accuracy: 83.87
Round  67, Train loss: 1.143, Test loss: 1.628, Test accuracy: 84.15
Round  68, Train loss: 1.141, Test loss: 1.629, Test accuracy: 83.73
Round  69, Train loss: 1.183, Test loss: 1.632, Test accuracy: 83.57
Round  70, Train loss: 1.099, Test loss: 1.631, Test accuracy: 83.68
Round  71, Train loss: 1.224, Test loss: 1.632, Test accuracy: 83.52
Round  72, Train loss: 1.141, Test loss: 1.634, Test accuracy: 83.40
Round  73, Train loss: 1.182, Test loss: 1.636, Test accuracy: 83.15
Round  74, Train loss: 1.265, Test loss: 1.637, Test accuracy: 82.95
Round  75, Train loss: 1.181, Test loss: 1.636, Test accuracy: 82.87
Round  76, Train loss: 1.141, Test loss: 1.642, Test accuracy: 82.35
Round  77, Train loss: 1.141, Test loss: 1.642, Test accuracy: 82.33
Round  78, Train loss: 1.223, Test loss: 1.640, Test accuracy: 82.65
Round  79, Train loss: 1.223, Test loss: 1.641, Test accuracy: 82.60
Round  80, Train loss: 1.142, Test loss: 1.642, Test accuracy: 82.48
Round  81, Train loss: 1.140, Test loss: 1.641, Test accuracy: 82.43
Round  82, Train loss: 1.143, Test loss: 1.644, Test accuracy: 82.20
Round  83, Train loss: 1.224, Test loss: 1.644, Test accuracy: 82.22
Round  84, Train loss: 1.182, Test loss: 1.639, Test accuracy: 82.70
Round  85, Train loss: 1.142, Test loss: 1.641, Test accuracy: 82.53
Round  86, Train loss: 1.182, Test loss: 1.647, Test accuracy: 81.95
Round  87, Train loss: 1.101, Test loss: 1.644, Test accuracy: 82.32
Round  88, Train loss: 1.142, Test loss: 1.644, Test accuracy: 82.18
Round  89, Train loss: 1.141, Test loss: 1.648, Test accuracy: 81.73
Round  90, Train loss: 1.182, Test loss: 1.651, Test accuracy: 81.47
Round  91, Train loss: 1.101, Test loss: 1.646, Test accuracy: 81.83
Round  92, Train loss: 1.183, Test loss: 1.654, Test accuracy: 81.28
Round  93, Train loss: 1.183, Test loss: 1.658, Test accuracy: 80.70
Round  94, Train loss: 1.223, Test loss: 1.665, Test accuracy: 79.75
Round  95, Train loss: 1.182, Test loss: 1.657, Test accuracy: 80.75
Round  96, Train loss: 1.184, Test loss: 1.661, Test accuracy: 80.03
Round  97, Train loss: 1.100, Test loss: 1.657, Test accuracy: 80.67
Round  98, Train loss: 1.223, Test loss: 1.664, Test accuracy: 79.53
Round  99, Train loss: 1.143, Test loss: 1.669, Test accuracy: 78.98
Final Round, Train loss: 1.162, Test loss: 1.673, Test accuracy: 78.87
Average accuracy final 10 rounds: 80.5
743.9132523536682
[]
[25.583333333333332, 39.46666666666667, 52.18333333333333, 64.65, 70.41666666666667, 74.56666666666666, 75.8, 77.85, 79.65, 81.15, 81.48333333333333, 81.68333333333334, 81.8, 81.75, 82.98333333333333, 83.05, 82.91666666666667, 83.23333333333333, 84.35, 84.25, 84.36666666666666, 85.0, 85.05, 85.1, 84.85, 86.18333333333334, 85.85, 85.71666666666667, 86.38333333333334, 86.4, 86.4, 86.33333333333333, 86.25, 86.5, 86.5, 86.43333333333334, 86.1, 86.08333333333333, 85.88333333333334, 85.96666666666667, 85.75, 85.96666666666667, 85.66666666666667, 85.58333333333333, 85.25, 86.01666666666667, 86.0, 85.81666666666666, 85.68333333333334, 85.6, 85.73333333333333, 85.78333333333333, 85.46666666666667, 84.83333333333333, 84.63333333333334, 85.03333333333333, 84.93333333333334, 84.81666666666666, 84.93333333333334, 84.76666666666667, 84.6, 84.5, 83.75, 83.83333333333333, 83.95, 83.9, 83.86666666666666, 84.15, 83.73333333333333, 83.56666666666666, 83.68333333333334, 83.51666666666667, 83.4, 83.15, 82.95, 82.86666666666666, 82.35, 82.33333333333333, 82.65, 82.6, 82.48333333333333, 82.43333333333334, 82.2, 82.21666666666667, 82.7, 82.53333333333333, 81.95, 82.31666666666666, 82.18333333333334, 81.73333333333333, 81.46666666666667, 81.83333333333333, 81.28333333333333, 80.7, 79.75, 80.75, 80.03333333333333, 80.66666666666667, 79.53333333333333, 78.98333333333333, 78.86666666666666]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.284, Test loss: 2.282, Test accuracy: 23.20
Round   1, Train loss: 2.252, Test loss: 2.254, Test accuracy: 27.02
Round   2, Train loss: 2.222, Test loss: 2.230, Test accuracy: 25.20
Round   3, Train loss: 2.087, Test loss: 2.180, Test accuracy: 30.78
Round   4, Train loss: 1.977, Test loss: 2.150, Test accuracy: 31.17
Round   5, Train loss: 2.153, Test loss: 2.200, Test accuracy: 20.65
Round   6, Train loss: 1.845, Test loss: 2.111, Test accuracy: 30.85
Round   7, Train loss: 1.850, Test loss: 2.069, Test accuracy: 36.05
Round   8, Train loss: 1.590, Test loss: 2.068, Test accuracy: 37.27
Round   9, Train loss: 1.824, Test loss: 2.093, Test accuracy: 33.22
Round  10, Train loss: 1.301, Test loss: 2.084, Test accuracy: 37.52
Round  11, Train loss: 1.554, Test loss: 2.132, Test accuracy: 32.45
Round  12, Train loss: 1.127, Test loss: 2.011, Test accuracy: 46.98
Round  13, Train loss: 1.305, Test loss: 1.990, Test accuracy: 48.82
Round  14, Train loss: 0.517, Test loss: 1.928, Test accuracy: 55.30
Round  15, Train loss: 0.911, Test loss: 1.965, Test accuracy: 54.05
Round  16, Train loss: 1.423, Test loss: 2.035, Test accuracy: 50.63
Round  17, Train loss: -0.193, Test loss: 1.933, Test accuracy: 59.22
Round  18, Train loss: -0.281, Test loss: 1.892, Test accuracy: 63.45
Round  19, Train loss: 0.623, Test loss: 1.861, Test accuracy: 65.55
Round  20, Train loss: 0.517, Test loss: 1.888, Test accuracy: 63.33
Round  21, Train loss: 0.275, Test loss: 1.920, Test accuracy: 58.37
Round  22, Train loss: 0.940, Test loss: 2.047, Test accuracy: 50.23
Round  23, Train loss: 0.615, Test loss: 2.134, Test accuracy: 42.35
Round  24, Train loss: -0.826, Test loss: 1.978, Test accuracy: 57.02
Round  25, Train loss: 0.134, Test loss: 1.882, Test accuracy: 66.62
Round  26, Train loss: -0.064, Test loss: 1.904, Test accuracy: 65.87
Round  27, Train loss: -1.218, Test loss: 1.877, Test accuracy: 69.17
Round  28, Train loss: -0.286, Test loss: 1.863, Test accuracy: 70.28
Round  29, Train loss: -1.131, Test loss: 1.754, Test accuracy: 77.37
Round  30, Train loss: -0.337, Test loss: 1.822, Test accuracy: 69.75
Round  31, Train loss: -1.293, Test loss: 1.775, Test accuracy: 73.07
Round  32, Train loss: -1.387, Test loss: 1.768, Test accuracy: 73.75
Round  33, Train loss: -1.229, Test loss: 1.755, Test accuracy: 74.85
Round  34, Train loss: -0.021, Test loss: 1.858, Test accuracy: 65.53
Round  35, Train loss: -1.966, Test loss: 1.775, Test accuracy: 73.43
Round  36, Train loss: -1.998, Test loss: 1.724, Test accuracy: 76.77
Round  37, Train loss: -1.437, Test loss: 1.709, Test accuracy: 77.45
Round  38, Train loss: -2.453, Test loss: 1.694, Test accuracy: 79.28
Round  39, Train loss: -2.405, Test loss: 1.635, Test accuracy: 84.27
Round  40, Train loss: -2.456, Test loss: 1.635, Test accuracy: 84.27
Round  41, Train loss: -2.707, Test loss: 1.595, Test accuracy: 88.47
Round  42, Train loss: -2.885, Test loss: 1.606, Test accuracy: 86.85
Round  43, Train loss: -4.105, Test loss: 1.584, Test accuracy: 88.02
Round  44, Train loss: -2.064, Test loss: 1.638, Test accuracy: 82.82
Round  45, Train loss: -2.465, Test loss: 1.629, Test accuracy: 82.88
Round  46, Train loss: -1.655, Test loss: 1.622, Test accuracy: 83.77
Round  47, Train loss: -2.025, Test loss: 1.656, Test accuracy: 80.97
Round  48, Train loss: -2.442, Test loss: 1.607, Test accuracy: 85.72
Round  49, Train loss: -2.947, Test loss: 1.620, Test accuracy: 84.22
Round  50, Train loss: -3.314, Test loss: 1.592, Test accuracy: 86.85
Round  51, Train loss: -1.682, Test loss: 1.622, Test accuracy: 84.72
Round  52, Train loss: -2.600, Test loss: 1.625, Test accuracy: 83.63
Round  53, Train loss: -1.888, Test loss: 1.646, Test accuracy: 82.08
Round  54, Train loss: -3.361, Test loss: 1.618, Test accuracy: 84.88
Round  55, Train loss: -3.214, Test loss: 1.596, Test accuracy: 86.52
Round  56, Train loss: -2.312, Test loss: 1.566, Test accuracy: 89.47
Round  57, Train loss: -3.956, Test loss: 1.582, Test accuracy: 87.88
Round  58, Train loss: -3.215, Test loss: 1.544, Test accuracy: 92.15
Round  59, Train loss: -3.049, Test loss: 1.566, Test accuracy: 89.73
Round  60, Train loss: -2.340, Test loss: 1.575, Test accuracy: 88.72
Round  61, Train loss: -3.110, Test loss: 1.575, Test accuracy: 88.70
Round  62, Train loss: -2.856, Test loss: 1.604, Test accuracy: 85.67
Round  63, Train loss: -3.361, Test loss: 1.565, Test accuracy: 89.90
Round  64, Train loss: -3.110, Test loss: 1.565, Test accuracy: 89.98
Round  65, Train loss: -3.692, Test loss: 1.579, Test accuracy: 88.33
Round  66, Train loss: -1.675, Test loss: 1.587, Test accuracy: 87.73
Round  67, Train loss: -3.288, Test loss: 1.595, Test accuracy: 86.77
Round  68, Train loss: -3.036, Test loss: 1.576, Test accuracy: 88.62
Round  69, Train loss: -3.358, Test loss: 1.591, Test accuracy: 87.07
Round  70, Train loss: -3.295, Test loss: 1.588, Test accuracy: 87.32
Round  71, Train loss: -2.177, Test loss: 1.561, Test accuracy: 90.05
Round  72, Train loss: -2.296, Test loss: 1.587, Test accuracy: 87.22
Round  73, Train loss: -3.013, Test loss: 1.586, Test accuracy: 87.30
Round  74, Train loss: -2.622, Test loss: 1.591, Test accuracy: 86.78
Round  75, Train loss: -3.287, Test loss: 1.574, Test accuracy: 88.58
Round  76, Train loss: -3.321, Test loss: 1.573, Test accuracy: 88.80
Round  77, Train loss: -3.410, Test loss: 1.571, Test accuracy: 88.78
Round  78, Train loss: -2.692, Test loss: 1.570, Test accuracy: 88.90
Round  79, Train loss: -2.930, Test loss: 1.585, Test accuracy: 87.40
Round  80, Train loss: -2.918, Test loss: 1.585, Test accuracy: 87.45
Round  81, Train loss: -2.804, Test loss: 1.586, Test accuracy: 87.35
Round  82, Train loss: -3.545, Test loss: 1.573, Test accuracy: 88.77
Round  83, Train loss: -2.848, Test loss: 1.532, Test accuracy: 92.88
Round  84, Train loss: -2.169, Test loss: 1.562, Test accuracy: 89.95
Round  85, Train loss: -2.994, Test loss: 1.563, Test accuracy: 89.83
Round  86, Train loss: -3.013, Test loss: 1.557, Test accuracy: 90.40
Round  87, Train loss: -3.538, Test loss: 1.574, Test accuracy: 88.75
Round  88, Train loss: -3.146, Test loss: 1.575, Test accuracy: 88.58
Round  89, Train loss: -2.727, Test loss: 1.546, Test accuracy: 91.50
Round  90, Train loss: -3.201, Test loss: 1.543, Test accuracy: 91.83
Round  91, Train loss: -2.278, Test loss: 1.574, Test accuracy: 88.77
Round  92, Train loss: -3.722, Test loss: 1.541, Test accuracy: 92.10
Round  93, Train loss: -3.505, Test loss: 1.557, Test accuracy: 90.45
Round  94, Train loss: -3.213, Test loss: 1.542, Test accuracy: 92.03
Round  95, Train loss: -2.794, Test loss: 1.558, Test accuracy: 90.32
Round  96, Train loss: -3.042, Test loss: 1.539, Test accuracy: 92.28
Round  97, Train loss: -2.427, Test loss: 1.568, Test accuracy: 89.43
Round  98, Train loss: -3.459, Test loss: 1.570, Test accuracy: 89.08
Round  99, Train loss: -2.657, Test loss: 1.586, Test accuracy: 87.45
Final Round, Train loss: 1.688, Test loss: 1.609, Test accuracy: 86.27
Average accuracy final 10 rounds: 90.375
Average global accuracy final 10 rounds: 90.375
578.2249844074249
[]
[23.2, 27.016666666666666, 25.2, 30.783333333333335, 31.166666666666668, 20.65, 30.85, 36.05, 37.266666666666666, 33.21666666666667, 37.516666666666666, 32.45, 46.983333333333334, 48.81666666666667, 55.3, 54.05, 50.63333333333333, 59.21666666666667, 63.45, 65.55, 63.333333333333336, 58.36666666666667, 50.233333333333334, 42.35, 57.016666666666666, 66.61666666666666, 65.86666666666666, 69.16666666666667, 70.28333333333333, 77.36666666666666, 69.75, 73.06666666666666, 73.75, 74.85, 65.53333333333333, 73.43333333333334, 76.76666666666667, 77.45, 79.28333333333333, 84.26666666666667, 84.26666666666667, 88.46666666666667, 86.85, 88.01666666666667, 82.81666666666666, 82.88333333333334, 83.76666666666667, 80.96666666666667, 85.71666666666667, 84.21666666666667, 86.85, 84.71666666666667, 83.63333333333334, 82.08333333333333, 84.88333333333334, 86.51666666666667, 89.46666666666667, 87.88333333333334, 92.15, 89.73333333333333, 88.71666666666667, 88.7, 85.66666666666667, 89.9, 89.98333333333333, 88.33333333333333, 87.73333333333333, 86.76666666666667, 88.61666666666666, 87.06666666666666, 87.31666666666666, 90.05, 87.21666666666667, 87.3, 86.78333333333333, 88.58333333333333, 88.8, 88.78333333333333, 88.9, 87.4, 87.45, 87.35, 88.76666666666667, 92.88333333333334, 89.95, 89.83333333333333, 90.4, 88.75, 88.58333333333333, 91.5, 91.83333333333333, 88.76666666666667, 92.1, 90.45, 92.03333333333333, 90.31666666666666, 92.28333333333333, 89.43333333333334, 89.08333333333333, 87.45, 86.26666666666667]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

prox
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.285, Test loss: 2.289, Test accuracy: 18.43
Round   0, Global train loss: 2.285, Global test loss: 2.301, Global test accuracy: 11.53
Round   1, Train loss: 2.263, Test loss: 2.256, Test accuracy: 27.90
Round   1, Global train loss: 2.263, Global test loss: 2.298, Global test accuracy: 13.33
Round   2, Train loss: 2.164, Test loss: 2.167, Test accuracy: 33.50
Round   2, Global train loss: 2.164, Global test loss: 2.291, Global test accuracy: 14.43
Round   3, Train loss: 2.011, Test loss: 2.052, Test accuracy: 46.87
Round   3, Global train loss: 2.011, Global test loss: 2.286, Global test accuracy: 15.27
Round   4, Train loss: 1.969, Test loss: 1.994, Test accuracy: 52.90
Round   4, Global train loss: 1.969, Global test loss: 2.280, Global test accuracy: 15.45
Round   5, Train loss: 2.010, Test loss: 1.902, Test accuracy: 60.27
Round   5, Global train loss: 2.010, Global test loss: 2.273, Global test accuracy: 15.08
Round   6, Train loss: 1.888, Test loss: 1.889, Test accuracy: 60.82
Round   6, Global train loss: 1.888, Global test loss: 2.272, Global test accuracy: 13.42
Round   7, Train loss: 1.868, Test loss: 1.864, Test accuracy: 62.03
Round   7, Global train loss: 1.868, Global test loss: 2.266, Global test accuracy: 18.30
Round   8, Train loss: 1.842, Test loss: 1.869, Test accuracy: 60.73
Round   8, Global train loss: 1.842, Global test loss: 2.275, Global test accuracy: 14.82
Round   9, Train loss: 1.831, Test loss: 1.821, Test accuracy: 65.48
Round   9, Global train loss: 1.831, Global test loss: 2.266, Global test accuracy: 19.42
Round  10, Train loss: 1.893, Test loss: 1.818, Test accuracy: 65.85
Round  10, Global train loss: 1.893, Global test loss: 2.257, Global test accuracy: 18.42
Round  11, Train loss: 1.798, Test loss: 1.800, Test accuracy: 67.35
Round  11, Global train loss: 1.798, Global test loss: 2.257, Global test accuracy: 17.85
Round  12, Train loss: 1.850, Test loss: 1.797, Test accuracy: 67.55
Round  12, Global train loss: 1.850, Global test loss: 2.213, Global test accuracy: 24.48
Round  13, Train loss: 1.753, Test loss: 1.782, Test accuracy: 68.87
Round  13, Global train loss: 1.753, Global test loss: 2.193, Global test accuracy: 26.82
Round  14, Train loss: 1.808, Test loss: 1.768, Test accuracy: 70.28
Round  14, Global train loss: 1.808, Global test loss: 2.206, Global test accuracy: 24.22
Round  15, Train loss: 1.800, Test loss: 1.766, Test accuracy: 70.35
Round  15, Global train loss: 1.800, Global test loss: 2.201, Global test accuracy: 25.25
Round  16, Train loss: 1.834, Test loss: 1.751, Test accuracy: 71.62
Round  16, Global train loss: 1.834, Global test loss: 2.191, Global test accuracy: 25.62
Round  17, Train loss: 1.773, Test loss: 1.751, Test accuracy: 71.48
Round  17, Global train loss: 1.773, Global test loss: 2.209, Global test accuracy: 24.92
Round  18, Train loss: 1.750, Test loss: 1.761, Test accuracy: 70.48
Round  18, Global train loss: 1.750, Global test loss: 2.215, Global test accuracy: 24.27
Round  19, Train loss: 1.774, Test loss: 1.758, Test accuracy: 70.62
Round  19, Global train loss: 1.774, Global test loss: 2.213, Global test accuracy: 23.63
Round  20, Train loss: 1.739, Test loss: 1.759, Test accuracy: 70.67
Round  20, Global train loss: 1.739, Global test loss: 2.204, Global test accuracy: 24.45
Round  21, Train loss: 1.783, Test loss: 1.733, Test accuracy: 73.23
Round  21, Global train loss: 1.783, Global test loss: 2.185, Global test accuracy: 27.78
Round  22, Train loss: 1.757, Test loss: 1.735, Test accuracy: 73.17
Round  22, Global train loss: 1.757, Global test loss: 2.191, Global test accuracy: 26.12
Round  23, Train loss: 1.645, Test loss: 1.733, Test accuracy: 73.23
Round  23, Global train loss: 1.645, Global test loss: 2.167, Global test accuracy: 29.45
Round  24, Train loss: 1.793, Test loss: 1.732, Test accuracy: 73.33
Round  24, Global train loss: 1.793, Global test loss: 2.194, Global test accuracy: 26.05
Round  25, Train loss: 1.795, Test loss: 1.731, Test accuracy: 73.55
Round  25, Global train loss: 1.795, Global test loss: 2.160, Global test accuracy: 30.63
Round  26, Train loss: 1.776, Test loss: 1.732, Test accuracy: 73.43
Round  26, Global train loss: 1.776, Global test loss: 2.157, Global test accuracy: 31.23
Round  27, Train loss: 1.787, Test loss: 1.733, Test accuracy: 73.35
Round  27, Global train loss: 1.787, Global test loss: 2.181, Global test accuracy: 27.85
Round  28, Train loss: 1.718, Test loss: 1.731, Test accuracy: 73.42
Round  28, Global train loss: 1.718, Global test loss: 2.209, Global test accuracy: 24.38
Round  29, Train loss: 1.737, Test loss: 1.729, Test accuracy: 73.48
Round  29, Global train loss: 1.737, Global test loss: 2.158, Global test accuracy: 30.80
Round  30, Train loss: 1.878, Test loss: 1.721, Test accuracy: 74.45
Round  30, Global train loss: 1.878, Global test loss: 2.170, Global test accuracy: 28.30
Round  31, Train loss: 1.691, Test loss: 1.715, Test accuracy: 74.98
Round  31, Global train loss: 1.691, Global test loss: 2.161, Global test accuracy: 29.85
Round  32, Train loss: 1.819, Test loss: 1.714, Test accuracy: 74.97
Round  32, Global train loss: 1.819, Global test loss: 2.216, Global test accuracy: 22.82
Round  33, Train loss: 1.842, Test loss: 1.714, Test accuracy: 74.85
Round  33, Global train loss: 1.842, Global test loss: 2.162, Global test accuracy: 30.38
Round  34, Train loss: 1.674, Test loss: 1.714, Test accuracy: 74.85
Round  34, Global train loss: 1.674, Global test loss: 2.168, Global test accuracy: 28.85
Round  35, Train loss: 1.942, Test loss: 1.745, Test accuracy: 71.58
Round  35, Global train loss: 1.942, Global test loss: 2.177, Global test accuracy: 28.37
Round  36, Train loss: 1.867, Test loss: 1.743, Test accuracy: 71.82
Round  36, Global train loss: 1.867, Global test loss: 2.184, Global test accuracy: 27.43
Round  37, Train loss: 1.677, Test loss: 1.729, Test accuracy: 73.38
Round  37, Global train loss: 1.677, Global test loss: 2.183, Global test accuracy: 27.23
Round  38, Train loss: 1.764, Test loss: 1.711, Test accuracy: 75.22
Round  38, Global train loss: 1.764, Global test loss: 2.183, Global test accuracy: 27.17
Round  39, Train loss: 1.756, Test loss: 1.699, Test accuracy: 76.58
Round  39, Global train loss: 1.756, Global test loss: 2.163, Global test accuracy: 29.78
Round  40, Train loss: 1.748, Test loss: 1.687, Test accuracy: 77.93
Round  40, Global train loss: 1.748, Global test loss: 2.204, Global test accuracy: 24.45
Round  41, Train loss: 1.730, Test loss: 1.687, Test accuracy: 77.93
Round  41, Global train loss: 1.730, Global test loss: 2.184, Global test accuracy: 26.85
Round  42, Train loss: 1.732, Test loss: 1.687, Test accuracy: 77.97
Round  42, Global train loss: 1.732, Global test loss: 2.188, Global test accuracy: 26.73
Round  43, Train loss: 1.701, Test loss: 1.683, Test accuracy: 78.33
Round  43, Global train loss: 1.701, Global test loss: 2.190, Global test accuracy: 26.50
Round  44, Train loss: 1.676, Test loss: 1.683, Test accuracy: 78.43
Round  44, Global train loss: 1.676, Global test loss: 2.206, Global test accuracy: 24.67
Round  45, Train loss: 1.672, Test loss: 1.682, Test accuracy: 78.28
Round  45, Global train loss: 1.672, Global test loss: 2.163, Global test accuracy: 29.67
Round  46, Train loss: 1.782, Test loss: 1.686, Test accuracy: 77.88
Round  46, Global train loss: 1.782, Global test loss: 2.174, Global test accuracy: 28.38
Round  47, Train loss: 1.664, Test loss: 1.684, Test accuracy: 77.98
Round  47, Global train loss: 1.664, Global test loss: 2.169, Global test accuracy: 28.88
Round  48, Train loss: 1.652, Test loss: 1.669, Test accuracy: 79.50
Round  48, Global train loss: 1.652, Global test loss: 2.169, Global test accuracy: 28.73
Round  49, Train loss: 1.646, Test loss: 1.667, Test accuracy: 79.68
Round  49, Global train loss: 1.646, Global test loss: 2.182, Global test accuracy: 27.73
Round  50, Train loss: 1.742, Test loss: 1.669, Test accuracy: 79.53
Round  50, Global train loss: 1.742, Global test loss: 2.192, Global test accuracy: 26.07
Round  51, Train loss: 1.698, Test loss: 1.656, Test accuracy: 80.88
Round  51, Global train loss: 1.698, Global test loss: 2.217, Global test accuracy: 22.47
Round  52, Train loss: 1.613, Test loss: 1.656, Test accuracy: 80.78
Round  52, Global train loss: 1.613, Global test loss: 2.187, Global test accuracy: 26.57
Round  53, Train loss: 1.712, Test loss: 1.684, Test accuracy: 78.07
Round  53, Global train loss: 1.712, Global test loss: 2.200, Global test accuracy: 24.73
Round  54, Train loss: 1.679, Test loss: 1.684, Test accuracy: 78.08
Round  54, Global train loss: 1.679, Global test loss: 2.195, Global test accuracy: 25.10
Round  55, Train loss: 1.662, Test loss: 1.684, Test accuracy: 78.07
Round  55, Global train loss: 1.662, Global test loss: 2.196, Global test accuracy: 24.25
Round  56, Train loss: 1.698, Test loss: 1.670, Test accuracy: 79.50
Round  56, Global train loss: 1.698, Global test loss: 2.214, Global test accuracy: 24.38
Round  57, Train loss: 1.668, Test loss: 1.671, Test accuracy: 79.40
Round  57, Global train loss: 1.668, Global test loss: 2.194, Global test accuracy: 26.35
Round  58, Train loss: 1.610, Test loss: 1.671, Test accuracy: 79.35
Round  58, Global train loss: 1.610, Global test loss: 2.200, Global test accuracy: 24.48
Round  59, Train loss: 1.643, Test loss: 1.674, Test accuracy: 79.00
Round  59, Global train loss: 1.643, Global test loss: 2.186, Global test accuracy: 26.70
Round  60, Train loss: 1.665, Test loss: 1.672, Test accuracy: 79.07
Round  60, Global train loss: 1.665, Global test loss: 2.179, Global test accuracy: 27.20
Round  61, Train loss: 1.682, Test loss: 1.670, Test accuracy: 79.30
Round  61, Global train loss: 1.682, Global test loss: 2.219, Global test accuracy: 21.98
Round  62, Train loss: 1.633, Test loss: 1.668, Test accuracy: 79.52
Round  62, Global train loss: 1.633, Global test loss: 2.174, Global test accuracy: 27.93
Round  63, Train loss: 1.706, Test loss: 1.669, Test accuracy: 79.52
Round  63, Global train loss: 1.706, Global test loss: 2.176, Global test accuracy: 28.12
Round  64, Train loss: 1.712, Test loss: 1.669, Test accuracy: 79.60
Round  64, Global train loss: 1.712, Global test loss: 2.225, Global test accuracy: 21.47
Round  65, Train loss: 1.741, Test loss: 1.682, Test accuracy: 78.13
Round  65, Global train loss: 1.741, Global test loss: 2.189, Global test accuracy: 26.67
Round  66, Train loss: 1.734, Test loss: 1.672, Test accuracy: 79.25
Round  66, Global train loss: 1.734, Global test loss: 2.254, Global test accuracy: 18.48
Round  67, Train loss: 1.617, Test loss: 1.672, Test accuracy: 79.32
Round  67, Global train loss: 1.617, Global test loss: 2.193, Global test accuracy: 26.68
Round  68, Train loss: 1.836, Test loss: 1.671, Test accuracy: 79.30
Round  68, Global train loss: 1.836, Global test loss: 2.227, Global test accuracy: 21.62
Round  69, Train loss: 1.716, Test loss: 1.686, Test accuracy: 77.78
Round  69, Global train loss: 1.716, Global test loss: 2.188, Global test accuracy: 24.97
Round  70, Train loss: 1.658, Test loss: 1.688, Test accuracy: 77.72
Round  70, Global train loss: 1.658, Global test loss: 2.168, Global test accuracy: 28.52
Round  71, Train loss: 1.815, Test loss: 1.683, Test accuracy: 78.22
Round  71, Global train loss: 1.815, Global test loss: 2.216, Global test accuracy: 22.40
Round  72, Train loss: 1.701, Test loss: 1.670, Test accuracy: 79.50
Round  72, Global train loss: 1.701, Global test loss: 2.199, Global test accuracy: 24.57
Round  73, Train loss: 1.687, Test loss: 1.668, Test accuracy: 79.62
Round  73, Global train loss: 1.687, Global test loss: 2.212, Global test accuracy: 23.60
Round  74, Train loss: 1.666, Test loss: 1.668, Test accuracy: 79.62
Round  74, Global train loss: 1.666, Global test loss: 2.211, Global test accuracy: 23.45
Round  75, Train loss: 1.699, Test loss: 1.669, Test accuracy: 79.45
Round  75, Global train loss: 1.699, Global test loss: 2.198, Global test accuracy: 25.50
Round  76, Train loss: 1.720, Test loss: 1.668, Test accuracy: 79.58
Round  76, Global train loss: 1.720, Global test loss: 2.197, Global test accuracy: 24.65
Round  77, Train loss: 1.509, Test loss: 1.668, Test accuracy: 79.72
Round  77, Global train loss: 1.509, Global test loss: 2.173, Global test accuracy: 27.55
Round  78, Train loss: 1.671, Test loss: 1.666, Test accuracy: 79.75
Round  78, Global train loss: 1.671, Global test loss: 2.186, Global test accuracy: 26.68
Round  79, Train loss: 1.670, Test loss: 1.666, Test accuracy: 79.80
Round  79, Global train loss: 1.670, Global test loss: 2.180, Global test accuracy: 26.92
Round  80, Train loss: 1.689, Test loss: 1.668, Test accuracy: 79.50
Round  80, Global train loss: 1.689, Global test loss: 2.214, Global test accuracy: 23.90
Round  81, Train loss: 1.744, Test loss: 1.684, Test accuracy: 77.80
Round  81, Global train loss: 1.744, Global test loss: 2.199, Global test accuracy: 24.53
Round  82, Train loss: 1.650, Test loss: 1.684, Test accuracy: 77.73
Round  82, Global train loss: 1.650, Global test loss: 2.191, Global test accuracy: 25.65
Round  83, Train loss: 1.647, Test loss: 1.668, Test accuracy: 79.38
Round  83, Global train loss: 1.647, Global test loss: 2.195, Global test accuracy: 24.62
Round  84, Train loss: 1.637, Test loss: 1.669, Test accuracy: 79.22
Round  84, Global train loss: 1.637, Global test loss: 2.166, Global test accuracy: 28.05
Round  85, Train loss: 1.735, Test loss: 1.668, Test accuracy: 79.48
Round  85, Global train loss: 1.735, Global test loss: 2.215, Global test accuracy: 22.63
Round  86, Train loss: 1.657, Test loss: 1.668, Test accuracy: 79.52
Round  86, Global train loss: 1.657, Global test loss: 2.151, Global test accuracy: 30.83
Round  87, Train loss: 1.659, Test loss: 1.667, Test accuracy: 79.55
Round  87, Global train loss: 1.659, Global test loss: 2.177, Global test accuracy: 27.60
Round  88, Train loss: 1.601, Test loss: 1.667, Test accuracy: 79.57
Round  88, Global train loss: 1.601, Global test loss: 2.149, Global test accuracy: 30.88
Round  89, Train loss: 1.745, Test loss: 1.666, Test accuracy: 79.67
Round  89, Global train loss: 1.745, Global test loss: 2.212, Global test accuracy: 23.05
Round  90, Train loss: 1.603, Test loss: 1.667, Test accuracy: 79.47
Round  90, Global train loss: 1.603, Global test loss: 2.213, Global test accuracy: 23.18
Round  91, Train loss: 1.720, Test loss: 1.666, Test accuracy: 79.58
Round  91, Global train loss: 1.720, Global test loss: 2.207, Global test accuracy: 23.97
Round  92, Train loss: 1.760, Test loss: 1.666, Test accuracy: 79.63
Round  92, Global train loss: 1.760, Global test loss: 2.184, Global test accuracy: 27.18
Round  93, Train loss: 1.649, Test loss: 1.650, Test accuracy: 81.28
Round  93, Global train loss: 1.649, Global test loss: 2.192, Global test accuracy: 26.02
Round  94, Train loss: 1.664, Test loss: 1.649, Test accuracy: 81.43
Round  94, Global train loss: 1.664, Global test loss: 2.150, Global test accuracy: 30.82
Round  95, Train loss: 1.751, Test loss: 1.649, Test accuracy: 81.47
Round  95, Global train loss: 1.751, Global test loss: 2.188, Global test accuracy: 27.23/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  96, Train loss: 1.614, Test loss: 1.650, Test accuracy: 81.35
Round  96, Global train loss: 1.614, Global test loss: 2.180, Global test accuracy: 26.22
Round  97, Train loss: 1.511, Test loss: 1.649, Test accuracy: 81.42
Round  97, Global train loss: 1.511, Global test loss: 2.209, Global test accuracy: 23.53
Round  98, Train loss: 1.655, Test loss: 1.665, Test accuracy: 79.73
Round  98, Global train loss: 1.655, Global test loss: 2.206, Global test accuracy: 24.52
Round  99, Train loss: 1.660, Test loss: 1.664, Test accuracy: 79.82
Round  99, Global train loss: 1.660, Global test loss: 2.191, Global test accuracy: 25.78
Final Round, Train loss: 1.628, Test loss: 1.650, Test accuracy: 81.32
Final Round, Global train loss: 1.628, Global test loss: 2.191, Global test accuracy: 25.78
Average accuracy final 10 rounds: 80.51833333333333 

Average global accuracy final 10 rounds: 25.845 

846.733895778656
[0.57804274559021, 1.15608549118042, 1.6829745769500732, 2.2098636627197266, 2.761111259460449, 3.312358856201172, 3.824007272720337, 4.335655689239502, 4.879597902297974, 5.423540115356445, 5.983346700668335, 6.543153285980225, 7.107400178909302, 7.671647071838379, 8.213751316070557, 8.755855560302734, 9.303269863128662, 9.85068416595459, 10.373991966247559, 10.897299766540527, 11.43413496017456, 11.970970153808594, 12.49705195426941, 13.023133754730225, 13.554348468780518, 14.08556318283081, 14.636625528335571, 15.187687873840332, 15.70579981803894, 16.22391176223755, 16.750887155532837, 17.277862548828125, 17.822569847106934, 18.367277145385742, 18.89548897743225, 19.42370080947876, 19.96110486984253, 20.4985089302063, 21.02591633796692, 21.55332374572754, 22.090792894363403, 22.628262042999268, 23.151132345199585, 23.674002647399902, 24.206029653549194, 24.738056659698486, 25.283995866775513, 25.82993507385254, 26.374509811401367, 26.919084548950195, 27.453184127807617, 27.98728370666504, 28.525198459625244, 29.06311321258545, 29.588151931762695, 30.11319065093994, 30.662616729736328, 31.212042808532715, 31.747101545333862, 32.28216028213501, 32.80874013900757, 33.33531999588013, 33.872673988342285, 34.41002798080444, 34.95443868637085, 35.498849391937256, 36.027525186538696, 36.55620098114014, 37.09159779548645, 37.626994609832764, 38.178041219711304, 38.729087829589844, 39.26405572891235, 39.79902362823486, 40.3287570476532, 40.85849046707153, 41.40203619003296, 41.945581912994385, 42.47615432739258, 43.00672674179077, 43.535802364349365, 44.06487798690796, 44.60270309448242, 45.140528202056885, 45.68479609489441, 46.229063987731934, 46.77568435668945, 47.32230472564697, 47.85554575920105, 48.38878679275513, 48.925344944000244, 49.46190309524536, 50.008217096328735, 50.55453109741211, 51.083580493927, 51.612629890441895, 52.13522791862488, 52.65782594680786, 53.20189094543457, 53.74595594406128, 54.2908992767334, 54.83584260940552, 55.347272634506226, 55.858702659606934, 56.39965200424194, 56.94060134887695, 57.48940920829773, 58.038217067718506, 58.57862448692322, 59.11903190612793, 59.647724628448486, 60.17641735076904, 60.69458317756653, 61.212749004364014, 61.73049855232239, 62.24824810028076, 62.80098509788513, 63.3537220954895, 63.8859498500824, 64.4181776046753, 64.95507788658142, 65.49197816848755, 66.04237580299377, 66.5927734375, 67.13014602661133, 67.66751861572266, 68.19835638999939, 68.72919416427612, 69.25839805603027, 69.78760194778442, 70.32470440864563, 70.86180686950684, 71.39104747772217, 71.9202880859375, 72.45254945755005, 72.9848108291626, 73.534104347229, 74.08339786529541, 74.63294720649719, 75.18249654769897, 75.72427010536194, 76.2660436630249, 76.79656744003296, 77.32709121704102, 77.87410950660706, 78.4211277961731, 78.95969319343567, 79.49825859069824, 80.0182454586029, 80.53823232650757, 81.05055689811707, 81.56288146972656, 82.10764336585999, 82.65240526199341, 83.19506549835205, 83.7377257347107, 84.26046204566956, 84.78319835662842, 85.33178496360779, 85.88037157058716, 86.42469882965088, 86.9690260887146, 87.46611475944519, 87.96320343017578, 88.49199223518372, 89.02078104019165, 89.56469464302063, 90.10860824584961, 90.65416049957275, 91.1997127532959, 91.73455214500427, 92.26939153671265, 92.80893325805664, 93.34847497940063, 93.90161299705505, 94.45475101470947, 95.004061460495, 95.55337190628052, 96.09662890434265, 96.63988590240479, 97.18552589416504, 97.7311658859253, 98.27441668510437, 98.81766748428345, 99.35061430931091, 99.88356113433838, 100.419016122818, 100.95447111129761, 101.450514793396, 101.94655847549438, 102.47586178779602, 103.00516510009766, 103.52770566940308, 104.0502462387085, 104.59103655815125, 105.131826877594, 105.67405700683594, 106.21628713607788, 106.74841403961182, 107.28054094314575, 108.3783049583435, 109.47606897354126]
[18.433333333333334, 18.433333333333334, 27.9, 27.9, 33.5, 33.5, 46.86666666666667, 46.86666666666667, 52.9, 52.9, 60.266666666666666, 60.266666666666666, 60.81666666666667, 60.81666666666667, 62.03333333333333, 62.03333333333333, 60.733333333333334, 60.733333333333334, 65.48333333333333, 65.48333333333333, 65.85, 65.85, 67.35, 67.35, 67.55, 67.55, 68.86666666666666, 68.86666666666666, 70.28333333333333, 70.28333333333333, 70.35, 70.35, 71.61666666666666, 71.61666666666666, 71.48333333333333, 71.48333333333333, 70.48333333333333, 70.48333333333333, 70.61666666666666, 70.61666666666666, 70.66666666666667, 70.66666666666667, 73.23333333333333, 73.23333333333333, 73.16666666666667, 73.16666666666667, 73.23333333333333, 73.23333333333333, 73.33333333333333, 73.33333333333333, 73.55, 73.55, 73.43333333333334, 73.43333333333334, 73.35, 73.35, 73.41666666666667, 73.41666666666667, 73.48333333333333, 73.48333333333333, 74.45, 74.45, 74.98333333333333, 74.98333333333333, 74.96666666666667, 74.96666666666667, 74.85, 74.85, 74.85, 74.85, 71.58333333333333, 71.58333333333333, 71.81666666666666, 71.81666666666666, 73.38333333333334, 73.38333333333334, 75.21666666666667, 75.21666666666667, 76.58333333333333, 76.58333333333333, 77.93333333333334, 77.93333333333334, 77.93333333333334, 77.93333333333334, 77.96666666666667, 77.96666666666667, 78.33333333333333, 78.33333333333333, 78.43333333333334, 78.43333333333334, 78.28333333333333, 78.28333333333333, 77.88333333333334, 77.88333333333334, 77.98333333333333, 77.98333333333333, 79.5, 79.5, 79.68333333333334, 79.68333333333334, 79.53333333333333, 79.53333333333333, 80.88333333333334, 80.88333333333334, 80.78333333333333, 80.78333333333333, 78.06666666666666, 78.06666666666666, 78.08333333333333, 78.08333333333333, 78.06666666666666, 78.06666666666666, 79.5, 79.5, 79.4, 79.4, 79.35, 79.35, 79.0, 79.0, 79.06666666666666, 79.06666666666666, 79.3, 79.3, 79.51666666666667, 79.51666666666667, 79.51666666666667, 79.51666666666667, 79.6, 79.6, 78.13333333333334, 78.13333333333334, 79.25, 79.25, 79.31666666666666, 79.31666666666666, 79.3, 79.3, 77.78333333333333, 77.78333333333333, 77.71666666666667, 77.71666666666667, 78.21666666666667, 78.21666666666667, 79.5, 79.5, 79.61666666666666, 79.61666666666666, 79.61666666666666, 79.61666666666666, 79.45, 79.45, 79.58333333333333, 79.58333333333333, 79.71666666666667, 79.71666666666667, 79.75, 79.75, 79.8, 79.8, 79.5, 79.5, 77.8, 77.8, 77.73333333333333, 77.73333333333333, 79.38333333333334, 79.38333333333334, 79.21666666666667, 79.21666666666667, 79.48333333333333, 79.48333333333333, 79.51666666666667, 79.51666666666667, 79.55, 79.55, 79.56666666666666, 79.56666666666666, 79.66666666666667, 79.66666666666667, 79.46666666666667, 79.46666666666667, 79.58333333333333, 79.58333333333333, 79.63333333333334, 79.63333333333334, 81.28333333333333, 81.28333333333333, 81.43333333333334, 81.43333333333334, 81.46666666666667, 81.46666666666667, 81.35, 81.35, 81.41666666666667, 81.41666666666667, 79.73333333333333, 79.73333333333333, 79.81666666666666, 79.81666666666666, 81.31666666666666, 81.31666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.293, Test loss: 2.300, Test accuracy: 14.88
Round   1, Train loss: 2.268, Test loss: 2.292, Test accuracy: 14.48
Round   2, Train loss: 2.167, Test loss: 2.279, Test accuracy: 15.00
Round   3, Train loss: 2.069, Test loss: 2.281, Test accuracy: 15.12
Round   4, Train loss: 1.873, Test loss: 2.267, Test accuracy: 16.37
Round   5, Train loss: 1.836, Test loss: 2.239, Test accuracy: 21.20
Round   6, Train loss: 1.951, Test loss: 2.259, Test accuracy: 15.70
Round   7, Train loss: 1.845, Test loss: 2.249, Test accuracy: 17.33
Round   8, Train loss: 1.731, Test loss: 2.234, Test accuracy: 20.47
Round   9, Train loss: 1.702, Test loss: 2.220, Test accuracy: 21.87
Round  10, Train loss: 1.749, Test loss: 2.233, Test accuracy: 21.38
Round  11, Train loss: 1.664, Test loss: 2.217, Test accuracy: 22.55
Round  12, Train loss: 1.659, Test loss: 2.242, Test accuracy: 19.92
Round  13, Train loss: 1.588, Test loss: 2.219, Test accuracy: 22.63
Round  14, Train loss: 1.727, Test loss: 2.234, Test accuracy: 20.30
Round  15, Train loss: 1.643, Test loss: 2.218, Test accuracy: 23.12
Round  16, Train loss: 1.681, Test loss: 2.228, Test accuracy: 20.70
Round  17, Train loss: 1.676, Test loss: 2.217, Test accuracy: 21.93
Round  18, Train loss: 1.727, Test loss: 2.230, Test accuracy: 22.08
Round  19, Train loss: 1.712, Test loss: 2.220, Test accuracy: 23.28
Round  20, Train loss: 1.668, Test loss: 2.224, Test accuracy: 21.42
Round  21, Train loss: 1.660, Test loss: 2.219, Test accuracy: 22.38
Round  22, Train loss: 1.669, Test loss: 2.223, Test accuracy: 21.73
Round  23, Train loss: 1.612, Test loss: 2.221, Test accuracy: 21.58
Round  24, Train loss: 1.720, Test loss: 2.209, Test accuracy: 23.27
Round  25, Train loss: 1.624, Test loss: 2.227, Test accuracy: 21.28
Round  26, Train loss: 1.612, Test loss: 2.215, Test accuracy: 22.27
Round  27, Train loss: 1.718, Test loss: 2.239, Test accuracy: 20.82
Round  28, Train loss: 1.558, Test loss: 2.236, Test accuracy: 20.18
Round  29, Train loss: 1.657, Test loss: 2.229, Test accuracy: 20.87
Round  30, Train loss: 1.596, Test loss: 2.217, Test accuracy: 22.92
Round  31, Train loss: 1.650, Test loss: 2.227, Test accuracy: 22.60
Round  32, Train loss: 1.702, Test loss: 2.237, Test accuracy: 21.38
Round  33, Train loss: 1.607, Test loss: 2.252, Test accuracy: 18.82
Round  34, Train loss: 1.639, Test loss: 2.212, Test accuracy: 23.58
Round  35, Train loss: 1.704, Test loss: 2.210, Test accuracy: 23.83
Round  36, Train loss: 1.561, Test loss: 2.209, Test accuracy: 23.65
Round  37, Train loss: 1.660, Test loss: 2.208, Test accuracy: 24.48
Round  38, Train loss: 1.601, Test loss: 2.217, Test accuracy: 23.03
Round  39, Train loss: 1.648, Test loss: 2.211, Test accuracy: 23.55
Round  40, Train loss: 1.506, Test loss: 2.236, Test accuracy: 20.82
Round  41, Train loss: 1.654, Test loss: 2.241, Test accuracy: 20.18
Round  42, Train loss: 1.698, Test loss: 2.212, Test accuracy: 23.52
Round  43, Train loss: 1.542, Test loss: 2.206, Test accuracy: 24.53
Round  44, Train loss: 1.608, Test loss: 2.228, Test accuracy: 21.65
Round  45, Train loss: 1.607, Test loss: 2.236, Test accuracy: 20.63
Round  46, Train loss: 1.642, Test loss: 2.231, Test accuracy: 20.63
Round  47, Train loss: 1.605, Test loss: 2.205, Test accuracy: 24.63
Round  48, Train loss: 1.695, Test loss: 2.217, Test accuracy: 22.93
Round  49, Train loss: 1.644, Test loss: 2.220, Test accuracy: 23.00
Round  50, Train loss: 1.692, Test loss: 2.206, Test accuracy: 24.48
Round  51, Train loss: 1.597, Test loss: 2.199, Test accuracy: 25.33
Round  52, Train loss: 1.588, Test loss: 2.205, Test accuracy: 24.10
Round  53, Train loss: 1.547, Test loss: 2.196, Test accuracy: 25.72
Round  54, Train loss: 1.703, Test loss: 2.206, Test accuracy: 24.73
Round  55, Train loss: 1.637, Test loss: 2.205, Test accuracy: 24.00
Round  56, Train loss: 1.638, Test loss: 2.195, Test accuracy: 25.77
Round  57, Train loss: 1.695, Test loss: 2.214, Test accuracy: 22.78
Round  58, Train loss: 1.598, Test loss: 2.213, Test accuracy: 23.58
Round  59, Train loss: 1.544, Test loss: 2.201, Test accuracy: 25.07
Round  60, Train loss: 1.533, Test loss: 2.202, Test accuracy: 24.67
Round  61, Train loss: 1.638, Test loss: 2.210, Test accuracy: 23.58
Round  62, Train loss: 1.640, Test loss: 2.211, Test accuracy: 23.70
Round  63, Train loss: 1.591, Test loss: 2.221, Test accuracy: 22.30
Round  64, Train loss: 1.589, Test loss: 2.242, Test accuracy: 19.35
Round  65, Train loss: 1.593, Test loss: 2.227, Test accuracy: 21.18
Round  66, Train loss: 1.587, Test loss: 2.225, Test accuracy: 21.40
Round  67, Train loss: 1.538, Test loss: 2.215, Test accuracy: 23.43
Round  68, Train loss: 1.540, Test loss: 2.211, Test accuracy: 23.57
Round  69, Train loss: 1.641, Test loss: 2.195, Test accuracy: 24.98
Round  70, Train loss: 1.690, Test loss: 2.212, Test accuracy: 22.72
Round  71, Train loss: 1.529, Test loss: 2.204, Test accuracy: 23.82
Round  72, Train loss: 1.532, Test loss: 2.201, Test accuracy: 24.52
Round  73, Train loss: 1.530, Test loss: 2.206, Test accuracy: 23.57
Round  74, Train loss: 1.588, Test loss: 2.215, Test accuracy: 22.82
Round  75, Train loss: 1.580, Test loss: 2.205, Test accuracy: 24.93
Round  76, Train loss: 1.527, Test loss: 2.193, Test accuracy: 25.82
Round  77, Train loss: 1.583, Test loss: 2.217, Test accuracy: 22.62
Round  78, Train loss: 1.693, Test loss: 2.188, Test accuracy: 26.63
Round  79, Train loss: 1.544, Test loss: 2.192, Test accuracy: 25.50
Round  80, Train loss: 1.538, Test loss: 2.232, Test accuracy: 21.55
Round  81, Train loss: 1.638, Test loss: 2.197, Test accuracy: 24.93
Round  82, Train loss: 1.537, Test loss: 2.210, Test accuracy: 23.70
Round  83, Train loss: 1.640, Test loss: 2.223, Test accuracy: 22.60
Round  84, Train loss: 1.589, Test loss: 2.205, Test accuracy: 24.12
Round  85, Train loss: 1.581, Test loss: 2.197, Test accuracy: 25.12
Round  86, Train loss: 1.527, Test loss: 2.179, Test accuracy: 27.37
Round  87, Train loss: 1.631, Test loss: 2.186, Test accuracy: 26.68
Round  88, Train loss: 1.588, Test loss: 2.201, Test accuracy: 24.52
Round  89, Train loss: 1.538, Test loss: 2.197, Test accuracy: 25.03
Round  90, Train loss: 1.688, Test loss: 2.208, Test accuracy: 23.37
Round  91, Train loss: 1.474, Test loss: 2.214, Test accuracy: 23.08
Round  92, Train loss: 1.582, Test loss: 2.207, Test accuracy: 23.92
Round  93, Train loss: 1.633, Test loss: 2.202, Test accuracy: 24.12
Round  94, Train loss: 1.687, Test loss: 2.202, Test accuracy: 24.72
Round  95, Train loss: 1.533, Test loss: 2.193, Test accuracy: 25.58
Round  96, Train loss: 1.487, Test loss: 2.208, Test accuracy: 23.93
Round  97, Train loss: 1.588, Test loss: 2.223, Test accuracy: 21.87
Round  98, Train loss: 1.539, Test loss: 2.203, Test accuracy: 24.47
Round  99, Train loss: 1.641, Test loss: 2.211, Test accuracy: 23.47
Final Round, Train loss: 1.589, Test loss: 2.193, Test accuracy: 25.80
Average accuracy final 10 rounds: 23.851666666666667
957.3709435462952
[1.518615961074829, 2.8333394527435303, 4.163748264312744, 5.485361337661743, 6.799467086791992, 8.117450475692749, 9.430359840393066, 10.72664999961853, 12.052261590957642, 13.363860607147217, 14.67194151878357, 15.982978343963623, 17.28136920928955, 18.591554641723633, 19.899033069610596, 21.222452640533447, 22.5295307636261, 23.835716724395752, 25.152029037475586, 26.458348989486694, 27.792194604873657, 29.115943908691406, 30.419103622436523, 31.732052326202393, 33.04634714126587, 34.35490894317627, 35.67889142036438, 36.9900426864624, 38.282095432281494, 39.618035078048706, 40.94431018829346, 42.25054383277893, 43.566264629364014, 44.87683463096619, 46.184117794036865, 47.49834990501404, 48.82496643066406, 50.12753105163574, 51.46251034736633, 52.75798940658569, 54.06284284591675, 55.38110280036926, 56.69460940361023, 58.00545883178711, 59.303184270858765, 60.613157749176025, 61.93325924873352, 63.25385022163391, 64.5561249256134, 65.8656895160675, 67.18202090263367, 68.49757695198059, 69.83357810974121, 71.15599513053894, 72.47602248191833, 73.7879695892334, 75.10134792327881, 76.42676997184753, 77.761301279068, 79.08785891532898, 80.40199589729309, 81.72591137886047, 83.05917978286743, 84.39208340644836, 85.68919658660889, 87.00851106643677, 88.31570816040039, 89.63988947868347, 90.95611262321472, 92.25758337974548, 93.58451986312866, 94.89270329475403, 96.19699811935425, 97.539475440979, 98.8601176738739, 100.15525603294373, 101.44811654090881, 102.77077007293701, 104.09704899787903, 105.41505193710327, 106.72115898132324, 107.99676656723022, 109.30402612686157, 110.62851095199585, 111.92174959182739, 113.22217679023743, 114.51082229614258, 115.77597641944885, 117.0672709941864, 118.34285378456116, 119.61543822288513, 120.89019227027893, 122.1351306438446, 123.4190559387207, 124.71435046195984, 126.01848483085632, 127.31573009490967, 128.60781478881836, 129.90140557289124, 131.18375277519226, 132.46259760856628]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[14.883333333333333, 14.483333333333333, 15.0, 15.116666666666667, 16.366666666666667, 21.2, 15.7, 17.333333333333332, 20.466666666666665, 21.866666666666667, 21.383333333333333, 22.55, 19.916666666666668, 22.633333333333333, 20.3, 23.116666666666667, 20.7, 21.933333333333334, 22.083333333333332, 23.283333333333335, 21.416666666666668, 22.383333333333333, 21.733333333333334, 21.583333333333332, 23.266666666666666, 21.283333333333335, 22.266666666666666, 20.816666666666666, 20.183333333333334, 20.866666666666667, 22.916666666666668, 22.6, 21.383333333333333, 18.816666666666666, 23.583333333333332, 23.833333333333332, 23.65, 24.483333333333334, 23.033333333333335, 23.55, 20.816666666666666, 20.183333333333334, 23.516666666666666, 24.533333333333335, 21.65, 20.633333333333333, 20.633333333333333, 24.633333333333333, 22.933333333333334, 23.0, 24.483333333333334, 25.333333333333332, 24.1, 25.716666666666665, 24.733333333333334, 24.0, 25.766666666666666, 22.783333333333335, 23.583333333333332, 25.066666666666666, 24.666666666666668, 23.583333333333332, 23.7, 22.3, 19.35, 21.183333333333334, 21.4, 23.433333333333334, 23.566666666666666, 24.983333333333334, 22.716666666666665, 23.816666666666666, 24.516666666666666, 23.566666666666666, 22.816666666666666, 24.933333333333334, 25.816666666666666, 22.616666666666667, 26.633333333333333, 25.5, 21.55, 24.933333333333334, 23.7, 22.6, 24.116666666666667, 25.116666666666667, 27.366666666666667, 26.683333333333334, 24.516666666666666, 25.033333333333335, 23.366666666666667, 23.083333333333332, 23.916666666666668, 24.116666666666667, 24.716666666666665, 25.583333333333332, 23.933333333333334, 21.866666666666667, 24.466666666666665, 23.466666666666665, 25.8]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.317, Test loss: 2.301, Test accuracy: 13.67
Round   1, Train loss: 2.296, Test loss: 2.299, Test accuracy: 16.33
Round   2, Train loss: 2.292, Test loss: 2.297, Test accuracy: 15.73
Round   3, Train loss: 2.285, Test loss: 2.294, Test accuracy: 17.58
Round   4, Train loss: 2.275, Test loss: 2.285, Test accuracy: 19.35
Round   5, Train loss: 2.215, Test loss: 2.257, Test accuracy: 22.23
Round   6, Train loss: 2.118, Test loss: 2.200, Test accuracy: 28.78
Round   7, Train loss: 2.056, Test loss: 2.166, Test accuracy: 30.87
Round   8, Train loss: 2.079, Test loss: 2.087, Test accuracy: 41.52
Round   9, Train loss: 1.947, Test loss: 2.035, Test accuracy: 47.07
Round  10, Train loss: 1.851, Test loss: 1.977, Test accuracy: 51.62
Round  11, Train loss: 1.921, Test loss: 1.927, Test accuracy: 59.00
Round  12, Train loss: 1.783, Test loss: 1.892, Test accuracy: 62.50
Round  13, Train loss: 1.786, Test loss: 1.856, Test accuracy: 66.57
Round  14, Train loss: 1.796, Test loss: 1.825, Test accuracy: 68.58
Round  15, Train loss: 1.706, Test loss: 1.781, Test accuracy: 73.67
Round  16, Train loss: 1.762, Test loss: 1.787, Test accuracy: 72.73
Round  17, Train loss: 1.676, Test loss: 1.772, Test accuracy: 73.50
Round  18, Train loss: 1.751, Test loss: 1.769, Test accuracy: 74.27
Round  19, Train loss: 1.758, Test loss: 1.769, Test accuracy: 74.00
Round  20, Train loss: 1.706, Test loss: 1.759, Test accuracy: 74.40
Round  21, Train loss: 1.679, Test loss: 1.758, Test accuracy: 73.40
Round  22, Train loss: 1.712, Test loss: 1.750, Test accuracy: 75.02
Round  23, Train loss: 1.700, Test loss: 1.747, Test accuracy: 74.98
Round  24, Train loss: 1.699, Test loss: 1.738, Test accuracy: 76.18
Round  25, Train loss: 1.662, Test loss: 1.725, Test accuracy: 77.03
Round  26, Train loss: 1.638, Test loss: 1.717, Test accuracy: 77.23
Round  27, Train loss: 1.559, Test loss: 1.705, Test accuracy: 78.42
Round  28, Train loss: 1.686, Test loss: 1.701, Test accuracy: 78.65
Round  29, Train loss: 1.630, Test loss: 1.689, Test accuracy: 81.00
Round  30, Train loss: 1.653, Test loss: 1.690, Test accuracy: 80.75
Round  31, Train loss: 1.661, Test loss: 1.682, Test accuracy: 82.02
Round  32, Train loss: 1.676, Test loss: 1.686, Test accuracy: 82.08
Round  33, Train loss: 1.723, Test loss: 1.670, Test accuracy: 83.45
Round  34, Train loss: 1.671, Test loss: 1.666, Test accuracy: 83.40
Round  35, Train loss: 1.614, Test loss: 1.654, Test accuracy: 84.63
Round  36, Train loss: 1.646, Test loss: 1.647, Test accuracy: 86.18
Round  37, Train loss: 1.636, Test loss: 1.650, Test accuracy: 86.87
Round  38, Train loss: 1.561, Test loss: 1.625, Test accuracy: 88.45
Round  39, Train loss: 1.584, Test loss: 1.622, Test accuracy: 88.87
Round  40, Train loss: 1.596, Test loss: 1.620, Test accuracy: 88.63
Round  41, Train loss: 1.635, Test loss: 1.617, Test accuracy: 88.78
Round  42, Train loss: 1.635, Test loss: 1.622, Test accuracy: 88.25
Round  43, Train loss: 1.640, Test loss: 1.624, Test accuracy: 87.77
Round  44, Train loss: 1.630, Test loss: 1.622, Test accuracy: 87.97
Round  45, Train loss: 1.569, Test loss: 1.611, Test accuracy: 88.70
Round  46, Train loss: 1.607, Test loss: 1.614, Test accuracy: 88.78
Round  47, Train loss: 1.580, Test loss: 1.605, Test accuracy: 89.25
Round  48, Train loss: 1.627, Test loss: 1.607, Test accuracy: 88.65
Round  49, Train loss: 1.573, Test loss: 1.604, Test accuracy: 89.15
Round  50, Train loss: 1.563, Test loss: 1.602, Test accuracy: 89.53
Round  51, Train loss: 1.562, Test loss: 1.600, Test accuracy: 89.27
Round  52, Train loss: 1.612, Test loss: 1.606, Test accuracy: 88.98
Round  53, Train loss: 1.626, Test loss: 1.603, Test accuracy: 89.10
Round  54, Train loss: 1.616, Test loss: 1.608, Test accuracy: 88.75
Round  55, Train loss: 1.562, Test loss: 1.604, Test accuracy: 88.90
Round  56, Train loss: 1.599, Test loss: 1.603, Test accuracy: 88.90
Round  57, Train loss: 1.506, Test loss: 1.600, Test accuracy: 89.18
Round  58, Train loss: 1.611, Test loss: 1.611, Test accuracy: 88.23
Round  59, Train loss: 1.629, Test loss: 1.609, Test accuracy: 88.40
Round  60, Train loss: 1.518, Test loss: 1.593, Test accuracy: 89.78
Round  61, Train loss: 1.549, Test loss: 1.596, Test accuracy: 89.57
Round  62, Train loss: 1.512, Test loss: 1.590, Test accuracy: 89.73
Round  63, Train loss: 1.554, Test loss: 1.591, Test accuracy: 89.63
Round  64, Train loss: 1.499, Test loss: 1.589, Test accuracy: 89.60
Round  65, Train loss: 1.616, Test loss: 1.592, Test accuracy: 89.60
Round  66, Train loss: 1.612, Test loss: 1.593, Test accuracy: 89.83
Round  67, Train loss: 1.604, Test loss: 1.596, Test accuracy: 89.52
Round  68, Train loss: 1.600, Test loss: 1.604, Test accuracy: 89.05
Round  69, Train loss: 1.511, Test loss: 1.591, Test accuracy: 89.63
Round  70, Train loss: 1.574, Test loss: 1.585, Test accuracy: 91.00
Round  71, Train loss: 1.609, Test loss: 1.587, Test accuracy: 90.88
Round  72, Train loss: 1.501, Test loss: 1.576, Test accuracy: 91.35
Round  73, Train loss: 1.503, Test loss: 1.573, Test accuracy: 91.45
Round  74, Train loss: 1.553, Test loss: 1.574, Test accuracy: 91.35
Round  75, Train loss: 1.558, Test loss: 1.574, Test accuracy: 91.62
Round  76, Train loss: 1.550, Test loss: 1.573, Test accuracy: 91.77
Round  77, Train loss: 1.560, Test loss: 1.578, Test accuracy: 91.02
Round  78, Train loss: 1.553, Test loss: 1.579, Test accuracy: 91.22
Round  79, Train loss: 1.548, Test loss: 1.572, Test accuracy: 91.48
Round  80, Train loss: 1.543, Test loss: 1.575, Test accuracy: 91.28
Round  81, Train loss: 1.548, Test loss: 1.579, Test accuracy: 91.22
Round  82, Train loss: 1.495, Test loss: 1.574, Test accuracy: 91.58
Round  83, Train loss: 1.495, Test loss: 1.571, Test accuracy: 91.63
Round  84, Train loss: 1.495, Test loss: 1.572, Test accuracy: 91.40
Round  85, Train loss: 1.491, Test loss: 1.570, Test accuracy: 91.40
Round  86, Train loss: 1.550, Test loss: 1.580, Test accuracy: 90.62
Round  87, Train loss: 1.598, Test loss: 1.576, Test accuracy: 91.32
Round  88, Train loss: 1.547, Test loss: 1.575, Test accuracy: 91.30
Round  89, Train loss: 1.491, Test loss: 1.571, Test accuracy: 91.48
Round  90, Train loss: 1.551, Test loss: 1.571, Test accuracy: 91.45
Round  91, Train loss: 1.550, Test loss: 1.569, Test accuracy: 91.55
Round  92, Train loss: 1.597, Test loss: 1.572, Test accuracy: 91.40
Round  93, Train loss: 1.490, Test loss: 1.566, Test accuracy: 91.75
Round  94, Train loss: 1.546, Test loss: 1.578, Test accuracy: 90.82/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.543, Test loss: 1.569, Test accuracy: 91.60
Round  96, Train loss: 1.488, Test loss: 1.566, Test accuracy: 91.72
Round  97, Train loss: 1.483, Test loss: 1.564, Test accuracy: 91.77
Round  98, Train loss: 1.539, Test loss: 1.562, Test accuracy: 91.82
Round  99, Train loss: 1.545, Test loss: 1.565, Test accuracy: 91.65
Final Round, Train loss: 1.526, Test loss: 1.560, Test accuracy: 92.02
Average accuracy final 10 rounds: 91.55166666666666
546.6309554576874
[0.7583909034729004, 1.4095795154571533, 2.0357861518859863, 2.6425013542175293, 3.2492127418518066, 3.8603618144989014, 4.482156753540039, 5.108623504638672, 5.729499578475952, 6.328318357467651, 6.947869539260864, 7.575791597366333, 8.2005455493927, 8.835094690322876, 9.443353414535522, 10.056640148162842, 10.680541038513184, 11.314345359802246, 11.928759574890137, 12.541599035263062, 13.16331934928894, 13.79828429222107, 14.417268514633179, 15.030408143997192, 15.648568153381348, 16.265137910842896, 16.85419511795044, 17.467886686325073, 18.092115879058838, 18.71192216873169, 19.345490217208862, 19.94608783721924, 20.565276384353638, 21.197402477264404, 21.82179021835327, 22.42439293861389, 23.0261709690094, 23.655540466308594, 24.299586296081543, 24.93047022819519, 25.54320502281189, 26.163805723190308, 26.78951096534729, 27.414361476898193, 28.02393078804016, 28.625158548355103, 29.222763299942017, 29.828725576400757, 30.433166980743408, 31.02551245689392, 31.6219425201416, 32.23457431793213, 32.852707862854004, 33.46615171432495, 34.05968165397644, 34.642760276794434, 35.25441312789917, 35.86384439468384, 36.454256772994995, 37.073423862457275, 37.70261907577515, 38.33874225616455, 38.97796273231506, 39.58853507041931, 40.191566467285156, 40.808473110198975, 41.436137199401855, 42.05797719955444, 42.67965602874756, 43.315590620040894, 43.944318532943726, 44.569945096969604, 45.19727897644043, 45.78834271430969, 46.41263389587402, 47.05937480926514, 47.68330955505371, 48.29141116142273, 48.91371822357178, 49.543808937072754, 50.17001295089722, 50.78225111961365, 51.408815145492554, 52.02319002151489, 52.647857427597046, 53.27233695983887, 53.89717745780945, 54.522743463516235, 55.15958333015442, 55.779130935668945, 56.39512252807617, 57.017884731292725, 57.63832664489746, 58.25391244888306, 58.872631549835205, 59.48058724403381, 60.08661150932312, 60.6923143863678, 61.31179451942444, 61.90448331832886, 62.89216947555542]
[13.666666666666666, 16.333333333333332, 15.733333333333333, 17.583333333333332, 19.35, 22.233333333333334, 28.783333333333335, 30.866666666666667, 41.516666666666666, 47.06666666666667, 51.61666666666667, 59.0, 62.5, 66.56666666666666, 68.58333333333333, 73.66666666666667, 72.73333333333333, 73.5, 74.26666666666667, 74.0, 74.4, 73.4, 75.01666666666667, 74.98333333333333, 76.18333333333334, 77.03333333333333, 77.23333333333333, 78.41666666666667, 78.65, 81.0, 80.75, 82.01666666666667, 82.08333333333333, 83.45, 83.4, 84.63333333333334, 86.18333333333334, 86.86666666666666, 88.45, 88.86666666666666, 88.63333333333334, 88.78333333333333, 88.25, 87.76666666666667, 87.96666666666667, 88.7, 88.78333333333333, 89.25, 88.65, 89.15, 89.53333333333333, 89.26666666666667, 88.98333333333333, 89.1, 88.75, 88.9, 88.9, 89.18333333333334, 88.23333333333333, 88.4, 89.78333333333333, 89.56666666666666, 89.73333333333333, 89.63333333333334, 89.6, 89.6, 89.83333333333333, 89.51666666666667, 89.05, 89.63333333333334, 91.0, 90.88333333333334, 91.35, 91.45, 91.35, 91.61666666666666, 91.76666666666667, 91.01666666666667, 91.21666666666667, 91.48333333333333, 91.28333333333333, 91.21666666666667, 91.58333333333333, 91.63333333333334, 91.4, 91.4, 90.61666666666666, 91.31666666666666, 91.3, 91.48333333333333, 91.45, 91.55, 91.4, 91.75, 90.81666666666666, 91.6, 91.71666666666667, 91.76666666666667, 91.81666666666666, 91.65, 92.01666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: mnist  

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
401408
401920
532992
533248
549632
549696
550336
550346
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.313, Test loss: 2.300, Test accuracy: 10.35
Round   1, Train loss: 2.302, Test loss: 2.294, Test accuracy: 14.93
Round   2, Train loss: 2.275, Test loss: 2.279, Test accuracy: 19.58
Round   3, Train loss: 2.186, Test loss: 2.250, Test accuracy: 14.90
Round   4, Train loss: 2.124, Test loss: 2.212, Test accuracy: 23.70
Round   5, Train loss: 2.099, Test loss: 2.180, Test accuracy: 30.02
Round   6, Train loss: 2.043, Test loss: 2.144, Test accuracy: 34.58
Round   7, Train loss: 2.016, Test loss: 2.116, Test accuracy: 36.42
Round   8, Train loss: 2.042, Test loss: 2.079, Test accuracy: 39.92
Round   9, Train loss: 1.951, Test loss: 2.030, Test accuracy: 47.32
Round  10, Train loss: 1.889, Test loss: 2.001, Test accuracy: 50.73
Round  11, Train loss: 1.836, Test loss: 1.941, Test accuracy: 56.95
Round  12, Train loss: 1.741, Test loss: 1.902, Test accuracy: 61.10
Round  13, Train loss: 1.747, Test loss: 1.878, Test accuracy: 63.67
Round  14, Train loss: 1.790, Test loss: 1.852, Test accuracy: 68.35
Round  15, Train loss: 1.676, Test loss: 1.832, Test accuracy: 71.52
Round  16, Train loss: 1.670, Test loss: 1.818, Test accuracy: 71.10
Round  17, Train loss: 1.794, Test loss: 1.810, Test accuracy: 72.27
Round  18, Train loss: 1.667, Test loss: 1.787, Test accuracy: 74.73
Round  19, Train loss: 1.636, Test loss: 1.783, Test accuracy: 75.93
Round  20, Train loss: 1.800, Test loss: 1.770, Test accuracy: 75.05
Round  21, Train loss: 1.645, Test loss: 1.771, Test accuracy: 76.12
Round  22, Train loss: 1.679, Test loss: 1.750, Test accuracy: 77.78
Round  23, Train loss: 1.721, Test loss: 1.712, Test accuracy: 81.28
Round  24, Train loss: 1.683, Test loss: 1.697, Test accuracy: 82.00
Round  25, Train loss: 1.654, Test loss: 1.675, Test accuracy: 84.60
Round  26, Train loss: 1.559, Test loss: 1.672, Test accuracy: 84.65
Round  27, Train loss: 1.618, Test loss: 1.662, Test accuracy: 85.13
Round  28, Train loss: 1.597, Test loss: 1.661, Test accuracy: 85.60
Round  29, Train loss: 1.716, Test loss: 1.665, Test accuracy: 86.12
Round  30, Train loss: 1.670, Test loss: 1.653, Test accuracy: 86.17
Round  31, Train loss: 1.603, Test loss: 1.650, Test accuracy: 86.55
Round  32, Train loss: 1.587, Test loss: 1.648, Test accuracy: 86.60
Round  33, Train loss: 1.598, Test loss: 1.643, Test accuracy: 86.35
Round  34, Train loss: 1.693, Test loss: 1.634, Test accuracy: 86.82
Round  35, Train loss: 1.593, Test loss: 1.636, Test accuracy: 86.13
Round  36, Train loss: 1.646, Test loss: 1.625, Test accuracy: 88.20
Round  37, Train loss: 1.538, Test loss: 1.620, Test accuracy: 88.77
Round  38, Train loss: 1.624, Test loss: 1.620, Test accuracy: 88.97
Round  39, Train loss: 1.588, Test loss: 1.617, Test accuracy: 88.75
Round  40, Train loss: 1.636, Test loss: 1.615, Test accuracy: 88.60
Round  41, Train loss: 1.643, Test loss: 1.608, Test accuracy: 90.45
Round  42, Train loss: 1.563, Test loss: 1.602, Test accuracy: 90.55
Round  43, Train loss: 1.587, Test loss: 1.599, Test accuracy: 90.65
Round  44, Train loss: 1.605, Test loss: 1.596, Test accuracy: 91.00
Round  45, Train loss: 1.542, Test loss: 1.593, Test accuracy: 90.63
Round  46, Train loss: 1.546, Test loss: 1.585, Test accuracy: 92.10
Round  47, Train loss: 1.527, Test loss: 1.581, Test accuracy: 92.20
Round  48, Train loss: 1.559, Test loss: 1.579, Test accuracy: 92.33
Round  49, Train loss: 1.593, Test loss: 1.568, Test accuracy: 93.85
Round  50, Train loss: 1.563, Test loss: 1.574, Test accuracy: 93.55
Round  51, Train loss: 1.523, Test loss: 1.567, Test accuracy: 93.90
Round  52, Train loss: 1.509, Test loss: 1.569, Test accuracy: 93.80
Round  53, Train loss: 1.519, Test loss: 1.567, Test accuracy: 93.67
Round  54, Train loss: 1.517, Test loss: 1.566, Test accuracy: 93.62
Round  55, Train loss: 1.524, Test loss: 1.562, Test accuracy: 93.85
Round  56, Train loss: 1.557, Test loss: 1.566, Test accuracy: 93.82
Round  57, Train loss: 1.508, Test loss: 1.563, Test accuracy: 94.08
Round  58, Train loss: 1.524, Test loss: 1.558, Test accuracy: 94.15
Round  59, Train loss: 1.510, Test loss: 1.557, Test accuracy: 94.47
Round  60, Train loss: 1.553, Test loss: 1.559, Test accuracy: 94.23
Round  61, Train loss: 1.511, Test loss: 1.559, Test accuracy: 94.27
Round  62, Train loss: 1.514, Test loss: 1.555, Test accuracy: 94.45
Round  63, Train loss: 1.507, Test loss: 1.555, Test accuracy: 94.38
Round  64, Train loss: 1.520, Test loss: 1.552, Test accuracy: 94.17
Round  65, Train loss: 1.562, Test loss: 1.555, Test accuracy: 94.13
Round  66, Train loss: 1.562, Test loss: 1.558, Test accuracy: 93.98
Round  67, Train loss: 1.549, Test loss: 1.555, Test accuracy: 94.25
Round  68, Train loss: 1.516, Test loss: 1.549, Test accuracy: 94.48
Round  69, Train loss: 1.513, Test loss: 1.548, Test accuracy: 94.47
Round  70, Train loss: 1.512, Test loss: 1.547, Test accuracy: 94.55
Round  71, Train loss: 1.503, Test loss: 1.548, Test accuracy: 94.53
Round  72, Train loss: 1.506, Test loss: 1.545, Test accuracy: 94.57
Round  73, Train loss: 1.505, Test loss: 1.545, Test accuracy: 94.77
Round  74, Train loss: 1.505, Test loss: 1.545, Test accuracy: 94.87
Round  75, Train loss: 1.508, Test loss: 1.547, Test accuracy: 94.20
Round  76, Train loss: 1.510, Test loss: 1.544, Test accuracy: 94.57
Round  77, Train loss: 1.541, Test loss: 1.543, Test accuracy: 94.43
Round  78, Train loss: 1.495, Test loss: 1.545, Test accuracy: 94.53
Round  79, Train loss: 1.506, Test loss: 1.543, Test accuracy: 94.47
Round  80, Train loss: 1.501, Test loss: 1.543, Test accuracy: 94.50
Round  81, Train loss: 1.495, Test loss: 1.543, Test accuracy: 94.55
Round  82, Train loss: 1.500, Test loss: 1.543, Test accuracy: 94.40
Round  83, Train loss: 1.498, Test loss: 1.543, Test accuracy: 94.43
Round  84, Train loss: 1.496, Test loss: 1.543, Test accuracy: 94.55
Round  85, Train loss: 1.520, Test loss: 1.532, Test accuracy: 96.18
Round  86, Train loss: 1.494, Test loss: 1.534, Test accuracy: 96.17
Round  87, Train loss: 1.491, Test loss: 1.535, Test accuracy: 95.98
Round  88, Train loss: 1.499, Test loss: 1.534, Test accuracy: 96.28
Round  89, Train loss: 1.498, Test loss: 1.534, Test accuracy: 95.97
Round  90, Train loss: 1.505, Test loss: 1.528, Test accuracy: 96.07
Round  91, Train loss: 1.498, Test loss: 1.527, Test accuracy: 96.15
Round  92, Train loss: 1.503, Test loss: 1.528, Test accuracy: 96.07
Round  93, Train loss: 1.496, Test loss: 1.525, Test accuracy: 96.40/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  94, Train loss: 1.492, Test loss: 1.526, Test accuracy: 96.38
Round  95, Train loss: 1.492, Test loss: 1.527, Test accuracy: 96.33
Round  96, Train loss: 1.503, Test loss: 1.525, Test accuracy: 96.23
Round  97, Train loss: 1.490, Test loss: 1.526, Test accuracy: 96.00
Round  98, Train loss: 1.495, Test loss: 1.528, Test accuracy: 95.92
Round  99, Train loss: 1.490, Test loss: 1.527, Test accuracy: 96.22
Final Round, Train loss: 1.482, Test loss: 1.520, Test accuracy: 96.30
Average accuracy final 10 rounds: 96.17666666666666
698.0526659488678
[0.7714250087738037, 1.5428500175476074, 2.191967725753784, 2.841085433959961, 3.5078177452087402, 4.1745500564575195, 4.804221868515015, 5.43389368057251, 6.047889709472656, 6.661885738372803, 7.294785022735596, 7.927684307098389, 8.537760257720947, 9.147836208343506, 9.755785942077637, 10.363735675811768, 10.988716840744019, 11.61369800567627, 12.230944871902466, 12.848191738128662, 13.459673404693604, 14.071155071258545, 14.680619716644287, 15.29008436203003, 15.868130683898926, 16.446177005767822, 17.04197120666504, 17.637765407562256, 18.229824542999268, 18.82188367843628, 19.429730892181396, 20.037578105926514, 20.639339447021484, 21.241100788116455, 21.809298038482666, 22.377495288848877, 22.988815307617188, 23.600135326385498, 24.178302526474, 24.7564697265625, 25.336151838302612, 25.915833950042725, 26.49224019050598, 27.06864643096924, 27.671831369400024, 28.27501630783081, 28.86211585998535, 29.449215412139893, 30.029374599456787, 30.60953378677368, 31.214513301849365, 31.81949281692505, 32.4036386013031, 32.98778438568115, 33.57984781265259, 34.17191123962402, 34.77447581291199, 35.37704038619995, 35.982067584991455, 36.58709478378296, 37.193119525909424, 37.79914426803589, 38.397690773010254, 38.99623727798462, 39.61287879943848, 40.229520320892334, 40.82366704940796, 41.417813777923584, 42.02363991737366, 42.62946605682373, 43.234853982925415, 43.8402419090271, 44.453967332839966, 45.06769275665283, 45.67238402366638, 46.27707529067993, 46.86618781089783, 47.45530033111572, 48.08645939826965, 48.717618465423584, 49.295528411865234, 49.873438358306885, 50.446303606033325, 51.019168853759766, 51.63379979133606, 52.24843072891235, 52.8784019947052, 53.50837326049805, 54.1326687335968, 54.75696420669556, 55.35331869125366, 55.94967317581177, 56.58853363990784, 57.227394104003906, 57.79096961021423, 58.35454511642456, 58.95293474197388, 59.55132436752319, 60.14866638183594, 60.74600839614868, 61.3764283657074, 62.00684833526611, 62.61769104003906, 63.22853374481201, 63.83549952507019, 64.44246530532837, 65.03921914100647, 65.63597297668457, 66.23983144760132, 66.84368991851807, 67.44913077354431, 68.05457162857056, 68.68264031410217, 69.31070899963379, 69.9303834438324, 70.550057888031, 71.15284252166748, 71.75562715530396, 72.32523155212402, 72.89483594894409, 73.52146029472351, 74.14808464050293, 74.75564384460449, 75.36320304870605, 75.97033619880676, 76.57746934890747, 77.203768491745, 77.83006763458252, 78.46256685256958, 79.09506607055664, 79.70829772949219, 80.32152938842773, 80.91495108604431, 81.50837278366089, 82.1223783493042, 82.73638391494751, 83.31235313415527, 83.88832235336304, 84.49527764320374, 85.10223293304443, 85.72291660308838, 86.34360027313232, 86.96274781227112, 87.58189535140991, 88.1824746131897, 88.78305387496948, 89.39368629455566, 90.00431871414185, 90.63329029083252, 91.2622618675232, 91.84616804122925, 92.4300742149353, 93.02337765693665, 93.61668109893799, 94.23150777816772, 94.84633445739746, 95.46506786346436, 96.08380126953125, 96.69571948051453, 97.3076376914978, 97.90102410316467, 98.49441051483154, 99.11758637428284, 99.74076223373413, 100.31506371498108, 100.88936519622803, 101.54520273208618, 102.20104026794434, 102.84827780723572, 103.4955153465271, 104.14878630638123, 104.80205726623535, 105.39330840110779, 105.98455953598022, 106.57257986068726, 107.16060018539429, 107.74677300453186, 108.33294582366943, 108.89468193054199, 109.45641803741455, 110.03469395637512, 110.6129698753357, 111.21389532089233, 111.81482076644897, 112.43479323387146, 113.05476570129395, 113.67122459411621, 114.28768348693848, 114.88180685043335, 115.47593021392822, 116.08467173576355, 116.69341325759888, 117.28844881057739, 117.88348436355591, 118.47972106933594, 119.07595777511597, 119.69340801239014, 120.3108582496643, 120.93064188957214, 121.55042552947998, 122.52136826515198, 123.49231100082397]
[10.35, 10.35, 14.933333333333334, 14.933333333333334, 19.583333333333332, 19.583333333333332, 14.9, 14.9, 23.7, 23.7, 30.016666666666666, 30.016666666666666, 34.583333333333336, 34.583333333333336, 36.416666666666664, 36.416666666666664, 39.916666666666664, 39.916666666666664, 47.31666666666667, 47.31666666666667, 50.733333333333334, 50.733333333333334, 56.95, 56.95, 61.1, 61.1, 63.666666666666664, 63.666666666666664, 68.35, 68.35, 71.51666666666667, 71.51666666666667, 71.1, 71.1, 72.26666666666667, 72.26666666666667, 74.73333333333333, 74.73333333333333, 75.93333333333334, 75.93333333333334, 75.05, 75.05, 76.11666666666666, 76.11666666666666, 77.78333333333333, 77.78333333333333, 81.28333333333333, 81.28333333333333, 82.0, 82.0, 84.6, 84.6, 84.65, 84.65, 85.13333333333334, 85.13333333333334, 85.6, 85.6, 86.11666666666666, 86.11666666666666, 86.16666666666667, 86.16666666666667, 86.55, 86.55, 86.6, 86.6, 86.35, 86.35, 86.81666666666666, 86.81666666666666, 86.13333333333334, 86.13333333333334, 88.2, 88.2, 88.76666666666667, 88.76666666666667, 88.96666666666667, 88.96666666666667, 88.75, 88.75, 88.6, 88.6, 90.45, 90.45, 90.55, 90.55, 90.65, 90.65, 91.0, 91.0, 90.63333333333334, 90.63333333333334, 92.1, 92.1, 92.2, 92.2, 92.33333333333333, 92.33333333333333, 93.85, 93.85, 93.55, 93.55, 93.9, 93.9, 93.8, 93.8, 93.66666666666667, 93.66666666666667, 93.61666666666666, 93.61666666666666, 93.85, 93.85, 93.81666666666666, 93.81666666666666, 94.08333333333333, 94.08333333333333, 94.15, 94.15, 94.46666666666667, 94.46666666666667, 94.23333333333333, 94.23333333333333, 94.26666666666667, 94.26666666666667, 94.45, 94.45, 94.38333333333334, 94.38333333333334, 94.16666666666667, 94.16666666666667, 94.13333333333334, 94.13333333333334, 93.98333333333333, 93.98333333333333, 94.25, 94.25, 94.48333333333333, 94.48333333333333, 94.46666666666667, 94.46666666666667, 94.55, 94.55, 94.53333333333333, 94.53333333333333, 94.56666666666666, 94.56666666666666, 94.76666666666667, 94.76666666666667, 94.86666666666666, 94.86666666666666, 94.2, 94.2, 94.56666666666666, 94.56666666666666, 94.43333333333334, 94.43333333333334, 94.53333333333333, 94.53333333333333, 94.46666666666667, 94.46666666666667, 94.5, 94.5, 94.55, 94.55, 94.4, 94.4, 94.43333333333334, 94.43333333333334, 94.55, 94.55, 96.18333333333334, 96.18333333333334, 96.16666666666667, 96.16666666666667, 95.98333333333333, 95.98333333333333, 96.28333333333333, 96.28333333333333, 95.96666666666667, 95.96666666666667, 96.06666666666666, 96.06666666666666, 96.15, 96.15, 96.06666666666666, 96.06666666666666, 96.4, 96.4, 96.38333333333334, 96.38333333333334, 96.33333333333333, 96.33333333333333, 96.23333333333333, 96.23333333333333, 96.0, 96.0, 95.91666666666667, 95.91666666666667, 96.21666666666667, 96.21666666666667, 96.3, 96.3]
