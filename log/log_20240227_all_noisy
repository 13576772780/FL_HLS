nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.191, Test loss: 2.050, Test accuracy: 25.01 

Round   0, Global train loss: 2.191, Global test loss: 2.042, Global test accuracy: 25.88 

Round   1, Train loss: 2.002, Test loss: 1.926, Test accuracy: 28.95 

Round   1, Global train loss: 2.002, Global test loss: 1.863, Global test accuracy: 31.64 

Round   2, Train loss: 1.899, Test loss: 1.872, Test accuracy: 31.32 

Round   2, Global train loss: 1.899, Global test loss: 1.773, Global test accuracy: 36.37 

Round   3, Train loss: 1.768, Test loss: 1.838, Test accuracy: 32.45 

Round   3, Global train loss: 1.768, Global test loss: 1.669, Global test accuracy: 39.53 

Round   4, Train loss: 1.697, Test loss: 1.836, Test accuracy: 32.20 

Round   4, Global train loss: 1.697, Global test loss: 1.607, Global test accuracy: 41.11 

Round   5, Train loss: 1.813, Test loss: 1.810, Test accuracy: 33.12 

Round   5, Global train loss: 1.813, Global test loss: 1.734, Global test accuracy: 37.30 

Round   6, Train loss: 1.679, Test loss: 1.797, Test accuracy: 33.76 

Round   6, Global train loss: 1.679, Global test loss: 1.619, Global test accuracy: 40.68 

Round   7, Train loss: 1.609, Test loss: 1.790, Test accuracy: 34.34 

Round   7, Global train loss: 1.609, Global test loss: 1.581, Global test accuracy: 42.85 

Round   8, Train loss: 1.582, Test loss: 1.771, Test accuracy: 35.27 

Round   8, Global train loss: 1.582, Global test loss: 1.585, Global test accuracy: 41.81 

Round   9, Train loss: 1.553, Test loss: 1.771, Test accuracy: 35.45 

Round   9, Global train loss: 1.553, Global test loss: 1.576, Global test accuracy: 42.56 

Round  10, Train loss: 1.590, Test loss: 1.768, Test accuracy: 35.91 

Round  10, Global train loss: 1.590, Global test loss: 1.634, Global test accuracy: 42.01 

Round  11, Train loss: 1.456, Test loss: 1.769, Test accuracy: 35.98 

Round  11, Global train loss: 1.456, Global test loss: 1.562, Global test accuracy: 43.30 

Round  12, Train loss: 1.512, Test loss: 1.746, Test accuracy: 36.85 

Round  12, Global train loss: 1.512, Global test loss: 1.596, Global test accuracy: 42.70 

Round  13, Train loss: 1.404, Test loss: 1.745, Test accuracy: 37.37 

Round  13, Global train loss: 1.404, Global test loss: 1.494, Global test accuracy: 44.55 

Round  14, Train loss: 1.491, Test loss: 1.725, Test accuracy: 38.36 

Round  14, Global train loss: 1.491, Global test loss: 1.559, Global test accuracy: 43.48 

Round  15, Train loss: 1.338, Test loss: 1.748, Test accuracy: 38.09 

Round  15, Global train loss: 1.338, Global test loss: 1.521, Global test accuracy: 43.90 

Round  16, Train loss: 1.264, Test loss: 1.781, Test accuracy: 37.66 

Round  16, Global train loss: 1.264, Global test loss: 1.506, Global test accuracy: 44.89 

Round  17, Train loss: 1.261, Test loss: 1.775, Test accuracy: 38.10 

Round  17, Global train loss: 1.261, Global test loss: 1.487, Global test accuracy: 45.13 

Round  18, Train loss: 1.107, Test loss: 1.787, Test accuracy: 38.78 

Round  18, Global train loss: 1.107, Global test loss: 1.499, Global test accuracy: 46.04 

Round  19, Train loss: 1.298, Test loss: 1.787, Test accuracy: 39.03 

Round  19, Global train loss: 1.298, Global test loss: 1.546, Global test accuracy: 43.50 

Round  20, Train loss: 1.276, Test loss: 1.785, Test accuracy: 39.36 

Round  20, Global train loss: 1.276, Global test loss: 1.509, Global test accuracy: 45.45 

Round  21, Train loss: 1.193, Test loss: 1.802, Test accuracy: 39.54 

Round  21, Global train loss: 1.193, Global test loss: 1.534, Global test accuracy: 44.38 

Round  22, Train loss: 1.106, Test loss: 1.817, Test accuracy: 39.80 

Round  22, Global train loss: 1.106, Global test loss: 1.450, Global test accuracy: 47.04 

Round  23, Train loss: 1.062, Test loss: 1.828, Test accuracy: 40.24 

Round  23, Global train loss: 1.062, Global test loss: 1.434, Global test accuracy: 47.98 

Round  24, Train loss: 1.251, Test loss: 1.841, Test accuracy: 40.13 

Round  24, Global train loss: 1.251, Global test loss: 1.546, Global test accuracy: 43.80 

Round  25, Train loss: 1.204, Test loss: 1.851, Test accuracy: 40.21 

Round  25, Global train loss: 1.204, Global test loss: 1.488, Global test accuracy: 46.00 

Round  26, Train loss: 0.896, Test loss: 1.892, Test accuracy: 40.23 

Round  26, Global train loss: 0.896, Global test loss: 1.495, Global test accuracy: 46.38 

Round  27, Train loss: 1.021, Test loss: 1.923, Test accuracy: 40.19 

Round  27, Global train loss: 1.021, Global test loss: 1.471, Global test accuracy: 46.61 

Round  28, Train loss: 1.146, Test loss: 1.947, Test accuracy: 40.14 

Round  28, Global train loss: 1.146, Global test loss: 1.501, Global test accuracy: 45.50 

Round  29, Train loss: 1.051, Test loss: 1.979, Test accuracy: 40.23 

Round  29, Global train loss: 1.051, Global test loss: 1.455, Global test accuracy: 47.85 

Round  30, Train loss: 0.838, Test loss: 2.011, Test accuracy: 40.01 

Round  30, Global train loss: 0.838, Global test loss: 1.449, Global test accuracy: 47.55 

Round  31, Train loss: 0.963, Test loss: 2.021, Test accuracy: 40.09 

Round  31, Global train loss: 0.963, Global test loss: 1.472, Global test accuracy: 48.24 

Round  32, Train loss: 0.793, Test loss: 2.049, Test accuracy: 40.11 

Round  32, Global train loss: 0.793, Global test loss: 1.462, Global test accuracy: 47.78 

Round  33, Train loss: 1.056, Test loss: 2.080, Test accuracy: 40.22 

Round  33, Global train loss: 1.056, Global test loss: 1.471, Global test accuracy: 46.61 

Round  34, Train loss: 0.835, Test loss: 2.103, Test accuracy: 40.73 

Round  34, Global train loss: 0.835, Global test loss: 1.447, Global test accuracy: 48.04 

Round  35, Train loss: 0.842, Test loss: 2.150, Test accuracy: 40.99 

Round  35, Global train loss: 0.842, Global test loss: 1.477, Global test accuracy: 47.67 

Round  36, Train loss: 0.694, Test loss: 2.154, Test accuracy: 41.34 

Round  36, Global train loss: 0.694, Global test loss: 1.464, Global test accuracy: 48.59 

Round  37, Train loss: 0.631, Test loss: 2.218, Test accuracy: 41.23 

Round  37, Global train loss: 0.631, Global test loss: 1.485, Global test accuracy: 48.22 

Round  38, Train loss: 0.787, Test loss: 2.270, Test accuracy: 40.99 

Round  38, Global train loss: 0.787, Global test loss: 1.478, Global test accuracy: 47.66 

Round  39, Train loss: 0.632, Test loss: 2.315, Test accuracy: 40.67 

Round  39, Global train loss: 0.632, Global test loss: 1.501, Global test accuracy: 47.07 

Round  40, Train loss: 0.720, Test loss: 2.365, Test accuracy: 40.75 

Round  40, Global train loss: 0.720, Global test loss: 1.493, Global test accuracy: 47.13 

Round  41, Train loss: 0.516, Test loss: 2.423, Test accuracy: 40.22 

Round  41, Global train loss: 0.516, Global test loss: 1.518, Global test accuracy: 48.54 

Round  42, Train loss: 0.616, Test loss: 2.420, Test accuracy: 40.64 

Round  42, Global train loss: 0.616, Global test loss: 1.547, Global test accuracy: 47.00 

Round  43, Train loss: 0.548, Test loss: 2.482, Test accuracy: 40.91 

Round  43, Global train loss: 0.548, Global test loss: 1.561, Global test accuracy: 46.85 

Round  44, Train loss: 0.695, Test loss: 2.474, Test accuracy: 41.37 

Round  44, Global train loss: 0.695, Global test loss: 1.497, Global test accuracy: 47.34 

Round  45, Train loss: 0.683, Test loss: 2.514, Test accuracy: 41.47 

Round  45, Global train loss: 0.683, Global test loss: 1.512, Global test accuracy: 46.09 

Round  46, Train loss: 0.552, Test loss: 2.549, Test accuracy: 41.14 

Round  46, Global train loss: 0.552, Global test loss: 1.483, Global test accuracy: 47.30 

Round  47, Train loss: 0.609, Test loss: 2.573, Test accuracy: 41.54 

Round  47, Global train loss: 0.609, Global test loss: 1.482, Global test accuracy: 48.66 

Round  48, Train loss: 0.580, Test loss: 2.607, Test accuracy: 41.52 

Round  48, Global train loss: 0.580, Global test loss: 1.541, Global test accuracy: 47.40 

Round  49, Train loss: 0.451, Test loss: 2.663, Test accuracy: 41.40 

Round  49, Global train loss: 0.451, Global test loss: 1.493, Global test accuracy: 49.44 

Round  50, Train loss: 0.453, Test loss: 2.719, Test accuracy: 40.96 

Round  50, Global train loss: 0.453, Global test loss: 1.516, Global test accuracy: 47.18 

Round  51, Train loss: 0.364, Test loss: 2.712, Test accuracy: 40.85 

Round  51, Global train loss: 0.364, Global test loss: 1.589, Global test accuracy: 46.45 

Round  52, Train loss: 0.422, Test loss: 2.819, Test accuracy: 40.45 

Round  52, Global train loss: 0.422, Global test loss: 1.605, Global test accuracy: 45.52 

Round  53, Train loss: 0.570, Test loss: 2.873, Test accuracy: 40.44 

Round  53, Global train loss: 0.570, Global test loss: 1.490, Global test accuracy: 47.84 

Round  54, Train loss: 0.457, Test loss: 2.900, Test accuracy: 40.79 

Round  54, Global train loss: 0.457, Global test loss: 1.549, Global test accuracy: 47.22 

Round  55, Train loss: 0.409, Test loss: 2.925, Test accuracy: 40.96 

Round  55, Global train loss: 0.409, Global test loss: 1.575, Global test accuracy: 46.97 

Round  56, Train loss: 0.369, Test loss: 2.951, Test accuracy: 41.33 

Round  56, Global train loss: 0.369, Global test loss: 1.567, Global test accuracy: 46.73 

Round  57, Train loss: 0.366, Test loss: 2.944, Test accuracy: 41.50 

Round  57, Global train loss: 0.366, Global test loss: 1.526, Global test accuracy: 49.29 

Round  58, Train loss: 0.459, Test loss: 2.959, Test accuracy: 41.20 

Round  58, Global train loss: 0.459, Global test loss: 1.592, Global test accuracy: 45.96 

Round  59, Train loss: 0.371, Test loss: 2.987, Test accuracy: 40.92 

Round  59, Global train loss: 0.371, Global test loss: 1.550, Global test accuracy: 46.70 

Round  60, Train loss: 0.390, Test loss: 3.001, Test accuracy: 40.91 

Round  60, Global train loss: 0.390, Global test loss: 1.530, Global test accuracy: 47.28 

Round  61, Train loss: 0.289, Test loss: 3.009, Test accuracy: 41.27 

Round  61, Global train loss: 0.289, Global test loss: 1.574, Global test accuracy: 47.65 

Round  62, Train loss: 0.425, Test loss: 3.040, Test accuracy: 41.48 

Round  62, Global train loss: 0.425, Global test loss: 1.584, Global test accuracy: 48.71 

Round  63, Train loss: 0.300, Test loss: 3.075, Test accuracy: 41.73 

Round  63, Global train loss: 0.300, Global test loss: 1.569, Global test accuracy: 47.10 

Round  64, Train loss: 0.402, Test loss: 3.104, Test accuracy: 41.27 

Round  64, Global train loss: 0.402, Global test loss: 1.615, Global test accuracy: 45.96 

Round  65, Train loss: 0.334, Test loss: 3.155, Test accuracy: 41.30 

Round  65, Global train loss: 0.334, Global test loss: 1.595, Global test accuracy: 49.66 

Round  66, Train loss: 0.304, Test loss: 3.209, Test accuracy: 41.37 

Round  66, Global train loss: 0.304, Global test loss: 1.604, Global test accuracy: 48.11 

Round  67, Train loss: 0.396, Test loss: 3.237, Test accuracy: 41.44 

Round  67, Global train loss: 0.396, Global test loss: 1.532, Global test accuracy: 48.78 

Round  68, Train loss: 0.276, Test loss: 3.321, Test accuracy: 41.50 

Round  68, Global train loss: 0.276, Global test loss: 1.606, Global test accuracy: 47.77 

Round  69, Train loss: 0.255, Test loss: 3.293, Test accuracy: 41.80 

Round  69, Global train loss: 0.255, Global test loss: 1.631, Global test accuracy: 47.55 

Round  70, Train loss: 0.250, Test loss: 3.304, Test accuracy: 41.48 

Round  70, Global train loss: 0.250, Global test loss: 1.600, Global test accuracy: 48.61 

Round  71, Train loss: 0.257, Test loss: 3.328, Test accuracy: 41.51 

Round  71, Global train loss: 0.257, Global test loss: 1.537, Global test accuracy: 48.31 

Round  72, Train loss: 0.287, Test loss: 3.376, Test accuracy: 41.66 

Round  72, Global train loss: 0.287, Global test loss: 1.586, Global test accuracy: 46.55 

Round  73, Train loss: 0.256, Test loss: 3.467, Test accuracy: 41.51 

Round  73, Global train loss: 0.256, Global test loss: 1.594, Global test accuracy: 49.33 

Round  74, Train loss: 0.215, Test loss: 3.451, Test accuracy: 41.63 

Round  74, Global train loss: 0.215, Global test loss: 1.621, Global test accuracy: 47.26 

Round  75, Train loss: 0.205, Test loss: 3.562, Test accuracy: 41.33 

Round  75, Global train loss: 0.205, Global test loss: 1.612, Global test accuracy: 47.13 

Round  76, Train loss: 0.249, Test loss: 3.550, Test accuracy: 41.55 

Round  76, Global train loss: 0.249, Global test loss: 1.604, Global test accuracy: 46.87 

Round  77, Train loss: 0.232, Test loss: 3.591, Test accuracy: 41.48 

Round  77, Global train loss: 0.232, Global test loss: 1.691, Global test accuracy: 45.65 

Round  78, Train loss: 0.217, Test loss: 3.626, Test accuracy: 41.62 

Round  78, Global train loss: 0.217, Global test loss: 1.600, Global test accuracy: 48.46 

Round  79, Train loss: 0.200, Test loss: 3.582, Test accuracy: 41.73 

Round  79, Global train loss: 0.200, Global test loss: 1.733, Global test accuracy: 47.37 

Round  80, Train loss: 0.256, Test loss: 3.577, Test accuracy: 41.78 

Round  80, Global train loss: 0.256, Global test loss: 1.636, Global test accuracy: 46.78 

Round  81, Train loss: 0.219, Test loss: 3.651, Test accuracy: 41.46 

Round  81, Global train loss: 0.219, Global test loss: 1.612, Global test accuracy: 47.15 

Round  82, Train loss: 0.257, Test loss: 3.705, Test accuracy: 41.29 

Round  82, Global train loss: 0.257, Global test loss: 1.670, Global test accuracy: 46.58 

Round  83, Train loss: 0.194, Test loss: 3.720, Test accuracy: 41.50 

Round  83, Global train loss: 0.194, Global test loss: 1.718, Global test accuracy: 47.49 

Round  84, Train loss: 0.206, Test loss: 3.711, Test accuracy: 41.55 

Round  84, Global train loss: 0.206, Global test loss: 1.669, Global test accuracy: 47.44 

Round  85, Train loss: 0.224, Test loss: 3.658, Test accuracy: 41.88 

Round  85, Global train loss: 0.224, Global test loss: 1.648, Global test accuracy: 47.27 

Round  86, Train loss: 0.188, Test loss: 3.735, Test accuracy: 41.67 

Round  86, Global train loss: 0.188, Global test loss: 1.623, Global test accuracy: 49.13 

Round  87, Train loss: 0.182, Test loss: 3.768, Test accuracy: 41.79 

Round  87, Global train loss: 0.182, Global test loss: 1.681, Global test accuracy: 47.13 

Round  88, Train loss: 0.245, Test loss: 3.830, Test accuracy: 41.71 

Round  88, Global train loss: 0.245, Global test loss: 1.628, Global test accuracy: 46.01 

Round  89, Train loss: 0.195, Test loss: 3.943, Test accuracy: 41.32 

Round  89, Global train loss: 0.195, Global test loss: 1.639, Global test accuracy: 46.26 

Round  90, Train loss: 0.200, Test loss: 3.923, Test accuracy: 41.28 

Round  90, Global train loss: 0.200, Global test loss: 1.657, Global test accuracy: 47.96 

Round  91, Train loss: 0.168, Test loss: 3.866, Test accuracy: 41.37 

Round  91, Global train loss: 0.168, Global test loss: 1.590, Global test accuracy: 46.38 

Round  92, Train loss: 0.167, Test loss: 3.906, Test accuracy: 41.27 

Round  92, Global train loss: 0.167, Global test loss: 1.691, Global test accuracy: 46.05 

Round  93, Train loss: 0.193, Test loss: 3.917, Test accuracy: 41.30 

Round  93, Global train loss: 0.193, Global test loss: 1.605, Global test accuracy: 49.19 

Round  94, Train loss: 0.188, Test loss: 3.937, Test accuracy: 41.60 

Round  94, Global train loss: 0.188, Global test loss: 1.615, Global test accuracy: 46.35 

Round  95, Train loss: 0.195, Test loss: 3.992, Test accuracy: 41.50 

Round  95, Global train loss: 0.195, Global test loss: 1.721, Global test accuracy: 46.42 

Round  96, Train loss: 0.195, Test loss: 4.003, Test accuracy: 41.69 

Round  96, Global train loss: 0.195, Global test loss: 1.692, Global test accuracy: 48.53 

Round  97, Train loss: 0.175, Test loss: 3.993, Test accuracy: 41.74 

Round  97, Global train loss: 0.175, Global test loss: 1.663, Global test accuracy: 48.51 

Round  98, Train loss: 0.154, Test loss: 4.027, Test accuracy: 41.59 

Round  98, Global train loss: 0.154, Global test loss: 1.701, Global test accuracy: 47.55 

Round  99, Train loss: 0.172, Test loss: 4.011, Test accuracy: 41.75 

Round  99, Global train loss: 0.172, Global test loss: 1.650, Global test accuracy: 45.27 

Final Round, Train loss: 0.152, Test loss: 4.144, Test accuracy: 41.98 

Final Round, Global train loss: 0.152, Global test loss: 1.650, Global test accuracy: 45.27 

Average accuracy final 10 rounds: 41.509 

Average global accuracy final 10 rounds: 47.2225 

2720.0121109485626
[1.4430246353149414, 2.6411259174346924, 3.8411319255828857, 5.0383827686309814, 6.236248970031738, 7.434859037399292, 8.6263747215271, 9.815443515777588, 11.008826494216919, 12.206361055374146, 13.400795936584473, 14.594072103500366, 15.763777494430542, 16.933435201644897, 18.107783317565918, 19.287601947784424, 20.471577167510986, 21.653663873672485, 22.82861590385437, 24.0065975189209, 25.18656587600708, 26.35455632209778, 27.525788068771362, 28.700339555740356, 29.876591682434082, 31.051866054534912, 32.233808755874634, 33.413718700408936, 34.59909772872925, 35.78049683570862, 36.95870661735535, 38.13662815093994, 39.312376499176025, 40.4980742931366, 41.678455114364624, 42.85081243515015, 44.02859687805176, 45.20444631576538, 46.378559589385986, 47.543784379959106, 48.71143817901611, 49.885605812072754, 51.05821919441223, 52.231995582580566, 53.401684284210205, 54.57003140449524, 55.74027729034424, 56.90864610671997, 58.08109998703003, 59.25033640861511, 60.43029713630676, 61.60419249534607, 62.776737689971924, 63.94550085067749, 65.11293125152588, 66.28647947311401, 67.45482134819031, 68.62369108200073, 69.79543852806091, 70.9614851474762, 72.12827777862549, 73.2975504398346, 74.47231841087341, 75.6421434879303, 76.8170371055603, 77.98631834983826, 79.16527247428894, 80.33508324623108, 81.50167679786682, 82.66970562934875, 83.83944201469421, 85.00981330871582, 86.19262957572937, 87.20433330535889, 88.2169017791748, 89.23329329490662, 90.24777317047119, 91.26022243499756, 92.27270197868347, 93.28485035896301, 94.29595971107483, 95.30775189399719, 96.31880569458008, 97.33212637901306, 98.343989610672, 99.36004614830017, 100.38769125938416, 101.40282225608826, 102.41780257225037, 103.4307062625885, 104.44442486763, 105.47224140167236, 106.49145936965942, 107.50359559059143, 108.51932907104492, 109.53541898727417, 110.55607080459595, 111.57471084594727, 112.59219694137573, 113.60762166976929, 115.642906665802]
[25.0125, 28.955, 31.32, 32.4475, 32.195, 33.125, 33.7625, 34.3375, 35.2675, 35.455, 35.91, 35.975, 36.85, 37.3675, 38.3575, 38.085, 37.655, 38.1025, 38.7825, 39.03, 39.36, 39.5425, 39.805, 40.24, 40.13, 40.2125, 40.225, 40.19, 40.1375, 40.225, 40.0075, 40.09, 40.11, 40.2175, 40.73, 40.995, 41.335, 41.225, 40.9875, 40.675, 40.75, 40.2225, 40.6375, 40.9075, 41.37, 41.465, 41.14, 41.5425, 41.52, 41.395, 40.96, 40.8475, 40.4525, 40.4425, 40.79, 40.96, 41.325, 41.5, 41.195, 40.9175, 40.905, 41.2725, 41.4775, 41.73, 41.2675, 41.3, 41.3675, 41.4425, 41.4975, 41.8025, 41.48, 41.5075, 41.665, 41.5075, 41.63, 41.3325, 41.5475, 41.4775, 41.6175, 41.73, 41.7825, 41.46, 41.29, 41.5025, 41.5475, 41.8775, 41.6725, 41.79, 41.7075, 41.32, 41.28, 41.365, 41.2725, 41.305, 41.5975, 41.4975, 41.6925, 41.745, 41.5875, 41.7475, 41.9825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.159, Test loss: 2.085, Test accuracy: 27.23 

Round   0, Global train loss: 1.159, Global test loss: 2.368, Global test accuracy: 19.02 

Round   1, Train loss: 0.881, Test loss: 1.859, Test accuracy: 37.84 

Round   1, Global train loss: 0.881, Global test loss: 2.293, Global test accuracy: 27.36 

Round   2, Train loss: 0.819, Test loss: 1.047, Test accuracy: 57.31 

Round   2, Global train loss: 0.819, Global test loss: 1.943, Global test accuracy: 28.78 

Round   3, Train loss: 0.749, Test loss: 0.961, Test accuracy: 61.58 

Round   3, Global train loss: 0.749, Global test loss: 2.327, Global test accuracy: 27.08 

Round   4, Train loss: 0.764, Test loss: 0.865, Test accuracy: 64.82 

Round   4, Global train loss: 0.764, Global test loss: 2.019, Global test accuracy: 31.82 

Round   5, Train loss: 0.645, Test loss: 0.853, Test accuracy: 66.48 

Round   5, Global train loss: 0.645, Global test loss: 2.085, Global test accuracy: 35.24 

Round   6, Train loss: 0.782, Test loss: 0.683, Test accuracy: 70.43 

Round   6, Global train loss: 0.782, Global test loss: 1.889, Global test accuracy: 33.73 

Round   7, Train loss: 0.749, Test loss: 0.667, Test accuracy: 71.29 

Round   7, Global train loss: 0.749, Global test loss: 1.766, Global test accuracy: 41.14 

Round   8, Train loss: 0.661, Test loss: 0.640, Test accuracy: 72.86 

Round   8, Global train loss: 0.661, Global test loss: 1.824, Global test accuracy: 39.17 

Round   9, Train loss: 0.638, Test loss: 0.645, Test accuracy: 72.44 

Round   9, Global train loss: 0.638, Global test loss: 1.935, Global test accuracy: 41.43 

Round  10, Train loss: 0.635, Test loss: 0.635, Test accuracy: 73.01 

Round  10, Global train loss: 0.635, Global test loss: 1.796, Global test accuracy: 43.72 

Round  11, Train loss: 0.650, Test loss: 0.617, Test accuracy: 73.96 

Round  11, Global train loss: 0.650, Global test loss: 2.007, Global test accuracy: 35.28 

Round  12, Train loss: 0.552, Test loss: 0.599, Test accuracy: 74.91 

Round  12, Global train loss: 0.552, Global test loss: 1.839, Global test accuracy: 47.84 

Round  13, Train loss: 0.614, Test loss: 0.597, Test accuracy: 75.42 

Round  13, Global train loss: 0.614, Global test loss: 1.698, Global test accuracy: 40.37 

Round  14, Train loss: 0.544, Test loss: 0.585, Test accuracy: 75.76 

Round  14, Global train loss: 0.544, Global test loss: 1.625, Global test accuracy: 42.49 

Round  15, Train loss: 0.558, Test loss: 0.579, Test accuracy: 75.93 

Round  15, Global train loss: 0.558, Global test loss: 1.813, Global test accuracy: 40.40 

Round  16, Train loss: 0.600, Test loss: 0.571, Test accuracy: 76.68 

Round  16, Global train loss: 0.600, Global test loss: 1.625, Global test accuracy: 44.81 

Round  17, Train loss: 0.604, Test loss: 0.561, Test accuracy: 77.17 

Round  17, Global train loss: 0.604, Global test loss: 1.609, Global test accuracy: 48.26 

Round  18, Train loss: 0.531, Test loss: 0.551, Test accuracy: 77.55 

Round  18, Global train loss: 0.531, Global test loss: 1.604, Global test accuracy: 43.41 

Round  19, Train loss: 0.552, Test loss: 0.551, Test accuracy: 77.54 

Round  19, Global train loss: 0.552, Global test loss: 1.597, Global test accuracy: 49.68 

Round  20, Train loss: 0.492, Test loss: 0.539, Test accuracy: 78.11 

Round  20, Global train loss: 0.492, Global test loss: 1.533, Global test accuracy: 48.64 

Round  21, Train loss: 0.497, Test loss: 0.544, Test accuracy: 78.16 

Round  21, Global train loss: 0.497, Global test loss: 1.587, Global test accuracy: 47.35 

Round  22, Train loss: 0.469, Test loss: 0.539, Test accuracy: 78.36 

Round  22, Global train loss: 0.469, Global test loss: 1.711, Global test accuracy: 48.12 

Round  23, Train loss: 0.557, Test loss: 0.550, Test accuracy: 77.94 

Round  23, Global train loss: 0.557, Global test loss: 1.543, Global test accuracy: 48.36 

Round  24, Train loss: 0.535, Test loss: 0.544, Test accuracy: 78.42 

Round  24, Global train loss: 0.535, Global test loss: 1.525, Global test accuracy: 47.87 

Round  25, Train loss: 0.480, Test loss: 0.530, Test accuracy: 78.87 

Round  25, Global train loss: 0.480, Global test loss: 1.475, Global test accuracy: 50.11 

Round  26, Train loss: 0.485, Test loss: 0.528, Test accuracy: 79.09 

Round  26, Global train loss: 0.485, Global test loss: 1.532, Global test accuracy: 47.94 

Round  27, Train loss: 0.441, Test loss: 0.513, Test accuracy: 79.69 

Round  27, Global train loss: 0.441, Global test loss: 1.418, Global test accuracy: 50.40 

Round  28, Train loss: 0.490, Test loss: 0.518, Test accuracy: 79.36 

Round  28, Global train loss: 0.490, Global test loss: 1.420, Global test accuracy: 51.08 

Round  29, Train loss: 0.562, Test loss: 0.524, Test accuracy: 79.36 

Round  29, Global train loss: 0.562, Global test loss: 1.372, Global test accuracy: 52.54 

Round  30, Train loss: 0.397, Test loss: 0.522, Test accuracy: 79.58 

Round  30, Global train loss: 0.397, Global test loss: 1.386, Global test accuracy: 53.09 

Round  31, Train loss: 0.400, Test loss: 0.516, Test accuracy: 79.67 

Round  31, Global train loss: 0.400, Global test loss: 1.506, Global test accuracy: 51.06 

Round  32, Train loss: 0.390, Test loss: 0.516, Test accuracy: 79.68 

Round  32, Global train loss: 0.390, Global test loss: 1.420, Global test accuracy: 48.14 

Round  33, Train loss: 0.489, Test loss: 0.508, Test accuracy: 80.08 

Round  33, Global train loss: 0.489, Global test loss: 1.438, Global test accuracy: 51.60 

Round  34, Train loss: 0.446, Test loss: 0.521, Test accuracy: 79.61 

Round  34, Global train loss: 0.446, Global test loss: 1.361, Global test accuracy: 54.11 

Round  35, Train loss: 0.394, Test loss: 0.513, Test accuracy: 80.03 

Round  35, Global train loss: 0.394, Global test loss: 1.375, Global test accuracy: 52.94 

Round  36, Train loss: 0.466, Test loss: 0.532, Test accuracy: 79.64 

Round  36, Global train loss: 0.466, Global test loss: 1.362, Global test accuracy: 53.16 

Round  37, Train loss: 0.456, Test loss: 0.513, Test accuracy: 80.18 

Round  37, Global train loss: 0.456, Global test loss: 1.542, Global test accuracy: 49.48 

Round  38, Train loss: 0.451, Test loss: 0.509, Test accuracy: 80.39 

Round  38, Global train loss: 0.451, Global test loss: 1.298, Global test accuracy: 56.58 

Round  39, Train loss: 0.407, Test loss: 0.514, Test accuracy: 80.11 

Round  39, Global train loss: 0.407, Global test loss: 1.692, Global test accuracy: 44.14 

Round  40, Train loss: 0.428, Test loss: 0.527, Test accuracy: 79.94 

Round  40, Global train loss: 0.428, Global test loss: 1.301, Global test accuracy: 55.96 

Round  41, Train loss: 0.359, Test loss: 0.532, Test accuracy: 79.67 

Round  41, Global train loss: 0.359, Global test loss: 1.435, Global test accuracy: 53.61 

Round  42, Train loss: 0.368, Test loss: 0.530, Test accuracy: 79.53 

Round  42, Global train loss: 0.368, Global test loss: 1.577, Global test accuracy: 50.67 

Round  43, Train loss: 0.370, Test loss: 0.507, Test accuracy: 80.19 

Round  43, Global train loss: 0.370, Global test loss: 1.672, Global test accuracy: 52.85 

Round  44, Train loss: 0.405, Test loss: 0.505, Test accuracy: 80.42 

Round  44, Global train loss: 0.405, Global test loss: 1.679, Global test accuracy: 46.04 

Round  45, Train loss: 0.343, Test loss: 0.517, Test accuracy: 80.44 

Round  45, Global train loss: 0.343, Global test loss: 1.497, Global test accuracy: 50.00 

Round  46, Train loss: 0.380, Test loss: 0.528, Test accuracy: 80.39 

Round  46, Global train loss: 0.380, Global test loss: 1.478, Global test accuracy: 52.76 

Round  47, Train loss: 0.316, Test loss: 0.537, Test accuracy: 80.14 

Round  47, Global train loss: 0.316, Global test loss: 1.214, Global test accuracy: 58.58 

Round  48, Train loss: 0.316, Test loss: 0.534, Test accuracy: 80.38 

Round  48, Global train loss: 0.316, Global test loss: 1.395, Global test accuracy: 52.13 

Round  49, Train loss: 0.339, Test loss: 0.534, Test accuracy: 80.35 

Round  49, Global train loss: 0.339, Global test loss: 1.430, Global test accuracy: 55.14 

Round  50, Train loss: 0.252, Test loss: 0.527, Test accuracy: 80.74 

Round  50, Global train loss: 0.252, Global test loss: 1.434, Global test accuracy: 52.12 

Round  51, Train loss: 0.368, Test loss: 0.523, Test accuracy: 80.93 

Round  51, Global train loss: 0.368, Global test loss: 1.493, Global test accuracy: 55.52 

Round  52, Train loss: 0.399, Test loss: 0.522, Test accuracy: 81.05 

Round  52, Global train loss: 0.399, Global test loss: 1.494, Global test accuracy: 52.02 

Round  53, Train loss: 0.363, Test loss: 0.524, Test accuracy: 81.05 

Round  53, Global train loss: 0.363, Global test loss: 1.297, Global test accuracy: 56.54 

Round  54, Train loss: 0.325, Test loss: 0.530, Test accuracy: 81.40 

Round  54, Global train loss: 0.325, Global test loss: 1.662, Global test accuracy: 49.77 

Round  55, Train loss: 0.369, Test loss: 0.519, Test accuracy: 81.75 

Round  55, Global train loss: 0.369, Global test loss: 1.229, Global test accuracy: 59.66 

Round  56, Train loss: 0.351, Test loss: 0.502, Test accuracy: 82.12 

Round  56, Global train loss: 0.351, Global test loss: 1.312, Global test accuracy: 56.61 

Round  57, Train loss: 0.323, Test loss: 0.512, Test accuracy: 82.00 

Round  57, Global train loss: 0.323, Global test loss: 1.469, Global test accuracy: 54.32 

Round  58, Train loss: 0.272, Test loss: 0.506, Test accuracy: 82.30 

Round  58, Global train loss: 0.272, Global test loss: 1.378, Global test accuracy: 55.51 

Round  59, Train loss: 0.288, Test loss: 0.514, Test accuracy: 82.06 

Round  59, Global train loss: 0.288, Global test loss: 1.316, Global test accuracy: 59.58 

Round  60, Train loss: 0.304, Test loss: 0.510, Test accuracy: 82.19 

Round  60, Global train loss: 0.304, Global test loss: 1.438, Global test accuracy: 56.12 

Round  61, Train loss: 0.296, Test loss: 0.522, Test accuracy: 81.82 

Round  61, Global train loss: 0.296, Global test loss: 1.533, Global test accuracy: 54.49 

Round  62, Train loss: 0.273, Test loss: 0.555, Test accuracy: 81.03 

Round  62, Global train loss: 0.273, Global test loss: 1.746, Global test accuracy: 50.96 

Round  63, Train loss: 0.292, Test loss: 0.528, Test accuracy: 81.91 

Round  63, Global train loss: 0.292, Global test loss: 1.443, Global test accuracy: 53.51 

Round  64, Train loss: 0.208, Test loss: 0.520, Test accuracy: 82.18 

Round  64, Global train loss: 0.208, Global test loss: 1.521, Global test accuracy: 57.24 

Round  65, Train loss: 0.284, Test loss: 0.513, Test accuracy: 82.22 

Round  65, Global train loss: 0.284, Global test loss: 1.326, Global test accuracy: 58.96 

Round  66, Train loss: 0.244, Test loss: 0.516, Test accuracy: 81.96 

Round  66, Global train loss: 0.244, Global test loss: 1.699, Global test accuracy: 52.51 

Round  67, Train loss: 0.292, Test loss: 0.520, Test accuracy: 81.84 

Round  67, Global train loss: 0.292, Global test loss: 1.486, Global test accuracy: 53.42 

Round  68, Train loss: 0.335, Test loss: 0.503, Test accuracy: 82.59 

Round  68, Global train loss: 0.335, Global test loss: 1.354, Global test accuracy: 56.80 

Round  69, Train loss: 0.313, Test loss: 0.503, Test accuracy: 82.54 

Round  69, Global train loss: 0.313, Global test loss: 1.416, Global test accuracy: 56.44 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 682, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.120, Test loss: 1.849, Test accuracy: 27.32 

Round   0, Global train loss: 1.120, Global test loss: 2.221, Global test accuracy: 17.63 

Round   1, Train loss: 0.956, Test loss: 1.537, Test accuracy: 37.11 

Round   1, Global train loss: 0.956, Global test loss: 2.266, Global test accuracy: 18.50 

Round   2, Train loss: 0.842, Test loss: 1.182, Test accuracy: 52.64 

Round   2, Global train loss: 0.842, Global test loss: 1.977, Global test accuracy: 29.95 

Round   3, Train loss: 0.827, Test loss: 1.055, Test accuracy: 55.71 

Round   3, Global train loss: 0.827, Global test loss: 2.331, Global test accuracy: 23.41 

Round   4, Train loss: 0.853, Test loss: 0.917, Test accuracy: 60.55 

Round   4, Global train loss: 0.853, Global test loss: 1.849, Global test accuracy: 32.20 

Round   5, Train loss: 0.783, Test loss: 0.788, Test accuracy: 64.03 

Round   5, Global train loss: 0.783, Global test loss: 1.841, Global test accuracy: 34.08 

Round   6, Train loss: 0.775, Test loss: 0.734, Test accuracy: 66.92 

Round   6, Global train loss: 0.775, Global test loss: 1.991, Global test accuracy: 34.49 

Round   7, Train loss: 0.767, Test loss: 0.741, Test accuracy: 66.99 

Round   7, Global train loss: 0.767, Global test loss: 1.812, Global test accuracy: 36.50 

Round   8, Train loss: 0.691, Test loss: 0.689, Test accuracy: 69.41 

Round   8, Global train loss: 0.691, Global test loss: 1.764, Global test accuracy: 33.66 

Round   9, Train loss: 0.671, Test loss: 0.687, Test accuracy: 69.77 

Round   9, Global train loss: 0.671, Global test loss: 1.650, Global test accuracy: 40.23 

Round  10, Train loss: 0.710, Test loss: 0.686, Test accuracy: 69.53 

Round  10, Global train loss: 0.710, Global test loss: 1.631, Global test accuracy: 42.60 

Round  11, Train loss: 0.654, Test loss: 0.667, Test accuracy: 70.91 

Round  11, Global train loss: 0.654, Global test loss: 1.720, Global test accuracy: 39.48 

Round  12, Train loss: 0.674, Test loss: 0.657, Test accuracy: 71.78 

Round  12, Global train loss: 0.674, Global test loss: 1.705, Global test accuracy: 37.46 

Round  13, Train loss: 0.595, Test loss: 0.658, Test accuracy: 72.07 

Round  13, Global train loss: 0.595, Global test loss: 1.943, Global test accuracy: 36.78 

Round  14, Train loss: 0.652, Test loss: 0.636, Test accuracy: 72.87 

Round  14, Global train loss: 0.652, Global test loss: 1.702, Global test accuracy: 41.46 

Round  15, Train loss: 0.576, Test loss: 0.630, Test accuracy: 73.03 

Round  15, Global train loss: 0.576, Global test loss: 1.519, Global test accuracy: 45.98 

Round  16, Train loss: 0.639, Test loss: 0.623, Test accuracy: 73.38 

Round  16, Global train loss: 0.639, Global test loss: 1.792, Global test accuracy: 33.08 

Round  17, Train loss: 0.660, Test loss: 0.617, Test accuracy: 73.77 

Round  17, Global train loss: 0.660, Global test loss: 2.097, Global test accuracy: 28.60 

Round  18, Train loss: 0.717, Test loss: 0.602, Test accuracy: 74.84 

Round  18, Global train loss: 0.717, Global test loss: 1.868, Global test accuracy: 38.41 

Round  19, Train loss: 0.566, Test loss: 0.599, Test accuracy: 74.92 

Round  19, Global train loss: 0.566, Global test loss: 1.446, Global test accuracy: 48.56 

Round  20, Train loss: 0.529, Test loss: 0.596, Test accuracy: 75.12 

Round  20, Global train loss: 0.529, Global test loss: 1.485, Global test accuracy: 45.97 

Round  21, Train loss: 0.621, Test loss: 0.597, Test accuracy: 75.20 

Round  21, Global train loss: 0.621, Global test loss: 1.637, Global test accuracy: 45.12 

Round  22, Train loss: 0.576, Test loss: 0.609, Test accuracy: 74.66 

Round  22, Global train loss: 0.576, Global test loss: 1.618, Global test accuracy: 42.01 

Round  23, Train loss: 0.503, Test loss: 0.592, Test accuracy: 75.41 

Round  23, Global train loss: 0.503, Global test loss: 1.659, Global test accuracy: 41.38 

Round  24, Train loss: 0.702, Test loss: 0.591, Test accuracy: 75.35 

Round  24, Global train loss: 0.702, Global test loss: 1.447, Global test accuracy: 48.22 

Round  25, Train loss: 0.535, Test loss: 0.621, Test accuracy: 74.33 

Round  25, Global train loss: 0.535, Global test loss: 1.501, Global test accuracy: 47.08 

Round  26, Train loss: 0.557, Test loss: 0.594, Test accuracy: 75.12 

Round  26, Global train loss: 0.557, Global test loss: 1.349, Global test accuracy: 52.43 

Round  27, Train loss: 0.520, Test loss: 0.583, Test accuracy: 75.70 

Round  27, Global train loss: 0.520, Global test loss: 1.517, Global test accuracy: 48.76 

Round  28, Train loss: 0.599, Test loss: 0.583, Test accuracy: 75.81 

Round  28, Global train loss: 0.599, Global test loss: 1.500, Global test accuracy: 47.31 

Round  29, Train loss: 0.592, Test loss: 0.571, Test accuracy: 76.12 

Round  29, Global train loss: 0.592, Global test loss: 1.353, Global test accuracy: 50.90 

Round  30, Train loss: 0.517, Test loss: 0.557, Test accuracy: 77.12 

Round  30, Global train loss: 0.517, Global test loss: 1.419, Global test accuracy: 49.71 

Round  31, Train loss: 0.601, Test loss: 0.546, Test accuracy: 77.63 

Round  31, Global train loss: 0.601, Global test loss: 1.359, Global test accuracy: 50.37 

Round  32, Train loss: 0.467, Test loss: 0.542, Test accuracy: 78.16 

Round  32, Global train loss: 0.467, Global test loss: 1.532, Global test accuracy: 47.23 

Round  33, Train loss: 0.478, Test loss: 0.539, Test accuracy: 78.39 

Round  33, Global train loss: 0.478, Global test loss: 1.466, Global test accuracy: 47.86 

Round  34, Train loss: 0.545, Test loss: 0.548, Test accuracy: 77.93 

Round  34, Global train loss: 0.545, Global test loss: 1.353, Global test accuracy: 51.57 

Round  35, Train loss: 0.447, Test loss: 0.552, Test accuracy: 77.69 

Round  35, Global train loss: 0.447, Global test loss: 1.832, Global test accuracy: 44.94 

Round  36, Train loss: 0.608, Test loss: 0.551, Test accuracy: 77.44 

Round  36, Global train loss: 0.608, Global test loss: 1.312, Global test accuracy: 53.76 

Round  37, Train loss: 0.453, Test loss: 0.549, Test accuracy: 77.58 

Round  37, Global train loss: 0.453, Global test loss: 1.305, Global test accuracy: 54.48 

Round  38, Train loss: 0.450, Test loss: 0.546, Test accuracy: 78.00 

Round  38, Global train loss: 0.450, Global test loss: 1.434, Global test accuracy: 51.09 

Round  39, Train loss: 0.551, Test loss: 0.537, Test accuracy: 78.38 

Round  39, Global train loss: 0.551, Global test loss: 1.386, Global test accuracy: 52.30 

Round  40, Train loss: 0.446, Test loss: 0.551, Test accuracy: 78.17 

Round  40, Global train loss: 0.446, Global test loss: 1.441, Global test accuracy: 50.60 

Round  41, Train loss: 0.458, Test loss: 0.555, Test accuracy: 78.36 

Round  41, Global train loss: 0.458, Global test loss: 1.503, Global test accuracy: 49.92 

Round  42, Train loss: 0.457, Test loss: 0.569, Test accuracy: 78.19 

Round  42, Global train loss: 0.457, Global test loss: 1.349, Global test accuracy: 52.76 

Round  43, Train loss: 0.396, Test loss: 0.559, Test accuracy: 78.25 

Round  43, Global train loss: 0.396, Global test loss: 1.341, Global test accuracy: 53.09 

Round  44, Train loss: 0.475, Test loss: 0.557, Test accuracy: 78.15 

Round  44, Global train loss: 0.475, Global test loss: 1.258, Global test accuracy: 55.02 

Round  45, Train loss: 0.462, Test loss: 0.547, Test accuracy: 78.64 

Round  45, Global train loss: 0.462, Global test loss: 1.240, Global test accuracy: 56.18 

Round  46, Train loss: 0.426, Test loss: 0.550, Test accuracy: 78.71 

Round  46, Global train loss: 0.426, Global test loss: 1.522, Global test accuracy: 51.33 

Round  47, Train loss: 0.481, Test loss: 0.549, Test accuracy: 78.86 

Round  47, Global train loss: 0.481, Global test loss: 1.534, Global test accuracy: 48.27 

Round  48, Train loss: 0.402, Test loss: 0.564, Test accuracy: 78.18 

Round  48, Global train loss: 0.402, Global test loss: 1.744, Global test accuracy: 45.76 

Round  49, Train loss: 0.431, Test loss: 0.548, Test accuracy: 78.71 

Round  49, Global train loss: 0.431, Global test loss: 1.367, Global test accuracy: 54.83 

Round  50, Train loss: 0.443, Test loss: 0.537, Test accuracy: 78.96 

Round  50, Global train loss: 0.443, Global test loss: 1.261, Global test accuracy: 55.81 

Round  51, Train loss: 0.421, Test loss: 0.539, Test accuracy: 79.05 

Round  51, Global train loss: 0.421, Global test loss: 1.529, Global test accuracy: 47.58 

Round  52, Train loss: 0.360, Test loss: 0.542, Test accuracy: 79.01 

Round  52, Global train loss: 0.360, Global test loss: 1.551, Global test accuracy: 49.56 

Round  53, Train loss: 0.456, Test loss: 0.541, Test accuracy: 79.06 

Round  53, Global train loss: 0.456, Global test loss: 1.257, Global test accuracy: 57.03 

Round  54, Train loss: 0.356, Test loss: 0.540, Test accuracy: 79.08 

Round  54, Global train loss: 0.356, Global test loss: 1.398, Global test accuracy: 55.98 

Round  55, Train loss: 0.312, Test loss: 0.542, Test accuracy: 79.26 

Round  55, Global train loss: 0.312, Global test loss: 1.505, Global test accuracy: 54.12 

Round  56, Train loss: 0.421, Test loss: 0.542, Test accuracy: 79.28 

Round  56, Global train loss: 0.421, Global test loss: 1.561, Global test accuracy: 50.16 

Round  57, Train loss: 0.384, Test loss: 0.536, Test accuracy: 79.33 

Round  57, Global train loss: 0.384, Global test loss: 1.437, Global test accuracy: 53.58 

Round  58, Train loss: 0.377, Test loss: 0.526, Test accuracy: 79.62 

Round  58, Global train loss: 0.377, Global test loss: 1.566, Global test accuracy: 49.52 

Round  59, Train loss: 0.476, Test loss: 0.525, Test accuracy: 80.06 

Round  59, Global train loss: 0.476, Global test loss: 1.371, Global test accuracy: 53.49 

Round  60, Train loss: 0.480, Test loss: 0.532, Test accuracy: 79.73 

Round  60, Global train loss: 0.480, Global test loss: 1.385, Global test accuracy: 51.79 

Round  61, Train loss: 0.347, Test loss: 0.553, Test accuracy: 79.42 

Round  61, Global train loss: 0.347, Global test loss: 1.692, Global test accuracy: 50.74 

Round  62, Train loss: 0.315, Test loss: 0.539, Test accuracy: 79.72 

Round  62, Global train loss: 0.315, Global test loss: 1.457, Global test accuracy: 53.75 

Round  63, Train loss: 0.349, Test loss: 0.546, Test accuracy: 79.28 

Round  63, Global train loss: 0.349, Global test loss: 1.327, Global test accuracy: 56.68 

Round  64, Train loss: 0.355, Test loss: 0.549, Test accuracy: 79.26 

Round  64, Global train loss: 0.355, Global test loss: 1.457, Global test accuracy: 55.11 

Round  65, Train loss: 0.325, Test loss: 0.562, Test accuracy: 78.93 

Round  65, Global train loss: 0.325, Global test loss: 1.509, Global test accuracy: 51.95 

Round  66, Train loss: 0.450, Test loss: 0.552, Test accuracy: 79.27 

Round  66, Global train loss: 0.450, Global test loss: 1.277, Global test accuracy: 56.45 

Round  67, Train loss: 0.451, Test loss: 0.551, Test accuracy: 79.56 

Round  67, Global train loss: 0.451, Global test loss: 1.329, Global test accuracy: 55.08 

Round  68, Train loss: 0.475, Test loss: 0.563, Test accuracy: 79.38 

Round  68, Global train loss: 0.475, Global test loss: 1.539, Global test accuracy: 50.46 

Round  69, Train loss: 0.383, Test loss: 0.548, Test accuracy: 80.05 

Round  69, Global train loss: 0.383, Global test loss: 1.376, Global test accuracy: 55.37 

Round  70, Train loss: 0.258, Test loss: 0.553, Test accuracy: 79.99 

Round  70, Global train loss: 0.258, Global test loss: 1.291, Global test accuracy: 58.43 

Round  71, Train loss: 0.343, Test loss: 0.564, Test accuracy: 79.81 

Round  71, Global train loss: 0.343, Global test loss: 1.301, Global test accuracy: 55.83 

Round  72, Train loss: 0.382, Test loss: 0.573, Test accuracy: 79.41 

Round  72, Global train loss: 0.382, Global test loss: 1.629, Global test accuracy: 48.22 

Round  73, Train loss: 0.365, Test loss: 0.546, Test accuracy: 80.14 

Round  73, Global train loss: 0.365, Global test loss: 1.266, Global test accuracy: 55.80 

Round  74, Train loss: 0.342, Test loss: 0.542, Test accuracy: 80.31 

Round  74, Global train loss: 0.342, Global test loss: 1.266, Global test accuracy: 57.38 

Round  75, Train loss: 0.352, Test loss: 0.539, Test accuracy: 80.58 

Round  75, Global train loss: 0.352, Global test loss: 1.214, Global test accuracy: 59.27 

Round  76, Train loss: 0.400, Test loss: 0.557, Test accuracy: 80.33 

Round  76, Global train loss: 0.400, Global test loss: 1.584, Global test accuracy: 51.34 

Round  77, Train loss: 0.361, Test loss: 0.555, Test accuracy: 80.47 

Round  77, Global train loss: 0.361, Global test loss: 1.431, Global test accuracy: 54.62 

Round  78, Train loss: 0.294, Test loss: 0.576, Test accuracy: 79.88 

Round  78, Global train loss: 0.294, Global test loss: 1.469, Global test accuracy: 53.53 

Round  79, Train loss: 0.305, Test loss: 0.582, Test accuracy: 79.75 

Round  79, Global train loss: 0.305, Global test loss: 1.423, Global test accuracy: 53.89 

Round  80, Train loss: 0.251, Test loss: 0.562, Test accuracy: 80.36 

Round  80, Global train loss: 0.251, Global test loss: 1.411, Global test accuracy: 55.24 

Round  81, Train loss: 0.353, Test loss: 0.556, Test accuracy: 80.42 

Round  81, Global train loss: 0.353, Global test loss: 1.281, Global test accuracy: 57.11 

Round  82, Train loss: 0.282, Test loss: 0.557, Test accuracy: 80.58 

Round  82, Global train loss: 0.282, Global test loss: 1.317, Global test accuracy: 59.58 

Round  83, Train loss: 0.345, Test loss: 0.580, Test accuracy: 79.96 

Round  83, Global train loss: 0.345, Global test loss: 1.203, Global test accuracy: 59.56 

Round  84, Train loss: 0.340, Test loss: 0.574, Test accuracy: 80.24 

Round  84, Global train loss: 0.340, Global test loss: 1.711, Global test accuracy: 48.59 

Round  85, Train loss: 0.317, Test loss: 0.592, Test accuracy: 79.81 

Round  85, Global train loss: 0.317, Global test loss: 1.251, Global test accuracy: 59.67 

Round  86, Train loss: 0.290, Test loss: 0.589, Test accuracy: 79.95 

Round  86, Global train loss: 0.290, Global test loss: 1.574, Global test accuracy: 51.19 

Round  87, Train loss: 0.311, Test loss: 0.570, Test accuracy: 80.66 

Round  87, Global train loss: 0.311, Global test loss: 1.199, Global test accuracy: 60.84 

Round  88, Train loss: 0.312, Test loss: 0.567, Test accuracy: 80.74 

Round  88, Global train loss: 0.312, Global test loss: 1.277, Global test accuracy: 58.74 

Round  89, Train loss: 0.321, Test loss: 0.567, Test accuracy: 80.79 

Round  89, Global train loss: 0.321, Global test loss: 1.420, Global test accuracy: 55.54 

Round  90, Train loss: 0.273, Test loss: 0.555, Test accuracy: 81.09 

Round  90, Global train loss: 0.273, Global test loss: 1.193, Global test accuracy: 60.65 

Round  91, Train loss: 0.318, Test loss: 0.557, Test accuracy: 81.04 

Round  91, Global train loss: 0.318, Global test loss: 1.160, Global test accuracy: 60.67 

Round  92, Train loss: 0.277, Test loss: 0.576, Test accuracy: 80.62 

Round  92, Global train loss: 0.277, Global test loss: 1.207, Global test accuracy: 60.47 

Round  93, Train loss: 0.300, Test loss: 0.563, Test accuracy: 81.14 
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  93, Global train loss: 0.300, Global test loss: 1.324, Global test accuracy: 58.29 

Round  94, Train loss: 0.241, Test loss: 0.568, Test accuracy: 80.90 

Round  94, Global train loss: 0.241, Global test loss: 1.330, Global test accuracy: 59.28 

Round  95, Train loss: 0.311, Test loss: 0.583, Test accuracy: 80.86 

Round  95, Global train loss: 0.311, Global test loss: 1.497, Global test accuracy: 54.12 

Round  96, Train loss: 0.347, Test loss: 0.576, Test accuracy: 80.94 

Round  96, Global train loss: 0.347, Global test loss: 1.416, Global test accuracy: 55.89 

Round  97, Train loss: 0.290, Test loss: 0.556, Test accuracy: 81.38 

Round  97, Global train loss: 0.290, Global test loss: 1.227, Global test accuracy: 60.37 

Round  98, Train loss: 0.269, Test loss: 0.560, Test accuracy: 81.17 

Round  98, Global train loss: 0.269, Global test loss: 1.426, Global test accuracy: 56.02 

Round  99, Train loss: 0.251, Test loss: 0.559, Test accuracy: 81.17 

Round  99, Global train loss: 0.251, Global test loss: 1.392, Global test accuracy: 56.51 

Final Round, Train loss: 0.240, Test loss: 0.609, Test accuracy: 80.79 

Final Round, Global train loss: 0.240, Global test loss: 1.392, Global test accuracy: 56.51 

Average accuracy final 10 rounds: 81.0325 

Average global accuracy final 10 rounds: 58.2275 

1377.9326560497284
[1.5792927742004395, 2.875380277633667, 4.183979272842407, 5.497691869735718, 6.819822311401367, 8.087236166000366, 9.40787410736084, 10.64490818977356, 11.964211463928223, 13.211131811141968, 14.4169282913208, 15.618069887161255, 16.797199487686157, 17.975306034088135, 19.251073360443115, 20.433619499206543, 21.710673332214355, 22.942397594451904, 24.16125178337097, 25.358602285385132, 26.626856327056885, 27.81697416305542, 29.02181577682495, 30.203792810440063, 31.408764600753784, 32.63149547576904, 33.82557010650635, 35.01764965057373, 36.2366156578064, 37.403050661087036, 38.61765766143799, 39.81263518333435, 41.00674104690552, 42.19772148132324, 43.502360582351685, 44.7256863117218, 45.951438426971436, 47.273029088974, 48.54951500892639, 49.82211470603943, 51.0732262134552, 52.34173583984375, 53.60551452636719, 54.80679965019226, 56.083479166030884, 57.36328959465027, 58.64258289337158, 59.876875162124634, 61.15339779853821, 62.3448646068573, 63.53134822845459, 64.73569941520691, 65.923499584198, 67.11719942092896, 68.31285262107849, 69.49862170219421, 70.68701386451721, 71.8770809173584, 73.06662464141846, 74.30231428146362, 75.49897360801697, 76.68672633171082, 77.94561409950256, 79.18453240394592, 80.27794742584229, 81.36664366722107, 82.46456956863403, 83.55682277679443, 84.65474081039429, 85.74564170837402, 86.84127950668335, 87.93620300292969, 89.03203320503235, 90.12406539916992, 91.21505045890808, 92.30990386009216, 93.4044291973114, 94.49906396865845, 95.593514919281, 96.68506407737732, 97.77883625030518, 98.87129497528076, 99.9647376537323, 101.05816674232483, 102.15245079994202, 103.2473304271698, 104.34255886077881, 105.43784523010254, 106.53416109085083, 107.63023853302002, 108.89542078971863, 110.07772159576416, 111.29285597801208, 112.39143753051758, 113.48869156837463, 114.58107590675354, 115.67564010620117, 116.77191185951233, 117.86385035514832, 118.96070432662964, 121.15151357650757]
[27.316666666666666, 37.108333333333334, 52.641666666666666, 55.708333333333336, 60.55, 64.025, 66.91666666666667, 66.99166666666666, 69.40833333333333, 69.76666666666667, 69.525, 70.90833333333333, 71.78333333333333, 72.06666666666666, 72.86666666666666, 73.03333333333333, 73.375, 73.76666666666667, 74.84166666666667, 74.91666666666667, 75.11666666666666, 75.2, 74.65833333333333, 75.40833333333333, 75.35, 74.33333333333333, 75.11666666666666, 75.7, 75.80833333333334, 76.11666666666666, 77.11666666666666, 77.63333333333334, 78.15833333333333, 78.39166666666667, 77.93333333333334, 77.69166666666666, 77.44166666666666, 77.58333333333333, 78.0, 78.375, 78.16666666666667, 78.35833333333333, 78.19166666666666, 78.25, 78.15, 78.64166666666667, 78.70833333333333, 78.85833333333333, 78.18333333333334, 78.70833333333333, 78.95833333333333, 79.05, 79.00833333333334, 79.05833333333334, 79.08333333333333, 79.25833333333334, 79.28333333333333, 79.33333333333333, 79.625, 80.05833333333334, 79.73333333333333, 79.425, 79.725, 79.275, 79.25833333333334, 78.93333333333334, 79.26666666666667, 79.55833333333334, 79.375, 80.05, 79.99166666666666, 79.80833333333334, 79.40833333333333, 80.14166666666667, 80.30833333333334, 80.575, 80.33333333333333, 80.475, 79.88333333333334, 79.75, 80.35833333333333, 80.425, 80.58333333333333, 79.95833333333333, 80.24166666666666, 79.80833333333334, 79.95, 80.65833333333333, 80.74166666666666, 80.79166666666667, 81.09166666666667, 81.04166666666667, 80.61666666666666, 81.14166666666667, 80.9, 80.85833333333333, 80.94166666666666, 81.38333333333334, 81.175, 81.175, 80.79166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  15.4700
Round 1 global test acc  22.6600
Round 2 global test acc  23.6800
Round 3 global test acc  24.8100
Round 4 global test acc  11.1100
Round 5 global test acc  15.2600
Round 6 global test acc  26.3900
Round 7 global test acc  20.0200
Round 8 global test acc  23.4100
Round 9 global test acc  22.2600
Round 10 global test acc  32.0700
Round 11 global test acc  26.3100
Round 12 global test acc  27.9100
Round 13 global test acc  33.4600
Round 14 global test acc  26.2900
Round 15 global test acc  31.1100
Round 16 global test acc  33.6000
Round 17 global test acc  29.1900
Round 18 global test acc  34.3600
Round 19 global test acc  32.1800
Round 20 global test acc  28.3000
Round 21 global test acc  34.9500
Round 22 global test acc  33.1000
Round 23 global test acc  35.5000
Round 24 global test acc  35.6600
Round 25 global test acc  36.9600
Round 26 global test acc  26.0700
Round 27 global test acc  38.9500
Round 28 global test acc  37.5900
Round 29 global test acc  40.0800
Round 30 global test acc  33.1900
Round 31 global test acc  29.4800
Round 32 global test acc  31.6000
Round 33 global test acc  28.2400
Round 34 global test acc  28.6900
Round 35 global test acc  34.8600
Round 36 global test acc  25.0400
Round 37 global test acc  36.7600
Round 38 global test acc  34.1900
Round 39 global test acc  34.4500
Round 40 global test acc  20.6800
Round 41 global test acc  33.8600
Round 42 global test acc  36.7200
Round 43 global test acc  43.0200
Round 44 global test acc  29.0900
Round 45 global test acc  34.4900
Round 46 global test acc  24.5100
Round 47 global test acc  31.9000
Round 48 global test acc  32.6800
Round 49 global test acc  30.3400
Round 50 global test acc  30.3000
Round 51 global test acc  42.8900
Round 52 global test acc  33.6300
Round 53 global test acc  36.2500
Round 54 global test acc  27.7200
Round 55 global test acc  34.2700
Round 56 global test acc  42.7900
Round 57 global test acc  39.4800
Round 58 global test acc  30.8700
Round 59 global test acc  20.1400
Round 60 global test acc  32.1100
Round 61 global test acc  33.0700
Round 62 global test acc  33.3800
Round 63 global test acc  42.2600
Round 64 global test acc  40.8600
Round 65 global test acc  43.7300
Round 66 global test acc  34.0200
Round 67 global test acc  33.8900
Round 68 global test acc  40.7100
Round 69 global test acc  30.0000
Round 70 global test acc  29.8000
Round 71 global test acc  25.7700
Round 72 global test acc  32.9000
Round 73 global test acc  40.8400
Round 74 global test acc  35.3100
Round 75 global test acc  43.0600
Round 76 global test acc  30.6200
Round 77 global test acc  35.5200
Round 78 global test acc  32.6300
Round 79 global test acc  45.0900
Round 80 global test acc  43.9600
Round 81 global test acc  41.5400
Round 82 global test acc  41.8000
Round 83 global test acc  40.2800
Round 84 global test acc  35.6500
Round 85 global test acc  32.6500
Round 86 global test acc  30.0800
Round 87 global test acc  28.3800
Round 88 global test acc  26.1500
Round 89 global test acc  25.3300
Round 90 global test acc  23.1700
Round 91 global test acc  21.4800
Round 92 global test acc  21.0600
Round 93 global test acc  20.5600
Round 94 global test acc  20.3300
Round 95 global test acc  20.3200
Round 96 global test acc  20.7400
Round 97 global test acc  20.1400
Round 98 global test acc  20.5300
Round 99 global test acc  20.0100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.562, Test loss: 2.239, Test accuracy: 19.69
Round   1, Train loss: 1.039, Test loss: 1.777, Test accuracy: 36.80
Round   2, Train loss: 0.918, Test loss: 1.542, Test accuracy: 45.92
Round   3, Train loss: 0.899, Test loss: 1.308, Test accuracy: 50.96
Round   4, Train loss: 0.758, Test loss: 1.214, Test accuracy: 56.41
Round   5, Train loss: 0.856, Test loss: 1.240, Test accuracy: 56.27
Round   6, Train loss: 0.756, Test loss: 1.111, Test accuracy: 59.09
Round   7, Train loss: 0.725, Test loss: 1.002, Test accuracy: 61.02
Round   8, Train loss: 0.710, Test loss: 1.063, Test accuracy: 61.71
Round   9, Train loss: 0.672, Test loss: 0.765, Test accuracy: 68.01
Round  10, Train loss: 0.755, Test loss: 0.662, Test accuracy: 72.11
Round  11, Train loss: 0.664, Test loss: 0.640, Test accuracy: 72.29
Round  12, Train loss: 0.660, Test loss: 0.641, Test accuracy: 72.04
Round  13, Train loss: 0.636, Test loss: 0.630, Test accuracy: 72.85
Round  14, Train loss: 0.675, Test loss: 0.620, Test accuracy: 73.57
Round  15, Train loss: 0.633, Test loss: 0.605, Test accuracy: 74.21
Round  16, Train loss: 0.610, Test loss: 0.605, Test accuracy: 74.79
Round  17, Train loss: 0.621, Test loss: 0.585, Test accuracy: 75.34
Round  18, Train loss: 0.605, Test loss: 0.572, Test accuracy: 75.61
Round  19, Train loss: 0.669, Test loss: 0.561, Test accuracy: 77.01
Round  20, Train loss: 0.677, Test loss: 0.568, Test accuracy: 77.00
Round  21, Train loss: 0.549, Test loss: 0.556, Test accuracy: 77.61
Round  22, Train loss: 0.589, Test loss: 0.539, Test accuracy: 78.38
Round  23, Train loss: 0.579, Test loss: 0.533, Test accuracy: 78.33
Round  24, Train loss: 0.634, Test loss: 0.539, Test accuracy: 78.45
Round  25, Train loss: 0.508, Test loss: 0.524, Test accuracy: 79.00
Round  26, Train loss: 0.658, Test loss: 0.529, Test accuracy: 78.67
Round  27, Train loss: 0.508, Test loss: 0.515, Test accuracy: 79.39
Round  28, Train loss: 0.489, Test loss: 0.512, Test accuracy: 79.42
Round  29, Train loss: 0.509, Test loss: 0.493, Test accuracy: 80.07
Round  30, Train loss: 0.508, Test loss: 0.495, Test accuracy: 80.03
Round  31, Train loss: 0.436, Test loss: 0.498, Test accuracy: 80.02
Round  32, Train loss: 0.474, Test loss: 0.491, Test accuracy: 80.31
Round  33, Train loss: 0.485, Test loss: 0.482, Test accuracy: 80.67
Round  34, Train loss: 0.514, Test loss: 0.483, Test accuracy: 80.86
Round  35, Train loss: 0.506, Test loss: 0.481, Test accuracy: 80.73
Round  36, Train loss: 0.480, Test loss: 0.480, Test accuracy: 80.94
Round  37, Train loss: 0.443, Test loss: 0.469, Test accuracy: 81.37
Round  38, Train loss: 0.494, Test loss: 0.473, Test accuracy: 81.02
Round  39, Train loss: 0.444, Test loss: 0.471, Test accuracy: 81.21
Round  40, Train loss: 0.479, Test loss: 0.467, Test accuracy: 81.27
Round  41, Train loss: 0.510, Test loss: 0.460, Test accuracy: 81.82
Round  42, Train loss: 0.439, Test loss: 0.449, Test accuracy: 82.17
Round  43, Train loss: 0.542, Test loss: 0.450, Test accuracy: 82.19
Round  44, Train loss: 0.528, Test loss: 0.454, Test accuracy: 82.00
Round  45, Train loss: 0.460, Test loss: 0.457, Test accuracy: 81.73
Round  46, Train loss: 0.411, Test loss: 0.446, Test accuracy: 82.30
Round  47, Train loss: 0.415, Test loss: 0.440, Test accuracy: 82.33
Round  48, Train loss: 0.385, Test loss: 0.441, Test accuracy: 82.35
Round  49, Train loss: 0.421, Test loss: 0.435, Test accuracy: 82.60
Round  50, Train loss: 0.426, Test loss: 0.427, Test accuracy: 82.99
Round  51, Train loss: 0.394, Test loss: 0.431, Test accuracy: 82.75
Round  52, Train loss: 0.508, Test loss: 0.434, Test accuracy: 82.75
Round  53, Train loss: 0.371, Test loss: 0.428, Test accuracy: 82.83
Round  54, Train loss: 0.378, Test loss: 0.428, Test accuracy: 82.80
Round  55, Train loss: 0.291, Test loss: 0.433, Test accuracy: 82.83
Round  56, Train loss: 0.411, Test loss: 0.441, Test accuracy: 82.31
Round  57, Train loss: 0.361, Test loss: 0.428, Test accuracy: 82.89
Round  58, Train loss: 0.318, Test loss: 0.426, Test accuracy: 83.04
Round  59, Train loss: 0.437, Test loss: 0.432, Test accuracy: 82.78
Round  60, Train loss: 0.327, Test loss: 0.420, Test accuracy: 83.28
Round  61, Train loss: 0.317, Test loss: 0.415, Test accuracy: 83.72
Round  62, Train loss: 0.337, Test loss: 0.419, Test accuracy: 83.57
Round  63, Train loss: 0.354, Test loss: 0.415, Test accuracy: 83.60
Round  64, Train loss: 0.367, Test loss: 0.415, Test accuracy: 83.49
Round  65, Train loss: 0.400, Test loss: 0.405, Test accuracy: 84.04
Round  66, Train loss: 0.370, Test loss: 0.412, Test accuracy: 83.68
Round  67, Train loss: 0.404, Test loss: 0.413, Test accuracy: 83.71
Round  68, Train loss: 0.401, Test loss: 0.413, Test accuracy: 83.81
Round  69, Train loss: 0.283, Test loss: 0.405, Test accuracy: 84.13
Round  70, Train loss: 0.346, Test loss: 0.405, Test accuracy: 84.00
Round  71, Train loss: 0.323, Test loss: 0.414, Test accuracy: 83.38
Round  72, Train loss: 0.330, Test loss: 0.404, Test accuracy: 84.13
Round  73, Train loss: 0.319, Test loss: 0.403, Test accuracy: 84.05
Round  74, Train loss: 0.305, Test loss: 0.402, Test accuracy: 84.08
Round  75, Train loss: 0.380, Test loss: 0.402, Test accuracy: 84.21
Round  76, Train loss: 0.330, Test loss: 0.391, Test accuracy: 84.37
Round  77, Train loss: 0.300, Test loss: 0.398, Test accuracy: 84.37
Round  78, Train loss: 0.296, Test loss: 0.404, Test accuracy: 84.17
Round  79, Train loss: 0.285, Test loss: 0.397, Test accuracy: 84.40
Round  80, Train loss: 0.303, Test loss: 0.396, Test accuracy: 84.44
Round  81, Train loss: 0.293, Test loss: 0.403, Test accuracy: 84.08
Round  82, Train loss: 0.329, Test loss: 0.403, Test accuracy: 83.98
Round  83, Train loss: 0.253, Test loss: 0.405, Test accuracy: 83.96
Round  84, Train loss: 0.274, Test loss: 0.399, Test accuracy: 84.25
Round  85, Train loss: 0.291, Test loss: 0.397, Test accuracy: 84.20
Round  86, Train loss: 0.264, Test loss: 0.395, Test accuracy: 84.62
Round  87, Train loss: 0.277, Test loss: 0.414, Test accuracy: 83.82
Round  88, Train loss: 0.254, Test loss: 0.404, Test accuracy: 84.28
Round  89, Train loss: 0.282, Test loss: 0.389, Test accuracy: 84.86
Round  90, Train loss: 0.333, Test loss: 0.399, Test accuracy: 84.41
Round  91, Train loss: 0.258, Test loss: 0.392, Test accuracy: 84.58
Round  92, Train loss: 0.332, Test loss: 0.390, Test accuracy: 84.90
Round  93, Train loss: 0.242, Test loss: 0.388, Test accuracy: 84.88
Round  94, Train loss: 0.311, Test loss: 0.392, Test accuracy: 84.88
Round  95, Train loss: 0.283, Test loss: 0.397, Test accuracy: 84.45
Round  96, Train loss: 0.264, Test loss: 0.405, Test accuracy: 84.18
Round  97, Train loss: 0.295, Test loss: 0.400, Test accuracy: 84.44
Round  98, Train loss: 0.247, Test loss: 0.393, Test accuracy: 84.88
Round  99, Train loss: 0.268, Test loss: 0.386, Test accuracy: 84.95
Final Round, Train loss: 0.230, Test loss: 0.388, Test accuracy: 84.86
Average accuracy final 10 rounds: 84.65416666666665
1396.8759651184082
[1.722672462463379, 3.074556589126587, 4.423901796340942, 5.719320297241211, 7.021973133087158, 8.400768041610718, 9.711387157440186, 11.023976564407349, 12.332882404327393, 13.64978575706482, 14.953185796737671, 16.25524139404297, 17.595046043395996, 18.890421152114868, 20.18223547935486, 21.49129557609558, 22.792558908462524, 24.090128898620605, 25.391576051712036, 26.69126796722412, 28.04697823524475, 29.401271104812622, 30.774839878082275, 32.11938500404358, 33.47932839393616, 34.84336256980896, 36.198089599609375, 37.56771492958069, 38.922422885894775, 40.273391246795654, 41.6296911239624, 42.97771120071411, 44.33162212371826, 45.687217235565186, 47.042258739471436, 48.40726327896118, 49.781153202056885, 51.14654278755188, 52.51505947113037, 53.88930535316467, 55.253841400146484, 56.611252307891846, 57.976052045822144, 59.34198069572449, 60.68576741218567, 61.899091958999634, 63.11024022102356, 64.31842279434204, 65.52834153175354, 66.72801518440247, 67.92842984199524, 69.14554953575134, 70.36822962760925, 71.58161354064941, 72.7939829826355, 74.01216912269592, 75.2261176109314, 76.44453501701355, 77.6476080417633, 78.85499334335327, 80.0611674785614, 81.27995371818542, 82.50310206413269, 83.72169017791748, 84.93536448478699, 86.13885450363159, 87.47910499572754, 88.75059628486633, 89.95288372039795, 91.15713930130005, 92.36687660217285, 93.57144832611084, 94.77327394485474, 95.97168350219727, 97.168283700943, 98.36893057823181, 99.56649351119995, 100.77004051208496, 101.96932792663574, 103.16796493530273, 104.36613368988037, 105.57200002670288, 106.77034306526184, 107.98319983482361, 109.1905152797699, 110.39293360710144, 111.59955382347107, 112.79847884178162, 114.0098614692688, 115.21131014823914, 116.41391134262085, 117.6234130859375, 118.82336401939392, 120.02463221549988, 121.22290778160095, 122.42968726158142, 123.641108751297, 124.84511470794678, 126.05474805831909, 127.25920343399048, 129.20589900016785]
[19.691666666666666, 36.8, 45.925, 50.958333333333336, 56.40833333333333, 56.270833333333336, 59.09166666666667, 61.020833333333336, 61.7125, 68.00833333333334, 72.10833333333333, 72.29166666666667, 72.0375, 72.84583333333333, 73.56666666666666, 74.2125, 74.7875, 75.3375, 75.60833333333333, 77.00833333333334, 76.99583333333334, 77.60833333333333, 78.38333333333334, 78.33333333333333, 78.45416666666667, 79.00416666666666, 78.67083333333333, 79.39166666666667, 79.41666666666667, 80.07083333333334, 80.025, 80.01666666666667, 80.30833333333334, 80.67083333333333, 80.8625, 80.73333333333333, 80.94166666666666, 81.36666666666666, 81.02083333333333, 81.2125, 81.27083333333333, 81.82083333333334, 82.175, 82.19166666666666, 81.99583333333334, 81.72916666666667, 82.30416666666666, 82.32916666666667, 82.35416666666667, 82.60416666666667, 82.9875, 82.74583333333334, 82.74583333333334, 82.825, 82.79583333333333, 82.825, 82.3125, 82.8875, 83.04166666666667, 82.78333333333333, 83.27916666666667, 83.725, 83.57083333333334, 83.59583333333333, 83.49166666666666, 84.0375, 83.67916666666666, 83.7125, 83.80833333333334, 84.12916666666666, 84.0, 83.375, 84.13333333333334, 84.05, 84.08333333333333, 84.20833333333333, 84.36666666666666, 84.36666666666666, 84.17083333333333, 84.4, 84.44166666666666, 84.08333333333333, 83.97916666666667, 83.9625, 84.24583333333334, 84.20416666666667, 84.61666666666666, 83.82083333333334, 84.28333333333333, 84.85833333333333, 84.40833333333333, 84.58333333333333, 84.89583333333333, 84.875, 84.87916666666666, 84.45, 84.18333333333334, 84.4375, 84.87916666666666, 84.95, 84.85833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8475
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3435
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8260
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1090
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6760
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7930
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0375
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6920
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5495
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8625
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.197, Test loss: 2.209, Test accuracy: 19.47 

Round   0, Global train loss: 2.197, Global test loss: 2.193, Global test accuracy: 19.84 

Round   1, Train loss: 2.196, Test loss: 2.204, Test accuracy: 22.20 

Round   1, Global train loss: 2.196, Global test loss: 2.163, Global test accuracy: 25.50 

Round   2, Train loss: 2.115, Test loss: 2.150, Test accuracy: 22.32 

Round   2, Global train loss: 2.115, Global test loss: 2.064, Global test accuracy: 24.60 

Round   3, Train loss: 2.126, Test loss: 2.168, Test accuracy: 23.12 

Round   3, Global train loss: 2.126, Global test loss: 2.113, Global test accuracy: 30.81 

Round   4, Train loss: 2.070, Test loss: 2.144, Test accuracy: 23.58 

Round   4, Global train loss: 2.070, Global test loss: 2.052, Global test accuracy: 29.40 

Round   5, Train loss: 2.007, Test loss: 2.123, Test accuracy: 23.69 

Round   5, Global train loss: 2.007, Global test loss: 1.992, Global test accuracy: 28.05 

Round   6, Train loss: 2.057, Test loss: 2.129, Test accuracy: 23.91 

Round   6, Global train loss: 2.057, Global test loss: 2.052, Global test accuracy: 29.04 

Round   7, Train loss: 2.064, Test loss: 2.134, Test accuracy: 23.87 

Round   7, Global train loss: 2.064, Global test loss: 2.110, Global test accuracy: 26.26 

Round   8, Train loss: 1.962, Test loss: 2.112, Test accuracy: 24.84 

Round   8, Global train loss: 1.962, Global test loss: 1.947, Global test accuracy: 31.16 

Round   9, Train loss: 2.079, Test loss: 2.102, Test accuracy: 25.25 

Round   9, Global train loss: 2.079, Global test loss: 2.105, Global test accuracy: 27.26 

Round  10, Train loss: 1.860, Test loss: 2.083, Test accuracy: 26.20 

Round  10, Global train loss: 1.860, Global test loss: 1.873, Global test accuracy: 33.78 

Round  11, Train loss: 1.965, Test loss: 2.095, Test accuracy: 25.89 

Round  11, Global train loss: 1.965, Global test loss: 2.022, Global test accuracy: 29.34 

Round  12, Train loss: 2.035, Test loss: 2.106, Test accuracy: 25.57 

Round  12, Global train loss: 2.035, Global test loss: 2.071, Global test accuracy: 30.23 

Round  13, Train loss: 1.805, Test loss: 2.123, Test accuracy: 25.66 

Round  13, Global train loss: 1.805, Global test loss: 1.869, Global test accuracy: 33.91 

Round  14, Train loss: 1.865, Test loss: 2.113, Test accuracy: 25.81 

Round  14, Global train loss: 1.865, Global test loss: 1.936, Global test accuracy: 33.59 

Round  15, Train loss: 1.714, Test loss: 2.115, Test accuracy: 26.04 

Round  15, Global train loss: 1.714, Global test loss: 1.815, Global test accuracy: 37.20 

Round  16, Train loss: 1.822, Test loss: 2.119, Test accuracy: 26.59 

Round  16, Global train loss: 1.822, Global test loss: 1.912, Global test accuracy: 35.31 

Round  17, Train loss: 1.831, Test loss: 2.116, Test accuracy: 26.93 

Round  17, Global train loss: 1.831, Global test loss: 1.941, Global test accuracy: 31.85 

Round  18, Train loss: 1.732, Test loss: 2.112, Test accuracy: 27.17 

Round  18, Global train loss: 1.732, Global test loss: 1.810, Global test accuracy: 36.35 

Round  19, Train loss: 1.745, Test loss: 2.102, Test accuracy: 27.33 

Round  19, Global train loss: 1.745, Global test loss: 1.874, Global test accuracy: 35.89 

Round  20, Train loss: 1.736, Test loss: 2.117, Test accuracy: 27.27 

Round  20, Global train loss: 1.736, Global test loss: 1.938, Global test accuracy: 34.28 

Round  21, Train loss: 1.787, Test loss: 2.121, Test accuracy: 27.62 

Round  21, Global train loss: 1.787, Global test loss: 1.887, Global test accuracy: 35.02 

Round  22, Train loss: 1.706, Test loss: 2.144, Test accuracy: 27.59 

Round  22, Global train loss: 1.706, Global test loss: 2.016, Global test accuracy: 31.53 

Round  23, Train loss: 1.705, Test loss: 2.175, Test accuracy: 27.29 

Round  23, Global train loss: 1.705, Global test loss: 1.895, Global test accuracy: 34.09 

Round  24, Train loss: 1.628, Test loss: 2.206, Test accuracy: 27.18 

Round  24, Global train loss: 1.628, Global test loss: 1.918, Global test accuracy: 35.00 

Round  25, Train loss: 1.501, Test loss: 2.183, Test accuracy: 27.46 

Round  25, Global train loss: 1.501, Global test loss: 1.817, Global test accuracy: 36.62 

Round  26, Train loss: 1.770, Test loss: 2.203, Test accuracy: 27.65 

Round  26, Global train loss: 1.770, Global test loss: 1.914, Global test accuracy: 36.49 

Round  27, Train loss: 1.445, Test loss: 2.238, Test accuracy: 27.74 

Round  27, Global train loss: 1.445, Global test loss: 1.748, Global test accuracy: 37.66 

Round  28, Train loss: 1.414, Test loss: 2.251, Test accuracy: 27.92 

Round  28, Global train loss: 1.414, Global test loss: 1.864, Global test accuracy: 35.63 

Round  29, Train loss: 1.408, Test loss: 2.289, Test accuracy: 28.02 

Round  29, Global train loss: 1.408, Global test loss: 1.793, Global test accuracy: 38.60 

Round  30, Train loss: 1.416, Test loss: 2.329, Test accuracy: 28.12 

Round  30, Global train loss: 1.416, Global test loss: 1.894, Global test accuracy: 37.21 

Round  31, Train loss: 1.570, Test loss: 2.343, Test accuracy: 27.37 

Round  31, Global train loss: 1.570, Global test loss: 1.868, Global test accuracy: 36.73 

Round  32, Train loss: 1.480, Test loss: 2.380, Test accuracy: 27.19 

Round  32, Global train loss: 1.480, Global test loss: 1.855, Global test accuracy: 37.55 

Round  33, Train loss: 1.295, Test loss: 2.432, Test accuracy: 27.23 

Round  33, Global train loss: 1.295, Global test loss: 1.796, Global test accuracy: 38.46 

Round  34, Train loss: 1.281, Test loss: 2.476, Test accuracy: 27.13 

Round  34, Global train loss: 1.281, Global test loss: 1.856, Global test accuracy: 34.38 

Round  35, Train loss: 1.274, Test loss: 2.516, Test accuracy: 26.86 

Round  35, Global train loss: 1.274, Global test loss: 1.820, Global test accuracy: 37.27 

Round  36, Train loss: 1.223, Test loss: 2.540, Test accuracy: 27.11 

Round  36, Global train loss: 1.223, Global test loss: 1.906, Global test accuracy: 35.26 

Round  37, Train loss: 1.414, Test loss: 2.578, Test accuracy: 27.34 

Round  37, Global train loss: 1.414, Global test loss: 1.930, Global test accuracy: 32.00 

Round  38, Train loss: 1.181, Test loss: 2.630, Test accuracy: 27.43 

Round  38, Global train loss: 1.181, Global test loss: 1.751, Global test accuracy: 37.63 

Round  39, Train loss: 1.332, Test loss: 2.694, Test accuracy: 27.49 

Round  39, Global train loss: 1.332, Global test loss: 1.967, Global test accuracy: 31.55 

Round  40, Train loss: 0.898, Test loss: 2.769, Test accuracy: 27.30 

Round  40, Global train loss: 0.898, Global test loss: 1.671, Global test accuracy: 41.22 

Round  41, Train loss: 1.477, Test loss: 2.855, Test accuracy: 26.56 

Round  41, Global train loss: 1.477, Global test loss: 2.029, Global test accuracy: 29.68 

Round  42, Train loss: 1.092, Test loss: 2.868, Test accuracy: 26.34 

Round  42, Global train loss: 1.092, Global test loss: 1.957, Global test accuracy: 32.57 

Round  43, Train loss: 1.045, Test loss: 2.946, Test accuracy: 26.53 

Round  43, Global train loss: 1.045, Global test loss: 1.828, Global test accuracy: 34.65 

Round  44, Train loss: 1.022, Test loss: 2.963, Test accuracy: 26.71 

Round  44, Global train loss: 1.022, Global test loss: 1.815, Global test accuracy: 36.91 

Round  45, Train loss: 1.133, Test loss: 3.032, Test accuracy: 26.96 

Round  45, Global train loss: 1.133, Global test loss: 1.967, Global test accuracy: 31.00 

Round  46, Train loss: 1.120, Test loss: 3.144, Test accuracy: 26.77 

Round  46, Global train loss: 1.120, Global test loss: 1.873, Global test accuracy: 35.62 

Round  47, Train loss: 0.956, Test loss: 3.261, Test accuracy: 26.56 

Round  47, Global train loss: 0.956, Global test loss: 1.885, Global test accuracy: 35.03 

Round  48, Train loss: 0.785, Test loss: 3.265, Test accuracy: 26.36 

Round  48, Global train loss: 0.785, Global test loss: 1.748, Global test accuracy: 36.97 

Round  49, Train loss: 0.809, Test loss: 3.379, Test accuracy: 26.53 

Round  49, Global train loss: 0.809, Global test loss: 1.793, Global test accuracy: 36.46 

Round  50, Train loss: 0.864, Test loss: 3.392, Test accuracy: 26.70 

Round  50, Global train loss: 0.864, Global test loss: 1.838, Global test accuracy: 34.09 

Round  51, Train loss: 0.769, Test loss: 3.494, Test accuracy: 26.48 

Round  51, Global train loss: 0.769, Global test loss: 1.799, Global test accuracy: 35.55 

Round  52, Train loss: 0.876, Test loss: 3.497, Test accuracy: 26.56 

Round  52, Global train loss: 0.876, Global test loss: 1.780, Global test accuracy: 35.83 

Round  53, Train loss: 0.897, Test loss: 3.567, Test accuracy: 26.78 

Round  53, Global train loss: 0.897, Global test loss: 1.712, Global test accuracy: 40.09 

Round  54, Train loss: 0.739, Test loss: 3.599, Test accuracy: 26.82 

Round  54, Global train loss: 0.739, Global test loss: 1.895, Global test accuracy: 31.60 

Round  55, Train loss: 0.764, Test loss: 3.656, Test accuracy: 26.77 

Round  55, Global train loss: 0.764, Global test loss: 1.811, Global test accuracy: 36.18 

Round  56, Train loss: 0.830, Test loss: 3.679, Test accuracy: 26.56 

Round  56, Global train loss: 0.830, Global test loss: 1.871, Global test accuracy: 36.10 

Round  57, Train loss: 0.724, Test loss: 3.802, Test accuracy: 26.03 

Round  57, Global train loss: 0.724, Global test loss: 1.751, Global test accuracy: 37.68 

Round  58, Train loss: 0.659, Test loss: 3.829, Test accuracy: 25.93 

Round  58, Global train loss: 0.659, Global test loss: 1.902, Global test accuracy: 33.05 

Round  59, Train loss: 0.677, Test loss: 3.898, Test accuracy: 26.31 

Round  59, Global train loss: 0.677, Global test loss: 1.872, Global test accuracy: 32.02 

Round  60, Train loss: 0.615, Test loss: 3.961, Test accuracy: 26.19 

Round  60, Global train loss: 0.615, Global test loss: 1.947, Global test accuracy: 30.50 

Round  61, Train loss: 0.787, Test loss: 4.069, Test accuracy: 26.32 

Round  61, Global train loss: 0.787, Global test loss: 1.960, Global test accuracy: 30.15 

Round  62, Train loss: 0.665, Test loss: 4.100, Test accuracy: 26.74 

Round  62, Global train loss: 0.665, Global test loss: 1.872, Global test accuracy: 33.66 

Round  63, Train loss: 0.419, Test loss: 4.061, Test accuracy: 26.85 

Round  63, Global train loss: 0.419, Global test loss: 1.727, Global test accuracy: 38.14 

Round  64, Train loss: 0.591, Test loss: 4.124, Test accuracy: 26.60 

Round  64, Global train loss: 0.591, Global test loss: 1.695, Global test accuracy: 39.59 

Round  65, Train loss: 0.697, Test loss: 4.244, Test accuracy: 26.48 

Round  65, Global train loss: 0.697, Global test loss: 2.072, Global test accuracy: 26.23 

Round  66, Train loss: 0.490, Test loss: 4.355, Test accuracy: 26.50 

Round  66, Global train loss: 0.490, Global test loss: 1.718, Global test accuracy: 38.75 

Round  67, Train loss: 0.613, Test loss: 4.410, Test accuracy: 26.35 

Round  67, Global train loss: 0.613, Global test loss: 1.957, Global test accuracy: 30.81 

Round  68, Train loss: 0.597, Test loss: 4.520, Test accuracy: 25.93 

Round  68, Global train loss: 0.597, Global test loss: 1.793, Global test accuracy: 35.72 

Round  69, Train loss: 0.407, Test loss: 4.490, Test accuracy: 26.00 

Round  69, Global train loss: 0.407, Global test loss: 1.848, Global test accuracy: 35.43 

Round  70, Train loss: 0.410, Test loss: 4.572, Test accuracy: 26.20 

Round  70, Global train loss: 0.410, Global test loss: 1.819, Global test accuracy: 34.84 

Round  71, Train loss: 0.443, Test loss: 4.549, Test accuracy: 25.85 

Round  71, Global train loss: 0.443, Global test loss: 1.913, Global test accuracy: 33.19 

Round  72, Train loss: 0.546, Test loss: 4.666, Test accuracy: 26.04 

Round  72, Global train loss: 0.546, Global test loss: 1.900, Global test accuracy: 32.42 

Round  73, Train loss: 0.469, Test loss: 4.707, Test accuracy: 25.93 

Round  73, Global train loss: 0.469, Global test loss: 1.845, Global test accuracy: 32.51 

Round  74, Train loss: 0.592, Test loss: 4.813, Test accuracy: 26.05 

Round  74, Global train loss: 0.592, Global test loss: 1.813, Global test accuracy: 34.40 

Round  75, Train loss: 0.395, Test loss: 4.764, Test accuracy: 26.33 

Round  75, Global train loss: 0.395, Global test loss: 1.743, Global test accuracy: 37.99 

Round  76, Train loss: 0.361, Test loss: 4.794, Test accuracy: 26.55 

Round  76, Global train loss: 0.361, Global test loss: 1.917, Global test accuracy: 31.07 

Round  77, Train loss: 0.378, Test loss: 4.840, Test accuracy: 26.15 

Round  77, Global train loss: 0.378, Global test loss: 1.880, Global test accuracy: 33.36 

Round  78, Train loss: 0.323, Test loss: 4.915, Test accuracy: 26.03 

Round  78, Global train loss: 0.323, Global test loss: 1.787, Global test accuracy: 34.88 

Round  79, Train loss: 0.452, Test loss: 5.029, Test accuracy: 26.11 

Round  79, Global train loss: 0.452, Global test loss: 1.833, Global test accuracy: 34.94 

Round  80, Train loss: 0.397, Test loss: 5.096, Test accuracy: 26.06 

Round  80, Global train loss: 0.397, Global test loss: 1.867, Global test accuracy: 33.08 

Round  81, Train loss: 0.317, Test loss: 5.131, Test accuracy: 26.37 

Round  81, Global train loss: 0.317, Global test loss: 1.760, Global test accuracy: 38.53 

Round  82, Train loss: 0.285, Test loss: 5.159, Test accuracy: 26.23 

Round  82, Global train loss: 0.285, Global test loss: 1.746, Global test accuracy: 38.27 

Round  83, Train loss: 0.505, Test loss: 5.210, Test accuracy: 26.38 

Round  83, Global train loss: 0.505, Global test loss: 2.041, Global test accuracy: 26.48 

Round  84, Train loss: 0.335, Test loss: 5.178, Test accuracy: 26.79 

Round  84, Global train loss: 0.335, Global test loss: 1.755, Global test accuracy: 37.66 

Round  85, Train loss: 0.399, Test loss: 5.318, Test accuracy: 26.54 

Round  85, Global train loss: 0.399, Global test loss: 1.873, Global test accuracy: 33.77 

Round  86, Train loss: 0.374, Test loss: 5.358, Test accuracy: 26.56 

Round  86, Global train loss: 0.374, Global test loss: 1.917, Global test accuracy: 31.59 

Round  87, Train loss: 0.283, Test loss: 5.397, Test accuracy: 26.71 

Round  87, Global train loss: 0.283, Global test loss: 1.818, Global test accuracy: 35.26 

Round  88, Train loss: 0.280, Test loss: 5.478, Test accuracy: 26.60 

Round  88, Global train loss: 0.280, Global test loss: 1.768, Global test accuracy: 36.99 

Round  89, Train loss: 0.301, Test loss: 5.554, Test accuracy: 26.45 

Round  89, Global train loss: 0.301, Global test loss: 1.890, Global test accuracy: 31.83 

Round  90, Train loss: 0.264, Test loss: 5.638, Test accuracy: 26.47 

Round  90, Global train loss: 0.264, Global test loss: 1.842, Global test accuracy: 33.62 

Round  91, Train loss: 0.357, Test loss: 5.750, Test accuracy: 26.18 

Round  91, Global train loss: 0.357, Global test loss: 1.979, Global test accuracy: 29.16 

Round  92, Train loss: 0.335, Test loss: 5.671, Test accuracy: 26.24 

Round  92, Global train loss: 0.335, Global test loss: 1.954, Global test accuracy: 30.58 

Round  93, Train loss: 0.248, Test loss: 5.700, Test accuracy: 26.45 

Round  93, Global train loss: 0.248, Global test loss: 1.833, Global test accuracy: 34.28 

Round  94, Train loss: 0.235, Test loss: 5.765, Test accuracy: 26.45 

Round  94, Global train loss: 0.235, Global test loss: 1.809, Global test accuracy: 36.53 

Round  95, Train loss: 0.300, Test loss: 5.737, Test accuracy: 26.44 

Round  95, Global train loss: 0.300, Global test loss: 1.920, Global test accuracy: 32.41 

Round  96, Train loss: 0.347, Test loss: 5.804, Test accuracy: 26.48 

Round  96, Global train loss: 0.347, Global test loss: 1.959, Global test accuracy: 29.46 

Round  97, Train loss: 0.295, Test loss: 5.857, Test accuracy: 26.45 

Round  97, Global train loss: 0.295, Global test loss: 1.893, Global test accuracy: 31.30 

Round  98, Train loss: 0.259, Test loss: 5.873, Test accuracy: 26.39 

Round  98, Global train loss: 0.259, Global test loss: 1.993, Global test accuracy: 29.38 

Round  99, Train loss: 0.258, Test loss: 5.876, Test accuracy: 26.24 

Round  99, Global train loss: 0.258, Global test loss: 1.955, Global test accuracy: 29.40 

Final Round, Train loss: 0.260, Test loss: 6.255, Test accuracy: 26.25 

Final Round, Global train loss: 0.260, Global test loss: 1.955, Global test accuracy: 29.40 

Average accuracy final 10 rounds: 26.38 

Average global accuracy final 10 rounds: 31.612 

2703.462076663971
[1.573887586593628, 2.733745813369751, 3.7895216941833496, 4.83144474029541, 5.8728346824646, 6.918057918548584, 8.08350682258606, 9.255499839782715, 10.42849349975586, 11.613410711288452, 12.786545276641846, 13.958740711212158, 15.163851499557495, 16.329471349716187, 17.508491039276123, 18.66019892692566, 19.84031653404236, 21.027414798736572, 22.198546648025513, 23.38509202003479, 24.55173110961914, 25.715769052505493, 26.875213861465454, 27.998631954193115, 29.1844961643219, 30.358500003814697, 31.54172158241272, 32.715481758117676, 33.924832582473755, 35.09821915626526, 36.26762104034424, 37.44962406158447, 38.55990028381348, 39.78706240653992, 40.95397758483887, 42.12137413024902, 43.29189443588257, 44.46181607246399, 45.63011956214905, 46.81470847129822, 48.00257897377014, 49.1889214515686, 50.371354818344116, 51.55472779273987, 52.73462510108948, 53.91663384437561, 55.10381317138672, 56.28886365890503, 57.4738974571228, 58.655858278274536, 59.83664894104004, 61.02373480796814, 62.05648469924927, 63.08129072189331, 64.10605549812317, 65.13224530220032, 66.16187167167664, 67.1864697933197, 68.21587061882019, 69.2385528087616, 70.26115107536316, 71.28273153305054, 72.30395317077637, 73.32752346992493, 74.35396099090576, 75.38122391700745, 76.40256214141846, 77.42717146873474, 78.45218062400818, 79.47638726234436, 80.5042359828949, 81.53020906448364, 82.55525755882263, 83.57977652549744, 84.60418105125427, 85.6266462802887, 86.65093398094177, 87.6784815788269, 88.70665621757507, 89.73272657394409, 90.76229071617126, 91.79263162612915, 92.81774115562439, 93.83875322341919, 94.8645932674408, 95.8887574672699, 96.91585779190063, 97.94106078147888, 98.96597003936768, 99.99243140220642, 101.0250940322876, 102.05656480789185, 103.08642935752869, 104.11367082595825, 105.1429877281189, 106.17047333717346, 107.19999957084656, 108.22760272026062, 109.25297689437866, 110.2795832157135, 112.33618903160095]
[19.4675, 22.195, 22.3225, 23.1225, 23.5775, 23.69, 23.905, 23.8725, 24.835, 25.2525, 26.205, 25.8925, 25.5725, 25.655, 25.8075, 26.04, 26.59, 26.935, 27.17, 27.33, 27.2675, 27.62, 27.5925, 27.285, 27.185, 27.4575, 27.6525, 27.7425, 27.9175, 28.02, 28.1175, 27.3675, 27.1875, 27.2325, 27.1325, 26.855, 27.1125, 27.3375, 27.425, 27.4925, 27.295, 26.56, 26.34, 26.5325, 26.715, 26.9575, 26.7675, 26.5625, 26.355, 26.53, 26.695, 26.475, 26.5625, 26.78, 26.82, 26.77, 26.56, 26.0325, 25.9325, 26.3125, 26.1875, 26.3175, 26.7375, 26.85, 26.5975, 26.4825, 26.5, 26.35, 25.9275, 26.005, 26.2, 25.8475, 26.035, 25.9275, 26.0475, 26.33, 26.5475, 26.1475, 26.03, 26.11, 26.0575, 26.3675, 26.225, 26.3825, 26.79, 26.5425, 26.56, 26.7075, 26.5975, 26.445, 26.47, 26.185, 26.24, 26.455, 26.45, 26.44, 26.475, 26.45, 26.395, 26.24, 26.25]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8475
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3420
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8290
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1070
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5680
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8050
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.2590
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6675
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5575
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8610
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.200, Test loss: 2.233, Test accuracy: 20.35 

Round   0, Global train loss: 2.200, Global test loss: 2.205, Global test accuracy: 20.98 

Round   1, Train loss: 2.099, Test loss: 2.148, Test accuracy: 22.62 

Round   1, Global train loss: 2.099, Global test loss: 2.053, Global test accuracy: 24.52 

Round   2, Train loss: 2.027, Test loss: 2.075, Test accuracy: 25.79 

Round   2, Global train loss: 2.027, Global test loss: 1.923, Global test accuracy: 30.18 

Round   3, Train loss: 1.927, Test loss: 2.060, Test accuracy: 26.83 

Round   3, Global train loss: 1.927, Global test loss: 1.821, Global test accuracy: 32.21 

Round   4, Train loss: 1.893, Test loss: 2.007, Test accuracy: 28.90 

Round   4, Global train loss: 1.893, Global test loss: 1.738, Global test accuracy: 38.71 

Round   5, Train loss: 1.892, Test loss: 1.970, Test accuracy: 30.41 

Round   5, Global train loss: 1.892, Global test loss: 1.710, Global test accuracy: 40.01 

Round   6, Train loss: 1.862, Test loss: 1.932, Test accuracy: 31.96 

Round   6, Global train loss: 1.862, Global test loss: 1.706, Global test accuracy: 41.78 

Round   7, Train loss: 1.823, Test loss: 1.889, Test accuracy: 33.66 

Round   7, Global train loss: 1.823, Global test loss: 1.644, Global test accuracy: 42.87 

Round   8, Train loss: 1.766, Test loss: 1.899, Test accuracy: 33.62 

Round   8, Global train loss: 1.766, Global test loss: 1.607, Global test accuracy: 42.60 

Round   9, Train loss: 1.808, Test loss: 1.884, Test accuracy: 34.45 

Round   9, Global train loss: 1.808, Global test loss: 1.652, Global test accuracy: 42.98 

Round  10, Train loss: 1.794, Test loss: 1.853, Test accuracy: 35.73 

Round  10, Global train loss: 1.794, Global test loss: 1.560, Global test accuracy: 46.52 

Round  11, Train loss: 1.722, Test loss: 1.842, Test accuracy: 36.30 

Round  11, Global train loss: 1.722, Global test loss: 1.537, Global test accuracy: 48.70 

Round  12, Train loss: 1.729, Test loss: 1.834, Test accuracy: 36.48 

Round  12, Global train loss: 1.729, Global test loss: 1.537, Global test accuracy: 46.61 

Round  13, Train loss: 1.690, Test loss: 1.824, Test accuracy: 37.08 

Round  13, Global train loss: 1.690, Global test loss: 1.475, Global test accuracy: 50.88 

Round  14, Train loss: 1.726, Test loss: 1.805, Test accuracy: 38.20 

Round  14, Global train loss: 1.726, Global test loss: 1.496, Global test accuracy: 50.13 

Round  15, Train loss: 1.702, Test loss: 1.787, Test accuracy: 38.66 

Round  15, Global train loss: 1.702, Global test loss: 1.514, Global test accuracy: 51.59 

Round  16, Train loss: 1.667, Test loss: 1.770, Test accuracy: 39.40 

Round  16, Global train loss: 1.667, Global test loss: 1.495, Global test accuracy: 50.97 

Round  17, Train loss: 1.595, Test loss: 1.781, Test accuracy: 38.96 

Round  17, Global train loss: 1.595, Global test loss: 1.432, Global test accuracy: 51.79 

Round  18, Train loss: 1.517, Test loss: 1.767, Test accuracy: 39.63 

Round  18, Global train loss: 1.517, Global test loss: 1.406, Global test accuracy: 51.97 

Round  19, Train loss: 1.550, Test loss: 1.761, Test accuracy: 40.02 

Round  19, Global train loss: 1.550, Global test loss: 1.375, Global test accuracy: 54.52 

Round  20, Train loss: 1.560, Test loss: 1.747, Test accuracy: 40.67 

Round  20, Global train loss: 1.560, Global test loss: 1.361, Global test accuracy: 54.53 

Round  21, Train loss: 1.606, Test loss: 1.750, Test accuracy: 40.63 

Round  21, Global train loss: 1.606, Global test loss: 1.444, Global test accuracy: 53.47 

Round  22, Train loss: 1.512, Test loss: 1.739, Test accuracy: 41.05 

Round  22, Global train loss: 1.512, Global test loss: 1.384, Global test accuracy: 53.99 

Round  23, Train loss: 1.634, Test loss: 1.737, Test accuracy: 41.41 

Round  23, Global train loss: 1.634, Global test loss: 1.465, Global test accuracy: 53.74 

Round  24, Train loss: 1.528, Test loss: 1.755, Test accuracy: 41.44 

Round  24, Global train loss: 1.528, Global test loss: 1.405, Global test accuracy: 54.38 

Round  25, Train loss: 1.518, Test loss: 1.748, Test accuracy: 42.09 

Round  25, Global train loss: 1.518, Global test loss: 1.380, Global test accuracy: 55.15 

Round  26, Train loss: 1.561, Test loss: 1.755, Test accuracy: 42.17 

Round  26, Global train loss: 1.561, Global test loss: 1.422, Global test accuracy: 54.31 

Round  27, Train loss: 1.425, Test loss: 1.749, Test accuracy: 42.45 

Round  27, Global train loss: 1.425, Global test loss: 1.350, Global test accuracy: 55.84 

Round  28, Train loss: 1.378, Test loss: 1.736, Test accuracy: 42.53 

Round  28, Global train loss: 1.378, Global test loss: 1.311, Global test accuracy: 55.91 

Round  29, Train loss: 1.414, Test loss: 1.745, Test accuracy: 42.43 

Round  29, Global train loss: 1.414, Global test loss: 1.322, Global test accuracy: 56.42 

Round  30, Train loss: 1.393, Test loss: 1.736, Test accuracy: 42.98 

Round  30, Global train loss: 1.393, Global test loss: 1.332, Global test accuracy: 56.01 

Round  31, Train loss: 1.455, Test loss: 1.763, Test accuracy: 42.48 

Round  31, Global train loss: 1.455, Global test loss: 1.419, Global test accuracy: 54.55 

Round  32, Train loss: 1.472, Test loss: 1.756, Test accuracy: 43.05 

Round  32, Global train loss: 1.472, Global test loss: 1.410, Global test accuracy: 54.19 

Round  33, Train loss: 1.322, Test loss: 1.751, Test accuracy: 43.33 

Round  33, Global train loss: 1.322, Global test loss: 1.319, Global test accuracy: 56.14 

Round  34, Train loss: 1.402, Test loss: 1.763, Test accuracy: 43.18 

Round  34, Global train loss: 1.402, Global test loss: 1.347, Global test accuracy: 55.88 

Round  35, Train loss: 1.309, Test loss: 1.759, Test accuracy: 43.26 

Round  35, Global train loss: 1.309, Global test loss: 1.343, Global test accuracy: 56.49 

Round  36, Train loss: 1.337, Test loss: 1.770, Test accuracy: 43.59 

Round  36, Global train loss: 1.337, Global test loss: 1.382, Global test accuracy: 54.98 

Round  37, Train loss: 1.421, Test loss: 1.802, Test accuracy: 43.14 

Round  37, Global train loss: 1.421, Global test loss: 1.408, Global test accuracy: 54.61 

Round  38, Train loss: 1.273, Test loss: 1.801, Test accuracy: 43.20 

Round  38, Global train loss: 1.273, Global test loss: 1.318, Global test accuracy: 56.57 

Round  39, Train loss: 1.291, Test loss: 1.828, Test accuracy: 42.79 

Round  39, Global train loss: 1.291, Global test loss: 1.351, Global test accuracy: 55.85 

Round  40, Train loss: 1.306, Test loss: 1.855, Test accuracy: 42.45 

Round  40, Global train loss: 1.306, Global test loss: 1.408, Global test accuracy: 54.26 

Round  41, Train loss: 1.336, Test loss: 1.884, Test accuracy: 42.38 

Round  41, Global train loss: 1.336, Global test loss: 1.396, Global test accuracy: 54.81 

Round  42, Train loss: 1.167, Test loss: 1.874, Test accuracy: 43.08 

Round  42, Global train loss: 1.167, Global test loss: 1.299, Global test accuracy: 56.65 

Round  43, Train loss: 1.139, Test loss: 1.894, Test accuracy: 42.97 

Round  43, Global train loss: 1.139, Global test loss: 1.240, Global test accuracy: 58.48 

Round  44, Train loss: 1.294, Test loss: 1.887, Test accuracy: 43.19 

Round  44, Global train loss: 1.294, Global test loss: 1.377, Global test accuracy: 55.66 

Round  45, Train loss: 1.298, Test loss: 1.888, Test accuracy: 42.96 

Round  45, Global train loss: 1.298, Global test loss: 1.425, Global test accuracy: 53.48 

Round  46, Train loss: 1.255, Test loss: 1.877, Test accuracy: 42.98 

Round  46, Global train loss: 1.255, Global test loss: 1.436, Global test accuracy: 53.76 

Round  47, Train loss: 1.078, Test loss: 1.894, Test accuracy: 42.76 

Round  47, Global train loss: 1.078, Global test loss: 1.359, Global test accuracy: 55.36 

Round  48, Train loss: 1.129, Test loss: 1.873, Test accuracy: 43.06 

Round  48, Global train loss: 1.129, Global test loss: 1.341, Global test accuracy: 56.02 

Round  49, Train loss: 1.041, Test loss: 1.883, Test accuracy: 43.23 

Round  49, Global train loss: 1.041, Global test loss: 1.280, Global test accuracy: 57.90 

Round  50, Train loss: 1.226, Test loss: 1.880, Test accuracy: 43.69 

Round  50, Global train loss: 1.226, Global test loss: 1.421, Global test accuracy: 53.95 

Round  51, Train loss: 1.049, Test loss: 1.906, Test accuracy: 43.55 

Round  51, Global train loss: 1.049, Global test loss: 1.341, Global test accuracy: 56.54 

Round  52, Train loss: 1.222, Test loss: 1.893, Test accuracy: 43.65 

Round  52, Global train loss: 1.222, Global test loss: 1.374, Global test accuracy: 55.72 

Round  53, Train loss: 1.177, Test loss: 1.913, Test accuracy: 43.53 

Round  53, Global train loss: 1.177, Global test loss: 1.301, Global test accuracy: 57.33 

Round  54, Train loss: 1.048, Test loss: 1.953, Test accuracy: 43.39 

Round  54, Global train loss: 1.048, Global test loss: 1.323, Global test accuracy: 56.69 

Round  55, Train loss: 1.180, Test loss: 1.957, Test accuracy: 43.26 

Round  55, Global train loss: 1.180, Global test loss: 1.365, Global test accuracy: 55.64 

Round  56, Train loss: 1.127, Test loss: 1.985, Test accuracy: 43.40 

Round  56, Global train loss: 1.127, Global test loss: 1.403, Global test accuracy: 54.49 

Round  57, Train loss: 1.190, Test loss: 1.988, Test accuracy: 43.42 

Round  57, Global train loss: 1.190, Global test loss: 1.452, Global test accuracy: 52.92 

Round  58, Train loss: 1.006, Test loss: 2.013, Test accuracy: 43.30 

Round  58, Global train loss: 1.006, Global test loss: 1.367, Global test accuracy: 56.55 

Round  59, Train loss: 1.096, Test loss: 2.024, Test accuracy: 43.19 

Round  59, Global train loss: 1.096, Global test loss: 1.399, Global test accuracy: 55.02 

Round  60, Train loss: 0.963, Test loss: 2.072, Test accuracy: 43.13 

Round  60, Global train loss: 0.963, Global test loss: 1.374, Global test accuracy: 56.76 

Round  61, Train loss: 1.085, Test loss: 2.076, Test accuracy: 43.37 

Round  61, Global train loss: 1.085, Global test loss: 1.386, Global test accuracy: 54.99 

Round  62, Train loss: 1.080, Test loss: 2.101, Test accuracy: 42.95 

Round  62, Global train loss: 1.080, Global test loss: 1.516, Global test accuracy: 51.86 

Round  63, Train loss: 1.032, Test loss: 2.125, Test accuracy: 42.80 

Round  63, Global train loss: 1.032, Global test loss: 1.468, Global test accuracy: 53.36 

Round  64, Train loss: 1.057, Test loss: 2.141, Test accuracy: 42.47 

Round  64, Global train loss: 1.057, Global test loss: 1.346, Global test accuracy: 56.74 

Round  65, Train loss: 0.991, Test loss: 2.104, Test accuracy: 42.80 

Round  65, Global train loss: 0.991, Global test loss: 1.449, Global test accuracy: 53.97 

Round  66, Train loss: 1.059, Test loss: 2.075, Test accuracy: 43.20 

Round  66, Global train loss: 1.059, Global test loss: 1.404, Global test accuracy: 55.19 

Round  67, Train loss: 0.960, Test loss: 2.113, Test accuracy: 42.82 

Round  67, Global train loss: 0.960, Global test loss: 1.403, Global test accuracy: 55.39 

Round  68, Train loss: 1.021, Test loss: 2.110, Test accuracy: 42.74 

Round  68, Global train loss: 1.021, Global test loss: 1.428, Global test accuracy: 54.00 

Round  69, Train loss: 0.843, Test loss: 2.142, Test accuracy: 42.95 

Round  69, Global train loss: 0.843, Global test loss: 1.395, Global test accuracy: 56.59 

Round  70, Train loss: 0.922, Test loss: 2.159, Test accuracy: 42.91 

Round  70, Global train loss: 0.922, Global test loss: 1.387, Global test accuracy: 56.61 

Round  71, Train loss: 0.874, Test loss: 2.189, Test accuracy: 43.14 

Round  71, Global train loss: 0.874, Global test loss: 1.489, Global test accuracy: 54.02 

Round  72, Train loss: 0.933, Test loss: 2.206, Test accuracy: 43.34 

Round  72, Global train loss: 0.933, Global test loss: 1.494, Global test accuracy: 53.58 

Round  73, Train loss: 0.919, Test loss: 2.226, Test accuracy: 43.30 

Round  73, Global train loss: 0.919, Global test loss: 1.358, Global test accuracy: 57.10 

Round  74, Train loss: 1.013, Test loss: 2.217, Test accuracy: 43.38 

Round  74, Global train loss: 1.013, Global test loss: 1.423, Global test accuracy: 54.92 

Round  75, Train loss: 1.056, Test loss: 2.207, Test accuracy: 43.28 

Round  75, Global train loss: 1.056, Global test loss: 1.470, Global test accuracy: 54.41 

Round  76, Train loss: 0.897, Test loss: 2.218, Test accuracy: 42.99 

Round  76, Global train loss: 0.897, Global test loss: 1.485, Global test accuracy: 54.09 

Round  77, Train loss: 0.854, Test loss: 2.206, Test accuracy: 43.28 

Round  77, Global train loss: 0.854, Global test loss: 1.479, Global test accuracy: 54.66 

Round  78, Train loss: 0.907, Test loss: 2.232, Test accuracy: 43.07 

Round  78, Global train loss: 0.907, Global test loss: 1.442, Global test accuracy: 55.79 

Round  79, Train loss: 0.932, Test loss: 2.226, Test accuracy: 43.07 

Round  79, Global train loss: 0.932, Global test loss: 1.481, Global test accuracy: 54.29 

Round  80, Train loss: 0.921, Test loss: 2.242, Test accuracy: 43.09 

Round  80, Global train loss: 0.921, Global test loss: 1.461, Global test accuracy: 55.20 

Round  81, Train loss: 0.837, Test loss: 2.258, Test accuracy: 43.16 

Round  81, Global train loss: 0.837, Global test loss: 1.436, Global test accuracy: 56.37 

Round  82, Train loss: 0.916, Test loss: 2.230, Test accuracy: 43.09 

Round  82, Global train loss: 0.916, Global test loss: 1.489, Global test accuracy: 54.48 

Round  83, Train loss: 0.954, Test loss: 2.226, Test accuracy: 43.08 

Round  83, Global train loss: 0.954, Global test loss: 1.496, Global test accuracy: 54.13 

Round  84, Train loss: 0.909, Test loss: 2.242, Test accuracy: 42.91 

Round  84, Global train loss: 0.909, Global test loss: 1.491, Global test accuracy: 54.73 

Round  85, Train loss: 0.983, Test loss: 2.254, Test accuracy: 43.13 

Round  85, Global train loss: 0.983, Global test loss: 1.548, Global test accuracy: 52.10 

Round  86, Train loss: 0.866, Test loss: 2.273, Test accuracy: 43.40 

Round  86, Global train loss: 0.866, Global test loss: 1.611, Global test accuracy: 52.12 

Round  87, Train loss: 0.875, Test loss: 2.272, Test accuracy: 43.51 

Round  87, Global train loss: 0.875, Global test loss: 1.446, Global test accuracy: 56.36 

Round  88, Train loss: 0.759, Test loss: 2.317, Test accuracy: 43.38 

Round  88, Global train loss: 0.759, Global test loss: 1.535, Global test accuracy: 54.66 

Round  89, Train loss: 0.920, Test loss: 2.314, Test accuracy: 42.98 

Round  89, Global train loss: 0.920, Global test loss: 1.511, Global test accuracy: 54.27 

Round  90, Train loss: 0.784, Test loss: 2.311, Test accuracy: 43.01 

Round  90, Global train loss: 0.784, Global test loss: 1.499, Global test accuracy: 55.64 

Round  91, Train loss: 0.894, Test loss: 2.335, Test accuracy: 42.99 

Round  91, Global train loss: 0.894, Global test loss: 1.511, Global test accuracy: 53.78 

Round  92, Train loss: 0.806, Test loss: 2.348, Test accuracy: 42.90 

Round  92, Global train loss: 0.806, Global test loss: 1.507, Global test accuracy: 55.02 

Round  93, Train loss: 0.865, Test loss: 2.367, Test accuracy: 42.66 

Round  93, Global train loss: 0.865, Global test loss: 1.528, Global test accuracy: 54.19 

Round  94, Train loss: 0.737, Test loss: 2.385, Test accuracy: 42.88 

Round  94, Global train loss: 0.737, Global test loss: 1.517, Global test accuracy: 56.71 

Round  95, Train loss: 0.783, Test loss: 2.384, Test accuracy: 42.87 

Round  95, Global train loss: 0.783, Global test loss: 1.504, Global test accuracy: 55.57 

Round  96, Train loss: 0.923, Test loss: 2.378, Test accuracy: 43.11 

Round  96, Global train loss: 0.923, Global test loss: 1.623, Global test accuracy: 51.26 

Round  97, Train loss: 0.833, Test loss: 2.390, Test accuracy: 43.01 

Round  97, Global train loss: 0.833, Global test loss: 1.525, Global test accuracy: 54.06 

Round  98, Train loss: 0.742, Test loss: 2.411, Test accuracy: 43.06 

Round  98, Global train loss: 0.742, Global test loss: 1.513, Global test accuracy: 55.70 

Round  99, Train loss: 0.772, Test loss: 2.381, Test accuracy: 42.91 

Round  99, Global train loss: 0.772, Global test loss: 1.557, Global test accuracy: 54.13 

Final Round, Train loss: 0.641, Test loss: 2.791, Test accuracy: 41.92 

Final Round, Global train loss: 0.641, Global test loss: 1.557, Global test accuracy: 54.13 

Average accuracy final 10 rounds: 42.939499999999995 

Average global accuracy final 10 rounds: 54.605500000000006 

2759.557494878769
[1.3752050399780273, 2.500314474105835, 3.62616229057312, 4.758394002914429, 5.942038297653198, 7.128351211547852, 8.314512014389038, 9.5054190158844, 10.692138910293579, 11.87162971496582, 13.053578615188599, 14.236131191253662, 15.41736102104187, 16.599066019058228, 17.781373262405396, 18.96816611289978, 20.144527435302734, 21.327942609786987, 22.524208784103394, 23.705413341522217, 24.897480010986328, 26.086979866027832, 27.27952742576599, 28.471380710601807, 29.666873455047607, 30.789562225341797, 31.988431692123413, 33.223530292510986, 34.42760753631592, 35.64219284057617, 36.850244998931885, 38.0465133190155, 39.25325155258179, 40.44507813453674, 41.64076805114746, 42.8427836894989, 44.04184079170227, 45.23195004463196, 46.43536138534546, 47.64047384262085, 48.83867573738098, 50.044689893722534, 51.25366497039795, 52.446908712387085, 53.65459108352661, 54.85813546180725, 56.05798697471619, 57.261462688446045, 58.463358879089355, 59.664751052856445, 60.86638903617859, 62.068925619125366, 63.27312684059143, 64.47905468940735, 65.68543601036072, 66.87660002708435, 68.07922911643982, 69.27874183654785, 70.47954034805298, 71.67938446998596, 72.87731909751892, 74.0750458240509, 75.27473616600037, 76.38736915588379, 77.41149139404297, 78.43233251571655, 79.53810977935791, 80.55907654762268, 81.58217096328735, 82.59871745109558, 83.6199803352356, 84.64002776145935, 85.6577479839325, 86.68201899528503, 87.70624279975891, 88.73214888572693, 89.76142764091492, 90.78316378593445, 91.81179547309875, 92.8423068523407, 93.86559748649597, 94.90155673027039, 95.92828226089478, 96.95343136787415, 97.98456597328186, 99.01376628875732, 100.04109239578247, 101.07550477981567, 102.0923125743866, 103.13824534416199, 104.20159101486206, 105.22237873077393, 106.25115036964417, 107.2827513217926, 108.3258798122406, 109.45212960243225, 110.48930931091309, 111.52584671974182, 112.56191825866699, 113.6012511253357, 115.84120774269104]
[20.3475, 22.6225, 25.785, 26.8275, 28.9025, 30.405, 31.9575, 33.66, 33.615, 34.4475, 35.73, 36.3025, 36.485, 37.0825, 38.205, 38.6575, 39.395, 38.96, 39.63, 40.015, 40.675, 40.6275, 41.05, 41.415, 41.4375, 42.09, 42.175, 42.4525, 42.53, 42.4325, 42.9825, 42.4775, 43.0525, 43.325, 43.1825, 43.2575, 43.59, 43.14, 43.1975, 42.79, 42.45, 42.385, 43.0825, 42.9675, 43.185, 42.9575, 42.9775, 42.755, 43.0575, 43.235, 43.69, 43.55, 43.6475, 43.53, 43.39, 43.2625, 43.4025, 43.42, 43.305, 43.1875, 43.13, 43.37, 42.95, 42.8, 42.465, 42.795, 43.1975, 42.82, 42.745, 42.945, 42.915, 43.1425, 43.34, 43.295, 43.38, 43.28, 42.99, 43.2825, 43.0675, 43.0675, 43.095, 43.165, 43.09, 43.0775, 42.905, 43.1325, 43.4025, 43.505, 43.375, 42.98, 43.0075, 42.995, 42.9, 42.665, 42.8775, 42.8675, 43.1075, 43.0075, 43.0575, 42.91, 41.9175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8520
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2845
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8185
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1110
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5645
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7955
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0670
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7030
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5205
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8755
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.212, Test loss: 2.218, Test accuracy: 20.18 

Round   0, Global train loss: 2.212, Global test loss: 2.199, Global test accuracy: 20.72 

Round   1, Train loss: 2.093, Test loss: 2.137, Test accuracy: 24.08 

Round   1, Global train loss: 2.093, Global test loss: 2.059, Global test accuracy: 26.69 

Round   2, Train loss: 2.029, Test loss: 2.081, Test accuracy: 26.41 

Round   2, Global train loss: 2.029, Global test loss: 1.951, Global test accuracy: 30.87 

Round   3, Train loss: 1.979, Test loss: 2.060, Test accuracy: 27.57 

Round   3, Global train loss: 1.979, Global test loss: 1.880, Global test accuracy: 34.32 

Round   4, Train loss: 1.886, Test loss: 2.008, Test accuracy: 28.59 

Round   4, Global train loss: 1.886, Global test loss: 1.783, Global test accuracy: 34.95 

Round   5, Train loss: 1.870, Test loss: 1.964, Test accuracy: 30.68 

Round   5, Global train loss: 1.870, Global test loss: 1.735, Global test accuracy: 38.52 

Round   6, Train loss: 1.800, Test loss: 1.911, Test accuracy: 32.71 

Round   6, Global train loss: 1.800, Global test loss: 1.651, Global test accuracy: 40.29 

Round   7, Train loss: 1.850, Test loss: 1.880, Test accuracy: 33.93 

Round   7, Global train loss: 1.850, Global test loss: 1.673, Global test accuracy: 42.19 

Round   8, Train loss: 1.714, Test loss: 1.858, Test accuracy: 34.38 

Round   8, Global train loss: 1.714, Global test loss: 1.579, Global test accuracy: 43.32 

Round   9, Train loss: 1.791, Test loss: 1.841, Test accuracy: 35.31 

Round   9, Global train loss: 1.791, Global test loss: 1.614, Global test accuracy: 44.45 

Round  10, Train loss: 1.778, Test loss: 1.795, Test accuracy: 36.84 

Round  10, Global train loss: 1.778, Global test loss: 1.600, Global test accuracy: 43.61 

Round  11, Train loss: 1.708, Test loss: 1.778, Test accuracy: 37.55 

Round  11, Global train loss: 1.708, Global test loss: 1.526, Global test accuracy: 46.97 

Round  12, Train loss: 1.666, Test loss: 1.786, Test accuracy: 37.22 

Round  12, Global train loss: 1.666, Global test loss: 1.519, Global test accuracy: 46.74 

Round  13, Train loss: 1.688, Test loss: 1.773, Test accuracy: 37.72 

Round  13, Global train loss: 1.688, Global test loss: 1.533, Global test accuracy: 48.97 

Round  14, Train loss: 1.653, Test loss: 1.735, Test accuracy: 39.08 

Round  14, Global train loss: 1.653, Global test loss: 1.481, Global test accuracy: 49.22 

Round  15, Train loss: 1.634, Test loss: 1.728, Test accuracy: 39.69 

Round  15, Global train loss: 1.634, Global test loss: 1.473, Global test accuracy: 49.18 

Round  16, Train loss: 1.663, Test loss: 1.709, Test accuracy: 40.65 

Round  16, Global train loss: 1.663, Global test loss: 1.452, Global test accuracy: 51.10 

Round  17, Train loss: 1.679, Test loss: 1.701, Test accuracy: 41.04 

Round  17, Global train loss: 1.679, Global test loss: 1.465, Global test accuracy: 50.29 

Round  18, Train loss: 1.559, Test loss: 1.692, Test accuracy: 41.50 

Round  18, Global train loss: 1.559, Global test loss: 1.398, Global test accuracy: 51.13 

Round  19, Train loss: 1.536, Test loss: 1.680, Test accuracy: 42.38 

Round  19, Global train loss: 1.536, Global test loss: 1.408, Global test accuracy: 52.25 

Round  20, Train loss: 1.605, Test loss: 1.670, Test accuracy: 42.94 

Round  20, Global train loss: 1.605, Global test loss: 1.443, Global test accuracy: 51.48 

Round  21, Train loss: 1.522, Test loss: 1.677, Test accuracy: 42.52 

Round  21, Global train loss: 1.522, Global test loss: 1.385, Global test accuracy: 51.95 

Round  22, Train loss: 1.603, Test loss: 1.658, Test accuracy: 43.09 

Round  22, Global train loss: 1.603, Global test loss: 1.430, Global test accuracy: 53.15 

Round  23, Train loss: 1.522, Test loss: 1.658, Test accuracy: 43.22 

Round  23, Global train loss: 1.522, Global test loss: 1.375, Global test accuracy: 53.20 

Round  24, Train loss: 1.496, Test loss: 1.665, Test accuracy: 43.12 

Round  24, Global train loss: 1.496, Global test loss: 1.353, Global test accuracy: 53.91 

Round  25, Train loss: 1.536, Test loss: 1.651, Test accuracy: 43.68 

Round  25, Global train loss: 1.536, Global test loss: 1.357, Global test accuracy: 55.36 

Round  26, Train loss: 1.510, Test loss: 1.634, Test accuracy: 44.28 

Round  26, Global train loss: 1.510, Global test loss: 1.348, Global test accuracy: 55.12 

Round  27, Train loss: 1.444, Test loss: 1.633, Test accuracy: 44.69 

Round  27, Global train loss: 1.444, Global test loss: 1.315, Global test accuracy: 55.55 

Round  28, Train loss: 1.414, Test loss: 1.646, Test accuracy: 44.38 

Round  28, Global train loss: 1.414, Global test loss: 1.332, Global test accuracy: 56.00 

Round  29, Train loss: 1.485, Test loss: 1.642, Test accuracy: 44.68 

Round  29, Global train loss: 1.485, Global test loss: 1.350, Global test accuracy: 55.81 

Round  30, Train loss: 1.437, Test loss: 1.647, Test accuracy: 44.41 

Round  30, Global train loss: 1.437, Global test loss: 1.334, Global test accuracy: 55.68 

Round  31, Train loss: 1.342, Test loss: 1.637, Test accuracy: 44.71 

Round  31, Global train loss: 1.342, Global test loss: 1.279, Global test accuracy: 57.62 

Round  32, Train loss: 1.340, Test loss: 1.628, Test accuracy: 45.24 

Round  32, Global train loss: 1.340, Global test loss: 1.241, Global test accuracy: 58.04 

Round  33, Train loss: 1.472, Test loss: 1.606, Test accuracy: 45.99 

Round  33, Global train loss: 1.472, Global test loss: 1.323, Global test accuracy: 56.48 

Round  34, Train loss: 1.359, Test loss: 1.603, Test accuracy: 46.11 

Round  34, Global train loss: 1.359, Global test loss: 1.282, Global test accuracy: 56.92 

Round  35, Train loss: 1.340, Test loss: 1.612, Test accuracy: 46.29 

Round  35, Global train loss: 1.340, Global test loss: 1.258, Global test accuracy: 58.59 

Round  36, Train loss: 1.279, Test loss: 1.613, Test accuracy: 46.12 

Round  36, Global train loss: 1.279, Global test loss: 1.257, Global test accuracy: 57.92 

Round  37, Train loss: 1.293, Test loss: 1.615, Test accuracy: 46.40 

Round  37, Global train loss: 1.293, Global test loss: 1.254, Global test accuracy: 57.92 

Round  38, Train loss: 1.277, Test loss: 1.627, Test accuracy: 46.29 

Round  38, Global train loss: 1.277, Global test loss: 1.274, Global test accuracy: 57.30 

Round  39, Train loss: 1.373, Test loss: 1.618, Test accuracy: 46.97 

Round  39, Global train loss: 1.373, Global test loss: 1.320, Global test accuracy: 56.96 

Round  40, Train loss: 1.258, Test loss: 1.630, Test accuracy: 46.84 

Round  40, Global train loss: 1.258, Global test loss: 1.226, Global test accuracy: 59.12 

Round  41, Train loss: 1.283, Test loss: 1.659, Test accuracy: 46.42 

Round  41, Global train loss: 1.283, Global test loss: 1.276, Global test accuracy: 57.58 

Round  42, Train loss: 1.324, Test loss: 1.626, Test accuracy: 47.41 

Round  42, Global train loss: 1.324, Global test loss: 1.276, Global test accuracy: 58.48 

Round  43, Train loss: 1.215, Test loss: 1.641, Test accuracy: 47.12 

Round  43, Global train loss: 1.215, Global test loss: 1.233, Global test accuracy: 58.97 

Round  44, Train loss: 1.186, Test loss: 1.644, Test accuracy: 47.27 

Round  44, Global train loss: 1.186, Global test loss: 1.239, Global test accuracy: 58.40 

Round  45, Train loss: 1.287, Test loss: 1.642, Test accuracy: 47.46 

Round  45, Global train loss: 1.287, Global test loss: 1.294, Global test accuracy: 56.80 

Round  46, Train loss: 1.124, Test loss: 1.642, Test accuracy: 47.59 

Round  46, Global train loss: 1.124, Global test loss: 1.259, Global test accuracy: 58.34 

Round  47, Train loss: 1.189, Test loss: 1.654, Test accuracy: 47.43 

Round  47, Global train loss: 1.189, Global test loss: 1.239, Global test accuracy: 58.66 

Round  48, Train loss: 1.155, Test loss: 1.662, Test accuracy: 47.65 

Round  48, Global train loss: 1.155, Global test loss: 1.232, Global test accuracy: 59.41 

Round  49, Train loss: 1.099, Test loss: 1.661, Test accuracy: 47.84 

Round  49, Global train loss: 1.099, Global test loss: 1.229, Global test accuracy: 59.30 

Round  50, Train loss: 1.161, Test loss: 1.665, Test accuracy: 47.97 

Round  50, Global train loss: 1.161, Global test loss: 1.205, Global test accuracy: 59.91 

Round  51, Train loss: 1.082, Test loss: 1.656, Test accuracy: 48.16 

Round  51, Global train loss: 1.082, Global test loss: 1.214, Global test accuracy: 59.14 

Round  52, Train loss: 1.135, Test loss: 1.668, Test accuracy: 47.99 

Round  52, Global train loss: 1.135, Global test loss: 1.198, Global test accuracy: 60.80 

Round  53, Train loss: 1.159, Test loss: 1.658, Test accuracy: 48.13 

Round  53, Global train loss: 1.159, Global test loss: 1.203, Global test accuracy: 60.34 

Round  54, Train loss: 1.031, Test loss: 1.656, Test accuracy: 48.22 

Round  54, Global train loss: 1.031, Global test loss: 1.218, Global test accuracy: 59.53 

Round  55, Train loss: 1.103, Test loss: 1.653, Test accuracy: 48.49 

Round  55, Global train loss: 1.103, Global test loss: 1.192, Global test accuracy: 60.51 

Round  56, Train loss: 1.013, Test loss: 1.675, Test accuracy: 48.36 

Round  56, Global train loss: 1.013, Global test loss: 1.174, Global test accuracy: 60.95 

Round  57, Train loss: 1.029, Test loss: 1.679, Test accuracy: 48.65 

Round  57, Global train loss: 1.029, Global test loss: 1.178, Global test accuracy: 60.91 

Round  58, Train loss: 1.145, Test loss: 1.696, Test accuracy: 48.73 

Round  58, Global train loss: 1.145, Global test loss: 1.281, Global test accuracy: 57.62 

Round  59, Train loss: 1.001, Test loss: 1.715, Test accuracy: 48.58 

Round  59, Global train loss: 1.001, Global test loss: 1.210, Global test accuracy: 60.30 

Round  60, Train loss: 1.064, Test loss: 1.720, Test accuracy: 48.36 

Round  60, Global train loss: 1.064, Global test loss: 1.283, Global test accuracy: 58.29 

Round  61, Train loss: 1.026, Test loss: 1.713, Test accuracy: 48.36 

Round  61, Global train loss: 1.026, Global test loss: 1.216, Global test accuracy: 60.43 

Round  62, Train loss: 0.943, Test loss: 1.708, Test accuracy: 48.46 

Round  62, Global train loss: 0.943, Global test loss: 1.213, Global test accuracy: 60.05 

Round  63, Train loss: 0.996, Test loss: 1.719, Test accuracy: 48.33 

Round  63, Global train loss: 0.996, Global test loss: 1.242, Global test accuracy: 59.41 

Round  64, Train loss: 1.042, Test loss: 1.725, Test accuracy: 48.47 

Round  64, Global train loss: 1.042, Global test loss: 1.200, Global test accuracy: 61.20 

Round  65, Train loss: 1.099, Test loss: 1.730, Test accuracy: 48.40 

Round  65, Global train loss: 1.099, Global test loss: 1.281, Global test accuracy: 58.56 

Round  66, Train loss: 1.053, Test loss: 1.727, Test accuracy: 48.59 

Round  66, Global train loss: 1.053, Global test loss: 1.190, Global test accuracy: 60.95 

Round  67, Train loss: 1.070, Test loss: 1.734, Test accuracy: 48.68 

Round  67, Global train loss: 1.070, Global test loss: 1.238, Global test accuracy: 60.25 

Round  68, Train loss: 0.981, Test loss: 1.768, Test accuracy: 48.38 

Round  68, Global train loss: 0.981, Global test loss: 1.229, Global test accuracy: 60.19 

Round  69, Train loss: 0.831, Test loss: 1.766, Test accuracy: 48.31 

Round  69, Global train loss: 0.831, Global test loss: 1.199, Global test accuracy: 61.22 

Round  70, Train loss: 0.938, Test loss: 1.770, Test accuracy: 48.21 

Round  70, Global train loss: 0.938, Global test loss: 1.239, Global test accuracy: 59.35 

Round  71, Train loss: 0.918, Test loss: 1.764, Test accuracy: 48.44 

Round  71, Global train loss: 0.918, Global test loss: 1.267, Global test accuracy: 58.97 

Round  72, Train loss: 0.914, Test loss: 1.784, Test accuracy: 48.29 

Round  72, Global train loss: 0.914, Global test loss: 1.277, Global test accuracy: 59.20 

Round  73, Train loss: 0.992, Test loss: 1.751, Test accuracy: 48.81 

Round  73, Global train loss: 0.992, Global test loss: 1.232, Global test accuracy: 60.28 

Round  74, Train loss: 0.984, Test loss: 1.763, Test accuracy: 48.83 

Round  74, Global train loss: 0.984, Global test loss: 1.222, Global test accuracy: 61.12 

Round  75, Train loss: 1.037, Test loss: 1.816, Test accuracy: 48.18 

Round  75, Global train loss: 1.037, Global test loss: 1.240, Global test accuracy: 60.17 

Round  76, Train loss: 0.869, Test loss: 1.811, Test accuracy: 48.45 

Round  76, Global train loss: 0.869, Global test loss: 1.270, Global test accuracy: 59.51 

Round  77, Train loss: 0.848, Test loss: 1.832, Test accuracy: 48.70 

Round  77, Global train loss: 0.848, Global test loss: 1.284, Global test accuracy: 59.51 

Round  78, Train loss: 0.908, Test loss: 1.847, Test accuracy: 48.21 

Round  78, Global train loss: 0.908, Global test loss: 1.289, Global test accuracy: 59.32 

Round  79, Train loss: 0.850, Test loss: 1.857, Test accuracy: 48.32 

Round  79, Global train loss: 0.850, Global test loss: 1.232, Global test accuracy: 61.08 

Round  80, Train loss: 0.878, Test loss: 1.839, Test accuracy: 48.80 

Round  80, Global train loss: 0.878, Global test loss: 1.231, Global test accuracy: 61.09 

Round  81, Train loss: 0.840, Test loss: 1.797, Test accuracy: 49.41 

Round  81, Global train loss: 0.840, Global test loss: 1.233, Global test accuracy: 61.03 

Round  82, Train loss: 0.832, Test loss: 1.802, Test accuracy: 49.37 

Round  82, Global train loss: 0.832, Global test loss: 1.255, Global test accuracy: 60.46 

Round  83, Train loss: 0.968, Test loss: 1.808, Test accuracy: 49.43 

Round  83, Global train loss: 0.968, Global test loss: 1.261, Global test accuracy: 60.12 

Round  84, Train loss: 0.827, Test loss: 1.851, Test accuracy: 49.10 

Round  84, Global train loss: 0.827, Global test loss: 1.259, Global test accuracy: 61.20 

Round  85, Train loss: 0.861, Test loss: 1.862, Test accuracy: 48.80 

Round  85, Global train loss: 0.861, Global test loss: 1.258, Global test accuracy: 60.80 

Round  86, Train loss: 0.776, Test loss: 1.860, Test accuracy: 48.88 

Round  86, Global train loss: 0.776, Global test loss: 1.315, Global test accuracy: 59.87 

Round  87, Train loss: 0.921, Test loss: 1.865, Test accuracy: 49.21 

Round  87, Global train loss: 0.921, Global test loss: 1.276, Global test accuracy: 60.28 

Round  88, Train loss: 0.789, Test loss: 1.858, Test accuracy: 49.16 

Round  88, Global train loss: 0.789, Global test loss: 1.295, Global test accuracy: 59.90 

Round  89, Train loss: 0.958, Test loss: 1.879, Test accuracy: 49.23 
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  89, Global train loss: 0.958, Global test loss: 1.358, Global test accuracy: 57.29 

Round  90, Train loss: 0.757, Test loss: 1.872, Test accuracy: 49.40 

Round  90, Global train loss: 0.757, Global test loss: 1.258, Global test accuracy: 61.73 

Round  91, Train loss: 0.909, Test loss: 1.874, Test accuracy: 49.45 

Round  91, Global train loss: 0.909, Global test loss: 1.326, Global test accuracy: 58.45 

Round  92, Train loss: 0.782, Test loss: 1.883, Test accuracy: 49.01 

Round  92, Global train loss: 0.782, Global test loss: 1.316, Global test accuracy: 59.25 

Round  93, Train loss: 0.788, Test loss: 1.888, Test accuracy: 48.88 

Round  93, Global train loss: 0.788, Global test loss: 1.343, Global test accuracy: 59.38 

Round  94, Train loss: 0.793, Test loss: 1.902, Test accuracy: 48.96 

Round  94, Global train loss: 0.793, Global test loss: 1.308, Global test accuracy: 60.22 

Round  95, Train loss: 0.724, Test loss: 1.918, Test accuracy: 48.70 

Round  95, Global train loss: 0.724, Global test loss: 1.311, Global test accuracy: 60.26 

Round  96, Train loss: 0.795, Test loss: 1.896, Test accuracy: 49.34 

Round  96, Global train loss: 0.795, Global test loss: 1.315, Global test accuracy: 59.52 

Round  97, Train loss: 0.834, Test loss: 1.907, Test accuracy: 49.60 

Round  97, Global train loss: 0.834, Global test loss: 1.313, Global test accuracy: 60.25 

Round  98, Train loss: 0.867, Test loss: 1.936, Test accuracy: 49.31 

Round  98, Global train loss: 0.867, Global test loss: 1.319, Global test accuracy: 59.05 

Round  99, Train loss: 0.738, Test loss: 1.946, Test accuracy: 49.35 

Round  99, Global train loss: 0.738, Global test loss: 1.371, Global test accuracy: 58.14 

Final Round, Train loss: 0.663, Test loss: 2.158, Test accuracy: 48.95 

Final Round, Global train loss: 0.663, Global test loss: 1.371, Global test accuracy: 58.14 

Average accuracy final 10 rounds: 49.200500000000005 

Average global accuracy final 10 rounds: 59.625249999999994 

2925.8720343112946
[1.4961600303649902, 2.74809193611145, 3.9716637134552, 5.077688455581665, 6.176013469696045, 7.308549880981445, 8.410317182540894, 9.508850574493408, 10.610980749130249, 11.712639093399048, 12.815217018127441, 13.914159536361694, 15.01830244064331, 16.13798689842224, 17.25935673713684, 18.69091010093689, 19.809722423553467, 21.026837825775146, 22.23638653755188, 23.348943948745728, 24.46430253982544, 25.576289176940918, 26.676084756851196, 27.773228645324707, 28.87572407722473, 29.977773427963257, 31.07010769844055, 32.172070026397705, 33.272671699523926, 34.375641107559204, 35.4793484210968, 36.5801796913147, 37.684282541275024, 38.787291049957275, 39.88834595680237, 40.99159264564514, 42.09272027015686, 43.19364833831787, 44.295963764190674, 45.398343563079834, 46.49439001083374, 47.60490131378174, 48.807145833969116, 49.91366934776306, 51.02383804321289, 52.35127663612366, 53.55081629753113, 54.91177940368652, 56.230955600738525, 57.437270641326904, 58.723533391952515, 59.97412943840027, 61.264883041381836, 62.38095211982727, 63.57586097717285, 64.89048671722412, 66.22963428497314, 67.48014307022095, 68.67357277870178, 70.06561636924744, 71.44487619400024, 72.74392461776733, 74.02269458770752, 75.36477422714233, 76.69815278053284, 77.9805998802185, 79.27493453025818, 80.59519457817078, 81.9379780292511, 83.2324469089508, 84.52049779891968, 85.81775903701782, 87.164621591568, 88.51659798622131, 89.80922174453735, 91.14194893836975, 92.43478441238403, 93.78640532493591, 95.13416981697083, 96.48038864135742, 97.83192563056946, 99.15642619132996, 100.49886417388916, 101.84659576416016, 103.13320398330688, 104.42365884780884, 105.83380031585693, 107.11147093772888, 108.45351815223694, 109.80396366119385, 111.09644865989685, 112.40835809707642, 113.72370195388794, 115.02394247055054, 116.30418372154236, 117.64489483833313, 118.97893142700195, 120.31451106071472, 121.63756132125854, 122.9863498210907, 125.65421462059021]
[20.185, 24.0775, 26.415, 27.57, 28.595, 30.6825, 32.71, 33.93, 34.38, 35.31, 36.8425, 37.5525, 37.215, 37.7225, 39.0775, 39.685, 40.6525, 41.04, 41.4975, 42.3825, 42.935, 42.515, 43.085, 43.215, 43.1225, 43.6825, 44.2775, 44.685, 44.38, 44.6825, 44.405, 44.71, 45.245, 45.9925, 46.1075, 46.29, 46.125, 46.4, 46.2925, 46.965, 46.845, 46.4225, 47.4075, 47.125, 47.2725, 47.4625, 47.5875, 47.4275, 47.6475, 47.8425, 47.965, 48.165, 47.995, 48.135, 48.2175, 48.4925, 48.3575, 48.645, 48.725, 48.575, 48.3625, 48.3625, 48.4625, 48.3275, 48.47, 48.3975, 48.5925, 48.68, 48.375, 48.31, 48.21, 48.4375, 48.29, 48.8075, 48.8275, 48.18, 48.4475, 48.7025, 48.2075, 48.3175, 48.805, 49.415, 49.3725, 49.4325, 49.1025, 48.8, 48.8825, 49.21, 49.16, 49.235, 49.395, 49.4475, 49.0125, 48.8825, 48.96, 48.7025, 49.34, 49.6025, 49.31, 49.3525, 48.945]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8490
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3440
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8255
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0280
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5640
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8035
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1745
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6790
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4495
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8595
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8395
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2890
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8180
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1055
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5900
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8115
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1010
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7305
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5415
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8595
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  24.9200
Round 1 global test acc  31.4900
Round 2 global test acc  37.9500
Round 3 global test acc  46.0100
Round 4 global test acc  42.8100
Round 5 global test acc  47.4200
Round 6 global test acc  47.4900
Round 7 global test acc  50.1400
Round 8 global test acc  51.4000
Round 9 global test acc  53.2200
Round 10 global test acc  51.8800
Round 11 global test acc  52.2500
Round 12 global test acc  55.6800
Round 13 global test acc  54.8300
Round 14 global test acc  52.0600
Round 15 global test acc  55.5000
Round 16 global test acc  56.6100
Round 17 global test acc  56.2000
Round 18 global test acc  56.5300
Round 19 global test acc  55.7200
Round 20 global test acc  55.8800
Round 21 global test acc  56.8400
Round 22 global test acc  57.6700
Round 23 global test acc  58.3600
Round 24 global test acc  57.3400
Round 25 global test acc  58.5200
Round 26 global test acc  56.4800
Round 27 global test acc  58.5700
Round 28 global test acc  57.8300
Round 29 global test acc  57.9500
Round 30 global test acc  59.5300
Round 31 global test acc  56.0300
Round 32 global test acc  60.6000
Round 33 global test acc  56.8500
Round 34 global test acc  58.5100
Round 35 global test acc  60.3600
Round 36 global test acc  60.7300
Round 37 global test acc  59.8900
Round 38 global test acc  58.7500
Round 39 global test acc  58.0900
Round 40 global test acc  60.6300
Round 41 global test acc  59.5000
Round 42 global test acc  59.6100
Round 43 global test acc  62.4700
Round 44 global test acc  59.2600
Round 45 global test acc  61.7500
Round 46 global test acc  59.6300
Round 47 global test acc  59.5800
Round 48 global test acc  57.9400
Round 49 global test acc  58.5300
Round 50 global test acc  63.3200
Round 51 global test acc  62.1600
Round 52 global test acc  63.7700
Round 53 global test acc  60.9300
Round 54 global test acc  61.3900
Round 55 global test acc  61.1900
Round 56 global test acc  61.6400
Round 57 global test acc  57.4200
Round 58 global test acc  63.4500
Round 59 global test acc  60.4400
Round 60 global test acc  64.9400
Round 61 global test acc  64.2000
Round 62 global test acc  63.5400
Round 63 global test acc  60.1600
Round 64 global test acc  63.3200
Round 65 global test acc  62.6900
Round 66 global test acc  62.7100
Round 67 global test acc  63.2100
Round 68 global test acc  61.1800
Round 69 global test acc  62.7600
Round 70 global test acc  63.9200
Round 71 global test acc  62.8800
Round 72 global test acc  59.5500
Round 73 global test acc  62.8200
Round 74 global test acc  63.1800
Round 75 global test acc  62.4000
Round 76 global test acc  62.3500
Round 77 global test acc  64.0600
Round 78 global test acc  62.3800
Round 79 global test acc  65.6600
Round 80 global test acc  64.3700
Round 81 global test acc  63.7000
Round 82 global test acc  62.7400
Round 83 global test acc  60.9200
Round 84 global test acc  60.6100
Round 85 global test acc  59.8900
Round 86 global test acc  59.0500
Round 87 global test acc  59.2500
Round 88 global test acc  58.9000
Round 89 global test acc  59.2100
Round 90 global test acc  58.8000
Round 91 global test acc  58.9200
Round 92 global test acc  59.0800
Round 93 global test acc  57.8200
Round 94 global test acc  58.0300
Round 95 global test acc  57.5200
Round 96 global test acc  57.1800
Round 97 global test acc  57.1500
Round 98 global test acc  56.8800
Round 99 global test acc  56.8400
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8570
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2885
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8240
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0270
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5920
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8410
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.4200
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6940
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4560
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8550
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.283, Test loss: 2.251, Test accuracy: 17.87
Round   1, Train loss: 2.237, Test loss: 2.230, Test accuracy: 20.50
Round   2, Train loss: 2.188, Test loss: 2.159, Test accuracy: 23.23
Round   3, Train loss: 2.183, Test loss: 2.160, Test accuracy: 25.11
Round   4, Train loss: 2.150, Test loss: 2.135, Test accuracy: 26.47
Round   5, Train loss: 2.091, Test loss: 2.082, Test accuracy: 28.25
Round   6, Train loss: 2.109, Test loss: 2.076, Test accuracy: 29.28
Round   7, Train loss: 2.135, Test loss: 2.067, Test accuracy: 29.94
Round   8, Train loss: 2.041, Test loss: 2.035, Test accuracy: 30.72
Round   9, Train loss: 2.145, Test loss: 2.046, Test accuracy: 30.80
Round  10, Train loss: 1.967, Test loss: 1.979, Test accuracy: 33.24
Round  11, Train loss: 2.062, Test loss: 1.986, Test accuracy: 33.09
Round  12, Train loss: 2.151, Test loss: 2.019, Test accuracy: 32.86
Round  13, Train loss: 1.984, Test loss: 1.961, Test accuracy: 34.43
Round  14, Train loss: 1.981, Test loss: 1.952, Test accuracy: 34.66
Round  15, Train loss: 1.885, Test loss: 1.922, Test accuracy: 35.20
Round  16, Train loss: 1.945, Test loss: 1.920, Test accuracy: 35.67
Round  17, Train loss: 1.981, Test loss: 1.909, Test accuracy: 35.76
Round  18, Train loss: 1.907, Test loss: 1.889, Test accuracy: 36.34
Round  19, Train loss: 1.962, Test loss: 1.886, Test accuracy: 37.34
Round  20, Train loss: 2.003, Test loss: 1.896, Test accuracy: 37.11
Round  21, Train loss: 1.934, Test loss: 1.859, Test accuracy: 38.25
Round  22, Train loss: 2.045, Test loss: 1.875, Test accuracy: 37.80
Round  23, Train loss: 1.975, Test loss: 1.884, Test accuracy: 37.16
Round  24, Train loss: 1.917, Test loss: 1.865, Test accuracy: 37.73
Round  25, Train loss: 1.876, Test loss: 1.851, Test accuracy: 37.86
Round  26, Train loss: 1.923, Test loss: 1.843, Test accuracy: 38.30
Round  27, Train loss: 1.836, Test loss: 1.826, Test accuracy: 38.45
Round  28, Train loss: 1.932, Test loss: 1.826, Test accuracy: 38.38
Round  29, Train loss: 1.824, Test loss: 1.818, Test accuracy: 39.05
Round  30, Train loss: 1.876, Test loss: 1.818, Test accuracy: 39.16
Round  31, Train loss: 1.907, Test loss: 1.820, Test accuracy: 39.30
Round  32, Train loss: 1.831, Test loss: 1.798, Test accuracy: 39.24
Round  33, Train loss: 1.822, Test loss: 1.794, Test accuracy: 39.46
Round  34, Train loss: 1.760, Test loss: 1.784, Test accuracy: 39.78
Round  35, Train loss: 1.865, Test loss: 1.781, Test accuracy: 39.62
Round  36, Train loss: 1.898, Test loss: 1.792, Test accuracy: 39.49
Round  37, Train loss: 1.877, Test loss: 1.789, Test accuracy: 39.61
Round  38, Train loss: 1.792, Test loss: 1.771, Test accuracy: 40.17
Round  39, Train loss: 1.946, Test loss: 1.784, Test accuracy: 40.10
Round  40, Train loss: 1.591, Test loss: 1.768, Test accuracy: 40.34
Round  41, Train loss: 2.001, Test loss: 1.790, Test accuracy: 39.72
Round  42, Train loss: 1.937, Test loss: 1.778, Test accuracy: 40.13
Round  43, Train loss: 1.792, Test loss: 1.763, Test accuracy: 40.21
Round  44, Train loss: 1.823, Test loss: 1.769, Test accuracy: 40.40
Round  45, Train loss: 1.842, Test loss: 1.767, Test accuracy: 40.17
Round  46, Train loss: 1.848, Test loss: 1.778, Test accuracy: 39.76
Round  47, Train loss: 1.888, Test loss: 1.769, Test accuracy: 40.01
Round  48, Train loss: 1.738, Test loss: 1.763, Test accuracy: 40.28
Round  49, Train loss: 1.737, Test loss: 1.763, Test accuracy: 40.21
Round  50, Train loss: 1.730, Test loss: 1.760, Test accuracy: 40.26
Round  51, Train loss: 1.694, Test loss: 1.751, Test accuracy: 40.47
Round  52, Train loss: 1.754, Test loss: 1.746, Test accuracy: 40.63
Round  53, Train loss: 1.681, Test loss: 1.731, Test accuracy: 40.92
Round  54, Train loss: 1.764, Test loss: 1.737, Test accuracy: 40.72
Round  55, Train loss: 1.590, Test loss: 1.728, Test accuracy: 41.14
Round  56, Train loss: 1.647, Test loss: 1.723, Test accuracy: 41.49
Round  57, Train loss: 1.630, Test loss: 1.728, Test accuracy: 41.09
Round  58, Train loss: 1.793, Test loss: 1.743, Test accuracy: 40.53
Round  59, Train loss: 1.764, Test loss: 1.730, Test accuracy: 41.34
Round  60, Train loss: 1.776, Test loss: 1.733, Test accuracy: 41.19
Round  61, Train loss: 1.791, Test loss: 1.732, Test accuracy: 40.89
Round  62, Train loss: 1.728, Test loss: 1.739, Test accuracy: 40.48
Round  63, Train loss: 1.460, Test loss: 1.719, Test accuracy: 41.20
Round  64, Train loss: 1.503, Test loss: 1.695, Test accuracy: 42.08
Round  65, Train loss: 1.908, Test loss: 1.726, Test accuracy: 40.92
Round  66, Train loss: 1.548, Test loss: 1.721, Test accuracy: 41.05
Round  67, Train loss: 1.729, Test loss: 1.720, Test accuracy: 40.88
Round  68, Train loss: 1.610, Test loss: 1.709, Test accuracy: 41.13
Round  69, Train loss: 1.556, Test loss: 1.719, Test accuracy: 40.96
Round  70, Train loss: 1.546, Test loss: 1.724, Test accuracy: 40.79
Round  71, Train loss: 1.595, Test loss: 1.733, Test accuracy: 40.58
Round  72, Train loss: 1.709, Test loss: 1.743, Test accuracy: 40.19
Round  73, Train loss: 1.607, Test loss: 1.722, Test accuracy: 41.05
Round  74, Train loss: 1.637, Test loss: 1.713, Test accuracy: 41.15
Round  75, Train loss: 1.486, Test loss: 1.703, Test accuracy: 41.51
Round  76, Train loss: 1.483, Test loss: 1.709, Test accuracy: 41.19
Round  77, Train loss: 1.532, Test loss: 1.725, Test accuracy: 40.78
Round  78, Train loss: 1.477, Test loss: 1.715, Test accuracy: 41.05
Round  79, Train loss: 1.603, Test loss: 1.720, Test accuracy: 40.97
Round  80, Train loss: 1.592, Test loss: 1.728, Test accuracy: 40.91
Round  81, Train loss: 1.381, Test loss: 1.718, Test accuracy: 41.01
Round  82, Train loss: 1.361, Test loss: 1.711, Test accuracy: 41.66
Round  83, Train loss: 1.795, Test loss: 1.742, Test accuracy: 40.45
Round  84, Train loss: 1.438, Test loss: 1.722, Test accuracy: 41.14
Round  85, Train loss: 1.529, Test loss: 1.729, Test accuracy: 40.82
Round  86, Train loss: 1.522, Test loss: 1.739, Test accuracy: 40.11
Round  87, Train loss: 1.499, Test loss: 1.719, Test accuracy: 40.77
Round  88, Train loss: 1.445, Test loss: 1.723, Test accuracy: 40.65
Round  89, Train loss: 1.459, Test loss: 1.722, Test accuracy: 40.84
Round  90, Train loss: 1.383, Test loss: 1.723, Test accuracy: 40.96
Round  91, Train loss: 1.631, Test loss: 1.751, Test accuracy: 40.19
Round  92, Train loss: 1.516, Test loss: 1.745, Test accuracy: 40.44
Round  93, Train loss: 1.414, Test loss: 1.755, Test accuracy: 40.15
Round  94, Train loss: 1.287, Test loss: 1.739, Test accuracy: 40.59
Round  95, Train loss: 1.531, Test loss: 1.762, Test accuracy: 39.81
Round  96, Train loss: 1.523, Test loss: 1.761, Test accuracy: 40.12
Round  97, Train loss: 1.420, Test loss: 1.755, Test accuracy: 40.43
Round  98, Train loss: 1.497, Test loss: 1.778, Test accuracy: 39.74
Round  99, Train loss: 1.408, Test loss: 1.799, Test accuracy: 39.32
Final Round, Train loss: 1.353, Test loss: 1.819, Test accuracy: 39.10
Average accuracy final 10 rounds: 40.175000000000004
1842.03275847435
[1.6406831741333008, 2.994313955307007, 4.3187785148620605, 5.665125370025635, 7.0310282707214355, 8.376205682754517, 9.738981485366821, 11.094267845153809, 12.449124336242676, 13.81663179397583, 15.177855253219604, 16.519243001937866, 17.87836766242981, 19.235944032669067, 20.585863828659058, 21.93458843231201, 23.279825925827026, 24.52827501296997, 25.7661075592041, 27.061787605285645, 28.402459859848022, 29.719491243362427, 31.05775022506714, 32.38764524459839, 33.70450949668884, 35.03192138671875, 36.38087558746338, 37.74530243873596, 39.102410316467285, 40.44806241989136, 41.76711463928223, 43.11305809020996, 44.46667695045471, 45.822402238845825, 47.166003465652466, 48.50641441345215, 49.84209179878235, 51.174965143203735, 52.52159833908081, 53.86426615715027, 55.206082820892334, 56.55923008918762, 57.90942978858948, 59.24826526641846, 60.59442734718323, 61.94252848625183, 63.278035402297974, 64.62656807899475, 65.97009825706482, 67.30826783180237, 68.64492416381836, 70.00247049331665, 71.36836886405945, 72.71229434013367, 74.04882836341858, 75.40412998199463, 76.75533485412598, 78.09786343574524, 79.42767238616943, 80.7740421295166, 82.1187596321106, 83.46992707252502, 84.81817150115967, 86.15159702301025, 87.48340034484863, 88.83237409591675, 90.19828629493713, 91.54734778404236, 92.88613176345825, 94.21552777290344, 95.56502938270569, 96.93238401412964, 98.15276718139648, 99.34991717338562, 100.54807567596436, 101.75647473335266, 102.9678966999054, 104.17409920692444, 105.36806917190552, 106.57961893081665, 107.78840970993042, 108.98509931564331, 110.19035029411316, 111.38345694541931, 112.59585618972778, 113.81197357177734, 115.01090669631958, 116.21316289901733, 117.41189074516296, 118.61345338821411, 119.9500298500061, 121.27137184143066, 122.59047555923462, 123.94106101989746, 125.28176641464233, 126.62333798408508, 127.96091437339783, 129.28216934204102, 130.5986363887787, 131.95044684410095, 133.99141788482666]
[17.87, 20.505, 23.2275, 25.105, 26.47, 28.2475, 29.2825, 29.9375, 30.7175, 30.8025, 33.24, 33.0925, 32.8625, 34.4325, 34.655, 35.1975, 35.67, 35.76, 36.3375, 37.335, 37.1075, 38.25, 37.8, 37.155, 37.7325, 37.86, 38.295, 38.455, 38.375, 39.045, 39.1625, 39.3, 39.2425, 39.4625, 39.785, 39.625, 39.495, 39.6075, 40.1675, 40.0975, 40.3375, 39.72, 40.1325, 40.2075, 40.4, 40.17, 39.7575, 40.01, 40.2825, 40.21, 40.2625, 40.47, 40.6275, 40.9175, 40.7225, 41.1375, 41.49, 41.09, 40.53, 41.345, 41.1875, 40.89, 40.4825, 41.2, 42.08, 40.92, 41.055, 40.88, 41.1275, 40.9625, 40.7925, 40.5775, 40.1925, 41.05, 41.145, 41.505, 41.1925, 40.785, 41.055, 40.965, 40.9125, 41.0125, 41.665, 40.455, 41.1375, 40.8175, 40.11, 40.7675, 40.6525, 40.835, 40.96, 40.1875, 40.44, 40.15, 40.585, 39.815, 40.115, 40.4325, 39.745, 39.32, 39.1025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8575
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.4680
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8335
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0815
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5695
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8315
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0930
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7060
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5440
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8595
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8760
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5810
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8730
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5155
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7605
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8430
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.6130
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7860
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7285
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8895
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.206, Test loss: 2.236, Test accuracy: 19.65 

Round   0, Global train loss: 2.206, Global test loss: 2.218, Global test accuracy: 20.93 

Round   1, Train loss: 2.201, Test loss: 2.215, Test accuracy: 21.46 

Round   1, Global train loss: 2.201, Global test loss: 2.192, Global test accuracy: 25.04 

Round   2, Train loss: 2.036, Test loss: 2.142, Test accuracy: 22.49 

Round   2, Global train loss: 2.036, Global test loss: 2.017, Global test accuracy: 28.15 

Round   3, Train loss: 2.118, Test loss: 2.169, Test accuracy: 21.90 

Round   3, Global train loss: 2.118, Global test loss: 2.073, Global test accuracy: 27.76 

Round   4, Train loss: 2.025, Test loss: 2.129, Test accuracy: 23.07 

Round   4, Global train loss: 2.025, Global test loss: 1.958, Global test accuracy: 29.16 

Round   5, Train loss: 2.176, Test loss: 2.154, Test accuracy: 23.16 

Round   5, Global train loss: 2.176, Global test loss: 2.182, Global test accuracy: 29.56 

Round   6, Train loss: 2.043, Test loss: 2.131, Test accuracy: 23.61 

Round   6, Global train loss: 2.043, Global test loss: 2.053, Global test accuracy: 32.90 

Round   7, Train loss: 1.916, Test loss: 2.132, Test accuracy: 23.44 

Round   7, Global train loss: 1.916, Global test loss: 1.955, Global test accuracy: 31.14 

Round   8, Train loss: 1.914, Test loss: 2.120, Test accuracy: 24.00 

Round   8, Global train loss: 1.914, Global test loss: 1.954, Global test accuracy: 32.00 

Round   9, Train loss: 1.877, Test loss: 2.113, Test accuracy: 24.21 

Round   9, Global train loss: 1.877, Global test loss: 1.909, Global test accuracy: 35.20 

Round  10, Train loss: 1.909, Test loss: 2.117, Test accuracy: 24.38 

Round  10, Global train loss: 1.909, Global test loss: 1.982, Global test accuracy: 33.51 

Round  11, Train loss: 1.820, Test loss: 2.118, Test accuracy: 24.79 

Round  11, Global train loss: 1.820, Global test loss: 1.911, Global test accuracy: 35.08 

Round  12, Train loss: 1.769, Test loss: 2.116, Test accuracy: 24.86 

Round  12, Global train loss: 1.769, Global test loss: 1.796, Global test accuracy: 39.17 

Round  13, Train loss: 1.838, Test loss: 2.117, Test accuracy: 25.19 

Round  13, Global train loss: 1.838, Global test loss: 1.932, Global test accuracy: 34.87 

Round  14, Train loss: 1.920, Test loss: 2.130, Test accuracy: 25.20 

Round  14, Global train loss: 1.920, Global test loss: 1.989, Global test accuracy: 34.00 

Round  15, Train loss: 1.781, Test loss: 2.136, Test accuracy: 25.03 

Round  15, Global train loss: 1.781, Global test loss: 1.872, Global test accuracy: 37.09 

Round  16, Train loss: 1.857, Test loss: 2.136, Test accuracy: 25.33 

Round  16, Global train loss: 1.857, Global test loss: 2.011, Global test accuracy: 33.55 

Round  17, Train loss: 1.867, Test loss: 2.147, Test accuracy: 25.45 

Round  17, Global train loss: 1.867, Global test loss: 2.091, Global test accuracy: 28.34 

Round  18, Train loss: 1.762, Test loss: 2.176, Test accuracy: 25.11 

Round  18, Global train loss: 1.762, Global test loss: 1.792, Global test accuracy: 39.42 

Round  19, Train loss: 1.776, Test loss: 2.186, Test accuracy: 25.29 

Round  19, Global train loss: 1.776, Global test loss: 2.032, Global test accuracy: 31.21 

Round  20, Train loss: 1.732, Test loss: 2.178, Test accuracy: 25.48 

Round  20, Global train loss: 1.732, Global test loss: 1.936, Global test accuracy: 32.77 

Round  21, Train loss: 1.795, Test loss: 2.185, Test accuracy: 25.51 

Round  21, Global train loss: 1.795, Global test loss: 1.892, Global test accuracy: 35.72 

Round  22, Train loss: 1.757, Test loss: 2.207, Test accuracy: 25.14 

Round  22, Global train loss: 1.757, Global test loss: 1.937, Global test accuracy: 37.13 

Round  23, Train loss: 1.343, Test loss: 2.238, Test accuracy: 25.25 

Round  23, Global train loss: 1.343, Global test loss: 1.721, Global test accuracy: 38.61 

Round  24, Train loss: 1.731, Test loss: 2.263, Test accuracy: 25.30 

Round  24, Global train loss: 1.731, Global test loss: 1.981, Global test accuracy: 33.54 

Round  25, Train loss: 1.475, Test loss: 2.251, Test accuracy: 25.39 

Round  25, Global train loss: 1.475, Global test loss: 1.780, Global test accuracy: 36.66 

Round  26, Train loss: 1.717, Test loss: 2.273, Test accuracy: 25.15 

Round  26, Global train loss: 1.717, Global test loss: 1.963, Global test accuracy: 32.84 

Round  27, Train loss: 1.411, Test loss: 2.314, Test accuracy: 25.07 

Round  27, Global train loss: 1.411, Global test loss: 1.886, Global test accuracy: 32.60 

Round  28, Train loss: 1.186, Test loss: 2.336, Test accuracy: 25.23 

Round  28, Global train loss: 1.186, Global test loss: 1.615, Global test accuracy: 42.20 

Round  29, Train loss: 1.387, Test loss: 2.374, Test accuracy: 25.21 

Round  29, Global train loss: 1.387, Global test loss: 1.896, Global test accuracy: 35.31 

Round  30, Train loss: 1.613, Test loss: 2.391, Test accuracy: 25.02 

Round  30, Global train loss: 1.613, Global test loss: 2.067, Global test accuracy: 31.56 

Round  31, Train loss: 1.607, Test loss: 2.420, Test accuracy: 25.16 

Round  31, Global train loss: 1.607, Global test loss: 1.995, Global test accuracy: 32.79 

Round  32, Train loss: 1.317, Test loss: 2.473, Test accuracy: 25.09 

Round  32, Global train loss: 1.317, Global test loss: 1.850, Global test accuracy: 34.35 

Round  33, Train loss: 1.129, Test loss: 2.510, Test accuracy: 25.00 

Round  33, Global train loss: 1.129, Global test loss: 1.699, Global test accuracy: 39.98 

Round  34, Train loss: 1.355, Test loss: 2.525, Test accuracy: 25.12 

Round  34, Global train loss: 1.355, Global test loss: 1.865, Global test accuracy: 33.54 

Round  35, Train loss: 1.287, Test loss: 2.571, Test accuracy: 25.17 

Round  35, Global train loss: 1.287, Global test loss: 1.845, Global test accuracy: 35.52 

Round  36, Train loss: 1.043, Test loss: 2.627, Test accuracy: 24.98 

Round  36, Global train loss: 1.043, Global test loss: 1.807, Global test accuracy: 35.48 

Round  37, Train loss: 1.323, Test loss: 2.684, Test accuracy: 24.97 

Round  37, Global train loss: 1.323, Global test loss: 1.821, Global test accuracy: 36.49 

Round  38, Train loss: 1.031, Test loss: 2.725, Test accuracy: 24.62 

Round  38, Global train loss: 1.031, Global test loss: 1.864, Global test accuracy: 34.36 

Round  39, Train loss: 1.151, Test loss: 2.781, Test accuracy: 24.58 

Round  39, Global train loss: 1.151, Global test loss: 1.893, Global test accuracy: 37.80 

Round  40, Train loss: 1.598, Test loss: 2.821, Test accuracy: 24.42 

Round  40, Global train loss: 1.598, Global test loss: 2.044, Global test accuracy: 30.89 

Round  41, Train loss: 0.898, Test loss: 2.829, Test accuracy: 24.52 

Round  41, Global train loss: 0.898, Global test loss: 1.728, Global test accuracy: 38.92 

Round  42, Train loss: 1.082, Test loss: 2.889, Test accuracy: 24.57 

Round  42, Global train loss: 1.082, Global test loss: 1.818, Global test accuracy: 37.31 

Round  43, Train loss: 1.127, Test loss: 2.930, Test accuracy: 24.72 

Round  43, Global train loss: 1.127, Global test loss: 1.811, Global test accuracy: 36.56 

Round  44, Train loss: 1.185, Test loss: 2.994, Test accuracy: 24.75 

Round  44, Global train loss: 1.185, Global test loss: 1.937, Global test accuracy: 32.01 

Round  45, Train loss: 0.803, Test loss: 3.066, Test accuracy: 24.11 

Round  45, Global train loss: 0.803, Global test loss: 1.666, Global test accuracy: 41.46 

Round  46, Train loss: 1.006, Test loss: 3.118, Test accuracy: 24.06 

Round  46, Global train loss: 1.006, Global test loss: 1.807, Global test accuracy: 36.79 

Round  47, Train loss: 1.014, Test loss: 3.129, Test accuracy: 24.18 

Round  47, Global train loss: 1.014, Global test loss: 1.884, Global test accuracy: 35.66 

Round  48, Train loss: 0.982, Test loss: 3.190, Test accuracy: 24.09 

Round  48, Global train loss: 0.982, Global test loss: 1.847, Global test accuracy: 35.62 

Round  49, Train loss: 1.133, Test loss: 3.237, Test accuracy: 24.24 

Round  49, Global train loss: 1.133, Global test loss: 2.008, Global test accuracy: 32.32 

Round  50, Train loss: 1.265, Test loss: 3.254, Test accuracy: 24.23 

Round  50, Global train loss: 1.265, Global test loss: 2.121, Global test accuracy: 25.31 

Round  51, Train loss: 0.832, Test loss: 3.347, Test accuracy: 24.09 

Round  51, Global train loss: 0.832, Global test loss: 1.836, Global test accuracy: 36.01 

Round  52, Train loss: 0.951, Test loss: 3.385, Test accuracy: 24.11 

Round  52, Global train loss: 0.951, Global test loss: 1.928, Global test accuracy: 34.66 

Round  53, Train loss: 0.817, Test loss: 3.459, Test accuracy: 23.91 

Round  53, Global train loss: 0.817, Global test loss: 1.808, Global test accuracy: 37.70 

Round  54, Train loss: 0.891, Test loss: 3.559, Test accuracy: 23.63 

Round  54, Global train loss: 0.891, Global test loss: 1.775, Global test accuracy: 38.03 

Round  55, Train loss: 1.068, Test loss: 3.657, Test accuracy: 23.50 

Round  55, Global train loss: 1.068, Global test loss: 1.968, Global test accuracy: 33.16 

Round  56, Train loss: 0.712, Test loss: 3.720, Test accuracy: 23.49 

Round  56, Global train loss: 0.712, Global test loss: 1.825, Global test accuracy: 36.29 

Round  57, Train loss: 0.794, Test loss: 3.755, Test accuracy: 23.98 

Round  57, Global train loss: 0.794, Global test loss: 1.956, Global test accuracy: 32.19 

Round  58, Train loss: 0.719, Test loss: 3.865, Test accuracy: 24.16 

Round  58, Global train loss: 0.719, Global test loss: 1.947, Global test accuracy: 32.99 

Round  59, Train loss: 0.690, Test loss: 3.925, Test accuracy: 23.87 

Round  59, Global train loss: 0.690, Global test loss: 1.892, Global test accuracy: 33.95 

Round  60, Train loss: 0.840, Test loss: 3.983, Test accuracy: 23.93 

Round  60, Global train loss: 0.840, Global test loss: 1.908, Global test accuracy: 34.59 

Round  61, Train loss: 0.612, Test loss: 3.992, Test accuracy: 24.02 

Round  61, Global train loss: 0.612, Global test loss: 1.862, Global test accuracy: 35.63 

Round  62, Train loss: 0.712, Test loss: 4.069, Test accuracy: 24.13 

Round  62, Global train loss: 0.712, Global test loss: 1.775, Global test accuracy: 38.30 

Round  63, Train loss: 0.821, Test loss: 4.113, Test accuracy: 24.16 

Round  63, Global train loss: 0.821, Global test loss: 2.045, Global test accuracy: 27.36 

Round  64, Train loss: 0.707, Test loss: 4.189, Test accuracy: 24.06 

Round  64, Global train loss: 0.707, Global test loss: 1.931, Global test accuracy: 32.33 

Round  65, Train loss: 0.512, Test loss: 4.250, Test accuracy: 24.14 

Round  65, Global train loss: 0.512, Global test loss: 1.867, Global test accuracy: 34.48 

Round  66, Train loss: 0.486, Test loss: 4.354, Test accuracy: 24.12 

Round  66, Global train loss: 0.486, Global test loss: 1.843, Global test accuracy: 35.97 

Round  67, Train loss: 0.708, Test loss: 4.390, Test accuracy: 24.09 

Round  67, Global train loss: 0.708, Global test loss: 2.001, Global test accuracy: 30.14 

Round  68, Train loss: 0.500, Test loss: 4.421, Test accuracy: 23.84 

Round  68, Global train loss: 0.500, Global test loss: 1.811, Global test accuracy: 36.06 

Round  69, Train loss: 0.479, Test loss: 4.565, Test accuracy: 23.79 

Round  69, Global train loss: 0.479, Global test loss: 1.849, Global test accuracy: 34.31 

Round  70, Train loss: 0.701, Test loss: 4.574, Test accuracy: 23.98 

Round  70, Global train loss: 0.701, Global test loss: 1.989, Global test accuracy: 29.60 

Round  71, Train loss: 0.424, Test loss: 4.608, Test accuracy: 24.14 

Round  71, Global train loss: 0.424, Global test loss: 1.836, Global test accuracy: 36.51 

Round  72, Train loss: 0.607, Test loss: 4.622, Test accuracy: 24.33 

Round  72, Global train loss: 0.607, Global test loss: 1.996, Global test accuracy: 29.16 

Round  73, Train loss: 0.388, Test loss: 4.624, Test accuracy: 24.18 

Round  73, Global train loss: 0.388, Global test loss: 1.950, Global test accuracy: 33.29 

Round  74, Train loss: 0.457, Test loss: 4.748, Test accuracy: 24.06 

Round  74, Global train loss: 0.457, Global test loss: 1.940, Global test accuracy: 32.01 

Round  75, Train loss: 0.576, Test loss: 4.836, Test accuracy: 23.94 

Round  75, Global train loss: 0.576, Global test loss: 2.030, Global test accuracy: 27.53 

Round  76, Train loss: 0.353, Test loss: 4.872, Test accuracy: 24.00 

Round  76, Global train loss: 0.353, Global test loss: 1.729, Global test accuracy: 38.18 

Round  77, Train loss: 0.581, Test loss: 4.971, Test accuracy: 24.14 

Round  77, Global train loss: 0.581, Global test loss: 1.940, Global test accuracy: 30.96 

Round  78, Train loss: 0.555, Test loss: 4.952, Test accuracy: 24.31 

Round  78, Global train loss: 0.555, Global test loss: 2.037, Global test accuracy: 26.97 

Round  79, Train loss: 0.482, Test loss: 4.980, Test accuracy: 24.18 

Round  79, Global train loss: 0.482, Global test loss: 1.877, Global test accuracy: 35.01 

Round  80, Train loss: 0.453, Test loss: 5.022, Test accuracy: 24.46 

Round  80, Global train loss: 0.453, Global test loss: 1.792, Global test accuracy: 35.40 

Round  81, Train loss: 0.258, Test loss: 5.022, Test accuracy: 24.56 

Round  81, Global train loss: 0.258, Global test loss: 1.778, Global test accuracy: 38.01 

Round  82, Train loss: 0.369, Test loss: 5.113, Test accuracy: 24.54 

Round  82, Global train loss: 0.369, Global test loss: 1.877, Global test accuracy: 33.69 

Round  83, Train loss: 0.324, Test loss: 5.205, Test accuracy: 24.35 

Round  83, Global train loss: 0.324, Global test loss: 1.866, Global test accuracy: 34.17 

Round  84, Train loss: 0.513, Test loss: 5.278, Test accuracy: 24.46 

Round  84, Global train loss: 0.513, Global test loss: 1.947, Global test accuracy: 31.07 

Round  85, Train loss: 0.428, Test loss: 5.363, Test accuracy: 24.45 

Round  85, Global train loss: 0.428, Global test loss: 1.946, Global test accuracy: 31.37 

Round  86, Train loss: 0.405, Test loss: 5.424, Test accuracy: 24.09 

Round  86, Global train loss: 0.405, Global test loss: 1.880, Global test accuracy: 33.06 

Round  87, Train loss: 0.404, Test loss: 5.530, Test accuracy: 24.10 

Round  87, Global train loss: 0.404, Global test loss: 1.944, Global test accuracy: 31.79 

Round  88, Train loss: 0.239, Test loss: 5.575, Test accuracy: 24.14 

Round  88, Global train loss: 0.239, Global test loss: 1.748, Global test accuracy: 39.74 

Round  89, Train loss: 0.293, Test loss: 5.563, Test accuracy: 24.13 

Round  89, Global train loss: 0.293, Global test loss: 1.977, Global test accuracy: 30.36 

Round  90, Train loss: 0.387, Test loss: 5.573, Test accuracy: 24.23 

Round  90, Global train loss: 0.387, Global test loss: 1.925, Global test accuracy: 30.25 

Round  91, Train loss: 0.353, Test loss: 5.650, Test accuracy: 24.12 

Round  91, Global train loss: 0.353, Global test loss: 1.995, Global test accuracy: 29.70 

Round  92, Train loss: 0.306, Test loss: 5.655, Test accuracy: 23.94 

Round  92, Global train loss: 0.306, Global test loss: 1.927, Global test accuracy: 31.41 

Round  93, Train loss: 0.323, Test loss: 5.619, Test accuracy: 24.24 

Round  93, Global train loss: 0.323, Global test loss: 1.831, Global test accuracy: 35.38 

Round  94, Train loss: 0.378, Test loss: 5.617, Test accuracy: 24.40 

Round  94, Global train loss: 0.378, Global test loss: 2.089, Global test accuracy: 22.37 

Round  95, Train loss: 0.303, Test loss: 5.739, Test accuracy: 24.25 

Round  95, Global train loss: 0.303, Global test loss: 1.882, Global test accuracy: 34.76 

Round  96, Train loss: 0.334, Test loss: 5.818, Test accuracy: 24.28 

Round  96, Global train loss: 0.334, Global test loss: 1.992, Global test accuracy: 29.36 

Round  97, Train loss: 0.372, Test loss: 5.853, Test accuracy: 24.23 

Round  97, Global train loss: 0.372, Global test loss: 2.059, Global test accuracy: 28.34 

Round  98, Train loss: 0.337, Test loss: 5.956, Test accuracy: 24.29 

Round  98, Global train loss: 0.337, Global test loss: 2.018, Global test accuracy: 27.91 

Round  99, Train loss: 0.259, Test loss: 6.055, Test accuracy: 24.21 

Round  99, Global train loss: 0.259, Global test loss: 2.004, Global test accuracy: 30.08 

Final Round, Train loss: 0.306, Test loss: 6.172, Test accuracy: 23.78 

Final Round, Global train loss: 0.306, Global test loss: 2.004, Global test accuracy: 30.08 

Average accuracy final 10 rounds: 24.2185 

Average global accuracy final 10 rounds: 29.95625 

2755.7198820114136
[1.4502027034759521, 2.663975954055786, 3.8740837574005127, 5.094020366668701, 6.300945043563843, 7.513774871826172, 8.722277164459229, 9.9246985912323, 11.129543542861938, 12.335002899169922, 13.537715196609497, 14.736268758773804, 15.939898014068604, 17.14767026901245, 18.340039253234863, 19.53866171836853, 20.738346576690674, 21.94153618812561, 23.135149002075195, 24.334784030914307, 25.536288022994995, 26.731714248657227, 27.930419921875, 29.1309757232666, 30.336440324783325, 31.551897764205933, 32.71939277648926, 33.903995513916016, 35.10586714744568, 36.31126165390015, 37.511592388153076, 38.71154546737671, 39.915764570236206, 41.12312889099121, 42.32567858695984, 43.50217270851135, 44.68296480178833, 45.87592959403992, 47.05963206291199, 48.25460338592529, 49.4475839138031, 50.629390478134155, 51.81696081161499, 53.01312184333801, 54.20010256767273, 55.39018750190735, 56.58320069313049, 57.78323793411255, 58.97070670127869, 60.0187714099884, 61.04616856575012, 62.06910800933838, 63.089674949645996, 64.11078262329102, 65.13579082489014, 66.15920782089233, 67.18153095245361, 68.20180177688599, 69.22156810760498, 70.2463538646698, 71.27123880386353, 72.29400658607483, 73.31634879112244, 74.33612656593323, 75.35706305503845, 76.37867331504822, 77.3995578289032, 78.4207124710083, 79.44623494148254, 80.46429228782654, 81.488285779953, 82.51065564155579, 83.53107023239136, 84.5586907863617, 85.5808048248291, 86.60223031044006, 87.62620830535889, 88.65178918838501, 89.67636704444885, 90.69880223274231, 91.72495746612549, 92.7516918182373, 93.77471232414246, 94.80113053321838, 95.82292938232422, 96.8510410785675, 97.87176752090454, 98.89248919487, 99.915944814682, 100.93676137924194, 101.95965576171875, 102.9852466583252, 104.00379371643066, 105.02753376960754, 106.04856467247009, 107.0714476108551, 108.09225487709045, 109.11314392089844, 110.14699625968933, 111.18348050117493, 113.27132415771484]
[19.6525, 21.4625, 22.49, 21.9025, 23.075, 23.155, 23.6075, 23.4375, 24.0025, 24.215, 24.38, 24.785, 24.8625, 25.19, 25.2025, 25.0275, 25.3275, 25.4475, 25.11, 25.2875, 25.485, 25.5125, 25.14, 25.25, 25.305, 25.395, 25.15, 25.075, 25.225, 25.2125, 25.015, 25.1625, 25.0925, 25.0025, 25.125, 25.1725, 24.9825, 24.9675, 24.6225, 24.5775, 24.4175, 24.515, 24.565, 24.7175, 24.7475, 24.1075, 24.0625, 24.185, 24.095, 24.24, 24.2275, 24.085, 24.115, 23.9075, 23.6325, 23.495, 23.4875, 23.9825, 24.165, 23.8675, 23.935, 24.0225, 24.13, 24.1575, 24.0575, 24.145, 24.1225, 24.0875, 23.8425, 23.7925, 23.9775, 24.1425, 24.33, 24.175, 24.0625, 23.94, 24.0, 24.1375, 24.3075, 24.18, 24.4575, 24.5625, 24.5425, 24.3475, 24.4575, 24.445, 24.09, 24.1, 24.145, 24.1275, 24.225, 24.1175, 23.9425, 24.2425, 24.4, 24.2475, 24.28, 24.225, 24.29, 24.215, 23.78]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8755
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6135
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8675
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5050
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7380
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8440
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5670
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7885
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7720
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8795
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.197, Test loss: 2.244, Test accuracy: 20.61 

Round   0, Global train loss: 2.197, Global test loss: 2.227, Global test accuracy: 20.77 

Round   1, Train loss: 2.083, Test loss: 2.152, Test accuracy: 24.14 

Round   1, Global train loss: 2.083, Global test loss: 2.064, Global test accuracy: 26.59 

Round   2, Train loss: 2.019, Test loss: 2.093, Test accuracy: 26.56 

Round   2, Global train loss: 2.019, Global test loss: 1.937, Global test accuracy: 32.59 

Round   3, Train loss: 1.982, Test loss: 2.078, Test accuracy: 27.48 

Round   3, Global train loss: 1.982, Global test loss: 1.889, Global test accuracy: 34.95 

Round   4, Train loss: 1.969, Test loss: 2.049, Test accuracy: 28.31 

Round   4, Global train loss: 1.969, Global test loss: 1.829, Global test accuracy: 36.20 

Round   5, Train loss: 1.918, Test loss: 2.012, Test accuracy: 29.84 

Round   5, Global train loss: 1.918, Global test loss: 1.764, Global test accuracy: 38.16 

Round   6, Train loss: 1.904, Test loss: 1.990, Test accuracy: 30.25 

Round   6, Global train loss: 1.904, Global test loss: 1.739, Global test accuracy: 41.75 

Round   7, Train loss: 1.892, Test loss: 1.962, Test accuracy: 31.43 

Round   7, Global train loss: 1.892, Global test loss: 1.731, Global test accuracy: 40.81 

Round   8, Train loss: 1.794, Test loss: 1.908, Test accuracy: 33.25 

Round   8, Global train loss: 1.794, Global test loss: 1.609, Global test accuracy: 44.02 

Round   9, Train loss: 1.728, Test loss: 1.877, Test accuracy: 34.36 

Round   9, Global train loss: 1.728, Global test loss: 1.588, Global test accuracy: 43.08 

Round  10, Train loss: 1.835, Test loss: 1.859, Test accuracy: 35.18 

Round  10, Global train loss: 1.835, Global test loss: 1.606, Global test accuracy: 45.77 

Round  11, Train loss: 1.713, Test loss: 1.837, Test accuracy: 36.43 

Round  11, Global train loss: 1.713, Global test loss: 1.533, Global test accuracy: 48.01 

Round  12, Train loss: 1.737, Test loss: 1.833, Test accuracy: 36.63 

Round  12, Global train loss: 1.737, Global test loss: 1.571, Global test accuracy: 47.22 

Round  13, Train loss: 1.655, Test loss: 1.814, Test accuracy: 37.26 

Round  13, Global train loss: 1.655, Global test loss: 1.514, Global test accuracy: 46.74 

Round  14, Train loss: 1.703, Test loss: 1.791, Test accuracy: 38.07 

Round  14, Global train loss: 1.703, Global test loss: 1.506, Global test accuracy: 49.63 

Round  15, Train loss: 1.756, Test loss: 1.782, Test accuracy: 38.69 

Round  15, Global train loss: 1.756, Global test loss: 1.573, Global test accuracy: 49.49 

Round  16, Train loss: 1.613, Test loss: 1.770, Test accuracy: 39.10 

Round  16, Global train loss: 1.613, Global test loss: 1.436, Global test accuracy: 51.41 

Round  17, Train loss: 1.633, Test loss: 1.758, Test accuracy: 39.81 

Round  17, Global train loss: 1.633, Global test loss: 1.475, Global test accuracy: 50.67 

Round  18, Train loss: 1.629, Test loss: 1.762, Test accuracy: 39.71 

Round  18, Global train loss: 1.629, Global test loss: 1.468, Global test accuracy: 50.35 

Round  19, Train loss: 1.656, Test loss: 1.754, Test accuracy: 40.21 

Round  19, Global train loss: 1.656, Global test loss: 1.494, Global test accuracy: 49.89 

Round  20, Train loss: 1.701, Test loss: 1.759, Test accuracy: 40.32 

Round  20, Global train loss: 1.701, Global test loss: 1.502, Global test accuracy: 52.45 

Round  21, Train loss: 1.580, Test loss: 1.759, Test accuracy: 40.70 

Round  21, Global train loss: 1.580, Global test loss: 1.408, Global test accuracy: 53.96 

Round  22, Train loss: 1.580, Test loss: 1.758, Test accuracy: 40.77 

Round  22, Global train loss: 1.580, Global test loss: 1.437, Global test accuracy: 52.50 

Round  23, Train loss: 1.546, Test loss: 1.768, Test accuracy: 40.95 

Round  23, Global train loss: 1.546, Global test loss: 1.377, Global test accuracy: 54.25 

Round  24, Train loss: 1.509, Test loss: 1.753, Test accuracy: 41.45 

Round  24, Global train loss: 1.509, Global test loss: 1.411, Global test accuracy: 53.12 

Round  25, Train loss: 1.549, Test loss: 1.758, Test accuracy: 41.85 

Round  25, Global train loss: 1.549, Global test loss: 1.414, Global test accuracy: 53.35 

Round  26, Train loss: 1.494, Test loss: 1.746, Test accuracy: 42.44 

Round  26, Global train loss: 1.494, Global test loss: 1.411, Global test accuracy: 54.17 

Round  27, Train loss: 1.583, Test loss: 1.741, Test accuracy: 42.55 

Round  27, Global train loss: 1.583, Global test loss: 1.436, Global test accuracy: 55.20 

Round  28, Train loss: 1.519, Test loss: 1.750, Test accuracy: 42.53 

Round  28, Global train loss: 1.519, Global test loss: 1.369, Global test accuracy: 55.23 

Round  29, Train loss: 1.434, Test loss: 1.753, Test accuracy: 42.27 

Round  29, Global train loss: 1.434, Global test loss: 1.403, Global test accuracy: 54.11 

Round  30, Train loss: 1.425, Test loss: 1.779, Test accuracy: 42.23 

Round  30, Global train loss: 1.425, Global test loss: 1.416, Global test accuracy: 53.02 

Round  31, Train loss: 1.407, Test loss: 1.796, Test accuracy: 41.75 

Round  31, Global train loss: 1.407, Global test loss: 1.359, Global test accuracy: 56.18 

Round  32, Train loss: 1.416, Test loss: 1.800, Test accuracy: 41.80 

Round  32, Global train loss: 1.416, Global test loss: 1.409, Global test accuracy: 54.19 

Round  33, Train loss: 1.373, Test loss: 1.827, Test accuracy: 41.49 

Round  33, Global train loss: 1.373, Global test loss: 1.365, Global test accuracy: 54.98 

Round  34, Train loss: 1.457, Test loss: 1.811, Test accuracy: 42.23 

Round  34, Global train loss: 1.457, Global test loss: 1.406, Global test accuracy: 53.23 

Round  35, Train loss: 1.447, Test loss: 1.811, Test accuracy: 42.66 

Round  35, Global train loss: 1.447, Global test loss: 1.372, Global test accuracy: 56.41 

Round  36, Train loss: 1.356, Test loss: 1.825, Test accuracy: 42.50 

Round  36, Global train loss: 1.356, Global test loss: 1.389, Global test accuracy: 55.11 

Round  37, Train loss: 1.314, Test loss: 1.835, Test accuracy: 42.72 

Round  37, Global train loss: 1.314, Global test loss: 1.377, Global test accuracy: 54.79 

Round  38, Train loss: 1.370, Test loss: 1.841, Test accuracy: 42.82 

Round  38, Global train loss: 1.370, Global test loss: 1.421, Global test accuracy: 54.07 

Round  39, Train loss: 1.248, Test loss: 1.833, Test accuracy: 42.95 

Round  39, Global train loss: 1.248, Global test loss: 1.364, Global test accuracy: 55.76 

Round  40, Train loss: 1.319, Test loss: 1.860, Test accuracy: 42.81 

Round  40, Global train loss: 1.319, Global test loss: 1.297, Global test accuracy: 57.03 

Round  41, Train loss: 1.247, Test loss: 1.871, Test accuracy: 42.95 

Round  41, Global train loss: 1.247, Global test loss: 1.382, Global test accuracy: 54.94 

Round  42, Train loss: 1.212, Test loss: 1.896, Test accuracy: 42.96 

Round  42, Global train loss: 1.212, Global test loss: 1.296, Global test accuracy: 56.77 

Round  43, Train loss: 1.330, Test loss: 1.887, Test accuracy: 43.11 

Round  43, Global train loss: 1.330, Global test loss: 1.353, Global test accuracy: 55.95 

Round  44, Train loss: 1.156, Test loss: 1.887, Test accuracy: 43.14 

Round  44, Global train loss: 1.156, Global test loss: 1.309, Global test accuracy: 56.27 

Round  45, Train loss: 1.279, Test loss: 1.893, Test accuracy: 43.10 

Round  45, Global train loss: 1.279, Global test loss: 1.392, Global test accuracy: 54.59 

Round  46, Train loss: 1.282, Test loss: 1.905, Test accuracy: 43.22 

Round  46, Global train loss: 1.282, Global test loss: 1.432, Global test accuracy: 53.37 

Round  47, Train loss: 1.082, Test loss: 1.942, Test accuracy: 43.21 

Round  47, Global train loss: 1.082, Global test loss: 1.299, Global test accuracy: 57.65 

Round  48, Train loss: 1.306, Test loss: 1.947, Test accuracy: 43.28 

Round  48, Global train loss: 1.306, Global test loss: 1.345, Global test accuracy: 56.35 

Round  49, Train loss: 1.177, Test loss: 1.929, Test accuracy: 43.69 

Round  49, Global train loss: 1.177, Global test loss: 1.367, Global test accuracy: 55.38 

Round  50, Train loss: 1.168, Test loss: 1.943, Test accuracy: 43.45 

Round  50, Global train loss: 1.168, Global test loss: 1.395, Global test accuracy: 53.94 

Round  51, Train loss: 1.164, Test loss: 1.952, Test accuracy: 43.42 

Round  51, Global train loss: 1.164, Global test loss: 1.352, Global test accuracy: 55.80 

Round  52, Train loss: 1.116, Test loss: 1.965, Test accuracy: 43.47 

Round  52, Global train loss: 1.116, Global test loss: 1.354, Global test accuracy: 56.31 

Round  53, Train loss: 1.161, Test loss: 1.975, Test accuracy: 43.51 

Round  53, Global train loss: 1.161, Global test loss: 1.433, Global test accuracy: 53.27 

Round  54, Train loss: 1.140, Test loss: 1.980, Test accuracy: 43.80 

Round  54, Global train loss: 1.140, Global test loss: 1.324, Global test accuracy: 56.93 

Round  55, Train loss: 1.032, Test loss: 2.019, Test accuracy: 43.17 

Round  55, Global train loss: 1.032, Global test loss: 1.343, Global test accuracy: 56.80 

Round  56, Train loss: 1.162, Test loss: 2.028, Test accuracy: 43.28 

Round  56, Global train loss: 1.162, Global test loss: 1.408, Global test accuracy: 54.84 

Round  57, Train loss: 1.101, Test loss: 2.030, Test accuracy: 43.27 

Round  57, Global train loss: 1.101, Global test loss: 1.379, Global test accuracy: 55.84 

Round  58, Train loss: 0.982, Test loss: 2.051, Test accuracy: 43.01 

Round  58, Global train loss: 0.982, Global test loss: 1.379, Global test accuracy: 56.36 

Round  59, Train loss: 1.012, Test loss: 2.038, Test accuracy: 43.30 

Round  59, Global train loss: 1.012, Global test loss: 1.453, Global test accuracy: 53.68 

Round  60, Train loss: 1.096, Test loss: 2.071, Test accuracy: 42.97 

Round  60, Global train loss: 1.096, Global test loss: 1.456, Global test accuracy: 54.03 

Round  61, Train loss: 1.038, Test loss: 2.081, Test accuracy: 43.21 

Round  61, Global train loss: 1.038, Global test loss: 1.434, Global test accuracy: 54.63 

Round  62, Train loss: 1.026, Test loss: 2.097, Test accuracy: 42.98 

Round  62, Global train loss: 1.026, Global test loss: 1.378, Global test accuracy: 56.04 

Round  63, Train loss: 0.987, Test loss: 2.110, Test accuracy: 42.91 

Round  63, Global train loss: 0.987, Global test loss: 1.411, Global test accuracy: 55.89 

Round  64, Train loss: 0.956, Test loss: 2.113, Test accuracy: 43.19 

Round  64, Global train loss: 0.956, Global test loss: 1.320, Global test accuracy: 57.86 

Round  65, Train loss: 0.934, Test loss: 2.123, Test accuracy: 42.97 

Round  65, Global train loss: 0.934, Global test loss: 1.408, Global test accuracy: 55.81 

Round  66, Train loss: 0.992, Test loss: 2.123, Test accuracy: 42.96 

Round  66, Global train loss: 0.992, Global test loss: 1.473, Global test accuracy: 54.24 

Round  67, Train loss: 0.950, Test loss: 2.149, Test accuracy: 42.82 

Round  67, Global train loss: 0.950, Global test loss: 1.458, Global test accuracy: 55.79 

Round  68, Train loss: 1.061, Test loss: 2.148, Test accuracy: 43.24 

Round  68, Global train loss: 1.061, Global test loss: 1.445, Global test accuracy: 54.25 

Round  69, Train loss: 0.975, Test loss: 2.121, Test accuracy: 43.83 

Round  69, Global train loss: 0.975, Global test loss: 1.523, Global test accuracy: 52.75 

Round  70, Train loss: 0.887, Test loss: 2.145, Test accuracy: 43.51 

Round  70, Global train loss: 0.887, Global test loss: 1.414, Global test accuracy: 56.45 

Round  71, Train loss: 1.020, Test loss: 2.154, Test accuracy: 43.09 

Round  71, Global train loss: 1.020, Global test loss: 1.467, Global test accuracy: 53.51 

Round  72, Train loss: 0.895, Test loss: 2.144, Test accuracy: 43.27 

Round  72, Global train loss: 0.895, Global test loss: 1.400, Global test accuracy: 56.63 

Round  73, Train loss: 0.869, Test loss: 2.151, Test accuracy: 43.35 

Round  73, Global train loss: 0.869, Global test loss: 1.465, Global test accuracy: 54.87 

Round  74, Train loss: 0.980, Test loss: 2.168, Test accuracy: 43.52 

Round  74, Global train loss: 0.980, Global test loss: 1.455, Global test accuracy: 55.17 

Round  75, Train loss: 1.002, Test loss: 2.170, Test accuracy: 43.78 

Round  75, Global train loss: 1.002, Global test loss: 1.386, Global test accuracy: 56.31 

Round  76, Train loss: 0.966, Test loss: 2.169, Test accuracy: 44.13 

Round  76, Global train loss: 0.966, Global test loss: 1.419, Global test accuracy: 56.57 

Round  77, Train loss: 1.048, Test loss: 2.188, Test accuracy: 43.67 

Round  77, Global train loss: 1.048, Global test loss: 1.482, Global test accuracy: 53.66 

Round  78, Train loss: 0.956, Test loss: 2.206, Test accuracy: 43.62 

Round  78, Global train loss: 0.956, Global test loss: 1.407, Global test accuracy: 56.47 

Round  79, Train loss: 0.900, Test loss: 2.238, Test accuracy: 43.68 

Round  79, Global train loss: 0.900, Global test loss: 1.448, Global test accuracy: 55.84 

Round  80, Train loss: 0.999, Test loss: 2.233, Test accuracy: 43.69 

Round  80, Global train loss: 0.999, Global test loss: 1.488, Global test accuracy: 53.88 

Round  81, Train loss: 0.900, Test loss: 2.229, Test accuracy: 43.89 

Round  81, Global train loss: 0.900, Global test loss: 1.510, Global test accuracy: 54.06 

Round  82, Train loss: 0.802, Test loss: 2.252, Test accuracy: 43.98 

Round  82, Global train loss: 0.802, Global test loss: 1.455, Global test accuracy: 56.14 

Round  83, Train loss: 0.856, Test loss: 2.254, Test accuracy: 43.84 

Round  83, Global train loss: 0.856, Global test loss: 1.443, Global test accuracy: 56.36 

Round  84, Train loss: 0.882, Test loss: 2.269, Test accuracy: 43.90 

Round  84, Global train loss: 0.882, Global test loss: 1.444, Global test accuracy: 57.25 

Round  85, Train loss: 0.942, Test loss: 2.285, Test accuracy: 43.55 

Round  85, Global train loss: 0.942, Global test loss: 1.539, Global test accuracy: 52.99 

Round  86, Train loss: 0.913, Test loss: 2.254, Test accuracy: 44.10 

Round  86, Global train loss: 0.913, Global test loss: 1.423, Global test accuracy: 56.16 

Round  87, Train loss: 0.831, Test loss: 2.243, Test accuracy: 44.23 

Round  87, Global train loss: 0.831, Global test loss: 1.481, Global test accuracy: 55.37 

Round  88, Train loss: 0.876, Test loss: 2.274, Test accuracy: 43.84 

Round  88, Global train loss: 0.876, Global test loss: 1.436, Global test accuracy: 56.74 

Round  89, Train loss: 0.787, Test loss: 2.252, Test accuracy: 43.97 

Round  89, Global train loss: 0.787, Global test loss: 1.490, Global test accuracy: 56.26 

Round  90, Train loss: 0.944, Test loss: 2.297, Test accuracy: 43.97 

Round  90, Global train loss: 0.944, Global test loss: 1.514, Global test accuracy: 53.33 

Round  91, Train loss: 0.809, Test loss: 2.284, Test accuracy: 44.03 

Round  91, Global train loss: 0.809, Global test loss: 1.464, Global test accuracy: 56.78 

Round  92, Train loss: 0.869, Test loss: 2.309, Test accuracy: 43.58 

Round  92, Global train loss: 0.869, Global test loss: 1.598, Global test accuracy: 51.43 

Round  93, Train loss: 0.969, Test loss: 2.353, Test accuracy: 43.52 

Round  93, Global train loss: 0.969, Global test loss: 1.621, Global test accuracy: 51.38 

Round  94, Train loss: 0.846, Test loss: 2.366, Test accuracy: 43.23 

Round  94, Global train loss: 0.846, Global test loss: 1.465, Global test accuracy: 55.97 

Round  95, Train loss: 0.841, Test loss: 2.343, Test accuracy: 43.27 

Round  95, Global train loss: 0.841, Global test loss: 1.500, Global test accuracy: 55.76 

Round  96, Train loss: 0.844, Test loss: 2.344, Test accuracy: 43.83 

Round  96, Global train loss: 0.844, Global test loss: 1.546, Global test accuracy: 55.17 

Round  97, Train loss: 0.817, Test loss: 2.360, Test accuracy: 43.70 

Round  97, Global train loss: 0.817, Global test loss: 1.560, Global test accuracy: 54.07 

Round  98, Train loss: 0.900, Test loss: 2.376, Test accuracy: 43.64 

Round  98, Global train loss: 0.900, Global test loss: 1.541, Global test accuracy: 53.44 

Round  99, Train loss: 0.746, Test loss: 2.403, Test accuracy: 43.24 

Round  99, Global train loss: 0.746, Global test loss: 1.595, Global test accuracy: 54.12 

Final Round, Train loss: 0.631, Test loss: 2.861, Test accuracy: 42.52 

Final Round, Global train loss: 0.631, Global test loss: 1.595, Global test accuracy: 54.12 

Average accuracy final 10 rounds: 43.602 

Average global accuracy final 10 rounds: 54.1435 

2642.4882214069366
[1.411585807800293, 2.5443549156188965, 3.69756817817688, 4.8092687129974365, 5.989975690841675, 7.1603100299835205, 8.33556342124939, 9.512266635894775, 10.68325161933899, 11.851062536239624, 13.03273868560791, 14.139572858810425, 15.254784345626831, 16.323952198028564, 17.497588634490967, 18.624200582504272, 19.791483163833618, 20.957277536392212, 22.140297651290894, 23.323973178863525, 24.503819704055786, 25.51102066040039, 26.507662296295166, 27.514694690704346, 28.51487421989441, 29.525180339813232, 30.531617164611816, 31.541172742843628, 32.54834246635437, 33.54975938796997, 34.55009174346924, 35.559523820877075, 36.55933690071106, 37.56708073616028, 38.56545948982239, 39.571720361709595, 40.57297325134277, 41.57941150665283, 42.592352628707886, 43.603848695755005, 44.609583139419556, 45.61420178413391, 46.6153450012207, 47.61914420127869, 48.618297815322876, 49.627378702163696, 50.6242458820343, 51.63002610206604, 52.62728834152222, 53.63041543960571, 54.629722595214844, 55.62848734855652, 56.64857840538025, 57.64836001396179, 58.66403532028198, 59.66259527206421, 60.670878410339355, 61.67093110084534, 62.67974352836609, 63.69213628768921, 64.69763565063477, 65.70145869255066, 66.70918440818787, 67.71211099624634, 68.72260403633118, 69.7302360534668, 70.74734425544739, 71.75396084785461, 72.7641441822052, 73.78322172164917, 74.79868483543396, 75.80889630317688, 76.81286931037903, 77.81396818161011, 78.81568908691406, 79.82959198951721, 80.83175230026245, 81.85055708885193, 82.86267352104187, 83.87539625167847, 84.89032888412476, 85.91054654121399, 86.91592812538147, 87.91769886016846, 88.91868615150452, 89.92512011528015, 90.93121933937073, 91.93298983573914, 92.9340124130249, 93.93609189987183, 94.94208669662476, 95.95529794692993, 96.96944952011108, 97.98482584953308, 98.9899377822876, 99.9949746131897, 100.99585342407227, 101.99655771255493, 103.00309181213379, 104.00853514671326, 106.01709866523743]
[20.605, 24.14, 26.5625, 27.48, 28.3075, 29.845, 30.25, 31.4325, 33.2475, 34.3575, 35.18, 36.4325, 36.6275, 37.2575, 38.07, 38.6925, 39.1, 39.8125, 39.7075, 40.21, 40.3175, 40.7025, 40.775, 40.9525, 41.4525, 41.8475, 42.4425, 42.545, 42.53, 42.27, 42.225, 41.75, 41.805, 41.4875, 42.23, 42.665, 42.4975, 42.7175, 42.8175, 42.9525, 42.8075, 42.9475, 42.9625, 43.1075, 43.1375, 43.1025, 43.2175, 43.21, 43.285, 43.6875, 43.4475, 43.425, 43.4725, 43.505, 43.8, 43.17, 43.2825, 43.275, 43.005, 43.305, 42.965, 43.2075, 42.9825, 42.9125, 43.1875, 42.965, 42.9625, 42.82, 43.2375, 43.825, 43.5125, 43.085, 43.265, 43.355, 43.515, 43.78, 44.1275, 43.67, 43.615, 43.68, 43.685, 43.89, 43.98, 43.84, 43.9, 43.555, 44.105, 44.2275, 43.8375, 43.965, 43.97, 44.035, 43.5775, 43.5225, 43.2325, 43.27, 43.8275, 43.7, 43.6425, 43.2425, 42.515]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8850
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5940
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8600
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5320
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7485
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8655
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5090
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7770
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7260
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8940
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.210, Test loss: 2.230, Test accuracy: 21.85 

Round   0, Global train loss: 2.210, Global test loss: 2.220, Global test accuracy: 22.12 

Round   1, Train loss: 2.106, Test loss: 2.088, Test accuracy: 24.34 

Round   1, Global train loss: 2.106, Global test loss: 2.020, Global test accuracy: 25.26 

Round   2, Train loss: 1.996, Test loss: 2.040, Test accuracy: 26.67 

Round   2, Global train loss: 1.996, Global test loss: 1.903, Global test accuracy: 31.62 

Round   3, Train loss: 1.996, Test loss: 2.037, Test accuracy: 26.84 

Round   3, Global train loss: 1.996, Global test loss: 1.894, Global test accuracy: 30.71 

Round   4, Train loss: 1.909, Test loss: 1.998, Test accuracy: 28.32 

Round   4, Global train loss: 1.909, Global test loss: 1.782, Global test accuracy: 36.84 

Round   5, Train loss: 1.946, Test loss: 1.975, Test accuracy: 29.34 

Round   5, Global train loss: 1.946, Global test loss: 1.764, Global test accuracy: 37.78 

Round   6, Train loss: 1.877, Test loss: 1.946, Test accuracy: 30.39 

Round   6, Global train loss: 1.877, Global test loss: 1.716, Global test accuracy: 38.86 

Round   7, Train loss: 1.817, Test loss: 1.916, Test accuracy: 31.78 

Round   7, Global train loss: 1.817, Global test loss: 1.642, Global test accuracy: 42.25 

Round   8, Train loss: 1.831, Test loss: 1.881, Test accuracy: 33.57 

Round   8, Global train loss: 1.831, Global test loss: 1.642, Global test accuracy: 42.66 

Round   9, Train loss: 1.784, Test loss: 1.853, Test accuracy: 34.21 

Round   9, Global train loss: 1.784, Global test loss: 1.618, Global test accuracy: 43.43 

Round  10, Train loss: 1.768, Test loss: 1.825, Test accuracy: 35.45 

Round  10, Global train loss: 1.768, Global test loss: 1.579, Global test accuracy: 44.80 

Round  11, Train loss: 1.766, Test loss: 1.804, Test accuracy: 35.88 

Round  11, Global train loss: 1.766, Global test loss: 1.582, Global test accuracy: 46.64 

Round  12, Train loss: 1.724, Test loss: 1.798, Test accuracy: 35.86 

Round  12, Global train loss: 1.724, Global test loss: 1.540, Global test accuracy: 46.80 

Round  13, Train loss: 1.706, Test loss: 1.784, Test accuracy: 36.58 

Round  13, Global train loss: 1.706, Global test loss: 1.541, Global test accuracy: 45.47 

Round  14, Train loss: 1.827, Test loss: 1.775, Test accuracy: 36.96 

Round  14, Global train loss: 1.827, Global test loss: 1.615, Global test accuracy: 42.73 

Round  15, Train loss: 1.638, Test loss: 1.758, Test accuracy: 37.84 

Round  15, Global train loss: 1.638, Global test loss: 1.469, Global test accuracy: 48.87 

Round  16, Train loss: 1.678, Test loss: 1.748, Test accuracy: 38.53 

Round  16, Global train loss: 1.678, Global test loss: 1.483, Global test accuracy: 48.77 

Round  17, Train loss: 1.648, Test loss: 1.726, Test accuracy: 39.21 

Round  17, Global train loss: 1.648, Global test loss: 1.477, Global test accuracy: 49.17 

Round  18, Train loss: 1.784, Test loss: 1.711, Test accuracy: 39.94 

Round  18, Global train loss: 1.784, Global test loss: 1.555, Global test accuracy: 48.37 

Round  19, Train loss: 1.622, Test loss: 1.710, Test accuracy: 40.38 

Round  19, Global train loss: 1.622, Global test loss: 1.446, Global test accuracy: 50.60 

Round  20, Train loss: 1.599, Test loss: 1.700, Test accuracy: 40.72 

Round  20, Global train loss: 1.599, Global test loss: 1.427, Global test accuracy: 50.02 

Round  21, Train loss: 1.654, Test loss: 1.699, Test accuracy: 40.94 

Round  21, Global train loss: 1.654, Global test loss: 1.413, Global test accuracy: 53.77 

Round  22, Train loss: 1.651, Test loss: 1.698, Test accuracy: 41.10 

Round  22, Global train loss: 1.651, Global test loss: 1.430, Global test accuracy: 52.14 

Round  23, Train loss: 1.585, Test loss: 1.680, Test accuracy: 41.94 

Round  23, Global train loss: 1.585, Global test loss: 1.432, Global test accuracy: 51.08 

Round  24, Train loss: 1.624, Test loss: 1.678, Test accuracy: 41.79 

Round  24, Global train loss: 1.624, Global test loss: 1.426, Global test accuracy: 52.79 

Round  25, Train loss: 1.519, Test loss: 1.693, Test accuracy: 41.69 

Round  25, Global train loss: 1.519, Global test loss: 1.413, Global test accuracy: 52.66 

Round  26, Train loss: 1.546, Test loss: 1.681, Test accuracy: 42.06 

Round  26, Global train loss: 1.546, Global test loss: 1.387, Global test accuracy: 54.44 

Round  27, Train loss: 1.506, Test loss: 1.672, Test accuracy: 42.34 

Round  27, Global train loss: 1.506, Global test loss: 1.377, Global test accuracy: 54.17 

Round  28, Train loss: 1.498, Test loss: 1.668, Test accuracy: 42.51 

Round  28, Global train loss: 1.498, Global test loss: 1.390, Global test accuracy: 53.78 

Round  29, Train loss: 1.416, Test loss: 1.666, Test accuracy: 42.71 

Round  29, Global train loss: 1.416, Global test loss: 1.319, Global test accuracy: 55.22 

Round  30, Train loss: 1.495, Test loss: 1.659, Test accuracy: 42.90 

Round  30, Global train loss: 1.495, Global test loss: 1.340, Global test accuracy: 55.15 

Round  31, Train loss: 1.574, Test loss: 1.660, Test accuracy: 43.10 

Round  31, Global train loss: 1.574, Global test loss: 1.398, Global test accuracy: 53.92 

Round  32, Train loss: 1.428, Test loss: 1.685, Test accuracy: 42.54 

Round  32, Global train loss: 1.428, Global test loss: 1.350, Global test accuracy: 54.56 

Round  33, Train loss: 1.507, Test loss: 1.681, Test accuracy: 43.19 

Round  33, Global train loss: 1.507, Global test loss: 1.398, Global test accuracy: 53.78 

Round  34, Train loss: 1.457, Test loss: 1.674, Test accuracy: 43.41 

Round  34, Global train loss: 1.457, Global test loss: 1.367, Global test accuracy: 54.63 

Round  35, Train loss: 1.511, Test loss: 1.677, Test accuracy: 43.51 

Round  35, Global train loss: 1.511, Global test loss: 1.393, Global test accuracy: 54.16 

Round  36, Train loss: 1.427, Test loss: 1.674, Test accuracy: 43.76 

Round  36, Global train loss: 1.427, Global test loss: 1.369, Global test accuracy: 54.05 

Round  37, Train loss: 1.294, Test loss: 1.665, Test accuracy: 44.08 

Round  37, Global train loss: 1.294, Global test loss: 1.258, Global test accuracy: 56.94 

Round  38, Train loss: 1.261, Test loss: 1.676, Test accuracy: 43.72 

Round  38, Global train loss: 1.261, Global test loss: 1.289, Global test accuracy: 56.27 

Round  39, Train loss: 1.314, Test loss: 1.682, Test accuracy: 43.76 

Round  39, Global train loss: 1.314, Global test loss: 1.280, Global test accuracy: 56.91 

Round  40, Train loss: 1.376, Test loss: 1.678, Test accuracy: 44.38 

Round  40, Global train loss: 1.376, Global test loss: 1.268, Global test accuracy: 57.72 

Round  41, Train loss: 1.229, Test loss: 1.666, Test accuracy: 44.77 

Round  41, Global train loss: 1.229, Global test loss: 1.262, Global test accuracy: 57.39 

Round  42, Train loss: 1.332, Test loss: 1.682, Test accuracy: 44.45 

Round  42, Global train loss: 1.332, Global test loss: 1.337, Global test accuracy: 54.71 

Round  43, Train loss: 1.370, Test loss: 1.687, Test accuracy: 44.35 

Round  43, Global train loss: 1.370, Global test loss: 1.316, Global test accuracy: 55.49 

Round  44, Train loss: 1.297, Test loss: 1.688, Test accuracy: 44.64 

Round  44, Global train loss: 1.297, Global test loss: 1.271, Global test accuracy: 56.64 

Round  45, Train loss: 1.257, Test loss: 1.687, Test accuracy: 45.26 

Round  45, Global train loss: 1.257, Global test loss: 1.267, Global test accuracy: 57.62 

Round  46, Train loss: 1.245, Test loss: 1.704, Test accuracy: 45.08 

Round  46, Global train loss: 1.245, Global test loss: 1.279, Global test accuracy: 57.23 

Round  47, Train loss: 1.219, Test loss: 1.717, Test accuracy: 44.72 

Round  47, Global train loss: 1.219, Global test loss: 1.269, Global test accuracy: 57.48 

Round  48, Train loss: 1.341, Test loss: 1.735, Test accuracy: 44.59 

Round  48, Global train loss: 1.341, Global test loss: 1.311, Global test accuracy: 56.04 

Round  49, Train loss: 1.349, Test loss: 1.740, Test accuracy: 44.55 

Round  49, Global train loss: 1.349, Global test loss: 1.358, Global test accuracy: 54.97 

Round  50, Train loss: 1.296, Test loss: 1.720, Test accuracy: 44.97 

Round  50, Global train loss: 1.296, Global test loss: 1.316, Global test accuracy: 55.63 

Round  51, Train loss: 1.187, Test loss: 1.749, Test accuracy: 44.56 

Round  51, Global train loss: 1.187, Global test loss: 1.268, Global test accuracy: 57.43 

Round  52, Train loss: 1.363, Test loss: 1.739, Test accuracy: 45.17 

Round  52, Global train loss: 1.363, Global test loss: 1.397, Global test accuracy: 54.10 

Round  53, Train loss: 1.242, Test loss: 1.769, Test accuracy: 44.72 

Round  53, Global train loss: 1.242, Global test loss: 1.311, Global test accuracy: 56.82 

Round  54, Train loss: 1.291, Test loss: 1.770, Test accuracy: 44.71 

Round  54, Global train loss: 1.291, Global test loss: 1.304, Global test accuracy: 57.07 

Round  55, Train loss: 1.349, Test loss: 1.761, Test accuracy: 44.93 

Round  55, Global train loss: 1.349, Global test loss: 1.379, Global test accuracy: 54.83 

Round  56, Train loss: 1.328, Test loss: 1.785, Test accuracy: 44.41 

Round  56, Global train loss: 1.328, Global test loss: 1.411, Global test accuracy: 53.14 

Round  57, Train loss: 1.107, Test loss: 1.808, Test accuracy: 44.15 

Round  57, Global train loss: 1.107, Global test loss: 1.316, Global test accuracy: 56.11 

Round  58, Train loss: 1.171, Test loss: 1.811, Test accuracy: 44.22 

Round  58, Global train loss: 1.171, Global test loss: 1.350, Global test accuracy: 54.34 

Round  59, Train loss: 1.114, Test loss: 1.813, Test accuracy: 44.51 

Round  59, Global train loss: 1.114, Global test loss: 1.292, Global test accuracy: 57.44 

Round  60, Train loss: 1.223, Test loss: 1.828, Test accuracy: 44.51 

Round  60, Global train loss: 1.223, Global test loss: 1.315, Global test accuracy: 56.33 

Round  61, Train loss: 1.064, Test loss: 1.806, Test accuracy: 44.88 

Round  61, Global train loss: 1.064, Global test loss: 1.259, Global test accuracy: 57.94 

Round  62, Train loss: 1.219, Test loss: 1.822, Test accuracy: 44.78 

Round  62, Global train loss: 1.219, Global test loss: 1.366, Global test accuracy: 55.31 

Round  63, Train loss: 1.214, Test loss: 1.857, Test accuracy: 44.30 

Round  63, Global train loss: 1.214, Global test loss: 1.396, Global test accuracy: 53.98 

Round  64, Train loss: 1.069, Test loss: 1.859, Test accuracy: 44.55 

Round  64, Global train loss: 1.069, Global test loss: 1.277, Global test accuracy: 56.79 

Round  65, Train loss: 1.026, Test loss: 1.863, Test accuracy: 44.63 

Round  65, Global train loss: 1.026, Global test loss: 1.322, Global test accuracy: 56.75 

Round  66, Train loss: 1.148, Test loss: 1.855, Test accuracy: 44.73 

Round  66, Global train loss: 1.148, Global test loss: 1.338, Global test accuracy: 54.99 

Round  67, Train loss: 1.231, Test loss: 1.862, Test accuracy: 44.52 

Round  67, Global train loss: 1.231, Global test loss: 1.465, Global test accuracy: 51.40 

Round  68, Train loss: 1.111, Test loss: 1.890, Test accuracy: 44.42 

Round  68, Global train loss: 1.111, Global test loss: 1.356, Global test accuracy: 54.95 

Round  69, Train loss: 1.024, Test loss: 1.911, Test accuracy: 44.23 

Round  69, Global train loss: 1.024, Global test loss: 1.408, Global test accuracy: 54.07 

Round  70, Train loss: 1.127, Test loss: 1.896, Test accuracy: 44.17 

Round  70, Global train loss: 1.127, Global test loss: 1.412, Global test accuracy: 53.24 

Round  71, Train loss: 0.991, Test loss: 1.934, Test accuracy: 43.97 

Round  71, Global train loss: 0.991, Global test loss: 1.328, Global test accuracy: 57.12 

Round  72, Train loss: 1.054, Test loss: 1.920, Test accuracy: 44.27 

Round  72, Global train loss: 1.054, Global test loss: 1.309, Global test accuracy: 57.19 

Round  73, Train loss: 0.916, Test loss: 1.922, Test accuracy: 44.66 

Round  73, Global train loss: 0.916, Global test loss: 1.335, Global test accuracy: 57.23 

Round  74, Train loss: 0.965, Test loss: 1.926, Test accuracy: 44.72 

Round  74, Global train loss: 0.965, Global test loss: 1.319, Global test accuracy: 57.08 

Round  75, Train loss: 1.171, Test loss: 1.936, Test accuracy: 44.31 

Round  75, Global train loss: 1.171, Global test loss: 1.403, Global test accuracy: 53.12 

Round  76, Train loss: 1.074, Test loss: 1.933, Test accuracy: 44.25 

Round  76, Global train loss: 1.074, Global test loss: 1.353, Global test accuracy: 55.47 

Round  77, Train loss: 1.047, Test loss: 1.957, Test accuracy: 43.92 

Round  77, Global train loss: 1.047, Global test loss: 1.321, Global test accuracy: 57.22 

Round  78, Train loss: 1.102, Test loss: 1.979, Test accuracy: 43.74 

Round  78, Global train loss: 1.102, Global test loss: 1.368, Global test accuracy: 55.35 

Round  79, Train loss: 0.992, Test loss: 1.983, Test accuracy: 44.13 

Round  79, Global train loss: 0.992, Global test loss: 1.321, Global test accuracy: 57.59 

Round  80, Train loss: 1.065, Test loss: 1.996, Test accuracy: 44.72 

Round  80, Global train loss: 1.065, Global test loss: 1.308, Global test accuracy: 56.41 

Round  81, Train loss: 0.910, Test loss: 2.004, Test accuracy: 44.15 

Round  81, Global train loss: 0.910, Global test loss: 1.347, Global test accuracy: 57.13 

Round  82, Train loss: 0.947, Test loss: 1.971, Test accuracy: 44.49 

Round  82, Global train loss: 0.947, Global test loss: 1.379, Global test accuracy: 55.81 

Round  83, Train loss: 0.868, Test loss: 2.009, Test accuracy: 44.19 

Round  83, Global train loss: 0.868, Global test loss: 1.363, Global test accuracy: 56.95 

Round  84, Train loss: 1.028, Test loss: 2.003, Test accuracy: 44.37 

Round  84, Global train loss: 1.028, Global test loss: 1.379, Global test accuracy: 56.44 

Round  85, Train loss: 0.991, Test loss: 1.978, Test accuracy: 44.74 

Round  85, Global train loss: 0.991, Global test loss: 1.364, Global test accuracy: 56.45 

Round  86, Train loss: 0.904, Test loss: 1.978, Test accuracy: 45.07 

Round  86, Global train loss: 0.904, Global test loss: 1.277, Global test accuracy: 58.60 

Round  87, Train loss: 0.942, Test loss: 1.986, Test accuracy: 45.04 

Round  87, Global train loss: 0.942, Global test loss: 1.326, Global test accuracy: 57.03 

Round  88, Train loss: 0.972, Test loss: 2.001, Test accuracy: 45.02 

Round  88, Global train loss: 0.972, Global test loss: 1.387, Global test accuracy: 55.15 

Round  89, Train loss: 0.829, Test loss: 1.990, Test accuracy: 45.16 
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  89, Global train loss: 0.829, Global test loss: 1.382, Global test accuracy: 56.83 

Round  90, Train loss: 0.871, Test loss: 2.008, Test accuracy: 45.16 

Round  90, Global train loss: 0.871, Global test loss: 1.362, Global test accuracy: 57.15 

Round  91, Train loss: 1.011, Test loss: 2.025, Test accuracy: 45.30 

Round  91, Global train loss: 1.011, Global test loss: 1.482, Global test accuracy: 52.82 

Round  92, Train loss: 0.863, Test loss: 2.024, Test accuracy: 45.26 

Round  92, Global train loss: 0.863, Global test loss: 1.502, Global test accuracy: 53.49 

Round  93, Train loss: 0.915, Test loss: 2.024, Test accuracy: 45.32 

Round  93, Global train loss: 0.915, Global test loss: 1.389, Global test accuracy: 56.16 

Round  94, Train loss: 0.959, Test loss: 2.034, Test accuracy: 45.42 

Round  94, Global train loss: 0.959, Global test loss: 1.411, Global test accuracy: 54.83 

Round  95, Train loss: 0.905, Test loss: 2.045, Test accuracy: 45.33 

Round  95, Global train loss: 0.905, Global test loss: 1.343, Global test accuracy: 57.80 

Round  96, Train loss: 0.970, Test loss: 2.042, Test accuracy: 45.24 

Round  96, Global train loss: 0.970, Global test loss: 1.424, Global test accuracy: 54.63 

Round  97, Train loss: 0.826, Test loss: 2.074, Test accuracy: 45.04 

Round  97, Global train loss: 0.826, Global test loss: 1.399, Global test accuracy: 56.89 

Round  98, Train loss: 1.007, Test loss: 2.065, Test accuracy: 45.27 

Round  98, Global train loss: 1.007, Global test loss: 1.365, Global test accuracy: 56.34 

Round  99, Train loss: 0.972, Test loss: 2.086, Test accuracy: 45.09 

Round  99, Global train loss: 0.972, Global test loss: 1.544, Global test accuracy: 50.99 

Final Round, Train loss: 0.729, Test loss: 2.314, Test accuracy: 44.15 

Final Round, Global train loss: 0.729, Global test loss: 1.544, Global test accuracy: 50.99 

Average accuracy final 10 rounds: 45.24275000000001 

Average global accuracy final 10 rounds: 55.108000000000004 

2717.7374601364136
[1.496361494064331, 2.7612593173980713, 4.025646209716797, 5.287736177444458, 6.54688572883606, 7.811468124389648, 9.073362827301025, 10.329586029052734, 11.590712785720825, 12.881981611251831, 14.002526044845581, 15.12631893157959, 16.244140148162842, 17.536739587783813, 18.658364295959473, 19.778419494628906, 20.89648461341858, 22.016291618347168, 23.135244131088257, 24.257281064987183, 25.378098487854004, 26.497803926467896, 27.614186763763428, 28.728941917419434, 29.848133087158203, 30.963643789291382, 32.08241105079651, 33.203887939453125, 34.328232288360596, 35.448647022247314, 36.57021737098694, 37.68690824508667, 38.80473256111145, 39.92062520980835, 41.03425216674805, 42.14805722236633, 43.262240171432495, 44.376861810684204, 45.49054980278015, 46.60241222381592, 47.7210111618042, 48.83659911155701, 49.955078125, 51.07941389083862, 52.20197057723999, 53.32304859161377, 54.444480895996094, 55.5693838596344, 56.70156192779541, 57.820672035217285, 58.94128370285034, 60.059672355651855, 61.177714109420776, 62.29499268531799, 63.41201949119568, 64.54226231575012, 65.65865874290466, 66.77272486686707, 67.89028835296631, 69.01228857040405, 70.13339805603027, 71.25666832923889, 72.3843502998352, 73.50960493087769, 74.76526665687561, 76.03627943992615, 77.3105936050415, 78.56735444068909, 79.78444314002991, 81.03597235679626, 82.28270030021667, 83.48363494873047, 84.7350754737854, 85.98438000679016, 87.2383017539978, 88.49817276000977, 89.75570511817932, 91.01354336738586, 92.27517867088318, 93.53742671012878, 94.80179977416992, 96.07631397247314, 97.33459830284119, 98.59073781967163, 99.84390497207642, 101.10245609283447, 102.18130898475647, 103.25876903533936, 104.33763551712036, 105.41476440429688, 106.4943585395813, 107.57363724708557, 108.65111207962036, 109.73142695426941, 110.8102548122406, 111.88846111297607, 112.96751976013184, 114.04483771324158, 115.12079787254333, 116.19608354568481, 118.38627934455872]
[21.85, 24.34, 26.6675, 26.845, 28.325, 29.3375, 30.3875, 31.7775, 33.57, 34.2075, 35.455, 35.8825, 35.8575, 36.5825, 36.9575, 37.8375, 38.5325, 39.21, 39.94, 40.3825, 40.715, 40.935, 41.105, 41.94, 41.7875, 41.6925, 42.0625, 42.34, 42.51, 42.7125, 42.895, 43.0975, 42.54, 43.19, 43.405, 43.5125, 43.76, 44.0775, 43.72, 43.76, 44.3825, 44.7725, 44.445, 44.3475, 44.6425, 45.26, 45.075, 44.715, 44.5925, 44.5525, 44.9675, 44.5575, 45.175, 44.7225, 44.7125, 44.9275, 44.415, 44.1475, 44.215, 44.505, 44.51, 44.8775, 44.78, 44.3, 44.5525, 44.635, 44.725, 44.52, 44.425, 44.23, 44.17, 43.965, 44.27, 44.6575, 44.7175, 44.315, 44.2475, 43.92, 43.745, 44.13, 44.7225, 44.1475, 44.4875, 44.1925, 44.365, 44.745, 45.0725, 45.04, 45.02, 45.165, 45.1625, 45.3, 45.255, 45.3225, 45.425, 45.325, 45.2425, 45.0375, 45.2725, 45.085, 44.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8710
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6035
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8700
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5360
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7400
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8445
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5080
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7880
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7145
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8805
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8685
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5880
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8630
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.4930
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7690
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8505
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4665
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7655
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7520
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8810
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  27.3900
Round 1 global test acc  32.5200
Round 2 global test acc  34.5400
Round 3 global test acc  42.9700
Round 4 global test acc  42.4700
Round 5 global test acc  42.9000
Round 6 global test acc  46.7200
Round 7 global test acc  45.2400
Round 8 global test acc  47.6100
Round 9 global test acc  46.0200
Round 10 global test acc  53.4700
Round 11 global test acc  45.8700
Round 12 global test acc  49.2500
Round 13 global test acc  50.9600
Round 14 global test acc  49.3000
Round 15 global test acc  52.1700
Round 16 global test acc  54.5800
Round 17 global test acc  54.1100
Round 18 global test acc  51.4400
Round 19 global test acc  55.6900
Round 20 global test acc  49.3100
Round 21 global test acc  51.5400
Round 22 global test acc  52.1100
Round 23 global test acc  54.7900
Round 24 global test acc  55.5800
Round 25 global test acc  58.2900
Round 26 global test acc  53.6100
Round 27 global test acc  55.5800
Round 28 global test acc  54.6200
Round 29 global test acc  58.1300
Round 30 global test acc  59.3100
Round 31 global test acc  54.2800
Round 32 global test acc  57.1300
Round 33 global test acc  55.5700
Round 34 global test acc  51.2200
Round 35 global test acc  55.4400
Round 36 global test acc  56.1200
Round 37 global test acc  54.9300
Round 38 global test acc  53.7400
Round 39 global test acc  54.9000
Round 40 global test acc  59.1000
Round 41 global test acc  54.6000
Round 42 global test acc  60.7700
Round 43 global test acc  59.5200
Round 44 global test acc  57.9300
Round 45 global test acc  56.7600
Round 46 global test acc  56.9300
Round 47 global test acc  55.2900
Round 48 global test acc  60.5100
Round 49 global test acc  57.3600
Round 50 global test acc  60.5600
Round 51 global test acc  51.5000
Round 52 global test acc  58.6600
Round 53 global test acc  61.4100
Round 54 global test acc  61.5300
Round 55 global test acc  58.7100
Round 56 global test acc  57.9500
Round 57 global test acc  60.4400
Round 58 global test acc  57.9400
Round 59 global test acc  59.5100
Round 60 global test acc  54.4700
Round 61 global test acc  62.3500
Round 62 global test acc  58.2400
Round 63 global test acc  57.4200
Round 64 global test acc  56.7900
Round 65 global test acc  53.6600
Round 66 global test acc  59.6000
Round 67 global test acc  61.2900
Round 68 global test acc  57.6100
Round 69 global test acc  59.1300
Round 70 global test acc  58.9600
Round 71 global test acc  57.9300
Round 72 global test acc  60.8300
Round 73 global test acc  60.5600
Round 74 global test acc  58.2600
Round 75 global test acc  58.5500
Round 76 global test acc  60.2000
Round 77 global test acc  60.7100
Round 78 global test acc  58.8600
Round 79 global test acc  60.6900
Round 80 global test acc  60.8000
Round 81 global test acc  59.5600
Round 82 global test acc  59.3800
Round 83 global test acc  59.0600
Round 84 global test acc  57.9000
Round 85 global test acc  58.7500
Round 86 global test acc  57.7800
Round 87 global test acc  57.8800
Round 88 global test acc  57.9300
Round 89 global test acc  57.2200
Round 90 global test acc  56.7700
Round 91 global test acc  57.2000
Round 92 global test acc  55.3900
Round 93 global test acc  55.6400
Round 94 global test acc  55.6000
Round 95 global test acc  54.6500
Round 96 global test acc  54.5000
Round 97 global test acc  56.2600
Round 98 global test acc  55.4500
Round 99 global test acc  55.4000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8855
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6160
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8695
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5100
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7260
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8555
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4965
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7825
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7515
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8895
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.558, Test loss: 0.976, Test accuracy: 68.11
Average accuracy final 10 rounds: 67.14500000000001
1923.3468129634857
[1.697258472442627, 3.081606388092041, 4.478444814682007, 5.866918563842773, 7.2450690269470215, 8.647677659988403, 10.046578168869019, 11.429511785507202, 12.821228742599487, 14.20957899093628, 15.592373847961426, 16.978964805603027, 18.37149143218994, 19.745290279388428, 21.120570421218872, 22.503907442092896, 23.886508464813232, 25.280659198760986, 26.66218400001526, 28.035426378250122, 29.41197180747986, 30.801541328430176, 32.187206983566284, 33.57357621192932, 34.95383048057556, 36.33375430107117, 37.71102213859558, 39.09268879890442, 40.48767614364624, 41.85925626754761, 43.23051905632019, 44.62098956108093, 46.0022337436676, 47.38621187210083, 48.77360725402832, 50.150599002838135, 51.52013611793518, 52.91150426864624, 54.301891803741455, 55.67518496513367, 57.060683488845825, 58.447649002075195, 59.82624340057373, 61.20786762237549, 62.610400915145874, 63.985575675964355, 65.36940288543701, 66.75646877288818, 68.14593625068665, 69.52324318885803, 70.90639615058899, 72.28624510765076, 73.67253828048706, 75.05602216720581, 76.44081664085388, 77.81569814682007, 79.19283413887024, 80.57217049598694, 81.95921277999878, 83.32604455947876, 84.70420527458191, 86.07492327690125, 87.4512267112732, 88.83338713645935, 90.20605969429016, 91.57465934753418, 92.95379376411438, 94.32878112792969, 95.69997262954712, 97.08249855041504, 98.45253705978394, 99.81779432296753, 101.2161500453949, 102.59980654716492, 103.96718406677246, 105.34515476226807, 106.7234570980072, 108.09343028068542, 109.48481750488281, 110.85854649543762, 112.23070502281189, 113.61521577835083, 115.00260806083679, 116.37016534805298, 117.74670028686523, 119.11683082580566, 120.49442458152771, 121.87574243545532, 123.25355744361877, 124.61854863166809, 126.00904679298401, 127.39300847053528, 128.76900124549866, 130.13607907295227, 131.5170624256134, 132.89770078659058, 134.27306175231934, 135.64557218551636, 137.02692532539368, 138.40200781822205, 140.5070390701294]
[19.3825, 24.95, 30.2875, 33.275, 35.3625, 37.4125, 39.4825, 40.745, 42.4375, 43.3025, 44.5075, 45.55, 46.86, 47.8, 49.1575, 50.1325, 48.86, 49.75, 51.465, 52.555, 53.3025, 54.17, 54.775, 54.6875, 55.745, 55.92, 56.785, 56.635, 57.58, 57.905, 58.2, 58.1275, 58.175, 59.3075, 59.8375, 60.2175, 59.87, 60.59, 61.2175, 60.9925, 61.055, 61.9925, 62.1025, 62.41, 62.455, 62.3325, 61.645, 62.3125, 63.1125, 62.155, 61.9425, 63.12, 63.4875, 64.105, 64.36, 63.6925, 64.2975, 64.2275, 64.01, 63.9675, 64.305, 64.3625, 64.3875, 64.4175, 65.245, 65.2775, 65.23, 65.405, 65.425, 65.6475, 65.42, 65.84, 65.885, 65.615, 65.8975, 65.7225, 65.8975, 66.42, 66.45, 66.355, 66.1525, 66.715, 67.12, 66.49, 66.5675, 67.2375, 66.9225, 67.1125, 67.0975, 66.74, 67.0875, 67.005, 67.2, 67.5275, 67.4425, 67.505, 67.425, 66.7025, 66.8925, 66.6625, 68.1075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8700
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5840
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8735
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5075
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.8075
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8680
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4875
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7945
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.6875
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8895
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2065
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0575
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7235
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1085
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5855
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5145
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0695
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2460
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6435
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3110
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.290, Test loss: 5.113, Test accuracy: 31.54
Final Round, Global train loss: 0.290, Global test loss: 1.706, Global test accuracy: 44.94
Average accuracy final 10 rounds: 31.601000000000003 

Average global accuracy final 10 rounds: 40.365249999999996 

3915.6937005519867
[1.4404387474060059, 2.8808774948120117, 4.087364673614502, 5.293851852416992, 6.502847671508789, 7.711843490600586, 8.917244672775269, 10.122645854949951, 11.326619386672974, 12.530592918395996, 13.742833614349365, 14.955074310302734, 16.157200813293457, 17.35932731628418, 18.566506385803223, 19.773685455322266, 20.977768421173096, 22.181851387023926, 23.381224870681763, 24.5805983543396, 25.777329444885254, 26.974060535430908, 28.16968846321106, 29.36531639099121, 30.574002742767334, 31.782689094543457, 32.98108386993408, 34.17947864532471, 35.37797665596008, 36.57647466659546, 37.77892446517944, 38.98137426376343, 40.18446230888367, 41.387550354003906, 42.58675217628479, 43.785953998565674, 44.98140907287598, 46.17686414718628, 47.37721562385559, 48.5775671005249, 49.77885937690735, 50.980151653289795, 52.17951583862305, 53.3788800239563, 54.584500551223755, 55.79012107849121, 56.983131408691406, 58.1761417388916, 59.37270164489746, 60.56926155090332, 61.76221561431885, 62.955169677734375, 64.1497814655304, 65.34439325332642, 66.54374480247498, 67.74309635162354, 68.93741607666016, 70.13173580169678, 71.32937479019165, 72.52701377868652, 73.7337429523468, 74.94047212600708, 76.14107775688171, 77.34168338775635, 78.53971147537231, 79.73773956298828, 80.93166828155518, 82.12559700012207, 83.33155250549316, 84.53750801086426, 85.74031043052673, 86.94311285018921, 88.14127206802368, 89.33943128585815, 90.55152750015259, 91.76362371444702, 92.96525645256042, 94.16688919067383, 95.35605478286743, 96.54522037506104, 97.73146986961365, 98.91771936416626, 100.11536931991577, 101.31301927566528, 102.51432037353516, 103.71562147140503, 104.92228364944458, 106.12894582748413, 107.33452916145325, 108.54011249542236, 109.74190330505371, 110.94369411468506, 112.14754724502563, 113.35140037536621, 114.55066776275635, 115.74993515014648, 116.94993185997009, 118.1499285697937, 119.35543632507324, 120.56094408035278, 121.75821661949158, 122.95548915863037, 124.1578438282013, 125.36019849777222, 126.57028937339783, 127.78038024902344, 128.96991324424744, 130.15944623947144, 131.3455934524536, 132.5317406654358, 133.7277648448944, 134.92378902435303, 136.12371492385864, 137.32364082336426, 138.5188820362091, 139.71412324905396, 140.92143201828003, 142.1287407875061, 143.3238320350647, 144.5189232826233, 145.7078139781952, 146.8967046737671, 148.0938277244568, 149.29095077514648, 150.48790645599365, 151.68486213684082, 152.8898892402649, 154.09491634368896, 155.2914547920227, 156.48799324035645, 157.6977412700653, 158.90748929977417, 160.1070101261139, 161.3065309524536, 162.5144658088684, 163.7224006652832, 164.92983675003052, 166.13727283477783, 167.33229970932007, 168.5273265838623, 169.7329604625702, 170.93859434127808, 172.1441752910614, 173.34975624084473, 174.56512331962585, 175.78049039840698, 176.9835865497589, 178.18668270111084, 179.38563656806946, 180.58459043502808, 181.7930600643158, 183.00152969360352, 184.20127296447754, 185.40101623535156, 186.60987377166748, 187.8187313079834, 189.01959252357483, 190.22045373916626, 191.41330003738403, 192.6061463356018, 193.8062081336975, 195.0062699317932, 196.21242594718933, 197.41858196258545, 198.62051820755005, 199.82245445251465, 200.86431527137756, 201.90617609024048, 202.94660353660583, 203.9870309829712, 205.0309865474701, 206.074942111969, 207.1141254901886, 208.1533088684082, 209.18635153770447, 210.21939420700073, 211.25928616523743, 212.29917812347412, 213.3456404209137, 214.39210271835327, 215.43532967567444, 216.4785566329956, 217.5241940021515, 218.56983137130737, 219.60876727104187, 220.64770317077637, 221.6891152858734, 222.73052740097046, 223.77207159996033, 224.8136157989502, 225.85149550437927, 226.88937520980835, 227.92674112319946, 228.96410703659058, 230.00769972801208, 231.0512924194336, 232.10159182548523, 233.15189123153687, 234.1983983516693, 235.24490547180176, 237.33003616333008, 239.4151668548584]
[17.675, 17.675, 24.0925, 24.0925, 26.1275, 26.1275, 26.5925, 26.5925, 26.8975, 26.8975, 27.97, 27.97, 28.81, 28.81, 29.7975, 29.7975, 29.9925, 29.9925, 30.235, 30.235, 30.635, 30.635, 30.905, 30.905, 31.2075, 31.2075, 31.6525, 31.6525, 32.205, 32.205, 32.6575, 32.6575, 32.7725, 32.7725, 33.3425, 33.3425, 33.32, 33.32, 33.7425, 33.7425, 33.9075, 33.9075, 33.525, 33.525, 33.785, 33.785, 33.995, 33.995, 33.935, 33.935, 33.0775, 33.0775, 33.3175, 33.3175, 33.36, 33.36, 33.4625, 33.4625, 33.5575, 33.5575, 33.1525, 33.1525, 33.035, 33.035, 33.165, 33.165, 33.585, 33.585, 33.34, 33.34, 33.035, 33.035, 32.8625, 32.8625, 33.16, 33.16, 33.155, 33.155, 32.6175, 32.6175, 32.7225, 32.7225, 32.3625, 32.3625, 32.69, 32.69, 32.4975, 32.4975, 32.705, 32.705, 32.22, 32.22, 32.3175, 32.3175, 32.2675, 32.2675, 32.3775, 32.3775, 32.4625, 32.4625, 32.5525, 32.5525, 32.025, 32.025, 32.25, 32.25, 32.1475, 32.1475, 31.86, 31.86, 31.8525, 31.8525, 32.0025, 32.0025, 32.0425, 32.0425, 32.0225, 32.0225, 31.8075, 31.8075, 31.84, 31.84, 31.775, 31.775, 31.77, 31.77, 31.6575, 31.6575, 31.6625, 31.6625, 31.2075, 31.2075, 31.4625, 31.4625, 31.775, 31.775, 31.77, 31.77, 32.28, 32.28, 31.8525, 31.8525, 31.725, 31.725, 31.6575, 31.6575, 31.8, 31.8, 31.9875, 31.9875, 31.8025, 31.8025, 31.7475, 31.7475, 31.5625, 31.5625, 31.7775, 31.7775, 31.9025, 31.9025, 31.7925, 31.7925, 31.7225, 31.7225, 31.6225, 31.6225, 31.6925, 31.6925, 31.5825, 31.5825, 31.59, 31.59, 31.75, 31.75, 31.8525, 31.8525, 31.93, 31.93, 31.46, 31.46, 31.765, 31.765, 31.5975, 31.5975, 31.66, 31.66, 31.8425, 31.8425, 31.5675, 31.5675, 31.5525, 31.5525, 31.7025, 31.7025, 31.75, 31.75, 31.2225, 31.2225, 31.35, 31.35, 31.54, 31.54]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2050
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0550
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7340
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2000
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5855
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4465
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1195
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3555
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6780
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3155
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.493, Test loss: 2.051, Test accuracy: 53.04
Final Round, Global train loss: 0.493, Global test loss: 1.307, Global test accuracy: 61.36
Average accuracy final 10 rounds: 53.47425 

Average global accuracy final 10 rounds: 62.219 

3895.357969522476
[1.4508700370788574, 2.901740074157715, 4.091190338134766, 5.280640602111816, 6.477564096450806, 7.674487590789795, 8.89134693145752, 10.108206272125244, 11.336436986923218, 12.564667701721191, 13.79271674156189, 15.020765781402588, 16.234060049057007, 17.447354316711426, 18.66580057144165, 19.884246826171875, 20.969895601272583, 22.05554437637329, 23.24549627304077, 24.435448169708252, 25.645458459854126, 26.85546875, 28.069326877593994, 29.28318500518799, 30.492393493652344, 31.7016019821167, 32.909448862075806, 34.11729574203491, 35.32285761833191, 36.528419494628906, 37.73210334777832, 38.935787200927734, 40.14143753051758, 41.34708786010742, 42.554643392562866, 43.76219892501831, 44.977288007736206, 46.1923770904541, 47.41206216812134, 48.631747245788574, 49.84007239341736, 51.04839754104614, 52.24306654930115, 53.43773555755615, 54.65088939666748, 55.86404323577881, 57.07034969329834, 58.27665615081787, 59.48507356643677, 60.693490982055664, 61.91071581840515, 63.12794065475464, 64.33794283866882, 65.54794502258301, 66.74582076072693, 67.94369649887085, 69.14781498908997, 70.35193347930908, 71.55466341972351, 72.75739336013794, 73.95411705970764, 75.15084075927734, 76.33681082725525, 77.52278089523315, 78.71687841415405, 79.91097593307495, 81.10768699645996, 82.30439805984497, 83.50448441505432, 84.70457077026367, 85.89554119110107, 87.08651161193848, 88.27712440490723, 89.46773719787598, 90.66155004501343, 91.85536289215088, 93.06173610687256, 94.26810932159424, 95.47018361091614, 96.67225790023804, 97.86507892608643, 99.05789995193481, 100.26172351837158, 101.46554708480835, 102.66192960739136, 103.85831212997437, 105.07111072540283, 106.2839093208313, 107.48340559005737, 108.68290185928345, 109.88722062110901, 111.09153938293457, 112.27288460731506, 113.45422983169556, 114.6389684677124, 115.82370710372925, 117.00171971321106, 118.17973232269287, 119.36350440979004, 120.5472764968872, 121.7218987941742, 122.89652109146118, 124.07158184051514, 125.24664258956909, 126.42383551597595, 127.60102844238281, 128.79186129570007, 129.98269414901733, 131.1781358718872, 132.37357759475708, 133.5715320110321, 134.76948642730713, 135.96543645858765, 137.16138648986816, 138.34493041038513, 139.5284743309021, 140.72238540649414, 141.91629648208618, 143.1130542755127, 144.3098120689392, 145.50625944137573, 146.70270681381226, 147.89063143730164, 149.07855606079102, 150.27928471565247, 151.48001337051392, 152.6692099571228, 153.8584065437317, 155.03820872306824, 156.21801090240479, 157.39359736442566, 158.56918382644653, 159.75232815742493, 160.93547248840332, 162.12618613243103, 163.31689977645874, 164.57212734222412, 165.8273549079895, 167.0235185623169, 168.2196822166443, 169.4039921760559, 170.58830213546753, 171.7756688594818, 172.9630355834961, 174.1512315273285, 175.3394274711609, 176.52269530296326, 177.70596313476562, 178.8956801891327, 180.08539724349976, 181.27674913406372, 182.46810102462769, 183.65654230117798, 184.84498357772827, 186.03339099884033, 187.2217984199524, 188.41342401504517, 189.60504961013794, 190.7949197292328, 191.98478984832764, 193.18006372451782, 194.375337600708, 195.56523251533508, 196.75512742996216, 197.93751573562622, 199.11990404129028, 200.3119695186615, 201.50403499603271, 202.68818998336792, 203.87234497070312, 205.04237365722656, 206.21240234375, 207.39820003509521, 208.58399772644043, 209.77827382087708, 210.97254991531372, 212.16665625572205, 213.36076259613037, 214.5510401725769, 215.74131774902344, 216.93263936042786, 218.12396097183228, 219.31339859962463, 220.502836227417, 221.69197726249695, 222.8811182975769, 224.07802820205688, 225.27493810653687, 226.47678804397583, 227.6786379814148, 228.86549043655396, 230.05234289169312, 231.236230134964, 232.42011737823486, 233.61113834381104, 234.8021593093872, 235.99202632904053, 237.18189334869385, 238.36716651916504, 239.55243968963623, 241.9498336315155, 244.34722757339478]
[23.5325, 23.5325, 26.255, 26.255, 28.16, 28.16, 29.2125, 29.2125, 30.175, 30.175, 32.22, 32.22, 33.75, 33.75, 36.485, 36.485, 37.5475, 37.5475, 38.345, 38.345, 39.8525, 39.8525, 40.3225, 40.3225, 40.9725, 40.9725, 42.4, 42.4, 43.595, 43.595, 44.2275, 44.2275, 44.4025, 44.4025, 44.9575, 44.9575, 46.17, 46.17, 46.585, 46.585, 46.3175, 46.3175, 46.91, 46.91, 47.0725, 47.0725, 47.765, 47.765, 48.3625, 48.3625, 48.01, 48.01, 48.1, 48.1, 48.42, 48.42, 48.64, 48.64, 48.6025, 48.6025, 49.0975, 49.0975, 49.1525, 49.1525, 49.47, 49.47, 49.73, 49.73, 49.925, 49.925, 50.055, 50.055, 49.895, 49.895, 50.16, 50.16, 50.7725, 50.7725, 50.9375, 50.9375, 50.5675, 50.5675, 50.8075, 50.8075, 51.1175, 51.1175, 51.44, 51.44, 51.7775, 51.7775, 51.825, 51.825, 51.39, 51.39, 51.4675, 51.4675, 51.4225, 51.4225, 51.65, 51.65, 51.685, 51.685, 51.4175, 51.4175, 51.5875, 51.5875, 51.9925, 51.9925, 52.1325, 52.1325, 52.08, 52.08, 51.7225, 51.7225, 51.8, 51.8, 51.7, 51.7, 51.8525, 51.8525, 51.9225, 51.9225, 52.32, 52.32, 52.4025, 52.4025, 52.4075, 52.4075, 52.24, 52.24, 52.5025, 52.5025, 52.545, 52.545, 52.6925, 52.6925, 52.66, 52.66, 52.71, 52.71, 52.78, 52.78, 52.93, 52.93, 53.1525, 53.1525, 53.2175, 53.2175, 53.1825, 53.1825, 53.405, 53.405, 53.565, 53.565, 52.9425, 52.9425, 52.9075, 52.9075, 52.825, 52.825, 53.21, 53.21, 53.355, 53.355, 53.5325, 53.5325, 53.545, 53.545, 53.675, 53.675, 53.44, 53.44, 53.33, 53.33, 53.4425, 53.4425, 53.5025, 53.5025, 53.4225, 53.4225, 53.425, 53.425, 53.2825, 53.2825, 53.3425, 53.3425, 53.3125, 53.3125, 53.565, 53.565, 53.4125, 53.4125, 53.7425, 53.7425, 53.6125, 53.6125, 53.515, 53.515, 53.5325, 53.5325, 53.04, 53.04]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2075
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0770
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7285
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1955
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5770
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4780
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3285
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3005
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6910
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4175
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.585, Test loss: 1.860, Test accuracy: 53.79
Final Round, Global train loss: 0.585, Global test loss: 1.265, Global test accuracy: 61.91
Average accuracy final 10 rounds: 54.018 

Average global accuracy final 10 rounds: 61.902750000000005 

3861.4236676692963
[1.5139551162719727, 3.0279102325439453, 4.309161186218262, 5.590412139892578, 6.874286651611328, 8.158161163330078, 9.435220956802368, 10.712280750274658, 12.003278255462646, 13.294275760650635, 14.579126834869385, 15.863977909088135, 17.15173888206482, 18.439499855041504, 19.72748613357544, 21.015472412109375, 22.302653551101685, 23.589834690093994, 24.880720615386963, 26.17160654067993, 27.456088542938232, 28.740570545196533, 30.027918100357056, 31.315265655517578, 32.59809327125549, 33.88092088699341, 35.168819189071655, 36.4567174911499, 37.747565269470215, 39.03841304779053, 40.3184597492218, 41.598506450653076, 42.88242959976196, 44.16635274887085, 45.43176627159119, 46.69717979431152, 47.9623646736145, 49.22754955291748, 50.4921760559082, 51.756802558898926, 53.023515462875366, 54.29022836685181, 55.55310320854187, 56.815978050231934, 58.07348322868347, 59.33098840713501, 60.607611894607544, 61.88423538208008, 63.16551756858826, 64.44679975509644, 65.73217487335205, 67.01754999160767, 68.30655813217163, 69.5955662727356, 70.86854028701782, 72.14151430130005, 73.4882402420044, 74.83496618270874, 76.12589120864868, 77.41681623458862, 78.70363116264343, 79.99044609069824, 81.27274322509766, 82.55504035949707, 83.83634686470032, 85.11765336990356, 86.39568662643433, 87.67371988296509, 88.97125434875488, 90.26878881454468, 91.54564166069031, 92.82249450683594, 93.92183017730713, 95.02116584777832, 96.12023591995239, 97.21930599212646, 98.3209490776062, 99.42259216308594, 100.52755379676819, 101.63251543045044, 102.73154282569885, 103.83057022094727, 104.93416929244995, 106.03776836395264, 107.14398241043091, 108.25019645690918, 109.35472583770752, 110.45925521850586, 111.55852270126343, 112.657790184021, 113.76476311683655, 114.8717360496521, 115.97728824615479, 117.08284044265747, 118.19345164299011, 119.30406284332275, 120.40670323371887, 121.50934362411499, 122.61707282066345, 123.72480201721191, 124.8199782371521, 125.91515445709229, 127.02101302146912, 128.12687158584595, 129.22703385353088, 130.32719612121582, 131.4287121295929, 132.53022813796997, 133.63137865066528, 134.7325291633606, 135.83498978614807, 136.93745040893555, 138.05277252197266, 139.16809463500977, 140.275399684906, 141.38270473480225, 142.48961472511292, 143.59652471542358, 144.70113134384155, 145.80573797225952, 146.92093896865845, 148.03613996505737, 149.1484820842743, 150.2608242034912, 151.37091064453125, 152.4809970855713, 153.5909080505371, 154.70081901550293, 155.81336784362793, 156.92591667175293, 158.0331952571869, 159.14047384262085, 160.25477719306946, 161.36908054351807, 162.4745090007782, 163.57993745803833, 164.6878457069397, 165.79575395584106, 166.90696096420288, 168.0181679725647, 169.12484192848206, 170.2315158843994, 171.34138202667236, 172.4512481689453, 173.55856490135193, 174.66588163375854, 175.77743244171143, 176.8889832496643, 177.996506690979, 179.1040301322937, 180.21876907348633, 181.33350801467896, 182.4471137523651, 183.56071949005127, 184.66849040985107, 185.77626132965088, 186.8921172618866, 188.00797319412231, 189.11098766326904, 190.21400213241577, 191.33190536499023, 192.4498085975647, 193.55724143981934, 194.66467428207397, 195.77969074249268, 196.89470720291138, 197.99980187416077, 199.10489654541016, 200.2336187362671, 201.36234092712402, 202.46883583068848, 203.57533073425293, 204.67747449874878, 205.77961826324463, 206.89608502388, 208.01255178451538, 209.1291356086731, 210.2457194328308, 211.35477542877197, 212.46383142471313, 213.57229280471802, 214.6807541847229, 215.7884223461151, 216.89609050750732, 218.00267219543457, 219.10925388336182, 220.21593284606934, 221.32261180877686, 222.42742538452148, 223.5322389602661, 224.64135313034058, 225.75046730041504, 226.86061453819275, 227.97076177597046, 229.08404207229614, 230.19732236862183, 231.30741095542908, 232.41749954223633, 233.52473306655884, 234.63196659088135, 236.85924243927002, 239.0865182876587]
[19.985, 19.985, 25.5575, 25.5575, 26.2825, 26.2825, 28.1425, 28.1425, 28.985, 28.985, 31.1175, 31.1175, 32.4025, 32.4025, 34.43, 34.43, 36.2575, 36.2575, 37.4175, 37.4175, 38.6125, 38.6125, 38.725, 38.725, 39.425, 39.425, 40.66, 40.66, 42.9075, 42.9075, 43.33, 43.33, 44.0475, 44.0475, 44.625, 44.625, 45.5475, 45.5475, 45.9475, 45.9475, 46.7625, 46.7625, 47.0725, 47.0725, 46.7475, 46.7475, 47.1225, 47.1225, 47.3, 47.3, 47.5375, 47.5375, 47.9525, 47.9525, 48.8325, 48.8325, 49.135, 49.135, 48.7975, 48.7975, 48.6575, 48.6575, 49.2875, 49.2875, 49.4225, 49.4225, 49.92, 49.92, 50.2775, 50.2775, 50.0625, 50.0625, 50.485, 50.485, 50.855, 50.855, 51.4025, 51.4025, 51.96, 51.96, 52.1075, 52.1075, 52.1525, 52.1525, 52.0875, 52.0875, 52.5125, 52.5125, 52.11, 52.11, 52.03, 52.03, 52.53, 52.53, 52.5775, 52.5775, 52.4875, 52.4875, 52.9225, 52.9225, 52.8625, 52.8625, 52.64, 52.64, 52.48, 52.48, 52.315, 52.315, 52.57, 52.57, 51.6725, 51.6725, 51.745, 51.745, 51.9925, 51.9925, 51.9475, 51.9475, 52.27, 52.27, 52.4525, 52.4525, 52.715, 52.715, 52.11, 52.11, 52.35, 52.35, 52.0, 52.0, 52.1525, 52.1525, 52.275, 52.275, 52.6675, 52.6675, 52.81, 52.81, 53.1175, 53.1175, 53.0775, 53.0775, 53.51, 53.51, 53.8325, 53.8325, 53.8825, 53.8825, 53.6825, 53.6825, 53.785, 53.785, 53.655, 53.655, 53.695, 53.695, 53.36, 53.36, 53.0725, 53.0725, 52.98, 52.98, 53.2275, 53.2275, 53.25, 53.25, 53.5875, 53.5875, 53.575, 53.575, 53.675, 53.675, 53.645, 53.645, 53.2975, 53.2975, 53.8025, 53.8025, 54.0175, 54.0175, 54.185, 54.185, 53.915, 53.915, 54.0225, 54.0225, 54.3425, 54.3425, 54.235, 54.235, 54.0175, 54.0175, 53.8025, 53.8025, 53.9425, 53.9425, 53.9075, 53.9075, 53.81, 53.81, 53.7875, 53.7875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2135
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0795
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7285
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0660
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5860
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4645
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0060
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3330
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6670
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3875
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2055
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0780
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7235
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0665
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5715
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4610
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0730
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2780
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6800
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3835
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  28.0500
Round 1 global test acc  31.5600
Round 2 global test acc  40.8300
Round 3 global test acc  46.0500
Round 4 global test acc  49.0700
Round 5 global test acc  46.0200
Round 6 global test acc  50.8300
Round 7 global test acc  50.9100
Round 8 global test acc  53.8900
Round 9 global test acc  53.2900
Round 10 global test acc  55.3100
Round 11 global test acc  54.2500
Round 12 global test acc  55.8000
Round 13 global test acc  55.5300
Round 14 global test acc  56.1600
Round 15 global test acc  56.6200
Round 16 global test acc  58.6900
Round 17 global test acc  56.6800
Round 18 global test acc  59.3400
Round 19 global test acc  59.1800
Round 20 global test acc  59.4600
Round 21 global test acc  59.6700
Round 22 global test acc  59.3900
Round 23 global test acc  60.6600
Round 24 global test acc  59.7800
Round 25 global test acc  61.5300
Round 26 global test acc  61.5800
Round 27 global test acc  61.7900
Round 28 global test acc  61.6700
Round 29 global test acc  59.9900
Round 30 global test acc  63.1100
Round 31 global test acc  63.2200
Round 32 global test acc  61.7000
Round 33 global test acc  64.3000
Round 34 global test acc  62.6200
Round 35 global test acc  62.3900
Round 36 global test acc  63.7400
Round 37 global test acc  62.1000
Round 38 global test acc  63.7900
Round 39 global test acc  63.0500
Round 40 global test acc  63.9600
Round 41 global test acc  63.1700
Round 42 global test acc  64.4000
Round 43 global test acc  61.7200
Round 44 global test acc  63.6300
Round 45 global test acc  65.0900
Round 46 global test acc  63.9900
Round 47 global test acc  65.1000
Round 48 global test acc  64.9700
Round 49 global test acc  65.3800
Round 50 global test acc  64.8100
Round 51 global test acc  65.3300
Round 52 global test acc  64.8000
Round 53 global test acc  65.3300
Round 54 global test acc  63.7400
Round 55 global test acc  64.9800
Round 56 global test acc  66.4400
Round 57 global test acc  65.8200
Round 58 global test acc  64.7600
Round 59 global test acc  66.8700
Round 60 global test acc  65.8600
Round 61 global test acc  65.3700
Round 62 global test acc  65.5800
Round 63 global test acc  65.6400
Round 64 global test acc  66.1900
Round 65 global test acc  66.6200
Round 66 global test acc  66.2000
Round 67 global test acc  66.0400
Round 68 global test acc  67.1900
Round 69 global test acc  67.2900
Round 70 global test acc  66.2200
Round 71 global test acc  67.3100
Round 72 global test acc  67.7200
Round 73 global test acc  67.4400
Round 74 global test acc  67.3400
Round 75 global test acc  67.2100
Round 76 global test acc  66.3900
Round 77 global test acc  68.3700
Round 78 global test acc  67.0700
Round 79 global test acc  67.4500
Round 80 global test acc  67.0800
Round 81 global test acc  65.7500
Round 82 global test acc  64.0700
Round 83 global test acc  63.6500
Round 84 global test acc  62.9300
Round 85 global test acc  61.6700
Round 86 global test acc  61.8400
Round 87 global test acc  60.5600
Round 88 global test acc  60.3900
Round 89 global test acc  59.9400
Round 90 global test acc  59.8200
Round 91 global test acc  59.5600
Round 92 global test acc  59.1300
Round 93 global test acc  58.3000
Round 94 global test acc  58.5600
Round 95 global test acc  58.5800
Round 96 global test acc  57.9300
Round 97 global test acc  58.5800
Round 98 global test acc  58.4400
Round 99 global test acc  58.6800
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2100
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0790
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7190
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0605
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5945
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4385
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1775
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.4135
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6820
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4105
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.552, Test loss: 0.986, Test accuracy: 66.90
Average accuracy final 10 rounds: 66.63575
1869.511577129364
[1.7423474788665771, 3.162517547607422, 4.435385704040527, 5.685310363769531, 6.952738046646118, 8.21934962272644, 9.467183828353882, 10.733124732971191, 11.98501181602478, 13.23247504234314, 14.483497858047485, 15.727579832077026, 17.104257106781006, 18.49926447868347, 19.873652696609497, 21.269036531448364, 22.653103590011597, 24.040428161621094, 25.42329216003418, 26.809062957763672, 28.199949502944946, 29.578269243240356, 30.837002515792847, 32.086429834365845, 33.33882451057434, 34.6111741065979, 35.858168840408325, 37.10947346687317, 38.368226051330566, 39.62485647201538, 40.868415117263794, 42.11947560310364, 43.38572311401367, 44.64317798614502, 45.89656710624695, 47.1551787853241, 48.41575336456299, 49.659428119659424, 50.91221904754639, 52.17652773857117, 53.4413857460022, 54.694557666778564, 55.95835566520691, 57.22023153305054, 58.46779441833496, 59.74613857269287, 60.998164653778076, 62.29347348213196, 63.54596424102783, 64.79876923561096, 66.07346749305725, 67.31487464904785, 68.58641815185547, 69.83780026435852, 71.11228203773499, 72.39739441871643, 73.6495623588562, 74.90674090385437, 76.16186738014221, 77.4235725402832, 78.68636798858643, 79.96986794471741, 81.23411822319031, 82.51621294021606, 83.77753233909607, 85.01832890510559, 86.2989935874939, 87.54059195518494, 88.79354238510132, 90.06597328186035, 91.32444095611572, 92.6110463142395, 93.88211369514465, 95.13938975334167, 96.40032911300659, 97.65236496925354, 98.9043881893158, 100.18515729904175, 101.44061470031738, 102.69387936592102, 103.97050762176514, 105.2143816947937, 106.48816442489624, 107.74442958831787, 109.00535440444946, 110.27513074874878, 111.52843737602234, 112.7796516418457, 114.03550910949707, 115.30001306533813, 116.54886794090271, 117.80744767189026, 119.06805515289307, 120.32971930503845, 121.58805346488953, 122.84438705444336, 124.10354661941528, 125.34059453010559, 126.59731245040894, 127.85133790969849, 129.87142062187195]
[21.775, 26.8675, 30.85, 33.9, 35.955, 37.915, 39.9225, 41.555, 41.8725, 42.7175, 43.575, 45.0025, 45.875, 47.0175, 48.825, 49.3825, 49.8325, 50.6825, 51.4575, 52.14, 53.1125, 53.3675, 53.93, 54.3, 54.8675, 55.35, 56.1075, 54.93, 56.145, 55.835, 57.0225, 57.9975, 57.8425, 58.9775, 58.59, 58.9325, 59.3225, 59.7825, 60.7775, 60.7775, 60.945, 60.1875, 60.265, 61.2575, 60.91, 62.035, 62.11, 62.8075, 62.6675, 62.3875, 62.0925, 63.0925, 63.2725, 63.6725, 63.225, 63.5725, 64.035, 64.1725, 63.48, 63.2275, 63.935, 64.5875, 64.3275, 64.545, 64.845, 64.4225, 64.73, 64.53, 64.95, 65.435, 65.2125, 65.51, 65.175, 65.0225, 65.425, 65.72, 65.8875, 65.69, 65.665, 66.0975, 66.7625, 65.705, 66.18, 65.955, 66.245, 66.295, 66.45, 66.475, 66.3925, 66.1725, 66.2725, 66.4575, 66.6675, 67.05, 66.825, 66.8975, 66.635, 66.415, 66.5375, 66.6, 66.9]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2050
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0590
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7270
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0645
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5930
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5010
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0750
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2260
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6560
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3360
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5485
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4745
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8230
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4995
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7495
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7540
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5265
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5745
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8000
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6950
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.283, Test loss: 5.990, Test accuracy: 25.00
Final Round, Global train loss: 0.283, Global test loss: 1.830, Global test accuracy: 36.45
Average accuracy final 10 rounds: 24.735249999999997 

Average global accuracy final 10 rounds: 35.19375 

3844.011223554611
[1.439725637435913, 2.879451274871826, 4.069550275802612, 5.259649276733398, 6.449251890182495, 7.638854503631592, 8.810436964035034, 9.982019424438477, 11.19034719467163, 12.398674964904785, 13.603553771972656, 14.808432579040527, 16.002712726593018, 17.196992874145508, 18.240825414657593, 19.284657955169678, 20.315175533294678, 21.345693111419678, 22.376420736312866, 23.407148361206055, 24.442850589752197, 25.47855281829834, 26.510735034942627, 27.542917251586914, 28.568559408187866, 29.59420156478882, 30.62482213973999, 31.655442714691162, 32.678650856018066, 33.70185899734497, 34.72815704345703, 35.75445508956909, 36.78863596916199, 37.82281684875488, 38.8487446308136, 39.874672412872314, 40.89932346343994, 41.92397451400757, 42.948086738586426, 43.97219896316528, 44.993180990219116, 46.01416301727295, 47.04643154144287, 48.07870006561279, 49.119043588638306, 50.15938711166382, 51.183411836624146, 52.20743656158447, 53.239540338516235, 54.271644115448, 55.304386377334595, 56.33712863922119, 57.357848167419434, 58.378567695617676, 59.404016971588135, 60.429466247558594, 61.46508860588074, 62.50071096420288, 63.52528357505798, 64.54985618591309, 65.5836181640625, 66.61738014221191, 67.64953899383545, 68.68169784545898, 69.70917749404907, 70.73665714263916, 71.76197743415833, 72.78729772567749, 73.80107688903809, 74.81485605239868, 75.83833193778992, 76.86180782318115, 77.8890585899353, 78.91630935668945, 79.94494724273682, 80.97358512878418, 82.01705718040466, 83.06052923202515, 84.13341283798218, 85.20629644393921, 86.39470934867859, 87.58312225341797, 88.78271961212158, 89.9823169708252, 91.18224930763245, 92.3821816444397, 93.57313871383667, 94.76409578323364, 95.94784903526306, 97.13160228729248, 98.32764601707458, 99.52368974685669, 100.71986484527588, 101.91603994369507, 103.10935592651367, 104.30267190933228, 105.4999144077301, 106.69715690612793, 107.89261555671692, 109.08807420730591, 110.28314638137817, 111.47821855545044, 112.67131471633911, 113.86441087722778, 115.04990315437317, 116.23539543151855, 117.42558884620667, 118.61578226089478, 119.81217837333679, 121.00857448577881, 122.20113754272461, 123.39370059967041, 124.58801555633545, 125.78233051300049, 126.98315024375916, 128.18396997451782, 129.3811912536621, 130.5784125328064, 131.7749924659729, 132.9715723991394, 134.16538906097412, 135.35920572280884, 136.54899263381958, 137.73877954483032, 138.94151163101196, 140.1442437171936, 141.32968282699585, 142.5151219367981, 143.70558500289917, 144.89604806900024, 146.09105563163757, 147.2860631942749, 148.47177290916443, 149.65748262405396, 150.84513592720032, 152.03278923034668, 153.10561323165894, 154.1784372329712, 155.240581035614, 156.30272483825684, 157.3592174053192, 158.4157099723816, 159.43681597709656, 160.45792198181152, 161.48271298408508, 162.50750398635864, 163.53958106040955, 164.57165813446045, 165.5959222316742, 166.62018632888794, 167.64871621131897, 168.67724609375, 169.7024483680725, 170.72765064239502, 171.758159160614, 172.788667678833, 173.82192254066467, 174.85517740249634, 175.881178855896, 176.90718030929565, 177.93877363204956, 178.97036695480347, 180.00265908241272, 181.03495121002197, 182.06470584869385, 183.09446048736572, 184.13572216033936, 185.176983833313, 186.20624423027039, 187.23550462722778, 188.2703151702881, 189.3051257133484, 190.33474159240723, 191.36435747146606, 192.39018297195435, 193.41600847244263, 194.4424033164978, 195.46879816055298, 196.50396370887756, 197.53912925720215, 198.56634855270386, 199.59356784820557, 200.62445282936096, 201.65533781051636, 202.68672037124634, 203.71810293197632, 204.74539947509766, 205.772696018219, 206.8064422607422, 207.84018850326538, 208.86829137802124, 209.8963942527771, 210.92263984680176, 211.94888544082642, 212.98491740226746, 214.0209493637085, 215.04957389831543, 216.07819843292236, 217.11077189445496, 218.14334535598755, 220.22619819641113, 222.30905103683472]
[18.14, 18.14, 22.63, 22.63, 24.8275, 24.8275, 24.08, 24.08, 23.8425, 23.8425, 23.8425, 23.8425, 23.815, 23.815, 24.14, 24.14, 23.9825, 23.9825, 24.8425, 24.8425, 25.1825, 25.1825, 25.5525, 25.5525, 25.65, 25.65, 25.9575, 25.9575, 26.2275, 26.2275, 26.3425, 26.3425, 26.145, 26.145, 25.975, 25.975, 25.9125, 25.9125, 26.1175, 26.1175, 26.455, 26.455, 26.7725, 26.7725, 26.5675, 26.5675, 26.57, 26.57, 26.5925, 26.5925, 26.5225, 26.5225, 26.6975, 26.6975, 26.7925, 26.7925, 26.7275, 26.7275, 26.86, 26.86, 26.575, 26.575, 26.715, 26.715, 26.6275, 26.6275, 26.7575, 26.7575, 26.74, 26.74, 26.3075, 26.3075, 26.17, 26.17, 26.625, 26.625, 26.525, 26.525, 26.255, 26.255, 25.9875, 25.9875, 25.82, 25.82, 26.06, 26.06, 25.95, 25.95, 25.76, 25.76, 25.845, 25.845, 25.6175, 25.6175, 25.8925, 25.8925, 25.775, 25.775, 25.735, 25.735, 25.6425, 25.6425, 25.405, 25.405, 25.1575, 25.1575, 25.2075, 25.2075, 24.93, 24.93, 25.0025, 25.0025, 25.21, 25.21, 24.89, 24.89, 24.9475, 24.9475, 25.0125, 25.0125, 25.195, 25.195, 24.9425, 24.9425, 24.975, 24.975, 24.7275, 24.7275, 24.64, 24.64, 25.0575, 25.0575, 24.8425, 24.8425, 24.6175, 24.6175, 24.84, 24.84, 24.5575, 24.5575, 24.7875, 24.7875, 24.915, 24.915, 25.0925, 25.0925, 24.9575, 24.9575, 24.8975, 24.8975, 24.725, 24.725, 25.055, 25.055, 24.9625, 24.9625, 24.945, 24.945, 24.8375, 24.8375, 24.59, 24.59, 24.69, 24.69, 24.5025, 24.5025, 24.4275, 24.4275, 24.5725, 24.5725, 24.565, 24.565, 24.665, 24.665, 24.7625, 24.7625, 24.885, 24.885, 24.59, 24.59, 24.545, 24.545, 24.7075, 24.7075, 24.9175, 24.9175, 24.7675, 24.7675, 24.4525, 24.4525, 24.57, 24.57, 24.74, 24.74, 24.96, 24.96, 24.94, 24.94, 24.7525, 24.7525, 25.0, 25.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5450
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4800
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8150
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4985
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7705
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6810
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5105
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6335
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7920
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7570
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52622 is out of bounds for axis 0 with size 50000
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5480
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4920
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8285
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5345
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7640
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7120
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.6045
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5885
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7910
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6720
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52738 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5480
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5160
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8160
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5395
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7550
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6930
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.4835
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5535
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7990
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7095
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5565
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4830
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8245
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5085
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7550
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6950
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.4805
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5915
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7800
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6110
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  16.8600
Round 1 global test acc  20.8100
Traceback (most recent call last):
  File "RFL.py", line 126, in <module>
    w_local, loss_local, f_k = local.train(copy.deepcopy(net_glob).to(args.device), copy.deepcopy(f_G).to(args.device),
  File "/home/ChenSM/code/FL_HLS/util/local_training.py", line 257, in train
    for batch_idx, (images, labels, idxs) in enumerate(self.ldr_train_tmp):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/util/local_training.py", line 48, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53443 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5470
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4885
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8195
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5440
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7350
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7125
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5935
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5825
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7875
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6470
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52309 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5490
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5085
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8275
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4685
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7665
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7180
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.4440
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6370
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7975
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6975
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8590
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1960
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.1590
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5770
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8000
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.3435
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6865
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7755
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1680
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6055
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5735
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8545
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3990
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.4205
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7190
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4550
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.293, Test loss: 6.339, Test accuracy: 22.28
Final Round, Global train loss: 0.293, Global test loss: 1.978, Global test accuracy: 29.01
Average accuracy final 10 rounds: 22.160750000000004 

Average global accuracy final 10 rounds: 28.644249999999996 

3826.2809324264526
[1.4464459419250488, 2.8928918838500977, 4.085153818130493, 5.277415752410889, 6.475033760070801, 7.672651767730713, 8.87558126449585, 10.078510761260986, 11.282162189483643, 12.485813617706299, 13.68700647354126, 14.88819932937622, 16.084317922592163, 17.280436515808105, 18.466741800308228, 19.65304708480835, 20.853047132492065, 22.05304718017578, 23.234498500823975, 24.415949821472168, 25.606696844100952, 26.797443866729736, 28.000561714172363, 29.20367956161499, 30.404908895492554, 31.606138229370117, 32.80012655258179, 33.99411487579346, 35.187758684158325, 36.38140249252319, 37.574437856674194, 38.767473220825195, 39.9666268825531, 41.165780544281006, 42.19328284263611, 43.22078514099121, 44.251548528671265, 45.28231191635132, 46.31514286994934, 47.34797382354736, 48.376192569732666, 49.40441131591797, 50.43646717071533, 51.468523025512695, 52.50281119346619, 53.53709936141968, 54.56498312950134, 55.59286689758301, 56.621416330337524, 57.64996576309204, 58.68178033828735, 59.713594913482666, 60.744229316711426, 61.774863719940186, 62.80884313583374, 63.842822551727295, 64.87239980697632, 65.90197706222534, 66.92852401733398, 67.95507097244263, 68.98311161994934, 70.01115226745605, 71.04491448402405, 72.07867670059204, 73.10623240470886, 74.13378810882568, 75.16181778907776, 76.18984746932983, 77.21890330314636, 78.24795913696289, 79.27304720878601, 80.29813528060913, 81.32480931282043, 82.35148334503174, 83.37952995300293, 84.40757656097412, 85.43534445762634, 86.46311235427856, 87.49085521697998, 88.5185980796814, 89.54364061355591, 90.56868314743042, 91.59391570091248, 92.61914825439453, 93.6488573551178, 94.67856645584106, 95.71154522895813, 96.7445240020752, 97.77138066291809, 98.79823732376099, 99.82846307754517, 100.85868883132935, 101.88886213302612, 102.9190354347229, 103.94536256790161, 104.97168970108032, 106.16253209114075, 107.35337448120117, 108.5448067188263, 109.73623895645142, 110.9290337562561, 112.12182855606079, 113.31933331489563, 114.51683807373047, 115.70842933654785, 116.90002059936523, 118.08940601348877, 119.2787914276123, 120.46606874465942, 121.65334606170654, 122.84237337112427, 124.03140068054199, 125.2289686203003, 126.4265365600586, 127.6153290271759, 128.8041214942932, 129.99852323532104, 131.19292497634888, 132.38421297073364, 133.5755009651184, 134.76689624786377, 135.95829153060913, 137.1516716480255, 138.3450517654419, 139.53408694267273, 140.72312211990356, 141.91483688354492, 143.10655164718628, 144.30113863945007, 145.49572563171387, 146.68932342529297, 147.88292121887207, 149.07564067840576, 150.26836013793945, 151.4722559452057, 152.67615175247192, 153.87274265289307, 155.0693335533142, 156.26287746429443, 157.45642137527466, 158.65050220489502, 159.84458303451538, 161.0416977405548, 162.23881244659424, 163.43494510650635, 164.63107776641846, 165.82725071907043, 167.0234236717224, 168.21656012535095, 169.4096965789795, 170.60240983963013, 171.79512310028076, 172.98788833618164, 174.18065357208252, 175.3699221611023, 176.55919075012207, 177.75316071510315, 178.94713068008423, 180.14244389533997, 181.3377571105957, 182.53581428527832, 183.73387145996094, 184.92892980575562, 186.1239881515503, 187.31460285186768, 188.50521755218506, 189.69925594329834, 190.89329433441162, 192.09009718894958, 193.28690004348755, 194.47978115081787, 195.6726622581482, 196.86415243148804, 198.05564260482788, 199.24739503860474, 200.4391474723816, 201.6341414451599, 202.82913541793823, 204.02361679077148, 205.21809816360474, 206.4153027534485, 207.61250734329224, 208.8046576976776, 209.996808052063, 211.27645564079285, 212.5561032295227, 213.80921173095703, 215.06232023239136, 216.31655931472778, 217.5707983970642, 218.83748865127563, 220.10417890548706, 221.36965012550354, 222.63512134552002, 223.90025997161865, 225.16539859771729, 226.44875693321228, 227.73211526870728, 229.00446343421936, 230.27681159973145, 232.91477465629578, 235.5527377128601]
[19.5175, 19.5175, 20.045, 20.045, 21.92, 21.92, 21.2, 21.2, 22.505, 22.505, 22.435, 22.435, 22.7225, 22.7225, 23.6025, 23.6025, 23.3775, 23.3775, 23.075, 23.075, 23.535, 23.535, 23.4325, 23.4325, 23.675, 23.675, 23.525, 23.525, 23.595, 23.595, 24.41, 24.41, 24.5175, 24.5175, 24.4475, 24.4475, 24.5275, 24.5275, 24.7875, 24.7875, 24.4825, 24.4825, 24.5275, 24.5275, 24.4925, 24.4925, 24.71, 24.71, 24.51, 24.51, 24.335, 24.335, 24.3825, 24.3825, 24.4925, 24.4925, 24.595, 24.595, 24.1975, 24.1975, 23.905, 23.905, 23.7875, 23.7875, 23.8675, 23.8675, 24.1375, 24.1375, 24.14, 24.14, 24.095, 24.095, 24.0225, 24.0225, 23.74, 23.74, 23.6975, 23.6975, 23.295, 23.295, 23.23, 23.23, 23.3, 23.3, 23.0625, 23.0625, 22.885, 22.885, 23.205, 23.205, 23.03, 23.03, 23.035, 23.035, 23.125, 23.125, 23.1975, 23.1975, 23.1375, 23.1375, 22.705, 22.705, 22.96, 22.96, 22.7, 22.7, 22.2125, 22.2125, 22.3075, 22.3075, 22.18, 22.18, 21.835, 21.835, 22.145, 22.145, 21.8725, 21.8725, 21.745, 21.745, 21.82, 21.82, 22.0275, 22.0275, 22.3925, 22.3925, 22.015, 22.015, 21.8775, 21.8775, 22.215, 22.215, 22.035, 22.035, 22.05, 22.05, 22.2525, 22.2525, 22.48, 22.48, 22.4475, 22.4475, 22.2375, 22.2375, 21.85, 21.85, 21.7275, 21.7275, 21.6775, 21.6775, 21.8275, 21.8275, 21.8375, 21.8375, 21.8525, 21.8525, 21.8275, 21.8275, 21.78, 21.78, 21.795, 21.795, 21.9725, 21.9725, 21.845, 21.845, 21.8175, 21.8175, 21.3875, 21.3875, 21.795, 21.795, 22.0, 22.0, 21.9275, 21.9275, 21.8875, 21.8875, 22.0925, 22.0925, 22.115, 22.115, 22.3325, 22.3325, 22.3125, 22.3125, 22.385, 22.385, 22.3675, 22.3675, 22.1275, 22.1275, 21.98, 21.98, 21.8775, 21.8775, 22.165, 22.165, 21.945, 21.945, 22.2825, 22.2825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8535
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2555
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0675
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5710
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7960
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1070
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6730
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7625
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2125
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5995
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5195
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8630
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1830
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3335
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7160
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.5665
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.625, Test loss: 2.586, Test accuracy: 44.61
Final Round, Global train loss: 0.625, Global test loss: 1.469, Global test accuracy: 57.38
Average accuracy final 10 rounds: 45.870250000000006 

Average global accuracy final 10 rounds: 56.226 

3751.4729454517365
[1.5056233406066895, 3.011246681213379, 4.2501280307769775, 5.489009380340576, 6.735947847366333, 7.98288631439209, 9.227727890014648, 10.472569465637207, 11.714467763900757, 12.956366062164307, 14.20319414138794, 15.450022220611572, 16.694219827651978, 17.938417434692383, 19.1867356300354, 20.435053825378418, 21.679521322250366, 22.923988819122314, 24.17101001739502, 25.418031215667725, 26.670897006988525, 27.923762798309326, 29.166789293289185, 30.409815788269043, 31.65851378440857, 32.907211780548096, 34.15625524520874, 35.405298709869385, 36.65734934806824, 37.90939998626709, 39.16477823257446, 40.420156478881836, 41.67955684661865, 42.93895721435547, 44.19328832626343, 45.44761943817139, 46.693562269210815, 47.939505100250244, 49.193105697631836, 50.44670629501343, 51.70250082015991, 52.9582953453064, 54.205947399139404, 55.45359945297241, 56.706878423690796, 57.96015739440918, 59.20632553100586, 60.45249366760254, 61.69247817993164, 62.93246269226074, 64.1745834350586, 65.41670417785645, 66.66302752494812, 67.9093508720398, 69.15384078025818, 70.39833068847656, 71.64371705055237, 72.88910341262817, 74.1360297203064, 75.38295602798462, 76.62615752220154, 77.86935901641846, 79.11500239372253, 80.36064577102661, 81.60809803009033, 82.85555028915405, 84.0997896194458, 85.34402894973755, 86.59444665908813, 87.84486436843872, 89.09158134460449, 90.33829832077026, 91.58298063278198, 92.8276629447937, 94.07740259170532, 95.32714223861694, 96.57841277122498, 97.82968330383301, 99.07554793357849, 100.32141256332397, 101.57283139228821, 102.82425022125244, 104.06809163093567, 105.3119330406189, 106.56169009208679, 107.81144714355469, 109.06290078163147, 110.31435441970825, 111.56284165382385, 112.81132888793945, 114.06293535232544, 115.31454181671143, 116.5640082359314, 117.81347465515137, 119.05415201187134, 120.29482936859131, 121.53985357284546, 122.78487777709961, 124.02232122421265, 125.25976467132568, 126.49487161636353, 127.72997856140137, 128.97332215309143, 130.2166657447815, 131.4582531452179, 132.6998405456543, 133.95444297790527, 135.20904541015625, 136.45525979995728, 137.7014741897583, 138.95061230659485, 140.1997504234314, 141.44365072250366, 142.68755102157593, 143.93092322349548, 145.17429542541504, 146.4240484237671, 147.67380142211914, 148.92469930648804, 150.17559719085693, 151.42116332054138, 152.66672945022583, 153.91189336776733, 155.15705728530884, 156.40680766105652, 157.6565580368042, 158.90761804580688, 160.15867805480957, 161.40285968780518, 162.64704132080078, 163.89807677268982, 165.14911222457886, 166.39480233192444, 167.64049243927002, 168.88833475112915, 170.13617706298828, 171.39133405685425, 172.64649105072021, 173.89257955551147, 175.13866806030273, 176.3867952823639, 177.63492250442505, 178.8875710964203, 180.14021968841553, 181.38764762878418, 182.63507556915283, 183.88405537605286, 185.13303518295288, 186.38077640533447, 187.62851762771606, 188.86861753463745, 190.10871744155884, 191.35888767242432, 192.6090579032898, 193.8491563796997, 195.08925485610962, 196.32939314842224, 197.56953144073486, 198.8339238166809, 200.09831619262695, 201.340656042099, 202.58299589157104, 203.8247106075287, 205.06642532348633, 206.31218886375427, 207.55795240402222, 208.80708861351013, 210.05622482299805, 211.3053011894226, 212.55437755584717, 213.80289959907532, 215.05142164230347, 216.29005551338196, 217.52868938446045, 218.77653074264526, 220.02437210083008, 221.26443934440613, 222.50450658798218, 223.76151418685913, 225.01852178573608, 226.26521015167236, 227.51189851760864, 228.75262260437012, 229.9933466911316, 231.24717783927917, 232.50100898742676, 233.7472071647644, 234.99340534210205, 236.2454538345337, 237.49750232696533, 238.74273490905762, 239.9879674911499, 241.23038935661316, 242.47281122207642, 243.72796440124512, 244.98311758041382, 246.24123811721802, 247.49935865402222, 248.74109029769897, 249.98282194137573, 252.47224760055542, 254.9616732597351]
[20.575, 20.575, 25.43, 25.43, 27.0175, 27.0175, 30.9625, 30.9625, 31.45, 31.45, 32.9325, 32.9325, 33.21, 33.21, 34.3975, 34.3975, 35.2475, 35.2475, 35.0975, 35.0975, 35.68, 35.68, 36.6075, 36.6075, 37.9425, 37.9425, 38.825, 38.825, 39.2575, 39.2575, 39.97, 39.97, 39.9825, 39.9825, 40.7675, 40.7675, 41.0025, 41.0025, 41.51, 41.51, 41.5425, 41.5425, 41.8625, 41.8625, 42.8725, 42.8725, 42.51, 42.51, 43.1475, 43.1475, 43.31, 43.31, 43.2325, 43.2325, 43.165, 43.165, 43.115, 43.115, 43.6, 43.6, 43.7725, 43.7725, 44.29, 44.29, 44.71, 44.71, 44.9825, 44.9825, 45.415, 45.415, 45.205, 45.205, 45.1675, 45.1675, 45.1875, 45.1875, 45.36, 45.36, 45.285, 45.285, 45.2, 45.2, 45.38, 45.38, 45.0825, 45.0825, 45.3825, 45.3825, 45.4175, 45.4175, 45.28, 45.28, 45.58, 45.58, 45.2925, 45.2925, 45.5825, 45.5825, 45.155, 45.155, 45.0975, 45.0975, 45.2425, 45.2425, 45.295, 45.295, 45.295, 45.295, 45.375, 45.375, 45.3525, 45.3525, 45.6425, 45.6425, 45.7225, 45.7225, 45.845, 45.845, 45.9275, 45.9275, 46.0925, 46.0925, 45.6875, 45.6875, 45.52, 45.52, 45.5775, 45.5775, 45.5175, 45.5175, 45.7325, 45.7325, 46.0675, 46.0675, 45.63, 45.63, 45.675, 45.675, 45.74, 45.74, 45.36, 45.36, 45.4375, 45.4375, 45.1525, 45.1525, 45.61, 45.61, 45.835, 45.835, 45.9725, 45.9725, 45.375, 45.375, 45.2425, 45.2425, 45.2525, 45.2525, 45.1775, 45.1775, 45.6575, 45.6575, 45.5625, 45.5625, 45.7425, 45.7425, 45.3275, 45.3275, 45.2375, 45.2375, 45.05, 45.05, 45.3725, 45.3725, 45.52, 45.52, 45.495, 45.495, 45.59, 45.59, 45.775, 45.775, 45.875, 45.875, 45.6325, 45.6325, 45.8125, 45.8125, 45.8375, 45.8375, 45.57, 45.57, 45.925, 45.925, 46.0025, 46.0025, 46.095, 46.095, 46.1775, 46.1775, 44.6125, 44.6125]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8505
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2015
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0655
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5725
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7840
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1760
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7210
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7730
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.4495
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5845
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4690
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8715
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0875
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3400
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7320
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4405
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.767, Test loss: 2.417, Test accuracy: 42.45
Final Round, Global train loss: 0.767, Global test loss: 1.544, Global test accuracy: 53.17
Average accuracy final 10 rounds: 42.81425 

Average global accuracy final 10 rounds: 54.4665 

3809.850860595703
[1.5070743560791016, 3.014148712158203, 4.283578157424927, 5.55300760269165, 6.832696914672852, 8.112386226654053, 9.39608883857727, 10.679791450500488, 11.960596084594727, 13.241400718688965, 14.502647876739502, 15.763895034790039, 17.047094583511353, 18.330294132232666, 19.618821144104004, 20.907348155975342, 22.18991780281067, 23.472487449645996, 24.75062084197998, 26.028754234313965, 27.305274486541748, 28.58179473876953, 29.847382307052612, 31.112969875335693, 32.391521692276, 33.67007350921631, 34.94673442840576, 36.223395347595215, 37.5049262046814, 38.78645706176758, 40.060418128967285, 41.33437919616699, 42.60302948951721, 43.87167978286743, 45.13487672805786, 46.39807367324829, 47.68002724647522, 48.96198081970215, 50.24472260475159, 51.527464389801025, 52.79211163520813, 54.056758880615234, 55.275107622146606, 56.49345636367798, 57.71123814582825, 58.929019927978516, 60.160518646240234, 61.39201736450195, 62.61804723739624, 63.84407711029053, 65.07578873634338, 66.30750036239624, 67.54319405555725, 68.77888774871826, 70.0133113861084, 71.24773502349854, 72.51809978485107, 73.78846454620361, 75.06347370147705, 76.33848285675049, 77.61337423324585, 78.88826560974121, 80.18038010597229, 81.47249460220337, 82.74302744865417, 84.01356029510498, 85.30008268356323, 86.58660507202148, 87.86831045150757, 89.15001583099365, 90.43078064918518, 91.71154546737671, 92.99212884902954, 94.27271223068237, 95.54914045333862, 96.82556867599487, 98.1157398223877, 99.40591096878052, 100.69376182556152, 101.98161268234253, 103.2672872543335, 104.55296182632446, 105.7917251586914, 107.03048849105835, 108.26306176185608, 109.49563503265381, 110.73165559768677, 111.96767616271973, 113.23954629898071, 114.5114164352417, 115.79050517082214, 117.06959390640259, 118.30321478843689, 119.53683567047119, 120.80789875984192, 122.07896184921265, 123.30696058273315, 124.53495931625366, 125.76414442062378, 126.9933295249939, 128.26518654823303, 129.53704357147217, 130.7910716533661, 132.04509973526, 133.3200261592865, 134.594952583313, 135.6984429359436, 136.80193328857422, 137.9038107395172, 139.0056881904602, 140.11052823066711, 141.21536827087402, 142.32974791526794, 143.44412755966187, 144.54798865318298, 145.6518497467041, 146.75720524787903, 147.86256074905396, 148.97369694709778, 150.0848331451416, 151.19162130355835, 152.2984094619751, 153.40809035301208, 154.51777124404907, 155.6215205192566, 156.7252697944641, 157.8328094482422, 158.94034910202026, 160.04967522621155, 161.15900135040283, 162.26269149780273, 163.36638164520264, 164.47841691970825, 165.59045219421387, 166.69462656974792, 167.79880094528198, 168.90400290489197, 170.00920486450195, 171.13073420524597, 172.25226354599, 173.36391425132751, 174.47556495666504, 175.5892791748047, 176.70299339294434, 177.81810760498047, 178.9332218170166, 180.04486417770386, 181.1565065383911, 182.26931262016296, 183.38211870193481, 184.49903178215027, 185.61594486236572, 186.73481345176697, 187.8536820411682, 188.9634234905243, 190.07316493988037, 191.17935132980347, 192.28553771972656, 193.39895915985107, 194.5123805999756, 195.6292130947113, 196.74604558944702, 197.85779309272766, 198.9695405960083, 200.0767056941986, 201.18387079238892, 202.30864787101746, 203.433424949646, 204.54383730888367, 205.65424966812134, 206.76628279685974, 207.87831592559814, 208.99396800994873, 210.10962009429932, 211.22228503227234, 212.33494997024536, 213.44903707504272, 214.5631241798401, 215.6687092781067, 216.7742943763733, 217.88718962669373, 219.00008487701416, 220.11264181137085, 221.22519874572754, 222.33445858955383, 223.44371843338013, 224.5521228313446, 225.66052722930908, 226.77311182022095, 227.8856964111328, 228.99622178077698, 230.10674715042114, 231.22494196891785, 232.34313678741455, 233.46520519256592, 234.58727359771729, 235.70467400550842, 236.82207441329956, 237.93735933303833, 239.0526442527771, 241.2795853614807, 243.50652647018433]
[17.3825, 17.3825, 21.1325, 21.1325, 23.8, 23.8, 26.57, 26.57, 27.6425, 27.6425, 28.4175, 28.4175, 29.14, 29.14, 30.15, 30.15, 31.62, 31.62, 30.5025, 30.5025, 32.1475, 32.1475, 32.3225, 32.3225, 33.665, 33.665, 34.6625, 34.6625, 35.5825, 35.5825, 36.4375, 36.4375, 37.2025, 37.2025, 37.2725, 37.2725, 38.425, 38.425, 38.6375, 38.6375, 39.2525, 39.2525, 39.76, 39.76, 40.6075, 40.6075, 40.515, 40.515, 40.85, 40.85, 41.38, 41.38, 41.525, 41.525, 41.8525, 41.8525, 42.235, 42.235, 41.7825, 41.7825, 41.6775, 41.6775, 41.29, 41.29, 41.8, 41.8, 41.935, 41.935, 41.9575, 41.9575, 42.3925, 42.3925, 42.6875, 42.6875, 42.685, 42.685, 42.54, 42.54, 42.6925, 42.6925, 42.1025, 42.1025, 42.085, 42.085, 42.44, 42.44, 43.1075, 43.1075, 42.8675, 42.8675, 43.1225, 43.1225, 43.51, 43.51, 43.4525, 43.4525, 43.5175, 43.5175, 43.655, 43.655, 43.5575, 43.5575, 43.31, 43.31, 43.075, 43.075, 43.495, 43.495, 43.9125, 43.9125, 43.7425, 43.7425, 43.575, 43.575, 44.025, 44.025, 43.9, 43.9, 43.5375, 43.5375, 43.2625, 43.2625, 43.335, 43.335, 43.125, 43.125, 43.8675, 43.8675, 43.29, 43.29, 43.25, 43.25, 43.7125, 43.7125, 43.24, 43.24, 43.4125, 43.4125, 43.35, 43.35, 43.25, 43.25, 43.4875, 43.4875, 43.6075, 43.6075, 43.4575, 43.4575, 43.2225, 43.2225, 43.1525, 43.1525, 43.2075, 43.2075, 43.11, 43.11, 43.3525, 43.3525, 44.0, 44.0, 43.385, 43.385, 43.045, 43.045, 42.795, 42.795, 42.855, 42.855, 42.94, 42.94, 42.675, 42.675, 42.65, 42.65, 42.8225, 42.8225, 43.15, 43.15, 43.195, 43.195, 42.83, 42.83, 42.9175, 42.9175, 43.135, 43.135, 42.7975, 42.7975, 42.7275, 42.7275, 43.0125, 43.0125, 42.7075, 42.7075, 42.46, 42.46, 42.665, 42.665, 42.89, 42.89, 42.4525, 42.4525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8590
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2015
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.2195
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6160
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8015
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0710
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6715
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7405
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1595
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6965
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4980
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8680
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.2345
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2260
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7105
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4445
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8605
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1905
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.1430
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6440
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8065
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0930
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6775
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7620
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.3095
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6270
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5975
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8670
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3740
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2680
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7070
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4130
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  29.5400
Round 1 global test acc  34.2300
Round 2 global test acc  38.2600
Round 3 global test acc  42.6600
Round 4 global test acc  45.2400
Round 5 global test acc  46.7000
Round 6 global test acc  47.1800
Round 7 global test acc  51.2700
Round 8 global test acc  49.4200
Round 9 global test acc  52.8900
Round 10 global test acc  48.6200
Round 11 global test acc  50.7300
Round 12 global test acc  49.7800
Round 13 global test acc  54.9900
Round 14 global test acc  49.2900
Round 15 global test acc  54.8500
Round 16 global test acc  56.0900
Round 17 global test acc  53.3100
Round 18 global test acc  56.0800
Round 19 global test acc  55.5600
Round 20 global test acc  54.9500
Round 21 global test acc  55.5300
Round 22 global test acc  56.8200
Round 23 global test acc  54.5600
Round 24 global test acc  56.2300
Round 25 global test acc  57.0200
Round 26 global test acc  58.3700
Round 27 global test acc  59.0400
Round 28 global test acc  56.8100
Round 29 global test acc  59.5500
Round 30 global test acc  58.2900
Round 31 global test acc  57.2200
Round 32 global test acc  57.3400
Round 33 global test acc  59.0700
Round 34 global test acc  59.1900
Round 35 global test acc  60.1600
Round 36 global test acc  58.3100
Round 37 global test acc  57.6000
Round 38 global test acc  57.4300
Round 39 global test acc  59.7400
Round 40 global test acc  60.0400
Round 41 global test acc  60.3400
Round 42 global test acc  60.0000
Round 43 global test acc  57.7800
Round 44 global test acc  55.3100
Round 45 global test acc  61.1700
Round 46 global test acc  57.8900
Round 47 global test acc  58.8100
Round 48 global test acc  60.3400
Round 49 global test acc  58.9600
Round 50 global test acc  60.4900
Round 51 global test acc  60.4000
Round 52 global test acc  61.6900
Round 53 global test acc  58.8800
Round 54 global test acc  62.6000
Round 55 global test acc  60.6700
Round 56 global test acc  59.4800
Round 57 global test acc  61.1200
Round 58 global test acc  60.2800
Round 59 global test acc  63.2200
Round 60 global test acc  60.5800
Round 61 global test acc  61.4100
Round 62 global test acc  60.8500
Round 63 global test acc  61.8800
Round 64 global test acc  63.5100
Round 65 global test acc  59.9600
Round 66 global test acc  63.7500
Round 67 global test acc  59.6600
Round 68 global test acc  62.3100
Round 69 global test acc  61.9800
Round 70 global test acc  62.4500
Round 71 global test acc  61.8000
Round 72 global test acc  60.4400
Round 73 global test acc  60.1200
Round 74 global test acc  62.1200
Round 75 global test acc  61.3300
Round 76 global test acc  60.8100
Round 77 global test acc  59.7200
Round 78 global test acc  60.8000
Round 79 global test acc  60.5600
Round 80 global test acc  62.7000
Round 81 global test acc  62.2600
Round 82 global test acc  61.2800
Round 83 global test acc  60.0900
Round 84 global test acc  60.0400
Round 85 global test acc  58.6900
Round 86 global test acc  58.2000
Round 87 global test acc  57.8400
Round 88 global test acc  57.6500
Round 89 global test acc  57.3200
Round 90 global test acc  56.5100
Round 91 global test acc  56.5400
Round 92 global test acc  56.7100
Round 93 global test acc  56.0700
Round 94 global test acc  55.0900
Round 95 global test acc  55.6500
Round 96 global test acc  55.4100
Round 97 global test acc  55.8500
Round 98 global test acc  55.0300
Round 99 global test acc  55.6000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8625
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1930
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0650
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5760
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7915
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0715
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7275
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7660
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1465
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6550
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5650
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8610
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.2260
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3530
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7115
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.7165
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.542, Test loss: 1.027, Test accuracy: 66.37
Average accuracy final 10 rounds: 65.86949999999999
1896.489592075348
[1.718017578125, 3.102440357208252, 4.490554332733154, 5.894584894180298, 7.265548229217529, 8.647363185882568, 9.975440502166748, 11.277823448181152, 12.598231077194214, 13.90581226348877, 15.217212200164795, 16.59109902381897, 17.98841643333435, 19.334319829940796, 20.66502857208252, 22.05969738960266, 23.44347834587097, 24.832438230514526, 26.210824966430664, 27.59810161590576, 28.98257327079773, 30.37893319129944, 31.75857663154602, 33.15029311180115, 34.543145418167114, 35.92668628692627, 37.3211088180542, 38.716620445251465, 40.09744334220886, 41.472604751586914, 42.867063999176025, 44.24536681175232, 45.633132219314575, 47.03359389305115, 48.41504955291748, 49.806748151779175, 51.18897366523743, 52.58297634124756, 53.97079944610596, 55.35341119766235, 56.75467658042908, 58.13215446472168, 59.50946521759033, 60.90104699134827, 62.260497093200684, 63.63868546485901, 65.03589916229248, 66.4107882976532, 67.79169178009033, 69.16249108314514, 70.53726577758789, 71.90923595428467, 73.29409289360046, 74.66973948478699, 76.0322642326355, 77.42190766334534, 78.79668307304382, 80.17597150802612, 81.56504583358765, 82.94435477256775, 84.32497549057007, 85.71402668952942, 87.06541204452515, 88.44731044769287, 89.82366299629211, 91.18593239784241, 92.56446981430054, 93.96230626106262, 95.31320524215698, 96.69122457504272, 98.07763242721558, 99.43833661079407, 100.81082201004028, 102.19786810874939, 103.55367946624756, 104.9310929775238, 106.31539249420166, 107.69248747825623, 109.0683479309082, 110.44506669044495, 111.82139348983765, 113.19840717315674, 114.57862901687622, 115.95391488075256, 117.32475185394287, 118.72491884231567, 120.11146068572998, 121.48123717308044, 122.87042808532715, 124.26038932800293, 125.63611030578613, 127.02452325820923, 128.4011948108673, 129.78051471710205, 131.15917468070984, 132.54694628715515, 133.92468738555908, 135.31219005584717, 136.7039499282837, 138.07823538780212, 140.163494348526]
[19.715, 26.165, 30.97, 33.005, 35.555, 38.52, 39.4375, 41.7925, 42.86, 43.1525, 45.0275, 46.78, 47.0775, 48.02, 48.49, 50.4, 50.3225, 51.5375, 51.805, 52.2325, 52.86, 52.64, 52.6, 52.2525, 53.6575, 54.8575, 55.9775, 56.4925, 56.31, 56.4425, 56.865, 57.48, 58.0025, 58.4125, 57.68, 58.675, 59.25, 59.025, 59.6275, 59.79, 60.2875, 61.275, 61.28, 61.7525, 61.7925, 62.63, 62.79, 62.6725, 63.005, 62.8375, 63.0975, 63.1575, 62.9325, 63.1225, 63.305, 63.255, 62.94, 63.3425, 64.0425, 64.125, 64.41, 64.4175, 64.535, 64.2725, 63.99, 64.235, 64.0575, 64.24, 64.1075, 64.0025, 64.0875, 64.215, 64.6475, 64.5625, 64.6775, 64.8075, 64.505, 64.775, 64.775, 64.365, 64.935, 65.3975, 65.1625, 65.2825, 65.545, 65.885, 65.99, 65.86, 65.8, 65.51, 65.715, 65.6775, 65.8225, 65.8125, 65.97, 65.725, 65.7675, 66.235, 66.0725, 65.8975, 66.3725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8485
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1955
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.1880
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5760
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7955
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1895
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6915
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7500
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2165
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6175
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5780
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8415
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3795
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2810
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7430
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3505
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8715
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5415
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4665
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7330
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8460
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5570
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8120
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8165
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5490
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7985
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7355
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.9015
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5530
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6115
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8205
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7585
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.379, Test loss: 6.937, Test accuracy: 16.21
Final Round, Global train loss: 0.379, Global test loss: 2.125, Global test accuracy: 21.86
Average accuracy final 10 rounds: 16.202249999999996 

Average global accuracy final 10 rounds: 19.78 

3863.186411857605
[1.501359224319458, 3.002718448638916, 4.2678444385528564, 5.532970428466797, 6.80104923248291, 8.069128036499023, 9.351171731948853, 10.633215427398682, 11.906213283538818, 13.179211139678955, 14.489233493804932, 15.799255847930908, 17.07257103919983, 18.34588623046875, 19.645201683044434, 20.944517135620117, 22.22214388847351, 23.499770641326904, 24.792673349380493, 26.085576057434082, 27.360023498535156, 28.63447093963623, 29.900633811950684, 31.166796684265137, 32.43725252151489, 33.70770835876465, 34.98573708534241, 36.263765811920166, 37.528847217559814, 38.79392862319946, 40.08108925819397, 41.36824989318848, 42.63888454437256, 43.90951919555664, 45.176559925079346, 46.44360065460205, 47.717214584350586, 48.99082851409912, 50.26288342475891, 51.5349383354187, 52.80485439300537, 54.07477045059204, 55.34996318817139, 56.62515592575073, 57.90046310424805, 59.17577028274536, 60.439603328704834, 61.70343637466431, 62.971585273742676, 64.23973417282104, 65.51290082931519, 66.78606748580933, 68.05672907829285, 69.32739067077637, 70.59079194068909, 71.8541932106018, 73.11678552627563, 74.37937784194946, 75.63745975494385, 76.89554166793823, 78.15060496330261, 79.40566825866699, 80.6624550819397, 81.9192419052124, 83.17176055908203, 84.42427921295166, 85.6790099143982, 86.93374061584473, 88.18996715545654, 89.44619369506836, 90.72586607933044, 92.00553846359253, 93.2650294303894, 94.52452039718628, 95.76984310150146, 97.01516580581665, 98.22157144546509, 99.42797708511353, 100.63357830047607, 101.83917951583862, 103.03963208198547, 104.24008464813232, 105.44083881378174, 106.64159297943115, 107.84186840057373, 109.04214382171631, 110.2451331615448, 111.44812250137329, 112.64840316772461, 113.84868383407593, 115.05640959739685, 116.26413536071777, 117.47135019302368, 118.67856502532959, 119.87728500366211, 121.07600498199463, 122.286376953125, 123.49674892425537, 124.70011878013611, 125.90348863601685, 127.10451459884644, 128.30554056167603, 129.51122307777405, 130.71690559387207, 131.92398762702942, 133.13106966018677, 134.33374691009521, 135.53642416000366, 136.74565601348877, 137.95488786697388, 139.15515851974487, 140.35542917251587, 141.56188678741455, 142.76834440231323, 143.97730040550232, 145.1862564086914, 146.40420842170715, 147.6221604347229, 148.8193163871765, 150.01647233963013, 151.232745885849, 152.44901943206787, 153.65776252746582, 154.86650562286377, 156.07118248939514, 157.2758593559265, 158.48380208015442, 159.69174480438232, 160.8910722732544, 162.09039974212646, 163.29542994499207, 164.50046014785767, 165.70417714118958, 166.90789413452148, 168.1086745262146, 169.30945491790771, 170.51805782318115, 171.7266607284546, 172.9327940940857, 174.1389274597168, 175.3439838886261, 176.5490403175354, 177.75605463981628, 178.96306896209717, 180.16934180259705, 181.37561464309692, 182.5827031135559, 183.7897915840149, 184.99969029426575, 186.2095890045166, 187.41315174102783, 188.61671447753906, 189.77497601509094, 190.93323755264282, 191.973623752594, 193.01400995254517, 194.05236387252808, 195.090717792511, 196.12876200675964, 197.1668062210083, 198.2009162902832, 199.2350263595581, 200.2656605243683, 201.29629468917847, 202.32914018630981, 203.36198568344116, 204.39152026176453, 205.4210548400879, 206.45739793777466, 207.49374103546143, 208.52541208267212, 209.5570831298828, 210.59666323661804, 211.63624334335327, 212.6730399131775, 213.7098364830017, 214.74079394340515, 215.7717514038086, 216.8112394809723, 217.850727558136, 218.88250637054443, 219.91428518295288, 220.9578821659088, 222.00147914886475, 223.04390144348145, 224.08632373809814, 225.12631487846375, 226.16630601882935, 227.21167516708374, 228.25704431533813, 229.28989362716675, 230.32274293899536, 231.3675239086151, 232.41230487823486, 233.44840812683105, 234.48451137542725, 235.5234785079956, 236.56244564056396, 237.60877346992493, 238.6551012992859, 240.7410490512848, 242.8269968032837]
[11.2575, 11.2575, 15.475, 15.475, 15.255, 15.255, 16.3425, 16.3425, 16.895, 16.895, 16.8475, 16.8475, 16.375, 16.375, 16.1225, 16.1225, 16.3975, 16.3975, 17.0825, 17.0825, 16.96, 16.96, 17.1375, 17.1375, 17.5725, 17.5725, 17.5925, 17.5925, 17.42, 17.42, 17.645, 17.645, 17.58, 17.58, 17.89, 17.89, 18.0725, 18.0725, 18.135, 18.135, 18.08, 18.08, 18.1525, 18.1525, 18.185, 18.185, 18.105, 18.105, 18.08, 18.08, 17.875, 17.875, 17.66, 17.66, 17.745, 17.745, 17.86, 17.86, 17.79, 17.79, 18.015, 18.015, 17.8325, 17.8325, 17.555, 17.555, 17.4675, 17.4675, 17.515, 17.515, 17.885, 17.885, 17.7925, 17.7925, 17.565, 17.565, 17.345, 17.345, 17.3925, 17.3925, 17.195, 17.195, 17.3775, 17.3775, 17.3075, 17.3075, 16.975, 16.975, 16.6625, 16.6625, 16.8875, 16.8875, 17.1775, 17.1775, 16.8875, 16.8875, 16.8375, 16.8375, 16.6625, 16.6625, 16.575, 16.575, 16.6225, 16.6225, 16.52, 16.52, 16.945, 16.945, 16.8825, 16.8825, 17.105, 17.105, 16.95, 16.95, 16.585, 16.585, 16.5275, 16.5275, 16.5575, 16.5575, 16.75, 16.75, 16.56, 16.56, 16.4875, 16.4875, 16.255, 16.255, 16.275, 16.275, 16.165, 16.165, 16.335, 16.335, 16.3825, 16.3825, 16.4175, 16.4175, 16.2825, 16.2825, 16.25, 16.25, 16.3875, 16.3875, 16.1675, 16.1675, 16.275, 16.275, 16.1375, 16.1375, 16.1925, 16.1925, 16.3025, 16.3025, 16.3875, 16.3875, 16.1975, 16.1975, 16.3, 16.3, 16.2975, 16.2975, 16.2725, 16.2725, 15.9175, 15.9175, 16.0575, 16.0575, 16.1975, 16.1975, 16.25, 16.25, 16.1975, 16.1975, 16.4475, 16.4475, 16.4675, 16.4675, 16.395, 16.395, 16.1525, 16.1525, 16.185, 16.185, 16.18, 16.18, 16.2475, 16.2475, 16.0275, 16.0275, 16.2825, 16.2825, 16.1125, 16.1125, 16.215, 16.215, 16.2075, 16.2075, 16.4125, 16.4125, 16.2075, 16.2075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8790
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5425
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5120
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7345
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8485
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5875
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8025
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8465
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.6100
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7445
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7110
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8845
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5165
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6485
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7980
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6765
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.755, Test loss: 3.060, Test accuracy: 36.22
Final Round, Global train loss: 0.755, Global test loss: 1.673, Global test accuracy: 48.31
Average accuracy final 10 rounds: 37.690250000000006 

Average global accuracy final 10 rounds: 47.8865 

3662.0091438293457
[1.4006922245025635, 2.801384449005127, 3.968996047973633, 5.136607646942139, 6.309640407562256, 7.482673168182373, 8.653927326202393, 9.825181484222412, 10.997901678085327, 12.170621871948242, 13.34009337425232, 14.509564876556396, 15.674345970153809, 16.83912706375122, 18.00687575340271, 19.1746244430542, 20.346039533615112, 21.517454624176025, 22.69444179534912, 23.871428966522217, 25.041305541992188, 26.211182117462158, 27.395820140838623, 28.580458164215088, 29.577624559402466, 30.574790954589844, 31.5846107006073, 32.594430446624756, 33.60437750816345, 34.61432456970215, 35.61087465286255, 36.60742473602295, 37.60736131668091, 38.60729789733887, 39.61564540863037, 40.623992919921875, 41.620845794677734, 42.617698669433594, 43.619322776794434, 44.62094688415527, 45.62510347366333, 46.62926006317139, 47.62989163398743, 48.63052320480347, 49.62876582145691, 50.62700843811035, 51.62760090827942, 52.628193378448486, 53.63073801994324, 54.63328266143799, 55.64180636405945, 56.65033006668091, 57.65289664268494, 58.655463218688965, 59.67288947105408, 60.69031572341919, 61.69112491607666, 62.69193410873413, 63.70908832550049, 64.72624254226685, 65.72892880439758, 66.73161506652832, 67.72867774963379, 68.72574043273926, 69.73466777801514, 70.74359512329102, 71.75536823272705, 72.76714134216309, 73.77307486534119, 74.77900838851929, 75.7874481678009, 76.79588794708252, 77.80193138122559, 78.80797481536865, 79.80889058113098, 80.80980634689331, 81.81381750106812, 82.81782865524292, 83.8277473449707, 84.83766603469849, 85.85049939155579, 86.86333274841309, 87.87611699104309, 88.8889012336731, 89.88820481300354, 90.88750839233398, 91.89816331863403, 92.90881824493408, 93.91222405433655, 94.91562986373901, 95.92949891090393, 96.94336795806885, 97.95142149925232, 98.95947504043579, 99.95817255973816, 100.95687007904053, 101.96367526054382, 102.97048044204712, 103.97566056251526, 104.9808406829834, 105.98139023780823, 106.98193979263306, 107.97914505004883, 108.9763503074646, 109.98479199409485, 110.9932336807251, 111.99440622329712, 112.99557876586914, 114.0028076171875, 115.01003646850586, 116.01458430290222, 117.01913213729858, 118.0174548625946, 119.01577758789062, 120.02519249916077, 121.03460741043091, 122.0464129447937, 123.0582184791565, 124.06677174568176, 125.07532501220703, 126.07614874839783, 127.07697248458862, 128.08273124694824, 129.08849000930786, 130.09047389030457, 131.09245777130127, 132.09212493896484, 133.09179210662842, 134.09199023246765, 135.09218835830688, 136.09172701835632, 137.09126567840576, 138.0917489528656, 139.09223222732544, 140.09296202659607, 141.0936918258667, 142.09745073318481, 143.10120964050293, 144.10082960128784, 145.10044956207275, 146.10096549987793, 147.1014814376831, 148.0998513698578, 149.09822130203247, 150.10134625434875, 151.10447120666504, 152.10877656936646, 153.11308193206787, 154.11384320259094, 155.114604473114, 156.11623072624207, 157.11785697937012, 158.11976718902588, 159.12167739868164, 160.11830925941467, 161.1149411201477, 162.11330103874207, 163.11166095733643, 164.1152148246765, 165.1187686920166, 166.11791563034058, 167.11706256866455, 168.11709141731262, 169.1171202659607, 170.11814284324646, 171.11916542053223, 172.1164424419403, 173.1137194633484, 174.11624145507812, 175.11876344680786, 176.12069606781006, 177.12262868881226, 178.12362027168274, 179.12461185455322, 180.12753534317017, 181.1304588317871, 182.1313455104828, 183.13223218917847, 184.12868213653564, 185.12513208389282, 186.12742376327515, 187.12971544265747, 188.13150835037231, 189.13330125808716, 190.13216614723206, 191.13103103637695, 192.13062143325806, 193.13021183013916, 194.12876081466675, 195.12730979919434, 196.12620377540588, 197.12509775161743, 198.12161421775818, 199.11813068389893, 200.12006998062134, 201.12200927734375, 202.12493443489075, 203.12785959243774, 204.12909030914307, 205.1303210258484, 207.4441077709198, 209.7578945159912]
[19.4925, 19.4925, 20.185, 20.185, 24.3175, 24.3175, 25.6975, 25.6975, 27.2925, 27.2925, 28.2975, 28.2975, 30.59, 30.59, 31.0275, 31.0275, 31.7775, 31.7775, 31.9575, 31.9575, 32.3125, 32.3125, 32.855, 32.855, 33.875, 33.875, 34.59, 34.59, 34.095, 34.095, 34.775, 34.775, 35.1975, 35.1975, 35.2225, 35.2225, 35.5575, 35.5575, 37.2225, 37.2225, 37.76, 37.76, 38.265, 38.265, 38.3925, 38.3925, 38.59, 38.59, 38.675, 38.675, 39.035, 39.035, 38.8475, 38.8475, 38.9025, 38.9025, 38.64, 38.64, 38.8975, 38.8975, 38.63, 38.63, 39.0125, 39.0125, 38.465, 38.465, 38.7375, 38.7375, 38.605, 38.605, 38.1475, 38.1475, 38.225, 38.225, 38.4125, 38.4125, 39.21, 39.21, 38.95, 38.95, 38.4775, 38.4775, 38.7075, 38.7075, 38.7475, 38.7475, 39.0725, 39.0725, 38.93, 38.93, 38.8325, 38.8325, 38.6725, 38.6725, 38.685, 38.685, 38.3225, 38.3225, 38.2225, 38.2225, 38.165, 38.165, 38.12, 38.12, 38.16, 38.16, 37.84, 37.84, 37.82, 37.82, 37.95, 37.95, 38.0975, 38.0975, 37.775, 37.775, 37.89, 37.89, 37.6225, 37.6225, 37.7575, 37.7575, 38.04, 38.04, 37.9925, 37.9925, 38.175, 38.175, 38.085, 38.085, 38.2825, 38.2825, 38.1875, 38.1875, 38.2375, 38.2375, 38.185, 38.185, 38.02, 38.02, 38.135, 38.135, 38.475, 38.475, 38.23, 38.23, 37.86, 37.86, 37.7175, 37.7175, 37.8325, 37.8325, 37.85, 37.85, 37.815, 37.815, 37.5275, 37.5275, 37.315, 37.315, 37.44, 37.44, 37.975, 37.975, 37.7075, 37.7075, 37.53, 37.53, 37.7175, 37.7175, 37.755, 37.755, 37.6525, 37.6525, 37.2875, 37.2875, 37.4325, 37.4325, 37.845, 37.845, 37.9, 37.9, 37.8025, 37.8025, 37.38, 37.38, 37.7575, 37.7575, 37.615, 37.615, 37.6, 37.6, 37.4875, 37.4875, 37.8, 37.8, 37.78, 37.78, 37.78, 37.78, 36.22, 36.22]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8770
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5745
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4640
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7430
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8555
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4920
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7905
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8230
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5420
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7890
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7445
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8720
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.7000
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6655
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8345
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7495
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.857, Test loss: 2.528, Test accuracy: 39.55
Final Round, Global train loss: 0.857, Global test loss: 1.602, Global test accuracy: 50.48
Average accuracy final 10 rounds: 39.657500000000006 

Average global accuracy final 10 rounds: 50.843 

3783.784375667572
[1.505894422531128, 3.011788845062256, 4.268938064575195, 5.526087284088135, 6.737793445587158, 7.949499607086182, 9.168991088867188, 10.388482570648193, 11.643943548202515, 12.899404525756836, 14.156278610229492, 15.413152694702148, 16.678386211395264, 17.94361972808838, 19.20196843147278, 20.460317134857178, 21.717904329299927, 22.975491523742676, 24.23509192466736, 25.49469232559204, 26.752084732055664, 28.009477138519287, 29.269852876663208, 30.53022861480713, 31.785342693328857, 33.040456771850586, 34.29375433921814, 35.54705190658569, 36.80937695503235, 38.071702003479004, 39.32838177680969, 40.58506155014038, 41.84329676628113, 43.101531982421875, 44.3609733581543, 45.62041473388672, 46.87265348434448, 48.124892234802246, 49.381630659103394, 50.63836908340454, 51.90086817741394, 53.16336727142334, 54.42207980155945, 55.68079233169556, 56.939688205718994, 58.19858407974243, 59.445554971694946, 60.69252586364746, 61.89466619491577, 63.09680652618408, 64.30736351013184, 65.51792049407959, 66.76607131958008, 68.01422214508057, 69.26276850700378, 70.511314868927, 71.7623348236084, 73.0133547782898, 74.26314640045166, 75.51293802261353, 76.77440237998962, 78.03586673736572, 79.28692984580994, 80.53799295425415, 81.77916526794434, 83.02033758163452, 84.27045059204102, 85.52056360244751, 86.76548910140991, 88.01041460037231, 89.2740535736084, 90.53769254684448, 91.79495167732239, 93.0522108078003, 94.3173279762268, 95.58244514465332, 96.84971737861633, 98.11698961257935, 99.37817072868347, 100.6393518447876, 101.90910363197327, 103.17885541915894, 104.44357872009277, 105.70830202102661, 106.97912573814392, 108.24994945526123, 109.51856231689453, 110.78717517852783, 112.0355122089386, 113.28384923934937, 114.53627490997314, 115.78870058059692, 117.0406904220581, 118.29268026351929, 119.54114031791687, 120.78960037231445, 122.04093861579895, 123.29227685928345, 124.5431342124939, 125.79399156570435, 127.04489231109619, 128.29579305648804, 129.55069231987, 130.80559158325195, 132.04835987091064, 133.29112815856934, 134.5494840145111, 135.80783987045288, 137.06699204444885, 138.32614421844482, 139.57592630386353, 140.82570838928223, 142.07757806777954, 143.32944774627686, 144.4231424331665, 145.51683712005615, 146.61392784118652, 147.7110185623169, 148.80850100517273, 149.90598344802856, 151.00466966629028, 152.103355884552, 153.21496200561523, 154.32656812667847, 155.41710543632507, 156.50764274597168, 157.60494947433472, 158.70225620269775, 159.79907083511353, 160.8958854675293, 162.0029981136322, 163.1101107597351, 164.2076277732849, 165.30514478683472, 166.4033362865448, 167.50152778625488, 168.59772276878357, 169.69391775131226, 170.7915050983429, 171.88909244537354, 172.98825645446777, 174.087420463562, 175.1842885017395, 176.281156539917, 177.37840175628662, 178.47564697265625, 179.57129216194153, 180.6669373512268, 181.76819610595703, 182.86945486068726, 183.97413992881775, 185.07882499694824, 186.1839325428009, 187.28904008865356, 188.39532589912415, 189.50161170959473, 190.60872101783752, 191.71583032608032, 192.82539057731628, 193.93495082855225, 195.04425930976868, 196.1535677909851, 197.25753378868103, 198.36149978637695, 199.46794509887695, 200.57439041137695, 201.67874908447266, 202.78310775756836, 203.89002180099487, 204.9969358444214, 206.09954643249512, 207.20215702056885, 208.30202889442444, 209.40190076828003, 210.50355648994446, 211.6052122116089, 212.70397305488586, 213.80273389816284, 214.90012621879578, 215.9975185394287, 217.09707760810852, 218.19663667678833, 219.30485010147095, 220.41306352615356, 221.5180835723877, 222.62310361862183, 223.725736618042, 224.82836961746216, 225.93496918678284, 227.04156875610352, 228.14577221870422, 229.24997568130493, 230.35166478157043, 231.45335388183594, 232.55627965927124, 233.65920543670654, 234.76493549346924, 235.87066555023193, 236.974018573761, 238.07737159729004, 240.28134036064148, 242.48530912399292]
[17.29, 17.29, 20.0775, 20.0775, 22.435, 22.435, 25.765, 25.765, 26.2625, 26.2625, 27.9275, 27.9275, 30.0925, 30.0925, 30.63, 30.63, 31.6025, 31.6025, 32.0525, 32.0525, 33.1925, 33.1925, 34.555, 34.555, 35.175, 35.175, 35.225, 35.225, 35.9175, 35.9175, 36.3725, 36.3725, 36.725, 36.725, 37.0125, 37.0125, 37.475, 37.475, 38.2, 38.2, 38.65, 38.65, 39.5475, 39.5475, 39.4825, 39.4825, 39.9725, 39.9725, 39.415, 39.415, 39.8825, 39.8825, 39.8625, 39.8625, 39.9225, 39.9225, 39.65, 39.65, 39.38, 39.38, 39.585, 39.585, 39.8975, 39.8975, 40.2875, 40.2875, 39.91, 39.91, 39.87, 39.87, 40.0975, 40.0975, 40.2925, 40.2925, 40.175, 40.175, 40.545, 40.545, 40.8675, 40.8675, 40.965, 40.965, 41.17, 41.17, 41.1975, 41.1975, 41.5075, 41.5075, 41.3925, 41.3925, 41.4375, 41.4375, 41.1525, 41.1525, 40.595, 40.595, 40.5925, 40.5925, 40.71, 40.71, 40.8075, 40.8075, 40.42, 40.42, 40.41, 40.41, 40.2425, 40.2425, 40.65, 40.65, 40.815, 40.815, 41.005, 41.005, 40.42, 40.42, 40.4475, 40.4475, 40.4875, 40.4875, 40.6625, 40.6625, 40.7075, 40.7075, 40.6, 40.6, 40.7575, 40.7575, 40.6525, 40.6525, 40.8325, 40.8325, 40.57, 40.57, 40.5675, 40.5675, 40.8825, 40.8825, 40.6825, 40.6825, 40.4225, 40.4225, 40.36, 40.36, 40.2475, 40.2475, 39.795, 39.795, 39.6875, 39.6875, 39.65, 39.65, 39.7625, 39.7625, 39.93, 39.93, 39.97, 39.97, 40.0575, 40.0575, 39.7625, 39.7625, 39.9775, 39.9775, 39.8275, 39.8275, 40.3, 40.3, 40.45, 40.45, 40.21, 40.21, 40.29, 40.29, 40.17, 40.17, 40.1425, 40.1425, 40.2125, 40.2125, 40.2375, 40.2375, 40.075, 40.075, 39.515, 39.515, 39.75, 39.75, 39.8225, 39.8225, 39.71, 39.71, 39.3925, 39.3925, 39.125, 39.125, 39.5525, 39.5525, 39.395, 39.395, 39.545, 39.545]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8735
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5335
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4875
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7390
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8480
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.6455
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8035
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8050
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5410
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7790
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7120
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8860
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.6400
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6540
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7855
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7585
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8765
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5635
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4885
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7275
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8375
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5260
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7900
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8170
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5395
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.8000
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7170
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8930
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5765
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6210
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7840
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6555
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  21.1600
Round 1 global test acc  23.9300
Round 2 global test acc  30.3500
Round 3 global test acc  37.6400
Round 4 global test acc  35.1100
Round 5 global test acc  35.3900
Round 6 global test acc  40.0900
Round 7 global test acc  42.5200
Round 8 global test acc  42.5200
Round 9 global test acc  44.8800
Round 10 global test acc  44.4500
Round 11 global test acc  37.6300
Round 12 global test acc  41.5500
Round 13 global test acc  44.4800
Round 14 global test acc  42.9700
Round 15 global test acc  41.3100
Round 16 global test acc  46.8600
Round 17 global test acc  44.9200
Round 18 global test acc  45.1300
Round 19 global test acc  47.1600
Round 20 global test acc  49.0600
Round 21 global test acc  49.7200
Round 22 global test acc  46.2700
Round 23 global test acc  47.8100
Round 24 global test acc  50.5500
Round 25 global test acc  47.1900
Round 26 global test acc  49.2700
Round 27 global test acc  43.7200
Round 28 global test acc  45.5800
Round 29 global test acc  51.6000
Round 30 global test acc  48.3900
Round 31 global test acc  48.5500
Round 32 global test acc  53.9100
Round 33 global test acc  47.8900
Round 34 global test acc  49.8300
Round 35 global test acc  52.4400
Round 36 global test acc  52.8500
Round 37 global test acc  50.1900
Round 38 global test acc  54.2400
Round 39 global test acc  47.9700
Round 40 global test acc  52.4700
Round 41 global test acc  44.1500
Round 42 global test acc  46.6600
Round 43 global test acc  51.5200
Round 44 global test acc  49.6600
Round 45 global test acc  50.4000
Round 46 global test acc  52.3800
Round 47 global test acc  54.5000
Round 48 global test acc  50.0600
Round 49 global test acc  54.8800
Round 50 global test acc  49.7500
Round 51 global test acc  50.9000
Round 52 global test acc  52.6600
Round 53 global test acc  54.3000
Round 54 global test acc  52.4200
Round 55 global test acc  53.2400
Round 56 global test acc  54.1900
Round 57 global test acc  56.1000
Round 58 global test acc  55.7900
Round 59 global test acc  50.6600
Round 60 global test acc  52.0600
Round 61 global test acc  55.2700
Round 62 global test acc  52.5000
Round 63 global test acc  54.2200
Round 64 global test acc  51.0700
Round 65 global test acc  55.3200
Round 66 global test acc  53.5900
Round 67 global test acc  53.3200
Round 68 global test acc  56.5600
Round 69 global test acc  54.4200
Round 70 global test acc  55.5400
Round 71 global test acc  55.9100
Round 72 global test acc  49.5400
Round 73 global test acc  57.4600
Round 74 global test acc  47.3300
Round 75 global test acc  55.5400
Round 76 global test acc  55.5300
Round 77 global test acc  56.2600
Round 78 global test acc  52.4400
Round 79 global test acc  55.5700
Round 80 global test acc  55.3300
Round 81 global test acc  54.6200
Round 82 global test acc  53.6000
Round 83 global test acc  52.8800
Round 84 global test acc  51.6100
Round 85 global test acc  50.6700
Round 86 global test acc  50.9700
Round 87 global test acc  51.1400
Round 88 global test acc  51.0800
Round 89 global test acc  51.0800
Round 90 global test acc  51.1700
Round 91 global test acc  51.4800
Round 92 global test acc  51.2100
Round 93 global test acc  51.2700
Round 94 global test acc  50.7300
Round 95 global test acc  50.2600
Round 96 global test acc  50.5000
Round 97 global test acc  49.0800
Round 98 global test acc  49.4800
Round 99 global test acc  49.5100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8840
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5430
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5045
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7255
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8425
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4880
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8215
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8335
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5785
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7785
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7600
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8910
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5440
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6370
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8290
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7165
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.534, Test loss: 0.978, Test accuracy: 68.10
Average accuracy final 10 rounds: 66.7965
1886.6552267074585
[1.6333057880401611, 3.0392637252807617, 4.446332693099976, 5.86373496055603, 7.286370277404785, 8.702430963516235, 10.102089881896973, 11.521392107009888, 12.934224843978882, 14.344097375869751, 15.754929065704346, 17.146668910980225, 18.552793979644775, 19.959420204162598, 21.370575666427612, 22.778414249420166, 24.18882131576538, 25.599222421646118, 27.019930362701416, 28.424686670303345, 29.82590413093567, 31.23768639564514, 32.65416383743286, 34.05926322937012, 35.473981618881226, 36.87856101989746, 38.28746581077576, 39.69406461715698, 41.091710567474365, 42.4975221157074, 43.90087842941284, 45.30244255065918, 46.71681213378906, 48.13325572013855, 49.54681348800659, 50.967479944229126, 52.38867926597595, 53.807904958724976, 55.21821451187134, 56.63117456436157, 58.03115797042847, 59.438621282577515, 60.84138107299805, 62.249125719070435, 63.643999099731445, 65.03744888305664, 66.44039916992188, 67.8364987373352, 69.23705983161926, 70.62624597549438, 72.03062176704407, 73.41573667526245, 74.79626274108887, 76.18925428390503, 77.58631634712219, 78.96668672561646, 80.36146473884583, 81.76257610321045, 83.1587028503418, 84.55825543403625, 85.93827772140503, 87.33486723899841, 88.72033762931824, 90.097177028656, 91.49642395973206, 92.87706327438354, 94.25540089607239, 95.63895297050476, 97.01853442192078, 98.40726399421692, 99.80805921554565, 101.19595432281494, 102.59008359909058, 103.97102856636047, 105.36556768417358, 106.76739001274109, 108.16446876525879, 109.54886960983276, 110.93658685684204, 112.31923937797546, 113.7088053226471, 115.08629608154297, 116.4778699874878, 117.86873269081116, 119.24735236167908, 120.64410829544067, 122.02737832069397, 123.4152262210846, 124.81359386444092, 126.20358633995056, 127.60120415687561, 128.9996337890625, 130.39359140396118, 131.7864384651184, 133.1884331703186, 134.60483145713806, 136.0123631954193, 137.41179084777832, 138.81791734695435, 140.20688152313232, 142.31046724319458]
[21.02, 26.1025, 29.35, 31.8875, 34.865, 37.2825, 40.325, 41.0075, 42.495, 44.28, 44.755, 45.4225, 46.7925, 47.9325, 48.0075, 49.6475, 49.3575, 50.5925, 50.175, 51.6825, 52.3325, 53.9025, 54.5675, 54.7175, 55.055, 55.0025, 55.82, 56.6525, 57.1875, 57.54, 57.89, 58.42, 58.865, 58.8525, 59.5, 59.935, 60.4025, 60.76, 60.57, 61.2175, 61.335, 61.565, 60.92, 61.9575, 61.93, 62.2175, 62.1175, 62.675, 62.895, 63.17, 63.71, 63.9325, 63.8675, 64.09, 63.8825, 64.335, 64.74, 64.5475, 65.0225, 65.035, 65.31, 65.1675, 64.7675, 65.2375, 65.025, 65.5475, 65.4075, 65.5575, 65.6725, 66.3275, 65.63, 65.5275, 65.755, 65.535, 65.1725, 66.07, 66.12, 66.0425, 65.97, 66.25, 66.5925, 66.085, 66.23, 66.635, 67.065, 67.0425, 66.7425, 67.35, 67.0275, 66.7825, 66.4925, 66.4475, 66.5925, 66.18, 66.2075, 66.75, 66.8275, 67.4075, 67.5225, 67.5375, 68.1]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8760
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5520
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4580
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7480
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8340
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4510
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8150
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8445
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.6340
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7900
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7275
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8735
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.6410
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6195
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7915
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6505
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.065, Test loss: 1.142, Test accuracy: 78.03
Final Round, Global train loss: 0.065, Global test loss: 2.202, Global test accuracy: 15.43
Average accuracy final 10 rounds: 78.1825 

Average global accuracy final 10 rounds: 24.961666666666666 

1707.5261039733887
[1.4782965183258057, 2.9565930366516113, 4.2071497440338135, 5.457706451416016, 6.701392889022827, 7.945079326629639, 9.1950523853302, 10.445025444030762, 11.695899963378906, 12.94677448272705, 14.19832730293274, 15.449880123138428, 16.701754570007324, 17.95362901687622, 19.2114155292511, 20.469202041625977, 21.71878433227539, 22.968366622924805, 24.146052360534668, 25.32373809814453, 26.498135089874268, 27.672532081604004, 28.845180988311768, 30.01782989501953, 31.189058303833008, 32.360286712646484, 33.511321783065796, 34.66235685348511, 35.870001554489136, 37.077646255493164, 38.28209853172302, 39.48655080795288, 40.69508624076843, 41.903621673583984, 43.103280782699585, 44.302939891815186, 45.495678663253784, 46.68841743469238, 47.94100522994995, 49.19359302520752, 50.451706886291504, 51.70982074737549, 52.96817326545715, 54.22652578353882, 55.483201026916504, 56.73987627029419, 57.995856046676636, 59.25183582305908, 60.50990533828735, 61.767974853515625, 63.02615213394165, 64.28432941436768, 65.54273533821106, 66.80114126205444, 68.05918884277344, 69.31723642349243, 70.577143907547, 71.83705139160156, 73.10274505615234, 74.36843872070312, 75.62255072593689, 76.87666273117065, 78.12834024429321, 79.38001775741577, 80.63271450996399, 81.8854112625122, 83.14229702949524, 84.39918279647827, 85.65040969848633, 86.90163660049438, 88.15159749984741, 89.40155839920044, 90.65259838104248, 91.90363836288452, 93.15584635734558, 94.40805435180664, 95.65670251846313, 96.90535068511963, 98.15516114234924, 99.40497159957886, 100.6597306728363, 101.91448974609375, 103.17048835754395, 104.42648696899414, 105.6878924369812, 106.94929790496826, 108.2060899734497, 109.46288204193115, 110.71901822090149, 111.97515439987183, 113.23216366767883, 114.48917293548584, 115.7424328327179, 116.99569272994995, 118.25247836112976, 119.50926399230957, 120.7642285823822, 122.01919317245483, 123.2827422618866, 124.54629135131836, 125.80200052261353, 127.05770969390869, 128.31185269355774, 129.5659956932068, 130.81935119628906, 132.07270669937134, 133.32836246490479, 134.58401823043823, 135.84507250785828, 137.10612678527832, 138.3657581806183, 139.62538957595825, 140.88672971725464, 142.14806985855103, 143.4039978981018, 144.6599259376526, 145.91305899620056, 147.16619205474854, 148.42073488235474, 149.67527770996094, 150.92958879470825, 152.18389987945557, 153.44062113761902, 154.69734239578247, 155.959401845932, 157.22146129608154, 158.48324012756348, 159.7450189590454, 161.00404858589172, 162.26307821273804, 163.52065896987915, 164.77823972702026, 166.02992153167725, 167.28160333633423, 168.53693342208862, 169.79226350784302, 171.03030228614807, 172.26834106445312, 173.52008032798767, 174.77181959152222, 176.03070068359375, 177.28958177566528, 178.54570293426514, 179.801824092865, 181.06120681762695, 182.32058954238892, 183.57540583610535, 184.83022212982178, 186.09039568901062, 187.35056924819946, 188.6052703857422, 189.8599715232849, 191.11716413497925, 192.37435674667358, 193.63612127304077, 194.89788579940796, 196.1511960029602, 197.40450620651245, 198.65571975708008, 199.9069333076477, 201.16196966171265, 202.4170060157776, 203.6723358631134, 204.92766571044922, 206.1862347126007, 207.4448037147522, 208.6956024169922, 209.94640111923218, 211.20079851150513, 212.45519590377808, 213.70804595947266, 214.96089601516724, 216.21560072898865, 217.47030544281006, 218.72275233268738, 219.9751992225647, 221.2276611328125, 222.4801230430603, 223.73011136054993, 224.98009967803955, 226.23657321929932, 227.49304676055908, 228.7457981109619, 229.99854946136475, 231.25270986557007, 232.5068702697754, 233.69555401802063, 234.88423776626587, 236.13903546333313, 237.3938331604004, 238.65064311027527, 239.90745306015015, 241.15983128547668, 242.41220951080322, 243.65611577033997, 244.9000220298767, 246.15504837036133, 247.41007471084595, 248.65838098526, 249.90668725967407, 252.39954590797424, 254.8924045562744]
[19.733333333333334, 19.733333333333334, 47.68333333333333, 47.68333333333333, 53.083333333333336, 53.083333333333336, 53.11666666666667, 53.11666666666667, 55.016666666666666, 55.016666666666666, 57.775, 57.775, 66.0, 66.0, 66.59166666666667, 66.59166666666667, 67.30833333333334, 67.30833333333334, 68.08333333333333, 68.08333333333333, 69.19166666666666, 69.19166666666666, 69.8, 69.8, 70.3, 70.3, 71.175, 71.175, 71.60833333333333, 71.60833333333333, 71.85, 71.85, 72.55833333333334, 72.55833333333334, 73.125, 73.125, 73.15, 73.15, 73.65833333333333, 73.65833333333333, 73.65, 73.65, 73.55, 73.55, 73.93333333333334, 73.93333333333334, 74.21666666666667, 74.21666666666667, 74.25, 74.25, 74.41666666666667, 74.41666666666667, 74.5, 74.5, 74.99166666666666, 74.99166666666666, 74.98333333333333, 74.98333333333333, 75.475, 75.475, 76.05, 76.05, 75.94166666666666, 75.94166666666666, 75.9, 75.9, 75.23333333333333, 75.23333333333333, 74.89166666666667, 74.89166666666667, 75.83333333333333, 75.83333333333333, 75.58333333333333, 75.58333333333333, 76.1, 76.1, 76.275, 76.275, 76.58333333333333, 76.58333333333333, 76.575, 76.575, 76.56666666666666, 76.56666666666666, 76.55, 76.55, 76.83333333333333, 76.83333333333333, 77.19166666666666, 77.19166666666666, 76.43333333333334, 76.43333333333334, 77.075, 77.075, 76.80833333333334, 76.80833333333334, 76.725, 76.725, 77.20833333333333, 77.20833333333333, 76.86666666666666, 76.86666666666666, 77.2, 77.2, 76.95, 76.95, 77.28333333333333, 77.28333333333333, 77.04166666666667, 77.04166666666667, 77.16666666666667, 77.16666666666667, 77.56666666666666, 77.56666666666666, 77.525, 77.525, 77.275, 77.275, 77.38333333333334, 77.38333333333334, 77.14166666666667, 77.14166666666667, 77.43333333333334, 77.43333333333334, 77.28333333333333, 77.28333333333333, 77.25, 77.25, 77.11666666666666, 77.11666666666666, 77.05, 77.05, 77.01666666666667, 77.01666666666667, 76.98333333333333, 76.98333333333333, 77.23333333333333, 77.23333333333333, 77.625, 77.625, 77.88333333333334, 77.88333333333334, 78.06666666666666, 78.06666666666666, 78.18333333333334, 78.18333333333334, 78.13333333333334, 78.13333333333334, 78.09166666666667, 78.09166666666667, 77.84166666666667, 77.84166666666667, 77.50833333333334, 77.50833333333334, 77.45, 77.45, 77.25, 77.25, 78.0, 78.0, 78.16666666666667, 78.16666666666667, 78.18333333333334, 78.18333333333334, 78.075, 78.075, 78.03333333333333, 78.03333333333333, 78.15, 78.15, 78.325, 78.325, 77.675, 77.675, 77.725, 77.725, 77.725, 77.725, 77.16666666666667, 77.16666666666667, 77.325, 77.325, 77.73333333333333, 77.73333333333333, 78.15, 78.15, 78.23333333333333, 78.23333333333333, 77.975, 77.975, 78.475, 78.475, 78.65, 78.65, 78.89166666666667, 78.89166666666667, 78.30833333333334, 78.30833333333334, 78.08333333333333, 78.08333333333333, 78.03333333333333, 78.03333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.237, Test loss: 1.025, Test accuracy: 74.75
Final Round, Global train loss: 0.237, Global test loss: 0.715, Global test accuracy: 78.52
Average accuracy final 10 rounds: 73.98275 

Average global accuracy final 10 rounds: 78.55024999999999 

5473.07971572876
[4.869009256362915, 9.73801851272583, 13.773975133895874, 17.809931755065918, 21.783809900283813, 25.75768804550171, 29.7183837890625, 33.67907953262329, 37.635916233062744, 41.5927529335022, 45.59769678115845, 49.6026406288147, 53.55557346343994, 57.508506298065186, 61.47843956947327, 65.44837284088135, 69.41030168533325, 73.37223052978516, 77.34782290458679, 81.32341527938843, 85.29310441017151, 89.26279354095459, 93.23922729492188, 97.21566104888916, 101.18666791915894, 105.15767478942871, 109.10768604278564, 113.05769729614258, 117.019202709198, 120.98070812225342, 124.93573188781738, 128.89075565338135, 132.86216402053833, 136.8335723876953, 140.804461479187, 144.7753505706787, 148.74297451972961, 152.71059846878052, 156.6870710849762, 160.66354370117188, 164.64324927330017, 168.62295484542847, 172.5864520072937, 176.54994916915894, 180.50838232040405, 184.46681547164917, 188.44020915031433, 192.4136028289795, 196.37338018417358, 200.33315753936768, 204.2903163433075, 208.24747514724731, 212.22754168510437, 216.20760822296143, 220.24288320541382, 224.2781581878662, 228.31984782218933, 232.36153745651245, 236.38075637817383, 240.3999752998352, 244.43105721473694, 248.46213912963867, 252.48594450950623, 256.5097498893738, 260.518381357193, 264.5270128250122, 268.531601190567, 272.5361895561218, 276.55506682395935, 280.5739440917969, 284.5873022079468, 288.6006603240967, 292.6208724975586, 296.6410846710205, 300.6917371749878, 304.7423896789551, 308.77400732040405, 312.805624961853, 316.83367800712585, 320.8617310523987, 324.89277505874634, 328.923819065094, 332.9575300216675, 336.99124097824097, 341.01492285728455, 345.0386047363281, 349.0619258880615, 353.0852470397949, 357.12605023384094, 361.16685342788696, 365.20480728149414, 369.2427611351013, 373.2607080936432, 377.27865505218506, 381.3074245452881, 385.3361940383911, 389.37247920036316, 393.4087643623352, 397.4280331134796, 401.447301864624, 405.4833493232727, 409.5193967819214, 413.5608777999878, 417.6023588180542, 421.62228560447693, 425.64221239089966, 429.67830538749695, 433.71439838409424, 437.7344436645508, 441.7544889450073, 445.78056168556213, 449.80663442611694, 453.8244261741638, 457.8422179222107, 461.8673028945923, 465.8923878669739, 469.91859698295593, 473.944806098938, 477.95936250686646, 481.9739189147949, 486.0207600593567, 490.06760120391846, 494.0959725379944, 498.1243438720703, 502.1349210739136, 506.14549827575684, 510.1805970668793, 514.2156958580017, 518.2289988994598, 522.242301940918, 526.2505171298981, 530.2587323188782, 534.2881283760071, 538.317524433136, 542.3504092693329, 546.3832941055298, 550.4185619354248, 554.4538297653198, 558.4871263504028, 562.5204229354858, 566.5569980144501, 570.5935730934143, 574.6326966285706, 578.6718201637268, 582.7057931423187, 586.7397661209106, 590.7654211521149, 594.7910761833191, 598.8070950508118, 602.8231139183044, 606.8342945575714, 610.8454751968384, 614.984979391098, 619.1244835853577, 623.2609434127808, 627.3974032402039, 631.5265839099884, 635.655764579773, 639.7275991439819, 643.7994337081909, 647.9221618175507, 652.0448899269104, 656.187488079071, 660.3300862312317, 664.4416992664337, 668.5533123016357, 672.6672840118408, 676.7812557220459, 680.9255559444427, 685.0698561668396, 689.1982016563416, 693.3265471458435, 697.4155130386353, 701.504478931427, 705.5900497436523, 709.6756205558777, 713.7761957645416, 717.8767709732056, 721.9728252887726, 726.0688796043396, 730.1708006858826, 734.2727217674255, 738.3699889183044, 742.4672560691833, 746.5788547992706, 750.6904535293579, 754.8006601333618, 758.9108667373657, 763.0389440059662, 767.1670212745667, 771.2807018756866, 775.3943824768066, 779.5052623748779, 783.6161422729492, 787.7057311534882, 791.7953200340271, 795.8973360061646, 799.999351978302, 804.1001715660095, 808.200991153717, 810.257246017456, 812.3135008811951]
[37.6125, 37.6125, 43.3275, 43.3275, 44.84, 44.84, 47.35, 47.35, 49.5475, 49.5475, 49.3775, 49.3775, 51.25, 51.25, 53.6275, 53.6275, 56.79, 56.79, 58.7225, 58.7225, 60.1075, 60.1075, 60.5575, 60.5575, 61.17, 61.17, 62.2175, 62.2175, 63.3575, 63.3575, 63.7075, 63.7075, 64.6325, 64.6325, 65.5925, 65.5925, 65.96, 65.96, 66.57, 66.57, 67.0575, 67.0575, 67.175, 67.175, 67.2275, 67.2275, 67.56, 67.56, 67.825, 67.825, 68.345, 68.345, 68.265, 68.265, 68.4575, 68.4575, 68.605, 68.605, 69.0175, 69.0175, 69.695, 69.695, 69.8825, 69.8825, 70.29, 70.29, 70.5975, 70.5975, 70.4925, 70.4925, 70.4675, 70.4675, 70.7, 70.7, 70.9675, 70.9675, 71.125, 71.125, 71.34, 71.34, 71.3925, 71.3925, 71.4875, 71.4875, 71.51, 71.51, 71.89, 71.89, 72.095, 72.095, 72.0825, 72.0825, 72.1175, 72.1175, 72.14, 72.14, 72.43, 72.43, 72.1925, 72.1925, 72.2975, 72.2975, 72.1375, 72.1375, 72.2875, 72.2875, 72.375, 72.375, 72.395, 72.395, 72.4725, 72.4725, 72.6775, 72.6775, 72.515, 72.515, 72.485, 72.485, 72.5175, 72.5175, 72.6, 72.6, 72.625, 72.625, 72.9975, 72.9975, 72.845, 72.845, 72.8775, 72.8775, 73.1025, 73.1025, 72.8625, 72.8625, 72.8375, 72.8375, 72.95, 72.95, 72.8625, 72.8625, 72.9375, 72.9375, 73.2975, 73.2975, 73.16, 73.16, 73.22, 73.22, 73.1025, 73.1025, 72.79, 72.79, 73.065, 73.065, 72.9825, 72.9825, 73.0875, 73.0875, 73.305, 73.305, 73.32, 73.32, 73.41, 73.41, 73.46, 73.46, 73.8025, 73.8025, 73.9725, 73.9725, 74.12, 74.12, 73.82, 73.82, 73.9175, 73.9175, 74.01, 74.01, 73.8325, 73.8325, 74.0075, 74.0075, 74.0625, 74.0625, 73.93, 73.93, 73.9625, 73.9625, 73.8825, 73.8825, 73.9125, 73.9125, 73.995, 73.995, 74.0175, 74.0175, 74.035, 74.035, 74.0225, 74.0225, 74.7525, 74.7525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.215, Test loss: 0.587, Test accuracy: 82.70
Final Round, Global train loss: 0.215, Global test loss: 1.439, Global test accuracy: 59.55
Average accuracy final 10 rounds: 81.55000000000001 

Average global accuracy final 10 rounds: 58.555 

968.1996169090271
[1.0218536853790283, 2.0437073707580566, 2.8178985118865967, 3.5920896530151367, 4.357477188110352, 5.122864723205566, 5.889501333236694, 6.656137943267822, 7.424045085906982, 8.191952228546143, 8.963021039962769, 9.734089851379395, 10.509078741073608, 11.284067630767822, 12.051963806152344, 12.819859981536865, 13.589805603027344, 14.359751224517822, 15.130078792572021, 15.90040636062622, 16.671228170394897, 17.442049980163574, 18.211364030838013, 18.98067808151245, 19.75227689743042, 20.52387571334839, 21.30080246925354, 22.07772922515869, 22.859543085098267, 23.641356945037842, 24.418758869171143, 25.196160793304443, 25.968863248825073, 26.741565704345703, 27.514831066131592, 28.28809642791748, 29.060957670211792, 29.833818912506104, 30.61855673789978, 31.403294563293457, 32.18340587615967, 32.96351718902588, 33.74018311500549, 34.51684904098511, 35.28856086730957, 36.06027269363403, 36.828633308410645, 37.596993923187256, 38.35115647315979, 39.105319023132324, 39.87264132499695, 40.63996362686157, 41.39730715751648, 42.15465068817139, 42.91693186759949, 43.67921304702759, 44.447792291641235, 45.21637153625488, 45.97746658325195, 46.73856163024902, 47.50032901763916, 48.2620964050293, 49.02050852775574, 49.77892065048218, 50.54380679130554, 51.308692932128906, 52.06671500205994, 52.82473707199097, 53.597060203552246, 54.369383335113525, 55.14667344093323, 55.92396354675293, 56.702436447143555, 57.48090934753418, 58.24868631362915, 59.01646327972412, 59.78531765937805, 60.55417203903198, 61.322184801101685, 62.09019756317139, 62.864142417907715, 63.63808727264404, 64.41142868995667, 65.18477010726929, 65.95907425880432, 66.73337841033936, 67.50127172470093, 68.2691650390625, 69.04027819633484, 69.81139135360718, 70.58540201187134, 71.3594126701355, 72.13267946243286, 72.90594625473022, 73.67843890190125, 74.45093154907227, 75.22926163673401, 76.00759172439575, 76.77448272705078, 77.54137372970581, 78.31463098526001, 79.08788824081421, 79.82666754722595, 80.5654468536377, 81.33777904510498, 82.11011123657227, 82.8883285522461, 83.66654586791992, 84.44568347930908, 85.22482109069824, 85.9999315738678, 86.77504205703735, 87.54848146438599, 88.32192087173462, 89.09712314605713, 89.87232542037964, 90.65064072608948, 91.42895603179932, 92.20373106002808, 92.97850608825684, 93.76042127609253, 94.54233646392822, 95.31239724159241, 96.08245801925659, 96.85387897491455, 97.62529993057251, 98.40021967887878, 99.17513942718506, 99.95009255409241, 100.72504568099976, 101.50787711143494, 102.29070854187012, 103.06121587753296, 103.8317232131958, 104.60735130310059, 105.38297939300537, 106.15809774398804, 106.9332160949707, 107.70384407043457, 108.47447204589844, 109.24476289749146, 110.01505374908447, 110.79061222076416, 111.56617069244385, 112.34098410606384, 113.11579751968384, 113.89464378356934, 114.67349004745483, 115.45697569847107, 116.2404613494873, 117.00946116447449, 117.77846097946167, 118.55104160308838, 119.32362222671509, 120.09726119041443, 120.87090015411377, 121.64254975318909, 122.4141993522644, 123.1874475479126, 123.96069574356079, 124.73281168937683, 125.50492763519287, 126.28168559074402, 127.05844354629517, 127.83515691757202, 128.61187028884888, 129.38462018966675, 130.15737009048462, 130.93020009994507, 131.70303010940552, 132.4793577194214, 133.25568532943726, 134.0325677394867, 134.80945014953613, 135.58464741706848, 136.35984468460083, 137.14293479919434, 137.92602491378784, 138.70017552375793, 139.47432613372803, 140.24175119400024, 141.00917625427246, 141.7789285182953, 142.54868078231812, 143.3197202682495, 144.0907597541809, 144.86302018165588, 145.63528060913086, 146.40219402313232, 147.1691074371338, 147.94238686561584, 148.7156662940979, 149.48675274848938, 150.25783920288086, 151.02290296554565, 151.78796672821045, 152.55629301071167, 153.3246192932129, 154.098637342453, 154.87265539169312, 156.4169147014618, 157.96117401123047]
[20.383333333333333, 20.383333333333333, 39.2, 39.2, 37.56666666666667, 37.56666666666667, 49.583333333333336, 49.583333333333336, 56.53333333333333, 56.53333333333333, 60.2, 60.2, 62.38333333333333, 62.38333333333333, 62.733333333333334, 62.733333333333334, 65.68333333333334, 65.68333333333334, 63.65, 63.65, 66.05, 66.05, 67.76666666666667, 67.76666666666667, 70.8, 70.8, 71.45, 71.45, 71.25, 71.25, 72.85, 72.85, 74.45, 74.45, 74.31666666666666, 74.31666666666666, 74.18333333333334, 74.18333333333334, 74.76666666666667, 74.76666666666667, 74.61666666666666, 74.61666666666666, 75.71666666666667, 75.71666666666667, 76.18333333333334, 76.18333333333334, 76.78333333333333, 76.78333333333333, 76.68333333333334, 76.68333333333334, 76.66666666666667, 76.66666666666667, 76.03333333333333, 76.03333333333333, 75.5, 75.5, 76.26666666666667, 76.26666666666667, 76.68333333333334, 76.68333333333334, 77.63333333333334, 77.63333333333334, 77.25, 77.25, 78.03333333333333, 78.03333333333333, 78.48333333333333, 78.48333333333333, 78.38333333333334, 78.38333333333334, 78.18333333333334, 78.18333333333334, 77.76666666666667, 77.76666666666667, 78.38333333333334, 78.38333333333334, 78.55, 78.55, 79.23333333333333, 79.23333333333333, 78.91666666666667, 78.91666666666667, 78.61666666666666, 78.61666666666666, 79.61666666666666, 79.61666666666666, 79.5, 79.5, 79.0, 79.0, 79.6, 79.6, 79.6, 79.6, 79.41666666666667, 79.41666666666667, 79.56666666666666, 79.56666666666666, 79.93333333333334, 79.93333333333334, 79.93333333333334, 79.93333333333334, 79.65, 79.65, 79.41666666666667, 79.41666666666667, 79.8, 79.8, 79.76666666666667, 79.76666666666667, 80.45, 80.45, 79.75, 79.75, 80.53333333333333, 80.53333333333333, 80.76666666666667, 80.76666666666667, 80.16666666666667, 80.16666666666667, 79.33333333333333, 79.33333333333333, 79.63333333333334, 79.63333333333334, 80.2, 80.2, 80.83333333333333, 80.83333333333333, 81.11666666666666, 81.11666666666666, 81.58333333333333, 81.58333333333333, 82.05, 82.05, 81.53333333333333, 81.53333333333333, 81.5, 81.5, 81.41666666666667, 81.41666666666667, 81.63333333333334, 81.63333333333334, 81.2, 81.2, 80.68333333333334, 80.68333333333334, 80.41666666666667, 80.41666666666667, 80.85, 80.85, 80.08333333333333, 80.08333333333333, 79.76666666666667, 79.76666666666667, 80.36666666666666, 80.36666666666666, 80.6, 80.6, 80.86666666666666, 80.86666666666666, 80.85, 80.85, 80.78333333333333, 80.78333333333333, 80.93333333333334, 80.93333333333334, 81.1, 81.1, 81.23333333333333, 81.23333333333333, 81.75, 81.75, 81.36666666666666, 81.36666666666666, 81.71666666666667, 81.71666666666667, 81.48333333333333, 81.48333333333333, 82.06666666666666, 82.06666666666666, 81.93333333333334, 81.93333333333334, 81.83333333333333, 81.83333333333333, 81.71666666666667, 81.71666666666667, 81.51666666666667, 81.51666666666667, 81.71666666666667, 81.71666666666667, 81.78333333333333, 81.78333333333333, 81.0, 81.0, 81.21666666666667, 81.21666666666667, 81.03333333333333, 81.03333333333333, 81.75, 81.75, 82.7, 82.7]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  10.0000
Round 1 global test acc  20.5800
Round 2 global test acc  17.5600
Round 3 global test acc  18.8200
Round 4 global test acc  19.8700
Round 5 global test acc  13.5900
Round 6 global test acc  24.6600
Round 7 global test acc  24.2500
Round 8 global test acc  19.7900
Round 9 global test acc  23.2200
Round 10 global test acc  28.7100
Round 11 global test acc  26.7000
Round 12 global test acc  26.9000
Round 13 global test acc  25.1200
Round 14 global test acc  29.7200
Round 15 global test acc  27.6100
Round 16 global test acc  24.0600
Round 17 global test acc  25.2300
Round 18 global test acc  29.4700
Round 19 global test acc  28.4000
Round 20 global test acc  29.5700
Round 21 global test acc  30.9500
Round 22 global test acc  27.7400
Round 23 global test acc  25.6200
Round 24 global test acc  27.3200
Round 25 global test acc  29.6100
Round 26 global test acc  29.3300
Round 27 global test acc  22.2500
Round 28 global test acc  24.6000
Round 29 global test acc  33.8000
Round 30 global test acc  28.0800
Round 31 global test acc  31.3000
Round 32 global test acc  23.4800
Round 33 global test acc  20.3400
Round 34 global test acc  32.6100
Round 35 global test acc  26.9500
Round 36 global test acc  28.0800
Round 37 global test acc  29.3100
Round 38 global test acc  35.5300
Round 39 global test acc  30.8800
Round 40 global test acc  29.3300
Round 41 global test acc  28.3400
Round 42 global test acc  33.6900
Round 43 global test acc  34.1600
Round 44 global test acc  35.7200
Round 45 global test acc  28.4100
Round 46 global test acc  27.7500
Round 47 global test acc  28.0500
Round 48 global test acc  30.5400
Round 49 global test acc  36.1100
Round 50 global test acc  30.6100
Round 51 global test acc  30.2500
Round 52 global test acc  32.4400
Round 53 global test acc  36.1000
Round 54 global test acc  31.5200
Round 55 global test acc  28.1600
Round 56 global test acc  32.2400
Round 57 global test acc  16.5100
Round 58 global test acc  32.2400
Round 59 global test acc  29.3700
Round 60 global test acc  37.6400
Round 61 global test acc  35.9700
Round 62 global test acc  32.9700
Round 63 global test acc  30.5000
Round 64 global test acc  33.4300
Round 65 global test acc  27.5700
Round 66 global test acc  26.2700
Round 67 global test acc  27.6200
Round 68 global test acc  34.8100
Round 69 global test acc  27.4300
Round 70 global test acc  34.9800
Round 71 global test acc  29.0100
Round 72 global test acc  29.2000
Round 73 global test acc  26.9300
Round 74 global test acc  31.1200
Round 75 global test acc  35.9400
Round 76 global test acc  34.2700
Round 77 global test acc  32.8900
Round 78 global test acc  36.5100
Round 79 global test acc  32.6000
Round 80 global test acc  30.8100
Round 81 global test acc  29.8400
Round 82 global test acc  28.7400
Round 83 global test acc  26.2200
Round 84 global test acc  24.1600
Round 85 global test acc  23.3800
Round 86 global test acc  23.9900
Round 87 global test acc  23.0900
Round 88 global test acc  22.1300
Round 89 global test acc  21.5800
Round 90 global test acc  20.0900
Round 91 global test acc  19.0100
Round 92 global test acc  18.4500
Round 93 global test acc  18.1600
Round 94 global test acc  18.1600
Round 95 global test acc  17.9200
Round 96 global test acc  17.9300
Round 97 global test acc  17.6800
Round 98 global test acc  17.3900
Round 99 global test acc  17.1600
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.239, Test loss: 0.404, Test accuracy: 83.92
Average accuracy final 10 rounds: 83.81333333333332
665.1079025268555
[1.1397919654846191, 1.9605164527893066, 2.790879487991333, 3.622084856033325, 4.451357126235962, 5.284015417098999, 6.111629247665405, 6.940386772155762, 7.765285015106201, 8.590050220489502, 9.412571668624878, 10.240137815475464, 11.065232038497925, 11.888102054595947, 12.716731548309326, 13.541863918304443, 14.373287439346313, 15.196743726730347, 16.022413969039917, 16.85075545310974, 17.682074546813965, 18.511798620224, 19.339515686035156, 20.166977167129517, 20.993170022964478, 21.81762170791626, 22.64827609062195, 23.47581720352173, 24.304017782211304, 25.13383460044861, 25.96569538116455, 26.789768934249878, 27.622798681259155, 28.453100442886353, 29.276028633117676, 30.106205940246582, 30.92833137512207, 31.753220558166504, 32.581395864486694, 33.40838837623596, 34.23942971229553, 35.06730651855469, 35.89870548248291, 36.72357511520386, 37.547354221343994, 38.37904071807861, 39.205689907073975, 40.03221940994263, 40.853296518325806, 41.68133282661438, 42.512261152267456, 43.34007024765015, 44.172563791275024, 45.00244998931885, 45.83726906776428, 46.694366455078125, 47.545732498168945, 48.38407611846924, 49.223326206207275, 50.043598890304565, 50.84798073768616, 51.61630606651306, 52.3927001953125, 53.22316122055054, 54.103126525878906, 54.92490220069885, 55.74067997932434, 56.58678412437439, 57.42372441291809, 58.26702523231506, 59.095306158065796, 59.932761669158936, 60.777544021606445, 61.631977558135986, 62.47392797470093, 63.32429099082947, 64.16899704933167, 65.01098728179932, 65.88914918899536, 66.74302053451538, 67.50882601737976, 68.26527285575867, 69.0242850780487, 69.79647922515869, 70.6279444694519, 71.46806478500366, 72.31893825531006, 73.11512637138367, 73.90501880645752, 74.69174599647522, 75.51489806175232, 76.31552934646606, 77.0947015285492, 77.87550806999207, 78.65583157539368, 79.43308472633362, 80.20919966697693, 80.924880027771, 81.64048528671265, 82.37663888931274, 83.65718173980713]
[20.2, 37.083333333333336, 44.516666666666666, 52.483333333333334, 58.016666666666666, 59.766666666666666, 66.95, 69.06666666666666, 68.51666666666667, 69.51666666666667, 70.91666666666667, 70.36666666666666, 72.35, 72.51666666666667, 73.21666666666667, 74.43333333333334, 74.21666666666667, 75.28333333333333, 75.6, 76.05, 75.8, 76.01666666666667, 76.26666666666667, 76.45, 77.13333333333334, 77.26666666666667, 77.75, 78.21666666666667, 78.46666666666667, 78.26666666666667, 78.01666666666667, 78.1, 78.61666666666666, 79.15, 79.5, 79.65, 79.9, 79.71666666666667, 79.66666666666667, 79.9, 80.25, 80.26666666666667, 80.48333333333333, 80.68333333333334, 80.55, 80.61666666666666, 80.33333333333333, 80.21666666666667, 80.66666666666667, 81.06666666666666, 81.21666666666667, 81.05, 81.28333333333333, 81.66666666666667, 81.51666666666667, 81.55, 81.63333333333334, 81.63333333333334, 81.65, 81.9, 82.15, 82.21666666666667, 82.55, 82.06666666666666, 82.08333333333333, 82.25, 82.51666666666667, 82.26666666666667, 82.1, 82.26666666666667, 82.96666666666667, 82.71666666666667, 83.23333333333333, 83.05, 82.8, 83.06666666666666, 82.9, 82.61666666666666, 83.35, 83.45, 83.5, 83.21666666666667, 82.88333333333334, 82.91666666666667, 82.75, 83.26666666666667, 83.43333333333334, 84.11666666666666, 83.91666666666667, 83.95, 84.0, 83.73333333333333, 84.03333333333333, 83.8, 83.58333333333333, 83.75, 83.71666666666667, 83.8, 83.78333333333333, 83.93333333333334, 83.91666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.193, Test loss: 1.867, Test accuracy: 35.41
Round   1, Train loss: 1.825, Test loss: 1.612, Test accuracy: 43.20
Round   2, Train loss: 1.650, Test loss: 1.495, Test accuracy: 47.74
Round   3, Train loss: 1.549, Test loss: 1.428, Test accuracy: 50.28
Round   4, Train loss: 1.475, Test loss: 1.337, Test accuracy: 53.24
Round   5, Train loss: 1.417, Test loss: 1.262, Test accuracy: 55.97
Round   6, Train loss: 1.348, Test loss: 1.229, Test accuracy: 57.20
Round   7, Train loss: 1.311, Test loss: 1.169, Test accuracy: 59.45
Round   8, Train loss: 1.242, Test loss: 1.181, Test accuracy: 58.48
Round   9, Train loss: 1.213, Test loss: 1.127, Test accuracy: 60.99
Round  10, Train loss: 1.168, Test loss: 1.103, Test accuracy: 61.36
Round  11, Train loss: 1.124, Test loss: 1.087, Test accuracy: 62.15
Round  12, Train loss: 1.086, Test loss: 1.101, Test accuracy: 61.98
Round  13, Train loss: 1.078, Test loss: 1.039, Test accuracy: 64.45
Round  14, Train loss: 1.057, Test loss: 1.000, Test accuracy: 65.46
Round  15, Train loss: 1.017, Test loss: 1.004, Test accuracy: 65.48
Round  16, Train loss: 1.015, Test loss: 0.968, Test accuracy: 66.62
Round  17, Train loss: 0.977, Test loss: 0.975, Test accuracy: 66.32
Round  18, Train loss: 0.978, Test loss: 0.939, Test accuracy: 67.61
Round  19, Train loss: 0.940, Test loss: 0.926, Test accuracy: 68.28
Round  20, Train loss: 0.948, Test loss: 0.882, Test accuracy: 69.42
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2200
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.5883
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0217
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4567
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5550
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4950
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3567
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6450
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.186, Test loss: 1.500, Test accuracy: 65.37
Final Round, Global train loss: 0.186, Global test loss: 2.053, Global test accuracy: 28.64
Average accuracy final 10 rounds: 66.0 

Average global accuracy final 10 rounds: 22.85666666666667 

1856.892256975174
[1.6366209983825684, 3.2732419967651367, 4.6496665477752686, 6.0260910987854, 7.415838003158569, 8.805584907531738, 10.223692178726196, 11.641799449920654, 13.004682302474976, 14.367565155029297, 15.807918787002563, 17.24827241897583, 18.616833925247192, 19.985395431518555, 21.356038331985474, 22.726681232452393, 24.17251205444336, 25.618342876434326, 26.993353605270386, 28.368364334106445, 29.72734022140503, 31.086316108703613, 32.5162193775177, 33.94612264633179, 35.32188153266907, 36.69764041900635, 38.0833466053009, 39.46905279159546, 40.85479712486267, 42.24054145812988, 43.605724573135376, 44.97090768814087, 46.410868406295776, 47.850829124450684, 49.23766493797302, 50.62450075149536, 51.97532391548157, 53.32614707946777, 54.822744846343994, 56.319342613220215, 57.746949195861816, 59.17455577850342, 60.52240014076233, 61.87024450302124, 63.26230072975159, 64.65435695648193, 66.02555203437805, 67.39674711227417, 68.75617408752441, 70.11560106277466, 71.48479628562927, 72.85399150848389, 74.21315789222717, 75.57232427597046, 77.07548022270203, 78.5786361694336, 79.96505689620972, 81.35147762298584, 82.71610856056213, 84.08073949813843, 85.51428580284119, 86.94783210754395, 88.3319342136383, 89.71603631973267, 91.0779218673706, 92.43980741500854, 93.86498999595642, 95.2901725769043, 96.69993185997009, 98.10969114303589, 99.47992825508118, 100.85016536712646, 102.24509692192078, 103.64002847671509, 104.99445247650146, 106.34887647628784, 107.75726127624512, 109.16564607620239, 110.54972720146179, 111.93380832672119, 113.30147504806519, 114.66914176940918, 116.07393836975098, 117.47873497009277, 118.87309694290161, 120.26745891571045, 121.66236448287964, 123.05727005004883, 124.45348525047302, 125.84970045089722, 127.24094533920288, 128.63219022750854, 130.0028154850006, 131.37344074249268, 132.7508556842804, 134.12827062606812, 135.52575397491455, 136.923237323761, 138.30602264404297, 139.68880796432495, 141.06603598594666, 142.44326400756836, 143.82693719863892, 145.21061038970947, 146.5891010761261, 147.96759176254272, 149.3337607383728, 150.69992971420288, 152.09566593170166, 153.49140214920044, 154.8796718120575, 156.26794147491455, 157.52211689949036, 158.77629232406616, 160.05317950248718, 161.3300666809082, 162.61052203178406, 163.8909773826599, 165.25438785552979, 166.61779832839966, 168.0022096633911, 169.38662099838257, 170.74497723579407, 172.10333347320557, 173.4936637878418, 174.88399410247803, 176.3137767314911, 177.74355936050415, 179.11686253547668, 180.49016571044922, 181.86045265197754, 183.23073959350586, 184.58790469169617, 185.94506978988647, 187.3204481601715, 188.69582653045654, 190.05952620506287, 191.4232258796692, 192.8237805366516, 194.22433519363403, 195.6673719882965, 197.11040878295898, 198.528822183609, 199.94723558425903, 201.32965755462646, 202.7120795249939, 204.08763980865479, 205.46320009231567, 206.88956928253174, 208.3159384727478, 209.68184280395508, 211.04774713516235, 212.3773696422577, 213.70699214935303, 215.07247686386108, 216.43796157836914, 217.76500511169434, 219.09204864501953, 220.34729027748108, 221.60253190994263, 222.92892909049988, 224.25532627105713, 225.57264351844788, 226.88996076583862, 228.64460802078247, 230.39925527572632, 232.2771348953247, 234.1550145149231, 235.70836234092712, 237.26171016693115, 238.83964705467224, 240.41758394241333, 242.02329421043396, 243.6290044784546, 245.20099258422852, 246.77298069000244, 248.05491971969604, 249.33685874938965, 250.60564494132996, 251.87443113327026, 253.15098929405212, 254.42754745483398, 255.78313827514648, 257.138729095459, 258.57819056510925, 260.0176520347595, 261.3434536457062, 262.66925525665283, 263.99574279785156, 265.3222303390503, 266.6330797672272, 267.94392919540405, 269.2516944408417, 270.5594596862793, 272.00469636917114, 273.449933052063, 274.83555364608765, 276.2211742401123, 277.5236644744873, 278.8261547088623, 280.96508741378784, 283.1040201187134]
[19.35, 19.35, 39.0, 39.0, 48.88333333333333, 48.88333333333333, 52.975, 52.975, 55.88333333333333, 55.88333333333333, 57.00833333333333, 57.00833333333333, 57.108333333333334, 57.108333333333334, 58.791666666666664, 58.791666666666664, 57.858333333333334, 57.858333333333334, 58.46666666666667, 58.46666666666667, 60.358333333333334, 60.358333333333334, 61.375, 61.375, 63.15, 63.15, 64.15833333333333, 64.15833333333333, 65.14166666666667, 65.14166666666667, 64.91666666666667, 64.91666666666667, 65.21666666666667, 65.21666666666667, 66.21666666666667, 66.21666666666667, 66.79166666666667, 66.79166666666667, 66.51666666666667, 66.51666666666667, 65.68333333333334, 65.68333333333334, 65.125, 65.125, 67.1, 67.1, 65.58333333333333, 65.58333333333333, 65.65833333333333, 65.65833333333333, 66.49166666666666, 66.49166666666666, 67.26666666666667, 67.26666666666667, 68.425, 68.425, 68.54166666666667, 68.54166666666667, 68.35, 68.35, 68.91666666666667, 68.91666666666667, 69.10833333333333, 69.10833333333333, 69.225, 69.225, 68.89166666666667, 68.89166666666667, 68.53333333333333, 68.53333333333333, 67.91666666666667, 67.91666666666667, 68.53333333333333, 68.53333333333333, 67.50833333333334, 67.50833333333334, 67.2, 67.2, 65.84166666666667, 65.84166666666667, 65.86666666666666, 65.86666666666666, 66.30833333333334, 66.30833333333334, 65.88333333333334, 65.88333333333334, 66.19166666666666, 66.19166666666666, 65.40833333333333, 65.40833333333333, 66.21666666666667, 66.21666666666667, 66.33333333333333, 66.33333333333333, 65.6, 65.6, 65.45833333333333, 65.45833333333333, 66.55833333333334, 66.55833333333334, 66.10833333333333, 66.10833333333333, 66.25833333333334, 66.25833333333334, 66.61666666666666, 66.61666666666666, 66.43333333333334, 66.43333333333334, 66.30833333333334, 66.30833333333334, 66.54166666666667, 66.54166666666667, 66.49166666666666, 66.49166666666666, 67.025, 67.025, 66.625, 66.625, 66.575, 66.575, 65.93333333333334, 65.93333333333334, 66.625, 66.625, 66.03333333333333, 66.03333333333333, 66.1, 66.1, 65.91666666666667, 65.91666666666667, 64.61666666666666, 64.61666666666666, 64.59166666666667, 64.59166666666667, 64.86666666666666, 64.86666666666666, 65.44166666666666, 65.44166666666666, 65.88333333333334, 65.88333333333334, 65.63333333333334, 65.63333333333334, 65.43333333333334, 65.43333333333334, 66.9, 66.9, 66.71666666666667, 66.71666666666667, 66.46666666666667, 66.46666666666667, 66.21666666666667, 66.21666666666667, 66.375, 66.375, 65.975, 65.975, 65.9, 65.9, 66.025, 66.025, 66.23333333333333, 66.23333333333333, 65.33333333333333, 65.33333333333333, 65.45, 65.45, 65.65, 65.65, 65.7, 65.7, 65.68333333333334, 65.68333333333334, 65.79166666666667, 65.79166666666667, 65.64166666666667, 65.64166666666667, 64.93333333333334, 64.93333333333334, 64.58333333333333, 64.58333333333333, 66.075, 66.075, 66.21666666666667, 66.21666666666667, 66.34166666666667, 66.34166666666667, 66.09166666666667, 66.09166666666667, 66.075, 66.075, 66.01666666666667, 66.01666666666667, 65.65, 65.65, 65.95833333333333, 65.95833333333333, 65.9, 65.9, 65.675, 65.675, 65.36666666666666, 65.36666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2200
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.5883
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0900
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4567
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5550
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4950
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3567
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6450
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.198, Test loss: 0.636, Test accuracy: 81.54
Final Round, Global train loss: 0.198, Global test loss: 1.375, Global test accuracy: 59.83
Average accuracy final 10 rounds: 81.58000000000001 

Average global accuracy final 10 rounds: 60.8475 

1796.5006561279297
[1.5844390392303467, 3.1688780784606934, 4.476438522338867, 5.783998966217041, 7.092049837112427, 8.400100708007812, 9.742328882217407, 11.084557056427002, 12.375775814056396, 13.666994571685791, 15.000941753387451, 16.33488893508911, 17.662131547927856, 18.9893741607666, 20.3917875289917, 21.794200897216797, 23.170111417770386, 24.546021938323975, 25.99099349975586, 27.435965061187744, 28.92479372024536, 30.41362237930298, 31.837559938430786, 33.261497497558594, 34.697306394577026, 36.13311529159546, 37.57965922355652, 39.02620315551758, 40.44385886192322, 41.86151456832886, 43.30060935020447, 44.73970413208008, 46.16809439659119, 47.596484661102295, 49.03118705749512, 50.46588945388794, 51.83141374588013, 53.196938037872314, 54.48067355155945, 55.76440906524658, 57.0249080657959, 58.285407066345215, 59.7121856212616, 61.13896417617798, 62.51363253593445, 63.88830089569092, 65.27489614486694, 66.66149139404297, 68.09210419654846, 69.52271699905396, 70.95550656318665, 72.38829612731934, 73.80087304115295, 75.21344995498657, 76.63557815551758, 78.05770635604858, 79.48518109321594, 80.9126558303833, 82.16639947891235, 83.4201431274414, 84.6758234500885, 85.9315037727356, 87.2146635055542, 88.4978232383728, 89.75708651542664, 91.01634979248047, 92.30666279792786, 93.59697580337524, 94.8611650466919, 96.12535429000854, 97.37435865402222, 98.62336301803589, 99.90428495407104, 101.1852068901062, 102.4552891254425, 103.72537136077881, 104.9905366897583, 106.2557020187378, 107.5377516746521, 108.8198013305664, 110.09031224250793, 111.36082315444946, 112.6289074420929, 113.89699172973633, 115.18553614616394, 116.47408056259155, 117.7325394153595, 118.99099826812744, 120.28873062133789, 121.58646297454834, 122.85309767723083, 124.11973237991333, 125.3725516796112, 126.62537097930908, 127.95750856399536, 129.28964614868164, 130.54109525680542, 131.7925443649292, 133.06105279922485, 134.3295612335205, 135.66825795173645, 137.0069546699524, 138.2692105770111, 139.53146648406982, 140.83600878715515, 142.14055109024048, 143.46897149085999, 144.7973918914795, 146.06334519386292, 147.32929849624634, 148.58477783203125, 149.84025716781616, 151.17124342918396, 152.50222969055176, 153.7659547328949, 155.02967977523804, 156.2961151599884, 157.56255054473877, 158.85743761062622, 160.15232467651367, 161.41690683364868, 162.6814889907837, 163.98662567138672, 165.29176235198975, 166.57829880714417, 167.86483526229858, 169.10131859779358, 170.33780193328857, 171.6386377811432, 172.9394736289978, 174.2112557888031, 175.4830379486084, 176.73761630058289, 177.99219465255737, 179.26726865768433, 180.54234266281128, 181.8045711517334, 183.06679964065552, 184.32278394699097, 185.57876825332642, 186.9226462841034, 188.26652431488037, 189.5359399318695, 190.80535554885864, 192.07426857948303, 193.34318161010742, 194.68778610229492, 196.03239059448242, 197.30153918266296, 198.5706877708435, 199.8390200138092, 201.1073522567749, 202.44640159606934, 203.78545093536377, 205.05570363998413, 206.3259563446045, 207.58794927597046, 208.84994220733643, 210.1085295677185, 211.3671169281006, 212.67769360542297, 213.98827028274536, 215.2542848587036, 216.52029943466187, 217.86289858818054, 219.20549774169922, 220.46685481071472, 221.72821187973022, 222.99168348312378, 224.25515508651733, 225.53029465675354, 226.80543422698975, 228.0878825187683, 229.37033081054688, 230.64758729934692, 231.92484378814697, 233.20003271102905, 234.47522163391113, 235.755024433136, 237.03482723236084, 238.29042053222656, 239.54601383209229, 240.8423457145691, 242.1386775970459, 243.41155529022217, 244.68443298339844, 245.95788955688477, 247.2313461303711, 248.50846433639526, 249.78558254241943, 251.06275391578674, 252.33992528915405, 253.6112084388733, 254.88249158859253, 256.2278275489807, 257.5731635093689, 258.8593454360962, 260.1455273628235, 261.4252579212189, 262.70498847961426, 264.9458770751953, 267.18676567077637]
[31.033333333333335, 31.033333333333335, 48.583333333333336, 48.583333333333336, 59.35, 59.35, 63.46666666666667, 63.46666666666667, 65.94166666666666, 65.94166666666666, 66.45833333333333, 66.45833333333333, 67.69166666666666, 67.69166666666666, 68.19166666666666, 68.19166666666666, 69.61666666666666, 69.61666666666666, 70.525, 70.525, 72.43333333333334, 72.43333333333334, 72.85, 72.85, 73.71666666666667, 73.71666666666667, 74.03333333333333, 74.03333333333333, 74.53333333333333, 74.53333333333333, 75.00833333333334, 75.00833333333334, 74.3, 74.3, 76.23333333333333, 76.23333333333333, 75.825, 75.825, 76.06666666666666, 76.06666666666666, 75.88333333333334, 75.88333333333334, 75.99166666666666, 75.99166666666666, 75.125, 75.125, 76.375, 76.375, 77.71666666666667, 77.71666666666667, 78.79166666666667, 78.79166666666667, 78.68333333333334, 78.68333333333334, 78.725, 78.725, 78.525, 78.525, 78.325, 78.325, 78.10833333333333, 78.10833333333333, 77.73333333333333, 77.73333333333333, 77.28333333333333, 77.28333333333333, 78.41666666666667, 78.41666666666667, 78.45, 78.45, 79.31666666666666, 79.31666666666666, 80.025, 80.025, 80.06666666666666, 80.06666666666666, 79.25833333333334, 79.25833333333334, 79.55, 79.55, 79.575, 79.575, 79.36666666666666, 79.36666666666666, 79.36666666666666, 79.36666666666666, 79.50833333333334, 79.50833333333334, 79.23333333333333, 79.23333333333333, 79.24166666666666, 79.24166666666666, 80.225, 80.225, 80.05833333333334, 80.05833333333334, 80.975, 80.975, 81.3, 81.3, 81.35, 81.35, 81.25, 81.25, 81.725, 81.725, 81.25833333333334, 81.25833333333334, 80.91666666666667, 80.91666666666667, 80.45833333333333, 80.45833333333333, 80.35833333333333, 80.35833333333333, 80.84166666666667, 80.84166666666667, 80.54166666666667, 80.54166666666667, 81.025, 81.025, 80.43333333333334, 80.43333333333334, 80.9, 80.9, 81.51666666666667, 81.51666666666667, 81.65833333333333, 81.65833333333333, 80.71666666666667, 80.71666666666667, 79.65833333333333, 79.65833333333333, 79.98333333333333, 79.98333333333333, 80.06666666666666, 80.06666666666666, 80.13333333333334, 80.13333333333334, 79.925, 79.925, 79.84166666666667, 79.84166666666667, 80.41666666666667, 80.41666666666667, 80.825, 80.825, 80.575, 80.575, 81.01666666666667, 81.01666666666667, 81.25, 81.25, 81.39166666666667, 81.39166666666667, 81.75833333333334, 81.75833333333334, 81.175, 81.175, 81.33333333333333, 81.33333333333333, 81.56666666666666, 81.56666666666666, 81.04166666666667, 81.04166666666667, 81.74166666666666, 81.74166666666666, 82.06666666666666, 82.06666666666666, 82.125, 82.125, 82.125, 82.125, 82.2, 82.2, 81.975, 81.975, 82.21666666666667, 82.21666666666667, 81.86666666666666, 81.86666666666666, 81.46666666666667, 81.46666666666667, 82.04166666666667, 82.04166666666667, 81.43333333333334, 81.43333333333334, 82.175, 82.175, 81.45, 81.45, 81.33333333333333, 81.33333333333333, 81.43333333333334, 81.43333333333334, 81.20833333333333, 81.20833333333333, 81.53333333333333, 81.53333333333333, 81.725, 81.725, 81.54166666666667, 81.54166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2200
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.5917
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0217
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4567
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5683
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.5617
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3567
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6450
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.236, Test loss: 0.575, Test accuracy: 81.81
Final Round, Global train loss: 0.236, Global test loss: 1.406, Global test accuracy: 58.88
Average accuracy final 10 rounds: 81.87083333333334 

Average global accuracy final 10 rounds: 60.039166666666674 

1932.3778190612793
[1.8066482543945312, 3.6132965087890625, 5.254975318908691, 6.89665412902832, 8.547063112258911, 10.197472095489502, 11.83715009689331, 13.47682809829712, 15.113670825958252, 16.750513553619385, 18.46420645713806, 20.17789936065674, 21.731703519821167, 23.285507678985596, 25.002135276794434, 26.71876287460327, 28.436591148376465, 30.154419422149658, 31.86663818359375, 33.57885694503784, 35.26536250114441, 36.95186805725098, 38.604955196380615, 40.258042335510254, 41.93112659454346, 43.60421085357666, 45.371985912323, 47.139760971069336, 48.99284815788269, 50.845935344696045, 52.71475887298584, 54.583582401275635, 56.520228147506714, 58.45687389373779, 60.32262349128723, 62.18837308883667, 64.05721807479858, 65.9260630607605, 67.54979515075684, 69.17352724075317, 70.7946503162384, 72.41577339172363, 74.0152530670166, 75.61473274230957, 77.22838234901428, 78.842031955719, 80.3672947883606, 81.8925576210022, 83.46511316299438, 85.03766870498657, 86.59879493713379, 88.159921169281, 89.72388362884521, 91.28784608840942, 92.84977865219116, 94.4117112159729, 96.20189356803894, 97.99207592010498, 99.55921459197998, 101.12635326385498, 102.68887662887573, 104.25139999389648, 105.83002376556396, 107.40864753723145, 109.03370451927185, 110.65876150131226, 112.23266983032227, 113.80657815933228, 115.37422966957092, 116.94188117980957, 118.583016872406, 120.22415256500244, 121.87656998634338, 123.52898740768433, 125.11470913887024, 126.70043087005615, 128.28958702087402, 129.8787431716919, 131.52932620048523, 133.17990922927856, 134.83285856246948, 136.4858078956604, 137.98557233810425, 139.4853367805481, 141.0744936466217, 142.6636505126953, 144.29704523086548, 145.93043994903564, 147.56798386573792, 149.20552778244019, 150.5517578125, 151.89798784255981, 153.52701473236084, 155.15604162216187, 156.69637513160706, 158.23670864105225, 159.7343873977661, 161.23206615447998, 162.73534178733826, 164.23861742019653, 165.7148096561432, 167.19100189208984, 168.6994504928589, 170.20789909362793, 171.73133993148804, 173.25478076934814, 174.75625038146973, 176.2577199935913, 177.79460310935974, 179.33148622512817, 180.84422326087952, 182.35696029663086, 183.84689474105835, 185.33682918548584, 186.81948065757751, 188.3021321296692, 189.81534576416016, 191.32855939865112, 192.79867243766785, 194.26878547668457, 195.7559859752655, 197.24318647384644, 198.59547591209412, 199.9477653503418, 201.2998070716858, 202.65184879302979, 204.0182056427002, 205.3845624923706, 206.73691272735596, 208.0892629623413, 209.43654894828796, 210.78383493423462, 212.1613028049469, 213.53877067565918, 214.8925895690918, 216.2464084625244, 217.60858416557312, 218.97075986862183, 220.3698709011078, 221.76898193359375, 223.09898209571838, 224.42898225784302, 225.82523846626282, 227.22149467468262, 228.596107006073, 229.97071933746338, 231.32768440246582, 232.68464946746826, 234.04167890548706, 235.39870834350586, 236.76636719703674, 238.13402605056763, 239.4818513393402, 240.8296766281128, 242.19753742218018, 243.56539821624756, 244.92265057563782, 246.27990293502808, 247.63269066810608, 248.98547840118408, 250.3987259864807, 251.81197357177734, 253.17295145988464, 254.53392934799194, 255.88346457481384, 257.23299980163574, 258.645733833313, 260.05846786499023, 261.43327498435974, 262.80808210372925, 264.174663066864, 265.5412440299988, 266.8980166912079, 268.254789352417, 269.60517477989197, 270.95556020736694, 272.3489623069763, 273.7423644065857, 275.151624917984, 276.5608854293823, 278.0012516975403, 279.44161796569824, 280.8136706352234, 282.18572330474854, 283.5401349067688, 284.89454650878906, 286.3016996383667, 287.70885276794434, 289.16622710227966, 290.623601436615, 292.0593755245209, 293.49514961242676, 294.9441123008728, 296.39307498931885, 297.8500144481659, 299.30695390701294, 300.66025161743164, 302.01354932785034, 303.41711354255676, 304.8206777572632, 307.17933917045593, 309.5380005836487]
[28.016666666666666, 28.016666666666666, 47.325, 47.325, 56.575, 56.575, 60.208333333333336, 60.208333333333336, 62.358333333333334, 62.358333333333334, 64.35, 64.35, 66.23333333333333, 66.23333333333333, 67.04166666666667, 67.04166666666667, 68.125, 68.125, 69.425, 69.425, 73.125, 73.125, 73.34166666666667, 73.34166666666667, 73.44166666666666, 73.44166666666666, 73.4, 73.4, 74.31666666666666, 74.31666666666666, 74.6, 74.6, 74.66666666666667, 74.66666666666667, 74.98333333333333, 74.98333333333333, 75.25833333333334, 75.25833333333334, 75.875, 75.875, 76.25, 76.25, 76.15833333333333, 76.15833333333333, 75.9, 75.9, 75.81666666666666, 75.81666666666666, 76.38333333333334, 76.38333333333334, 76.34166666666667, 76.34166666666667, 76.325, 76.325, 76.94166666666666, 76.94166666666666, 77.46666666666667, 77.46666666666667, 76.65833333333333, 76.65833333333333, 76.55, 76.55, 76.85833333333333, 76.85833333333333, 78.08333333333333, 78.08333333333333, 78.53333333333333, 78.53333333333333, 78.775, 78.775, 78.83333333333333, 78.83333333333333, 78.9, 78.9, 78.71666666666667, 78.71666666666667, 77.73333333333333, 77.73333333333333, 78.05833333333334, 78.05833333333334, 78.25833333333334, 78.25833333333334, 78.225, 78.225, 78.49166666666666, 78.49166666666666, 78.35833333333333, 78.35833333333333, 78.05, 78.05, 78.86666666666666, 78.86666666666666, 79.84166666666667, 79.84166666666667, 79.91666666666667, 79.91666666666667, 80.075, 80.075, 79.5, 79.5, 80.18333333333334, 80.18333333333334, 79.85, 79.85, 79.76666666666667, 79.76666666666667, 79.9, 79.9, 79.09166666666667, 79.09166666666667, 79.48333333333333, 79.48333333333333, 80.59166666666667, 80.59166666666667, 80.0, 80.0, 79.1, 79.1, 79.95, 79.95, 79.525, 79.525, 80.425, 80.425, 80.1, 80.1, 80.06666666666666, 80.06666666666666, 80.03333333333333, 80.03333333333333, 79.69166666666666, 79.69166666666666, 80.55833333333334, 80.55833333333334, 80.9, 80.9, 80.46666666666667, 80.46666666666667, 81.14166666666667, 81.14166666666667, 80.725, 80.725, 80.475, 80.475, 80.9, 80.9, 81.39166666666667, 81.39166666666667, 81.075, 81.075, 80.66666666666667, 80.66666666666667, 80.51666666666667, 80.51666666666667, 80.35833333333333, 80.35833333333333, 80.025, 80.025, 80.875, 80.875, 80.525, 80.525, 80.78333333333333, 80.78333333333333, 81.14166666666667, 81.14166666666667, 80.99166666666666, 80.99166666666666, 81.60833333333333, 81.60833333333333, 81.45833333333333, 81.45833333333333, 81.56666666666666, 81.56666666666666, 81.71666666666667, 81.71666666666667, 82.21666666666667, 82.21666666666667, 81.65, 81.65, 81.45833333333333, 81.45833333333333, 81.375, 81.375, 81.58333333333333, 81.58333333333333, 81.675, 81.675, 82.1, 82.1, 82.25833333333334, 82.25833333333334, 82.50833333333334, 82.50833333333334, 82.65, 82.65, 81.49166666666666, 81.49166666666666, 81.60833333333333, 81.60833333333333, 81.80833333333334, 81.80833333333334]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2200
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.5883
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0217
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4567
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5550
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4950
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3567
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6550
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2200
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.5883
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0217
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4567
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5550
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4950
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3567
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6450
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  13.5800
Round 1 global test acc  21.3800
Round 2 global test acc  22.1200
Round 3 global test acc  25.1800
Round 4 global test acc  15.7200
Round 5 global test acc  31.7700
Round 6 global test acc  21.0300
Round 7 global test acc  26.5400
Round 8 global test acc  18.9500
Round 9 global test acc  24.9500
Round 10 global test acc  16.3400
Round 11 global test acc  18.5300
Round 12 global test acc  20.9100
Round 13 global test acc  31.2400
Round 14 global test acc  27.4100
Round 15 global test acc  26.9900
Round 16 global test acc  26.5900
Round 17 global test acc  32.7500
Round 18 global test acc  34.1900
Round 19 global test acc  23.4000
Round 20 global test acc  26.9900
Round 21 global test acc  27.6500
Round 22 global test acc  25.3600
Round 23 global test acc  30.3600
Round 24 global test acc  25.0700
Round 25 global test acc  29.8200
Round 26 global test acc  29.2700
Round 27 global test acc  25.3300
Round 28 global test acc  22.8400
Round 29 global test acc  27.6400
Round 30 global test acc  29.9300
Round 31 global test acc  23.7700
Round 32 global test acc  33.3200
Round 33 global test acc  28.2200
Round 34 global test acc  33.3500
Round 35 global test acc  26.8700
Round 36 global test acc  26.4000
Round 37 global test acc  23.7100
Round 38 global test acc  35.9700
Round 39 global test acc  23.4400
Round 40 global test acc  29.7500
Round 41 global test acc  29.4200
Round 42 global test acc  28.9800
Round 43 global test acc  24.1800
Round 44 global test acc  25.8500
Round 45 global test acc  24.6000
Round 46 global test acc  27.4500
Round 47 global test acc  37.5700
Round 48 global test acc  24.8200
Round 49 global test acc  25.6100
Round 50 global test acc  31.2900
Round 51 global test acc  26.7700
Round 52 global test acc  30.3000
Round 53 global test acc  28.2700
Round 54 global test acc  36.0200
Round 55 global test acc  32.3700
Round 56 global test acc  25.9100
Round 57 global test acc  30.4000
Round 58 global test acc  30.0500
Round 59 global test acc  35.3100
Round 60 global test acc  34.3700
Round 61 global test acc  33.8100
Round 62 global test acc  40.0800
Round 63 global test acc  37.3800
Round 64 global test acc  29.3000
Round 65 global test acc  31.4800
Round 66 global test acc  27.2800
Round 67 global test acc  31.2700
Round 68 global test acc  34.5500
Round 69 global test acc  33.3600
Round 70 global test acc  32.1900
Round 71 global test acc  26.8000
Round 72 global test acc  31.6100
Round 73 global test acc  27.4300
Round 74 global test acc  34.9800
Round 75 global test acc  32.4900
Round 76 global test acc  36.1800
Round 77 global test acc  43.7700
Round 78 global test acc  41.1700
Round 79 global test acc  37.1900
Round 80 global test acc  32.6000
Round 81 global test acc  27.4700
Round 82 global test acc  26.0300
Round 83 global test acc  24.4900
Round 84 global test acc  23.6800
Round 85 global test acc  22.5500
Round 86 global test acc  21.8000
Round 87 global test acc  20.9600
Round 88 global test acc  20.2800
Round 89 global test acc  19.8300
Round 90 global test acc  19.4700
Round 91 global test acc  19.8900
Round 92 global test acc  19.2200
Round 93 global test acc  18.9000
Round 94 global test acc  18.7300
Round 95 global test acc  18.6900
Round 96 global test acc  18.1700
Round 97 global test acc  17.6400
Round 98 global test acc  17.1300
Round 99 global test acc  17.0400
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2200
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.5883
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0217
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4567
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5550
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4950
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4650
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6533
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.337, Test loss: 0.479, Test accuracy: 80.92
Average accuracy final 10 rounds: 81.40833333333333
1327.9971477985382
[1.883173942565918, 3.4594545364379883, 5.025841951370239, 6.539039134979248, 8.086883306503296, 9.648778200149536, 11.22026777267456, 12.73739504814148, 14.297114610671997, 15.861423015594482, 17.438185214996338, 19.0595121383667, 20.727989196777344, 22.418708324432373, 24.037936449050903, 25.662859439849854, 27.28189444541931, 28.917540550231934, 30.586384773254395, 32.19222640991211, 33.824848651885986, 35.4809935092926, 37.038925886154175, 38.60993146896362, 40.286537170410156, 41.89649772644043, 43.58006286621094, 45.294355154037476, 47.006004095077515, 48.702061891555786, 50.366589307785034, 52.05524134635925, 53.74681210517883, 55.44269919395447, 57.10871386528015, 58.781880378723145, 60.50092267990112, 62.158154249191284, 63.802014112472534, 65.50027132034302, 67.22035765647888, 68.89810490608215, 70.60133862495422, 72.32269334793091, 74.01059794425964, 75.66200089454651, 77.3505654335022, 79.11209630966187, 80.8066520690918, 82.37774634361267, 83.98838114738464, 85.56440043449402, 87.10298347473145, 88.65040159225464, 90.30296587944031, 91.8580276966095, 93.42063403129578, 94.98252701759338, 96.54845857620239, 98.11897420883179, 99.6944329738617, 101.29975748062134, 102.9330506324768, 104.59723830223083, 106.32165217399597, 107.95943880081177, 109.6575858592987, 111.303307056427, 112.96281361579895, 114.5355052947998, 116.19761228561401, 117.79209160804749, 119.35495519638062, 120.99403309822083, 122.56393694877625, 124.12484502792358, 125.69841814041138, 127.40559411048889, 128.96185994148254, 130.64005637168884, 132.37403059005737, 134.0734097957611, 135.74019742012024, 137.4226484298706, 139.09749746322632, 140.77374148368835, 142.44516897201538, 144.09562420845032, 145.82033109664917, 147.41264176368713, 149.0692138671875, 150.76292777061462, 152.42569375038147, 154.11603212356567, 155.77422881126404, 157.43354892730713, 159.0973711013794, 160.82029342651367, 162.38847017288208, 163.99006724357605, 166.16917061805725]
[21.241666666666667, 33.608333333333334, 50.708333333333336, 52.583333333333336, 55.11666666666667, 59.25, 61.75833333333333, 65.24166666666666, 67.66666666666667, 68.98333333333333, 71.2, 71.40833333333333, 72.06666666666666, 71.63333333333334, 72.15833333333333, 72.85, 73.2, 74.30833333333334, 74.90833333333333, 75.96666666666667, 75.06666666666666, 75.38333333333334, 75.50833333333334, 74.675, 75.26666666666667, 76.34166666666667, 76.64166666666667, 76.31666666666666, 76.275, 76.5, 77.41666666666667, 78.55, 77.525, 77.40833333333333, 77.71666666666667, 78.75, 78.61666666666666, 78.775, 78.64166666666667, 79.16666666666667, 79.33333333333333, 80.15, 79.875, 80.05, 79.65, 79.51666666666667, 80.44166666666666, 80.85833333333333, 80.44166666666666, 79.66666666666667, 80.69166666666666, 80.58333333333333, 79.35, 80.05, 80.93333333333334, 80.15, 80.76666666666667, 80.53333333333333, 80.19166666666666, 79.81666666666666, 79.40833333333333, 80.65, 80.03333333333333, 80.15833333333333, 80.65833333333333, 80.19166666666666, 81.14166666666667, 82.06666666666666, 81.55833333333334, 82.20833333333333, 82.00833333333334, 81.08333333333333, 81.89166666666667, 81.58333333333333, 80.55, 80.95833333333333, 80.60833333333333, 80.98333333333333, 80.26666666666667, 80.98333333333333, 81.03333333333333, 80.75, 80.4, 80.75, 81.15833333333333, 81.26666666666667, 81.00833333333334, 80.95, 80.50833333333334, 80.99166666666666, 80.56666666666666, 81.16666666666667, 81.075, 81.2, 81.35833333333333, 81.55, 81.35833333333333, 81.45, 82.09166666666667, 82.26666666666667, 80.91666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3650
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.5883
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0217
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4567
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5550
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4950
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.3567
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6450
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1977, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train_local):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 56125 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.6617
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4683
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6350
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3617
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5200
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6300
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3467
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.6183
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5017
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6700
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.232, Test loss: 2.124, Test accuracy: 54.63
Final Round, Global train loss: 0.232, Global test loss: 2.144, Global test accuracy: 28.64
Average accuracy final 10 rounds: 55.375 

Average global accuracy final 10 rounds: 21.659166666666664 

1800.6045246124268
[1.65690016746521, 3.31380033493042, 4.739489793777466, 6.165179252624512, 7.517734050750732, 8.870288848876953, 10.192299604415894, 11.514310359954834, 12.812883377075195, 14.111456394195557, 15.484700441360474, 16.85794448852539, 18.252541303634644, 19.647138118743896, 21.092782020568848, 22.5384259223938, 23.99160885810852, 25.444791793823242, 26.843582153320312, 28.242372512817383, 29.618876934051514, 30.995381355285645, 32.408660888671875, 33.821940422058105, 35.19988775253296, 36.57783508300781, 37.934550762176514, 39.291266441345215, 40.577396869659424, 41.86352729797363, 43.125983476638794, 44.388439655303955, 45.66684126853943, 46.9452428817749, 48.2373321056366, 49.52942132949829, 50.79316782951355, 52.05691432952881, 53.34464716911316, 54.63238000869751, 55.96046423912048, 57.28854846954346, 58.57347345352173, 59.8583984375, 61.128207206726074, 62.39801597595215, 63.714162826538086, 65.03030967712402, 66.34526920318604, 67.66022872924805, 68.94926762580872, 70.23830652236938, 71.564453125, 72.89059972763062, 74.21276783943176, 75.53493595123291, 76.80835771560669, 78.08177947998047, 79.38038301467896, 80.67898654937744, 81.93580675125122, 83.192626953125, 84.46844792366028, 85.74426889419556, 87.05644822120667, 88.36862754821777, 89.72047591209412, 91.07232427597046, 92.36577844619751, 93.65923261642456, 94.99735999107361, 96.33548736572266, 97.59988069534302, 98.86427402496338, 100.16286182403564, 101.46144962310791, 102.866450548172, 104.27145147323608, 105.52487659454346, 106.77830171585083, 108.1188473701477, 109.45939302444458, 110.78700971603394, 112.11462640762329, 113.39168334007263, 114.66874027252197, 115.9644832611084, 117.26022624969482, 118.54609298706055, 119.83195972442627, 121.09996724128723, 122.3679747581482, 123.6699275970459, 124.9718804359436, 126.25761532783508, 127.54335021972656, 128.83915257453918, 130.1349549293518, 131.42671966552734, 132.71848440170288, 133.9965796470642, 135.27467489242554, 136.54076480865479, 137.80685472488403, 139.1057162284851, 140.40457773208618, 141.72004556655884, 143.0355134010315, 144.30368733406067, 145.57186126708984, 146.88210463523865, 148.19234800338745, 149.48560285568237, 150.7788577079773, 152.09886956214905, 153.4188814163208, 154.70502614974976, 155.9911708831787, 157.28791999816895, 158.58466911315918, 159.87691831588745, 161.16916751861572, 162.44871473312378, 163.72826194763184, 165.01897168159485, 166.30968141555786, 167.6446692943573, 168.97965717315674, 170.27846693992615, 171.57727670669556, 172.85254955291748, 174.1278223991394, 175.4098150730133, 176.6918077468872, 177.9682331085205, 179.2446584701538, 180.48397636413574, 181.72329425811768, 183.0158407688141, 184.3083872795105, 185.57399940490723, 186.83961153030396, 188.11031293869019, 189.38101434707642, 190.6912055015564, 192.00139665603638, 193.2614598274231, 194.52152299880981, 195.86882328987122, 197.21612358093262, 198.5560266971588, 199.895929813385, 201.14079451560974, 202.38565921783447, 203.68301463127136, 204.98037004470825, 206.28555011749268, 207.5907301902771, 208.84316778182983, 210.09560537338257, 211.35933136940002, 212.62305736541748, 213.88455367088318, 215.14604997634888, 216.38913655281067, 217.63222312927246, 218.91487002372742, 220.19751691818237, 221.49239134788513, 222.7872657775879, 224.05272102355957, 225.31817626953125, 226.59695410728455, 227.87573194503784, 229.14063143730164, 230.40553092956543, 231.94967198371887, 233.49381303787231, 234.76911997795105, 236.04442691802979, 237.3062219619751, 238.5680170059204, 239.8216037750244, 241.07519054412842, 242.35046887397766, 243.6257472038269, 244.8767523765564, 246.1277575492859, 247.39741969108582, 248.66708183288574, 249.94911360740662, 251.2311453819275, 252.50192308425903, 253.77270078659058, 255.04964017868042, 256.32657957077026, 257.6187560558319, 258.91093254089355, 260.1662874221802, 261.4216423034668, 263.6240539550781, 265.82646560668945]
[19.258333333333333, 19.258333333333333, 33.575, 33.575, 38.40833333333333, 38.40833333333333, 44.85, 44.85, 49.483333333333334, 49.483333333333334, 50.56666666666667, 50.56666666666667, 50.6, 50.6, 53.15, 53.15, 55.608333333333334, 55.608333333333334, 56.25833333333333, 56.25833333333333, 55.2, 55.2, 55.56666666666667, 55.56666666666667, 55.7, 55.7, 57.53333333333333, 57.53333333333333, 56.95, 56.95, 57.05833333333333, 57.05833333333333, 56.93333333333333, 56.93333333333333, 57.49166666666667, 57.49166666666667, 57.858333333333334, 57.858333333333334, 58.15833333333333, 58.15833333333333, 57.483333333333334, 57.483333333333334, 57.45, 57.45, 56.53333333333333, 56.53333333333333, 56.791666666666664, 56.791666666666664, 57.975, 57.975, 59.083333333333336, 59.083333333333336, 58.28333333333333, 58.28333333333333, 57.65, 57.65, 57.44166666666667, 57.44166666666667, 57.7, 57.7, 58.05833333333333, 58.05833333333333, 57.88333333333333, 57.88333333333333, 59.0, 59.0, 58.40833333333333, 58.40833333333333, 58.15, 58.15, 58.025, 58.025, 58.541666666666664, 58.541666666666664, 59.00833333333333, 59.00833333333333, 59.18333333333333, 59.18333333333333, 59.083333333333336, 59.083333333333336, 58.833333333333336, 58.833333333333336, 57.94166666666667, 57.94166666666667, 57.575, 57.575, 57.15833333333333, 57.15833333333333, 57.291666666666664, 57.291666666666664, 57.016666666666666, 57.016666666666666, 57.35, 57.35, 57.65, 57.65, 57.25, 57.25, 58.016666666666666, 58.016666666666666, 57.05, 57.05, 57.166666666666664, 57.166666666666664, 56.55, 56.55, 57.53333333333333, 57.53333333333333, 58.00833333333333, 58.00833333333333, 58.25833333333333, 58.25833333333333, 57.458333333333336, 57.458333333333336, 57.358333333333334, 57.358333333333334, 57.46666666666667, 57.46666666666667, 56.63333333333333, 56.63333333333333, 56.06666666666667, 56.06666666666667, 56.025, 56.025, 57.016666666666666, 57.016666666666666, 57.291666666666664, 57.291666666666664, 57.1, 57.1, 57.43333333333333, 57.43333333333333, 57.74166666666667, 57.74166666666667, 56.916666666666664, 56.916666666666664, 56.99166666666667, 56.99166666666667, 56.94166666666667, 56.94166666666667, 56.225, 56.225, 56.35, 56.35, 56.03333333333333, 56.03333333333333, 56.96666666666667, 56.96666666666667, 56.09166666666667, 56.09166666666667, 55.94166666666667, 55.94166666666667, 55.68333333333333, 55.68333333333333, 55.733333333333334, 55.733333333333334, 56.00833333333333, 56.00833333333333, 56.50833333333333, 56.50833333333333, 56.90833333333333, 56.90833333333333, 56.708333333333336, 56.708333333333336, 55.99166666666667, 55.99166666666667, 56.00833333333333, 56.00833333333333, 55.90833333333333, 55.90833333333333, 55.94166666666667, 55.94166666666667, 55.53333333333333, 55.53333333333333, 55.88333333333333, 55.88333333333333, 55.69166666666667, 55.69166666666667, 55.49166666666667, 55.49166666666667, 55.85, 55.85, 55.50833333333333, 55.50833333333333, 55.225, 55.225, 55.125, 55.125, 55.25833333333333, 55.25833333333333, 54.90833333333333, 54.90833333333333, 55.21666666666667, 55.21666666666667, 55.25833333333333, 55.25833333333333, 55.641666666666666, 55.641666666666666, 55.75833333333333, 55.75833333333333, 54.63333333333333, 54.63333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.6617
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4683
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6350
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3617
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.6083
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6300
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3467
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5983
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5017
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6700
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.271, Test loss: 0.942, Test accuracy: 75.37
Final Round, Global train loss: 0.271, Global test loss: 1.475, Global test accuracy: 52.63
Average accuracy final 10 rounds: 75.43916666666667 

Average global accuracy final 10 rounds: 51.82749999999999 

1809.111374616623
[1.7844316959381104, 3.5688633918762207, 5.00078010559082, 6.43269681930542, 7.856048583984375, 9.27940034866333, 10.706563234329224, 12.133726119995117, 13.471889972686768, 14.810053825378418, 16.266569137573242, 17.723084449768066, 19.190654277801514, 20.65822410583496, 21.99774193763733, 23.337259769439697, 24.67295002937317, 26.00864028930664, 27.33084797859192, 28.653055667877197, 29.951616287231445, 31.250176906585693, 32.54857325553894, 33.84696960449219, 35.2243766784668, 36.601783752441406, 38.015116930007935, 39.42845010757446, 40.85685086250305, 42.28525161743164, 43.71851634979248, 45.15178108215332, 46.553993701934814, 47.95620632171631, 49.36977815628052, 50.78334999084473, 52.05599641799927, 53.32864284515381, 54.585036277770996, 55.841429710388184, 57.10494327545166, 58.36845684051514, 59.6404755115509, 60.91249418258667, 62.18834066390991, 63.464187145233154, 64.73669481277466, 66.00920248031616, 67.28054428100586, 68.55188608169556, 69.83488607406616, 71.11788606643677, 72.57382774353027, 74.02976942062378, 75.42850637435913, 76.82724332809448, 78.23488450050354, 79.6425256729126, 81.08056378364563, 82.51860189437866, 83.93858027458191, 85.35855865478516, 86.80316638946533, 88.24777412414551, 89.69087171554565, 91.1339693069458, 92.53716993331909, 93.94037055969238, 95.3913345336914, 96.84229850769043, 98.11221504211426, 99.38213157653809, 100.67069864273071, 101.95926570892334, 103.24818825721741, 104.53711080551147, 105.82206702232361, 107.10702323913574, 108.41067266464233, 109.71432209014893, 111.01523351669312, 112.3161449432373, 113.61280846595764, 114.90947198867798, 116.19932103157043, 117.48917007446289, 118.79136347770691, 120.09355688095093, 121.39357781410217, 122.69359874725342, 123.99259209632874, 125.29158544540405, 126.61750960350037, 127.94343376159668, 129.2470474243164, 130.55066108703613, 131.9242708683014, 133.29788064956665, 134.58333563804626, 135.86879062652588, 137.1772599220276, 138.4857292175293, 139.77547025680542, 141.06521129608154, 142.34722924232483, 143.62924718856812, 144.9079110622406, 146.1865749359131, 147.47962260246277, 148.77267026901245, 150.06123971939087, 151.3498091697693, 152.6338586807251, 153.9179081916809, 155.21278953552246, 156.507670879364, 157.8168044090271, 159.12593793869019, 160.42438626289368, 161.72283458709717, 163.26931595802307, 164.81579732894897, 166.1024351119995, 167.38907289505005, 168.65698432922363, 169.92489576339722, 171.21323251724243, 172.50156927108765, 173.77526545524597, 175.0489616394043, 176.33215618133545, 177.6153507232666, 178.89045882225037, 180.16556692123413, 181.42777633666992, 182.6899857521057, 183.94558882713318, 185.20119190216064, 186.4477550983429, 187.69431829452515, 188.93114733695984, 190.16797637939453, 191.41740131378174, 192.66682624816895, 193.9248559474945, 195.18288564682007, 196.4175021648407, 197.65211868286133, 198.89653539657593, 200.14095211029053, 201.3867871761322, 202.63262224197388, 203.88155341148376, 205.13048458099365, 206.37035942077637, 207.61023426055908, 208.8615846633911, 210.11293506622314, 211.36145782470703, 212.60998058319092, 213.8488085269928, 215.08763647079468, 216.3407700061798, 217.59390354156494, 218.83313059806824, 220.07235765457153, 221.31488752365112, 222.5574173927307, 223.79720401763916, 225.0369906425476, 226.27742075920105, 227.5178508758545, 228.76444721221924, 230.01104354858398, 231.25489354133606, 232.49874353408813, 233.73754143714905, 234.97633934020996, 236.22972512245178, 237.4831109046936, 238.75805926322937, 240.03300762176514, 241.31476259231567, 242.5965175628662, 243.88446760177612, 245.17241764068604, 246.4582633972168, 247.74410915374756, 249.01226115226746, 250.28041315078735, 251.5591220855713, 252.83783102035522, 254.1132926940918, 255.38875436782837, 256.66452407836914, 257.9402937889099, 259.22570872306824, 260.51112365722656, 261.793918132782, 263.0767126083374, 265.20359206199646, 267.3304715156555]
[23.883333333333333, 23.883333333333333, 35.03333333333333, 35.03333333333333, 50.958333333333336, 50.958333333333336, 55.21666666666667, 55.21666666666667, 63.05833333333333, 63.05833333333333, 64.35, 64.35, 64.30833333333334, 64.30833333333334, 68.43333333333334, 68.43333333333334, 68.39166666666667, 68.39166666666667, 69.30833333333334, 69.30833333333334, 67.70833333333333, 67.70833333333333, 68.73333333333333, 68.73333333333333, 69.20833333333333, 69.20833333333333, 70.15, 70.15, 72.225, 72.225, 72.81666666666666, 72.81666666666666, 72.73333333333333, 72.73333333333333, 72.98333333333333, 72.98333333333333, 73.525, 73.525, 73.68333333333334, 73.68333333333334, 72.975, 72.975, 73.23333333333333, 73.23333333333333, 73.675, 73.675, 73.53333333333333, 73.53333333333333, 72.7, 72.7, 73.73333333333333, 73.73333333333333, 73.76666666666667, 73.76666666666667, 74.54166666666667, 74.54166666666667, 74.95, 74.95, 74.625, 74.625, 74.675, 74.675, 74.69166666666666, 74.69166666666666, 75.03333333333333, 75.03333333333333, 74.81666666666666, 74.81666666666666, 74.53333333333333, 74.53333333333333, 74.56666666666666, 74.56666666666666, 74.525, 74.525, 74.3, 74.3, 74.35833333333333, 74.35833333333333, 74.61666666666666, 74.61666666666666, 74.275, 74.275, 74.04166666666667, 74.04166666666667, 74.13333333333334, 74.13333333333334, 74.75, 74.75, 75.0, 75.0, 74.7, 74.7, 74.7, 74.7, 74.91666666666667, 74.91666666666667, 75.14166666666667, 75.14166666666667, 75.80833333333334, 75.80833333333334, 74.81666666666666, 74.81666666666666, 75.18333333333334, 75.18333333333334, 75.025, 75.025, 75.05833333333334, 75.05833333333334, 74.95833333333333, 74.95833333333333, 74.33333333333333, 74.33333333333333, 74.7, 74.7, 74.825, 74.825, 75.325, 75.325, 75.46666666666667, 75.46666666666667, 75.84166666666667, 75.84166666666667, 75.14166666666667, 75.14166666666667, 75.4, 75.4, 74.625, 74.625, 74.69166666666666, 74.69166666666666, 75.14166666666667, 75.14166666666667, 74.61666666666666, 74.61666666666666, 73.96666666666667, 73.96666666666667, 74.13333333333334, 74.13333333333334, 74.56666666666666, 74.56666666666666, 75.48333333333333, 75.48333333333333, 75.175, 75.175, 75.2, 75.2, 74.95, 74.95, 74.90833333333333, 74.90833333333333, 74.73333333333333, 74.73333333333333, 75.76666666666667, 75.76666666666667, 76.01666666666667, 76.01666666666667, 74.575, 74.575, 74.84166666666667, 74.84166666666667, 75.24166666666666, 75.24166666666666, 75.30833333333334, 75.30833333333334, 74.59166666666667, 74.59166666666667, 74.08333333333333, 74.08333333333333, 74.25833333333334, 74.25833333333334, 74.76666666666667, 74.76666666666667, 75.48333333333333, 75.48333333333333, 75.23333333333333, 75.23333333333333, 74.94166666666666, 74.94166666666666, 75.20833333333333, 75.20833333333333, 74.575, 74.575, 75.05833333333334, 75.05833333333334, 75.84166666666667, 75.84166666666667, 76.43333333333334, 76.43333333333334, 76.18333333333334, 76.18333333333334, 76.26666666666667, 76.26666666666667, 75.25833333333334, 75.25833333333334, 75.06666666666666, 75.06666666666666, 74.90833333333333, 74.90833333333333, 74.8, 74.8, 75.36666666666666, 75.36666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.6617
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5350
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6350
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3617
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5200
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6300
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4517
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5983
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.6067
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6717
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.263, Test loss: 0.684, Test accuracy: 78.49
Final Round, Global train loss: 0.263, Global test loss: 1.253, Global test accuracy: 62.76
Average accuracy final 10 rounds: 78.53083333333333 

Average global accuracy final 10 rounds: 62.474166666666655 

2003.9139199256897
[1.8684990406036377, 3.7369980812072754, 5.370856046676636, 7.004714012145996, 8.644112586975098, 10.2835111618042, 11.905749559402466, 13.527987957000732, 15.172093629837036, 16.81619930267334, 18.452953577041626, 20.089707851409912, 21.72587561607361, 23.362043380737305, 24.98088240623474, 26.599721431732178, 28.234646558761597, 29.869571685791016, 31.497730016708374, 33.12588834762573, 34.76476049423218, 36.40363264083862, 38.046168088912964, 39.688703536987305, 41.33310341835022, 42.977503299713135, 44.63112688064575, 46.28475046157837, 47.922505378723145, 49.56026029586792, 51.201653718948364, 52.84304714202881, 54.4889075756073, 56.13476800918579, 57.77261209487915, 59.41045618057251, 61.046950578689575, 62.68344497680664, 64.3172721862793, 65.95109939575195, 67.58533835411072, 69.21957731246948, 70.87098360061646, 72.52238988876343, 74.15945482254028, 75.79651975631714, 77.4363911151886, 79.07626247406006, 80.71334505081177, 82.35042762756348, 83.98223805427551, 85.61404848098755, 87.25508499145508, 88.89612150192261, 90.55247449874878, 92.20882749557495, 93.85529065132141, 95.50175380706787, 97.14098811149597, 98.78022241592407, 100.4123306274414, 102.04443883895874, 103.68039226531982, 105.31634569168091, 106.95980286598206, 108.6032600402832, 110.23914337158203, 111.87502670288086, 113.51101517677307, 115.14700365066528, 116.80063009262085, 118.45425653457642, 120.10030436515808, 121.74635219573975, 123.40367102622986, 125.06098985671997, 126.71930360794067, 128.37761735916138, 130.02311873435974, 131.6686201095581, 133.32830834388733, 134.98799657821655, 136.63282680511475, 138.27765703201294, 139.92163228988647, 141.56560754776, 143.23798632621765, 144.9103651046753, 146.55023503303528, 148.19010496139526, 149.82758164405823, 151.4650583267212, 153.0949535369873, 154.72484874725342, 156.37362504005432, 158.02240133285522, 159.65699076652527, 161.2915802001953, 162.92721509933472, 164.56284999847412, 166.197026014328, 167.83120203018188, 169.47196865081787, 171.11273527145386, 172.76246643066406, 174.41219758987427, 176.08076310157776, 177.74932861328125, 179.39476799964905, 181.04020738601685, 182.6837978363037, 184.32738828659058, 185.97595381736755, 187.62451934814453, 189.25669622421265, 190.88887310028076, 192.52935147285461, 194.16982984542847, 195.80527567863464, 197.44072151184082, 199.08469915390015, 200.72867679595947, 202.3687345981598, 204.0087924003601, 205.64751505851746, 207.2862377166748, 208.91737842559814, 210.54851913452148, 212.18280172348022, 213.81708431243896, 215.44695949554443, 217.0768346786499, 218.71025323867798, 220.34367179870605, 221.98509407043457, 223.6265163421631, 225.28261995315552, 226.93872356414795, 228.58389472961426, 230.22906589508057, 231.86832857131958, 233.5075912475586, 235.13900709152222, 236.77042293548584, 238.3929238319397, 240.01542472839355, 241.66110754013062, 243.30679035186768, 244.95610666275024, 246.6054229736328, 248.25761818885803, 249.90981340408325, 251.56352019309998, 253.2172269821167, 254.87457752227783, 256.53192806243896, 258.19251132011414, 259.8530945777893, 261.54386472702026, 263.2346348762512, 264.9151906967163, 266.5957465171814, 268.2714776992798, 269.9472088813782, 271.6149523258209, 273.2826957702637, 274.94445753097534, 276.606219291687, 278.2791337966919, 279.9520483016968, 281.61165952682495, 283.2712707519531, 284.9034502506256, 286.5356297492981, 288.1690971851349, 289.8025646209717, 291.432888507843, 293.06321239471436, 294.69736194610596, 296.33151149749756, 297.98318672180176, 299.63486194610596, 301.2918884754181, 302.9489150047302, 304.6095521450043, 306.2701892852783, 307.9286696910858, 309.5871500968933, 311.25267577171326, 312.9182014465332, 314.5727734565735, 316.22734546661377, 317.87246561050415, 319.51758575439453, 321.1874361038208, 322.85728645324707, 324.50765466690063, 326.1580228805542, 327.8037075996399, 329.4493923187256, 332.20607233047485, 334.9627523422241]
[24.3, 24.3, 38.19166666666667, 38.19166666666667, 51.40833333333333, 51.40833333333333, 55.95, 55.95, 55.68333333333333, 55.68333333333333, 62.666666666666664, 62.666666666666664, 64.16666666666667, 64.16666666666667, 66.39166666666667, 66.39166666666667, 67.13333333333334, 67.13333333333334, 67.43333333333334, 67.43333333333334, 66.29166666666667, 66.29166666666667, 68.85833333333333, 68.85833333333333, 69.28333333333333, 69.28333333333333, 70.15833333333333, 70.15833333333333, 70.40833333333333, 70.40833333333333, 71.975, 71.975, 72.48333333333333, 72.48333333333333, 72.48333333333333, 72.48333333333333, 72.575, 72.575, 73.54166666666667, 73.54166666666667, 73.88333333333334, 73.88333333333334, 74.425, 74.425, 73.53333333333333, 73.53333333333333, 73.375, 73.375, 73.43333333333334, 73.43333333333334, 74.65833333333333, 74.65833333333333, 73.34166666666667, 73.34166666666667, 74.08333333333333, 74.08333333333333, 76.075, 76.075, 75.18333333333334, 75.18333333333334, 75.975, 75.975, 75.775, 75.775, 75.375, 75.375, 76.175, 76.175, 76.91666666666667, 76.91666666666667, 75.66666666666667, 75.66666666666667, 76.20833333333333, 76.20833333333333, 77.45833333333333, 77.45833333333333, 76.83333333333333, 76.83333333333333, 76.7, 76.7, 76.20833333333333, 76.20833333333333, 76.98333333333333, 76.98333333333333, 76.525, 76.525, 76.0, 76.0, 75.91666666666667, 75.91666666666667, 75.75, 75.75, 76.4, 76.4, 77.20833333333333, 77.20833333333333, 77.68333333333334, 77.68333333333334, 77.38333333333334, 77.38333333333334, 77.65833333333333, 77.65833333333333, 77.90833333333333, 77.90833333333333, 77.61666666666666, 77.61666666666666, 77.99166666666666, 77.99166666666666, 77.8, 77.8, 77.45833333333333, 77.45833333333333, 77.34166666666667, 77.34166666666667, 77.69166666666666, 77.69166666666666, 77.65833333333333, 77.65833333333333, 77.375, 77.375, 78.8, 78.8, 78.45833333333333, 78.45833333333333, 78.28333333333333, 78.28333333333333, 78.775, 78.775, 78.91666666666667, 78.91666666666667, 78.45833333333333, 78.45833333333333, 77.00833333333334, 77.00833333333334, 78.43333333333334, 78.43333333333334, 78.0, 78.0, 78.375, 78.375, 78.44166666666666, 78.44166666666666, 78.11666666666666, 78.11666666666666, 77.66666666666667, 77.66666666666667, 77.28333333333333, 77.28333333333333, 78.225, 78.225, 77.74166666666666, 77.74166666666666, 78.25, 78.25, 78.51666666666667, 78.51666666666667, 78.30833333333334, 78.30833333333334, 77.90833333333333, 77.90833333333333, 78.05, 78.05, 77.90833333333333, 77.90833333333333, 78.99166666666666, 78.99166666666666, 78.05, 78.05, 78.28333333333333, 78.28333333333333, 78.425, 78.425, 79.475, 79.475, 79.56666666666666, 79.56666666666666, 79.84166666666667, 79.84166666666667, 80.03333333333333, 80.03333333333333, 79.6, 79.6, 78.95833333333333, 78.95833333333333, 77.68333333333334, 77.68333333333334, 77.7, 77.7, 78.45, 78.45, 78.39166666666667, 78.39166666666667, 79.3, 79.3, 78.76666666666667, 78.76666666666667, 78.075, 78.075, 78.38333333333334, 78.38333333333334, 78.49166666666666, 78.49166666666666]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.6617
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4683
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6350
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3617
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5367
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6300
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3467
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5983
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5667
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6700
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.6617
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4683
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6350
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3617
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5200
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6383
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3467
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5983
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5017
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6700
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  12.5400
Round 1 global test acc  16.7600
Round 2 global test acc  19.7600
Round 3 global test acc  13.6900
Round 4 global test acc  19.6800
Round 5 global test acc  20.1300
Round 6 global test acc  26.2000
Round 7 global test acc  23.3500
Round 8 global test acc  31.0600
Round 9 global test acc  25.2200
Round 10 global test acc  23.9400
Round 11 global test acc  31.0700
Round 12 global test acc  32.3700
Round 13 global test acc  21.4500
Round 14 global test acc  26.0600
Round 15 global test acc  22.8900
Round 16 global test acc  30.1000
Round 17 global test acc  32.9900
Round 18 global test acc  30.0200
Round 19 global test acc  27.7000
Round 20 global test acc  25.6800
Round 21 global test acc  28.7000
Round 22 global test acc  25.0100
Round 23 global test acc  25.1700
Round 24 global test acc  25.4200
Round 25 global test acc  28.7200
Round 26 global test acc  20.7200
Round 27 global test acc  22.5500
Round 28 global test acc  32.0600
Round 29 global test acc  23.2500
Round 30 global test acc  38.0000
Round 31 global test acc  27.7500
Round 32 global test acc  31.7000
Round 33 global test acc  24.0200
Round 34 global test acc  34.8100
Round 35 global test acc  38.4400
Round 36 global test acc  38.2700
Round 37 global test acc  40.3600
Round 38 global test acc  22.8300
Round 39 global test acc  26.6400
Round 40 global test acc  32.8400
Round 41 global test acc  30.2800
Round 42 global test acc  36.5200
Round 43 global test acc  29.3900
Round 44 global test acc  33.7500
Round 45 global test acc  25.3800
Round 46 global test acc  26.6900
Round 47 global test acc  21.6900
Round 48 global test acc  31.5000
Round 49 global test acc  33.6400
Round 50 global test acc  35.1200
Round 51 global test acc  32.8400
Round 52 global test acc  23.4000
Round 53 global test acc  35.2800
Round 54 global test acc  28.9100
Round 55 global test acc  31.0200
Round 56 global test acc  31.4700
Round 57 global test acc  39.6400
Round 58 global test acc  30.3500
Round 59 global test acc  39.6400
Round 60 global test acc  30.7200
Round 61 global test acc  32.7200
Round 62 global test acc  23.8700
Round 63 global test acc  32.9600
Round 64 global test acc  37.5200
Round 65 global test acc  38.0300
Round 66 global test acc  28.2200
Round 67 global test acc  31.9500
Round 68 global test acc  25.8200
Round 69 global test acc  29.9700
Round 70 global test acc  42.9100
Round 71 global test acc  35.1700
Round 72 global test acc  32.5800
Round 73 global test acc  33.5600
Round 74 global test acc  43.5900
Round 75 global test acc  39.7700
Round 76 global test acc  34.3800
Round 77 global test acc  39.9900
Round 78 global test acc  32.5300
Round 79 global test acc  39.7700
Round 80 global test acc  35.0500
Round 81 global test acc  32.9700
Round 82 global test acc  30.8900
Round 83 global test acc  29.3000
Round 84 global test acc  28.1200
Round 85 global test acc  27.8100
Round 86 global test acc  27.1900
Round 87 global test acc  27.2300
Round 88 global test acc  26.7300
Round 89 global test acc  26.2700
Round 90 global test acc  25.9500
Round 91 global test acc  24.8600
Round 92 global test acc  23.9600
Round 93 global test acc  23.3100
Round 94 global test acc  23.3700
Round 95 global test acc  22.7900
Round 96 global test acc  21.4300
Round 97 global test acc  21.4900
Round 98 global test acc  20.8000
Round 99 global test acc  20.7300
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.6617
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4683
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6350
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3617
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5200
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6300
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3467
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.6233
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5017
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6700
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.501, Test loss: 0.764, Test accuracy: 74.85
Average accuracy final 10 rounds: 74.51875
4739.092650413513
[5.524484157562256, 10.771759748458862, 15.962769746780396, 21.130956888198853, 26.569135427474976, 31.63788080215454, 36.37251019477844, 41.30026650428772, 46.33711910247803, 51.27283024787903, 56.21351742744446, 61.13178110122681, 66.02611756324768, 70.88710522651672, 75.7521243095398, 80.62519717216492, 85.62129282951355, 90.61998963356018, 95.6349675655365, 100.62224054336548, 105.63179278373718, 110.66376900672913, 115.64903903007507, 120.64134860038757, 126.46660804748535, 132.29626870155334, 138.3038308620453, 144.28027415275574, 150.32656240463257, 156.60746049880981, 162.73273992538452, 168.77640295028687, 174.96814012527466, 181.18445229530334, 187.41163849830627, 193.59791946411133, 199.72352051734924, 206.09289455413818, 212.3007481098175, 218.54076600074768, 224.86598110198975, 231.0935115814209, 237.30606174468994, 243.53789973258972, 249.9620463848114, 256.26131987571716, 262.6185882091522, 268.93287467956543, 275.09398007392883, 281.36305952072144, 287.6216926574707, 293.958731174469, 300.18429827690125, 306.41248893737793, 312.6012279987335, 319.0216236114502, 325.1476860046387, 331.5491042137146, 337.7926023006439, 343.95255160331726, 349.9188823699951, 356.0716173648834, 362.34656381607056, 368.3835036754608, 374.71940445899963, 380.9696271419525, 387.32828879356384, 393.4825105667114, 399.72649478912354, 405.90276980400085, 412.1570963859558, 418.15221643447876, 424.10719060897827, 430.3356382846832, 436.52377676963806, 442.732519865036, 448.9401099681854, 455.1089358329773, 461.46975350379944, 467.3835084438324, 473.3366713523865, 479.17323064804077, 484.9935953617096, 490.83649349212646, 497.77748441696167, 504.44676518440247, 511.1967432498932, 517.9790511131287, 524.8225064277649, 531.4605123996735, 537.8866305351257, 544.2616055011749, 550.7773635387421, 557.4499554634094, 564.215371131897, 571.3160412311554, 577.8940372467041, 584.182386636734, 590.5802893638611, 597.2232406139374, 599.8650145530701]
[30.2925, 38.1975, 42.1625, 47.425, 49.53, 51.1425, 54.2525, 55.715, 56.7575, 57.9275, 58.9375, 60.7425, 61.3425, 62.1325, 62.16, 62.96, 63.015, 63.8775, 64.5825, 64.445, 65.86, 65.83, 67.15, 66.945, 66.9975, 67.885, 68.315, 68.9375, 69.06, 68.9925, 69.4575, 69.81, 70.0825, 69.995, 70.0725, 70.7675, 70.795, 71.655, 71.1575, 71.3825, 71.7525, 71.91, 71.56, 71.8875, 71.9225, 71.805, 72.2725, 72.0025, 72.7775, 72.285, 72.255, 73.0025, 72.2525, 72.605, 72.87, 72.725, 73.02, 73.1425, 73.0125, 73.1925, 72.95, 73.3875, 73.6325, 73.715, 73.4775, 73.355, 73.47, 72.995, 73.805, 73.5825, 73.5, 73.8825, 73.7525, 73.3525, 73.815, 73.5425, 73.7875, 73.9425, 74.015, 73.53, 73.1725, 73.6775, 74.045, 74.01, 73.875, 74.125, 74.525, 74.7825, 74.2125, 74.5125, 74.7425, 74.715, 74.645, 74.29, 74.675, 75.07, 74.4625, 74.6125, 74.165, 73.81, 74.8475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.6617
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.4683
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.6350
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.3617
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.5200
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.6300
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.3467
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.5983
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.5017
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.6700
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.529, Test loss: 1.976, Test accuracy: 25.18
Round   1, Train loss: 0.945, Test loss: 1.614, Test accuracy: 39.00
Round   2, Train loss: 0.861, Test loss: 1.297, Test accuracy: 46.16
Round   3, Train loss: 0.770, Test loss: 1.078, Test accuracy: 56.64
Round   4, Train loss: 0.685, Test loss: 0.945, Test accuracy: 63.36
Round   5, Train loss: 0.673, Test loss: 0.872, Test accuracy: 68.16
Round   6, Train loss: 0.642, Test loss: 0.864, Test accuracy: 69.22
Round   7, Train loss: 0.675, Test loss: 0.721, Test accuracy: 71.46
Round   8, Train loss: 0.658, Test loss: 0.666, Test accuracy: 73.38
Round   9, Train loss: 0.584, Test loss: 0.659, Test accuracy: 72.96
Round  10, Train loss: 0.494, Test loss: 0.622, Test accuracy: 74.14
Round  11, Train loss: 0.585, Test loss: 0.653, Test accuracy: 73.89
Round  12, Train loss: 0.534, Test loss: 0.595, Test accuracy: 77.03
Round  13, Train loss: 0.614, Test loss: 0.673, Test accuracy: 75.33
Round  14, Train loss: 0.604, Test loss: 0.528, Test accuracy: 79.02
Round  15, Train loss: 0.469, Test loss: 0.514, Test accuracy: 78.92
Round  16, Train loss: 0.534, Test loss: 0.504, Test accuracy: 79.91
Round  17, Train loss: 0.432, Test loss: 0.500, Test accuracy: 80.19
Round  18, Train loss: 0.463, Test loss: 0.473, Test accuracy: 81.16
Round  19, Train loss: 0.520, Test loss: 0.476, Test accuracy: 81.18
Round  20, Train loss: 0.468, Test loss: 0.479, Test accuracy: 81.15
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1617
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0317
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5083
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0483
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4600
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3200
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1467
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1633
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4750
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2017
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.114, Test loss: 1.389, Test accuracy: 70.53
Final Round, Global train loss: 0.114, Global test loss: 2.200, Global test accuracy: 13.68
Average accuracy final 10 rounds: 70.00666666666666 

Average global accuracy final 10 rounds: 21.086666666666666 

2257.0382494926453
[1.896054744720459, 3.792109489440918, 5.400502443313599, 7.008895397186279, 8.697690725326538, 10.386486053466797, 12.05027437210083, 13.714062690734863, 15.331087827682495, 16.948112964630127, 18.632063150405884, 20.31601333618164, 22.005120277404785, 23.69422721862793, 25.2934627532959, 26.892698287963867, 28.566898107528687, 30.241097927093506, 31.87322163581848, 33.50534534454346, 35.11753582954407, 36.72972631454468, 38.384336948394775, 40.03894758224487, 41.55971384048462, 43.080480098724365, 44.53815269470215, 45.99582529067993, 47.464032888412476, 48.93224048614502, 50.45233225822449, 51.972424030303955, 53.459229946136475, 54.946035861968994, 56.44235634803772, 57.938676834106445, 59.36419367790222, 60.789710521698, 62.2849235534668, 63.780136585235596, 65.23751592636108, 66.69489526748657, 68.18396019935608, 69.67302513122559, 71.11765265464783, 72.56228017807007, 73.97492742538452, 75.38757467269897, 76.84005165100098, 78.29252862930298, 79.88154673576355, 81.47056484222412, 83.11485505104065, 84.75914525985718, 86.37944507598877, 87.99974489212036, 89.62250089645386, 91.24525690078735, 92.85470461845398, 94.4641523361206, 96.0636978149414, 97.6632432937622, 99.28301477432251, 100.90278625488281, 102.51723074913025, 104.13167524337769, 105.73082780838013, 107.32998037338257, 108.82057285308838, 110.31116533279419, 111.83763527870178, 113.36410522460938, 114.79882192611694, 116.23353862762451, 117.7078685760498, 119.1821985244751, 120.83949613571167, 122.49679374694824, 124.05951428413391, 125.62223482131958, 127.18548655509949, 128.7487382888794, 130.34355998039246, 131.93838167190552, 133.4957902431488, 135.0531988143921, 136.6468324661255, 138.2404661178589, 139.81737804412842, 141.39428997039795, 142.94773268699646, 144.50117540359497, 146.10191297531128, 147.7026505470276, 149.27398347854614, 150.8453164100647, 152.46261382102966, 154.07991123199463, 155.60478591918945, 157.12966060638428, 158.69151902198792, 160.25337743759155, 161.71574664115906, 163.17811584472656, 164.80300736427307, 166.42789888381958, 167.99916887283325, 169.57043886184692, 171.05996108055115, 172.54948329925537, 174.1226019859314, 175.69572067260742, 177.2622389793396, 178.82875728607178, 180.40665102005005, 181.98454475402832, 183.49191331863403, 184.99928188323975, 186.56834816932678, 188.13741445541382, 189.7020182609558, 191.2666220664978, 192.85977387428284, 194.45292568206787, 196.0252287387848, 197.5975317955017, 199.19759893417358, 200.79766607284546, 202.38011360168457, 203.96256113052368, 205.56773281097412, 207.17290449142456, 208.8089964389801, 210.44508838653564, 212.00268149375916, 213.56027460098267, 214.94357538223267, 216.32687616348267, 217.77991437911987, 219.23295259475708, 220.66369724273682, 222.09444189071655, 223.53130197525024, 224.96816205978394, 226.42496490478516, 227.88176774978638, 229.3640444278717, 230.84632110595703, 232.2711353302002, 233.69594955444336, 235.0747094154358, 236.45346927642822, 237.96633863449097, 239.4792079925537, 240.97450351715088, 242.46979904174805, 243.94193959236145, 245.41408014297485, 246.86797547340393, 248.321870803833, 249.74847316741943, 251.17507553100586, 252.66322588920593, 254.151376247406, 255.60103011131287, 257.0506839752197, 258.5491542816162, 260.0476245880127, 261.49138283729553, 262.93514108657837, 264.3799171447754, 265.8246932029724, 267.2908811569214, 268.75706911087036, 270.1910071372986, 271.6249451637268, 273.058345079422, 274.4917449951172, 275.95571088790894, 277.4196767807007, 278.87983798980713, 280.3399991989136, 281.82094073295593, 283.3018822669983, 284.7926142215729, 286.28334617614746, 287.6960904598236, 289.10883474349976, 290.5559151172638, 292.00299549102783, 293.4193379878998, 294.83568048477173, 296.26723623275757, 297.6987919807434, 299.10600686073303, 300.51322174072266, 301.95452785491943, 303.3958339691162, 304.84720373153687, 306.2985734939575, 308.72273421287537, 311.1468949317932]
[28.508333333333333, 28.508333333333333, 37.35, 37.35, 47.475, 47.475, 53.483333333333334, 53.483333333333334, 60.125, 60.125, 63.15, 63.15, 66.625, 66.625, 66.39166666666667, 66.39166666666667, 65.725, 65.725, 68.38333333333334, 68.38333333333334, 67.48333333333333, 67.48333333333333, 67.33333333333333, 67.33333333333333, 67.95833333333333, 67.95833333333333, 66.44166666666666, 66.44166666666666, 69.775, 69.775, 70.50833333333334, 70.50833333333334, 70.54166666666667, 70.54166666666667, 71.16666666666667, 71.16666666666667, 70.45, 70.45, 71.41666666666667, 71.41666666666667, 70.85, 70.85, 70.69166666666666, 70.69166666666666, 71.40833333333333, 71.40833333333333, 71.58333333333333, 71.58333333333333, 72.04166666666667, 72.04166666666667, 71.83333333333333, 71.83333333333333, 71.325, 71.325, 71.09166666666667, 71.09166666666667, 71.525, 71.525, 71.55, 71.55, 71.59166666666667, 71.59166666666667, 72.35833333333333, 72.35833333333333, 72.30833333333334, 72.30833333333334, 72.48333333333333, 72.48333333333333, 72.325, 72.325, 72.21666666666667, 72.21666666666667, 71.23333333333333, 71.23333333333333, 71.4, 71.4, 71.5, 71.5, 71.59166666666667, 71.59166666666667, 72.18333333333334, 72.18333333333334, 71.925, 71.925, 72.125, 72.125, 71.60833333333333, 71.60833333333333, 71.86666666666666, 71.86666666666666, 71.31666666666666, 71.31666666666666, 71.575, 71.575, 71.08333333333333, 71.08333333333333, 71.575, 71.575, 71.50833333333334, 71.50833333333334, 70.88333333333334, 70.88333333333334, 71.05, 71.05, 70.7, 70.7, 70.51666666666667, 70.51666666666667, 70.43333333333334, 70.43333333333334, 70.30833333333334, 70.30833333333334, 70.81666666666666, 70.81666666666666, 71.16666666666667, 71.16666666666667, 71.08333333333333, 71.08333333333333, 70.95, 70.95, 70.68333333333334, 70.68333333333334, 70.29166666666667, 70.29166666666667, 70.525, 70.525, 70.425, 70.425, 70.16666666666667, 70.16666666666667, 70.34166666666667, 70.34166666666667, 70.525, 70.525, 70.95, 70.95, 70.825, 70.825, 70.65833333333333, 70.65833333333333, 70.925, 70.925, 70.65, 70.65, 70.38333333333334, 70.38333333333334, 70.15833333333333, 70.15833333333333, 70.6, 70.6, 70.31666666666666, 70.31666666666666, 70.41666666666667, 70.41666666666667, 70.65, 70.65, 70.10833333333333, 70.10833333333333, 71.04166666666667, 71.04166666666667, 70.44166666666666, 70.44166666666666, 70.81666666666666, 70.81666666666666, 70.81666666666666, 70.81666666666666, 70.775, 70.775, 70.45833333333333, 70.45833333333333, 70.525, 70.525, 70.325, 70.325, 70.20833333333333, 70.20833333333333, 70.31666666666666, 70.31666666666666, 70.45, 70.45, 70.625, 70.625, 70.15833333333333, 70.15833333333333, 69.975, 69.975, 70.26666666666667, 70.26666666666667, 69.625, 69.625, 70.04166666666667, 70.04166666666667, 69.98333333333333, 69.98333333333333, 69.46666666666667, 69.46666666666667, 70.025, 70.025, 69.9, 69.9, 70.525, 70.525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1617
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0317
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5083
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0483
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4600
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3200
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1633
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.5233
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2017
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.166, Test loss: 0.537, Test accuracy: 85.51
Final Round, Global train loss: 0.166, Global test loss: 1.467, Global test accuracy: 58.55
Average accuracy final 10 rounds: 85.27777777777779 

Average global accuracy final 10 rounds: 60.48222222222222 

3379.7852153778076
[2.5581483840942383, 5.116296768188477, 7.529486179351807, 9.942675590515137, 12.382473945617676, 14.822272300720215, 17.270596504211426, 19.718920707702637, 22.149514198303223, 24.58010768890381, 27.08443546295166, 29.58876323699951, 32.0741662979126, 34.559569358825684, 36.88821506500244, 39.2168607711792, 41.539023637771606, 43.861186504364014, 46.144704818725586, 48.42822313308716, 50.62816762924194, 52.82811212539673, 55.124640464782715, 57.4211688041687, 59.73471474647522, 62.04826068878174, 64.39461493492126, 66.74096918106079, 69.10576391220093, 71.47055864334106, 73.78822207450867, 76.10588550567627, 78.4018702507019, 80.69785499572754, 83.0244972705841, 85.35113954544067, 87.60754013061523, 89.8639407157898, 92.14347100257874, 94.42300128936768, 96.73218297958374, 99.0413646697998, 101.45210599899292, 103.86284732818604, 106.31267666816711, 108.7625060081482, 111.0264503955841, 113.29039478302002, 115.64525389671326, 118.0001130104065, 120.37879657745361, 122.75748014450073, 125.12689542770386, 127.49631071090698, 129.75141787528992, 132.00652503967285, 134.24692392349243, 136.487322807312, 138.6690924167633, 140.8508620262146, 143.1395914554596, 145.4283208847046, 147.6948721408844, 149.9614233970642, 152.26530289649963, 154.56918239593506, 156.7873134613037, 159.00544452667236, 161.19540429115295, 163.38536405563354, 165.5928270816803, 167.80029010772705, 169.97158670425415, 172.14288330078125, 174.35521984100342, 176.5675563812256, 178.759601354599, 180.9516463279724, 183.16424536705017, 185.37684440612793, 187.62457132339478, 189.87229824066162, 192.09724187850952, 194.32218551635742, 196.54394793510437, 198.76571035385132, 201.14270401000977, 203.5196976661682, 205.95556473731995, 208.39143180847168, 210.77933287620544, 213.1672339439392, 215.52922868728638, 217.89122343063354, 220.22759747505188, 222.56397151947021, 225.01436614990234, 227.46476078033447, 229.81139039993286, 232.15802001953125, 234.50645852088928, 236.85489702224731, 239.2018105983734, 241.5487241744995, 244.08596205711365, 246.62319993972778, 249.1665382385254, 251.709876537323, 254.14103364944458, 256.57219076156616, 258.9898452758789, 261.40749979019165, 263.7919056415558, 266.1763114929199, 268.43287563323975, 270.68943977355957, 273.014221906662, 275.3390040397644, 277.64673352241516, 279.9544630050659, 282.23266434669495, 284.510865688324, 286.7029302120209, 288.8949947357178, 291.1730604171753, 293.4511260986328, 295.7159996032715, 297.98087310791016, 300.19653844833374, 302.4122037887573, 304.5992934703827, 306.78638315200806, 309.0048117637634, 311.2232403755188, 313.4878239631653, 315.75240755081177, 317.99078154563904, 320.2291555404663, 322.52307987213135, 324.8170042037964, 327.0767161846161, 329.3364281654358, 331.8265070915222, 334.31658601760864, 336.5620822906494, 338.8075785636902, 341.22660779953003, 343.6456370353699, 345.96519947052, 348.28476190567017, 350.63822984695435, 352.9916977882385, 355.2984585762024, 357.60521936416626, 359.90480971336365, 362.20440006256104, 364.43624782562256, 366.6680955886841, 368.8970220088959, 371.12594842910767, 373.40294098854065, 375.67993354797363, 377.95150876045227, 380.2230839729309, 382.4059934616089, 384.58890295028687, 386.8636636734009, 389.1384243965149, 391.40510988235474, 393.6717953681946, 395.8999364376068, 398.12807750701904, 400.33485674858093, 402.5416359901428, 404.8161389827728, 407.09064197540283, 409.3919277191162, 411.6932134628296, 413.9708993434906, 416.2485852241516, 418.45492815971375, 420.6612710952759, 422.8594756126404, 425.0576801300049, 427.3021192550659, 429.54655838012695, 431.72652316093445, 433.90648794174194, 436.0857357978821, 438.2649836540222, 440.4742431640625, 442.6835026741028, 444.8657646179199, 447.04802656173706, 449.2409460544586, 451.4338655471802, 453.63927030563354, 455.8446750640869, 458.0380835533142, 460.2314920425415, 462.8640761375427, 465.49666023254395]
[31.91111111111111, 31.91111111111111, 43.766666666666666, 43.766666666666666, 59.76111111111111, 59.76111111111111, 63.577777777777776, 63.577777777777776, 66.49444444444444, 66.49444444444444, 71.57777777777778, 71.57777777777778, 75.63333333333334, 75.63333333333334, 75.5, 75.5, 76.30555555555556, 76.30555555555556, 76.99444444444444, 76.99444444444444, 77.40555555555555, 77.40555555555555, 78.2388888888889, 78.2388888888889, 79.76666666666667, 79.76666666666667, 80.61111111111111, 80.61111111111111, 80.55555555555556, 80.55555555555556, 80.97777777777777, 80.97777777777777, 81.28888888888889, 81.28888888888889, 81.2, 81.2, 81.85555555555555, 81.85555555555555, 82.23333333333333, 82.23333333333333, 83.03333333333333, 83.03333333333333, 83.40555555555555, 83.40555555555555, 83.77222222222223, 83.77222222222223, 83.81111111111112, 83.81111111111112, 84.12222222222222, 84.12222222222222, 83.2611111111111, 83.2611111111111, 83.27777777777777, 83.27777777777777, 83.04444444444445, 83.04444444444445, 83.90555555555555, 83.90555555555555, 84.36666666666666, 84.36666666666666, 84.61666666666666, 84.61666666666666, 84.46111111111111, 84.46111111111111, 84.76666666666667, 84.76666666666667, 84.58888888888889, 84.58888888888889, 84.46111111111111, 84.46111111111111, 84.63333333333334, 84.63333333333334, 84.24444444444444, 84.24444444444444, 84.31111111111112, 84.31111111111112, 84.11666666666666, 84.11666666666666, 84.64444444444445, 84.64444444444445, 85.06666666666666, 85.06666666666666, 84.64444444444445, 84.64444444444445, 84.31666666666666, 84.31666666666666, 84.8, 84.8, 85.02222222222223, 85.02222222222223, 84.87222222222222, 84.87222222222222, 85.03888888888889, 85.03888888888889, 84.96111111111111, 84.96111111111111, 84.79444444444445, 84.79444444444445, 84.81111111111112, 84.81111111111112, 84.28888888888889, 84.28888888888889, 84.57222222222222, 84.57222222222222, 84.21111111111111, 84.21111111111111, 84.36111111111111, 84.36111111111111, 84.1, 84.1, 83.92777777777778, 83.92777777777778, 84.04444444444445, 84.04444444444445, 84.04444444444445, 84.04444444444445, 84.57222222222222, 84.57222222222222, 84.77777777777777, 84.77777777777777, 84.49444444444444, 84.49444444444444, 84.62222222222222, 84.62222222222222, 84.91111111111111, 84.91111111111111, 85.12222222222222, 85.12222222222222, 85.22777777777777, 85.22777777777777, 85.17777777777778, 85.17777777777778, 84.77222222222223, 84.77222222222223, 85.27222222222223, 85.27222222222223, 85.47777777777777, 85.47777777777777, 85.97777777777777, 85.97777777777777, 85.65555555555555, 85.65555555555555, 85.20555555555555, 85.20555555555555, 85.03333333333333, 85.03333333333333, 84.7388888888889, 84.7388888888889, 84.83333333333333, 84.83333333333333, 85.11111111111111, 85.11111111111111, 85.02777777777777, 85.02777777777777, 85.55555555555556, 85.55555555555556, 85.72777777777777, 85.72777777777777, 85.65, 85.65, 85.25, 85.25, 85.37222222222222, 85.37222222222222, 85.14444444444445, 85.14444444444445, 84.75, 84.75, 84.6, 84.6, 85.28888888888889, 85.28888888888889, 85.03888888888889, 85.03888888888889, 85.20555555555555, 85.20555555555555, 85.20555555555555, 85.20555555555555, 85.23333333333333, 85.23333333333333, 85.06666666666666, 85.06666666666666, 85.4888888888889, 85.4888888888889, 85.25, 85.25, 85.4, 85.4, 85.22222222222223, 85.22222222222223, 85.38888888888889, 85.38888888888889, 85.24444444444444, 85.24444444444444, 85.33888888888889, 85.33888888888889, 85.2, 85.2, 85.17777777777778, 85.17777777777778, 85.5111111111111, 85.5111111111111]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1617
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0317
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5083
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0483
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4600
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3200
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1633
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4750
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2017
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.203, Test loss: 0.468, Test accuracy: 85.69
Final Round, Global train loss: 0.203, Global test loss: 1.462, Global test accuracy: 58.83
Average accuracy final 10 rounds: 85.11888888888889 

Average global accuracy final 10 rounds: 60.93833333333333 

3528.328815460205
[2.941089153289795, 5.88217830657959, 8.498441457748413, 11.114704608917236, 13.767801284790039, 16.420897960662842, 19.058281421661377, 21.695664882659912, 24.339228630065918, 26.982792377471924, 29.563777685165405, 32.14476299285889, 34.73405385017395, 37.323344707489014, 39.86031913757324, 42.39729356765747, 44.98734903335571, 47.577404499053955, 50.145076751708984, 52.712749004364014, 55.29367136955261, 57.87459373474121, 60.417420625686646, 62.96024751663208, 65.49082899093628, 68.02141046524048, 70.57479047775269, 73.12817049026489, 75.66691637039185, 78.2056622505188, 80.78165650367737, 83.35765075683594, 85.90777230262756, 88.45789384841919, 90.98118090629578, 93.50446796417236, 96.03036880493164, 98.55626964569092, 101.07597160339355, 103.59567356109619, 106.12134718894958, 108.64702081680298, 111.20439338684082, 113.76176595687866, 116.33533596992493, 118.90890598297119, 121.52327013015747, 124.13763427734375, 126.77688980102539, 129.41614532470703, 132.05140566825867, 134.6866660118103, 137.36057877540588, 140.03449153900146, 142.71386170387268, 145.3932318687439, 148.0609335899353, 150.7286353111267, 153.37882947921753, 156.02902364730835, 158.6043348312378, 161.17964601516724, 163.86457109451294, 166.54949617385864, 169.14130640029907, 171.7331166267395, 174.3514325618744, 176.96974849700928, 179.56054210662842, 182.15133571624756, 184.8595130443573, 187.56769037246704, 190.2752070426941, 192.98272371292114, 195.65513610839844, 198.32754850387573, 201.02417039871216, 203.72079229354858, 206.3394684791565, 208.9581446647644, 211.5518341064453, 214.14552354812622, 216.70963501930237, 219.27374649047852, 221.91480445861816, 224.5558624267578, 227.2261347770691, 229.89640712738037, 232.4367790222168, 234.97715091705322, 237.6621401309967, 240.34712934494019, 242.94808340072632, 245.54903745651245, 248.16493964195251, 250.78084182739258, 253.37321376800537, 255.96558570861816, 258.69323110580444, 261.4208765029907, 264.1685469150543, 266.9162173271179, 269.6166777610779, 272.31713819503784, 274.92958211898804, 277.54202604293823, 280.14203000068665, 282.74203395843506, 285.33268451690674, 287.9233350753784, 290.55735993385315, 293.1913847923279, 295.7359035015106, 298.28042221069336, 300.8593945503235, 303.4383668899536, 306.0693299770355, 308.70029306411743, 311.37709856033325, 314.0539040565491, 316.6880712509155, 319.322238445282, 321.96025586128235, 324.5982732772827, 327.22093772888184, 329.84360218048096, 332.4782464504242, 335.11289072036743, 337.8192036151886, 340.52551651000977, 343.11076045036316, 345.69600439071655, 348.2985348701477, 350.90106534957886, 353.4696829319, 356.0383005142212, 358.71145129203796, 361.38460206985474, 363.99617981910706, 366.6077575683594, 369.24175238609314, 371.8757472038269, 374.48280119895935, 377.0898551940918, 379.6946141719818, 382.2993731498718, 384.86801171302795, 387.4366502761841, 390.0976846218109, 392.75871896743774, 395.42495918273926, 398.09119939804077, 400.69543647766113, 403.2996735572815, 405.9308214187622, 408.5619692802429, 411.14521193504333, 413.72845458984375, 416.3716769218445, 419.0148992538452, 421.6542088985443, 424.2935185432434, 426.91229128837585, 429.5310640335083, 432.21056270599365, 434.890061378479, 437.5204236507416, 440.15078592300415, 442.8502252101898, 445.5496644973755, 448.1047763824463, 450.6598882675171, 453.163117647171, 455.66634702682495, 458.12115693092346, 460.575966835022, 463.24528455734253, 465.9146022796631, 468.59115266799927, 471.26770305633545, 474.02489852905273, 476.78209400177, 479.4629952907562, 482.14389657974243, 484.87886142730713, 487.6138262748718, 490.332754611969, 493.05168294906616, 495.7037343978882, 498.3557858467102, 501.1104488372803, 503.86511182785034, 506.5371286869049, 509.2091455459595, 511.85586977005005, 514.5025939941406, 517.2283186912537, 519.9540433883667, 522.6501903533936, 525.3463373184204, 528.3688759803772, 531.391414642334]
[31.933333333333334, 31.933333333333334, 44.87222222222222, 44.87222222222222, 58.96666666666667, 58.96666666666667, 61.11666666666667, 61.11666666666667, 63.833333333333336, 63.833333333333336, 68.7, 68.7, 73.41666666666667, 73.41666666666667, 73.81666666666666, 73.81666666666666, 74.52777777777777, 74.52777777777777, 75.49444444444444, 75.49444444444444, 76.28333333333333, 76.28333333333333, 76.81111111111112, 76.81111111111112, 77.71666666666667, 77.71666666666667, 78.84444444444445, 78.84444444444445, 79.20555555555555, 79.20555555555555, 78.95555555555555, 78.95555555555555, 78.64444444444445, 78.64444444444445, 79.03333333333333, 79.03333333333333, 79.79444444444445, 79.79444444444445, 79.5111111111111, 79.5111111111111, 79.78888888888889, 79.78888888888889, 80.41111111111111, 80.41111111111111, 80.97777777777777, 80.97777777777777, 81.06111111111112, 81.06111111111112, 82.49444444444444, 82.49444444444444, 82.2611111111111, 82.2611111111111, 81.93333333333334, 81.93333333333334, 82.52777777777777, 82.52777777777777, 82.73333333333333, 82.73333333333333, 82.97222222222223, 82.97222222222223, 83.7611111111111, 83.7611111111111, 83.34444444444445, 83.34444444444445, 83.57777777777778, 83.57777777777778, 83.33888888888889, 83.33888888888889, 83.63888888888889, 83.63888888888889, 83.4, 83.4, 83.11111111111111, 83.11111111111111, 83.24444444444444, 83.24444444444444, 82.8, 82.8, 82.75555555555556, 82.75555555555556, 82.74444444444444, 82.74444444444444, 83.02222222222223, 83.02222222222223, 82.95555555555555, 82.95555555555555, 82.9, 82.9, 82.79444444444445, 82.79444444444445, 83.38888888888889, 83.38888888888889, 83.66111111111111, 83.66111111111111, 83.86111111111111, 83.86111111111111, 84.04444444444445, 84.04444444444445, 84.65555555555555, 84.65555555555555, 84.08888888888889, 84.08888888888889, 84.13888888888889, 84.13888888888889, 84.03888888888889, 84.03888888888889, 84.66666666666667, 84.66666666666667, 84.8, 84.8, 84.94444444444444, 84.94444444444444, 84.98333333333333, 84.98333333333333, 84.46111111111111, 84.46111111111111, 84.82777777777778, 84.82777777777778, 84.83333333333333, 84.83333333333333, 84.61111111111111, 84.61111111111111, 84.35555555555555, 84.35555555555555, 84.15, 84.15, 84.26666666666667, 84.26666666666667, 84.31666666666666, 84.31666666666666, 84.6, 84.6, 84.77222222222223, 84.77222222222223, 85.22222222222223, 85.22222222222223, 85.2, 85.2, 85.13333333333334, 85.13333333333334, 85.05555555555556, 85.05555555555556, 85.69444444444444, 85.69444444444444, 85.56666666666666, 85.56666666666666, 85.49444444444444, 85.49444444444444, 85.40555555555555, 85.40555555555555, 85.5, 85.5, 85.42222222222222, 85.42222222222222, 85.13333333333334, 85.13333333333334, 85.36666666666666, 85.36666666666666, 85.27777777777777, 85.27777777777777, 85.4888888888889, 85.4888888888889, 85.47222222222223, 85.47222222222223, 84.81666666666666, 84.81666666666666, 85.20555555555555, 85.20555555555555, 85.23333333333333, 85.23333333333333, 85.18888888888888, 85.18888888888888, 84.57777777777778, 84.57777777777778, 84.74444444444444, 84.74444444444444, 85.40555555555555, 85.40555555555555, 85.82777777777778, 85.82777777777778, 85.51666666666667, 85.51666666666667, 85.68333333333334, 85.68333333333334, 85.03888888888889, 85.03888888888889, 84.87777777777778, 84.87777777777778, 84.69444444444444, 84.69444444444444, 84.62777777777778, 84.62777777777778, 84.98333333333333, 84.98333333333333, 85.15, 85.15, 85.2, 85.2, 85.41666666666667, 85.41666666666667, 85.68888888888888, 85.68888888888888]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1617
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0317
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5083
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0483
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4600
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3200
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1633
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.5200
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2017
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1617
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0317
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5183
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0483
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4600
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3200
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1633
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4750
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2017
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  29.5400
Round 1 global test acc  39.9100
Round 2 global test acc  43.3800
Round 3 global test acc  48.0200
Round 4 global test acc  49.5500
Round 5 global test acc  50.3500
Round 6 global test acc  51.2700
Round 7 global test acc  53.7200
Round 8 global test acc  54.7200
Round 9 global test acc  55.6500
Round 10 global test acc  55.7100
Round 11 global test acc  57.8600
Round 12 global test acc  56.5000
Round 13 global test acc  58.3200
Round 14 global test acc  57.9100
Round 15 global test acc  59.1700
Round 16 global test acc  59.4400
Round 17 global test acc  56.6800
Round 18 global test acc  60.4600
Round 19 global test acc  60.4600
Round 20 global test acc  61.1600
Round 21 global test acc  60.0400
Round 22 global test acc  60.8600
Round 23 global test acc  61.9700
Round 24 global test acc  61.2900
Round 25 global test acc  60.9000
Round 26 global test acc  62.0400
Round 27 global test acc  61.2200
Round 28 global test acc  62.9600
Round 29 global test acc  62.7500
Round 30 global test acc  63.9900
Round 31 global test acc  62.8700
Round 32 global test acc  63.1400
Round 33 global test acc  63.4500
Round 34 global test acc  63.5800
Round 35 global test acc  63.2300
Round 36 global test acc  63.9900
Round 37 global test acc  63.8900
Round 38 global test acc  63.8400
Round 39 global test acc  64.1400
Round 40 global test acc  64.8700
Round 41 global test acc  63.8300
Round 42 global test acc  64.9000
Round 43 global test acc  64.6800
Round 44 global test acc  64.7000
Round 45 global test acc  64.9700
Round 46 global test acc  63.7900
Round 47 global test acc  65.4400
Round 48 global test acc  64.5900
Round 49 global test acc  65.7500
Round 50 global test acc  65.5400
Round 51 global test acc  65.6600
Round 52 global test acc  65.7200
Round 53 global test acc  65.6500
Round 54 global test acc  66.1100
Round 55 global test acc  66.2600
Round 56 global test acc  66.4500
Round 57 global test acc  65.8100
Round 58 global test acc  66.4400
Round 59 global test acc  65.2200
Round 60 global test acc  65.8900
Round 61 global test acc  65.5200
Round 62 global test acc  66.1400
Round 63 global test acc  66.1200
Round 64 global test acc  66.3900
Round 65 global test acc  66.5000
Round 66 global test acc  65.6200
Round 67 global test acc  66.0000
Round 68 global test acc  67.0400
Round 69 global test acc  66.4300
Round 70 global test acc  67.3000
Round 71 global test acc  67.2200
Round 72 global test acc  66.7100
Round 73 global test acc  66.1000
Round 74 global test acc  66.5200
Round 75 global test acc  66.0400
Round 76 global test acc  66.3800
Round 77 global test acc  65.7800
Round 78 global test acc  66.5000
Round 79 global test acc  68.0900
Round 80 global test acc  66.3700
Round 81 global test acc  64.9200
Round 82 global test acc  64.0700
Round 83 global test acc  63.1900
Round 84 global test acc  62.5200
Round 85 global test acc  62.3400
Round 86 global test acc  62.2800
Round 87 global test acc  61.9900
Round 88 global test acc  61.3000
Round 89 global test acc  60.4800
Round 90 global test acc  59.3500
Round 91 global test acc  59.3600
Round 92 global test acc  59.4200
Round 93 global test acc  59.1900
Round 94 global test acc  59.2400
Round 95 global test acc  58.6500
Round 96 global test acc  58.6200
Round 97 global test acc  58.5100
Round 98 global test acc  58.3400
Round 99 global test acc  57.1300
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1617
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0317
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5083
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0483
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4600
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3883
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0083
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2400
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4750
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2017
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.480, Test loss: 0.745, Test accuracy: 76.05
Average accuracy final 10 rounds: 75.94475
4620.312031984329
[6.2761571407318115, 12.65687608718872, 18.728023290634155, 24.783329248428345, 30.85653281211853, 37.08634805679321, 44.140679359436035, 50.190025329589844, 56.246591329574585, 62.29324293136597, 68.75860977172852, 74.80899858474731, 80.86089849472046, 87.01817107200623, 93.15073609352112, 99.22926592826843, 105.29954600334167, 111.34157991409302, 117.48619508743286, 123.58389949798584, 129.64295864105225, 135.8094413280487, 141.96386647224426, 148.05292558670044, 154.18711185455322, 160.2589466571808, 166.41707849502563, 172.55218267440796, 178.56633591651917, 184.54264855384827, 190.52392268180847, 196.5585696697235, 202.57174587249756, 208.55840229988098, 214.60206770896912, 222.52211594581604, 228.50898957252502, 234.51205968856812, 240.56170439720154, 246.64017581939697, 252.66422176361084, 258.7333290576935, 264.7553868293762, 270.7595524787903, 276.77216815948486, 282.7842164039612, 288.8741421699524, 294.9061267375946, 300.8823606967926, 306.9106216430664, 312.94109773635864, 319.07166290283203, 325.0724256038666, 331.08667731285095, 337.1218304634094, 343.1559932231903, 349.2253317832947, 355.238648891449, 361.2519829273224, 367.2820188999176, 373.3551375865936, 379.41845440864563, 385.4987494945526, 391.5448966026306, 397.52198123931885, 403.51421427726746, 409.55078291893005, 414.84290528297424, 420.15138697624207, 425.4435520172119, 430.72527623176575, 436.04219007492065, 441.4126842021942, 446.6819758415222, 451.9609680175781, 457.1998403072357, 462.49175548553467, 467.8020849227905, 473.0421712398529, 478.41230821609497, 483.73162603378296, 488.9423713684082, 494.2715337276459, 499.4740114212036, 504.75253653526306, 510.00681233406067, 515.2524907588959, 520.5073125362396, 525.7470610141754, 530.9319682121277, 536.1169953346252, 541.4538733959198, 546.6926505565643, 551.953122138977, 557.3097901344299, 562.556408405304, 567.777218580246, 572.9995160102844, 578.2631547451019, 583.4862022399902, 585.5829527378082]
[33.61, 40.445, 44.3725, 47.7425, 51.5575, 54.15, 55.9825, 57.345, 58.8075, 60.3725, 61.1325, 63.015, 63.245, 64.285, 64.6825, 65.8675, 66.44, 66.635, 67.4075, 67.9125, 68.6875, 68.895, 69.3575, 68.8775, 69.6875, 70.105, 70.355, 70.1525, 70.445, 70.675, 71.0425, 71.27, 72.3125, 72.3125, 72.27, 71.915, 72.5925, 73.135, 73.0225, 72.905, 73.4125, 73.7175, 73.425, 73.79, 73.4075, 74.1425, 73.76, 74.2675, 74.0375, 74.0925, 74.1475, 74.36, 74.315, 74.5225, 74.605, 74.9125, 74.125, 74.96, 74.9075, 75.015, 75.015, 74.6025, 75.2175, 75.24, 75.05, 75.0975, 75.235, 75.1775, 75.1725, 75.1375, 75.1675, 75.2625, 74.8225, 74.9875, 74.7025, 75.1875, 75.1825, 75.1925, 75.4975, 75.345, 75.6175, 75.615, 75.11, 75.4575, 75.5575, 75.06, 75.21, 75.8275, 75.3975, 75.66, 75.5275, 75.6475, 75.9325, 76.1525, 75.51, 76.21, 76.365, 76.385, 75.9525, 75.765, 76.0475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1617
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0317
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5083
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0483
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4600
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3200
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1633
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4800
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3117
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.221, Test loss: 2.094, Test accuracy: 25.61
Round   1, Train loss: 2.035, Test loss: 1.897, Test accuracy: 33.35
Round   2, Train loss: 2.054, Test loss: 1.944, Test accuracy: 37.30
Round   3, Train loss: 1.896, Test loss: 1.808, Test accuracy: 40.21
Round   4, Train loss: 1.883, Test loss: 1.754, Test accuracy: 43.73
Round   5, Train loss: 2.106, Test loss: 1.863, Test accuracy: 44.62
Round   6, Train loss: 2.006, Test loss: 1.783, Test accuracy: 44.62
Round   7, Train loss: 1.868, Test loss: 1.749, Test accuracy: 46.50
Round   8, Train loss: 1.988, Test loss: 1.739, Test accuracy: 47.41
Round   9, Train loss: 1.913, Test loss: 1.776, Test accuracy: 48.56
Round  10, Train loss: 1.978, Test loss: 1.739, Test accuracy: 49.46
Round  11, Train loss: 1.709, Test loss: 1.636, Test accuracy: 51.09
Round  12, Train loss: 2.035, Test loss: 1.699, Test accuracy: 50.96
Round  13, Train loss: 1.944, Test loss: 1.719, Test accuracy: 50.32
Round  14, Train loss: 1.923, Test loss: 1.657, Test accuracy: 51.98
Round  15, Train loss: 1.793, Test loss: 1.632, Test accuracy: 52.11
Round  16, Train loss: 1.748, Test loss: 1.611, Test accuracy: 53.37
Round  17, Train loss: 1.756, Test loss: 1.613, Test accuracy: 53.82
Round  18, Train loss: 1.779, Test loss: 1.593, Test accuracy: 53.99
Round  19, Train loss: 1.649, Test loss: 1.568, Test accuracy: 54.59
Round  20, Train loss: 1.836, Test loss: 1.584, Test accuracy: 54.44
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4200
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3500
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.5933
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3833
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4850
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3283
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4233
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5217
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4133
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.200, Test loss: 1.718, Test accuracy: 60.22
Final Round, Global train loss: 0.200, Global test loss: 2.173, Global test accuracy: 22.65
Average accuracy final 10 rounds: 59.405 

Average global accuracy final 10 rounds: 22.928333333333335 

1800.5532307624817
[1.701890468597412, 3.403780937194824, 4.869682550430298, 6.3355841636657715, 7.7868332862854, 9.23808240890503, 10.694949388504028, 12.151816368103027, 13.62356185913086, 15.095307350158691, 16.52747893333435, 17.95965051651001, 19.416680574417114, 20.87371063232422, 22.33861804008484, 23.80352544784546, 25.067433834075928, 26.331342220306396, 27.593522548675537, 28.855702877044678, 30.141607999801636, 31.427513122558594, 32.68733763694763, 33.94716215133667, 35.21823000907898, 36.48929786682129, 37.76625084877014, 39.043203830718994, 40.30774927139282, 41.57229471206665, 42.84138584136963, 44.11047697067261, 45.36965608596802, 46.62883520126343, 47.89779043197632, 49.16674566268921, 50.42272067070007, 51.67869567871094, 52.94465756416321, 54.21061944961548, 55.47938799858093, 56.74815654754639, 58.01450777053833, 59.28085899353027, 60.53076481819153, 61.78067064285278, 63.047520875930786, 64.31437110900879, 65.59928607940674, 66.88420104980469, 68.13247489929199, 69.3807487487793, 70.65349984169006, 71.92625093460083, 73.17472887039185, 74.42320680618286, 75.67819023132324, 76.93317365646362, 78.19486951828003, 79.45656538009644, 80.70707559585571, 81.95758581161499, 83.20894598960876, 84.46030616760254, 85.72277426719666, 86.98524236679077, 88.24316692352295, 89.50109148025513, 90.73954343795776, 91.9779953956604, 93.2572271823883, 94.53645896911621, 95.77622389793396, 97.01598882675171, 98.2775068283081, 99.5390248298645, 100.82779908180237, 102.11657333374023, 103.364018201828, 104.61146306991577, 105.8747239112854, 107.13798475265503, 108.40728306770325, 109.67658138275146, 110.92847084999084, 112.18036031723022, 113.44568800926208, 114.71101570129395, 115.99136424064636, 117.27171277999878, 118.51664781570435, 119.76158285140991, 121.02957916259766, 122.2975754737854, 123.57491827011108, 124.85226106643677, 126.11379361152649, 127.37532615661621, 128.6607265472412, 129.9461269378662, 131.21318125724792, 132.48023557662964, 133.73276901245117, 134.9853024482727, 136.24573254585266, 137.50616264343262, 138.7752046585083, 140.04424667358398, 141.29557466506958, 142.54690265655518, 143.80218482017517, 145.05746698379517, 146.31544518470764, 147.57342338562012, 148.8200159072876, 150.06660842895508, 151.33759665489197, 152.60858488082886, 153.85712337493896, 155.10566186904907, 156.3595254421234, 157.61338901519775, 158.8666706085205, 160.11995220184326, 161.3877727985382, 162.65559339523315, 163.910010099411, 165.16442680358887, 166.4276294708252, 167.69083213806152, 168.96021676063538, 170.22960138320923, 171.48105096817017, 172.7325005531311, 173.98597645759583, 175.23945236206055, 176.49190020561218, 177.74434804916382, 179.01543426513672, 180.28652048110962, 181.5352668762207, 182.7840132713318, 184.03682374954224, 185.28963422775269, 186.55727458000183, 187.82491493225098, 189.08838176727295, 190.35184860229492, 191.63183045387268, 192.91181230545044, 194.18209648132324, 195.45238065719604, 196.7204930782318, 197.98860549926758, 199.2405047416687, 200.49240398406982, 201.7790858745575, 203.06576776504517, 204.32413697242737, 205.58250617980957, 206.84059023857117, 208.09867429733276, 209.375638961792, 210.65260362625122, 211.90664339065552, 213.16068315505981, 214.41966891288757, 215.67865467071533, 216.94522190093994, 218.21178913116455, 219.4725694656372, 220.73334980010986, 221.98417472839355, 223.23499965667725, 224.50353980064392, 225.7720799446106, 227.02220821380615, 228.2723364830017, 229.54322981834412, 230.81412315368652, 232.08342838287354, 233.35273361206055, 234.59024620056152, 235.8277587890625, 237.08055520057678, 238.33335161209106, 239.59747886657715, 240.86160612106323, 242.1067337989807, 243.3518614768982, 244.59714794158936, 245.84243440628052, 247.0990788936615, 248.35572338104248, 249.61242032051086, 250.86911725997925, 252.12106800079346, 253.37301874160767, 254.63490962982178, 255.8968005180359, 258.00943064689636, 260.12206077575684]
[16.941666666666666, 16.941666666666666, 36.775, 36.775, 42.525, 42.525, 43.46666666666667, 43.46666666666667, 46.74166666666667, 46.74166666666667, 47.225, 47.225, 50.275, 50.275, 53.53333333333333, 53.53333333333333, 53.925, 53.925, 55.50833333333333, 55.50833333333333, 56.975, 56.975, 58.7, 58.7, 59.1, 59.1, 60.166666666666664, 60.166666666666664, 59.80833333333333, 59.80833333333333, 60.641666666666666, 60.641666666666666, 60.84166666666667, 60.84166666666667, 60.791666666666664, 60.791666666666664, 60.53333333333333, 60.53333333333333, 61.61666666666667, 61.61666666666667, 60.5, 60.5, 61.38333333333333, 61.38333333333333, 61.93333333333333, 61.93333333333333, 61.94166666666667, 61.94166666666667, 61.541666666666664, 61.541666666666664, 61.69166666666667, 61.69166666666667, 62.0, 62.0, 62.03333333333333, 62.03333333333333, 62.15, 62.15, 62.75833333333333, 62.75833333333333, 61.85, 61.85, 61.358333333333334, 61.358333333333334, 61.81666666666667, 61.81666666666667, 61.1, 61.1, 62.09166666666667, 62.09166666666667, 62.483333333333334, 62.483333333333334, 62.325, 62.325, 61.49166666666667, 61.49166666666667, 61.28333333333333, 61.28333333333333, 61.11666666666667, 61.11666666666667, 61.96666666666667, 61.96666666666667, 61.31666666666667, 61.31666666666667, 61.38333333333333, 61.38333333333333, 62.075, 62.075, 61.36666666666667, 61.36666666666667, 60.725, 60.725, 60.583333333333336, 60.583333333333336, 60.11666666666667, 60.11666666666667, 60.625, 60.625, 60.891666666666666, 60.891666666666666, 60.28333333333333, 60.28333333333333, 60.391666666666666, 60.391666666666666, 60.90833333333333, 60.90833333333333, 60.425, 60.425, 60.233333333333334, 60.233333333333334, 59.958333333333336, 59.958333333333336, 60.38333333333333, 60.38333333333333, 61.041666666666664, 61.041666666666664, 60.541666666666664, 60.541666666666664, 59.725, 59.725, 60.208333333333336, 60.208333333333336, 60.325, 60.325, 60.166666666666664, 60.166666666666664, 60.225, 60.225, 59.958333333333336, 59.958333333333336, 60.525, 60.525, 60.275, 60.275, 60.13333333333333, 60.13333333333333, 60.50833333333333, 60.50833333333333, 59.8, 59.8, 59.983333333333334, 59.983333333333334, 59.55, 59.55, 59.55, 59.55, 59.36666666666667, 59.36666666666667, 59.18333333333333, 59.18333333333333, 59.25833333333333, 59.25833333333333, 58.96666666666667, 58.96666666666667, 58.725, 58.725, 58.333333333333336, 58.333333333333336, 57.975, 57.975, 57.81666666666667, 57.81666666666667, 58.24166666666667, 58.24166666666667, 58.56666666666667, 58.56666666666667, 58.266666666666666, 58.266666666666666, 58.6, 58.6, 58.775, 58.775, 58.641666666666666, 58.641666666666666, 59.0, 59.0, 59.025, 59.025, 60.09166666666667, 60.09166666666667, 59.96666666666667, 59.96666666666667, 59.84166666666667, 59.84166666666667, 59.40833333333333, 59.40833333333333, 59.291666666666664, 59.291666666666664, 59.13333333333333, 59.13333333333333, 58.99166666666667, 58.99166666666667, 59.05833333333333, 59.05833333333333, 59.55833333333333, 59.55833333333333, 59.583333333333336, 59.583333333333336, 59.21666666666667, 59.21666666666667, 60.21666666666667, 60.21666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4200
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3500
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.6100
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3833
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4850
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3283
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4817
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5217
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4133
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.278, Test loss: 0.851, Test accuracy: 76.33
Final Round, Global train loss: 0.278, Global test loss: 1.643, Global test accuracy: 53.37
Average accuracy final 10 rounds: 76.78666666666666 

Average global accuracy final 10 rounds: 54.49916666666666 

1833.842521429062
[1.6373441219329834, 3.274688243865967, 4.674486398696899, 6.074284553527832, 7.495852470397949, 8.917420387268066, 10.260642766952515, 11.603865146636963, 13.034350156784058, 14.464835166931152, 15.873880863189697, 17.282926559448242, 18.701925039291382, 20.12092351913452, 21.52277445793152, 22.924625396728516, 24.358070850372314, 25.791516304016113, 31.45678973197937, 37.12206315994263, 38.41804790496826, 39.7140326499939, 40.992297410964966, 42.270562171936035, 43.54398250579834, 44.817402839660645, 46.07681155204773, 47.336220264434814, 48.60011029243469, 49.86400032043457, 51.13505697250366, 52.406113624572754, 53.67752480506897, 54.948935985565186, 56.221818923950195, 57.494701862335205, 58.771300077438354, 60.047898292541504, 61.33158230781555, 62.6152663230896, 63.891053438186646, 65.16684055328369, 66.4443519115448, 67.72186326980591, 69.00075721740723, 70.27965116500854, 71.54642009735107, 72.8131890296936, 74.07834148406982, 75.34349393844604, 76.6070020198822, 77.87051010131836, 79.13870477676392, 80.40689945220947, 81.67160105705261, 82.93630266189575, 84.21386528015137, 85.49142789840698, 86.76067161560059, 88.02991533279419, 89.30528116226196, 90.58064699172974, 91.85515904426575, 93.12967109680176, 94.41229009628296, 95.69490909576416, 96.96076822280884, 98.22662734985352, 99.5081524848938, 100.78967761993408, 102.06496953964233, 103.34026145935059, 104.61650967597961, 105.89275789260864, 107.18376088142395, 108.47476387023926, 109.74603652954102, 111.01730918884277, 112.27436852455139, 113.53142786026001, 114.816326379776, 116.10122489929199, 117.38319730758667, 118.66516971588135, 119.93063592910767, 121.19610214233398, 122.4660873413086, 123.7360725402832, 125.00436902046204, 126.27266550064087, 127.5285017490387, 128.78433799743652, 130.04867482185364, 131.31301164627075, 132.59884572029114, 133.88467979431152, 135.16464400291443, 136.44460821151733, 137.71583724021912, 138.9870662689209, 140.24949598312378, 141.51192569732666, 142.79368901252747, 144.07545232772827, 145.35952353477478, 146.6435947418213, 147.9107928276062, 149.1779909133911, 150.4604640007019, 151.7429370880127, 153.03262305259705, 154.3223090171814, 155.58744835853577, 156.85258769989014, 158.11047911643982, 159.3683705329895, 160.64570593833923, 161.92304134368896, 163.1874644756317, 164.45188760757446, 165.71825051307678, 166.9846134185791, 168.25525569915771, 169.52589797973633, 170.78843474388123, 172.05097150802612, 173.31961584091187, 174.5882601737976, 175.85752630233765, 177.12679243087769, 178.41642594337463, 179.70605945587158, 180.98544478416443, 182.26483011245728, 183.51832056045532, 184.77181100845337, 186.04058718681335, 187.30936336517334, 188.5931990146637, 189.87703466415405, 191.15155673027039, 192.42607879638672, 193.6954791545868, 194.96487951278687, 196.22345876693726, 197.48203802108765, 198.7613079547882, 200.04057788848877, 201.31664109230042, 202.59270429611206, 203.8716790676117, 205.15065383911133, 206.4338550567627, 207.71705627441406, 208.98038625717163, 210.2437162399292, 211.5069637298584, 212.7702112197876, 214.04030680656433, 215.31040239334106, 216.58407878875732, 217.85775518417358, 219.1311230659485, 220.4044909477234, 221.67823195457458, 222.95197296142578, 224.22413778305054, 225.4963026046753, 226.7807219028473, 228.0651412010193, 229.33138513565063, 230.59762907028198, 231.85498785972595, 233.11234664916992, 234.3806209564209, 235.64889526367188, 236.9231526851654, 238.19741010665894, 239.46088123321533, 240.72435235977173, 241.9934413433075, 243.26253032684326, 244.53633332252502, 245.8101363182068, 247.09042811393738, 248.37071990966797, 249.63376426696777, 250.89680862426758, 252.1813292503357, 253.4658498764038, 254.73861622810364, 256.01138257980347, 257.2746663093567, 258.5379500389099, 259.8277816772461, 261.1176133155823, 262.4098014831543, 263.7019896507263, 264.98915123939514, 266.27631282806396, 268.4342465400696, 270.5921802520752]
[22.783333333333335, 22.783333333333335, 43.74166666666667, 43.74166666666667, 47.541666666666664, 47.541666666666664, 50.34166666666667, 50.34166666666667, 53.19166666666667, 53.19166666666667, 55.00833333333333, 55.00833333333333, 60.35, 60.35, 63.166666666666664, 63.166666666666664, 64.76666666666667, 64.76666666666667, 66.61666666666666, 66.61666666666666, 68.1, 68.1, 67.98333333333333, 67.98333333333333, 70.66666666666667, 70.66666666666667, 71.55833333333334, 71.55833333333334, 71.35, 71.35, 71.43333333333334, 71.43333333333334, 71.30833333333334, 71.30833333333334, 71.175, 71.175, 71.30833333333334, 71.30833333333334, 70.7, 70.7, 71.85, 71.85, 71.38333333333334, 71.38333333333334, 70.94166666666666, 70.94166666666666, 71.68333333333334, 71.68333333333334, 72.55833333333334, 72.55833333333334, 71.90833333333333, 71.90833333333333, 72.525, 72.525, 73.18333333333334, 73.18333333333334, 73.33333333333333, 73.33333333333333, 73.95833333333333, 73.95833333333333, 74.23333333333333, 74.23333333333333, 73.83333333333333, 73.83333333333333, 74.0, 74.0, 74.875, 74.875, 74.64166666666667, 74.64166666666667, 74.85833333333333, 74.85833333333333, 74.375, 74.375, 74.825, 74.825, 75.46666666666667, 75.46666666666667, 75.675, 75.675, 74.88333333333334, 74.88333333333334, 74.91666666666667, 74.91666666666667, 75.475, 75.475, 75.34166666666667, 75.34166666666667, 74.83333333333333, 74.83333333333333, 75.29166666666667, 75.29166666666667, 74.96666666666667, 74.96666666666667, 76.30833333333334, 76.30833333333334, 74.70833333333333, 74.70833333333333, 74.6, 74.6, 74.64166666666667, 74.64166666666667, 74.20833333333333, 74.20833333333333, 74.69166666666666, 74.69166666666666, 74.21666666666667, 74.21666666666667, 74.825, 74.825, 75.00833333333334, 75.00833333333334, 75.85833333333333, 75.85833333333333, 76.83333333333333, 76.83333333333333, 75.90833333333333, 75.90833333333333, 75.38333333333334, 75.38333333333334, 74.5, 74.5, 74.775, 74.775, 75.49166666666666, 75.49166666666666, 75.95, 75.95, 76.6, 76.6, 76.88333333333334, 76.88333333333334, 76.29166666666667, 76.29166666666667, 75.69166666666666, 75.69166666666666, 76.35833333333333, 76.35833333333333, 76.31666666666666, 76.31666666666666, 76.86666666666666, 76.86666666666666, 76.3, 76.3, 76.8, 76.8, 76.85, 76.85, 76.85833333333333, 76.85833333333333, 76.41666666666667, 76.41666666666667, 76.325, 76.325, 75.89166666666667, 75.89166666666667, 76.55, 76.55, 76.50833333333334, 76.50833333333334, 76.325, 76.325, 76.59166666666667, 76.59166666666667, 76.63333333333334, 76.63333333333334, 77.05833333333334, 77.05833333333334, 76.18333333333334, 76.18333333333334, 76.01666666666667, 76.01666666666667, 76.575, 76.575, 77.175, 77.175, 76.71666666666667, 76.71666666666667, 76.48333333333333, 76.48333333333333, 76.23333333333333, 76.23333333333333, 76.3, 76.3, 76.56666666666666, 76.56666666666666, 76.55833333333334, 76.55833333333334, 77.325, 77.325, 76.85833333333333, 76.85833333333333, 77.125, 77.125, 76.55, 76.55, 77.08333333333333, 77.08333333333333, 77.26666666666667, 77.26666666666667, 76.325, 76.325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4200
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3500
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.5933
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3833
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4850
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3283
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4233
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5217
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4133
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.283, Test loss: 0.602, Test accuracy: 80.72
Final Round, Global train loss: 0.283, Global test loss: 1.283, Global test accuracy: 62.35
Average accuracy final 10 rounds: 80.35916666666665 

Average global accuracy final 10 rounds: 60.37083333333333 

1990.1691148281097
[1.7886207103729248, 3.5772414207458496, 5.143767833709717, 6.710294246673584, 8.275309801101685, 9.840325355529785, 11.392656087875366, 12.944986820220947, 14.491151094436646, 16.037315368652344, 17.617408752441406, 19.19750213623047, 20.74489140510559, 22.292280673980713, 23.85011386871338, 25.407947063446045, 26.988473415374756, 28.568999767303467, 30.141723155975342, 31.714446544647217, 33.267404556274414, 34.82036256790161, 36.387831687927246, 37.95530080795288, 39.52063179016113, 41.085962772369385, 42.648107290267944, 44.210251808166504, 45.76423740386963, 47.318222999572754, 48.8848934173584, 50.45156383514404, 52.02041578292847, 53.58926773071289, 55.14977169036865, 56.710275650024414, 58.26385259628296, 59.817429542541504, 61.38040852546692, 62.943387508392334, 64.47128224372864, 65.99917697906494, 67.46401715278625, 68.92885732650757, 70.44912767410278, 71.969398021698, 73.50311183929443, 75.03682565689087, 76.57177066802979, 78.1067156791687, 79.65856313705444, 81.21041059494019, 82.74449610710144, 84.2785816192627, 85.82548546791077, 87.37238931655884, 88.91793417930603, 90.46347904205322, 92.00880074501038, 93.55412244796753, 95.09914994239807, 96.64417743682861, 98.17616534233093, 99.70815324783325, 101.24955224990845, 102.79095125198364, 104.33381843566895, 105.87668561935425, 107.41671204566956, 108.95673847198486, 110.4920027256012, 112.02726697921753, 113.57481408119202, 115.1223611831665, 116.66051769256592, 118.19867420196533, 119.73876523971558, 121.27885627746582, 122.83860802650452, 124.39835977554321, 125.96170115470886, 127.52504253387451, 129.05164527893066, 130.57824802398682, 132.111656665802, 133.6450653076172, 135.18380784988403, 136.72255039215088, 138.25339102745056, 139.78423166275024, 141.31417870521545, 142.84412574768066, 144.3847577571869, 145.92538976669312, 147.4699718952179, 149.01455402374268, 150.53855156898499, 152.0625491142273, 153.62139511108398, 155.18024110794067, 156.72949194908142, 158.27874279022217, 159.80504727363586, 161.33135175704956, 162.88598155975342, 164.44061136245728, 166.00331044197083, 167.56600952148438, 169.10351657867432, 170.64102363586426, 172.16982889175415, 173.69863414764404, 175.25996661186218, 176.82129907608032, 178.36141729354858, 179.90153551101685, 181.4452953338623, 182.98905515670776, 184.5305507183075, 186.07204627990723, 187.61679792404175, 189.16154956817627, 190.68400168418884, 192.20645380020142, 193.7415292263031, 195.27660465240479, 196.82634782791138, 198.37609100341797, 199.9028069972992, 201.42952299118042, 202.98282742500305, 204.53613185882568, 206.0887610912323, 207.64139032363892, 209.1790497303009, 210.7167091369629, 212.10390973091125, 213.49111032485962, 215.03250193595886, 216.5738935470581, 218.1166868209839, 219.65948009490967, 221.1988446712494, 222.7382092475891, 224.29313254356384, 225.84805583953857, 227.39652252197266, 228.94498920440674, 230.5014636516571, 232.05793809890747, 233.60219597816467, 235.14645385742188, 236.7044062614441, 238.2623586654663, 239.81573033332825, 241.36910200119019, 242.92844319343567, 244.48778438568115, 246.0297291278839, 247.57167387008667, 249.12208890914917, 250.67250394821167, 252.23224639892578, 253.7919888496399, 255.3404197692871, 256.8888506889343, 258.4602065086365, 260.0315623283386, 261.5762050151825, 263.12084770202637, 264.6607418060303, 266.2006359100342, 267.75020933151245, 269.2997827529907, 270.8482623100281, 272.39674186706543, 273.93147444725037, 275.4662070274353, 277.02607893943787, 278.58595085144043, 280.15376019477844, 281.72156953811646, 283.281005859375, 284.84044218063354, 286.29789686203003, 287.7553515434265, 289.3169240951538, 290.8784966468811, 292.42604398727417, 293.97359132766724, 295.523916721344, 297.07424211502075, 298.6238205432892, 300.1733989715576, 301.72414922714233, 303.27489948272705, 304.8218243122101, 306.3687491416931, 307.91914081573486, 309.4695324897766, 312.05624628067017, 314.6429600715637]
[31.575, 31.575, 43.81666666666667, 43.81666666666667, 51.075, 51.075, 52.416666666666664, 52.416666666666664, 55.53333333333333, 55.53333333333333, 58.775, 58.775, 60.075, 60.075, 64.23333333333333, 64.23333333333333, 67.425, 67.425, 68.425, 68.425, 70.68333333333334, 70.68333333333334, 70.225, 70.225, 74.35, 74.35, 73.98333333333333, 73.98333333333333, 75.29166666666667, 75.29166666666667, 74.1, 74.1, 74.21666666666667, 74.21666666666667, 75.1, 75.1, 74.575, 74.575, 74.65833333333333, 74.65833333333333, 76.14166666666667, 76.14166666666667, 76.86666666666666, 76.86666666666666, 76.94166666666666, 76.94166666666666, 77.05, 77.05, 76.74166666666666, 76.74166666666666, 76.98333333333333, 76.98333333333333, 76.00833333333334, 76.00833333333334, 76.91666666666667, 76.91666666666667, 78.33333333333333, 78.33333333333333, 77.975, 77.975, 78.18333333333334, 78.18333333333334, 77.71666666666667, 77.71666666666667, 78.325, 78.325, 78.09166666666667, 78.09166666666667, 77.53333333333333, 77.53333333333333, 78.10833333333333, 78.10833333333333, 79.34166666666667, 79.34166666666667, 79.68333333333334, 79.68333333333334, 79.86666666666666, 79.86666666666666, 80.125, 80.125, 79.66666666666667, 79.66666666666667, 80.525, 80.525, 80.65, 80.65, 80.46666666666667, 80.46666666666667, 80.725, 80.725, 80.15, 80.15, 80.56666666666666, 80.56666666666666, 80.61666666666666, 80.61666666666666, 80.875, 80.875, 80.80833333333334, 80.80833333333334, 79.65, 79.65, 80.58333333333333, 80.58333333333333, 80.425, 80.425, 80.55833333333334, 80.55833333333334, 80.64166666666667, 80.64166666666667, 80.825, 80.825, 80.95, 80.95, 80.65, 80.65, 80.70833333333333, 80.70833333333333, 79.06666666666666, 79.06666666666666, 79.45, 79.45, 79.51666666666667, 79.51666666666667, 79.85833333333333, 79.85833333333333, 78.725, 78.725, 79.025, 79.025, 79.675, 79.675, 79.575, 79.575, 79.05, 79.05, 80.23333333333333, 80.23333333333333, 80.01666666666667, 80.01666666666667, 80.76666666666667, 80.76666666666667, 79.94166666666666, 79.94166666666666, 80.31666666666666, 80.31666666666666, 80.01666666666667, 80.01666666666667, 79.975, 79.975, 80.25833333333334, 80.25833333333334, 80.3, 80.3, 81.46666666666667, 81.46666666666667, 79.7, 79.7, 80.16666666666667, 80.16666666666667, 79.49166666666666, 79.49166666666666, 79.875, 79.875, 79.30833333333334, 79.30833333333334, 79.65833333333333, 79.65833333333333, 81.25833333333334, 81.25833333333334, 81.25833333333334, 81.25833333333334, 79.76666666666667, 79.76666666666667, 79.83333333333333, 79.83333333333333, 79.5, 79.5, 80.575, 80.575, 79.825, 79.825, 80.40833333333333, 80.40833333333333, 80.34166666666667, 80.34166666666667, 80.98333333333333, 80.98333333333333, 81.10833333333333, 81.10833333333333, 80.99166666666666, 80.99166666666666, 80.16666666666667, 80.16666666666667, 80.04166666666667, 80.04166666666667, 79.69166666666666, 79.69166666666666, 80.03333333333333, 80.03333333333333, 80.71666666666667, 80.71666666666667]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4200
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3500
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.5933
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3833
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4850
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3283
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4800
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5383
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4133
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4200
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3500
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.5933
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3833
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4850
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3933
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4233
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5217
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4133
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  16.3200
Round 1 global test acc  18.2400
Round 2 global test acc  22.9300
Round 3 global test acc  21.6400
Round 4 global test acc  27.8000
Round 5 global test acc  25.8900
Round 6 global test acc  21.7800
Round 7 global test acc  20.1500
Round 8 global test acc  30.7100
Round 9 global test acc  24.9000
Round 10 global test acc  31.4800
Round 11 global test acc  25.0000
Round 12 global test acc  23.1900
Round 13 global test acc  34.6300
Round 14 global test acc  29.8800
Round 15 global test acc  28.4500
Round 16 global test acc  20.1700
Round 17 global test acc  20.3300
Round 18 global test acc  34.5800
Round 19 global test acc  34.8400
Round 20 global test acc  30.6300
Round 21 global test acc  25.1500
Round 22 global test acc  30.8000
Round 23 global test acc  23.9200
Round 24 global test acc  31.4900
Round 25 global test acc  29.3400
Round 26 global test acc  30.1600
Round 27 global test acc  35.5600
Round 28 global test acc  32.0800
Round 29 global test acc  35.6300
Round 30 global test acc  22.6800
Round 31 global test acc  37.4000
Round 32 global test acc  32.9900
Round 33 global test acc  25.4900
Round 34 global test acc  27.0300
Round 35 global test acc  35.7000
Round 36 global test acc  32.6000
Round 37 global test acc  25.6900
Round 38 global test acc  36.3100
Round 39 global test acc  25.7000
Round 40 global test acc  37.0900
Round 41 global test acc  34.4600
Round 42 global test acc  30.0600
Round 43 global test acc  31.9800
Round 44 global test acc  31.5700
Round 45 global test acc  37.0600
Round 46 global test acc  30.0500
Round 47 global test acc  37.9400
Round 48 global test acc  35.0900
Round 49 global test acc  37.6900
Round 50 global test acc  37.2600
Round 51 global test acc  37.9800
Round 52 global test acc  26.9900
Round 53 global test acc  29.1000
Round 54 global test acc  39.0800
Round 55 global test acc  30.6900
Round 56 global test acc  31.4100
Round 57 global test acc  35.7100
Round 58 global test acc  41.6800
Round 59 global test acc  40.6100
Round 60 global test acc  41.2000
Round 61 global test acc  32.0300
Round 62 global test acc  37.9200
Round 63 global test acc  35.5000
Round 64 global test acc  34.8500
Round 65 global test acc  27.9700
Round 66 global test acc  27.1600
Round 67 global test acc  38.4200
Round 68 global test acc  40.3100
Round 69 global test acc  40.4700
Round 70 global test acc  33.0400
Round 71 global test acc  29.2600
Round 72 global test acc  36.3800
Round 73 global test acc  32.9700
Round 74 global test acc  31.4700
Round 75 global test acc  32.2400
Round 76 global test acc  33.0000
Round 77 global test acc  28.7800
Round 78 global test acc  38.3200
Round 79 global test acc  29.5200
Round 80 global test acc  24.9800
Round 81 global test acc  21.3800
Round 82 global test acc  17.7800
Round 83 global test acc  18.3800
Round 84 global test acc  16.7100
Round 85 global test acc  14.6400
Round 86 global test acc  13.8000
Round 87 global test acc  12.9400
Round 88 global test acc  12.4700
Round 89 global test acc  12.4500
Round 90 global test acc  12.4000
Round 91 global test acc  12.6900
Round 92 global test acc  12.7800
Round 93 global test acc  13.4600
Round 94 global test acc  15.6700
Round 95 global test acc  14.3700
Round 96 global test acc  14.1900
Round 97 global test acc  15.0000
Round 98 global test acc  15.0400
Round 99 global test acc  13.6000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4200
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3500
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.5933
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.3833
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.5300
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3900
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.4233
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5217
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4133
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.319, Test loss: 0.440, Test accuracy: 84.58
Average accuracy final 10 rounds: 84.5775
1343.0979919433594
[2.0103564262390137, 3.755617141723633, 5.5041258335113525, 7.235880136489868, 8.965280771255493, 10.71788477897644, 12.487027883529663, 14.221415042877197, 15.805302143096924, 17.382317304611206, 18.96473741531372, 20.561373949050903, 22.15948486328125, 23.745849132537842, 25.32955574989319, 26.908674478530884, 28.49711012840271, 30.09543228149414, 31.676491737365723, 33.26776099205017, 34.87910079956055, 36.471596240997314, 38.05144000053406, 39.63877749443054, 41.23184132575989, 42.823402643203735, 44.39488196372986, 45.971282958984375, 47.56241822242737, 49.145601749420166, 50.73298358917236, 52.32008957862854, 53.876195669174194, 55.46586799621582, 57.05380845069885, 58.64701771736145, 60.24399948120117, 61.84833002090454, 63.43580722808838, 65.02758550643921, 66.6237485408783, 68.20398020744324, 69.79210495948792, 71.3842089176178, 72.9780056476593, 74.57995009422302, 76.1828773021698, 77.77731132507324, 79.38699722290039, 81.01498913764954, 82.6399712562561, 84.24323177337646, 85.84888935089111, 87.44488477706909, 89.04520845413208, 90.65118598937988, 92.26652145385742, 93.89174771308899, 95.47338724136353, 97.07300162315369, 98.65631079673767, 100.26310610771179, 101.8711166381836, 103.47988176345825, 105.09566760063171, 106.69117045402527, 108.29505610466003, 109.90142345428467, 111.51717495918274, 113.12097144126892, 114.74277973175049, 116.34086179733276, 117.9456877708435, 119.53782224655151, 121.14172530174255, 122.74544644355774, 124.36862707138062, 125.9769070148468, 127.59277129173279, 129.22431182861328, 130.83180117607117, 132.4582552909851, 134.05593490600586, 135.67166566848755, 137.29453492164612, 138.88161659240723, 140.4692792892456, 142.0919804573059, 143.70989656448364, 145.3388102054596, 146.93674445152283, 148.550297498703, 150.1645393371582, 151.77999544143677, 153.40634775161743, 154.99738693237305, 156.63313460350037, 158.26745223999023, 159.8711166381836, 161.48689436912537, 163.61310124397278]
[27.95, 39.416666666666664, 45.3, 49.69166666666667, 53.55, 58.166666666666664, 60.166666666666664, 63.49166666666667, 65.875, 66.16666666666667, 68.75833333333334, 69.44166666666666, 74.275, 74.94166666666666, 75.91666666666667, 75.625, 75.98333333333333, 75.8, 77.68333333333334, 77.45, 78.25833333333334, 78.05833333333334, 78.125, 78.5, 79.4, 79.24166666666666, 80.06666666666666, 80.1, 80.61666666666666, 80.05, 80.65, 81.0, 81.01666666666667, 79.7, 80.60833333333333, 81.05833333333334, 81.50833333333334, 81.55, 81.88333333333334, 82.15, 82.11666666666666, 81.825, 81.95, 81.90833333333333, 82.18333333333334, 82.1, 82.83333333333333, 82.21666666666667, 82.38333333333334, 81.93333333333334, 82.59166666666667, 82.725, 82.8, 82.55, 82.55, 82.55833333333334, 82.625, 82.35, 82.99166666666666, 82.35833333333333, 82.225, 82.56666666666666, 82.875, 82.70833333333333, 83.225, 83.65, 83.40833333333333, 83.325, 83.61666666666666, 83.41666666666667, 83.8, 83.13333333333334, 84.13333333333334, 84.15, 84.08333333333333, 84.30833333333334, 84.025, 84.23333333333333, 83.75833333333334, 83.66666666666667, 83.875, 83.95, 83.725, 83.9, 84.16666666666667, 84.16666666666667, 84.55, 84.4, 83.98333333333333, 84.29166666666667, 84.55, 84.19166666666666, 84.61666666666666, 84.675, 84.35, 84.80833333333334, 84.925, 84.66666666666667, 84.45, 84.54166666666667, 84.58333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.4200
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.3500
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.5933
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4833
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.5700
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.4850
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.3283
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5517
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.5217
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.4133
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.514, Test loss: 2.097, Test accuracy: 28.32
Round   1, Train loss: 0.970, Test loss: 1.801, Test accuracy: 41.91
Round   2, Train loss: 0.888, Test loss: 1.416, Test accuracy: 47.92
Round   3, Train loss: 0.770, Test loss: 1.560, Test accuracy: 50.33
Round   4, Train loss: 0.717, Test loss: 1.320, Test accuracy: 55.39
Round   5, Train loss: 0.705, Test loss: 1.109, Test accuracy: 60.77
Round   6, Train loss: 0.588, Test loss: 1.086, Test accuracy: 61.50
Round   7, Train loss: 0.698, Test loss: 0.946, Test accuracy: 64.92
Round   8, Train loss: 0.694, Test loss: 0.872, Test accuracy: 67.38
Round   9, Train loss: 0.566, Test loss: 0.916, Test accuracy: 69.01
Round  10, Train loss: 0.660, Test loss: 0.683, Test accuracy: 73.13
Round  11, Train loss: 0.630, Test loss: 0.753, Test accuracy: 72.76
Round  12, Train loss: 0.603, Test loss: 0.568, Test accuracy: 76.97
Round  13, Train loss: 0.561, Test loss: 0.575, Test accuracy: 77.11
Round  14, Train loss: 0.518, Test loss: 0.563, Test accuracy: 77.23
Round  15, Train loss: 0.528, Test loss: 0.551, Test accuracy: 77.87
Round  16, Train loss: 0.497, Test loss: 0.546, Test accuracy: 78.17
Round  17, Train loss: 0.582, Test loss: 0.526, Test accuracy: 78.86
Round  18, Train loss: 0.545, Test loss: 0.511, Test accuracy: 79.23
Round  19, Train loss: 0.651, Test loss: 0.511, Test accuracy: 79.59
Round  20, Train loss: 0.554, Test loss: 0.498, Test accuracy: 79.76
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 235, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx], iter_num_now = iter, train_iter=iter)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1966, in train
    self.filter_by_loss2(net=net, concept_matrix_local=concept_matrix_local, iter_num=iter2,
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1809, in filter_by_loss2
    loss = self.loss_func(log_probs, lable_tmp)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1179, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/nn/functional.py", line 3059, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1550
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.2517
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4050
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5833
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4900
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5183
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0567
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4200
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.2833
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6350
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0533
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1750
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4867
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.2567
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.211, Test loss: 1.849, Test accuracy: 59.50
Final Round, Global train loss: 0.211, Global test loss: 2.169, Global test accuracy: 25.12
Average accuracy final 10 rounds: 58.989166666666655 

Average global accuracy final 10 rounds: 28.815833333333327 

1959.2967808246613
[1.7472174167633057, 3.4944348335266113, 4.982839107513428, 6.471243381500244, 7.895496129989624, 9.319748878479004, 10.751505613327026, 12.183262348175049, 13.614614248275757, 15.045966148376465, 16.458154678344727, 17.87034320831299, 19.28513479232788, 20.699926376342773, 22.101481676101685, 23.503036975860596, 24.921481609344482, 26.33992624282837, 27.841894388198853, 29.343862533569336, 30.844578742980957, 32.34529495239258, 33.844239950180054, 35.34318494796753, 36.82672905921936, 38.31027317047119, 39.785780906677246, 41.2612886428833, 42.7511146068573, 44.2409405708313, 45.714067220687866, 47.187193870544434, 48.66753792762756, 50.14788198471069, 51.632964849472046, 53.1180477142334, 54.622803688049316, 56.127559661865234, 57.6313054561615, 59.135051250457764, 60.62659740447998, 62.1181435585022, 63.604825496673584, 65.09150743484497, 66.57306551933289, 68.0546236038208, 69.53756713867188, 71.02051067352295, 72.50407719612122, 73.98764371871948, 75.45978832244873, 76.93193292617798, 78.42895030975342, 79.92596769332886, 81.42413997650146, 82.92231225967407, 84.4276831150055, 85.93305397033691, 87.43296074867249, 88.93286752700806, 90.41054630279541, 91.88822507858276, 93.37493252754211, 94.86163997650146, 96.35809898376465, 97.85455799102783, 99.35006499290466, 100.8455719947815, 102.33202862739563, 103.81848526000977, 105.30063104629517, 106.78277683258057, 108.27061939239502, 109.75846195220947, 111.25738453865051, 112.75630712509155, 114.22591543197632, 115.69552373886108, 117.17234396934509, 118.6491641998291, 120.14950180053711, 121.64983940124512, 123.14001750946045, 124.63019561767578, 126.12195348739624, 127.6137113571167, 129.0979449748993, 130.58217859268188, 132.08340883255005, 133.5846390724182, 135.08128333091736, 136.5779275894165, 138.08170628547668, 139.58548498153687, 141.0368950366974, 142.4883050918579, 143.86523342132568, 145.24216175079346, 146.62702131271362, 148.0118808746338, 149.4291455745697, 150.84641027450562, 152.2229745388031, 153.5995388031006, 154.8941729068756, 156.18880701065063, 157.48558163642883, 158.78235626220703, 160.0867748260498, 161.39119338989258, 162.68430423736572, 163.97741508483887, 165.27050137519836, 166.56358766555786, 167.848979473114, 169.13437128067017, 170.41373205184937, 171.69309282302856, 172.98080396652222, 174.26851511001587, 175.5472137928009, 176.82591247558594, 178.12165331840515, 179.41739416122437, 180.69635367393494, 181.9753131866455, 183.36306142807007, 184.75080966949463, 186.15209674835205, 187.55338382720947, 188.94985222816467, 190.34632062911987, 191.73861956596375, 193.13091850280762, 194.53481030464172, 195.93870210647583, 197.35263633728027, 198.76657056808472, 200.1858148574829, 201.6050591468811, 203.01643991470337, 204.42782068252563, 205.8722333908081, 207.31664609909058, 208.7851059436798, 210.25356578826904, 211.73884558677673, 213.22412538528442, 214.69686794281006, 216.1696105003357, 217.64291167259216, 219.11621284484863, 220.5940055847168, 222.07179832458496, 223.54216146469116, 225.01252460479736, 226.48324513435364, 227.9539656639099, 229.42449522018433, 230.89502477645874, 232.3647620677948, 233.83449935913086, 235.30981636047363, 236.7851333618164, 238.2608392238617, 239.73654508590698, 241.14392185211182, 242.55129861831665, 243.99247407913208, 245.4336495399475, 246.84422612190247, 248.25480270385742, 249.6644070148468, 251.07401132583618, 252.55178213119507, 254.02955293655396, 255.50296783447266, 256.97638273239136, 258.44531893730164, 259.9142551422119, 261.37766337394714, 262.8410716056824, 264.32158756256104, 265.8021035194397, 267.2780272960663, 268.75395107269287, 270.2293972969055, 271.70484352111816, 273.18052530288696, 274.65620708465576, 276.17827796936035, 277.70034885406494, 279.20115852355957, 280.7019681930542, 282.19017577171326, 283.6783833503723, 285.15577960014343, 286.63317584991455, 288.1245675086975, 289.61595916748047, 292.12470626831055, 294.6334533691406]
[28.166666666666668, 28.166666666666668, 35.78333333333333, 35.78333333333333, 42.03333333333333, 42.03333333333333, 44.358333333333334, 44.358333333333334, 50.375, 50.375, 54.358333333333334, 54.358333333333334, 56.35, 56.35, 57.0, 57.0, 57.475, 57.475, 59.05, 59.05, 60.516666666666666, 60.516666666666666, 61.2, 61.2, 60.825, 60.825, 61.49166666666667, 61.49166666666667, 62.416666666666664, 62.416666666666664, 62.49166666666667, 62.49166666666667, 62.075, 62.075, 63.81666666666667, 63.81666666666667, 62.8, 62.8, 63.166666666666664, 63.166666666666664, 63.03333333333333, 63.03333333333333, 63.80833333333333, 63.80833333333333, 62.833333333333336, 62.833333333333336, 62.125, 62.125, 61.825, 61.825, 62.166666666666664, 62.166666666666664, 62.516666666666666, 62.516666666666666, 62.21666666666667, 62.21666666666667, 62.13333333333333, 62.13333333333333, 61.06666666666667, 61.06666666666667, 61.81666666666667, 61.81666666666667, 63.09166666666667, 63.09166666666667, 63.19166666666667, 63.19166666666667, 63.3, 63.3, 64.35833333333333, 64.35833333333333, 63.708333333333336, 63.708333333333336, 63.13333333333333, 63.13333333333333, 63.766666666666666, 63.766666666666666, 63.46666666666667, 63.46666666666667, 64.21666666666667, 64.21666666666667, 63.916666666666664, 63.916666666666664, 63.94166666666667, 63.94166666666667, 62.983333333333334, 62.983333333333334, 62.6, 62.6, 62.24166666666667, 62.24166666666667, 61.391666666666666, 61.391666666666666, 60.916666666666664, 60.916666666666664, 61.483333333333334, 61.483333333333334, 61.141666666666666, 61.141666666666666, 60.71666666666667, 60.71666666666667, 61.266666666666666, 61.266666666666666, 60.483333333333334, 60.483333333333334, 59.708333333333336, 59.708333333333336, 60.333333333333336, 60.333333333333336, 60.15, 60.15, 60.041666666666664, 60.041666666666664, 60.525, 60.525, 59.825, 59.825, 60.225, 60.225, 60.666666666666664, 60.666666666666664, 60.3, 60.3, 60.5, 60.5, 60.475, 60.475, 60.96666666666667, 60.96666666666667, 61.025, 61.025, 60.80833333333333, 60.80833333333333, 61.09166666666667, 61.09166666666667, 61.81666666666667, 61.81666666666667, 60.56666666666667, 60.56666666666667, 59.483333333333334, 59.483333333333334, 59.65, 59.65, 60.13333333333333, 60.13333333333333, 59.708333333333336, 59.708333333333336, 59.675, 59.675, 60.05833333333333, 60.05833333333333, 59.875, 59.875, 60.525, 60.525, 60.641666666666666, 60.641666666666666, 60.166666666666664, 60.166666666666664, 59.775, 59.775, 59.34166666666667, 59.34166666666667, 58.833333333333336, 58.833333333333336, 58.96666666666667, 58.96666666666667, 58.56666666666667, 58.56666666666667, 58.975, 58.975, 58.825, 58.825, 59.09166666666667, 59.09166666666667, 59.75, 59.75, 60.333333333333336, 60.333333333333336, 59.85, 59.85, 59.075, 59.075, 59.208333333333336, 59.208333333333336, 58.84166666666667, 58.84166666666667, 58.93333333333333, 58.93333333333333, 58.475, 58.475, 58.74166666666667, 58.74166666666667, 58.075, 58.075, 59.18333333333333, 59.18333333333333, 59.95, 59.95, 59.40833333333333, 59.40833333333333, 59.5, 59.5]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1550
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0433
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4050
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5833
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0083
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4900
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5183
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0567
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4200
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3817
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6317
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.4833
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4883
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3933
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.307, Test loss: 1.102, Test accuracy: 72.51
Final Round, Global train loss: 0.307, Global test loss: 0.733, Global test accuracy: 78.33
Average accuracy final 10 rounds: 71.56375 

Average global accuracy final 10 rounds: 77.8935 

6209.057677030563
[5.481778860092163, 10.963557720184326, 16.158980131149292, 21.354402542114258, 26.586080074310303, 31.817757606506348, 37.04833650588989, 42.27891540527344, 47.52492094039917, 52.7709264755249, 58.04335641860962, 63.315786361694336, 68.48512125015259, 73.65445613861084, 78.68230509757996, 83.71015405654907, 88.72142124176025, 93.73268842697144, 98.72413420677185, 103.71557998657227, 108.72953939437866, 113.74349880218506, 118.76137065887451, 123.77924251556396, 128.7738709449768, 133.76849937438965, 138.8040645122528, 143.83962965011597, 148.87707614898682, 153.91452264785767, 158.91830778121948, 163.9220929145813, 168.9589822292328, 173.99587154388428, 179.05618691444397, 184.11650228500366, 189.3450222015381, 194.5735421180725, 199.57852363586426, 204.583505153656, 209.59701013565063, 214.61051511764526, 219.64234066009521, 224.67416620254517, 229.69830918312073, 234.7224521636963, 239.9860064983368, 245.2495608329773, 249.67254376411438, 254.09552669525146, 258.6163589954376, 263.1371912956238, 267.52130913734436, 271.90542697906494, 276.29197907447815, 280.67853116989136, 285.065372467041, 289.4522137641907, 293.81020188331604, 298.1681900024414, 302.5750985145569, 306.98200702667236, 311.35497760772705, 315.72794818878174, 320.0873353481293, 324.4467225074768, 328.80770087242126, 333.1686792373657, 338.2955050468445, 343.42233085632324, 347.78828525543213, 352.154239654541, 356.4759101867676, 360.79758071899414, 365.1531753540039, 369.5087699890137, 373.8889675140381, 378.2691650390625, 382.6172556877136, 386.96534633636475, 391.59204268455505, 396.21873903274536, 400.6404585838318, 405.0621781349182, 409.4419894218445, 413.82180070877075, 418.5922408103943, 423.3626809120178, 427.7302360534668, 432.09779119491577, 436.46185779571533, 440.8259243965149, 445.1914687156677, 449.55701303482056, 453.9645285606384, 458.3720440864563, 462.7258620262146, 467.0796799659729, 471.46340227127075, 475.8471245765686, 480.2126655578613, 484.57820653915405, 488.9236147403717, 493.26902294158936, 497.59556221961975, 501.92210149765015, 506.2338891029358, 510.54567670822144, 514.8851773738861, 519.2246780395508, 523.7262072563171, 528.2277364730835, 532.5629563331604, 536.8981761932373, 541.2170610427856, 545.535945892334, 549.9402351379395, 554.3445243835449, 558.7104487419128, 563.0763731002808, 567.4519698619843, 571.8275666236877, 576.1825895309448, 580.5376124382019, 584.8976407051086, 589.2576689720154, 593.6427390575409, 598.0278091430664, 602.3987782001495, 606.7697472572327, 611.093507528305, 615.4172677993774, 619.8549847602844, 624.2927017211914, 628.8343322277069, 633.3759627342224, 637.8754723072052, 642.374981880188, 646.9112071990967, 651.4474325180054, 655.8091406822205, 660.1708488464355, 664.5189125537872, 668.8669762611389, 673.2237713336945, 677.58056640625, 681.9696207046509, 686.3586750030518, 690.673734664917, 694.9887943267822, 699.4203202724457, 703.8518462181091, 708.2931907176971, 712.7345352172852, 717.0758180618286, 721.4171009063721, 725.7182459831238, 730.0193910598755, 734.6400420665741, 739.2606930732727, 743.6412651538849, 748.0218372344971, 752.4035153388977, 756.7851934432983, 761.1629486083984, 765.5407037734985, 769.8736088275909, 774.2065138816833, 778.5659675598145, 782.9254212379456, 787.3297605514526, 791.7340998649597, 796.1292581558228, 800.5244164466858, 804.8761942386627, 809.2279720306396, 813.5333621501923, 817.8387522697449, 822.1076033115387, 826.3764543533325, 830.6548244953156, 834.9331946372986, 839.2235355377197, 843.5138764381409, 847.8034267425537, 852.0929770469666, 856.3492107391357, 860.6054444313049, 864.8625044822693, 869.1195645332336, 873.4156174659729, 877.7116703987122, 882.0190672874451, 886.326464176178, 890.6575951576233, 894.9887261390686, 899.3254001140594, 903.6620740890503, 908.0012102127075, 912.3403463363647, 914.506998538971, 916.6736507415771]
[37.3, 37.3, 41.975, 41.975, 44.9625, 44.9625, 48.2525, 48.2525, 48.9025, 48.9025, 51.0, 51.0, 52.2775, 52.2775, 52.6, 52.6, 55.8625, 55.8625, 56.4075, 56.4075, 57.46, 57.46, 58.4325, 58.4325, 59.6425, 59.6425, 59.96, 59.96, 61.03, 61.03, 61.585, 61.585, 61.85, 61.85, 62.86, 62.86, 64.05, 64.05, 64.1925, 64.1925, 64.205, 64.205, 64.815, 64.815, 65.28, 65.28, 65.32, 65.32, 65.62, 65.62, 66.4, 66.4, 66.995, 66.995, 67.0275, 67.0275, 67.3125, 67.3125, 67.7, 67.7, 67.5175, 67.5175, 67.5725, 67.5725, 67.99, 67.99, 67.94, 67.94, 67.9375, 67.9375, 68.3025, 68.3025, 68.8025, 68.8025, 68.915, 68.915, 68.9325, 68.9325, 68.915, 68.915, 68.8775, 68.8775, 69.1075, 69.1075, 69.2575, 69.2575, 69.31, 69.31, 69.425, 69.425, 69.2225, 69.2225, 69.2575, 69.2575, 69.5975, 69.5975, 69.8325, 69.8325, 69.42, 69.42, 69.4325, 69.4325, 69.39, 69.39, 69.685, 69.685, 69.86, 69.86, 70.3275, 70.3275, 70.2, 70.2, 70.27, 70.27, 70.4475, 70.4475, 70.4825, 70.4825, 70.52, 70.52, 70.4125, 70.4125, 70.555, 70.555, 70.695, 70.695, 70.4925, 70.4925, 70.8075, 70.8075, 70.7425, 70.7425, 71.1625, 71.1625, 71.2025, 71.2025, 71.3225, 71.3225, 71.125, 71.125, 70.9775, 70.9775, 71.045, 71.045, 70.9475, 70.9475, 70.7425, 70.7425, 70.765, 70.765, 70.9, 70.9, 71.1475, 71.1475, 71.2075, 71.2075, 71.0975, 71.0975, 71.1025, 71.1025, 71.2475, 71.2475, 71.5575, 71.5575, 71.3275, 71.3275, 71.29, 71.29, 71.7525, 71.7525, 71.6725, 71.6725, 71.2825, 71.2825, 71.5675, 71.5675, 71.5275, 71.5275, 71.775, 71.775, 71.7775, 71.7775, 71.7625, 71.7625, 71.8475, 71.8475, 71.5975, 71.5975, 71.25, 71.25, 71.375, 71.375, 71.66, 71.66, 71.6675, 71.6675, 71.325, 71.325, 71.375, 71.375, 72.5125, 72.5125]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.6333
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1550
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0433
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.4050
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.5833
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0233
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.4900
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.5267
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0667
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.4200
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.3867
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.6317
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0000
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.1750
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.4767
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3900
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.433, Test loss: 1.052, Test accuracy: 69.43
Final Round, Global train loss: 0.433, Global test loss: 0.738, Global test accuracy: 76.36
Average accuracy final 10 rounds: 70.11 

Average global accuracy final 10 rounds: 76.30874999999999 

6578.355760335922
[5.794817209243774, 11.589634418487549, 17.215885162353516, 22.842135906219482, 28.44204354286194, 34.041951179504395, 39.66023230552673, 45.27851343154907, 50.892247676849365, 56.50598192214966, 62.111422061920166, 67.71686220169067, 73.30906295776367, 78.90126371383667, 84.49393486976624, 90.0866060256958, 95.71170282363892, 101.33679962158203, 106.93196702003479, 112.52713441848755, 118.06572341918945, 123.60431241989136, 129.19810795783997, 134.79190349578857, 140.34940648078918, 145.9069094657898, 151.46411561965942, 157.02132177352905, 162.6098747253418, 168.19842767715454, 173.78607487678528, 179.37372207641602, 184.90879464149475, 190.4438672065735, 195.99012517929077, 201.53638315200806, 207.10671496391296, 212.67704677581787, 218.25403714179993, 223.83102750778198, 229.41879844665527, 235.00656938552856, 240.55932879447937, 246.11208820343018, 251.71013140678406, 257.30817461013794, 262.90157556533813, 268.49497652053833, 274.08751368522644, 279.68005084991455, 285.1973292827606, 290.7146077156067, 296.22184348106384, 301.729079246521, 307.2364647388458, 312.74385023117065, 318.21818804740906, 323.69252586364746, 329.2005560398102, 334.7085862159729, 340.21367025375366, 345.7187542915344, 351.24546098709106, 356.7721676826477, 362.30451250076294, 367.8368573188782, 373.37819361686707, 378.91952991485596, 384.4605996608734, 390.00166940689087, 395.5476529598236, 401.09363651275635, 406.64755034446716, 412.201464176178, 417.7165665626526, 423.2316689491272, 428.772677898407, 434.31368684768677, 439.8444883823395, 445.3752899169922, 450.891117811203, 456.4069457054138, 461.9383957386017, 467.46984577178955, 472.9851679801941, 478.50049018859863, 484.0669014453888, 489.63331270217896, 495.19624972343445, 500.75918674468994, 506.324259519577, 511.8893322944641, 517.4229557514191, 522.956579208374, 527.6084263324738, 532.2602734565735, 536.91344165802, 541.5666098594666, 546.220046043396, 550.8734822273254, 555.4987185001373, 560.1239547729492, 564.7598676681519, 569.3957805633545, 574.0556740760803, 578.7155675888062, 583.4609339237213, 588.2063002586365, 592.8663365840912, 597.5263729095459, 602.1548478603363, 606.7833228111267, 611.4095029830933, 616.0356831550598, 620.683248758316, 625.3308143615723, 629.9921653270721, 634.653516292572, 639.2974801063538, 643.9414439201355, 648.6226494312286, 653.3038549423218, 657.9405584335327, 662.5772619247437, 667.2084269523621, 671.8395919799805, 676.9901151657104, 682.1406383514404, 687.0385925769806, 691.9365468025208, 696.6408171653748, 701.3450875282288, 706.1292049884796, 710.9133224487305, 715.5810699462891, 720.2488174438477, 725.546275138855, 730.8437328338623, 736.1609842777252, 741.4782357215881, 746.7808232307434, 752.0834107398987, 757.3948497772217, 762.7062888145447, 768.0500490665436, 773.3938093185425, 778.7064592838287, 784.019109249115, 789.2916870117188, 794.5642647743225, 799.8262979984283, 805.0883312225342, 810.2948923110962, 815.5014533996582, 820.759819984436, 826.0181865692139, 831.311404466629, 836.6046223640442, 841.2804272174835, 845.9562320709229, 850.6170527935028, 855.2778735160828, 859.9589178562164, 864.6399621963501, 869.2900605201721, 873.9401588439941, 878.5652797222137, 883.1904006004333, 887.8820445537567, 892.5736885070801, 897.2199599742889, 901.8662314414978, 907.1212692260742, 912.3763070106506, 917.6446213722229, 922.9129357337952, 928.1346218585968, 933.3563079833984, 938.4710690975189, 943.5858302116394, 948.7160139083862, 953.846197605133, 958.4973611831665, 963.1485247612, 967.7937333583832, 972.4389419555664, 977.0921957492828, 981.7454495429993, 986.4123890399933, 991.0793285369873, 995.7766773700714, 1000.4740262031555, 1005.1795380115509, 1009.8850498199463, 1014.5511004924774, 1019.2171511650085, 1023.9428150653839, 1028.6684789657593, 1033.3272907733917, 1037.9861025810242, 1040.3358266353607, 1042.6855506896973]
[30.2925, 30.2925, 35.6475, 35.6475, 38.4775, 38.4775, 40.93, 40.93, 42.185, 42.185, 43.7125, 43.7125, 45.0425, 45.0425, 45.2, 45.2, 48.5875, 48.5875, 49.1975, 49.1975, 49.95, 49.95, 51.455, 51.455, 52.675, 52.675, 53.1825, 53.1825, 54.6925, 54.6925, 56.0625, 56.0625, 56.8925, 56.8925, 57.8425, 57.8425, 58.2975, 58.2975, 58.745, 58.745, 58.9425, 58.9425, 59.7725, 59.7725, 60.3025, 60.3025, 60.295, 60.295, 61.0575, 61.0575, 62.1025, 62.1025, 62.7, 62.7, 62.715, 62.715, 62.9825, 62.9825, 63.3175, 63.3175, 63.7575, 63.7575, 63.9625, 63.9625, 64.5375, 64.5375, 64.4525, 64.4525, 64.725, 64.725, 65.045, 65.045, 65.0875, 65.0875, 64.8325, 64.8325, 64.89, 64.89, 64.9825, 64.9825, 65.1775, 65.1775, 65.32, 65.32, 65.8325, 65.8325, 65.935, 65.935, 65.91, 65.91, 66.2425, 66.2425, 66.0725, 66.0725, 66.48, 66.48, 66.8575, 66.8575, 67.0375, 67.0375, 66.8225, 66.8225, 66.8075, 66.8075, 66.6275, 66.6275, 66.9675, 66.9675, 66.5025, 66.5025, 66.9325, 66.9325, 67.69, 67.69, 67.91, 67.91, 68.2, 68.2, 68.1275, 68.1275, 68.15, 68.15, 68.08, 68.08, 68.4875, 68.4875, 68.5625, 68.5625, 68.545, 68.545, 68.065, 68.065, 68.225, 68.225, 67.9875, 67.9875, 68.5975, 68.5975, 68.6475, 68.6475, 69.13, 69.13, 68.805, 68.805, 68.4625, 68.4625, 68.5325, 68.5325, 68.545, 68.545, 68.87, 68.87, 68.5225, 68.5225, 68.32, 68.32, 68.825, 68.825, 69.0025, 69.0025, 68.71, 68.71, 68.87, 68.87, 68.9175, 68.9175, 68.84, 68.84, 68.9475, 68.9475, 69.1175, 69.1175, 69.37, 69.37, 68.975, 68.975, 69.0675, 69.0675, 69.4375, 69.4375, 70.07, 70.07, 70.0775, 70.0775, 70.6125, 70.6125, 70.31, 70.31, 69.8075, 69.8075, 70.0025, 70.0025, 70.03, 70.03, 70.01, 70.01, 70.085, 70.085, 70.095, 70.095, 69.4325, 69.4325]
