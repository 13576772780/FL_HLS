nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.191, Test loss: 2.050, Test accuracy: 25.01 

Round   0, Global train loss: 2.191, Global test loss: 2.042, Global test accuracy: 25.88 

Round   1, Train loss: 2.002, Test loss: 1.926, Test accuracy: 28.95 

Round   1, Global train loss: 2.002, Global test loss: 1.863, Global test accuracy: 31.64 

Round   2, Train loss: 1.899, Test loss: 1.872, Test accuracy: 31.32 

Round   2, Global train loss: 1.899, Global test loss: 1.773, Global test accuracy: 36.37 

Round   3, Train loss: 1.768, Test loss: 1.838, Test accuracy: 32.45 

Round   3, Global train loss: 1.768, Global test loss: 1.669, Global test accuracy: 39.53 

Round   4, Train loss: 1.697, Test loss: 1.836, Test accuracy: 32.20 

Round   4, Global train loss: 1.697, Global test loss: 1.607, Global test accuracy: 41.11 

Round   5, Train loss: 1.813, Test loss: 1.810, Test accuracy: 33.12 

Round   5, Global train loss: 1.813, Global test loss: 1.734, Global test accuracy: 37.30 

Round   6, Train loss: 1.679, Test loss: 1.797, Test accuracy: 33.76 

Round   6, Global train loss: 1.679, Global test loss: 1.619, Global test accuracy: 40.68 

Round   7, Train loss: 1.609, Test loss: 1.790, Test accuracy: 34.34 

Round   7, Global train loss: 1.609, Global test loss: 1.581, Global test accuracy: 42.85 

Round   8, Train loss: 1.582, Test loss: 1.771, Test accuracy: 35.27 

Round   8, Global train loss: 1.582, Global test loss: 1.585, Global test accuracy: 41.81 

Round   9, Train loss: 1.553, Test loss: 1.771, Test accuracy: 35.45 

Round   9, Global train loss: 1.553, Global test loss: 1.576, Global test accuracy: 42.56 

Round  10, Train loss: 1.590, Test loss: 1.768, Test accuracy: 35.91 

Round  10, Global train loss: 1.590, Global test loss: 1.634, Global test accuracy: 42.01 

Round  11, Train loss: 1.456, Test loss: 1.769, Test accuracy: 35.98 

Round  11, Global train loss: 1.456, Global test loss: 1.562, Global test accuracy: 43.30 

Round  12, Train loss: 1.512, Test loss: 1.746, Test accuracy: 36.85 

Round  12, Global train loss: 1.512, Global test loss: 1.596, Global test accuracy: 42.70 

Round  13, Train loss: 1.404, Test loss: 1.745, Test accuracy: 37.37 

Round  13, Global train loss: 1.404, Global test loss: 1.494, Global test accuracy: 44.55 

Round  14, Train loss: 1.491, Test loss: 1.725, Test accuracy: 38.36 

Round  14, Global train loss: 1.491, Global test loss: 1.559, Global test accuracy: 43.48 

Round  15, Train loss: 1.338, Test loss: 1.748, Test accuracy: 38.09 

Round  15, Global train loss: 1.338, Global test loss: 1.521, Global test accuracy: 43.90 

Round  16, Train loss: 1.264, Test loss: 1.781, Test accuracy: 37.66 

Round  16, Global train loss: 1.264, Global test loss: 1.506, Global test accuracy: 44.89 

Round  17, Train loss: 1.261, Test loss: 1.775, Test accuracy: 38.10 

Round  17, Global train loss: 1.261, Global test loss: 1.487, Global test accuracy: 45.13 

Round  18, Train loss: 1.107, Test loss: 1.787, Test accuracy: 38.78 

Round  18, Global train loss: 1.107, Global test loss: 1.499, Global test accuracy: 46.04 

Round  19, Train loss: 1.298, Test loss: 1.787, Test accuracy: 39.03 

Round  19, Global train loss: 1.298, Global test loss: 1.546, Global test accuracy: 43.50 

Round  20, Train loss: 1.276, Test loss: 1.785, Test accuracy: 39.36 

Round  20, Global train loss: 1.276, Global test loss: 1.509, Global test accuracy: 45.45 

Round  21, Train loss: 1.193, Test loss: 1.802, Test accuracy: 39.54 

Round  21, Global train loss: 1.193, Global test loss: 1.534, Global test accuracy: 44.38 

Round  22, Train loss: 1.106, Test loss: 1.817, Test accuracy: 39.80 

Round  22, Global train loss: 1.106, Global test loss: 1.450, Global test accuracy: 47.04 

Round  23, Train loss: 1.062, Test loss: 1.828, Test accuracy: 40.24 

Round  23, Global train loss: 1.062, Global test loss: 1.434, Global test accuracy: 47.98 

Round  24, Train loss: 1.251, Test loss: 1.841, Test accuracy: 40.13 

Round  24, Global train loss: 1.251, Global test loss: 1.546, Global test accuracy: 43.80 

Round  25, Train loss: 1.204, Test loss: 1.851, Test accuracy: 40.21 

Round  25, Global train loss: 1.204, Global test loss: 1.488, Global test accuracy: 46.00 

Round  26, Train loss: 0.896, Test loss: 1.892, Test accuracy: 40.23 

Round  26, Global train loss: 0.896, Global test loss: 1.495, Global test accuracy: 46.38 

Round  27, Train loss: 1.021, Test loss: 1.923, Test accuracy: 40.19 

Round  27, Global train loss: 1.021, Global test loss: 1.471, Global test accuracy: 46.61 

Round  28, Train loss: 1.146, Test loss: 1.947, Test accuracy: 40.14 

Round  28, Global train loss: 1.146, Global test loss: 1.501, Global test accuracy: 45.50 

Round  29, Train loss: 1.051, Test loss: 1.979, Test accuracy: 40.23 

Round  29, Global train loss: 1.051, Global test loss: 1.455, Global test accuracy: 47.85 

Round  30, Train loss: 0.838, Test loss: 2.011, Test accuracy: 40.01 

Round  30, Global train loss: 0.838, Global test loss: 1.449, Global test accuracy: 47.55 

Round  31, Train loss: 0.963, Test loss: 2.021, Test accuracy: 40.09 

Round  31, Global train loss: 0.963, Global test loss: 1.472, Global test accuracy: 48.24 

Round  32, Train loss: 0.793, Test loss: 2.049, Test accuracy: 40.11 

Round  32, Global train loss: 0.793, Global test loss: 1.462, Global test accuracy: 47.78 

Round  33, Train loss: 1.056, Test loss: 2.080, Test accuracy: 40.22 

Round  33, Global train loss: 1.056, Global test loss: 1.471, Global test accuracy: 46.61 

Round  34, Train loss: 0.835, Test loss: 2.103, Test accuracy: 40.73 

Round  34, Global train loss: 0.835, Global test loss: 1.447, Global test accuracy: 48.04 

Round  35, Train loss: 0.842, Test loss: 2.150, Test accuracy: 40.99 

Round  35, Global train loss: 0.842, Global test loss: 1.477, Global test accuracy: 47.67 

Round  36, Train loss: 0.694, Test loss: 2.154, Test accuracy: 41.34 

Round  36, Global train loss: 0.694, Global test loss: 1.464, Global test accuracy: 48.59 

Round  37, Train loss: 0.631, Test loss: 2.218, Test accuracy: 41.23 

Round  37, Global train loss: 0.631, Global test loss: 1.485, Global test accuracy: 48.22 

Round  38, Train loss: 0.787, Test loss: 2.270, Test accuracy: 40.99 

Round  38, Global train loss: 0.787, Global test loss: 1.478, Global test accuracy: 47.66 

Round  39, Train loss: 0.632, Test loss: 2.315, Test accuracy: 40.67 

Round  39, Global train loss: 0.632, Global test loss: 1.501, Global test accuracy: 47.07 

Round  40, Train loss: 0.720, Test loss: 2.365, Test accuracy: 40.75 

Round  40, Global train loss: 0.720, Global test loss: 1.493, Global test accuracy: 47.13 

Round  41, Train loss: 0.516, Test loss: 2.423, Test accuracy: 40.22 

Round  41, Global train loss: 0.516, Global test loss: 1.518, Global test accuracy: 48.54 

Round  42, Train loss: 0.616, Test loss: 2.420, Test accuracy: 40.64 

Round  42, Global train loss: 0.616, Global test loss: 1.547, Global test accuracy: 47.00 

Round  43, Train loss: 0.548, Test loss: 2.482, Test accuracy: 40.91 

Round  43, Global train loss: 0.548, Global test loss: 1.561, Global test accuracy: 46.85 

Round  44, Train loss: 0.695, Test loss: 2.474, Test accuracy: 41.37 

Round  44, Global train loss: 0.695, Global test loss: 1.497, Global test accuracy: 47.34 

Round  45, Train loss: 0.683, Test loss: 2.514, Test accuracy: 41.47 

Round  45, Global train loss: 0.683, Global test loss: 1.512, Global test accuracy: 46.09 

Round  46, Train loss: 0.552, Test loss: 2.549, Test accuracy: 41.14 

Round  46, Global train loss: 0.552, Global test loss: 1.483, Global test accuracy: 47.30 

Round  47, Train loss: 0.609, Test loss: 2.573, Test accuracy: 41.54 

Round  47, Global train loss: 0.609, Global test loss: 1.482, Global test accuracy: 48.66 

Round  48, Train loss: 0.580, Test loss: 2.607, Test accuracy: 41.52 

Round  48, Global train loss: 0.580, Global test loss: 1.541, Global test accuracy: 47.40 

Round  49, Train loss: 0.451, Test loss: 2.663, Test accuracy: 41.40 

Round  49, Global train loss: 0.451, Global test loss: 1.493, Global test accuracy: 49.44 

Round  50, Train loss: 0.453, Test loss: 2.719, Test accuracy: 40.96 

Round  50, Global train loss: 0.453, Global test loss: 1.516, Global test accuracy: 47.18 

Round  51, Train loss: 0.364, Test loss: 2.712, Test accuracy: 40.85 

Round  51, Global train loss: 0.364, Global test loss: 1.589, Global test accuracy: 46.45 

Round  52, Train loss: 0.422, Test loss: 2.819, Test accuracy: 40.45 

Round  52, Global train loss: 0.422, Global test loss: 1.605, Global test accuracy: 45.52 

Round  53, Train loss: 0.570, Test loss: 2.873, Test accuracy: 40.44 

Round  53, Global train loss: 0.570, Global test loss: 1.490, Global test accuracy: 47.84 

Round  54, Train loss: 0.457, Test loss: 2.900, Test accuracy: 40.79 

Round  54, Global train loss: 0.457, Global test loss: 1.549, Global test accuracy: 47.22 

Round  55, Train loss: 0.409, Test loss: 2.925, Test accuracy: 40.96 

Round  55, Global train loss: 0.409, Global test loss: 1.575, Global test accuracy: 46.97 

Round  56, Train loss: 0.369, Test loss: 2.951, Test accuracy: 41.33 

Round  56, Global train loss: 0.369, Global test loss: 1.567, Global test accuracy: 46.73 

Round  57, Train loss: 0.366, Test loss: 2.944, Test accuracy: 41.50 

Round  57, Global train loss: 0.366, Global test loss: 1.526, Global test accuracy: 49.29 

Round  58, Train loss: 0.459, Test loss: 2.959, Test accuracy: 41.20 

Round  58, Global train loss: 0.459, Global test loss: 1.592, Global test accuracy: 45.96 

Round  59, Train loss: 0.371, Test loss: 2.987, Test accuracy: 40.92 

Round  59, Global train loss: 0.371, Global test loss: 1.550, Global test accuracy: 46.70 

Round  60, Train loss: 0.390, Test loss: 3.001, Test accuracy: 40.91 

Round  60, Global train loss: 0.390, Global test loss: 1.530, Global test accuracy: 47.28 

Round  61, Train loss: 0.289, Test loss: 3.009, Test accuracy: 41.27 

Round  61, Global train loss: 0.289, Global test loss: 1.574, Global test accuracy: 47.65 

Round  62, Train loss: 0.425, Test loss: 3.040, Test accuracy: 41.48 

Round  62, Global train loss: 0.425, Global test loss: 1.584, Global test accuracy: 48.71 

Round  63, Train loss: 0.300, Test loss: 3.075, Test accuracy: 41.73 

Round  63, Global train loss: 0.300, Global test loss: 1.569, Global test accuracy: 47.10 

Round  64, Train loss: 0.402, Test loss: 3.104, Test accuracy: 41.27 

Round  64, Global train loss: 0.402, Global test loss: 1.615, Global test accuracy: 45.96 

Round  65, Train loss: 0.334, Test loss: 3.155, Test accuracy: 41.30 

Round  65, Global train loss: 0.334, Global test loss: 1.595, Global test accuracy: 49.66 

Round  66, Train loss: 0.304, Test loss: 3.209, Test accuracy: 41.37 

Round  66, Global train loss: 0.304, Global test loss: 1.604, Global test accuracy: 48.11 

Round  67, Train loss: 0.396, Test loss: 3.237, Test accuracy: 41.44 

Round  67, Global train loss: 0.396, Global test loss: 1.532, Global test accuracy: 48.78 

Round  68, Train loss: 0.276, Test loss: 3.321, Test accuracy: 41.50 

Round  68, Global train loss: 0.276, Global test loss: 1.606, Global test accuracy: 47.77 

Round  69, Train loss: 0.255, Test loss: 3.293, Test accuracy: 41.80 

Round  69, Global train loss: 0.255, Global test loss: 1.631, Global test accuracy: 47.55 

Round  70, Train loss: 0.250, Test loss: 3.304, Test accuracy: 41.48 

Round  70, Global train loss: 0.250, Global test loss: 1.600, Global test accuracy: 48.61 

Round  71, Train loss: 0.257, Test loss: 3.328, Test accuracy: 41.51 

Round  71, Global train loss: 0.257, Global test loss: 1.537, Global test accuracy: 48.31 

Round  72, Train loss: 0.287, Test loss: 3.376, Test accuracy: 41.66 

Round  72, Global train loss: 0.287, Global test loss: 1.586, Global test accuracy: 46.55 

Round  73, Train loss: 0.256, Test loss: 3.467, Test accuracy: 41.51 

Round  73, Global train loss: 0.256, Global test loss: 1.594, Global test accuracy: 49.33 

Round  74, Train loss: 0.215, Test loss: 3.451, Test accuracy: 41.63 

Round  74, Global train loss: 0.215, Global test loss: 1.621, Global test accuracy: 47.26 

Round  75, Train loss: 0.205, Test loss: 3.562, Test accuracy: 41.33 

Round  75, Global train loss: 0.205, Global test loss: 1.612, Global test accuracy: 47.13 

Round  76, Train loss: 0.249, Test loss: 3.550, Test accuracy: 41.55 

Round  76, Global train loss: 0.249, Global test loss: 1.604, Global test accuracy: 46.87 

Round  77, Train loss: 0.232, Test loss: 3.591, Test accuracy: 41.48 

Round  77, Global train loss: 0.232, Global test loss: 1.691, Global test accuracy: 45.65 

Round  78, Train loss: 0.217, Test loss: 3.626, Test accuracy: 41.62 

Round  78, Global train loss: 0.217, Global test loss: 1.600, Global test accuracy: 48.46 

Round  79, Train loss: 0.200, Test loss: 3.582, Test accuracy: 41.73 

Round  79, Global train loss: 0.200, Global test loss: 1.733, Global test accuracy: 47.37 

Round  80, Train loss: 0.256, Test loss: 3.577, Test accuracy: 41.78 

Round  80, Global train loss: 0.256, Global test loss: 1.636, Global test accuracy: 46.78 

Round  81, Train loss: 0.219, Test loss: 3.651, Test accuracy: 41.46 

Round  81, Global train loss: 0.219, Global test loss: 1.612, Global test accuracy: 47.15 

Round  82, Train loss: 0.257, Test loss: 3.705, Test accuracy: 41.29 

Round  82, Global train loss: 0.257, Global test loss: 1.670, Global test accuracy: 46.58 

Round  83, Train loss: 0.194, Test loss: 3.720, Test accuracy: 41.50 

Round  83, Global train loss: 0.194, Global test loss: 1.718, Global test accuracy: 47.49 

Round  84, Train loss: 0.206, Test loss: 3.711, Test accuracy: 41.55 

Round  84, Global train loss: 0.206, Global test loss: 1.669, Global test accuracy: 47.44 

Round  85, Train loss: 0.224, Test loss: 3.658, Test accuracy: 41.88 

Round  85, Global train loss: 0.224, Global test loss: 1.648, Global test accuracy: 47.27 

Round  86, Train loss: 0.188, Test loss: 3.735, Test accuracy: 41.67 

Round  86, Global train loss: 0.188, Global test loss: 1.623, Global test accuracy: 49.13 

Round  87, Train loss: 0.182, Test loss: 3.768, Test accuracy: 41.79 

Round  87, Global train loss: 0.182, Global test loss: 1.681, Global test accuracy: 47.13 

Round  88, Train loss: 0.245, Test loss: 3.830, Test accuracy: 41.71 

Round  88, Global train loss: 0.245, Global test loss: 1.628, Global test accuracy: 46.01 

Round  89, Train loss: 0.195, Test loss: 3.943, Test accuracy: 41.32 

Round  89, Global train loss: 0.195, Global test loss: 1.639, Global test accuracy: 46.26 

Round  90, Train loss: 0.200, Test loss: 3.923, Test accuracy: 41.28 

Round  90, Global train loss: 0.200, Global test loss: 1.657, Global test accuracy: 47.96 

Round  91, Train loss: 0.168, Test loss: 3.866, Test accuracy: 41.37 

Round  91, Global train loss: 0.168, Global test loss: 1.590, Global test accuracy: 46.38 

Round  92, Train loss: 0.167, Test loss: 3.906, Test accuracy: 41.27 

Round  92, Global train loss: 0.167, Global test loss: 1.691, Global test accuracy: 46.05 

Round  93, Train loss: 0.193, Test loss: 3.917, Test accuracy: 41.30 

Round  93, Global train loss: 0.193, Global test loss: 1.605, Global test accuracy: 49.19 

Round  94, Train loss: 0.188, Test loss: 3.937, Test accuracy: 41.60 

Round  94, Global train loss: 0.188, Global test loss: 1.615, Global test accuracy: 46.35 

Round  95, Train loss: 0.195, Test loss: 3.992, Test accuracy: 41.50 

Round  95, Global train loss: 0.195, Global test loss: 1.721, Global test accuracy: 46.42 

Round  96, Train loss: 0.195, Test loss: 4.003, Test accuracy: 41.69 

Round  96, Global train loss: 0.195, Global test loss: 1.692, Global test accuracy: 48.53 

Round  97, Train loss: 0.175, Test loss: 3.993, Test accuracy: 41.74 

Round  97, Global train loss: 0.175, Global test loss: 1.663, Global test accuracy: 48.51 

Round  98, Train loss: 0.154, Test loss: 4.027, Test accuracy: 41.59 

Round  98, Global train loss: 0.154, Global test loss: 1.701, Global test accuracy: 47.55 

Round  99, Train loss: 0.172, Test loss: 4.011, Test accuracy: 41.75 

Round  99, Global train loss: 0.172, Global test loss: 1.650, Global test accuracy: 45.27 

Final Round, Train loss: 0.152, Test loss: 4.144, Test accuracy: 41.98 

Final Round, Global train loss: 0.152, Global test loss: 1.650, Global test accuracy: 45.27 

Average accuracy final 10 rounds: 41.509 

Average global accuracy final 10 rounds: 47.2225 

2720.0121109485626
[1.4430246353149414, 2.6411259174346924, 3.8411319255828857, 5.0383827686309814, 6.236248970031738, 7.434859037399292, 8.6263747215271, 9.815443515777588, 11.008826494216919, 12.206361055374146, 13.400795936584473, 14.594072103500366, 15.763777494430542, 16.933435201644897, 18.107783317565918, 19.287601947784424, 20.471577167510986, 21.653663873672485, 22.82861590385437, 24.0065975189209, 25.18656587600708, 26.35455632209778, 27.525788068771362, 28.700339555740356, 29.876591682434082, 31.051866054534912, 32.233808755874634, 33.413718700408936, 34.59909772872925, 35.78049683570862, 36.95870661735535, 38.13662815093994, 39.312376499176025, 40.4980742931366, 41.678455114364624, 42.85081243515015, 44.02859687805176, 45.20444631576538, 46.378559589385986, 47.543784379959106, 48.71143817901611, 49.885605812072754, 51.05821919441223, 52.231995582580566, 53.401684284210205, 54.57003140449524, 55.74027729034424, 56.90864610671997, 58.08109998703003, 59.25033640861511, 60.43029713630676, 61.60419249534607, 62.776737689971924, 63.94550085067749, 65.11293125152588, 66.28647947311401, 67.45482134819031, 68.62369108200073, 69.79543852806091, 70.9614851474762, 72.12827777862549, 73.2975504398346, 74.47231841087341, 75.6421434879303, 76.8170371055603, 77.98631834983826, 79.16527247428894, 80.33508324623108, 81.50167679786682, 82.66970562934875, 83.83944201469421, 85.00981330871582, 86.19262957572937, 87.20433330535889, 88.2169017791748, 89.23329329490662, 90.24777317047119, 91.26022243499756, 92.27270197868347, 93.28485035896301, 94.29595971107483, 95.30775189399719, 96.31880569458008, 97.33212637901306, 98.343989610672, 99.36004614830017, 100.38769125938416, 101.40282225608826, 102.41780257225037, 103.4307062625885, 104.44442486763, 105.47224140167236, 106.49145936965942, 107.50359559059143, 108.51932907104492, 109.53541898727417, 110.55607080459595, 111.57471084594727, 112.59219694137573, 113.60762166976929, 115.642906665802]
[25.0125, 28.955, 31.32, 32.4475, 32.195, 33.125, 33.7625, 34.3375, 35.2675, 35.455, 35.91, 35.975, 36.85, 37.3675, 38.3575, 38.085, 37.655, 38.1025, 38.7825, 39.03, 39.36, 39.5425, 39.805, 40.24, 40.13, 40.2125, 40.225, 40.19, 40.1375, 40.225, 40.0075, 40.09, 40.11, 40.2175, 40.73, 40.995, 41.335, 41.225, 40.9875, 40.675, 40.75, 40.2225, 40.6375, 40.9075, 41.37, 41.465, 41.14, 41.5425, 41.52, 41.395, 40.96, 40.8475, 40.4525, 40.4425, 40.79, 40.96, 41.325, 41.5, 41.195, 40.9175, 40.905, 41.2725, 41.4775, 41.73, 41.2675, 41.3, 41.3675, 41.4425, 41.4975, 41.8025, 41.48, 41.5075, 41.665, 41.5075, 41.63, 41.3325, 41.5475, 41.4775, 41.6175, 41.73, 41.7825, 41.46, 41.29, 41.5025, 41.5475, 41.8775, 41.6725, 41.79, 41.7075, 41.32, 41.28, 41.365, 41.2725, 41.305, 41.5975, 41.4975, 41.6925, 41.745, 41.5875, 41.7475, 41.9825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.159, Test loss: 2.085, Test accuracy: 27.23 

Round   0, Global train loss: 1.159, Global test loss: 2.368, Global test accuracy: 19.02 

Round   1, Train loss: 0.881, Test loss: 1.859, Test accuracy: 37.84 

Round   1, Global train loss: 0.881, Global test loss: 2.293, Global test accuracy: 27.36 

Round   2, Train loss: 0.819, Test loss: 1.047, Test accuracy: 57.31 

Round   2, Global train loss: 0.819, Global test loss: 1.943, Global test accuracy: 28.78 

Round   3, Train loss: 0.749, Test loss: 0.961, Test accuracy: 61.58 

Round   3, Global train loss: 0.749, Global test loss: 2.327, Global test accuracy: 27.08 

Round   4, Train loss: 0.764, Test loss: 0.865, Test accuracy: 64.82 

Round   4, Global train loss: 0.764, Global test loss: 2.019, Global test accuracy: 31.82 

Round   5, Train loss: 0.645, Test loss: 0.853, Test accuracy: 66.48 

Round   5, Global train loss: 0.645, Global test loss: 2.085, Global test accuracy: 35.24 

Round   6, Train loss: 0.782, Test loss: 0.683, Test accuracy: 70.43 

Round   6, Global train loss: 0.782, Global test loss: 1.889, Global test accuracy: 33.73 

Round   7, Train loss: 0.749, Test loss: 0.667, Test accuracy: 71.29 

Round   7, Global train loss: 0.749, Global test loss: 1.766, Global test accuracy: 41.14 

Round   8, Train loss: 0.661, Test loss: 0.640, Test accuracy: 72.86 

Round   8, Global train loss: 0.661, Global test loss: 1.824, Global test accuracy: 39.17 

Round   9, Train loss: 0.638, Test loss: 0.645, Test accuracy: 72.44 

Round   9, Global train loss: 0.638, Global test loss: 1.935, Global test accuracy: 41.43 

Round  10, Train loss: 0.635, Test loss: 0.635, Test accuracy: 73.01 

Round  10, Global train loss: 0.635, Global test loss: 1.796, Global test accuracy: 43.72 

Round  11, Train loss: 0.650, Test loss: 0.617, Test accuracy: 73.96 

Round  11, Global train loss: 0.650, Global test loss: 2.007, Global test accuracy: 35.28 

Round  12, Train loss: 0.552, Test loss: 0.599, Test accuracy: 74.91 

Round  12, Global train loss: 0.552, Global test loss: 1.839, Global test accuracy: 47.84 

Round  13, Train loss: 0.614, Test loss: 0.597, Test accuracy: 75.42 

Round  13, Global train loss: 0.614, Global test loss: 1.698, Global test accuracy: 40.37 

Round  14, Train loss: 0.544, Test loss: 0.585, Test accuracy: 75.76 

Round  14, Global train loss: 0.544, Global test loss: 1.625, Global test accuracy: 42.49 

Round  15, Train loss: 0.558, Test loss: 0.579, Test accuracy: 75.93 

Round  15, Global train loss: 0.558, Global test loss: 1.813, Global test accuracy: 40.40 

Round  16, Train loss: 0.600, Test loss: 0.571, Test accuracy: 76.68 

Round  16, Global train loss: 0.600, Global test loss: 1.625, Global test accuracy: 44.81 

Round  17, Train loss: 0.604, Test loss: 0.561, Test accuracy: 77.17 

Round  17, Global train loss: 0.604, Global test loss: 1.609, Global test accuracy: 48.26 

Round  18, Train loss: 0.531, Test loss: 0.551, Test accuracy: 77.55 

Round  18, Global train loss: 0.531, Global test loss: 1.604, Global test accuracy: 43.41 

Round  19, Train loss: 0.552, Test loss: 0.551, Test accuracy: 77.54 

Round  19, Global train loss: 0.552, Global test loss: 1.597, Global test accuracy: 49.68 

Round  20, Train loss: 0.492, Test loss: 0.539, Test accuracy: 78.11 

Round  20, Global train loss: 0.492, Global test loss: 1.533, Global test accuracy: 48.64 

Round  21, Train loss: 0.497, Test loss: 0.544, Test accuracy: 78.16 

Round  21, Global train loss: 0.497, Global test loss: 1.587, Global test accuracy: 47.35 

Round  22, Train loss: 0.469, Test loss: 0.539, Test accuracy: 78.36 

Round  22, Global train loss: 0.469, Global test loss: 1.711, Global test accuracy: 48.12 

Round  23, Train loss: 0.557, Test loss: 0.550, Test accuracy: 77.94 

Round  23, Global train loss: 0.557, Global test loss: 1.543, Global test accuracy: 48.36 

Round  24, Train loss: 0.535, Test loss: 0.544, Test accuracy: 78.42 

Round  24, Global train loss: 0.535, Global test loss: 1.525, Global test accuracy: 47.87 

Round  25, Train loss: 0.480, Test loss: 0.530, Test accuracy: 78.87 

Round  25, Global train loss: 0.480, Global test loss: 1.475, Global test accuracy: 50.11 

Round  26, Train loss: 0.485, Test loss: 0.528, Test accuracy: 79.09 

Round  26, Global train loss: 0.485, Global test loss: 1.532, Global test accuracy: 47.94 

Round  27, Train loss: 0.441, Test loss: 0.513, Test accuracy: 79.69 

Round  27, Global train loss: 0.441, Global test loss: 1.418, Global test accuracy: 50.40 

Round  28, Train loss: 0.490, Test loss: 0.518, Test accuracy: 79.36 

Round  28, Global train loss: 0.490, Global test loss: 1.420, Global test accuracy: 51.08 

Round  29, Train loss: 0.562, Test loss: 0.524, Test accuracy: 79.36 

Round  29, Global train loss: 0.562, Global test loss: 1.372, Global test accuracy: 52.54 

Round  30, Train loss: 0.397, Test loss: 0.522, Test accuracy: 79.58 

Round  30, Global train loss: 0.397, Global test loss: 1.386, Global test accuracy: 53.09 

Round  31, Train loss: 0.400, Test loss: 0.516, Test accuracy: 79.67 

Round  31, Global train loss: 0.400, Global test loss: 1.506, Global test accuracy: 51.06 

Round  32, Train loss: 0.390, Test loss: 0.516, Test accuracy: 79.68 

Round  32, Global train loss: 0.390, Global test loss: 1.420, Global test accuracy: 48.14 

Round  33, Train loss: 0.489, Test loss: 0.508, Test accuracy: 80.08 

Round  33, Global train loss: 0.489, Global test loss: 1.438, Global test accuracy: 51.60 

Round  34, Train loss: 0.446, Test loss: 0.521, Test accuracy: 79.61 

Round  34, Global train loss: 0.446, Global test loss: 1.361, Global test accuracy: 54.11 

Round  35, Train loss: 0.394, Test loss: 0.513, Test accuracy: 80.03 

Round  35, Global train loss: 0.394, Global test loss: 1.375, Global test accuracy: 52.94 

Round  36, Train loss: 0.466, Test loss: 0.532, Test accuracy: 79.64 

Round  36, Global train loss: 0.466, Global test loss: 1.362, Global test accuracy: 53.16 

Round  37, Train loss: 0.456, Test loss: 0.513, Test accuracy: 80.18 

Round  37, Global train loss: 0.456, Global test loss: 1.542, Global test accuracy: 49.48 

Round  38, Train loss: 0.451, Test loss: 0.509, Test accuracy: 80.39 

Round  38, Global train loss: 0.451, Global test loss: 1.298, Global test accuracy: 56.58 

Round  39, Train loss: 0.407, Test loss: 0.514, Test accuracy: 80.11 

Round  39, Global train loss: 0.407, Global test loss: 1.692, Global test accuracy: 44.14 

Round  40, Train loss: 0.428, Test loss: 0.527, Test accuracy: 79.94 

Round  40, Global train loss: 0.428, Global test loss: 1.301, Global test accuracy: 55.96 

Round  41, Train loss: 0.359, Test loss: 0.532, Test accuracy: 79.67 

Round  41, Global train loss: 0.359, Global test loss: 1.435, Global test accuracy: 53.61 

Round  42, Train loss: 0.368, Test loss: 0.530, Test accuracy: 79.53 

Round  42, Global train loss: 0.368, Global test loss: 1.577, Global test accuracy: 50.67 

Round  43, Train loss: 0.370, Test loss: 0.507, Test accuracy: 80.19 

Round  43, Global train loss: 0.370, Global test loss: 1.672, Global test accuracy: 52.85 

Round  44, Train loss: 0.405, Test loss: 0.505, Test accuracy: 80.42 

Round  44, Global train loss: 0.405, Global test loss: 1.679, Global test accuracy: 46.04 

Round  45, Train loss: 0.343, Test loss: 0.517, Test accuracy: 80.44 

Round  45, Global train loss: 0.343, Global test loss: 1.497, Global test accuracy: 50.00 

Round  46, Train loss: 0.380, Test loss: 0.528, Test accuracy: 80.39 

Round  46, Global train loss: 0.380, Global test loss: 1.478, Global test accuracy: 52.76 

Round  47, Train loss: 0.316, Test loss: 0.537, Test accuracy: 80.14 

Round  47, Global train loss: 0.316, Global test loss: 1.214, Global test accuracy: 58.58 

Round  48, Train loss: 0.316, Test loss: 0.534, Test accuracy: 80.38 

Round  48, Global train loss: 0.316, Global test loss: 1.395, Global test accuracy: 52.13 

Round  49, Train loss: 0.339, Test loss: 0.534, Test accuracy: 80.35 

Round  49, Global train loss: 0.339, Global test loss: 1.430, Global test accuracy: 55.14 

Round  50, Train loss: 0.252, Test loss: 0.527, Test accuracy: 80.74 

Round  50, Global train loss: 0.252, Global test loss: 1.434, Global test accuracy: 52.12 

Round  51, Train loss: 0.368, Test loss: 0.523, Test accuracy: 80.93 

Round  51, Global train loss: 0.368, Global test loss: 1.493, Global test accuracy: 55.52 

Round  52, Train loss: 0.399, Test loss: 0.522, Test accuracy: 81.05 

Round  52, Global train loss: 0.399, Global test loss: 1.494, Global test accuracy: 52.02 

Round  53, Train loss: 0.363, Test loss: 0.524, Test accuracy: 81.05 

Round  53, Global train loss: 0.363, Global test loss: 1.297, Global test accuracy: 56.54 

Round  54, Train loss: 0.325, Test loss: 0.530, Test accuracy: 81.40 

Round  54, Global train loss: 0.325, Global test loss: 1.662, Global test accuracy: 49.77 

Round  55, Train loss: 0.369, Test loss: 0.519, Test accuracy: 81.75 

Round  55, Global train loss: 0.369, Global test loss: 1.229, Global test accuracy: 59.66 

Round  56, Train loss: 0.351, Test loss: 0.502, Test accuracy: 82.12 

Round  56, Global train loss: 0.351, Global test loss: 1.312, Global test accuracy: 56.61 

Round  57, Train loss: 0.323, Test loss: 0.512, Test accuracy: 82.00 

Round  57, Global train loss: 0.323, Global test loss: 1.469, Global test accuracy: 54.32 

Round  58, Train loss: 0.272, Test loss: 0.506, Test accuracy: 82.30 

Round  58, Global train loss: 0.272, Global test loss: 1.378, Global test accuracy: 55.51 

Round  59, Train loss: 0.288, Test loss: 0.514, Test accuracy: 82.06 

Round  59, Global train loss: 0.288, Global test loss: 1.316, Global test accuracy: 59.58 

Round  60, Train loss: 0.304, Test loss: 0.510, Test accuracy: 82.19 

Round  60, Global train loss: 0.304, Global test loss: 1.438, Global test accuracy: 56.12 

Round  61, Train loss: 0.296, Test loss: 0.522, Test accuracy: 81.82 

Round  61, Global train loss: 0.296, Global test loss: 1.533, Global test accuracy: 54.49 

Round  62, Train loss: 0.273, Test loss: 0.555, Test accuracy: 81.03 

Round  62, Global train loss: 0.273, Global test loss: 1.746, Global test accuracy: 50.96 

Round  63, Train loss: 0.292, Test loss: 0.528, Test accuracy: 81.91 

Round  63, Global train loss: 0.292, Global test loss: 1.443, Global test accuracy: 53.51 

Round  64, Train loss: 0.208, Test loss: 0.520, Test accuracy: 82.18 

Round  64, Global train loss: 0.208, Global test loss: 1.521, Global test accuracy: 57.24 

Round  65, Train loss: 0.284, Test loss: 0.513, Test accuracy: 82.22 

Round  65, Global train loss: 0.284, Global test loss: 1.326, Global test accuracy: 58.96 

Round  66, Train loss: 0.244, Test loss: 0.516, Test accuracy: 81.96 

Round  66, Global train loss: 0.244, Global test loss: 1.699, Global test accuracy: 52.51 

Round  67, Train loss: 0.292, Test loss: 0.520, Test accuracy: 81.84 

Round  67, Global train loss: 0.292, Global test loss: 1.486, Global test accuracy: 53.42 

Round  68, Train loss: 0.335, Test loss: 0.503, Test accuracy: 82.59 

Round  68, Global train loss: 0.335, Global test loss: 1.354, Global test accuracy: 56.80 

Round  69, Train loss: 0.313, Test loss: 0.503, Test accuracy: 82.54 

Round  69, Global train loss: 0.313, Global test loss: 1.416, Global test accuracy: 56.44 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 682, in train
    batch_loss.append(loss.item())
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.120, Test loss: 1.849, Test accuracy: 27.32 

Round   0, Global train loss: 1.120, Global test loss: 2.221, Global test accuracy: 17.63 

Round   1, Train loss: 0.956, Test loss: 1.537, Test accuracy: 37.11 

Round   1, Global train loss: 0.956, Global test loss: 2.266, Global test accuracy: 18.50 

Round   2, Train loss: 0.842, Test loss: 1.182, Test accuracy: 52.64 

Round   2, Global train loss: 0.842, Global test loss: 1.977, Global test accuracy: 29.95 

Round   3, Train loss: 0.827, Test loss: 1.055, Test accuracy: 55.71 

Round   3, Global train loss: 0.827, Global test loss: 2.331, Global test accuracy: 23.41 

Round   4, Train loss: 0.853, Test loss: 0.917, Test accuracy: 60.55 

Round   4, Global train loss: 0.853, Global test loss: 1.849, Global test accuracy: 32.20 

Round   5, Train loss: 0.783, Test loss: 0.788, Test accuracy: 64.03 

Round   5, Global train loss: 0.783, Global test loss: 1.841, Global test accuracy: 34.08 

Round   6, Train loss: 0.775, Test loss: 0.734, Test accuracy: 66.92 

Round   6, Global train loss: 0.775, Global test loss: 1.991, Global test accuracy: 34.49 

Round   7, Train loss: 0.767, Test loss: 0.741, Test accuracy: 66.99 

Round   7, Global train loss: 0.767, Global test loss: 1.812, Global test accuracy: 36.50 

Round   8, Train loss: 0.691, Test loss: 0.689, Test accuracy: 69.41 

Round   8, Global train loss: 0.691, Global test loss: 1.764, Global test accuracy: 33.66 

Round   9, Train loss: 0.671, Test loss: 0.687, Test accuracy: 69.77 

Round   9, Global train loss: 0.671, Global test loss: 1.650, Global test accuracy: 40.23 

Round  10, Train loss: 0.710, Test loss: 0.686, Test accuracy: 69.53 

Round  10, Global train loss: 0.710, Global test loss: 1.631, Global test accuracy: 42.60 

Round  11, Train loss: 0.654, Test loss: 0.667, Test accuracy: 70.91 

Round  11, Global train loss: 0.654, Global test loss: 1.720, Global test accuracy: 39.48 

Round  12, Train loss: 0.674, Test loss: 0.657, Test accuracy: 71.78 

Round  12, Global train loss: 0.674, Global test loss: 1.705, Global test accuracy: 37.46 

Round  13, Train loss: 0.595, Test loss: 0.658, Test accuracy: 72.07 

Round  13, Global train loss: 0.595, Global test loss: 1.943, Global test accuracy: 36.78 

Round  14, Train loss: 0.652, Test loss: 0.636, Test accuracy: 72.87 

Round  14, Global train loss: 0.652, Global test loss: 1.702, Global test accuracy: 41.46 

Round  15, Train loss: 0.576, Test loss: 0.630, Test accuracy: 73.03 

Round  15, Global train loss: 0.576, Global test loss: 1.519, Global test accuracy: 45.98 

Round  16, Train loss: 0.639, Test loss: 0.623, Test accuracy: 73.38 

Round  16, Global train loss: 0.639, Global test loss: 1.792, Global test accuracy: 33.08 

Round  17, Train loss: 0.660, Test loss: 0.617, Test accuracy: 73.77 

Round  17, Global train loss: 0.660, Global test loss: 2.097, Global test accuracy: 28.60 

Round  18, Train loss: 0.717, Test loss: 0.602, Test accuracy: 74.84 

Round  18, Global train loss: 0.717, Global test loss: 1.868, Global test accuracy: 38.41 

Round  19, Train loss: 0.566, Test loss: 0.599, Test accuracy: 74.92 

Round  19, Global train loss: 0.566, Global test loss: 1.446, Global test accuracy: 48.56 

Round  20, Train loss: 0.529, Test loss: 0.596, Test accuracy: 75.12 

Round  20, Global train loss: 0.529, Global test loss: 1.485, Global test accuracy: 45.97 

Round  21, Train loss: 0.621, Test loss: 0.597, Test accuracy: 75.20 

Round  21, Global train loss: 0.621, Global test loss: 1.637, Global test accuracy: 45.12 

Round  22, Train loss: 0.576, Test loss: 0.609, Test accuracy: 74.66 

Round  22, Global train loss: 0.576, Global test loss: 1.618, Global test accuracy: 42.01 

Round  23, Train loss: 0.503, Test loss: 0.592, Test accuracy: 75.41 

Round  23, Global train loss: 0.503, Global test loss: 1.659, Global test accuracy: 41.38 

Round  24, Train loss: 0.702, Test loss: 0.591, Test accuracy: 75.35 

Round  24, Global train loss: 0.702, Global test loss: 1.447, Global test accuracy: 48.22 

Round  25, Train loss: 0.535, Test loss: 0.621, Test accuracy: 74.33 

Round  25, Global train loss: 0.535, Global test loss: 1.501, Global test accuracy: 47.08 

Round  26, Train loss: 0.557, Test loss: 0.594, Test accuracy: 75.12 

Round  26, Global train loss: 0.557, Global test loss: 1.349, Global test accuracy: 52.43 

Round  27, Train loss: 0.520, Test loss: 0.583, Test accuracy: 75.70 

Round  27, Global train loss: 0.520, Global test loss: 1.517, Global test accuracy: 48.76 

Round  28, Train loss: 0.599, Test loss: 0.583, Test accuracy: 75.81 

Round  28, Global train loss: 0.599, Global test loss: 1.500, Global test accuracy: 47.31 

Round  29, Train loss: 0.592, Test loss: 0.571, Test accuracy: 76.12 

Round  29, Global train loss: 0.592, Global test loss: 1.353, Global test accuracy: 50.90 

Round  30, Train loss: 0.517, Test loss: 0.557, Test accuracy: 77.12 

Round  30, Global train loss: 0.517, Global test loss: 1.419, Global test accuracy: 49.71 

Round  31, Train loss: 0.601, Test loss: 0.546, Test accuracy: 77.63 

Round  31, Global train loss: 0.601, Global test loss: 1.359, Global test accuracy: 50.37 

Round  32, Train loss: 0.467, Test loss: 0.542, Test accuracy: 78.16 

Round  32, Global train loss: 0.467, Global test loss: 1.532, Global test accuracy: 47.23 

Round  33, Train loss: 0.478, Test loss: 0.539, Test accuracy: 78.39 

Round  33, Global train loss: 0.478, Global test loss: 1.466, Global test accuracy: 47.86 

Round  34, Train loss: 0.545, Test loss: 0.548, Test accuracy: 77.93 

Round  34, Global train loss: 0.545, Global test loss: 1.353, Global test accuracy: 51.57 

Round  35, Train loss: 0.447, Test loss: 0.552, Test accuracy: 77.69 

Round  35, Global train loss: 0.447, Global test loss: 1.832, Global test accuracy: 44.94 

Round  36, Train loss: 0.608, Test loss: 0.551, Test accuracy: 77.44 

Round  36, Global train loss: 0.608, Global test loss: 1.312, Global test accuracy: 53.76 

Round  37, Train loss: 0.453, Test loss: 0.549, Test accuracy: 77.58 

Round  37, Global train loss: 0.453, Global test loss: 1.305, Global test accuracy: 54.48 

Round  38, Train loss: 0.450, Test loss: 0.546, Test accuracy: 78.00 

Round  38, Global train loss: 0.450, Global test loss: 1.434, Global test accuracy: 51.09 

Round  39, Train loss: 0.551, Test loss: 0.537, Test accuracy: 78.38 

Round  39, Global train loss: 0.551, Global test loss: 1.386, Global test accuracy: 52.30 

Round  40, Train loss: 0.446, Test loss: 0.551, Test accuracy: 78.17 

Round  40, Global train loss: 0.446, Global test loss: 1.441, Global test accuracy: 50.60 

Round  41, Train loss: 0.458, Test loss: 0.555, Test accuracy: 78.36 

Round  41, Global train loss: 0.458, Global test loss: 1.503, Global test accuracy: 49.92 

Round  42, Train loss: 0.457, Test loss: 0.569, Test accuracy: 78.19 

Round  42, Global train loss: 0.457, Global test loss: 1.349, Global test accuracy: 52.76 

Round  43, Train loss: 0.396, Test loss: 0.559, Test accuracy: 78.25 

Round  43, Global train loss: 0.396, Global test loss: 1.341, Global test accuracy: 53.09 

Round  44, Train loss: 0.475, Test loss: 0.557, Test accuracy: 78.15 

Round  44, Global train loss: 0.475, Global test loss: 1.258, Global test accuracy: 55.02 

Round  45, Train loss: 0.462, Test loss: 0.547, Test accuracy: 78.64 

Round  45, Global train loss: 0.462, Global test loss: 1.240, Global test accuracy: 56.18 

Round  46, Train loss: 0.426, Test loss: 0.550, Test accuracy: 78.71 

Round  46, Global train loss: 0.426, Global test loss: 1.522, Global test accuracy: 51.33 

Round  47, Train loss: 0.481, Test loss: 0.549, Test accuracy: 78.86 

Round  47, Global train loss: 0.481, Global test loss: 1.534, Global test accuracy: 48.27 

Round  48, Train loss: 0.402, Test loss: 0.564, Test accuracy: 78.18 

Round  48, Global train loss: 0.402, Global test loss: 1.744, Global test accuracy: 45.76 

Round  49, Train loss: 0.431, Test loss: 0.548, Test accuracy: 78.71 

Round  49, Global train loss: 0.431, Global test loss: 1.367, Global test accuracy: 54.83 

Round  50, Train loss: 0.443, Test loss: 0.537, Test accuracy: 78.96 

Round  50, Global train loss: 0.443, Global test loss: 1.261, Global test accuracy: 55.81 

Round  51, Train loss: 0.421, Test loss: 0.539, Test accuracy: 79.05 

Round  51, Global train loss: 0.421, Global test loss: 1.529, Global test accuracy: 47.58 

Round  52, Train loss: 0.360, Test loss: 0.542, Test accuracy: 79.01 

Round  52, Global train loss: 0.360, Global test loss: 1.551, Global test accuracy: 49.56 

Round  53, Train loss: 0.456, Test loss: 0.541, Test accuracy: 79.06 

Round  53, Global train loss: 0.456, Global test loss: 1.257, Global test accuracy: 57.03 

Round  54, Train loss: 0.356, Test loss: 0.540, Test accuracy: 79.08 

Round  54, Global train loss: 0.356, Global test loss: 1.398, Global test accuracy: 55.98 

Round  55, Train loss: 0.312, Test loss: 0.542, Test accuracy: 79.26 

Round  55, Global train loss: 0.312, Global test loss: 1.505, Global test accuracy: 54.12 

Round  56, Train loss: 0.421, Test loss: 0.542, Test accuracy: 79.28 

Round  56, Global train loss: 0.421, Global test loss: 1.561, Global test accuracy: 50.16 

Round  57, Train loss: 0.384, Test loss: 0.536, Test accuracy: 79.33 

Round  57, Global train loss: 0.384, Global test loss: 1.437, Global test accuracy: 53.58 

Round  58, Train loss: 0.377, Test loss: 0.526, Test accuracy: 79.62 

Round  58, Global train loss: 0.377, Global test loss: 1.566, Global test accuracy: 49.52 

Round  59, Train loss: 0.476, Test loss: 0.525, Test accuracy: 80.06 

Round  59, Global train loss: 0.476, Global test loss: 1.371, Global test accuracy: 53.49 

Round  60, Train loss: 0.480, Test loss: 0.532, Test accuracy: 79.73 

Round  60, Global train loss: 0.480, Global test loss: 1.385, Global test accuracy: 51.79 

Round  61, Train loss: 0.347, Test loss: 0.553, Test accuracy: 79.42 

Round  61, Global train loss: 0.347, Global test loss: 1.692, Global test accuracy: 50.74 

Round  62, Train loss: 0.315, Test loss: 0.539, Test accuracy: 79.72 

Round  62, Global train loss: 0.315, Global test loss: 1.457, Global test accuracy: 53.75 

Round  63, Train loss: 0.349, Test loss: 0.546, Test accuracy: 79.28 

Round  63, Global train loss: 0.349, Global test loss: 1.327, Global test accuracy: 56.68 

Round  64, Train loss: 0.355, Test loss: 0.549, Test accuracy: 79.26 

Round  64, Global train loss: 0.355, Global test loss: 1.457, Global test accuracy: 55.11 

Round  65, Train loss: 0.325, Test loss: 0.562, Test accuracy: 78.93 

Round  65, Global train loss: 0.325, Global test loss: 1.509, Global test accuracy: 51.95 

Round  66, Train loss: 0.450, Test loss: 0.552, Test accuracy: 79.27 

Round  66, Global train loss: 0.450, Global test loss: 1.277, Global test accuracy: 56.45 

Round  67, Train loss: 0.451, Test loss: 0.551, Test accuracy: 79.56 

Round  67, Global train loss: 0.451, Global test loss: 1.329, Global test accuracy: 55.08 

Round  68, Train loss: 0.475, Test loss: 0.563, Test accuracy: 79.38 

Round  68, Global train loss: 0.475, Global test loss: 1.539, Global test accuracy: 50.46 

Round  69, Train loss: 0.383, Test loss: 0.548, Test accuracy: 80.05 

Round  69, Global train loss: 0.383, Global test loss: 1.376, Global test accuracy: 55.37 

Round  70, Train loss: 0.258, Test loss: 0.553, Test accuracy: 79.99 

Round  70, Global train loss: 0.258, Global test loss: 1.291, Global test accuracy: 58.43 

Round  71, Train loss: 0.343, Test loss: 0.564, Test accuracy: 79.81 

Round  71, Global train loss: 0.343, Global test loss: 1.301, Global test accuracy: 55.83 

Round  72, Train loss: 0.382, Test loss: 0.573, Test accuracy: 79.41 

Round  72, Global train loss: 0.382, Global test loss: 1.629, Global test accuracy: 48.22 

Round  73, Train loss: 0.365, Test loss: 0.546, Test accuracy: 80.14 

Round  73, Global train loss: 0.365, Global test loss: 1.266, Global test accuracy: 55.80 

Round  74, Train loss: 0.342, Test loss: 0.542, Test accuracy: 80.31 

Round  74, Global train loss: 0.342, Global test loss: 1.266, Global test accuracy: 57.38 

Round  75, Train loss: 0.352, Test loss: 0.539, Test accuracy: 80.58 

Round  75, Global train loss: 0.352, Global test loss: 1.214, Global test accuracy: 59.27 

Round  76, Train loss: 0.400, Test loss: 0.557, Test accuracy: 80.33 

Round  76, Global train loss: 0.400, Global test loss: 1.584, Global test accuracy: 51.34 

Round  77, Train loss: 0.361, Test loss: 0.555, Test accuracy: 80.47 

Round  77, Global train loss: 0.361, Global test loss: 1.431, Global test accuracy: 54.62 

Round  78, Train loss: 0.294, Test loss: 0.576, Test accuracy: 79.88 

Round  78, Global train loss: 0.294, Global test loss: 1.469, Global test accuracy: 53.53 

Round  79, Train loss: 0.305, Test loss: 0.582, Test accuracy: 79.75 

Round  79, Global train loss: 0.305, Global test loss: 1.423, Global test accuracy: 53.89 

Round  80, Train loss: 0.251, Test loss: 0.562, Test accuracy: 80.36 

Round  80, Global train loss: 0.251, Global test loss: 1.411, Global test accuracy: 55.24 

Round  81, Train loss: 0.353, Test loss: 0.556, Test accuracy: 80.42 

Round  81, Global train loss: 0.353, Global test loss: 1.281, Global test accuracy: 57.11 

Round  82, Train loss: 0.282, Test loss: 0.557, Test accuracy: 80.58 

Round  82, Global train loss: 0.282, Global test loss: 1.317, Global test accuracy: 59.58 

Round  83, Train loss: 0.345, Test loss: 0.580, Test accuracy: 79.96 

Round  83, Global train loss: 0.345, Global test loss: 1.203, Global test accuracy: 59.56 

Round  84, Train loss: 0.340, Test loss: 0.574, Test accuracy: 80.24 

Round  84, Global train loss: 0.340, Global test loss: 1.711, Global test accuracy: 48.59 

Round  85, Train loss: 0.317, Test loss: 0.592, Test accuracy: 79.81 

Round  85, Global train loss: 0.317, Global test loss: 1.251, Global test accuracy: 59.67 

Round  86, Train loss: 0.290, Test loss: 0.589, Test accuracy: 79.95 

Round  86, Global train loss: 0.290, Global test loss: 1.574, Global test accuracy: 51.19 

Round  87, Train loss: 0.311, Test loss: 0.570, Test accuracy: 80.66 

Round  87, Global train loss: 0.311, Global test loss: 1.199, Global test accuracy: 60.84 

Round  88, Train loss: 0.312, Test loss: 0.567, Test accuracy: 80.74 

Round  88, Global train loss: 0.312, Global test loss: 1.277, Global test accuracy: 58.74 

Round  89, Train loss: 0.321, Test loss: 0.567, Test accuracy: 80.79 

Round  89, Global train loss: 0.321, Global test loss: 1.420, Global test accuracy: 55.54 

Round  90, Train loss: 0.273, Test loss: 0.555, Test accuracy: 81.09 

Round  90, Global train loss: 0.273, Global test loss: 1.193, Global test accuracy: 60.65 

Round  91, Train loss: 0.318, Test loss: 0.557, Test accuracy: 81.04 

Round  91, Global train loss: 0.318, Global test loss: 1.160, Global test accuracy: 60.67 

Round  92, Train loss: 0.277, Test loss: 0.576, Test accuracy: 80.62 

Round  92, Global train loss: 0.277, Global test loss: 1.207, Global test accuracy: 60.47 

Round  93, Train loss: 0.300, Test loss: 0.563, Test accuracy: 81.14 
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  93, Global train loss: 0.300, Global test loss: 1.324, Global test accuracy: 58.29 

Round  94, Train loss: 0.241, Test loss: 0.568, Test accuracy: 80.90 

Round  94, Global train loss: 0.241, Global test loss: 1.330, Global test accuracy: 59.28 

Round  95, Train loss: 0.311, Test loss: 0.583, Test accuracy: 80.86 

Round  95, Global train loss: 0.311, Global test loss: 1.497, Global test accuracy: 54.12 

Round  96, Train loss: 0.347, Test loss: 0.576, Test accuracy: 80.94 

Round  96, Global train loss: 0.347, Global test loss: 1.416, Global test accuracy: 55.89 

Round  97, Train loss: 0.290, Test loss: 0.556, Test accuracy: 81.38 

Round  97, Global train loss: 0.290, Global test loss: 1.227, Global test accuracy: 60.37 

Round  98, Train loss: 0.269, Test loss: 0.560, Test accuracy: 81.17 

Round  98, Global train loss: 0.269, Global test loss: 1.426, Global test accuracy: 56.02 

Round  99, Train loss: 0.251, Test loss: 0.559, Test accuracy: 81.17 

Round  99, Global train loss: 0.251, Global test loss: 1.392, Global test accuracy: 56.51 

Final Round, Train loss: 0.240, Test loss: 0.609, Test accuracy: 80.79 

Final Round, Global train loss: 0.240, Global test loss: 1.392, Global test accuracy: 56.51 

Average accuracy final 10 rounds: 81.0325 

Average global accuracy final 10 rounds: 58.2275 

1377.9326560497284
[1.5792927742004395, 2.875380277633667, 4.183979272842407, 5.497691869735718, 6.819822311401367, 8.087236166000366, 9.40787410736084, 10.64490818977356, 11.964211463928223, 13.211131811141968, 14.4169282913208, 15.618069887161255, 16.797199487686157, 17.975306034088135, 19.251073360443115, 20.433619499206543, 21.710673332214355, 22.942397594451904, 24.16125178337097, 25.358602285385132, 26.626856327056885, 27.81697416305542, 29.02181577682495, 30.203792810440063, 31.408764600753784, 32.63149547576904, 33.82557010650635, 35.01764965057373, 36.2366156578064, 37.403050661087036, 38.61765766143799, 39.81263518333435, 41.00674104690552, 42.19772148132324, 43.502360582351685, 44.7256863117218, 45.951438426971436, 47.273029088974, 48.54951500892639, 49.82211470603943, 51.0732262134552, 52.34173583984375, 53.60551452636719, 54.80679965019226, 56.083479166030884, 57.36328959465027, 58.64258289337158, 59.876875162124634, 61.15339779853821, 62.3448646068573, 63.53134822845459, 64.73569941520691, 65.923499584198, 67.11719942092896, 68.31285262107849, 69.49862170219421, 70.68701386451721, 71.8770809173584, 73.06662464141846, 74.30231428146362, 75.49897360801697, 76.68672633171082, 77.94561409950256, 79.18453240394592, 80.27794742584229, 81.36664366722107, 82.46456956863403, 83.55682277679443, 84.65474081039429, 85.74564170837402, 86.84127950668335, 87.93620300292969, 89.03203320503235, 90.12406539916992, 91.21505045890808, 92.30990386009216, 93.4044291973114, 94.49906396865845, 95.593514919281, 96.68506407737732, 97.77883625030518, 98.87129497528076, 99.9647376537323, 101.05816674232483, 102.15245079994202, 103.2473304271698, 104.34255886077881, 105.43784523010254, 106.53416109085083, 107.63023853302002, 108.89542078971863, 110.07772159576416, 111.29285597801208, 112.39143753051758, 113.48869156837463, 114.58107590675354, 115.67564010620117, 116.77191185951233, 117.86385035514832, 118.96070432662964, 121.15151357650757]
[27.316666666666666, 37.108333333333334, 52.641666666666666, 55.708333333333336, 60.55, 64.025, 66.91666666666667, 66.99166666666666, 69.40833333333333, 69.76666666666667, 69.525, 70.90833333333333, 71.78333333333333, 72.06666666666666, 72.86666666666666, 73.03333333333333, 73.375, 73.76666666666667, 74.84166666666667, 74.91666666666667, 75.11666666666666, 75.2, 74.65833333333333, 75.40833333333333, 75.35, 74.33333333333333, 75.11666666666666, 75.7, 75.80833333333334, 76.11666666666666, 77.11666666666666, 77.63333333333334, 78.15833333333333, 78.39166666666667, 77.93333333333334, 77.69166666666666, 77.44166666666666, 77.58333333333333, 78.0, 78.375, 78.16666666666667, 78.35833333333333, 78.19166666666666, 78.25, 78.15, 78.64166666666667, 78.70833333333333, 78.85833333333333, 78.18333333333334, 78.70833333333333, 78.95833333333333, 79.05, 79.00833333333334, 79.05833333333334, 79.08333333333333, 79.25833333333334, 79.28333333333333, 79.33333333333333, 79.625, 80.05833333333334, 79.73333333333333, 79.425, 79.725, 79.275, 79.25833333333334, 78.93333333333334, 79.26666666666667, 79.55833333333334, 79.375, 80.05, 79.99166666666666, 79.80833333333334, 79.40833333333333, 80.14166666666667, 80.30833333333334, 80.575, 80.33333333333333, 80.475, 79.88333333333334, 79.75, 80.35833333333333, 80.425, 80.58333333333333, 79.95833333333333, 80.24166666666666, 79.80833333333334, 79.95, 80.65833333333333, 80.74166666666666, 80.79166666666667, 81.09166666666667, 81.04166666666667, 80.61666666666666, 81.14166666666667, 80.9, 80.85833333333333, 80.94166666666666, 81.38333333333334, 81.175, 81.175, 80.79166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  15.4700
Round 1 global test acc  22.6600
Round 2 global test acc  23.6800
Round 3 global test acc  24.8100
Round 4 global test acc  11.1100
Round 5 global test acc  15.2600
Round 6 global test acc  26.3900
Round 7 global test acc  20.0200
Round 8 global test acc  23.4100
Round 9 global test acc  22.2600
Round 10 global test acc  32.0700
Round 11 global test acc  26.3100
Round 12 global test acc  27.9100
Round 13 global test acc  33.4600
Round 14 global test acc  26.2900
Round 15 global test acc  31.1100
Round 16 global test acc  33.6000
Round 17 global test acc  29.1900
Round 18 global test acc  34.3600
Round 19 global test acc  32.1800
Round 20 global test acc  28.3000
Round 21 global test acc  34.9500
Round 22 global test acc  33.1000
Round 23 global test acc  35.5000
Round 24 global test acc  35.6600
Round 25 global test acc  36.9600
Round 26 global test acc  26.0700
Round 27 global test acc  38.9500
Round 28 global test acc  37.5900
Round 29 global test acc  40.0800
Round 30 global test acc  33.1900
Round 31 global test acc  29.4800
Round 32 global test acc  31.6000
Round 33 global test acc  28.2400
Round 34 global test acc  28.6900
Round 35 global test acc  34.8600
Round 36 global test acc  25.0400
Round 37 global test acc  36.7600
Round 38 global test acc  34.1900
Round 39 global test acc  34.4500
Round 40 global test acc  20.6800
Round 41 global test acc  33.8600
Round 42 global test acc  36.7200
Round 43 global test acc  43.0200
Round 44 global test acc  29.0900
Round 45 global test acc  34.4900
Round 46 global test acc  24.5100
Round 47 global test acc  31.9000
Round 48 global test acc  32.6800
Round 49 global test acc  30.3400
Round 50 global test acc  30.3000
Round 51 global test acc  42.8900
Round 52 global test acc  33.6300
Round 53 global test acc  36.2500
Round 54 global test acc  27.7200
Round 55 global test acc  34.2700
Round 56 global test acc  42.7900
Round 57 global test acc  39.4800
Round 58 global test acc  30.8700
Round 59 global test acc  20.1400
Round 60 global test acc  32.1100
Round 61 global test acc  33.0700
Round 62 global test acc  33.3800
Round 63 global test acc  42.2600
Round 64 global test acc  40.8600
Round 65 global test acc  43.7300
Round 66 global test acc  34.0200
Round 67 global test acc  33.8900
Round 68 global test acc  40.7100
Round 69 global test acc  30.0000
Round 70 global test acc  29.8000
Round 71 global test acc  25.7700
Round 72 global test acc  32.9000
Round 73 global test acc  40.8400
Round 74 global test acc  35.3100
Round 75 global test acc  43.0600
Round 76 global test acc  30.6200
Round 77 global test acc  35.5200
Round 78 global test acc  32.6300
Round 79 global test acc  45.0900
Round 80 global test acc  43.9600
Round 81 global test acc  41.5400
Round 82 global test acc  41.8000
Round 83 global test acc  40.2800
Round 84 global test acc  35.6500
Round 85 global test acc  32.6500
Round 86 global test acc  30.0800
Round 87 global test acc  28.3800
Round 88 global test acc  26.1500
Round 89 global test acc  25.3300
Round 90 global test acc  23.1700
Round 91 global test acc  21.4800
Round 92 global test acc  21.0600
Round 93 global test acc  20.5600
Round 94 global test acc  20.3300
Round 95 global test acc  20.3200
Round 96 global test acc  20.7400
Round 97 global test acc  20.1400
Round 98 global test acc  20.5300
Round 99 global test acc  20.0100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.562, Test loss: 2.239, Test accuracy: 19.69
Round   1, Train loss: 1.039, Test loss: 1.777, Test accuracy: 36.80
Round   2, Train loss: 0.918, Test loss: 1.542, Test accuracy: 45.92
Round   3, Train loss: 0.899, Test loss: 1.308, Test accuracy: 50.96
Round   4, Train loss: 0.758, Test loss: 1.214, Test accuracy: 56.41
Round   5, Train loss: 0.856, Test loss: 1.240, Test accuracy: 56.27
Round   6, Train loss: 0.756, Test loss: 1.111, Test accuracy: 59.09
Round   7, Train loss: 0.725, Test loss: 1.002, Test accuracy: 61.02
Round   8, Train loss: 0.710, Test loss: 1.063, Test accuracy: 61.71
Round   9, Train loss: 0.672, Test loss: 0.765, Test accuracy: 68.01
Round  10, Train loss: 0.755, Test loss: 0.662, Test accuracy: 72.11
Round  11, Train loss: 0.664, Test loss: 0.640, Test accuracy: 72.29
Round  12, Train loss: 0.660, Test loss: 0.641, Test accuracy: 72.04
Round  13, Train loss: 0.636, Test loss: 0.630, Test accuracy: 72.85
Round  14, Train loss: 0.675, Test loss: 0.620, Test accuracy: 73.57
Round  15, Train loss: 0.633, Test loss: 0.605, Test accuracy: 74.21
Round  16, Train loss: 0.610, Test loss: 0.605, Test accuracy: 74.79
Round  17, Train loss: 0.621, Test loss: 0.585, Test accuracy: 75.34
Round  18, Train loss: 0.605, Test loss: 0.572, Test accuracy: 75.61
Round  19, Train loss: 0.669, Test loss: 0.561, Test accuracy: 77.01
Round  20, Train loss: 0.677, Test loss: 0.568, Test accuracy: 77.00
Round  21, Train loss: 0.549, Test loss: 0.556, Test accuracy: 77.61
Round  22, Train loss: 0.589, Test loss: 0.539, Test accuracy: 78.38
Round  23, Train loss: 0.579, Test loss: 0.533, Test accuracy: 78.33
Round  24, Train loss: 0.634, Test loss: 0.539, Test accuracy: 78.45
Round  25, Train loss: 0.508, Test loss: 0.524, Test accuracy: 79.00
Round  26, Train loss: 0.658, Test loss: 0.529, Test accuracy: 78.67
Round  27, Train loss: 0.508, Test loss: 0.515, Test accuracy: 79.39
Round  28, Train loss: 0.489, Test loss: 0.512, Test accuracy: 79.42
Round  29, Train loss: 0.509, Test loss: 0.493, Test accuracy: 80.07
Round  30, Train loss: 0.508, Test loss: 0.495, Test accuracy: 80.03
Round  31, Train loss: 0.436, Test loss: 0.498, Test accuracy: 80.02
Round  32, Train loss: 0.474, Test loss: 0.491, Test accuracy: 80.31
Round  33, Train loss: 0.485, Test loss: 0.482, Test accuracy: 80.67
Round  34, Train loss: 0.514, Test loss: 0.483, Test accuracy: 80.86
Round  35, Train loss: 0.506, Test loss: 0.481, Test accuracy: 80.73
Round  36, Train loss: 0.480, Test loss: 0.480, Test accuracy: 80.94
Round  37, Train loss: 0.443, Test loss: 0.469, Test accuracy: 81.37
Round  38, Train loss: 0.494, Test loss: 0.473, Test accuracy: 81.02
Round  39, Train loss: 0.444, Test loss: 0.471, Test accuracy: 81.21
Round  40, Train loss: 0.479, Test loss: 0.467, Test accuracy: 81.27
Round  41, Train loss: 0.510, Test loss: 0.460, Test accuracy: 81.82
Round  42, Train loss: 0.439, Test loss: 0.449, Test accuracy: 82.17
Round  43, Train loss: 0.542, Test loss: 0.450, Test accuracy: 82.19
Round  44, Train loss: 0.528, Test loss: 0.454, Test accuracy: 82.00
Round  45, Train loss: 0.460, Test loss: 0.457, Test accuracy: 81.73
Round  46, Train loss: 0.411, Test loss: 0.446, Test accuracy: 82.30
Round  47, Train loss: 0.415, Test loss: 0.440, Test accuracy: 82.33
Round  48, Train loss: 0.385, Test loss: 0.441, Test accuracy: 82.35
Round  49, Train loss: 0.421, Test loss: 0.435, Test accuracy: 82.60
Round  50, Train loss: 0.426, Test loss: 0.427, Test accuracy: 82.99
Round  51, Train loss: 0.394, Test loss: 0.431, Test accuracy: 82.75
Round  52, Train loss: 0.508, Test loss: 0.434, Test accuracy: 82.75
Round  53, Train loss: 0.371, Test loss: 0.428, Test accuracy: 82.83
Round  54, Train loss: 0.378, Test loss: 0.428, Test accuracy: 82.80
Round  55, Train loss: 0.291, Test loss: 0.433, Test accuracy: 82.83
Round  56, Train loss: 0.411, Test loss: 0.441, Test accuracy: 82.31
Round  57, Train loss: 0.361, Test loss: 0.428, Test accuracy: 82.89
Round  58, Train loss: 0.318, Test loss: 0.426, Test accuracy: 83.04
Round  59, Train loss: 0.437, Test loss: 0.432, Test accuracy: 82.78
Round  60, Train loss: 0.327, Test loss: 0.420, Test accuracy: 83.28
Round  61, Train loss: 0.317, Test loss: 0.415, Test accuracy: 83.72
Round  62, Train loss: 0.337, Test loss: 0.419, Test accuracy: 83.57
Round  63, Train loss: 0.354, Test loss: 0.415, Test accuracy: 83.60
Round  64, Train loss: 0.367, Test loss: 0.415, Test accuracy: 83.49
Round  65, Train loss: 0.400, Test loss: 0.405, Test accuracy: 84.04
Round  66, Train loss: 0.370, Test loss: 0.412, Test accuracy: 83.68
Round  67, Train loss: 0.404, Test loss: 0.413, Test accuracy: 83.71
Round  68, Train loss: 0.401, Test loss: 0.413, Test accuracy: 83.81
Round  69, Train loss: 0.283, Test loss: 0.405, Test accuracy: 84.13
Round  70, Train loss: 0.346, Test loss: 0.405, Test accuracy: 84.00
Round  71, Train loss: 0.323, Test loss: 0.414, Test accuracy: 83.38
Round  72, Train loss: 0.330, Test loss: 0.404, Test accuracy: 84.13
Round  73, Train loss: 0.319, Test loss: 0.403, Test accuracy: 84.05
Round  74, Train loss: 0.305, Test loss: 0.402, Test accuracy: 84.08
Round  75, Train loss: 0.380, Test loss: 0.402, Test accuracy: 84.21
Round  76, Train loss: 0.330, Test loss: 0.391, Test accuracy: 84.37
Round  77, Train loss: 0.300, Test loss: 0.398, Test accuracy: 84.37
Round  78, Train loss: 0.296, Test loss: 0.404, Test accuracy: 84.17
Round  79, Train loss: 0.285, Test loss: 0.397, Test accuracy: 84.40
Round  80, Train loss: 0.303, Test loss: 0.396, Test accuracy: 84.44
Round  81, Train loss: 0.293, Test loss: 0.403, Test accuracy: 84.08
Round  82, Train loss: 0.329, Test loss: 0.403, Test accuracy: 83.98
Round  83, Train loss: 0.253, Test loss: 0.405, Test accuracy: 83.96
Round  84, Train loss: 0.274, Test loss: 0.399, Test accuracy: 84.25
Round  85, Train loss: 0.291, Test loss: 0.397, Test accuracy: 84.20
Round  86, Train loss: 0.264, Test loss: 0.395, Test accuracy: 84.62
Round  87, Train loss: 0.277, Test loss: 0.414, Test accuracy: 83.82
Round  88, Train loss: 0.254, Test loss: 0.404, Test accuracy: 84.28
Round  89, Train loss: 0.282, Test loss: 0.389, Test accuracy: 84.86
Round  90, Train loss: 0.333, Test loss: 0.399, Test accuracy: 84.41
Round  91, Train loss: 0.258, Test loss: 0.392, Test accuracy: 84.58
Round  92, Train loss: 0.332, Test loss: 0.390, Test accuracy: 84.90
Round  93, Train loss: 0.242, Test loss: 0.388, Test accuracy: 84.88
Round  94, Train loss: 0.311, Test loss: 0.392, Test accuracy: 84.88
Round  95, Train loss: 0.283, Test loss: 0.397, Test accuracy: 84.45
Round  96, Train loss: 0.264, Test loss: 0.405, Test accuracy: 84.18
Round  97, Train loss: 0.295, Test loss: 0.400, Test accuracy: 84.44
Round  98, Train loss: 0.247, Test loss: 0.393, Test accuracy: 84.88
Round  99, Train loss: 0.268, Test loss: 0.386, Test accuracy: 84.95
Final Round, Train loss: 0.230, Test loss: 0.388, Test accuracy: 84.86
Average accuracy final 10 rounds: 84.65416666666665
1396.8759651184082
[1.722672462463379, 3.074556589126587, 4.423901796340942, 5.719320297241211, 7.021973133087158, 8.400768041610718, 9.711387157440186, 11.023976564407349, 12.332882404327393, 13.64978575706482, 14.953185796737671, 16.25524139404297, 17.595046043395996, 18.890421152114868, 20.18223547935486, 21.49129557609558, 22.792558908462524, 24.090128898620605, 25.391576051712036, 26.69126796722412, 28.04697823524475, 29.401271104812622, 30.774839878082275, 32.11938500404358, 33.47932839393616, 34.84336256980896, 36.198089599609375, 37.56771492958069, 38.922422885894775, 40.273391246795654, 41.6296911239624, 42.97771120071411, 44.33162212371826, 45.687217235565186, 47.042258739471436, 48.40726327896118, 49.781153202056885, 51.14654278755188, 52.51505947113037, 53.88930535316467, 55.253841400146484, 56.611252307891846, 57.976052045822144, 59.34198069572449, 60.68576741218567, 61.899091958999634, 63.11024022102356, 64.31842279434204, 65.52834153175354, 66.72801518440247, 67.92842984199524, 69.14554953575134, 70.36822962760925, 71.58161354064941, 72.7939829826355, 74.01216912269592, 75.2261176109314, 76.44453501701355, 77.6476080417633, 78.85499334335327, 80.0611674785614, 81.27995371818542, 82.50310206413269, 83.72169017791748, 84.93536448478699, 86.13885450363159, 87.47910499572754, 88.75059628486633, 89.95288372039795, 91.15713930130005, 92.36687660217285, 93.57144832611084, 94.77327394485474, 95.97168350219727, 97.168283700943, 98.36893057823181, 99.56649351119995, 100.77004051208496, 101.96932792663574, 103.16796493530273, 104.36613368988037, 105.57200002670288, 106.77034306526184, 107.98319983482361, 109.1905152797699, 110.39293360710144, 111.59955382347107, 112.79847884178162, 114.0098614692688, 115.21131014823914, 116.41391134262085, 117.6234130859375, 118.82336401939392, 120.02463221549988, 121.22290778160095, 122.42968726158142, 123.641108751297, 124.84511470794678, 126.05474805831909, 127.25920343399048, 129.20589900016785]
[19.691666666666666, 36.8, 45.925, 50.958333333333336, 56.40833333333333, 56.270833333333336, 59.09166666666667, 61.020833333333336, 61.7125, 68.00833333333334, 72.10833333333333, 72.29166666666667, 72.0375, 72.84583333333333, 73.56666666666666, 74.2125, 74.7875, 75.3375, 75.60833333333333, 77.00833333333334, 76.99583333333334, 77.60833333333333, 78.38333333333334, 78.33333333333333, 78.45416666666667, 79.00416666666666, 78.67083333333333, 79.39166666666667, 79.41666666666667, 80.07083333333334, 80.025, 80.01666666666667, 80.30833333333334, 80.67083333333333, 80.8625, 80.73333333333333, 80.94166666666666, 81.36666666666666, 81.02083333333333, 81.2125, 81.27083333333333, 81.82083333333334, 82.175, 82.19166666666666, 81.99583333333334, 81.72916666666667, 82.30416666666666, 82.32916666666667, 82.35416666666667, 82.60416666666667, 82.9875, 82.74583333333334, 82.74583333333334, 82.825, 82.79583333333333, 82.825, 82.3125, 82.8875, 83.04166666666667, 82.78333333333333, 83.27916666666667, 83.725, 83.57083333333334, 83.59583333333333, 83.49166666666666, 84.0375, 83.67916666666666, 83.7125, 83.80833333333334, 84.12916666666666, 84.0, 83.375, 84.13333333333334, 84.05, 84.08333333333333, 84.20833333333333, 84.36666666666666, 84.36666666666666, 84.17083333333333, 84.4, 84.44166666666666, 84.08333333333333, 83.97916666666667, 83.9625, 84.24583333333334, 84.20416666666667, 84.61666666666666, 83.82083333333334, 84.28333333333333, 84.85833333333333, 84.40833333333333, 84.58333333333333, 84.89583333333333, 84.875, 84.87916666666666, 84.45, 84.18333333333334, 84.4375, 84.87916666666666, 84.95, 84.85833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8475
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3435
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8260
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1090
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6760
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7930
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0375
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6920
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5495
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8625
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.197, Test loss: 2.209, Test accuracy: 19.47 

Round   0, Global train loss: 2.197, Global test loss: 2.193, Global test accuracy: 19.84 

Round   1, Train loss: 2.196, Test loss: 2.204, Test accuracy: 22.20 

Round   1, Global train loss: 2.196, Global test loss: 2.163, Global test accuracy: 25.50 

Round   2, Train loss: 2.115, Test loss: 2.150, Test accuracy: 22.32 

Round   2, Global train loss: 2.115, Global test loss: 2.064, Global test accuracy: 24.60 

Round   3, Train loss: 2.126, Test loss: 2.168, Test accuracy: 23.12 

Round   3, Global train loss: 2.126, Global test loss: 2.113, Global test accuracy: 30.81 

Round   4, Train loss: 2.070, Test loss: 2.144, Test accuracy: 23.58 

Round   4, Global train loss: 2.070, Global test loss: 2.052, Global test accuracy: 29.40 

Round   5, Train loss: 2.007, Test loss: 2.123, Test accuracy: 23.69 

Round   5, Global train loss: 2.007, Global test loss: 1.992, Global test accuracy: 28.05 

Round   6, Train loss: 2.057, Test loss: 2.129, Test accuracy: 23.91 

Round   6, Global train loss: 2.057, Global test loss: 2.052, Global test accuracy: 29.04 

Round   7, Train loss: 2.064, Test loss: 2.134, Test accuracy: 23.87 

Round   7, Global train loss: 2.064, Global test loss: 2.110, Global test accuracy: 26.26 

Round   8, Train loss: 1.962, Test loss: 2.112, Test accuracy: 24.84 

Round   8, Global train loss: 1.962, Global test loss: 1.947, Global test accuracy: 31.16 

Round   9, Train loss: 2.079, Test loss: 2.102, Test accuracy: 25.25 

Round   9, Global train loss: 2.079, Global test loss: 2.105, Global test accuracy: 27.26 

Round  10, Train loss: 1.860, Test loss: 2.083, Test accuracy: 26.20 

Round  10, Global train loss: 1.860, Global test loss: 1.873, Global test accuracy: 33.78 

Round  11, Train loss: 1.965, Test loss: 2.095, Test accuracy: 25.89 

Round  11, Global train loss: 1.965, Global test loss: 2.022, Global test accuracy: 29.34 

Round  12, Train loss: 2.035, Test loss: 2.106, Test accuracy: 25.57 

Round  12, Global train loss: 2.035, Global test loss: 2.071, Global test accuracy: 30.23 

Round  13, Train loss: 1.805, Test loss: 2.123, Test accuracy: 25.66 

Round  13, Global train loss: 1.805, Global test loss: 1.869, Global test accuracy: 33.91 

Round  14, Train loss: 1.865, Test loss: 2.113, Test accuracy: 25.81 

Round  14, Global train loss: 1.865, Global test loss: 1.936, Global test accuracy: 33.59 

Round  15, Train loss: 1.714, Test loss: 2.115, Test accuracy: 26.04 

Round  15, Global train loss: 1.714, Global test loss: 1.815, Global test accuracy: 37.20 

Round  16, Train loss: 1.822, Test loss: 2.119, Test accuracy: 26.59 

Round  16, Global train loss: 1.822, Global test loss: 1.912, Global test accuracy: 35.31 

Round  17, Train loss: 1.831, Test loss: 2.116, Test accuracy: 26.93 

Round  17, Global train loss: 1.831, Global test loss: 1.941, Global test accuracy: 31.85 

Round  18, Train loss: 1.732, Test loss: 2.112, Test accuracy: 27.17 

Round  18, Global train loss: 1.732, Global test loss: 1.810, Global test accuracy: 36.35 

Round  19, Train loss: 1.745, Test loss: 2.102, Test accuracy: 27.33 

Round  19, Global train loss: 1.745, Global test loss: 1.874, Global test accuracy: 35.89 

Round  20, Train loss: 1.736, Test loss: 2.117, Test accuracy: 27.27 

Round  20, Global train loss: 1.736, Global test loss: 1.938, Global test accuracy: 34.28 

Round  21, Train loss: 1.787, Test loss: 2.121, Test accuracy: 27.62 

Round  21, Global train loss: 1.787, Global test loss: 1.887, Global test accuracy: 35.02 

Round  22, Train loss: 1.706, Test loss: 2.144, Test accuracy: 27.59 

Round  22, Global train loss: 1.706, Global test loss: 2.016, Global test accuracy: 31.53 

Round  23, Train loss: 1.705, Test loss: 2.175, Test accuracy: 27.29 

Round  23, Global train loss: 1.705, Global test loss: 1.895, Global test accuracy: 34.09 

Round  24, Train loss: 1.628, Test loss: 2.206, Test accuracy: 27.18 

Round  24, Global train loss: 1.628, Global test loss: 1.918, Global test accuracy: 35.00 

Round  25, Train loss: 1.501, Test loss: 2.183, Test accuracy: 27.46 

Round  25, Global train loss: 1.501, Global test loss: 1.817, Global test accuracy: 36.62 

Round  26, Train loss: 1.770, Test loss: 2.203, Test accuracy: 27.65 

Round  26, Global train loss: 1.770, Global test loss: 1.914, Global test accuracy: 36.49 

Round  27, Train loss: 1.445, Test loss: 2.238, Test accuracy: 27.74 

Round  27, Global train loss: 1.445, Global test loss: 1.748, Global test accuracy: 37.66 

Round  28, Train loss: 1.414, Test loss: 2.251, Test accuracy: 27.92 

Round  28, Global train loss: 1.414, Global test loss: 1.864, Global test accuracy: 35.63 

Round  29, Train loss: 1.408, Test loss: 2.289, Test accuracy: 28.02 

Round  29, Global train loss: 1.408, Global test loss: 1.793, Global test accuracy: 38.60 

Round  30, Train loss: 1.416, Test loss: 2.329, Test accuracy: 28.12 

Round  30, Global train loss: 1.416, Global test loss: 1.894, Global test accuracy: 37.21 

Round  31, Train loss: 1.570, Test loss: 2.343, Test accuracy: 27.37 

Round  31, Global train loss: 1.570, Global test loss: 1.868, Global test accuracy: 36.73 

Round  32, Train loss: 1.480, Test loss: 2.380, Test accuracy: 27.19 

Round  32, Global train loss: 1.480, Global test loss: 1.855, Global test accuracy: 37.55 

Round  33, Train loss: 1.295, Test loss: 2.432, Test accuracy: 27.23 

Round  33, Global train loss: 1.295, Global test loss: 1.796, Global test accuracy: 38.46 

Round  34, Train loss: 1.281, Test loss: 2.476, Test accuracy: 27.13 

Round  34, Global train loss: 1.281, Global test loss: 1.856, Global test accuracy: 34.38 

Round  35, Train loss: 1.274, Test loss: 2.516, Test accuracy: 26.86 

Round  35, Global train loss: 1.274, Global test loss: 1.820, Global test accuracy: 37.27 

Round  36, Train loss: 1.223, Test loss: 2.540, Test accuracy: 27.11 

Round  36, Global train loss: 1.223, Global test loss: 1.906, Global test accuracy: 35.26 

Round  37, Train loss: 1.414, Test loss: 2.578, Test accuracy: 27.34 

Round  37, Global train loss: 1.414, Global test loss: 1.930, Global test accuracy: 32.00 

Round  38, Train loss: 1.181, Test loss: 2.630, Test accuracy: 27.43 

Round  38, Global train loss: 1.181, Global test loss: 1.751, Global test accuracy: 37.63 

Round  39, Train loss: 1.332, Test loss: 2.694, Test accuracy: 27.49 

Round  39, Global train loss: 1.332, Global test loss: 1.967, Global test accuracy: 31.55 

Round  40, Train loss: 0.898, Test loss: 2.769, Test accuracy: 27.30 

Round  40, Global train loss: 0.898, Global test loss: 1.671, Global test accuracy: 41.22 

Round  41, Train loss: 1.477, Test loss: 2.855, Test accuracy: 26.56 

Round  41, Global train loss: 1.477, Global test loss: 2.029, Global test accuracy: 29.68 

Round  42, Train loss: 1.092, Test loss: 2.868, Test accuracy: 26.34 

Round  42, Global train loss: 1.092, Global test loss: 1.957, Global test accuracy: 32.57 

Round  43, Train loss: 1.045, Test loss: 2.946, Test accuracy: 26.53 

Round  43, Global train loss: 1.045, Global test loss: 1.828, Global test accuracy: 34.65 

Round  44, Train loss: 1.022, Test loss: 2.963, Test accuracy: 26.71 

Round  44, Global train loss: 1.022, Global test loss: 1.815, Global test accuracy: 36.91 

Round  45, Train loss: 1.133, Test loss: 3.032, Test accuracy: 26.96 

Round  45, Global train loss: 1.133, Global test loss: 1.967, Global test accuracy: 31.00 

Round  46, Train loss: 1.120, Test loss: 3.144, Test accuracy: 26.77 

Round  46, Global train loss: 1.120, Global test loss: 1.873, Global test accuracy: 35.62 

Round  47, Train loss: 0.956, Test loss: 3.261, Test accuracy: 26.56 

Round  47, Global train loss: 0.956, Global test loss: 1.885, Global test accuracy: 35.03 

Round  48, Train loss: 0.785, Test loss: 3.265, Test accuracy: 26.36 

Round  48, Global train loss: 0.785, Global test loss: 1.748, Global test accuracy: 36.97 

Round  49, Train loss: 0.809, Test loss: 3.379, Test accuracy: 26.53 

Round  49, Global train loss: 0.809, Global test loss: 1.793, Global test accuracy: 36.46 

Round  50, Train loss: 0.864, Test loss: 3.392, Test accuracy: 26.70 

Round  50, Global train loss: 0.864, Global test loss: 1.838, Global test accuracy: 34.09 

Round  51, Train loss: 0.769, Test loss: 3.494, Test accuracy: 26.48 

Round  51, Global train loss: 0.769, Global test loss: 1.799, Global test accuracy: 35.55 

Round  52, Train loss: 0.876, Test loss: 3.497, Test accuracy: 26.56 

Round  52, Global train loss: 0.876, Global test loss: 1.780, Global test accuracy: 35.83 

Round  53, Train loss: 0.897, Test loss: 3.567, Test accuracy: 26.78 

Round  53, Global train loss: 0.897, Global test loss: 1.712, Global test accuracy: 40.09 

Round  54, Train loss: 0.739, Test loss: 3.599, Test accuracy: 26.82 

Round  54, Global train loss: 0.739, Global test loss: 1.895, Global test accuracy: 31.60 

Round  55, Train loss: 0.764, Test loss: 3.656, Test accuracy: 26.77 

Round  55, Global train loss: 0.764, Global test loss: 1.811, Global test accuracy: 36.18 

Round  56, Train loss: 0.830, Test loss: 3.679, Test accuracy: 26.56 

Round  56, Global train loss: 0.830, Global test loss: 1.871, Global test accuracy: 36.10 

Round  57, Train loss: 0.724, Test loss: 3.802, Test accuracy: 26.03 

Round  57, Global train loss: 0.724, Global test loss: 1.751, Global test accuracy: 37.68 

Round  58, Train loss: 0.659, Test loss: 3.829, Test accuracy: 25.93 

Round  58, Global train loss: 0.659, Global test loss: 1.902, Global test accuracy: 33.05 

Round  59, Train loss: 0.677, Test loss: 3.898, Test accuracy: 26.31 

Round  59, Global train loss: 0.677, Global test loss: 1.872, Global test accuracy: 32.02 

Round  60, Train loss: 0.615, Test loss: 3.961, Test accuracy: 26.19 

Round  60, Global train loss: 0.615, Global test loss: 1.947, Global test accuracy: 30.50 

Round  61, Train loss: 0.787, Test loss: 4.069, Test accuracy: 26.32 

Round  61, Global train loss: 0.787, Global test loss: 1.960, Global test accuracy: 30.15 

Round  62, Train loss: 0.665, Test loss: 4.100, Test accuracy: 26.74 

Round  62, Global train loss: 0.665, Global test loss: 1.872, Global test accuracy: 33.66 

Round  63, Train loss: 0.419, Test loss: 4.061, Test accuracy: 26.85 

Round  63, Global train loss: 0.419, Global test loss: 1.727, Global test accuracy: 38.14 

Round  64, Train loss: 0.591, Test loss: 4.124, Test accuracy: 26.60 

Round  64, Global train loss: 0.591, Global test loss: 1.695, Global test accuracy: 39.59 

Round  65, Train loss: 0.697, Test loss: 4.244, Test accuracy: 26.48 

Round  65, Global train loss: 0.697, Global test loss: 2.072, Global test accuracy: 26.23 

Round  66, Train loss: 0.490, Test loss: 4.355, Test accuracy: 26.50 

Round  66, Global train loss: 0.490, Global test loss: 1.718, Global test accuracy: 38.75 

Round  67, Train loss: 0.613, Test loss: 4.410, Test accuracy: 26.35 

Round  67, Global train loss: 0.613, Global test loss: 1.957, Global test accuracy: 30.81 

Round  68, Train loss: 0.597, Test loss: 4.520, Test accuracy: 25.93 

Round  68, Global train loss: 0.597, Global test loss: 1.793, Global test accuracy: 35.72 

Round  69, Train loss: 0.407, Test loss: 4.490, Test accuracy: 26.00 

Round  69, Global train loss: 0.407, Global test loss: 1.848, Global test accuracy: 35.43 

Round  70, Train loss: 0.410, Test loss: 4.572, Test accuracy: 26.20 

Round  70, Global train loss: 0.410, Global test loss: 1.819, Global test accuracy: 34.84 

Round  71, Train loss: 0.443, Test loss: 4.549, Test accuracy: 25.85 

Round  71, Global train loss: 0.443, Global test loss: 1.913, Global test accuracy: 33.19 

Round  72, Train loss: 0.546, Test loss: 4.666, Test accuracy: 26.04 

Round  72, Global train loss: 0.546, Global test loss: 1.900, Global test accuracy: 32.42 

Round  73, Train loss: 0.469, Test loss: 4.707, Test accuracy: 25.93 

Round  73, Global train loss: 0.469, Global test loss: 1.845, Global test accuracy: 32.51 

Round  74, Train loss: 0.592, Test loss: 4.813, Test accuracy: 26.05 

Round  74, Global train loss: 0.592, Global test loss: 1.813, Global test accuracy: 34.40 

Round  75, Train loss: 0.395, Test loss: 4.764, Test accuracy: 26.33 

Round  75, Global train loss: 0.395, Global test loss: 1.743, Global test accuracy: 37.99 

Round  76, Train loss: 0.361, Test loss: 4.794, Test accuracy: 26.55 

Round  76, Global train loss: 0.361, Global test loss: 1.917, Global test accuracy: 31.07 

Round  77, Train loss: 0.378, Test loss: 4.840, Test accuracy: 26.15 

Round  77, Global train loss: 0.378, Global test loss: 1.880, Global test accuracy: 33.36 

Round  78, Train loss: 0.323, Test loss: 4.915, Test accuracy: 26.03 

Round  78, Global train loss: 0.323, Global test loss: 1.787, Global test accuracy: 34.88 

Round  79, Train loss: 0.452, Test loss: 5.029, Test accuracy: 26.11 

Round  79, Global train loss: 0.452, Global test loss: 1.833, Global test accuracy: 34.94 

Round  80, Train loss: 0.397, Test loss: 5.096, Test accuracy: 26.06 

Round  80, Global train loss: 0.397, Global test loss: 1.867, Global test accuracy: 33.08 

Round  81, Train loss: 0.317, Test loss: 5.131, Test accuracy: 26.37 

Round  81, Global train loss: 0.317, Global test loss: 1.760, Global test accuracy: 38.53 

Round  82, Train loss: 0.285, Test loss: 5.159, Test accuracy: 26.23 

Round  82, Global train loss: 0.285, Global test loss: 1.746, Global test accuracy: 38.27 

Round  83, Train loss: 0.505, Test loss: 5.210, Test accuracy: 26.38 

Round  83, Global train loss: 0.505, Global test loss: 2.041, Global test accuracy: 26.48 

Round  84, Train loss: 0.335, Test loss: 5.178, Test accuracy: 26.79 

Round  84, Global train loss: 0.335, Global test loss: 1.755, Global test accuracy: 37.66 

Round  85, Train loss: 0.399, Test loss: 5.318, Test accuracy: 26.54 

Round  85, Global train loss: 0.399, Global test loss: 1.873, Global test accuracy: 33.77 

Round  86, Train loss: 0.374, Test loss: 5.358, Test accuracy: 26.56 

Round  86, Global train loss: 0.374, Global test loss: 1.917, Global test accuracy: 31.59 

Round  87, Train loss: 0.283, Test loss: 5.397, Test accuracy: 26.71 

Round  87, Global train loss: 0.283, Global test loss: 1.818, Global test accuracy: 35.26 

Round  88, Train loss: 0.280, Test loss: 5.478, Test accuracy: 26.60 

Round  88, Global train loss: 0.280, Global test loss: 1.768, Global test accuracy: 36.99 

Round  89, Train loss: 0.301, Test loss: 5.554, Test accuracy: 26.45 

Round  89, Global train loss: 0.301, Global test loss: 1.890, Global test accuracy: 31.83 

Round  90, Train loss: 0.264, Test loss: 5.638, Test accuracy: 26.47 

Round  90, Global train loss: 0.264, Global test loss: 1.842, Global test accuracy: 33.62 

Round  91, Train loss: 0.357, Test loss: 5.750, Test accuracy: 26.18 

Round  91, Global train loss: 0.357, Global test loss: 1.979, Global test accuracy: 29.16 

Round  92, Train loss: 0.335, Test loss: 5.671, Test accuracy: 26.24 

Round  92, Global train loss: 0.335, Global test loss: 1.954, Global test accuracy: 30.58 

Round  93, Train loss: 0.248, Test loss: 5.700, Test accuracy: 26.45 

Round  93, Global train loss: 0.248, Global test loss: 1.833, Global test accuracy: 34.28 

Round  94, Train loss: 0.235, Test loss: 5.765, Test accuracy: 26.45 

Round  94, Global train loss: 0.235, Global test loss: 1.809, Global test accuracy: 36.53 

Round  95, Train loss: 0.300, Test loss: 5.737, Test accuracy: 26.44 

Round  95, Global train loss: 0.300, Global test loss: 1.920, Global test accuracy: 32.41 

Round  96, Train loss: 0.347, Test loss: 5.804, Test accuracy: 26.48 

Round  96, Global train loss: 0.347, Global test loss: 1.959, Global test accuracy: 29.46 

Round  97, Train loss: 0.295, Test loss: 5.857, Test accuracy: 26.45 

Round  97, Global train loss: 0.295, Global test loss: 1.893, Global test accuracy: 31.30 

Round  98, Train loss: 0.259, Test loss: 5.873, Test accuracy: 26.39 

Round  98, Global train loss: 0.259, Global test loss: 1.993, Global test accuracy: 29.38 

Round  99, Train loss: 0.258, Test loss: 5.876, Test accuracy: 26.24 

Round  99, Global train loss: 0.258, Global test loss: 1.955, Global test accuracy: 29.40 

Final Round, Train loss: 0.260, Test loss: 6.255, Test accuracy: 26.25 

Final Round, Global train loss: 0.260, Global test loss: 1.955, Global test accuracy: 29.40 

Average accuracy final 10 rounds: 26.38 

Average global accuracy final 10 rounds: 31.612 

2703.462076663971
[1.573887586593628, 2.733745813369751, 3.7895216941833496, 4.83144474029541, 5.8728346824646, 6.918057918548584, 8.08350682258606, 9.255499839782715, 10.42849349975586, 11.613410711288452, 12.786545276641846, 13.958740711212158, 15.163851499557495, 16.329471349716187, 17.508491039276123, 18.66019892692566, 19.84031653404236, 21.027414798736572, 22.198546648025513, 23.38509202003479, 24.55173110961914, 25.715769052505493, 26.875213861465454, 27.998631954193115, 29.1844961643219, 30.358500003814697, 31.54172158241272, 32.715481758117676, 33.924832582473755, 35.09821915626526, 36.26762104034424, 37.44962406158447, 38.55990028381348, 39.78706240653992, 40.95397758483887, 42.12137413024902, 43.29189443588257, 44.46181607246399, 45.63011956214905, 46.81470847129822, 48.00257897377014, 49.1889214515686, 50.371354818344116, 51.55472779273987, 52.73462510108948, 53.91663384437561, 55.10381317138672, 56.28886365890503, 57.4738974571228, 58.655858278274536, 59.83664894104004, 61.02373480796814, 62.05648469924927, 63.08129072189331, 64.10605549812317, 65.13224530220032, 66.16187167167664, 67.1864697933197, 68.21587061882019, 69.2385528087616, 70.26115107536316, 71.28273153305054, 72.30395317077637, 73.32752346992493, 74.35396099090576, 75.38122391700745, 76.40256214141846, 77.42717146873474, 78.45218062400818, 79.47638726234436, 80.5042359828949, 81.53020906448364, 82.55525755882263, 83.57977652549744, 84.60418105125427, 85.6266462802887, 86.65093398094177, 87.6784815788269, 88.70665621757507, 89.73272657394409, 90.76229071617126, 91.79263162612915, 92.81774115562439, 93.83875322341919, 94.8645932674408, 95.8887574672699, 96.91585779190063, 97.94106078147888, 98.96597003936768, 99.99243140220642, 101.0250940322876, 102.05656480789185, 103.08642935752869, 104.11367082595825, 105.1429877281189, 106.17047333717346, 107.19999957084656, 108.22760272026062, 109.25297689437866, 110.2795832157135, 112.33618903160095]
[19.4675, 22.195, 22.3225, 23.1225, 23.5775, 23.69, 23.905, 23.8725, 24.835, 25.2525, 26.205, 25.8925, 25.5725, 25.655, 25.8075, 26.04, 26.59, 26.935, 27.17, 27.33, 27.2675, 27.62, 27.5925, 27.285, 27.185, 27.4575, 27.6525, 27.7425, 27.9175, 28.02, 28.1175, 27.3675, 27.1875, 27.2325, 27.1325, 26.855, 27.1125, 27.3375, 27.425, 27.4925, 27.295, 26.56, 26.34, 26.5325, 26.715, 26.9575, 26.7675, 26.5625, 26.355, 26.53, 26.695, 26.475, 26.5625, 26.78, 26.82, 26.77, 26.56, 26.0325, 25.9325, 26.3125, 26.1875, 26.3175, 26.7375, 26.85, 26.5975, 26.4825, 26.5, 26.35, 25.9275, 26.005, 26.2, 25.8475, 26.035, 25.9275, 26.0475, 26.33, 26.5475, 26.1475, 26.03, 26.11, 26.0575, 26.3675, 26.225, 26.3825, 26.79, 26.5425, 26.56, 26.7075, 26.5975, 26.445, 26.47, 26.185, 26.24, 26.455, 26.45, 26.44, 26.475, 26.45, 26.395, 26.24, 26.25]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8475
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3420
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8290
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1070
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5680
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8050
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.2590
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6675
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5575
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8610
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.200, Test loss: 2.233, Test accuracy: 20.35 

Round   0, Global train loss: 2.200, Global test loss: 2.205, Global test accuracy: 20.98 

Round   1, Train loss: 2.099, Test loss: 2.148, Test accuracy: 22.62 

Round   1, Global train loss: 2.099, Global test loss: 2.053, Global test accuracy: 24.52 

Round   2, Train loss: 2.027, Test loss: 2.075, Test accuracy: 25.79 

Round   2, Global train loss: 2.027, Global test loss: 1.923, Global test accuracy: 30.18 

Round   3, Train loss: 1.927, Test loss: 2.060, Test accuracy: 26.83 

Round   3, Global train loss: 1.927, Global test loss: 1.821, Global test accuracy: 32.21 

Round   4, Train loss: 1.893, Test loss: 2.007, Test accuracy: 28.90 

Round   4, Global train loss: 1.893, Global test loss: 1.738, Global test accuracy: 38.71 

Round   5, Train loss: 1.892, Test loss: 1.970, Test accuracy: 30.41 

Round   5, Global train loss: 1.892, Global test loss: 1.710, Global test accuracy: 40.01 

Round   6, Train loss: 1.862, Test loss: 1.932, Test accuracy: 31.96 

Round   6, Global train loss: 1.862, Global test loss: 1.706, Global test accuracy: 41.78 

Round   7, Train loss: 1.823, Test loss: 1.889, Test accuracy: 33.66 

Round   7, Global train loss: 1.823, Global test loss: 1.644, Global test accuracy: 42.87 

Round   8, Train loss: 1.766, Test loss: 1.899, Test accuracy: 33.62 

Round   8, Global train loss: 1.766, Global test loss: 1.607, Global test accuracy: 42.60 

Round   9, Train loss: 1.808, Test loss: 1.884, Test accuracy: 34.45 

Round   9, Global train loss: 1.808, Global test loss: 1.652, Global test accuracy: 42.98 

Round  10, Train loss: 1.794, Test loss: 1.853, Test accuracy: 35.73 

Round  10, Global train loss: 1.794, Global test loss: 1.560, Global test accuracy: 46.52 

Round  11, Train loss: 1.722, Test loss: 1.842, Test accuracy: 36.30 

Round  11, Global train loss: 1.722, Global test loss: 1.537, Global test accuracy: 48.70 

Round  12, Train loss: 1.729, Test loss: 1.834, Test accuracy: 36.48 

Round  12, Global train loss: 1.729, Global test loss: 1.537, Global test accuracy: 46.61 

Round  13, Train loss: 1.690, Test loss: 1.824, Test accuracy: 37.08 

Round  13, Global train loss: 1.690, Global test loss: 1.475, Global test accuracy: 50.88 

Round  14, Train loss: 1.726, Test loss: 1.805, Test accuracy: 38.20 

Round  14, Global train loss: 1.726, Global test loss: 1.496, Global test accuracy: 50.13 

Round  15, Train loss: 1.702, Test loss: 1.787, Test accuracy: 38.66 

Round  15, Global train loss: 1.702, Global test loss: 1.514, Global test accuracy: 51.59 

Round  16, Train loss: 1.667, Test loss: 1.770, Test accuracy: 39.40 

Round  16, Global train loss: 1.667, Global test loss: 1.495, Global test accuracy: 50.97 

Round  17, Train loss: 1.595, Test loss: 1.781, Test accuracy: 38.96 

Round  17, Global train loss: 1.595, Global test loss: 1.432, Global test accuracy: 51.79 

Round  18, Train loss: 1.517, Test loss: 1.767, Test accuracy: 39.63 

Round  18, Global train loss: 1.517, Global test loss: 1.406, Global test accuracy: 51.97 

Round  19, Train loss: 1.550, Test loss: 1.761, Test accuracy: 40.02 

Round  19, Global train loss: 1.550, Global test loss: 1.375, Global test accuracy: 54.52 

Round  20, Train loss: 1.560, Test loss: 1.747, Test accuracy: 40.67 

Round  20, Global train loss: 1.560, Global test loss: 1.361, Global test accuracy: 54.53 

Round  21, Train loss: 1.606, Test loss: 1.750, Test accuracy: 40.63 

Round  21, Global train loss: 1.606, Global test loss: 1.444, Global test accuracy: 53.47 

Round  22, Train loss: 1.512, Test loss: 1.739, Test accuracy: 41.05 

Round  22, Global train loss: 1.512, Global test loss: 1.384, Global test accuracy: 53.99 

Round  23, Train loss: 1.634, Test loss: 1.737, Test accuracy: 41.41 

Round  23, Global train loss: 1.634, Global test loss: 1.465, Global test accuracy: 53.74 

Round  24, Train loss: 1.528, Test loss: 1.755, Test accuracy: 41.44 

Round  24, Global train loss: 1.528, Global test loss: 1.405, Global test accuracy: 54.38 

Round  25, Train loss: 1.518, Test loss: 1.748, Test accuracy: 42.09 

Round  25, Global train loss: 1.518, Global test loss: 1.380, Global test accuracy: 55.15 

Round  26, Train loss: 1.561, Test loss: 1.755, Test accuracy: 42.17 

Round  26, Global train loss: 1.561, Global test loss: 1.422, Global test accuracy: 54.31 

Round  27, Train loss: 1.425, Test loss: 1.749, Test accuracy: 42.45 

Round  27, Global train loss: 1.425, Global test loss: 1.350, Global test accuracy: 55.84 

Round  28, Train loss: 1.378, Test loss: 1.736, Test accuracy: 42.53 

Round  28, Global train loss: 1.378, Global test loss: 1.311, Global test accuracy: 55.91 

Round  29, Train loss: 1.414, Test loss: 1.745, Test accuracy: 42.43 

Round  29, Global train loss: 1.414, Global test loss: 1.322, Global test accuracy: 56.42 

Round  30, Train loss: 1.393, Test loss: 1.736, Test accuracy: 42.98 

Round  30, Global train loss: 1.393, Global test loss: 1.332, Global test accuracy: 56.01 

Round  31, Train loss: 1.455, Test loss: 1.763, Test accuracy: 42.48 

Round  31, Global train loss: 1.455, Global test loss: 1.419, Global test accuracy: 54.55 

Round  32, Train loss: 1.472, Test loss: 1.756, Test accuracy: 43.05 

Round  32, Global train loss: 1.472, Global test loss: 1.410, Global test accuracy: 54.19 

Round  33, Train loss: 1.322, Test loss: 1.751, Test accuracy: 43.33 

Round  33, Global train loss: 1.322, Global test loss: 1.319, Global test accuracy: 56.14 

Round  34, Train loss: 1.402, Test loss: 1.763, Test accuracy: 43.18 

Round  34, Global train loss: 1.402, Global test loss: 1.347, Global test accuracy: 55.88 

Round  35, Train loss: 1.309, Test loss: 1.759, Test accuracy: 43.26 

Round  35, Global train loss: 1.309, Global test loss: 1.343, Global test accuracy: 56.49 

Round  36, Train loss: 1.337, Test loss: 1.770, Test accuracy: 43.59 

Round  36, Global train loss: 1.337, Global test loss: 1.382, Global test accuracy: 54.98 

Round  37, Train loss: 1.421, Test loss: 1.802, Test accuracy: 43.14 

Round  37, Global train loss: 1.421, Global test loss: 1.408, Global test accuracy: 54.61 

Round  38, Train loss: 1.273, Test loss: 1.801, Test accuracy: 43.20 

Round  38, Global train loss: 1.273, Global test loss: 1.318, Global test accuracy: 56.57 

Round  39, Train loss: 1.291, Test loss: 1.828, Test accuracy: 42.79 

Round  39, Global train loss: 1.291, Global test loss: 1.351, Global test accuracy: 55.85 

Round  40, Train loss: 1.306, Test loss: 1.855, Test accuracy: 42.45 

Round  40, Global train loss: 1.306, Global test loss: 1.408, Global test accuracy: 54.26 

Round  41, Train loss: 1.336, Test loss: 1.884, Test accuracy: 42.38 

Round  41, Global train loss: 1.336, Global test loss: 1.396, Global test accuracy: 54.81 

Round  42, Train loss: 1.167, Test loss: 1.874, Test accuracy: 43.08 

Round  42, Global train loss: 1.167, Global test loss: 1.299, Global test accuracy: 56.65 

Round  43, Train loss: 1.139, Test loss: 1.894, Test accuracy: 42.97 

Round  43, Global train loss: 1.139, Global test loss: 1.240, Global test accuracy: 58.48 

Round  44, Train loss: 1.294, Test loss: 1.887, Test accuracy: 43.19 

Round  44, Global train loss: 1.294, Global test loss: 1.377, Global test accuracy: 55.66 

Round  45, Train loss: 1.298, Test loss: 1.888, Test accuracy: 42.96 

Round  45, Global train loss: 1.298, Global test loss: 1.425, Global test accuracy: 53.48 

Round  46, Train loss: 1.255, Test loss: 1.877, Test accuracy: 42.98 

Round  46, Global train loss: 1.255, Global test loss: 1.436, Global test accuracy: 53.76 

Round  47, Train loss: 1.078, Test loss: 1.894, Test accuracy: 42.76 

Round  47, Global train loss: 1.078, Global test loss: 1.359, Global test accuracy: 55.36 

Round  48, Train loss: 1.129, Test loss: 1.873, Test accuracy: 43.06 

Round  48, Global train loss: 1.129, Global test loss: 1.341, Global test accuracy: 56.02 

Round  49, Train loss: 1.041, Test loss: 1.883, Test accuracy: 43.23 

Round  49, Global train loss: 1.041, Global test loss: 1.280, Global test accuracy: 57.90 

Round  50, Train loss: 1.226, Test loss: 1.880, Test accuracy: 43.69 

Round  50, Global train loss: 1.226, Global test loss: 1.421, Global test accuracy: 53.95 

Round  51, Train loss: 1.049, Test loss: 1.906, Test accuracy: 43.55 

Round  51, Global train loss: 1.049, Global test loss: 1.341, Global test accuracy: 56.54 

Round  52, Train loss: 1.222, Test loss: 1.893, Test accuracy: 43.65 

Round  52, Global train loss: 1.222, Global test loss: 1.374, Global test accuracy: 55.72 

Round  53, Train loss: 1.177, Test loss: 1.913, Test accuracy: 43.53 

Round  53, Global train loss: 1.177, Global test loss: 1.301, Global test accuracy: 57.33 

Round  54, Train loss: 1.048, Test loss: 1.953, Test accuracy: 43.39 

Round  54, Global train loss: 1.048, Global test loss: 1.323, Global test accuracy: 56.69 

Round  55, Train loss: 1.180, Test loss: 1.957, Test accuracy: 43.26 

Round  55, Global train loss: 1.180, Global test loss: 1.365, Global test accuracy: 55.64 

Round  56, Train loss: 1.127, Test loss: 1.985, Test accuracy: 43.40 

Round  56, Global train loss: 1.127, Global test loss: 1.403, Global test accuracy: 54.49 

Round  57, Train loss: 1.190, Test loss: 1.988, Test accuracy: 43.42 

Round  57, Global train loss: 1.190, Global test loss: 1.452, Global test accuracy: 52.92 

Round  58, Train loss: 1.006, Test loss: 2.013, Test accuracy: 43.30 

Round  58, Global train loss: 1.006, Global test loss: 1.367, Global test accuracy: 56.55 

Round  59, Train loss: 1.096, Test loss: 2.024, Test accuracy: 43.19 

Round  59, Global train loss: 1.096, Global test loss: 1.399, Global test accuracy: 55.02 

Round  60, Train loss: 0.963, Test loss: 2.072, Test accuracy: 43.13 

Round  60, Global train loss: 0.963, Global test loss: 1.374, Global test accuracy: 56.76 

Round  61, Train loss: 1.085, Test loss: 2.076, Test accuracy: 43.37 

Round  61, Global train loss: 1.085, Global test loss: 1.386, Global test accuracy: 54.99 

Round  62, Train loss: 1.080, Test loss: 2.101, Test accuracy: 42.95 

Round  62, Global train loss: 1.080, Global test loss: 1.516, Global test accuracy: 51.86 

Round  63, Train loss: 1.032, Test loss: 2.125, Test accuracy: 42.80 

Round  63, Global train loss: 1.032, Global test loss: 1.468, Global test accuracy: 53.36 

Round  64, Train loss: 1.057, Test loss: 2.141, Test accuracy: 42.47 

Round  64, Global train loss: 1.057, Global test loss: 1.346, Global test accuracy: 56.74 

Round  65, Train loss: 0.991, Test loss: 2.104, Test accuracy: 42.80 

Round  65, Global train loss: 0.991, Global test loss: 1.449, Global test accuracy: 53.97 

Round  66, Train loss: 1.059, Test loss: 2.075, Test accuracy: 43.20 

Round  66, Global train loss: 1.059, Global test loss: 1.404, Global test accuracy: 55.19 

Round  67, Train loss: 0.960, Test loss: 2.113, Test accuracy: 42.82 

Round  67, Global train loss: 0.960, Global test loss: 1.403, Global test accuracy: 55.39 

Round  68, Train loss: 1.021, Test loss: 2.110, Test accuracy: 42.74 

Round  68, Global train loss: 1.021, Global test loss: 1.428, Global test accuracy: 54.00 

Round  69, Train loss: 0.843, Test loss: 2.142, Test accuracy: 42.95 

Round  69, Global train loss: 0.843, Global test loss: 1.395, Global test accuracy: 56.59 

Round  70, Train loss: 0.922, Test loss: 2.159, Test accuracy: 42.91 

Round  70, Global train loss: 0.922, Global test loss: 1.387, Global test accuracy: 56.61 

Round  71, Train loss: 0.874, Test loss: 2.189, Test accuracy: 43.14 

Round  71, Global train loss: 0.874, Global test loss: 1.489, Global test accuracy: 54.02 

Round  72, Train loss: 0.933, Test loss: 2.206, Test accuracy: 43.34 

Round  72, Global train loss: 0.933, Global test loss: 1.494, Global test accuracy: 53.58 

Round  73, Train loss: 0.919, Test loss: 2.226, Test accuracy: 43.30 

Round  73, Global train loss: 0.919, Global test loss: 1.358, Global test accuracy: 57.10 

Round  74, Train loss: 1.013, Test loss: 2.217, Test accuracy: 43.38 

Round  74, Global train loss: 1.013, Global test loss: 1.423, Global test accuracy: 54.92 

Round  75, Train loss: 1.056, Test loss: 2.207, Test accuracy: 43.28 

Round  75, Global train loss: 1.056, Global test loss: 1.470, Global test accuracy: 54.41 

Round  76, Train loss: 0.897, Test loss: 2.218, Test accuracy: 42.99 

Round  76, Global train loss: 0.897, Global test loss: 1.485, Global test accuracy: 54.09 

Round  77, Train loss: 0.854, Test loss: 2.206, Test accuracy: 43.28 

Round  77, Global train loss: 0.854, Global test loss: 1.479, Global test accuracy: 54.66 

Round  78, Train loss: 0.907, Test loss: 2.232, Test accuracy: 43.07 

Round  78, Global train loss: 0.907, Global test loss: 1.442, Global test accuracy: 55.79 

Round  79, Train loss: 0.932, Test loss: 2.226, Test accuracy: 43.07 

Round  79, Global train loss: 0.932, Global test loss: 1.481, Global test accuracy: 54.29 

Round  80, Train loss: 0.921, Test loss: 2.242, Test accuracy: 43.09 

Round  80, Global train loss: 0.921, Global test loss: 1.461, Global test accuracy: 55.20 

Round  81, Train loss: 0.837, Test loss: 2.258, Test accuracy: 43.16 

Round  81, Global train loss: 0.837, Global test loss: 1.436, Global test accuracy: 56.37 

Round  82, Train loss: 0.916, Test loss: 2.230, Test accuracy: 43.09 

Round  82, Global train loss: 0.916, Global test loss: 1.489, Global test accuracy: 54.48 

Round  83, Train loss: 0.954, Test loss: 2.226, Test accuracy: 43.08 

Round  83, Global train loss: 0.954, Global test loss: 1.496, Global test accuracy: 54.13 

Round  84, Train loss: 0.909, Test loss: 2.242, Test accuracy: 42.91 

Round  84, Global train loss: 0.909, Global test loss: 1.491, Global test accuracy: 54.73 

Round  85, Train loss: 0.983, Test loss: 2.254, Test accuracy: 43.13 

Round  85, Global train loss: 0.983, Global test loss: 1.548, Global test accuracy: 52.10 

Round  86, Train loss: 0.866, Test loss: 2.273, Test accuracy: 43.40 

Round  86, Global train loss: 0.866, Global test loss: 1.611, Global test accuracy: 52.12 

Round  87, Train loss: 0.875, Test loss: 2.272, Test accuracy: 43.51 

Round  87, Global train loss: 0.875, Global test loss: 1.446, Global test accuracy: 56.36 

Round  88, Train loss: 0.759, Test loss: 2.317, Test accuracy: 43.38 

Round  88, Global train loss: 0.759, Global test loss: 1.535, Global test accuracy: 54.66 

Round  89, Train loss: 0.920, Test loss: 2.314, Test accuracy: 42.98 

Round  89, Global train loss: 0.920, Global test loss: 1.511, Global test accuracy: 54.27 

Round  90, Train loss: 0.784, Test loss: 2.311, Test accuracy: 43.01 

Round  90, Global train loss: 0.784, Global test loss: 1.499, Global test accuracy: 55.64 

Round  91, Train loss: 0.894, Test loss: 2.335, Test accuracy: 42.99 

Round  91, Global train loss: 0.894, Global test loss: 1.511, Global test accuracy: 53.78 

Round  92, Train loss: 0.806, Test loss: 2.348, Test accuracy: 42.90 

Round  92, Global train loss: 0.806, Global test loss: 1.507, Global test accuracy: 55.02 

Round  93, Train loss: 0.865, Test loss: 2.367, Test accuracy: 42.66 

Round  93, Global train loss: 0.865, Global test loss: 1.528, Global test accuracy: 54.19 

Round  94, Train loss: 0.737, Test loss: 2.385, Test accuracy: 42.88 

Round  94, Global train loss: 0.737, Global test loss: 1.517, Global test accuracy: 56.71 

Round  95, Train loss: 0.783, Test loss: 2.384, Test accuracy: 42.87 

Round  95, Global train loss: 0.783, Global test loss: 1.504, Global test accuracy: 55.57 

Round  96, Train loss: 0.923, Test loss: 2.378, Test accuracy: 43.11 

Round  96, Global train loss: 0.923, Global test loss: 1.623, Global test accuracy: 51.26 

Round  97, Train loss: 0.833, Test loss: 2.390, Test accuracy: 43.01 

Round  97, Global train loss: 0.833, Global test loss: 1.525, Global test accuracy: 54.06 

Round  98, Train loss: 0.742, Test loss: 2.411, Test accuracy: 43.06 

Round  98, Global train loss: 0.742, Global test loss: 1.513, Global test accuracy: 55.70 

Round  99, Train loss: 0.772, Test loss: 2.381, Test accuracy: 42.91 

Round  99, Global train loss: 0.772, Global test loss: 1.557, Global test accuracy: 54.13 

Final Round, Train loss: 0.641, Test loss: 2.791, Test accuracy: 41.92 

Final Round, Global train loss: 0.641, Global test loss: 1.557, Global test accuracy: 54.13 

Average accuracy final 10 rounds: 42.939499999999995 

Average global accuracy final 10 rounds: 54.605500000000006 

2759.557494878769
[1.3752050399780273, 2.500314474105835, 3.62616229057312, 4.758394002914429, 5.942038297653198, 7.128351211547852, 8.314512014389038, 9.5054190158844, 10.692138910293579, 11.87162971496582, 13.053578615188599, 14.236131191253662, 15.41736102104187, 16.599066019058228, 17.781373262405396, 18.96816611289978, 20.144527435302734, 21.327942609786987, 22.524208784103394, 23.705413341522217, 24.897480010986328, 26.086979866027832, 27.27952742576599, 28.471380710601807, 29.666873455047607, 30.789562225341797, 31.988431692123413, 33.223530292510986, 34.42760753631592, 35.64219284057617, 36.850244998931885, 38.0465133190155, 39.25325155258179, 40.44507813453674, 41.64076805114746, 42.8427836894989, 44.04184079170227, 45.23195004463196, 46.43536138534546, 47.64047384262085, 48.83867573738098, 50.044689893722534, 51.25366497039795, 52.446908712387085, 53.65459108352661, 54.85813546180725, 56.05798697471619, 57.261462688446045, 58.463358879089355, 59.664751052856445, 60.86638903617859, 62.068925619125366, 63.27312684059143, 64.47905468940735, 65.68543601036072, 66.87660002708435, 68.07922911643982, 69.27874183654785, 70.47954034805298, 71.67938446998596, 72.87731909751892, 74.0750458240509, 75.27473616600037, 76.38736915588379, 77.41149139404297, 78.43233251571655, 79.53810977935791, 80.55907654762268, 81.58217096328735, 82.59871745109558, 83.6199803352356, 84.64002776145935, 85.6577479839325, 86.68201899528503, 87.70624279975891, 88.73214888572693, 89.76142764091492, 90.78316378593445, 91.81179547309875, 92.8423068523407, 93.86559748649597, 94.90155673027039, 95.92828226089478, 96.95343136787415, 97.98456597328186, 99.01376628875732, 100.04109239578247, 101.07550477981567, 102.0923125743866, 103.13824534416199, 104.20159101486206, 105.22237873077393, 106.25115036964417, 107.2827513217926, 108.3258798122406, 109.45212960243225, 110.48930931091309, 111.52584671974182, 112.56191825866699, 113.6012511253357, 115.84120774269104]
[20.3475, 22.6225, 25.785, 26.8275, 28.9025, 30.405, 31.9575, 33.66, 33.615, 34.4475, 35.73, 36.3025, 36.485, 37.0825, 38.205, 38.6575, 39.395, 38.96, 39.63, 40.015, 40.675, 40.6275, 41.05, 41.415, 41.4375, 42.09, 42.175, 42.4525, 42.53, 42.4325, 42.9825, 42.4775, 43.0525, 43.325, 43.1825, 43.2575, 43.59, 43.14, 43.1975, 42.79, 42.45, 42.385, 43.0825, 42.9675, 43.185, 42.9575, 42.9775, 42.755, 43.0575, 43.235, 43.69, 43.55, 43.6475, 43.53, 43.39, 43.2625, 43.4025, 43.42, 43.305, 43.1875, 43.13, 43.37, 42.95, 42.8, 42.465, 42.795, 43.1975, 42.82, 42.745, 42.945, 42.915, 43.1425, 43.34, 43.295, 43.38, 43.28, 42.99, 43.2825, 43.0675, 43.0675, 43.095, 43.165, 43.09, 43.0775, 42.905, 43.1325, 43.4025, 43.505, 43.375, 42.98, 43.0075, 42.995, 42.9, 42.665, 42.8775, 42.8675, 43.1075, 43.0075, 43.0575, 42.91, 41.9175]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8520
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2845
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8185
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1110
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5645
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7955
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0670
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7030
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5205
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8755
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.212, Test loss: 2.218, Test accuracy: 20.18 

Round   0, Global train loss: 2.212, Global test loss: 2.199, Global test accuracy: 20.72 

Round   1, Train loss: 2.093, Test loss: 2.137, Test accuracy: 24.08 

Round   1, Global train loss: 2.093, Global test loss: 2.059, Global test accuracy: 26.69 

Round   2, Train loss: 2.029, Test loss: 2.081, Test accuracy: 26.41 

Round   2, Global train loss: 2.029, Global test loss: 1.951, Global test accuracy: 30.87 

Round   3, Train loss: 1.979, Test loss: 2.060, Test accuracy: 27.57 

Round   3, Global train loss: 1.979, Global test loss: 1.880, Global test accuracy: 34.32 

Round   4, Train loss: 1.886, Test loss: 2.008, Test accuracy: 28.59 

Round   4, Global train loss: 1.886, Global test loss: 1.783, Global test accuracy: 34.95 

Round   5, Train loss: 1.870, Test loss: 1.964, Test accuracy: 30.68 

Round   5, Global train loss: 1.870, Global test loss: 1.735, Global test accuracy: 38.52 

Round   6, Train loss: 1.800, Test loss: 1.911, Test accuracy: 32.71 

Round   6, Global train loss: 1.800, Global test loss: 1.651, Global test accuracy: 40.29 

Round   7, Train loss: 1.850, Test loss: 1.880, Test accuracy: 33.93 

Round   7, Global train loss: 1.850, Global test loss: 1.673, Global test accuracy: 42.19 

Round   8, Train loss: 1.714, Test loss: 1.858, Test accuracy: 34.38 

Round   8, Global train loss: 1.714, Global test loss: 1.579, Global test accuracy: 43.32 

Round   9, Train loss: 1.791, Test loss: 1.841, Test accuracy: 35.31 

Round   9, Global train loss: 1.791, Global test loss: 1.614, Global test accuracy: 44.45 

Round  10, Train loss: 1.778, Test loss: 1.795, Test accuracy: 36.84 

Round  10, Global train loss: 1.778, Global test loss: 1.600, Global test accuracy: 43.61 

Round  11, Train loss: 1.708, Test loss: 1.778, Test accuracy: 37.55 

Round  11, Global train loss: 1.708, Global test loss: 1.526, Global test accuracy: 46.97 

Round  12, Train loss: 1.666, Test loss: 1.786, Test accuracy: 37.22 

Round  12, Global train loss: 1.666, Global test loss: 1.519, Global test accuracy: 46.74 

Round  13, Train loss: 1.688, Test loss: 1.773, Test accuracy: 37.72 

Round  13, Global train loss: 1.688, Global test loss: 1.533, Global test accuracy: 48.97 

Round  14, Train loss: 1.653, Test loss: 1.735, Test accuracy: 39.08 

Round  14, Global train loss: 1.653, Global test loss: 1.481, Global test accuracy: 49.22 

Round  15, Train loss: 1.634, Test loss: 1.728, Test accuracy: 39.69 

Round  15, Global train loss: 1.634, Global test loss: 1.473, Global test accuracy: 49.18 

Round  16, Train loss: 1.663, Test loss: 1.709, Test accuracy: 40.65 

Round  16, Global train loss: 1.663, Global test loss: 1.452, Global test accuracy: 51.10 

Round  17, Train loss: 1.679, Test loss: 1.701, Test accuracy: 41.04 

Round  17, Global train loss: 1.679, Global test loss: 1.465, Global test accuracy: 50.29 

Round  18, Train loss: 1.559, Test loss: 1.692, Test accuracy: 41.50 

Round  18, Global train loss: 1.559, Global test loss: 1.398, Global test accuracy: 51.13 

Round  19, Train loss: 1.536, Test loss: 1.680, Test accuracy: 42.38 

Round  19, Global train loss: 1.536, Global test loss: 1.408, Global test accuracy: 52.25 

Round  20, Train loss: 1.605, Test loss: 1.670, Test accuracy: 42.94 

Round  20, Global train loss: 1.605, Global test loss: 1.443, Global test accuracy: 51.48 

Round  21, Train loss: 1.522, Test loss: 1.677, Test accuracy: 42.52 

Round  21, Global train loss: 1.522, Global test loss: 1.385, Global test accuracy: 51.95 

Round  22, Train loss: 1.603, Test loss: 1.658, Test accuracy: 43.09 

Round  22, Global train loss: 1.603, Global test loss: 1.430, Global test accuracy: 53.15 

Round  23, Train loss: 1.522, Test loss: 1.658, Test accuracy: 43.22 

Round  23, Global train loss: 1.522, Global test loss: 1.375, Global test accuracy: 53.20 

Round  24, Train loss: 1.496, Test loss: 1.665, Test accuracy: 43.12 

Round  24, Global train loss: 1.496, Global test loss: 1.353, Global test accuracy: 53.91 

Round  25, Train loss: 1.536, Test loss: 1.651, Test accuracy: 43.68 

Round  25, Global train loss: 1.536, Global test loss: 1.357, Global test accuracy: 55.36 

Round  26, Train loss: 1.510, Test loss: 1.634, Test accuracy: 44.28 

Round  26, Global train loss: 1.510, Global test loss: 1.348, Global test accuracy: 55.12 

Round  27, Train loss: 1.444, Test loss: 1.633, Test accuracy: 44.69 

Round  27, Global train loss: 1.444, Global test loss: 1.315, Global test accuracy: 55.55 

Round  28, Train loss: 1.414, Test loss: 1.646, Test accuracy: 44.38 

Round  28, Global train loss: 1.414, Global test loss: 1.332, Global test accuracy: 56.00 

Round  29, Train loss: 1.485, Test loss: 1.642, Test accuracy: 44.68 

Round  29, Global train loss: 1.485, Global test loss: 1.350, Global test accuracy: 55.81 

Round  30, Train loss: 1.437, Test loss: 1.647, Test accuracy: 44.41 

Round  30, Global train loss: 1.437, Global test loss: 1.334, Global test accuracy: 55.68 

Round  31, Train loss: 1.342, Test loss: 1.637, Test accuracy: 44.71 

Round  31, Global train loss: 1.342, Global test loss: 1.279, Global test accuracy: 57.62 

Round  32, Train loss: 1.340, Test loss: 1.628, Test accuracy: 45.24 

Round  32, Global train loss: 1.340, Global test loss: 1.241, Global test accuracy: 58.04 

Round  33, Train loss: 1.472, Test loss: 1.606, Test accuracy: 45.99 

Round  33, Global train loss: 1.472, Global test loss: 1.323, Global test accuracy: 56.48 

Round  34, Train loss: 1.359, Test loss: 1.603, Test accuracy: 46.11 

Round  34, Global train loss: 1.359, Global test loss: 1.282, Global test accuracy: 56.92 

Round  35, Train loss: 1.340, Test loss: 1.612, Test accuracy: 46.29 

Round  35, Global train loss: 1.340, Global test loss: 1.258, Global test accuracy: 58.59 

Round  36, Train loss: 1.279, Test loss: 1.613, Test accuracy: 46.12 

Round  36, Global train loss: 1.279, Global test loss: 1.257, Global test accuracy: 57.92 

Round  37, Train loss: 1.293, Test loss: 1.615, Test accuracy: 46.40 

Round  37, Global train loss: 1.293, Global test loss: 1.254, Global test accuracy: 57.92 

Round  38, Train loss: 1.277, Test loss: 1.627, Test accuracy: 46.29 

Round  38, Global train loss: 1.277, Global test loss: 1.274, Global test accuracy: 57.30 

Round  39, Train loss: 1.373, Test loss: 1.618, Test accuracy: 46.97 

Round  39, Global train loss: 1.373, Global test loss: 1.320, Global test accuracy: 56.96 

Round  40, Train loss: 1.258, Test loss: 1.630, Test accuracy: 46.84 

Round  40, Global train loss: 1.258, Global test loss: 1.226, Global test accuracy: 59.12 

Round  41, Train loss: 1.283, Test loss: 1.659, Test accuracy: 46.42 

Round  41, Global train loss: 1.283, Global test loss: 1.276, Global test accuracy: 57.58 

Round  42, Train loss: 1.324, Test loss: 1.626, Test accuracy: 47.41 

Round  42, Global train loss: 1.324, Global test loss: 1.276, Global test accuracy: 58.48 

Round  43, Train loss: 1.215, Test loss: 1.641, Test accuracy: 47.12 

Round  43, Global train loss: 1.215, Global test loss: 1.233, Global test accuracy: 58.97 

Round  44, Train loss: 1.186, Test loss: 1.644, Test accuracy: 47.27 

Round  44, Global train loss: 1.186, Global test loss: 1.239, Global test accuracy: 58.40 

Round  45, Train loss: 1.287, Test loss: 1.642, Test accuracy: 47.46 

Round  45, Global train loss: 1.287, Global test loss: 1.294, Global test accuracy: 56.80 

Round  46, Train loss: 1.124, Test loss: 1.642, Test accuracy: 47.59 

Round  46, Global train loss: 1.124, Global test loss: 1.259, Global test accuracy: 58.34 

Round  47, Train loss: 1.189, Test loss: 1.654, Test accuracy: 47.43 

Round  47, Global train loss: 1.189, Global test loss: 1.239, Global test accuracy: 58.66 

Round  48, Train loss: 1.155, Test loss: 1.662, Test accuracy: 47.65 

Round  48, Global train loss: 1.155, Global test loss: 1.232, Global test accuracy: 59.41 

Round  49, Train loss: 1.099, Test loss: 1.661, Test accuracy: 47.84 

Round  49, Global train loss: 1.099, Global test loss: 1.229, Global test accuracy: 59.30 

Round  50, Train loss: 1.161, Test loss: 1.665, Test accuracy: 47.97 

Round  50, Global train loss: 1.161, Global test loss: 1.205, Global test accuracy: 59.91 

Round  51, Train loss: 1.082, Test loss: 1.656, Test accuracy: 48.16 

Round  51, Global train loss: 1.082, Global test loss: 1.214, Global test accuracy: 59.14 

Round  52, Train loss: 1.135, Test loss: 1.668, Test accuracy: 47.99 

Round  52, Global train loss: 1.135, Global test loss: 1.198, Global test accuracy: 60.80 

Round  53, Train loss: 1.159, Test loss: 1.658, Test accuracy: 48.13 

Round  53, Global train loss: 1.159, Global test loss: 1.203, Global test accuracy: 60.34 

Round  54, Train loss: 1.031, Test loss: 1.656, Test accuracy: 48.22 

Round  54, Global train loss: 1.031, Global test loss: 1.218, Global test accuracy: 59.53 

Round  55, Train loss: 1.103, Test loss: 1.653, Test accuracy: 48.49 

Round  55, Global train loss: 1.103, Global test loss: 1.192, Global test accuracy: 60.51 

Round  56, Train loss: 1.013, Test loss: 1.675, Test accuracy: 48.36 

Round  56, Global train loss: 1.013, Global test loss: 1.174, Global test accuracy: 60.95 

Round  57, Train loss: 1.029, Test loss: 1.679, Test accuracy: 48.65 

Round  57, Global train loss: 1.029, Global test loss: 1.178, Global test accuracy: 60.91 

Round  58, Train loss: 1.145, Test loss: 1.696, Test accuracy: 48.73 

Round  58, Global train loss: 1.145, Global test loss: 1.281, Global test accuracy: 57.62 

Round  59, Train loss: 1.001, Test loss: 1.715, Test accuracy: 48.58 

Round  59, Global train loss: 1.001, Global test loss: 1.210, Global test accuracy: 60.30 

Round  60, Train loss: 1.064, Test loss: 1.720, Test accuracy: 48.36 

Round  60, Global train loss: 1.064, Global test loss: 1.283, Global test accuracy: 58.29 

Round  61, Train loss: 1.026, Test loss: 1.713, Test accuracy: 48.36 

Round  61, Global train loss: 1.026, Global test loss: 1.216, Global test accuracy: 60.43 

Round  62, Train loss: 0.943, Test loss: 1.708, Test accuracy: 48.46 

Round  62, Global train loss: 0.943, Global test loss: 1.213, Global test accuracy: 60.05 

Round  63, Train loss: 0.996, Test loss: 1.719, Test accuracy: 48.33 

Round  63, Global train loss: 0.996, Global test loss: 1.242, Global test accuracy: 59.41 

Round  64, Train loss: 1.042, Test loss: 1.725, Test accuracy: 48.47 

Round  64, Global train loss: 1.042, Global test loss: 1.200, Global test accuracy: 61.20 

Round  65, Train loss: 1.099, Test loss: 1.730, Test accuracy: 48.40 

Round  65, Global train loss: 1.099, Global test loss: 1.281, Global test accuracy: 58.56 

Round  66, Train loss: 1.053, Test loss: 1.727, Test accuracy: 48.59 

Round  66, Global train loss: 1.053, Global test loss: 1.190, Global test accuracy: 60.95 

Round  67, Train loss: 1.070, Test loss: 1.734, Test accuracy: 48.68 

Round  67, Global train loss: 1.070, Global test loss: 1.238, Global test accuracy: 60.25 

Round  68, Train loss: 0.981, Test loss: 1.768, Test accuracy: 48.38 

Round  68, Global train loss: 0.981, Global test loss: 1.229, Global test accuracy: 60.19 

Round  69, Train loss: 0.831, Test loss: 1.766, Test accuracy: 48.31 

Round  69, Global train loss: 0.831, Global test loss: 1.199, Global test accuracy: 61.22 

Round  70, Train loss: 0.938, Test loss: 1.770, Test accuracy: 48.21 

Round  70, Global train loss: 0.938, Global test loss: 1.239, Global test accuracy: 59.35 

Round  71, Train loss: 0.918, Test loss: 1.764, Test accuracy: 48.44 

Round  71, Global train loss: 0.918, Global test loss: 1.267, Global test accuracy: 58.97 

Round  72, Train loss: 0.914, Test loss: 1.784, Test accuracy: 48.29 

Round  72, Global train loss: 0.914, Global test loss: 1.277, Global test accuracy: 59.20 

Round  73, Train loss: 0.992, Test loss: 1.751, Test accuracy: 48.81 

Round  73, Global train loss: 0.992, Global test loss: 1.232, Global test accuracy: 60.28 

Round  74, Train loss: 0.984, Test loss: 1.763, Test accuracy: 48.83 

Round  74, Global train loss: 0.984, Global test loss: 1.222, Global test accuracy: 61.12 

Round  75, Train loss: 1.037, Test loss: 1.816, Test accuracy: 48.18 

Round  75, Global train loss: 1.037, Global test loss: 1.240, Global test accuracy: 60.17 

Round  76, Train loss: 0.869, Test loss: 1.811, Test accuracy: 48.45 

Round  76, Global train loss: 0.869, Global test loss: 1.270, Global test accuracy: 59.51 

Round  77, Train loss: 0.848, Test loss: 1.832, Test accuracy: 48.70 

Round  77, Global train loss: 0.848, Global test loss: 1.284, Global test accuracy: 59.51 

Round  78, Train loss: 0.908, Test loss: 1.847, Test accuracy: 48.21 

Round  78, Global train loss: 0.908, Global test loss: 1.289, Global test accuracy: 59.32 

Round  79, Train loss: 0.850, Test loss: 1.857, Test accuracy: 48.32 

Round  79, Global train loss: 0.850, Global test loss: 1.232, Global test accuracy: 61.08 

Round  80, Train loss: 0.878, Test loss: 1.839, Test accuracy: 48.80 

Round  80, Global train loss: 0.878, Global test loss: 1.231, Global test accuracy: 61.09 

Round  81, Train loss: 0.840, Test loss: 1.797, Test accuracy: 49.41 

Round  81, Global train loss: 0.840, Global test loss: 1.233, Global test accuracy: 61.03 

Round  82, Train loss: 0.832, Test loss: 1.802, Test accuracy: 49.37 

Round  82, Global train loss: 0.832, Global test loss: 1.255, Global test accuracy: 60.46 

Round  83, Train loss: 0.968, Test loss: 1.808, Test accuracy: 49.43 

Round  83, Global train loss: 0.968, Global test loss: 1.261, Global test accuracy: 60.12 

Round  84, Train loss: 0.827, Test loss: 1.851, Test accuracy: 49.10 

Round  84, Global train loss: 0.827, Global test loss: 1.259, Global test accuracy: 61.20 

Round  85, Train loss: 0.861, Test loss: 1.862, Test accuracy: 48.80 

Round  85, Global train loss: 0.861, Global test loss: 1.258, Global test accuracy: 60.80 

Round  86, Train loss: 0.776, Test loss: 1.860, Test accuracy: 48.88 

Round  86, Global train loss: 0.776, Global test loss: 1.315, Global test accuracy: 59.87 

Round  87, Train loss: 0.921, Test loss: 1.865, Test accuracy: 49.21 

Round  87, Global train loss: 0.921, Global test loss: 1.276, Global test accuracy: 60.28 

Round  88, Train loss: 0.789, Test loss: 1.858, Test accuracy: 49.16 

Round  88, Global train loss: 0.789, Global test loss: 1.295, Global test accuracy: 59.90 

Round  89, Train loss: 0.958, Test loss: 1.879, Test accuracy: 49.23 
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  89, Global train loss: 0.958, Global test loss: 1.358, Global test accuracy: 57.29 

Round  90, Train loss: 0.757, Test loss: 1.872, Test accuracy: 49.40 

Round  90, Global train loss: 0.757, Global test loss: 1.258, Global test accuracy: 61.73 

Round  91, Train loss: 0.909, Test loss: 1.874, Test accuracy: 49.45 

Round  91, Global train loss: 0.909, Global test loss: 1.326, Global test accuracy: 58.45 

Round  92, Train loss: 0.782, Test loss: 1.883, Test accuracy: 49.01 

Round  92, Global train loss: 0.782, Global test loss: 1.316, Global test accuracy: 59.25 

Round  93, Train loss: 0.788, Test loss: 1.888, Test accuracy: 48.88 

Round  93, Global train loss: 0.788, Global test loss: 1.343, Global test accuracy: 59.38 

Round  94, Train loss: 0.793, Test loss: 1.902, Test accuracy: 48.96 

Round  94, Global train loss: 0.793, Global test loss: 1.308, Global test accuracy: 60.22 

Round  95, Train loss: 0.724, Test loss: 1.918, Test accuracy: 48.70 

Round  95, Global train loss: 0.724, Global test loss: 1.311, Global test accuracy: 60.26 

Round  96, Train loss: 0.795, Test loss: 1.896, Test accuracy: 49.34 

Round  96, Global train loss: 0.795, Global test loss: 1.315, Global test accuracy: 59.52 

Round  97, Train loss: 0.834, Test loss: 1.907, Test accuracy: 49.60 

Round  97, Global train loss: 0.834, Global test loss: 1.313, Global test accuracy: 60.25 

Round  98, Train loss: 0.867, Test loss: 1.936, Test accuracy: 49.31 

Round  98, Global train loss: 0.867, Global test loss: 1.319, Global test accuracy: 59.05 

Round  99, Train loss: 0.738, Test loss: 1.946, Test accuracy: 49.35 

Round  99, Global train loss: 0.738, Global test loss: 1.371, Global test accuracy: 58.14 

Final Round, Train loss: 0.663, Test loss: 2.158, Test accuracy: 48.95 

Final Round, Global train loss: 0.663, Global test loss: 1.371, Global test accuracy: 58.14 

Average accuracy final 10 rounds: 49.200500000000005 

Average global accuracy final 10 rounds: 59.625249999999994 

2925.8720343112946
[1.4961600303649902, 2.74809193611145, 3.9716637134552, 5.077688455581665, 6.176013469696045, 7.308549880981445, 8.410317182540894, 9.508850574493408, 10.610980749130249, 11.712639093399048, 12.815217018127441, 13.914159536361694, 15.01830244064331, 16.13798689842224, 17.25935673713684, 18.69091010093689, 19.809722423553467, 21.026837825775146, 22.23638653755188, 23.348943948745728, 24.46430253982544, 25.576289176940918, 26.676084756851196, 27.773228645324707, 28.87572407722473, 29.977773427963257, 31.07010769844055, 32.172070026397705, 33.272671699523926, 34.375641107559204, 35.4793484210968, 36.5801796913147, 37.684282541275024, 38.787291049957275, 39.88834595680237, 40.99159264564514, 42.09272027015686, 43.19364833831787, 44.295963764190674, 45.398343563079834, 46.49439001083374, 47.60490131378174, 48.807145833969116, 49.91366934776306, 51.02383804321289, 52.35127663612366, 53.55081629753113, 54.91177940368652, 56.230955600738525, 57.437270641326904, 58.723533391952515, 59.97412943840027, 61.264883041381836, 62.38095211982727, 63.57586097717285, 64.89048671722412, 66.22963428497314, 67.48014307022095, 68.67357277870178, 70.06561636924744, 71.44487619400024, 72.74392461776733, 74.02269458770752, 75.36477422714233, 76.69815278053284, 77.9805998802185, 79.27493453025818, 80.59519457817078, 81.9379780292511, 83.2324469089508, 84.52049779891968, 85.81775903701782, 87.164621591568, 88.51659798622131, 89.80922174453735, 91.14194893836975, 92.43478441238403, 93.78640532493591, 95.13416981697083, 96.48038864135742, 97.83192563056946, 99.15642619132996, 100.49886417388916, 101.84659576416016, 103.13320398330688, 104.42365884780884, 105.83380031585693, 107.11147093772888, 108.45351815223694, 109.80396366119385, 111.09644865989685, 112.40835809707642, 113.72370195388794, 115.02394247055054, 116.30418372154236, 117.64489483833313, 118.97893142700195, 120.31451106071472, 121.63756132125854, 122.9863498210907, 125.65421462059021]
[20.185, 24.0775, 26.415, 27.57, 28.595, 30.6825, 32.71, 33.93, 34.38, 35.31, 36.8425, 37.5525, 37.215, 37.7225, 39.0775, 39.685, 40.6525, 41.04, 41.4975, 42.3825, 42.935, 42.515, 43.085, 43.215, 43.1225, 43.6825, 44.2775, 44.685, 44.38, 44.6825, 44.405, 44.71, 45.245, 45.9925, 46.1075, 46.29, 46.125, 46.4, 46.2925, 46.965, 46.845, 46.4225, 47.4075, 47.125, 47.2725, 47.4625, 47.5875, 47.4275, 47.6475, 47.8425, 47.965, 48.165, 47.995, 48.135, 48.2175, 48.4925, 48.3575, 48.645, 48.725, 48.575, 48.3625, 48.3625, 48.4625, 48.3275, 48.47, 48.3975, 48.5925, 48.68, 48.375, 48.31, 48.21, 48.4375, 48.29, 48.8075, 48.8275, 48.18, 48.4475, 48.7025, 48.2075, 48.3175, 48.805, 49.415, 49.3725, 49.4325, 49.1025, 48.8, 48.8825, 49.21, 49.16, 49.235, 49.395, 49.4475, 49.0125, 48.8825, 48.96, 48.7025, 49.34, 49.6025, 49.31, 49.3525, 48.945]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8490
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.3440
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8255
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0280
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5640
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8035
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1745
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6790
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4495
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8595
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8395
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2890
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8180
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.1055
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5900
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8115
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1010
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7305
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5415
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8595
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  24.9200
Round 1 global test acc  31.4900
Round 2 global test acc  37.9500
Round 3 global test acc  46.0100
Round 4 global test acc  42.8100
Round 5 global test acc  47.4200
Round 6 global test acc  47.4900
Round 7 global test acc  50.1400
Round 8 global test acc  51.4000
Round 9 global test acc  53.2200
Round 10 global test acc  51.8800
Round 11 global test acc  52.2500
Round 12 global test acc  55.6800
Round 13 global test acc  54.8300
Round 14 global test acc  52.0600
Round 15 global test acc  55.5000
Round 16 global test acc  56.6100
Round 17 global test acc  56.2000
Round 18 global test acc  56.5300
Round 19 global test acc  55.7200
Round 20 global test acc  55.8800
Round 21 global test acc  56.8400
Round 22 global test acc  57.6700
Round 23 global test acc  58.3600
Round 24 global test acc  57.3400
Round 25 global test acc  58.5200
Round 26 global test acc  56.4800
Round 27 global test acc  58.5700
Round 28 global test acc  57.8300
Round 29 global test acc  57.9500
Round 30 global test acc  59.5300
Round 31 global test acc  56.0300
Round 32 global test acc  60.6000
Round 33 global test acc  56.8500
Round 34 global test acc  58.5100
Round 35 global test acc  60.3600
Round 36 global test acc  60.7300
Round 37 global test acc  59.8900
Round 38 global test acc  58.7500
Round 39 global test acc  58.0900
Round 40 global test acc  60.6300
Round 41 global test acc  59.5000
Round 42 global test acc  59.6100
Round 43 global test acc  62.4700
Round 44 global test acc  59.2600
Round 45 global test acc  61.7500
Round 46 global test acc  59.6300
Round 47 global test acc  59.5800
Round 48 global test acc  57.9400
Round 49 global test acc  58.5300
Round 50 global test acc  63.3200
Round 51 global test acc  62.1600
Round 52 global test acc  63.7700
Round 53 global test acc  60.9300
Round 54 global test acc  61.3900
Round 55 global test acc  61.1900
Round 56 global test acc  61.6400
Round 57 global test acc  57.4200
Round 58 global test acc  63.4500
Round 59 global test acc  60.4400
Round 60 global test acc  64.9400
Round 61 global test acc  64.2000
Round 62 global test acc  63.5400
Round 63 global test acc  60.1600
Round 64 global test acc  63.3200
Round 65 global test acc  62.6900
Round 66 global test acc  62.7100
Round 67 global test acc  63.2100
Round 68 global test acc  61.1800
Round 69 global test acc  62.7600
Round 70 global test acc  63.9200
Round 71 global test acc  62.8800
Round 72 global test acc  59.5500
Round 73 global test acc  62.8200
Round 74 global test acc  63.1800
Round 75 global test acc  62.4000
Round 76 global test acc  62.3500
Round 77 global test acc  64.0600
Round 78 global test acc  62.3800
Round 79 global test acc  65.6600
Round 80 global test acc  64.3700
Round 81 global test acc  63.7000
Round 82 global test acc  62.7400
Round 83 global test acc  60.9200
Round 84 global test acc  60.6100
Round 85 global test acc  59.8900
Round 86 global test acc  59.0500
Round 87 global test acc  59.2500
Round 88 global test acc  58.9000
Round 89 global test acc  59.2100
Round 90 global test acc  58.8000
Round 91 global test acc  58.9200
Round 92 global test acc  59.0800
Round 93 global test acc  57.8200
Round 94 global test acc  58.0300
Round 95 global test acc  57.5200
Round 96 global test acc  57.1800
Round 97 global test acc  57.1500
Round 98 global test acc  56.8800
Round 99 global test acc  56.8400
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8570
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.2885
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8240
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0270
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5920
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8410
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.4200
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6940
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.4560
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8550
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.283, Test loss: 2.251, Test accuracy: 17.87
Round   1, Train loss: 2.237, Test loss: 2.230, Test accuracy: 20.50
Round   2, Train loss: 2.188, Test loss: 2.159, Test accuracy: 23.23
Round   3, Train loss: 2.183, Test loss: 2.160, Test accuracy: 25.11
Round   4, Train loss: 2.150, Test loss: 2.135, Test accuracy: 26.47
Round   5, Train loss: 2.091, Test loss: 2.082, Test accuracy: 28.25
Round   6, Train loss: 2.109, Test loss: 2.076, Test accuracy: 29.28
Round   7, Train loss: 2.135, Test loss: 2.067, Test accuracy: 29.94
Round   8, Train loss: 2.041, Test loss: 2.035, Test accuracy: 30.72
Round   9, Train loss: 2.145, Test loss: 2.046, Test accuracy: 30.80
Round  10, Train loss: 1.967, Test loss: 1.979, Test accuracy: 33.24
Round  11, Train loss: 2.062, Test loss: 1.986, Test accuracy: 33.09
Round  12, Train loss: 2.151, Test loss: 2.019, Test accuracy: 32.86
Round  13, Train loss: 1.984, Test loss: 1.961, Test accuracy: 34.43
Round  14, Train loss: 1.981, Test loss: 1.952, Test accuracy: 34.66
Round  15, Train loss: 1.885, Test loss: 1.922, Test accuracy: 35.20
Round  16, Train loss: 1.945, Test loss: 1.920, Test accuracy: 35.67
Round  17, Train loss: 1.981, Test loss: 1.909, Test accuracy: 35.76
Round  18, Train loss: 1.907, Test loss: 1.889, Test accuracy: 36.34
Round  19, Train loss: 1.962, Test loss: 1.886, Test accuracy: 37.34
Round  20, Train loss: 2.003, Test loss: 1.896, Test accuracy: 37.11
Round  21, Train loss: 1.934, Test loss: 1.859, Test accuracy: 38.25
Round  22, Train loss: 2.045, Test loss: 1.875, Test accuracy: 37.80
Round  23, Train loss: 1.975, Test loss: 1.884, Test accuracy: 37.16
Round  24, Train loss: 1.917, Test loss: 1.865, Test accuracy: 37.73
Round  25, Train loss: 1.876, Test loss: 1.851, Test accuracy: 37.86
Round  26, Train loss: 1.923, Test loss: 1.843, Test accuracy: 38.30
Round  27, Train loss: 1.836, Test loss: 1.826, Test accuracy: 38.45
Round  28, Train loss: 1.932, Test loss: 1.826, Test accuracy: 38.38
Round  29, Train loss: 1.824, Test loss: 1.818, Test accuracy: 39.05
Round  30, Train loss: 1.876, Test loss: 1.818, Test accuracy: 39.16
Round  31, Train loss: 1.907, Test loss: 1.820, Test accuracy: 39.30
Round  32, Train loss: 1.831, Test loss: 1.798, Test accuracy: 39.24
Round  33, Train loss: 1.822, Test loss: 1.794, Test accuracy: 39.46
Round  34, Train loss: 1.760, Test loss: 1.784, Test accuracy: 39.78
Round  35, Train loss: 1.865, Test loss: 1.781, Test accuracy: 39.62
Round  36, Train loss: 1.898, Test loss: 1.792, Test accuracy: 39.49
Round  37, Train loss: 1.877, Test loss: 1.789, Test accuracy: 39.61
Round  38, Train loss: 1.792, Test loss: 1.771, Test accuracy: 40.17
Round  39, Train loss: 1.946, Test loss: 1.784, Test accuracy: 40.10
Round  40, Train loss: 1.591, Test loss: 1.768, Test accuracy: 40.34
Round  41, Train loss: 2.001, Test loss: 1.790, Test accuracy: 39.72
Round  42, Train loss: 1.937, Test loss: 1.778, Test accuracy: 40.13
Round  43, Train loss: 1.792, Test loss: 1.763, Test accuracy: 40.21
Round  44, Train loss: 1.823, Test loss: 1.769, Test accuracy: 40.40
Round  45, Train loss: 1.842, Test loss: 1.767, Test accuracy: 40.17
Round  46, Train loss: 1.848, Test loss: 1.778, Test accuracy: 39.76
Round  47, Train loss: 1.888, Test loss: 1.769, Test accuracy: 40.01
Round  48, Train loss: 1.738, Test loss: 1.763, Test accuracy: 40.28
Round  49, Train loss: 1.737, Test loss: 1.763, Test accuracy: 40.21
Round  50, Train loss: 1.730, Test loss: 1.760, Test accuracy: 40.26
Round  51, Train loss: 1.694, Test loss: 1.751, Test accuracy: 40.47
Round  52, Train loss: 1.754, Test loss: 1.746, Test accuracy: 40.63
Round  53, Train loss: 1.681, Test loss: 1.731, Test accuracy: 40.92
Round  54, Train loss: 1.764, Test loss: 1.737, Test accuracy: 40.72
Round  55, Train loss: 1.590, Test loss: 1.728, Test accuracy: 41.14
Round  56, Train loss: 1.647, Test loss: 1.723, Test accuracy: 41.49
Round  57, Train loss: 1.630, Test loss: 1.728, Test accuracy: 41.09
Round  58, Train loss: 1.793, Test loss: 1.743, Test accuracy: 40.53
Round  59, Train loss: 1.764, Test loss: 1.730, Test accuracy: 41.34
Round  60, Train loss: 1.776, Test loss: 1.733, Test accuracy: 41.19
Round  61, Train loss: 1.791, Test loss: 1.732, Test accuracy: 40.89
Round  62, Train loss: 1.728, Test loss: 1.739, Test accuracy: 40.48
Round  63, Train loss: 1.460, Test loss: 1.719, Test accuracy: 41.20
Round  64, Train loss: 1.503, Test loss: 1.695, Test accuracy: 42.08
Round  65, Train loss: 1.908, Test loss: 1.726, Test accuracy: 40.92
Round  66, Train loss: 1.548, Test loss: 1.721, Test accuracy: 41.05
Round  67, Train loss: 1.729, Test loss: 1.720, Test accuracy: 40.88
Round  68, Train loss: 1.610, Test loss: 1.709, Test accuracy: 41.13
Round  69, Train loss: 1.556, Test loss: 1.719, Test accuracy: 40.96
Round  70, Train loss: 1.546, Test loss: 1.724, Test accuracy: 40.79
Round  71, Train loss: 1.595, Test loss: 1.733, Test accuracy: 40.58
Round  72, Train loss: 1.709, Test loss: 1.743, Test accuracy: 40.19
Round  73, Train loss: 1.607, Test loss: 1.722, Test accuracy: 41.05
Round  74, Train loss: 1.637, Test loss: 1.713, Test accuracy: 41.15
Round  75, Train loss: 1.486, Test loss: 1.703, Test accuracy: 41.51
Round  76, Train loss: 1.483, Test loss: 1.709, Test accuracy: 41.19
Round  77, Train loss: 1.532, Test loss: 1.725, Test accuracy: 40.78
Round  78, Train loss: 1.477, Test loss: 1.715, Test accuracy: 41.05
Round  79, Train loss: 1.603, Test loss: 1.720, Test accuracy: 40.97
Round  80, Train loss: 1.592, Test loss: 1.728, Test accuracy: 40.91
Round  81, Train loss: 1.381, Test loss: 1.718, Test accuracy: 41.01
Round  82, Train loss: 1.361, Test loss: 1.711, Test accuracy: 41.66
Round  83, Train loss: 1.795, Test loss: 1.742, Test accuracy: 40.45
Round  84, Train loss: 1.438, Test loss: 1.722, Test accuracy: 41.14
Round  85, Train loss: 1.529, Test loss: 1.729, Test accuracy: 40.82
Round  86, Train loss: 1.522, Test loss: 1.739, Test accuracy: 40.11
Round  87, Train loss: 1.499, Test loss: 1.719, Test accuracy: 40.77
Round  88, Train loss: 1.445, Test loss: 1.723, Test accuracy: 40.65
Round  89, Train loss: 1.459, Test loss: 1.722, Test accuracy: 40.84
Round  90, Train loss: 1.383, Test loss: 1.723, Test accuracy: 40.96
Round  91, Train loss: 1.631, Test loss: 1.751, Test accuracy: 40.19
Round  92, Train loss: 1.516, Test loss: 1.745, Test accuracy: 40.44
Round  93, Train loss: 1.414, Test loss: 1.755, Test accuracy: 40.15
Round  94, Train loss: 1.287, Test loss: 1.739, Test accuracy: 40.59
Round  95, Train loss: 1.531, Test loss: 1.762, Test accuracy: 39.81
Round  96, Train loss: 1.523, Test loss: 1.761, Test accuracy: 40.12
Round  97, Train loss: 1.420, Test loss: 1.755, Test accuracy: 40.43
Round  98, Train loss: 1.497, Test loss: 1.778, Test accuracy: 39.74
Round  99, Train loss: 1.408, Test loss: 1.799, Test accuracy: 39.32
Final Round, Train loss: 1.353, Test loss: 1.819, Test accuracy: 39.10
Average accuracy final 10 rounds: 40.175000000000004
1842.03275847435
[1.6406831741333008, 2.994313955307007, 4.3187785148620605, 5.665125370025635, 7.0310282707214355, 8.376205682754517, 9.738981485366821, 11.094267845153809, 12.449124336242676, 13.81663179397583, 15.177855253219604, 16.519243001937866, 17.87836766242981, 19.235944032669067, 20.585863828659058, 21.93458843231201, 23.279825925827026, 24.52827501296997, 25.7661075592041, 27.061787605285645, 28.402459859848022, 29.719491243362427, 31.05775022506714, 32.38764524459839, 33.70450949668884, 35.03192138671875, 36.38087558746338, 37.74530243873596, 39.102410316467285, 40.44806241989136, 41.76711463928223, 43.11305809020996, 44.46667695045471, 45.822402238845825, 47.166003465652466, 48.50641441345215, 49.84209179878235, 51.174965143203735, 52.52159833908081, 53.86426615715027, 55.206082820892334, 56.55923008918762, 57.90942978858948, 59.24826526641846, 60.59442734718323, 61.94252848625183, 63.278035402297974, 64.62656807899475, 65.97009825706482, 67.30826783180237, 68.64492416381836, 70.00247049331665, 71.36836886405945, 72.71229434013367, 74.04882836341858, 75.40412998199463, 76.75533485412598, 78.09786343574524, 79.42767238616943, 80.7740421295166, 82.1187596321106, 83.46992707252502, 84.81817150115967, 86.15159702301025, 87.48340034484863, 88.83237409591675, 90.19828629493713, 91.54734778404236, 92.88613176345825, 94.21552777290344, 95.56502938270569, 96.93238401412964, 98.15276718139648, 99.34991717338562, 100.54807567596436, 101.75647473335266, 102.9678966999054, 104.17409920692444, 105.36806917190552, 106.57961893081665, 107.78840970993042, 108.98509931564331, 110.19035029411316, 111.38345694541931, 112.59585618972778, 113.81197357177734, 115.01090669631958, 116.21316289901733, 117.41189074516296, 118.61345338821411, 119.9500298500061, 121.27137184143066, 122.59047555923462, 123.94106101989746, 125.28176641464233, 126.62333798408508, 127.96091437339783, 129.28216934204102, 130.5986363887787, 131.95044684410095, 133.99141788482666]
[17.87, 20.505, 23.2275, 25.105, 26.47, 28.2475, 29.2825, 29.9375, 30.7175, 30.8025, 33.24, 33.0925, 32.8625, 34.4325, 34.655, 35.1975, 35.67, 35.76, 36.3375, 37.335, 37.1075, 38.25, 37.8, 37.155, 37.7325, 37.86, 38.295, 38.455, 38.375, 39.045, 39.1625, 39.3, 39.2425, 39.4625, 39.785, 39.625, 39.495, 39.6075, 40.1675, 40.0975, 40.3375, 39.72, 40.1325, 40.2075, 40.4, 40.17, 39.7575, 40.01, 40.2825, 40.21, 40.2625, 40.47, 40.6275, 40.9175, 40.7225, 41.1375, 41.49, 41.09, 40.53, 41.345, 41.1875, 40.89, 40.4825, 41.2, 42.08, 40.92, 41.055, 40.88, 41.1275, 40.9625, 40.7925, 40.5775, 40.1925, 41.05, 41.145, 41.505, 41.1925, 40.785, 41.055, 40.965, 40.9125, 41.0125, 41.665, 40.455, 41.1375, 40.8175, 40.11, 40.7675, 40.6525, 40.835, 40.96, 40.1875, 40.44, 40.15, 40.585, 39.815, 40.115, 40.4325, 39.745, 39.32, 39.1025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8575
Client 2, noise level: 0.3194 (0.2875), real noise ratio: 0.4680
Client 3, noise level: 0.9178 (0.8260), real noise ratio: 0.8335
Client 4, noise level: 0.0319 (0.0287), real noise ratio: 0.0815
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5695
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8315
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0930
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7060
Client 13, noise level: 0.5093 (0.4583), real noise ratio: 0.5440
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8595
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8760
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5810
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8730
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5155
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7605
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8430
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.6130
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7860
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7285
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8895
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.206, Test loss: 2.236, Test accuracy: 19.65 

Round   0, Global train loss: 2.206, Global test loss: 2.218, Global test accuracy: 20.93 

Round   1, Train loss: 2.201, Test loss: 2.215, Test accuracy: 21.46 

Round   1, Global train loss: 2.201, Global test loss: 2.192, Global test accuracy: 25.04 

Round   2, Train loss: 2.036, Test loss: 2.142, Test accuracy: 22.49 

Round   2, Global train loss: 2.036, Global test loss: 2.017, Global test accuracy: 28.15 

Round   3, Train loss: 2.118, Test loss: 2.169, Test accuracy: 21.90 

Round   3, Global train loss: 2.118, Global test loss: 2.073, Global test accuracy: 27.76 

Round   4, Train loss: 2.025, Test loss: 2.129, Test accuracy: 23.07 

Round   4, Global train loss: 2.025, Global test loss: 1.958, Global test accuracy: 29.16 

Round   5, Train loss: 2.176, Test loss: 2.154, Test accuracy: 23.16 

Round   5, Global train loss: 2.176, Global test loss: 2.182, Global test accuracy: 29.56 

Round   6, Train loss: 2.043, Test loss: 2.131, Test accuracy: 23.61 

Round   6, Global train loss: 2.043, Global test loss: 2.053, Global test accuracy: 32.90 

Round   7, Train loss: 1.916, Test loss: 2.132, Test accuracy: 23.44 

Round   7, Global train loss: 1.916, Global test loss: 1.955, Global test accuracy: 31.14 

Round   8, Train loss: 1.914, Test loss: 2.120, Test accuracy: 24.00 

Round   8, Global train loss: 1.914, Global test loss: 1.954, Global test accuracy: 32.00 

Round   9, Train loss: 1.877, Test loss: 2.113, Test accuracy: 24.21 

Round   9, Global train loss: 1.877, Global test loss: 1.909, Global test accuracy: 35.20 

Round  10, Train loss: 1.909, Test loss: 2.117, Test accuracy: 24.38 

Round  10, Global train loss: 1.909, Global test loss: 1.982, Global test accuracy: 33.51 

Round  11, Train loss: 1.820, Test loss: 2.118, Test accuracy: 24.79 

Round  11, Global train loss: 1.820, Global test loss: 1.911, Global test accuracy: 35.08 

Round  12, Train loss: 1.769, Test loss: 2.116, Test accuracy: 24.86 

Round  12, Global train loss: 1.769, Global test loss: 1.796, Global test accuracy: 39.17 

Round  13, Train loss: 1.838, Test loss: 2.117, Test accuracy: 25.19 

Round  13, Global train loss: 1.838, Global test loss: 1.932, Global test accuracy: 34.87 

Round  14, Train loss: 1.920, Test loss: 2.130, Test accuracy: 25.20 

Round  14, Global train loss: 1.920, Global test loss: 1.989, Global test accuracy: 34.00 

Round  15, Train loss: 1.781, Test loss: 2.136, Test accuracy: 25.03 

Round  15, Global train loss: 1.781, Global test loss: 1.872, Global test accuracy: 37.09 

Round  16, Train loss: 1.857, Test loss: 2.136, Test accuracy: 25.33 

Round  16, Global train loss: 1.857, Global test loss: 2.011, Global test accuracy: 33.55 

Round  17, Train loss: 1.867, Test loss: 2.147, Test accuracy: 25.45 

Round  17, Global train loss: 1.867, Global test loss: 2.091, Global test accuracy: 28.34 

Round  18, Train loss: 1.762, Test loss: 2.176, Test accuracy: 25.11 

Round  18, Global train loss: 1.762, Global test loss: 1.792, Global test accuracy: 39.42 

Round  19, Train loss: 1.776, Test loss: 2.186, Test accuracy: 25.29 

Round  19, Global train loss: 1.776, Global test loss: 2.032, Global test accuracy: 31.21 

Round  20, Train loss: 1.732, Test loss: 2.178, Test accuracy: 25.48 

Round  20, Global train loss: 1.732, Global test loss: 1.936, Global test accuracy: 32.77 

Round  21, Train loss: 1.795, Test loss: 2.185, Test accuracy: 25.51 

Round  21, Global train loss: 1.795, Global test loss: 1.892, Global test accuracy: 35.72 

Round  22, Train loss: 1.757, Test loss: 2.207, Test accuracy: 25.14 

Round  22, Global train loss: 1.757, Global test loss: 1.937, Global test accuracy: 37.13 

Round  23, Train loss: 1.343, Test loss: 2.238, Test accuracy: 25.25 

Round  23, Global train loss: 1.343, Global test loss: 1.721, Global test accuracy: 38.61 

Round  24, Train loss: 1.731, Test loss: 2.263, Test accuracy: 25.30 

Round  24, Global train loss: 1.731, Global test loss: 1.981, Global test accuracy: 33.54 

Round  25, Train loss: 1.475, Test loss: 2.251, Test accuracy: 25.39 

Round  25, Global train loss: 1.475, Global test loss: 1.780, Global test accuracy: 36.66 

Round  26, Train loss: 1.717, Test loss: 2.273, Test accuracy: 25.15 

Round  26, Global train loss: 1.717, Global test loss: 1.963, Global test accuracy: 32.84 

Round  27, Train loss: 1.411, Test loss: 2.314, Test accuracy: 25.07 

Round  27, Global train loss: 1.411, Global test loss: 1.886, Global test accuracy: 32.60 

Round  28, Train loss: 1.186, Test loss: 2.336, Test accuracy: 25.23 

Round  28, Global train loss: 1.186, Global test loss: 1.615, Global test accuracy: 42.20 

Round  29, Train loss: 1.387, Test loss: 2.374, Test accuracy: 25.21 

Round  29, Global train loss: 1.387, Global test loss: 1.896, Global test accuracy: 35.31 

Round  30, Train loss: 1.613, Test loss: 2.391, Test accuracy: 25.02 

Round  30, Global train loss: 1.613, Global test loss: 2.067, Global test accuracy: 31.56 

Round  31, Train loss: 1.607, Test loss: 2.420, Test accuracy: 25.16 

Round  31, Global train loss: 1.607, Global test loss: 1.995, Global test accuracy: 32.79 

Round  32, Train loss: 1.317, Test loss: 2.473, Test accuracy: 25.09 

Round  32, Global train loss: 1.317, Global test loss: 1.850, Global test accuracy: 34.35 

Round  33, Train loss: 1.129, Test loss: 2.510, Test accuracy: 25.00 

Round  33, Global train loss: 1.129, Global test loss: 1.699, Global test accuracy: 39.98 

Round  34, Train loss: 1.355, Test loss: 2.525, Test accuracy: 25.12 

Round  34, Global train loss: 1.355, Global test loss: 1.865, Global test accuracy: 33.54 

Round  35, Train loss: 1.287, Test loss: 2.571, Test accuracy: 25.17 

Round  35, Global train loss: 1.287, Global test loss: 1.845, Global test accuracy: 35.52 

Round  36, Train loss: 1.043, Test loss: 2.627, Test accuracy: 24.98 

Round  36, Global train loss: 1.043, Global test loss: 1.807, Global test accuracy: 35.48 

Round  37, Train loss: 1.323, Test loss: 2.684, Test accuracy: 24.97 

Round  37, Global train loss: 1.323, Global test loss: 1.821, Global test accuracy: 36.49 

Round  38, Train loss: 1.031, Test loss: 2.725, Test accuracy: 24.62 

Round  38, Global train loss: 1.031, Global test loss: 1.864, Global test accuracy: 34.36 

Round  39, Train loss: 1.151, Test loss: 2.781, Test accuracy: 24.58 

Round  39, Global train loss: 1.151, Global test loss: 1.893, Global test accuracy: 37.80 

Round  40, Train loss: 1.598, Test loss: 2.821, Test accuracy: 24.42 

Round  40, Global train loss: 1.598, Global test loss: 2.044, Global test accuracy: 30.89 

Round  41, Train loss: 0.898, Test loss: 2.829, Test accuracy: 24.52 

Round  41, Global train loss: 0.898, Global test loss: 1.728, Global test accuracy: 38.92 

Round  42, Train loss: 1.082, Test loss: 2.889, Test accuracy: 24.57 

Round  42, Global train loss: 1.082, Global test loss: 1.818, Global test accuracy: 37.31 

Round  43, Train loss: 1.127, Test loss: 2.930, Test accuracy: 24.72 

Round  43, Global train loss: 1.127, Global test loss: 1.811, Global test accuracy: 36.56 

Round  44, Train loss: 1.185, Test loss: 2.994, Test accuracy: 24.75 

Round  44, Global train loss: 1.185, Global test loss: 1.937, Global test accuracy: 32.01 

Round  45, Train loss: 0.803, Test loss: 3.066, Test accuracy: 24.11 

Round  45, Global train loss: 0.803, Global test loss: 1.666, Global test accuracy: 41.46 

Round  46, Train loss: 1.006, Test loss: 3.118, Test accuracy: 24.06 

Round  46, Global train loss: 1.006, Global test loss: 1.807, Global test accuracy: 36.79 

Round  47, Train loss: 1.014, Test loss: 3.129, Test accuracy: 24.18 

Round  47, Global train loss: 1.014, Global test loss: 1.884, Global test accuracy: 35.66 

Round  48, Train loss: 0.982, Test loss: 3.190, Test accuracy: 24.09 

Round  48, Global train loss: 0.982, Global test loss: 1.847, Global test accuracy: 35.62 

Round  49, Train loss: 1.133, Test loss: 3.237, Test accuracy: 24.24 

Round  49, Global train loss: 1.133, Global test loss: 2.008, Global test accuracy: 32.32 

Round  50, Train loss: 1.265, Test loss: 3.254, Test accuracy: 24.23 

Round  50, Global train loss: 1.265, Global test loss: 2.121, Global test accuracy: 25.31 

Round  51, Train loss: 0.832, Test loss: 3.347, Test accuracy: 24.09 

Round  51, Global train loss: 0.832, Global test loss: 1.836, Global test accuracy: 36.01 

Round  52, Train loss: 0.951, Test loss: 3.385, Test accuracy: 24.11 

Round  52, Global train loss: 0.951, Global test loss: 1.928, Global test accuracy: 34.66 

Round  53, Train loss: 0.817, Test loss: 3.459, Test accuracy: 23.91 

Round  53, Global train loss: 0.817, Global test loss: 1.808, Global test accuracy: 37.70 

Round  54, Train loss: 0.891, Test loss: 3.559, Test accuracy: 23.63 

Round  54, Global train loss: 0.891, Global test loss: 1.775, Global test accuracy: 38.03 

Round  55, Train loss: 1.068, Test loss: 3.657, Test accuracy: 23.50 

Round  55, Global train loss: 1.068, Global test loss: 1.968, Global test accuracy: 33.16 

Round  56, Train loss: 0.712, Test loss: 3.720, Test accuracy: 23.49 

Round  56, Global train loss: 0.712, Global test loss: 1.825, Global test accuracy: 36.29 

Round  57, Train loss: 0.794, Test loss: 3.755, Test accuracy: 23.98 

Round  57, Global train loss: 0.794, Global test loss: 1.956, Global test accuracy: 32.19 

Round  58, Train loss: 0.719, Test loss: 3.865, Test accuracy: 24.16 

Round  58, Global train loss: 0.719, Global test loss: 1.947, Global test accuracy: 32.99 

Round  59, Train loss: 0.690, Test loss: 3.925, Test accuracy: 23.87 

Round  59, Global train loss: 0.690, Global test loss: 1.892, Global test accuracy: 33.95 

Round  60, Train loss: 0.840, Test loss: 3.983, Test accuracy: 23.93 

Round  60, Global train loss: 0.840, Global test loss: 1.908, Global test accuracy: 34.59 

Round  61, Train loss: 0.612, Test loss: 3.992, Test accuracy: 24.02 

Round  61, Global train loss: 0.612, Global test loss: 1.862, Global test accuracy: 35.63 

Round  62, Train loss: 0.712, Test loss: 4.069, Test accuracy: 24.13 

Round  62, Global train loss: 0.712, Global test loss: 1.775, Global test accuracy: 38.30 

Round  63, Train loss: 0.821, Test loss: 4.113, Test accuracy: 24.16 

Round  63, Global train loss: 0.821, Global test loss: 2.045, Global test accuracy: 27.36 

Round  64, Train loss: 0.707, Test loss: 4.189, Test accuracy: 24.06 

Round  64, Global train loss: 0.707, Global test loss: 1.931, Global test accuracy: 32.33 

Round  65, Train loss: 0.512, Test loss: 4.250, Test accuracy: 24.14 

Round  65, Global train loss: 0.512, Global test loss: 1.867, Global test accuracy: 34.48 

Round  66, Train loss: 0.486, Test loss: 4.354, Test accuracy: 24.12 

Round  66, Global train loss: 0.486, Global test loss: 1.843, Global test accuracy: 35.97 

Round  67, Train loss: 0.708, Test loss: 4.390, Test accuracy: 24.09 

Round  67, Global train loss: 0.708, Global test loss: 2.001, Global test accuracy: 30.14 

Round  68, Train loss: 0.500, Test loss: 4.421, Test accuracy: 23.84 

Round  68, Global train loss: 0.500, Global test loss: 1.811, Global test accuracy: 36.06 

Round  69, Train loss: 0.479, Test loss: 4.565, Test accuracy: 23.79 

Round  69, Global train loss: 0.479, Global test loss: 1.849, Global test accuracy: 34.31 

Round  70, Train loss: 0.701, Test loss: 4.574, Test accuracy: 23.98 

Round  70, Global train loss: 0.701, Global test loss: 1.989, Global test accuracy: 29.60 

Round  71, Train loss: 0.424, Test loss: 4.608, Test accuracy: 24.14 

Round  71, Global train loss: 0.424, Global test loss: 1.836, Global test accuracy: 36.51 

Round  72, Train loss: 0.607, Test loss: 4.622, Test accuracy: 24.33 

Round  72, Global train loss: 0.607, Global test loss: 1.996, Global test accuracy: 29.16 

Round  73, Train loss: 0.388, Test loss: 4.624, Test accuracy: 24.18 

Round  73, Global train loss: 0.388, Global test loss: 1.950, Global test accuracy: 33.29 

Round  74, Train loss: 0.457, Test loss: 4.748, Test accuracy: 24.06 

Round  74, Global train loss: 0.457, Global test loss: 1.940, Global test accuracy: 32.01 

Round  75, Train loss: 0.576, Test loss: 4.836, Test accuracy: 23.94 

Round  75, Global train loss: 0.576, Global test loss: 2.030, Global test accuracy: 27.53 

Round  76, Train loss: 0.353, Test loss: 4.872, Test accuracy: 24.00 

Round  76, Global train loss: 0.353, Global test loss: 1.729, Global test accuracy: 38.18 

Round  77, Train loss: 0.581, Test loss: 4.971, Test accuracy: 24.14 

Round  77, Global train loss: 0.581, Global test loss: 1.940, Global test accuracy: 30.96 

Round  78, Train loss: 0.555, Test loss: 4.952, Test accuracy: 24.31 

Round  78, Global train loss: 0.555, Global test loss: 2.037, Global test accuracy: 26.97 

Round  79, Train loss: 0.482, Test loss: 4.980, Test accuracy: 24.18 

Round  79, Global train loss: 0.482, Global test loss: 1.877, Global test accuracy: 35.01 

Round  80, Train loss: 0.453, Test loss: 5.022, Test accuracy: 24.46 

Round  80, Global train loss: 0.453, Global test loss: 1.792, Global test accuracy: 35.40 

Round  81, Train loss: 0.258, Test loss: 5.022, Test accuracy: 24.56 

Round  81, Global train loss: 0.258, Global test loss: 1.778, Global test accuracy: 38.01 

Round  82, Train loss: 0.369, Test loss: 5.113, Test accuracy: 24.54 

Round  82, Global train loss: 0.369, Global test loss: 1.877, Global test accuracy: 33.69 

Round  83, Train loss: 0.324, Test loss: 5.205, Test accuracy: 24.35 

Round  83, Global train loss: 0.324, Global test loss: 1.866, Global test accuracy: 34.17 

Round  84, Train loss: 0.513, Test loss: 5.278, Test accuracy: 24.46 

Round  84, Global train loss: 0.513, Global test loss: 1.947, Global test accuracy: 31.07 

Round  85, Train loss: 0.428, Test loss: 5.363, Test accuracy: 24.45 

Round  85, Global train loss: 0.428, Global test loss: 1.946, Global test accuracy: 31.37 

Round  86, Train loss: 0.405, Test loss: 5.424, Test accuracy: 24.09 

Round  86, Global train loss: 0.405, Global test loss: 1.880, Global test accuracy: 33.06 

Round  87, Train loss: 0.404, Test loss: 5.530, Test accuracy: 24.10 

Round  87, Global train loss: 0.404, Global test loss: 1.944, Global test accuracy: 31.79 

Round  88, Train loss: 0.239, Test loss: 5.575, Test accuracy: 24.14 

Round  88, Global train loss: 0.239, Global test loss: 1.748, Global test accuracy: 39.74 

Round  89, Train loss: 0.293, Test loss: 5.563, Test accuracy: 24.13 

Round  89, Global train loss: 0.293, Global test loss: 1.977, Global test accuracy: 30.36 

Round  90, Train loss: 0.387, Test loss: 5.573, Test accuracy: 24.23 

Round  90, Global train loss: 0.387, Global test loss: 1.925, Global test accuracy: 30.25 

Round  91, Train loss: 0.353, Test loss: 5.650, Test accuracy: 24.12 

Round  91, Global train loss: 0.353, Global test loss: 1.995, Global test accuracy: 29.70 

Round  92, Train loss: 0.306, Test loss: 5.655, Test accuracy: 23.94 

Round  92, Global train loss: 0.306, Global test loss: 1.927, Global test accuracy: 31.41 

Round  93, Train loss: 0.323, Test loss: 5.619, Test accuracy: 24.24 

Round  93, Global train loss: 0.323, Global test loss: 1.831, Global test accuracy: 35.38 

Round  94, Train loss: 0.378, Test loss: 5.617, Test accuracy: 24.40 

Round  94, Global train loss: 0.378, Global test loss: 2.089, Global test accuracy: 22.37 

Round  95, Train loss: 0.303, Test loss: 5.739, Test accuracy: 24.25 

Round  95, Global train loss: 0.303, Global test loss: 1.882, Global test accuracy: 34.76 

Round  96, Train loss: 0.334, Test loss: 5.818, Test accuracy: 24.28 

Round  96, Global train loss: 0.334, Global test loss: 1.992, Global test accuracy: 29.36 

Round  97, Train loss: 0.372, Test loss: 5.853, Test accuracy: 24.23 

Round  97, Global train loss: 0.372, Global test loss: 2.059, Global test accuracy: 28.34 

Round  98, Train loss: 0.337, Test loss: 5.956, Test accuracy: 24.29 

Round  98, Global train loss: 0.337, Global test loss: 2.018, Global test accuracy: 27.91 

Round  99, Train loss: 0.259, Test loss: 6.055, Test accuracy: 24.21 

Round  99, Global train loss: 0.259, Global test loss: 2.004, Global test accuracy: 30.08 

Final Round, Train loss: 0.306, Test loss: 6.172, Test accuracy: 23.78 

Final Round, Global train loss: 0.306, Global test loss: 2.004, Global test accuracy: 30.08 

Average accuracy final 10 rounds: 24.2185 

Average global accuracy final 10 rounds: 29.95625 

2755.7198820114136
[1.4502027034759521, 2.663975954055786, 3.8740837574005127, 5.094020366668701, 6.300945043563843, 7.513774871826172, 8.722277164459229, 9.9246985912323, 11.129543542861938, 12.335002899169922, 13.537715196609497, 14.736268758773804, 15.939898014068604, 17.14767026901245, 18.340039253234863, 19.53866171836853, 20.738346576690674, 21.94153618812561, 23.135149002075195, 24.334784030914307, 25.536288022994995, 26.731714248657227, 27.930419921875, 29.1309757232666, 30.336440324783325, 31.551897764205933, 32.71939277648926, 33.903995513916016, 35.10586714744568, 36.31126165390015, 37.511592388153076, 38.71154546737671, 39.915764570236206, 41.12312889099121, 42.32567858695984, 43.50217270851135, 44.68296480178833, 45.87592959403992, 47.05963206291199, 48.25460338592529, 49.4475839138031, 50.629390478134155, 51.81696081161499, 53.01312184333801, 54.20010256767273, 55.39018750190735, 56.58320069313049, 57.78323793411255, 58.97070670127869, 60.0187714099884, 61.04616856575012, 62.06910800933838, 63.089674949645996, 64.11078262329102, 65.13579082489014, 66.15920782089233, 67.18153095245361, 68.20180177688599, 69.22156810760498, 70.2463538646698, 71.27123880386353, 72.29400658607483, 73.31634879112244, 74.33612656593323, 75.35706305503845, 76.37867331504822, 77.3995578289032, 78.4207124710083, 79.44623494148254, 80.46429228782654, 81.488285779953, 82.51065564155579, 83.53107023239136, 84.5586907863617, 85.5808048248291, 86.60223031044006, 87.62620830535889, 88.65178918838501, 89.67636704444885, 90.69880223274231, 91.72495746612549, 92.7516918182373, 93.77471232414246, 94.80113053321838, 95.82292938232422, 96.8510410785675, 97.87176752090454, 98.89248919487, 99.915944814682, 100.93676137924194, 101.95965576171875, 102.9852466583252, 104.00379371643066, 105.02753376960754, 106.04856467247009, 107.0714476108551, 108.09225487709045, 109.11314392089844, 110.14699625968933, 111.18348050117493, 113.27132415771484]
[19.6525, 21.4625, 22.49, 21.9025, 23.075, 23.155, 23.6075, 23.4375, 24.0025, 24.215, 24.38, 24.785, 24.8625, 25.19, 25.2025, 25.0275, 25.3275, 25.4475, 25.11, 25.2875, 25.485, 25.5125, 25.14, 25.25, 25.305, 25.395, 25.15, 25.075, 25.225, 25.2125, 25.015, 25.1625, 25.0925, 25.0025, 25.125, 25.1725, 24.9825, 24.9675, 24.6225, 24.5775, 24.4175, 24.515, 24.565, 24.7175, 24.7475, 24.1075, 24.0625, 24.185, 24.095, 24.24, 24.2275, 24.085, 24.115, 23.9075, 23.6325, 23.495, 23.4875, 23.9825, 24.165, 23.8675, 23.935, 24.0225, 24.13, 24.1575, 24.0575, 24.145, 24.1225, 24.0875, 23.8425, 23.7925, 23.9775, 24.1425, 24.33, 24.175, 24.0625, 23.94, 24.0, 24.1375, 24.3075, 24.18, 24.4575, 24.5625, 24.5425, 24.3475, 24.4575, 24.445, 24.09, 24.1, 24.145, 24.1275, 24.225, 24.1175, 23.9425, 24.2425, 24.4, 24.2475, 24.28, 24.225, 24.29, 24.215, 23.78]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8755
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6135
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8675
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5050
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7380
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8440
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5670
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7885
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7720
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8795
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.197, Test loss: 2.244, Test accuracy: 20.61 

Round   0, Global train loss: 2.197, Global test loss: 2.227, Global test accuracy: 20.77 

Round   1, Train loss: 2.083, Test loss: 2.152, Test accuracy: 24.14 

Round   1, Global train loss: 2.083, Global test loss: 2.064, Global test accuracy: 26.59 

Round   2, Train loss: 2.019, Test loss: 2.093, Test accuracy: 26.56 

Round   2, Global train loss: 2.019, Global test loss: 1.937, Global test accuracy: 32.59 

Round   3, Train loss: 1.982, Test loss: 2.078, Test accuracy: 27.48 

Round   3, Global train loss: 1.982, Global test loss: 1.889, Global test accuracy: 34.95 

Round   4, Train loss: 1.969, Test loss: 2.049, Test accuracy: 28.31 

Round   4, Global train loss: 1.969, Global test loss: 1.829, Global test accuracy: 36.20 

Round   5, Train loss: 1.918, Test loss: 2.012, Test accuracy: 29.84 

Round   5, Global train loss: 1.918, Global test loss: 1.764, Global test accuracy: 38.16 

Round   6, Train loss: 1.904, Test loss: 1.990, Test accuracy: 30.25 

Round   6, Global train loss: 1.904, Global test loss: 1.739, Global test accuracy: 41.75 

Round   7, Train loss: 1.892, Test loss: 1.962, Test accuracy: 31.43 

Round   7, Global train loss: 1.892, Global test loss: 1.731, Global test accuracy: 40.81 

Round   8, Train loss: 1.794, Test loss: 1.908, Test accuracy: 33.25 

Round   8, Global train loss: 1.794, Global test loss: 1.609, Global test accuracy: 44.02 

Round   9, Train loss: 1.728, Test loss: 1.877, Test accuracy: 34.36 

Round   9, Global train loss: 1.728, Global test loss: 1.588, Global test accuracy: 43.08 

Round  10, Train loss: 1.835, Test loss: 1.859, Test accuracy: 35.18 

Round  10, Global train loss: 1.835, Global test loss: 1.606, Global test accuracy: 45.77 

Round  11, Train loss: 1.713, Test loss: 1.837, Test accuracy: 36.43 

Round  11, Global train loss: 1.713, Global test loss: 1.533, Global test accuracy: 48.01 

Round  12, Train loss: 1.737, Test loss: 1.833, Test accuracy: 36.63 

Round  12, Global train loss: 1.737, Global test loss: 1.571, Global test accuracy: 47.22 

Round  13, Train loss: 1.655, Test loss: 1.814, Test accuracy: 37.26 

Round  13, Global train loss: 1.655, Global test loss: 1.514, Global test accuracy: 46.74 

Round  14, Train loss: 1.703, Test loss: 1.791, Test accuracy: 38.07 

Round  14, Global train loss: 1.703, Global test loss: 1.506, Global test accuracy: 49.63 

Round  15, Train loss: 1.756, Test loss: 1.782, Test accuracy: 38.69 

Round  15, Global train loss: 1.756, Global test loss: 1.573, Global test accuracy: 49.49 

Round  16, Train loss: 1.613, Test loss: 1.770, Test accuracy: 39.10 

Round  16, Global train loss: 1.613, Global test loss: 1.436, Global test accuracy: 51.41 

Round  17, Train loss: 1.633, Test loss: 1.758, Test accuracy: 39.81 

Round  17, Global train loss: 1.633, Global test loss: 1.475, Global test accuracy: 50.67 

Round  18, Train loss: 1.629, Test loss: 1.762, Test accuracy: 39.71 

Round  18, Global train loss: 1.629, Global test loss: 1.468, Global test accuracy: 50.35 

Round  19, Train loss: 1.656, Test loss: 1.754, Test accuracy: 40.21 

Round  19, Global train loss: 1.656, Global test loss: 1.494, Global test accuracy: 49.89 

Round  20, Train loss: 1.701, Test loss: 1.759, Test accuracy: 40.32 

Round  20, Global train loss: 1.701, Global test loss: 1.502, Global test accuracy: 52.45 

Round  21, Train loss: 1.580, Test loss: 1.759, Test accuracy: 40.70 

Round  21, Global train loss: 1.580, Global test loss: 1.408, Global test accuracy: 53.96 

Round  22, Train loss: 1.580, Test loss: 1.758, Test accuracy: 40.77 

Round  22, Global train loss: 1.580, Global test loss: 1.437, Global test accuracy: 52.50 

Round  23, Train loss: 1.546, Test loss: 1.768, Test accuracy: 40.95 

Round  23, Global train loss: 1.546, Global test loss: 1.377, Global test accuracy: 54.25 

Round  24, Train loss: 1.509, Test loss: 1.753, Test accuracy: 41.45 

Round  24, Global train loss: 1.509, Global test loss: 1.411, Global test accuracy: 53.12 

Round  25, Train loss: 1.549, Test loss: 1.758, Test accuracy: 41.85 

Round  25, Global train loss: 1.549, Global test loss: 1.414, Global test accuracy: 53.35 

Round  26, Train loss: 1.494, Test loss: 1.746, Test accuracy: 42.44 

Round  26, Global train loss: 1.494, Global test loss: 1.411, Global test accuracy: 54.17 

Round  27, Train loss: 1.583, Test loss: 1.741, Test accuracy: 42.55 

Round  27, Global train loss: 1.583, Global test loss: 1.436, Global test accuracy: 55.20 

Round  28, Train loss: 1.519, Test loss: 1.750, Test accuracy: 42.53 

Round  28, Global train loss: 1.519, Global test loss: 1.369, Global test accuracy: 55.23 

Round  29, Train loss: 1.434, Test loss: 1.753, Test accuracy: 42.27 

Round  29, Global train loss: 1.434, Global test loss: 1.403, Global test accuracy: 54.11 

Round  30, Train loss: 1.425, Test loss: 1.779, Test accuracy: 42.23 

Round  30, Global train loss: 1.425, Global test loss: 1.416, Global test accuracy: 53.02 

Round  31, Train loss: 1.407, Test loss: 1.796, Test accuracy: 41.75 

Round  31, Global train loss: 1.407, Global test loss: 1.359, Global test accuracy: 56.18 

Round  32, Train loss: 1.416, Test loss: 1.800, Test accuracy: 41.80 

Round  32, Global train loss: 1.416, Global test loss: 1.409, Global test accuracy: 54.19 

Round  33, Train loss: 1.373, Test loss: 1.827, Test accuracy: 41.49 

Round  33, Global train loss: 1.373, Global test loss: 1.365, Global test accuracy: 54.98 

Round  34, Train loss: 1.457, Test loss: 1.811, Test accuracy: 42.23 

Round  34, Global train loss: 1.457, Global test loss: 1.406, Global test accuracy: 53.23 

Round  35, Train loss: 1.447, Test loss: 1.811, Test accuracy: 42.66 

Round  35, Global train loss: 1.447, Global test loss: 1.372, Global test accuracy: 56.41 

Round  36, Train loss: 1.356, Test loss: 1.825, Test accuracy: 42.50 

Round  36, Global train loss: 1.356, Global test loss: 1.389, Global test accuracy: 55.11 

Round  37, Train loss: 1.314, Test loss: 1.835, Test accuracy: 42.72 

Round  37, Global train loss: 1.314, Global test loss: 1.377, Global test accuracy: 54.79 

Round  38, Train loss: 1.370, Test loss: 1.841, Test accuracy: 42.82 

Round  38, Global train loss: 1.370, Global test loss: 1.421, Global test accuracy: 54.07 

Round  39, Train loss: 1.248, Test loss: 1.833, Test accuracy: 42.95 

Round  39, Global train loss: 1.248, Global test loss: 1.364, Global test accuracy: 55.76 

Round  40, Train loss: 1.319, Test loss: 1.860, Test accuracy: 42.81 

Round  40, Global train loss: 1.319, Global test loss: 1.297, Global test accuracy: 57.03 

Round  41, Train loss: 1.247, Test loss: 1.871, Test accuracy: 42.95 

Round  41, Global train loss: 1.247, Global test loss: 1.382, Global test accuracy: 54.94 

Round  42, Train loss: 1.212, Test loss: 1.896, Test accuracy: 42.96 

Round  42, Global train loss: 1.212, Global test loss: 1.296, Global test accuracy: 56.77 

Round  43, Train loss: 1.330, Test loss: 1.887, Test accuracy: 43.11 

Round  43, Global train loss: 1.330, Global test loss: 1.353, Global test accuracy: 55.95 

Round  44, Train loss: 1.156, Test loss: 1.887, Test accuracy: 43.14 

Round  44, Global train loss: 1.156, Global test loss: 1.309, Global test accuracy: 56.27 

Round  45, Train loss: 1.279, Test loss: 1.893, Test accuracy: 43.10 

Round  45, Global train loss: 1.279, Global test loss: 1.392, Global test accuracy: 54.59 

Round  46, Train loss: 1.282, Test loss: 1.905, Test accuracy: 43.22 

Round  46, Global train loss: 1.282, Global test loss: 1.432, Global test accuracy: 53.37 

Round  47, Train loss: 1.082, Test loss: 1.942, Test accuracy: 43.21 

Round  47, Global train loss: 1.082, Global test loss: 1.299, Global test accuracy: 57.65 

Round  48, Train loss: 1.306, Test loss: 1.947, Test accuracy: 43.28 

Round  48, Global train loss: 1.306, Global test loss: 1.345, Global test accuracy: 56.35 

Round  49, Train loss: 1.177, Test loss: 1.929, Test accuracy: 43.69 

Round  49, Global train loss: 1.177, Global test loss: 1.367, Global test accuracy: 55.38 

Round  50, Train loss: 1.168, Test loss: 1.943, Test accuracy: 43.45 

Round  50, Global train loss: 1.168, Global test loss: 1.395, Global test accuracy: 53.94 

Round  51, Train loss: 1.164, Test loss: 1.952, Test accuracy: 43.42 

Round  51, Global train loss: 1.164, Global test loss: 1.352, Global test accuracy: 55.80 

Round  52, Train loss: 1.116, Test loss: 1.965, Test accuracy: 43.47 

Round  52, Global train loss: 1.116, Global test loss: 1.354, Global test accuracy: 56.31 

Round  53, Train loss: 1.161, Test loss: 1.975, Test accuracy: 43.51 

Round  53, Global train loss: 1.161, Global test loss: 1.433, Global test accuracy: 53.27 

Round  54, Train loss: 1.140, Test loss: 1.980, Test accuracy: 43.80 

Round  54, Global train loss: 1.140, Global test loss: 1.324, Global test accuracy: 56.93 

Round  55, Train loss: 1.032, Test loss: 2.019, Test accuracy: 43.17 

Round  55, Global train loss: 1.032, Global test loss: 1.343, Global test accuracy: 56.80 

Round  56, Train loss: 1.162, Test loss: 2.028, Test accuracy: 43.28 

Round  56, Global train loss: 1.162, Global test loss: 1.408, Global test accuracy: 54.84 

Round  57, Train loss: 1.101, Test loss: 2.030, Test accuracy: 43.27 

Round  57, Global train loss: 1.101, Global test loss: 1.379, Global test accuracy: 55.84 

Round  58, Train loss: 0.982, Test loss: 2.051, Test accuracy: 43.01 

Round  58, Global train loss: 0.982, Global test loss: 1.379, Global test accuracy: 56.36 

Round  59, Train loss: 1.012, Test loss: 2.038, Test accuracy: 43.30 

Round  59, Global train loss: 1.012, Global test loss: 1.453, Global test accuracy: 53.68 

Round  60, Train loss: 1.096, Test loss: 2.071, Test accuracy: 42.97 

Round  60, Global train loss: 1.096, Global test loss: 1.456, Global test accuracy: 54.03 

Round  61, Train loss: 1.038, Test loss: 2.081, Test accuracy: 43.21 

Round  61, Global train loss: 1.038, Global test loss: 1.434, Global test accuracy: 54.63 

Round  62, Train loss: 1.026, Test loss: 2.097, Test accuracy: 42.98 

Round  62, Global train loss: 1.026, Global test loss: 1.378, Global test accuracy: 56.04 

Round  63, Train loss: 0.987, Test loss: 2.110, Test accuracy: 42.91 

Round  63, Global train loss: 0.987, Global test loss: 1.411, Global test accuracy: 55.89 

Round  64, Train loss: 0.956, Test loss: 2.113, Test accuracy: 43.19 

Round  64, Global train loss: 0.956, Global test loss: 1.320, Global test accuracy: 57.86 

Round  65, Train loss: 0.934, Test loss: 2.123, Test accuracy: 42.97 

Round  65, Global train loss: 0.934, Global test loss: 1.408, Global test accuracy: 55.81 

Round  66, Train loss: 0.992, Test loss: 2.123, Test accuracy: 42.96 

Round  66, Global train loss: 0.992, Global test loss: 1.473, Global test accuracy: 54.24 

Round  67, Train loss: 0.950, Test loss: 2.149, Test accuracy: 42.82 

Round  67, Global train loss: 0.950, Global test loss: 1.458, Global test accuracy: 55.79 

Round  68, Train loss: 1.061, Test loss: 2.148, Test accuracy: 43.24 

Round  68, Global train loss: 1.061, Global test loss: 1.445, Global test accuracy: 54.25 

Round  69, Train loss: 0.975, Test loss: 2.121, Test accuracy: 43.83 

Round  69, Global train loss: 0.975, Global test loss: 1.523, Global test accuracy: 52.75 

Round  70, Train loss: 0.887, Test loss: 2.145, Test accuracy: 43.51 

Round  70, Global train loss: 0.887, Global test loss: 1.414, Global test accuracy: 56.45 

Round  71, Train loss: 1.020, Test loss: 2.154, Test accuracy: 43.09 

Round  71, Global train loss: 1.020, Global test loss: 1.467, Global test accuracy: 53.51 

Round  72, Train loss: 0.895, Test loss: 2.144, Test accuracy: 43.27 

Round  72, Global train loss: 0.895, Global test loss: 1.400, Global test accuracy: 56.63 

Round  73, Train loss: 0.869, Test loss: 2.151, Test accuracy: 43.35 

Round  73, Global train loss: 0.869, Global test loss: 1.465, Global test accuracy: 54.87 

Round  74, Train loss: 0.980, Test loss: 2.168, Test accuracy: 43.52 

Round  74, Global train loss: 0.980, Global test loss: 1.455, Global test accuracy: 55.17 

Round  75, Train loss: 1.002, Test loss: 2.170, Test accuracy: 43.78 

Round  75, Global train loss: 1.002, Global test loss: 1.386, Global test accuracy: 56.31 

Round  76, Train loss: 0.966, Test loss: 2.169, Test accuracy: 44.13 

Round  76, Global train loss: 0.966, Global test loss: 1.419, Global test accuracy: 56.57 

Round  77, Train loss: 1.048, Test loss: 2.188, Test accuracy: 43.67 

Round  77, Global train loss: 1.048, Global test loss: 1.482, Global test accuracy: 53.66 

Round  78, Train loss: 0.956, Test loss: 2.206, Test accuracy: 43.62 

Round  78, Global train loss: 0.956, Global test loss: 1.407, Global test accuracy: 56.47 

Round  79, Train loss: 0.900, Test loss: 2.238, Test accuracy: 43.68 

Round  79, Global train loss: 0.900, Global test loss: 1.448, Global test accuracy: 55.84 

Round  80, Train loss: 0.999, Test loss: 2.233, Test accuracy: 43.69 

Round  80, Global train loss: 0.999, Global test loss: 1.488, Global test accuracy: 53.88 

Round  81, Train loss: 0.900, Test loss: 2.229, Test accuracy: 43.89 

Round  81, Global train loss: 0.900, Global test loss: 1.510, Global test accuracy: 54.06 

Round  82, Train loss: 0.802, Test loss: 2.252, Test accuracy: 43.98 

Round  82, Global train loss: 0.802, Global test loss: 1.455, Global test accuracy: 56.14 

Round  83, Train loss: 0.856, Test loss: 2.254, Test accuracy: 43.84 

Round  83, Global train loss: 0.856, Global test loss: 1.443, Global test accuracy: 56.36 

Round  84, Train loss: 0.882, Test loss: 2.269, Test accuracy: 43.90 

Round  84, Global train loss: 0.882, Global test loss: 1.444, Global test accuracy: 57.25 

Round  85, Train loss: 0.942, Test loss: 2.285, Test accuracy: 43.55 

Round  85, Global train loss: 0.942, Global test loss: 1.539, Global test accuracy: 52.99 

Round  86, Train loss: 0.913, Test loss: 2.254, Test accuracy: 44.10 

Round  86, Global train loss: 0.913, Global test loss: 1.423, Global test accuracy: 56.16 

Round  87, Train loss: 0.831, Test loss: 2.243, Test accuracy: 44.23 

Round  87, Global train loss: 0.831, Global test loss: 1.481, Global test accuracy: 55.37 

Round  88, Train loss: 0.876, Test loss: 2.274, Test accuracy: 43.84 

Round  88, Global train loss: 0.876, Global test loss: 1.436, Global test accuracy: 56.74 

Round  89, Train loss: 0.787, Test loss: 2.252, Test accuracy: 43.97 

Round  89, Global train loss: 0.787, Global test loss: 1.490, Global test accuracy: 56.26 

Round  90, Train loss: 0.944, Test loss: 2.297, Test accuracy: 43.97 

Round  90, Global train loss: 0.944, Global test loss: 1.514, Global test accuracy: 53.33 

Round  91, Train loss: 0.809, Test loss: 2.284, Test accuracy: 44.03 

Round  91, Global train loss: 0.809, Global test loss: 1.464, Global test accuracy: 56.78 

Round  92, Train loss: 0.869, Test loss: 2.309, Test accuracy: 43.58 

Round  92, Global train loss: 0.869, Global test loss: 1.598, Global test accuracy: 51.43 

Round  93, Train loss: 0.969, Test loss: 2.353, Test accuracy: 43.52 

Round  93, Global train loss: 0.969, Global test loss: 1.621, Global test accuracy: 51.38 

Round  94, Train loss: 0.846, Test loss: 2.366, Test accuracy: 43.23 

Round  94, Global train loss: 0.846, Global test loss: 1.465, Global test accuracy: 55.97 

Round  95, Train loss: 0.841, Test loss: 2.343, Test accuracy: 43.27 

Round  95, Global train loss: 0.841, Global test loss: 1.500, Global test accuracy: 55.76 

Round  96, Train loss: 0.844, Test loss: 2.344, Test accuracy: 43.83 

Round  96, Global train loss: 0.844, Global test loss: 1.546, Global test accuracy: 55.17 

Round  97, Train loss: 0.817, Test loss: 2.360, Test accuracy: 43.70 

Round  97, Global train loss: 0.817, Global test loss: 1.560, Global test accuracy: 54.07 

Round  98, Train loss: 0.900, Test loss: 2.376, Test accuracy: 43.64 

Round  98, Global train loss: 0.900, Global test loss: 1.541, Global test accuracy: 53.44 

Round  99, Train loss: 0.746, Test loss: 2.403, Test accuracy: 43.24 

Round  99, Global train loss: 0.746, Global test loss: 1.595, Global test accuracy: 54.12 

Final Round, Train loss: 0.631, Test loss: 2.861, Test accuracy: 42.52 

Final Round, Global train loss: 0.631, Global test loss: 1.595, Global test accuracy: 54.12 

Average accuracy final 10 rounds: 43.602 

Average global accuracy final 10 rounds: 54.1435 

2642.4882214069366
[1.411585807800293, 2.5443549156188965, 3.69756817817688, 4.8092687129974365, 5.989975690841675, 7.1603100299835205, 8.33556342124939, 9.512266635894775, 10.68325161933899, 11.851062536239624, 13.03273868560791, 14.139572858810425, 15.254784345626831, 16.323952198028564, 17.497588634490967, 18.624200582504272, 19.791483163833618, 20.957277536392212, 22.140297651290894, 23.323973178863525, 24.503819704055786, 25.51102066040039, 26.507662296295166, 27.514694690704346, 28.51487421989441, 29.525180339813232, 30.531617164611816, 31.541172742843628, 32.54834246635437, 33.54975938796997, 34.55009174346924, 35.559523820877075, 36.55933690071106, 37.56708073616028, 38.56545948982239, 39.571720361709595, 40.57297325134277, 41.57941150665283, 42.592352628707886, 43.603848695755005, 44.609583139419556, 45.61420178413391, 46.6153450012207, 47.61914420127869, 48.618297815322876, 49.627378702163696, 50.6242458820343, 51.63002610206604, 52.62728834152222, 53.63041543960571, 54.629722595214844, 55.62848734855652, 56.64857840538025, 57.64836001396179, 58.66403532028198, 59.66259527206421, 60.670878410339355, 61.67093110084534, 62.67974352836609, 63.69213628768921, 64.69763565063477, 65.70145869255066, 66.70918440818787, 67.71211099624634, 68.72260403633118, 69.7302360534668, 70.74734425544739, 71.75396084785461, 72.7641441822052, 73.78322172164917, 74.79868483543396, 75.80889630317688, 76.81286931037903, 77.81396818161011, 78.81568908691406, 79.82959198951721, 80.83175230026245, 81.85055708885193, 82.86267352104187, 83.87539625167847, 84.89032888412476, 85.91054654121399, 86.91592812538147, 87.91769886016846, 88.91868615150452, 89.92512011528015, 90.93121933937073, 91.93298983573914, 92.9340124130249, 93.93609189987183, 94.94208669662476, 95.95529794692993, 96.96944952011108, 97.98482584953308, 98.9899377822876, 99.9949746131897, 100.99585342407227, 101.99655771255493, 103.00309181213379, 104.00853514671326, 106.01709866523743]
[20.605, 24.14, 26.5625, 27.48, 28.3075, 29.845, 30.25, 31.4325, 33.2475, 34.3575, 35.18, 36.4325, 36.6275, 37.2575, 38.07, 38.6925, 39.1, 39.8125, 39.7075, 40.21, 40.3175, 40.7025, 40.775, 40.9525, 41.4525, 41.8475, 42.4425, 42.545, 42.53, 42.27, 42.225, 41.75, 41.805, 41.4875, 42.23, 42.665, 42.4975, 42.7175, 42.8175, 42.9525, 42.8075, 42.9475, 42.9625, 43.1075, 43.1375, 43.1025, 43.2175, 43.21, 43.285, 43.6875, 43.4475, 43.425, 43.4725, 43.505, 43.8, 43.17, 43.2825, 43.275, 43.005, 43.305, 42.965, 43.2075, 42.9825, 42.9125, 43.1875, 42.965, 42.9625, 42.82, 43.2375, 43.825, 43.5125, 43.085, 43.265, 43.355, 43.515, 43.78, 44.1275, 43.67, 43.615, 43.68, 43.685, 43.89, 43.98, 43.84, 43.9, 43.555, 44.105, 44.2275, 43.8375, 43.965, 43.97, 44.035, 43.5775, 43.5225, 43.2325, 43.27, 43.8275, 43.7, 43.6425, 43.2425, 42.515]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8850
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5940
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8600
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5320
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7485
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8655
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5090
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7770
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7260
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8940
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.210, Test loss: 2.230, Test accuracy: 21.85 

Round   0, Global train loss: 2.210, Global test loss: 2.220, Global test accuracy: 22.12 

Round   1, Train loss: 2.106, Test loss: 2.088, Test accuracy: 24.34 

Round   1, Global train loss: 2.106, Global test loss: 2.020, Global test accuracy: 25.26 

Round   2, Train loss: 1.996, Test loss: 2.040, Test accuracy: 26.67 

Round   2, Global train loss: 1.996, Global test loss: 1.903, Global test accuracy: 31.62 

Round   3, Train loss: 1.996, Test loss: 2.037, Test accuracy: 26.84 

Round   3, Global train loss: 1.996, Global test loss: 1.894, Global test accuracy: 30.71 

Round   4, Train loss: 1.909, Test loss: 1.998, Test accuracy: 28.32 

Round   4, Global train loss: 1.909, Global test loss: 1.782, Global test accuracy: 36.84 

Round   5, Train loss: 1.946, Test loss: 1.975, Test accuracy: 29.34 

Round   5, Global train loss: 1.946, Global test loss: 1.764, Global test accuracy: 37.78 

Round   6, Train loss: 1.877, Test loss: 1.946, Test accuracy: 30.39 

Round   6, Global train loss: 1.877, Global test loss: 1.716, Global test accuracy: 38.86 

Round   7, Train loss: 1.817, Test loss: 1.916, Test accuracy: 31.78 

Round   7, Global train loss: 1.817, Global test loss: 1.642, Global test accuracy: 42.25 

Round   8, Train loss: 1.831, Test loss: 1.881, Test accuracy: 33.57 

Round   8, Global train loss: 1.831, Global test loss: 1.642, Global test accuracy: 42.66 

Round   9, Train loss: 1.784, Test loss: 1.853, Test accuracy: 34.21 

Round   9, Global train loss: 1.784, Global test loss: 1.618, Global test accuracy: 43.43 

Round  10, Train loss: 1.768, Test loss: 1.825, Test accuracy: 35.45 

Round  10, Global train loss: 1.768, Global test loss: 1.579, Global test accuracy: 44.80 

Round  11, Train loss: 1.766, Test loss: 1.804, Test accuracy: 35.88 

Round  11, Global train loss: 1.766, Global test loss: 1.582, Global test accuracy: 46.64 

Round  12, Train loss: 1.724, Test loss: 1.798, Test accuracy: 35.86 

Round  12, Global train loss: 1.724, Global test loss: 1.540, Global test accuracy: 46.80 

Round  13, Train loss: 1.706, Test loss: 1.784, Test accuracy: 36.58 

Round  13, Global train loss: 1.706, Global test loss: 1.541, Global test accuracy: 45.47 

Round  14, Train loss: 1.827, Test loss: 1.775, Test accuracy: 36.96 

Round  14, Global train loss: 1.827, Global test loss: 1.615, Global test accuracy: 42.73 

Round  15, Train loss: 1.638, Test loss: 1.758, Test accuracy: 37.84 

Round  15, Global train loss: 1.638, Global test loss: 1.469, Global test accuracy: 48.87 

Round  16, Train loss: 1.678, Test loss: 1.748, Test accuracy: 38.53 

Round  16, Global train loss: 1.678, Global test loss: 1.483, Global test accuracy: 48.77 

Round  17, Train loss: 1.648, Test loss: 1.726, Test accuracy: 39.21 

Round  17, Global train loss: 1.648, Global test loss: 1.477, Global test accuracy: 49.17 

Round  18, Train loss: 1.784, Test loss: 1.711, Test accuracy: 39.94 

Round  18, Global train loss: 1.784, Global test loss: 1.555, Global test accuracy: 48.37 

Round  19, Train loss: 1.622, Test loss: 1.710, Test accuracy: 40.38 

Round  19, Global train loss: 1.622, Global test loss: 1.446, Global test accuracy: 50.60 

Round  20, Train loss: 1.599, Test loss: 1.700, Test accuracy: 40.72 

Round  20, Global train loss: 1.599, Global test loss: 1.427, Global test accuracy: 50.02 

Round  21, Train loss: 1.654, Test loss: 1.699, Test accuracy: 40.94 

Round  21, Global train loss: 1.654, Global test loss: 1.413, Global test accuracy: 53.77 

Round  22, Train loss: 1.651, Test loss: 1.698, Test accuracy: 41.10 

Round  22, Global train loss: 1.651, Global test loss: 1.430, Global test accuracy: 52.14 

Round  23, Train loss: 1.585, Test loss: 1.680, Test accuracy: 41.94 

Round  23, Global train loss: 1.585, Global test loss: 1.432, Global test accuracy: 51.08 

Round  24, Train loss: 1.624, Test loss: 1.678, Test accuracy: 41.79 

Round  24, Global train loss: 1.624, Global test loss: 1.426, Global test accuracy: 52.79 

Round  25, Train loss: 1.519, Test loss: 1.693, Test accuracy: 41.69 

Round  25, Global train loss: 1.519, Global test loss: 1.413, Global test accuracy: 52.66 

Round  26, Train loss: 1.546, Test loss: 1.681, Test accuracy: 42.06 

Round  26, Global train loss: 1.546, Global test loss: 1.387, Global test accuracy: 54.44 

Round  27, Train loss: 1.506, Test loss: 1.672, Test accuracy: 42.34 

Round  27, Global train loss: 1.506, Global test loss: 1.377, Global test accuracy: 54.17 

Round  28, Train loss: 1.498, Test loss: 1.668, Test accuracy: 42.51 

Round  28, Global train loss: 1.498, Global test loss: 1.390, Global test accuracy: 53.78 

Round  29, Train loss: 1.416, Test loss: 1.666, Test accuracy: 42.71 

Round  29, Global train loss: 1.416, Global test loss: 1.319, Global test accuracy: 55.22 

Round  30, Train loss: 1.495, Test loss: 1.659, Test accuracy: 42.90 

Round  30, Global train loss: 1.495, Global test loss: 1.340, Global test accuracy: 55.15 

Round  31, Train loss: 1.574, Test loss: 1.660, Test accuracy: 43.10 

Round  31, Global train loss: 1.574, Global test loss: 1.398, Global test accuracy: 53.92 

Round  32, Train loss: 1.428, Test loss: 1.685, Test accuracy: 42.54 

Round  32, Global train loss: 1.428, Global test loss: 1.350, Global test accuracy: 54.56 

Round  33, Train loss: 1.507, Test loss: 1.681, Test accuracy: 43.19 

Round  33, Global train loss: 1.507, Global test loss: 1.398, Global test accuracy: 53.78 

Round  34, Train loss: 1.457, Test loss: 1.674, Test accuracy: 43.41 

Round  34, Global train loss: 1.457, Global test loss: 1.367, Global test accuracy: 54.63 

Round  35, Train loss: 1.511, Test loss: 1.677, Test accuracy: 43.51 

Round  35, Global train loss: 1.511, Global test loss: 1.393, Global test accuracy: 54.16 

Round  36, Train loss: 1.427, Test loss: 1.674, Test accuracy: 43.76 

Round  36, Global train loss: 1.427, Global test loss: 1.369, Global test accuracy: 54.05 

Round  37, Train loss: 1.294, Test loss: 1.665, Test accuracy: 44.08 

Round  37, Global train loss: 1.294, Global test loss: 1.258, Global test accuracy: 56.94 

Round  38, Train loss: 1.261, Test loss: 1.676, Test accuracy: 43.72 

Round  38, Global train loss: 1.261, Global test loss: 1.289, Global test accuracy: 56.27 

Round  39, Train loss: 1.314, Test loss: 1.682, Test accuracy: 43.76 

Round  39, Global train loss: 1.314, Global test loss: 1.280, Global test accuracy: 56.91 

Round  40, Train loss: 1.376, Test loss: 1.678, Test accuracy: 44.38 

Round  40, Global train loss: 1.376, Global test loss: 1.268, Global test accuracy: 57.72 

Round  41, Train loss: 1.229, Test loss: 1.666, Test accuracy: 44.77 

Round  41, Global train loss: 1.229, Global test loss: 1.262, Global test accuracy: 57.39 

Round  42, Train loss: 1.332, Test loss: 1.682, Test accuracy: 44.45 

Round  42, Global train loss: 1.332, Global test loss: 1.337, Global test accuracy: 54.71 

Round  43, Train loss: 1.370, Test loss: 1.687, Test accuracy: 44.35 

Round  43, Global train loss: 1.370, Global test loss: 1.316, Global test accuracy: 55.49 

Round  44, Train loss: 1.297, Test loss: 1.688, Test accuracy: 44.64 

Round  44, Global train loss: 1.297, Global test loss: 1.271, Global test accuracy: 56.64 

Round  45, Train loss: 1.257, Test loss: 1.687, Test accuracy: 45.26 

Round  45, Global train loss: 1.257, Global test loss: 1.267, Global test accuracy: 57.62 

Round  46, Train loss: 1.245, Test loss: 1.704, Test accuracy: 45.08 

Round  46, Global train loss: 1.245, Global test loss: 1.279, Global test accuracy: 57.23 

Round  47, Train loss: 1.219, Test loss: 1.717, Test accuracy: 44.72 

Round  47, Global train loss: 1.219, Global test loss: 1.269, Global test accuracy: 57.48 

Round  48, Train loss: 1.341, Test loss: 1.735, Test accuracy: 44.59 

Round  48, Global train loss: 1.341, Global test loss: 1.311, Global test accuracy: 56.04 

Round  49, Train loss: 1.349, Test loss: 1.740, Test accuracy: 44.55 

Round  49, Global train loss: 1.349, Global test loss: 1.358, Global test accuracy: 54.97 

Round  50, Train loss: 1.296, Test loss: 1.720, Test accuracy: 44.97 

Round  50, Global train loss: 1.296, Global test loss: 1.316, Global test accuracy: 55.63 

Round  51, Train loss: 1.187, Test loss: 1.749, Test accuracy: 44.56 

Round  51, Global train loss: 1.187, Global test loss: 1.268, Global test accuracy: 57.43 

Round  52, Train loss: 1.363, Test loss: 1.739, Test accuracy: 45.17 

Round  52, Global train loss: 1.363, Global test loss: 1.397, Global test accuracy: 54.10 

Round  53, Train loss: 1.242, Test loss: 1.769, Test accuracy: 44.72 

Round  53, Global train loss: 1.242, Global test loss: 1.311, Global test accuracy: 56.82 

Round  54, Train loss: 1.291, Test loss: 1.770, Test accuracy: 44.71 

Round  54, Global train loss: 1.291, Global test loss: 1.304, Global test accuracy: 57.07 

Round  55, Train loss: 1.349, Test loss: 1.761, Test accuracy: 44.93 

Round  55, Global train loss: 1.349, Global test loss: 1.379, Global test accuracy: 54.83 

Round  56, Train loss: 1.328, Test loss: 1.785, Test accuracy: 44.41 

Round  56, Global train loss: 1.328, Global test loss: 1.411, Global test accuracy: 53.14 

Round  57, Train loss: 1.107, Test loss: 1.808, Test accuracy: 44.15 

Round  57, Global train loss: 1.107, Global test loss: 1.316, Global test accuracy: 56.11 

Round  58, Train loss: 1.171, Test loss: 1.811, Test accuracy: 44.22 

Round  58, Global train loss: 1.171, Global test loss: 1.350, Global test accuracy: 54.34 

Round  59, Train loss: 1.114, Test loss: 1.813, Test accuracy: 44.51 

Round  59, Global train loss: 1.114, Global test loss: 1.292, Global test accuracy: 57.44 

Round  60, Train loss: 1.223, Test loss: 1.828, Test accuracy: 44.51 

Round  60, Global train loss: 1.223, Global test loss: 1.315, Global test accuracy: 56.33 

Round  61, Train loss: 1.064, Test loss: 1.806, Test accuracy: 44.88 

Round  61, Global train loss: 1.064, Global test loss: 1.259, Global test accuracy: 57.94 

Round  62, Train loss: 1.219, Test loss: 1.822, Test accuracy: 44.78 

Round  62, Global train loss: 1.219, Global test loss: 1.366, Global test accuracy: 55.31 

Round  63, Train loss: 1.214, Test loss: 1.857, Test accuracy: 44.30 

Round  63, Global train loss: 1.214, Global test loss: 1.396, Global test accuracy: 53.98 

Round  64, Train loss: 1.069, Test loss: 1.859, Test accuracy: 44.55 

Round  64, Global train loss: 1.069, Global test loss: 1.277, Global test accuracy: 56.79 

Round  65, Train loss: 1.026, Test loss: 1.863, Test accuracy: 44.63 

Round  65, Global train loss: 1.026, Global test loss: 1.322, Global test accuracy: 56.75 

Round  66, Train loss: 1.148, Test loss: 1.855, Test accuracy: 44.73 

Round  66, Global train loss: 1.148, Global test loss: 1.338, Global test accuracy: 54.99 

Round  67, Train loss: 1.231, Test loss: 1.862, Test accuracy: 44.52 

Round  67, Global train loss: 1.231, Global test loss: 1.465, Global test accuracy: 51.40 

Round  68, Train loss: 1.111, Test loss: 1.890, Test accuracy: 44.42 

Round  68, Global train loss: 1.111, Global test loss: 1.356, Global test accuracy: 54.95 

Round  69, Train loss: 1.024, Test loss: 1.911, Test accuracy: 44.23 

Round  69, Global train loss: 1.024, Global test loss: 1.408, Global test accuracy: 54.07 

Round  70, Train loss: 1.127, Test loss: 1.896, Test accuracy: 44.17 

Round  70, Global train loss: 1.127, Global test loss: 1.412, Global test accuracy: 53.24 

Round  71, Train loss: 0.991, Test loss: 1.934, Test accuracy: 43.97 

Round  71, Global train loss: 0.991, Global test loss: 1.328, Global test accuracy: 57.12 

Round  72, Train loss: 1.054, Test loss: 1.920, Test accuracy: 44.27 

Round  72, Global train loss: 1.054, Global test loss: 1.309, Global test accuracy: 57.19 

Round  73, Train loss: 0.916, Test loss: 1.922, Test accuracy: 44.66 

Round  73, Global train loss: 0.916, Global test loss: 1.335, Global test accuracy: 57.23 

Round  74, Train loss: 0.965, Test loss: 1.926, Test accuracy: 44.72 

Round  74, Global train loss: 0.965, Global test loss: 1.319, Global test accuracy: 57.08 

Round  75, Train loss: 1.171, Test loss: 1.936, Test accuracy: 44.31 

Round  75, Global train loss: 1.171, Global test loss: 1.403, Global test accuracy: 53.12 

Round  76, Train loss: 1.074, Test loss: 1.933, Test accuracy: 44.25 

Round  76, Global train loss: 1.074, Global test loss: 1.353, Global test accuracy: 55.47 

Round  77, Train loss: 1.047, Test loss: 1.957, Test accuracy: 43.92 

Round  77, Global train loss: 1.047, Global test loss: 1.321, Global test accuracy: 57.22 

Round  78, Train loss: 1.102, Test loss: 1.979, Test accuracy: 43.74 

Round  78, Global train loss: 1.102, Global test loss: 1.368, Global test accuracy: 55.35 

Round  79, Train loss: 0.992, Test loss: 1.983, Test accuracy: 44.13 

Round  79, Global train loss: 0.992, Global test loss: 1.321, Global test accuracy: 57.59 

Round  80, Train loss: 1.065, Test loss: 1.996, Test accuracy: 44.72 

Round  80, Global train loss: 1.065, Global test loss: 1.308, Global test accuracy: 56.41 

Round  81, Train loss: 0.910, Test loss: 2.004, Test accuracy: 44.15 

Round  81, Global train loss: 0.910, Global test loss: 1.347, Global test accuracy: 57.13 

Round  82, Train loss: 0.947, Test loss: 1.971, Test accuracy: 44.49 

Round  82, Global train loss: 0.947, Global test loss: 1.379, Global test accuracy: 55.81 

Round  83, Train loss: 0.868, Test loss: 2.009, Test accuracy: 44.19 

Round  83, Global train loss: 0.868, Global test loss: 1.363, Global test accuracy: 56.95 

Round  84, Train loss: 1.028, Test loss: 2.003, Test accuracy: 44.37 

Round  84, Global train loss: 1.028, Global test loss: 1.379, Global test accuracy: 56.44 

Round  85, Train loss: 0.991, Test loss: 1.978, Test accuracy: 44.74 

Round  85, Global train loss: 0.991, Global test loss: 1.364, Global test accuracy: 56.45 

Round  86, Train loss: 0.904, Test loss: 1.978, Test accuracy: 45.07 

Round  86, Global train loss: 0.904, Global test loss: 1.277, Global test accuracy: 58.60 

Round  87, Train loss: 0.942, Test loss: 1.986, Test accuracy: 45.04 

Round  87, Global train loss: 0.942, Global test loss: 1.326, Global test accuracy: 57.03 

Round  88, Train loss: 0.972, Test loss: 2.001, Test accuracy: 45.02 

Round  88, Global train loss: 0.972, Global test loss: 1.387, Global test accuracy: 55.15 

Round  89, Train loss: 0.829, Test loss: 1.990, Test accuracy: 45.16 
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  89, Global train loss: 0.829, Global test loss: 1.382, Global test accuracy: 56.83 

Round  90, Train loss: 0.871, Test loss: 2.008, Test accuracy: 45.16 

Round  90, Global train loss: 0.871, Global test loss: 1.362, Global test accuracy: 57.15 

Round  91, Train loss: 1.011, Test loss: 2.025, Test accuracy: 45.30 

Round  91, Global train loss: 1.011, Global test loss: 1.482, Global test accuracy: 52.82 

Round  92, Train loss: 0.863, Test loss: 2.024, Test accuracy: 45.26 

Round  92, Global train loss: 0.863, Global test loss: 1.502, Global test accuracy: 53.49 

Round  93, Train loss: 0.915, Test loss: 2.024, Test accuracy: 45.32 

Round  93, Global train loss: 0.915, Global test loss: 1.389, Global test accuracy: 56.16 

Round  94, Train loss: 0.959, Test loss: 2.034, Test accuracy: 45.42 

Round  94, Global train loss: 0.959, Global test loss: 1.411, Global test accuracy: 54.83 

Round  95, Train loss: 0.905, Test loss: 2.045, Test accuracy: 45.33 

Round  95, Global train loss: 0.905, Global test loss: 1.343, Global test accuracy: 57.80 

Round  96, Train loss: 0.970, Test loss: 2.042, Test accuracy: 45.24 

Round  96, Global train loss: 0.970, Global test loss: 1.424, Global test accuracy: 54.63 

Round  97, Train loss: 0.826, Test loss: 2.074, Test accuracy: 45.04 

Round  97, Global train loss: 0.826, Global test loss: 1.399, Global test accuracy: 56.89 

Round  98, Train loss: 1.007, Test loss: 2.065, Test accuracy: 45.27 

Round  98, Global train loss: 1.007, Global test loss: 1.365, Global test accuracy: 56.34 

Round  99, Train loss: 0.972, Test loss: 2.086, Test accuracy: 45.09 

Round  99, Global train loss: 0.972, Global test loss: 1.544, Global test accuracy: 50.99 

Final Round, Train loss: 0.729, Test loss: 2.314, Test accuracy: 44.15 

Final Round, Global train loss: 0.729, Global test loss: 1.544, Global test accuracy: 50.99 

Average accuracy final 10 rounds: 45.24275000000001 

Average global accuracy final 10 rounds: 55.108000000000004 

2717.7374601364136
[1.496361494064331, 2.7612593173980713, 4.025646209716797, 5.287736177444458, 6.54688572883606, 7.811468124389648, 9.073362827301025, 10.329586029052734, 11.590712785720825, 12.881981611251831, 14.002526044845581, 15.12631893157959, 16.244140148162842, 17.536739587783813, 18.658364295959473, 19.778419494628906, 20.89648461341858, 22.016291618347168, 23.135244131088257, 24.257281064987183, 25.378098487854004, 26.497803926467896, 27.614186763763428, 28.728941917419434, 29.848133087158203, 30.963643789291382, 32.08241105079651, 33.203887939453125, 34.328232288360596, 35.448647022247314, 36.57021737098694, 37.68690824508667, 38.80473256111145, 39.92062520980835, 41.03425216674805, 42.14805722236633, 43.262240171432495, 44.376861810684204, 45.49054980278015, 46.60241222381592, 47.7210111618042, 48.83659911155701, 49.955078125, 51.07941389083862, 52.20197057723999, 53.32304859161377, 54.444480895996094, 55.5693838596344, 56.70156192779541, 57.820672035217285, 58.94128370285034, 60.059672355651855, 61.177714109420776, 62.29499268531799, 63.41201949119568, 64.54226231575012, 65.65865874290466, 66.77272486686707, 67.89028835296631, 69.01228857040405, 70.13339805603027, 71.25666832923889, 72.3843502998352, 73.50960493087769, 74.76526665687561, 76.03627943992615, 77.3105936050415, 78.56735444068909, 79.78444314002991, 81.03597235679626, 82.28270030021667, 83.48363494873047, 84.7350754737854, 85.98438000679016, 87.2383017539978, 88.49817276000977, 89.75570511817932, 91.01354336738586, 92.27517867088318, 93.53742671012878, 94.80179977416992, 96.07631397247314, 97.33459830284119, 98.59073781967163, 99.84390497207642, 101.10245609283447, 102.18130898475647, 103.25876903533936, 104.33763551712036, 105.41476440429688, 106.4943585395813, 107.57363724708557, 108.65111207962036, 109.73142695426941, 110.8102548122406, 111.88846111297607, 112.96751976013184, 114.04483771324158, 115.12079787254333, 116.19608354568481, 118.38627934455872]
[21.85, 24.34, 26.6675, 26.845, 28.325, 29.3375, 30.3875, 31.7775, 33.57, 34.2075, 35.455, 35.8825, 35.8575, 36.5825, 36.9575, 37.8375, 38.5325, 39.21, 39.94, 40.3825, 40.715, 40.935, 41.105, 41.94, 41.7875, 41.6925, 42.0625, 42.34, 42.51, 42.7125, 42.895, 43.0975, 42.54, 43.19, 43.405, 43.5125, 43.76, 44.0775, 43.72, 43.76, 44.3825, 44.7725, 44.445, 44.3475, 44.6425, 45.26, 45.075, 44.715, 44.5925, 44.5525, 44.9675, 44.5575, 45.175, 44.7225, 44.7125, 44.9275, 44.415, 44.1475, 44.215, 44.505, 44.51, 44.8775, 44.78, 44.3, 44.5525, 44.635, 44.725, 44.52, 44.425, 44.23, 44.17, 43.965, 44.27, 44.6575, 44.7175, 44.315, 44.2475, 43.92, 43.745, 44.13, 44.7225, 44.1475, 44.4875, 44.1925, 44.365, 44.745, 45.0725, 45.04, 45.02, 45.165, 45.1625, 45.3, 45.255, 45.3225, 45.425, 45.325, 45.2425, 45.0375, 45.2725, 45.085, 44.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8710
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6035
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8700
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5360
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7400
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8445
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5080
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7880
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7145
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8805
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8685
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5880
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8630
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.4930
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7690
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8505
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4665
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7655
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7520
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8810
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  27.3900
Round 1 global test acc  32.5200
Round 2 global test acc  34.5400
Round 3 global test acc  42.9700
Round 4 global test acc  42.4700
Round 5 global test acc  42.9000
Round 6 global test acc  46.7200
Round 7 global test acc  45.2400
Round 8 global test acc  47.6100
Round 9 global test acc  46.0200
Round 10 global test acc  53.4700
Round 11 global test acc  45.8700
Round 12 global test acc  49.2500
Round 13 global test acc  50.9600
Round 14 global test acc  49.3000
Round 15 global test acc  52.1700
Round 16 global test acc  54.5800
Round 17 global test acc  54.1100
Round 18 global test acc  51.4400
Round 19 global test acc  55.6900
Round 20 global test acc  49.3100
Round 21 global test acc  51.5400
Round 22 global test acc  52.1100
Round 23 global test acc  54.7900
Round 24 global test acc  55.5800
Round 25 global test acc  58.2900
Round 26 global test acc  53.6100
Round 27 global test acc  55.5800
Round 28 global test acc  54.6200
Round 29 global test acc  58.1300
Round 30 global test acc  59.3100
Round 31 global test acc  54.2800
Round 32 global test acc  57.1300
Round 33 global test acc  55.5700
Round 34 global test acc  51.2200
Round 35 global test acc  55.4400
Round 36 global test acc  56.1200
Round 37 global test acc  54.9300
Round 38 global test acc  53.7400
Round 39 global test acc  54.9000
Round 40 global test acc  59.1000
Round 41 global test acc  54.6000
Round 42 global test acc  60.7700
Round 43 global test acc  59.5200
Round 44 global test acc  57.9300
Round 45 global test acc  56.7600
Round 46 global test acc  56.9300
Round 47 global test acc  55.2900
Round 48 global test acc  60.5100
Round 49 global test acc  57.3600
Round 50 global test acc  60.5600
Round 51 global test acc  51.5000
Round 52 global test acc  58.6600
Round 53 global test acc  61.4100
Round 54 global test acc  61.5300
Round 55 global test acc  58.7100
Round 56 global test acc  57.9500
Round 57 global test acc  60.4400
Round 58 global test acc  57.9400
Round 59 global test acc  59.5100
Round 60 global test acc  54.4700
Round 61 global test acc  62.3500
Round 62 global test acc  58.2400
Round 63 global test acc  57.4200
Round 64 global test acc  56.7900
Round 65 global test acc  53.6600
Round 66 global test acc  59.6000
Round 67 global test acc  61.2900
Round 68 global test acc  57.6100
Round 69 global test acc  59.1300
Round 70 global test acc  58.9600
Round 71 global test acc  57.9300
Round 72 global test acc  60.8300
Round 73 global test acc  60.5600
Round 74 global test acc  58.2600
Round 75 global test acc  58.5500
Round 76 global test acc  60.2000
Round 77 global test acc  60.7100
Round 78 global test acc  58.8600
Round 79 global test acc  60.6900
Round 80 global test acc  60.8000
Round 81 global test acc  59.5600
Round 82 global test acc  59.3800
Round 83 global test acc  59.0600
Round 84 global test acc  57.9000
Round 85 global test acc  58.7500
Round 86 global test acc  57.7800
Round 87 global test acc  57.8800
Round 88 global test acc  57.9300
Round 89 global test acc  57.2200
Round 90 global test acc  56.7700
Round 91 global test acc  57.2000
Round 92 global test acc  55.3900
Round 93 global test acc  55.6400
Round 94 global test acc  55.6000
Round 95 global test acc  54.6500
Round 96 global test acc  54.5000
Round 97 global test acc  56.2600
Round 98 global test acc  55.4500
Round 99 global test acc  55.4000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8855
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.6160
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8695
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5100
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7260
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8555
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4965
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7825
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.7515
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8895
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.558, Test loss: 0.976, Test accuracy: 68.11
Average accuracy final 10 rounds: 67.14500000000001
1923.3468129634857
[1.697258472442627, 3.081606388092041, 4.478444814682007, 5.866918563842773, 7.2450690269470215, 8.647677659988403, 10.046578168869019, 11.429511785507202, 12.821228742599487, 14.20957899093628, 15.592373847961426, 16.978964805603027, 18.37149143218994, 19.745290279388428, 21.120570421218872, 22.503907442092896, 23.886508464813232, 25.280659198760986, 26.66218400001526, 28.035426378250122, 29.41197180747986, 30.801541328430176, 32.187206983566284, 33.57357621192932, 34.95383048057556, 36.33375430107117, 37.71102213859558, 39.09268879890442, 40.48767614364624, 41.85925626754761, 43.23051905632019, 44.62098956108093, 46.0022337436676, 47.38621187210083, 48.77360725402832, 50.150599002838135, 51.52013611793518, 52.91150426864624, 54.301891803741455, 55.67518496513367, 57.060683488845825, 58.447649002075195, 59.82624340057373, 61.20786762237549, 62.610400915145874, 63.985575675964355, 65.36940288543701, 66.75646877288818, 68.14593625068665, 69.52324318885803, 70.90639615058899, 72.28624510765076, 73.67253828048706, 75.05602216720581, 76.44081664085388, 77.81569814682007, 79.19283413887024, 80.57217049598694, 81.95921277999878, 83.32604455947876, 84.70420527458191, 86.07492327690125, 87.4512267112732, 88.83338713645935, 90.20605969429016, 91.57465934753418, 92.95379376411438, 94.32878112792969, 95.69997262954712, 97.08249855041504, 98.45253705978394, 99.81779432296753, 101.2161500453949, 102.59980654716492, 103.96718406677246, 105.34515476226807, 106.7234570980072, 108.09343028068542, 109.48481750488281, 110.85854649543762, 112.23070502281189, 113.61521577835083, 115.00260806083679, 116.37016534805298, 117.74670028686523, 119.11683082580566, 120.49442458152771, 121.87574243545532, 123.25355744361877, 124.61854863166809, 126.00904679298401, 127.39300847053528, 128.76900124549866, 130.13607907295227, 131.5170624256134, 132.89770078659058, 134.27306175231934, 135.64557218551636, 137.02692532539368, 138.40200781822205, 140.5070390701294]
[19.3825, 24.95, 30.2875, 33.275, 35.3625, 37.4125, 39.4825, 40.745, 42.4375, 43.3025, 44.5075, 45.55, 46.86, 47.8, 49.1575, 50.1325, 48.86, 49.75, 51.465, 52.555, 53.3025, 54.17, 54.775, 54.6875, 55.745, 55.92, 56.785, 56.635, 57.58, 57.905, 58.2, 58.1275, 58.175, 59.3075, 59.8375, 60.2175, 59.87, 60.59, 61.2175, 60.9925, 61.055, 61.9925, 62.1025, 62.41, 62.455, 62.3325, 61.645, 62.3125, 63.1125, 62.155, 61.9425, 63.12, 63.4875, 64.105, 64.36, 63.6925, 64.2975, 64.2275, 64.01, 63.9675, 64.305, 64.3625, 64.3875, 64.4175, 65.245, 65.2775, 65.23, 65.405, 65.425, 65.6475, 65.42, 65.84, 65.885, 65.615, 65.8975, 65.7225, 65.8975, 66.42, 66.45, 66.355, 66.1525, 66.715, 67.12, 66.49, 66.5675, 67.2375, 66.9225, 67.1125, 67.0975, 66.74, 67.0875, 67.005, 67.2, 67.5275, 67.4425, 67.505, 67.425, 66.7025, 66.8925, 66.6625, 68.1075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8700
Client 2, noise level: 0.6597 (0.5937), real noise ratio: 0.5840
Client 3, noise level: 0.9589 (0.8630), real noise ratio: 0.8735
Client 4, noise level: 0.5160 (0.4644), real noise ratio: 0.5075
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.8075
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8680
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.4875
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.7945
Client 13, noise level: 0.7546 (0.6792), real noise ratio: 0.6875
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8895
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2065
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0575
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7235
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1085
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5855
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5145
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0695
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2460
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6435
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3110
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.290, Test loss: 5.113, Test accuracy: 31.54
Final Round, Global train loss: 0.290, Global test loss: 1.706, Global test accuracy: 44.94
Average accuracy final 10 rounds: 31.601000000000003 

Average global accuracy final 10 rounds: 40.365249999999996 

3915.6937005519867
[1.4404387474060059, 2.8808774948120117, 4.087364673614502, 5.293851852416992, 6.502847671508789, 7.711843490600586, 8.917244672775269, 10.122645854949951, 11.326619386672974, 12.530592918395996, 13.742833614349365, 14.955074310302734, 16.157200813293457, 17.35932731628418, 18.566506385803223, 19.773685455322266, 20.977768421173096, 22.181851387023926, 23.381224870681763, 24.5805983543396, 25.777329444885254, 26.974060535430908, 28.16968846321106, 29.36531639099121, 30.574002742767334, 31.782689094543457, 32.98108386993408, 34.17947864532471, 35.37797665596008, 36.57647466659546, 37.77892446517944, 38.98137426376343, 40.18446230888367, 41.387550354003906, 42.58675217628479, 43.785953998565674, 44.98140907287598, 46.17686414718628, 47.37721562385559, 48.5775671005249, 49.77885937690735, 50.980151653289795, 52.17951583862305, 53.3788800239563, 54.584500551223755, 55.79012107849121, 56.983131408691406, 58.1761417388916, 59.37270164489746, 60.56926155090332, 61.76221561431885, 62.955169677734375, 64.1497814655304, 65.34439325332642, 66.54374480247498, 67.74309635162354, 68.93741607666016, 70.13173580169678, 71.32937479019165, 72.52701377868652, 73.7337429523468, 74.94047212600708, 76.14107775688171, 77.34168338775635, 78.53971147537231, 79.73773956298828, 80.93166828155518, 82.12559700012207, 83.33155250549316, 84.53750801086426, 85.74031043052673, 86.94311285018921, 88.14127206802368, 89.33943128585815, 90.55152750015259, 91.76362371444702, 92.96525645256042, 94.16688919067383, 95.35605478286743, 96.54522037506104, 97.73146986961365, 98.91771936416626, 100.11536931991577, 101.31301927566528, 102.51432037353516, 103.71562147140503, 104.92228364944458, 106.12894582748413, 107.33452916145325, 108.54011249542236, 109.74190330505371, 110.94369411468506, 112.14754724502563, 113.35140037536621, 114.55066776275635, 115.74993515014648, 116.94993185997009, 118.1499285697937, 119.35543632507324, 120.56094408035278, 121.75821661949158, 122.95548915863037, 124.1578438282013, 125.36019849777222, 126.57028937339783, 127.78038024902344, 128.96991324424744, 130.15944623947144, 131.3455934524536, 132.5317406654358, 133.7277648448944, 134.92378902435303, 136.12371492385864, 137.32364082336426, 138.5188820362091, 139.71412324905396, 140.92143201828003, 142.1287407875061, 143.3238320350647, 144.5189232826233, 145.7078139781952, 146.8967046737671, 148.0938277244568, 149.29095077514648, 150.48790645599365, 151.68486213684082, 152.8898892402649, 154.09491634368896, 155.2914547920227, 156.48799324035645, 157.6977412700653, 158.90748929977417, 160.1070101261139, 161.3065309524536, 162.5144658088684, 163.7224006652832, 164.92983675003052, 166.13727283477783, 167.33229970932007, 168.5273265838623, 169.7329604625702, 170.93859434127808, 172.1441752910614, 173.34975624084473, 174.56512331962585, 175.78049039840698, 176.9835865497589, 178.18668270111084, 179.38563656806946, 180.58459043502808, 181.7930600643158, 183.00152969360352, 184.20127296447754, 185.40101623535156, 186.60987377166748, 187.8187313079834, 189.01959252357483, 190.22045373916626, 191.41330003738403, 192.6061463356018, 193.8062081336975, 195.0062699317932, 196.21242594718933, 197.41858196258545, 198.62051820755005, 199.82245445251465, 200.86431527137756, 201.90617609024048, 202.94660353660583, 203.9870309829712, 205.0309865474701, 206.074942111969, 207.1141254901886, 208.1533088684082, 209.18635153770447, 210.21939420700073, 211.25928616523743, 212.29917812347412, 213.3456404209137, 214.39210271835327, 215.43532967567444, 216.4785566329956, 217.5241940021515, 218.56983137130737, 219.60876727104187, 220.64770317077637, 221.6891152858734, 222.73052740097046, 223.77207159996033, 224.8136157989502, 225.85149550437927, 226.88937520980835, 227.92674112319946, 228.96410703659058, 230.00769972801208, 231.0512924194336, 232.10159182548523, 233.15189123153687, 234.1983983516693, 235.24490547180176, 237.33003616333008, 239.4151668548584]
[17.675, 17.675, 24.0925, 24.0925, 26.1275, 26.1275, 26.5925, 26.5925, 26.8975, 26.8975, 27.97, 27.97, 28.81, 28.81, 29.7975, 29.7975, 29.9925, 29.9925, 30.235, 30.235, 30.635, 30.635, 30.905, 30.905, 31.2075, 31.2075, 31.6525, 31.6525, 32.205, 32.205, 32.6575, 32.6575, 32.7725, 32.7725, 33.3425, 33.3425, 33.32, 33.32, 33.7425, 33.7425, 33.9075, 33.9075, 33.525, 33.525, 33.785, 33.785, 33.995, 33.995, 33.935, 33.935, 33.0775, 33.0775, 33.3175, 33.3175, 33.36, 33.36, 33.4625, 33.4625, 33.5575, 33.5575, 33.1525, 33.1525, 33.035, 33.035, 33.165, 33.165, 33.585, 33.585, 33.34, 33.34, 33.035, 33.035, 32.8625, 32.8625, 33.16, 33.16, 33.155, 33.155, 32.6175, 32.6175, 32.7225, 32.7225, 32.3625, 32.3625, 32.69, 32.69, 32.4975, 32.4975, 32.705, 32.705, 32.22, 32.22, 32.3175, 32.3175, 32.2675, 32.2675, 32.3775, 32.3775, 32.4625, 32.4625, 32.5525, 32.5525, 32.025, 32.025, 32.25, 32.25, 32.1475, 32.1475, 31.86, 31.86, 31.8525, 31.8525, 32.0025, 32.0025, 32.0425, 32.0425, 32.0225, 32.0225, 31.8075, 31.8075, 31.84, 31.84, 31.775, 31.775, 31.77, 31.77, 31.6575, 31.6575, 31.6625, 31.6625, 31.2075, 31.2075, 31.4625, 31.4625, 31.775, 31.775, 31.77, 31.77, 32.28, 32.28, 31.8525, 31.8525, 31.725, 31.725, 31.6575, 31.6575, 31.8, 31.8, 31.9875, 31.9875, 31.8025, 31.8025, 31.7475, 31.7475, 31.5625, 31.5625, 31.7775, 31.7775, 31.9025, 31.9025, 31.7925, 31.7925, 31.7225, 31.7225, 31.6225, 31.6225, 31.6925, 31.6925, 31.5825, 31.5825, 31.59, 31.59, 31.75, 31.75, 31.8525, 31.8525, 31.93, 31.93, 31.46, 31.46, 31.765, 31.765, 31.5975, 31.5975, 31.66, 31.66, 31.8425, 31.8425, 31.5675, 31.5675, 31.5525, 31.5525, 31.7025, 31.7025, 31.75, 31.75, 31.2225, 31.2225, 31.35, 31.35, 31.54, 31.54]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2050
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0550
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7340
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2000
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5855
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4465
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1195
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3555
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6780
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3155
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.493, Test loss: 2.051, Test accuracy: 53.04
Final Round, Global train loss: 0.493, Global test loss: 1.307, Global test accuracy: 61.36
Average accuracy final 10 rounds: 53.47425 

Average global accuracy final 10 rounds: 62.219 

3895.357969522476
[1.4508700370788574, 2.901740074157715, 4.091190338134766, 5.280640602111816, 6.477564096450806, 7.674487590789795, 8.89134693145752, 10.108206272125244, 11.336436986923218, 12.564667701721191, 13.79271674156189, 15.020765781402588, 16.234060049057007, 17.447354316711426, 18.66580057144165, 19.884246826171875, 20.969895601272583, 22.05554437637329, 23.24549627304077, 24.435448169708252, 25.645458459854126, 26.85546875, 28.069326877593994, 29.28318500518799, 30.492393493652344, 31.7016019821167, 32.909448862075806, 34.11729574203491, 35.32285761833191, 36.528419494628906, 37.73210334777832, 38.935787200927734, 40.14143753051758, 41.34708786010742, 42.554643392562866, 43.76219892501831, 44.977288007736206, 46.1923770904541, 47.41206216812134, 48.631747245788574, 49.84007239341736, 51.04839754104614, 52.24306654930115, 53.43773555755615, 54.65088939666748, 55.86404323577881, 57.07034969329834, 58.27665615081787, 59.48507356643677, 60.693490982055664, 61.91071581840515, 63.12794065475464, 64.33794283866882, 65.54794502258301, 66.74582076072693, 67.94369649887085, 69.14781498908997, 70.35193347930908, 71.55466341972351, 72.75739336013794, 73.95411705970764, 75.15084075927734, 76.33681082725525, 77.52278089523315, 78.71687841415405, 79.91097593307495, 81.10768699645996, 82.30439805984497, 83.50448441505432, 84.70457077026367, 85.89554119110107, 87.08651161193848, 88.27712440490723, 89.46773719787598, 90.66155004501343, 91.85536289215088, 93.06173610687256, 94.26810932159424, 95.47018361091614, 96.67225790023804, 97.86507892608643, 99.05789995193481, 100.26172351837158, 101.46554708480835, 102.66192960739136, 103.85831212997437, 105.07111072540283, 106.2839093208313, 107.48340559005737, 108.68290185928345, 109.88722062110901, 111.09153938293457, 112.27288460731506, 113.45422983169556, 114.6389684677124, 115.82370710372925, 117.00171971321106, 118.17973232269287, 119.36350440979004, 120.5472764968872, 121.7218987941742, 122.89652109146118, 124.07158184051514, 125.24664258956909, 126.42383551597595, 127.60102844238281, 128.79186129570007, 129.98269414901733, 131.1781358718872, 132.37357759475708, 133.5715320110321, 134.76948642730713, 135.96543645858765, 137.16138648986816, 138.34493041038513, 139.5284743309021, 140.72238540649414, 141.91629648208618, 143.1130542755127, 144.3098120689392, 145.50625944137573, 146.70270681381226, 147.89063143730164, 149.07855606079102, 150.27928471565247, 151.48001337051392, 152.6692099571228, 153.8584065437317, 155.03820872306824, 156.21801090240479, 157.39359736442566, 158.56918382644653, 159.75232815742493, 160.93547248840332, 162.12618613243103, 163.31689977645874, 164.57212734222412, 165.8273549079895, 167.0235185623169, 168.2196822166443, 169.4039921760559, 170.58830213546753, 171.7756688594818, 172.9630355834961, 174.1512315273285, 175.3394274711609, 176.52269530296326, 177.70596313476562, 178.8956801891327, 180.08539724349976, 181.27674913406372, 182.46810102462769, 183.65654230117798, 184.84498357772827, 186.03339099884033, 187.2217984199524, 188.41342401504517, 189.60504961013794, 190.7949197292328, 191.98478984832764, 193.18006372451782, 194.375337600708, 195.56523251533508, 196.75512742996216, 197.93751573562622, 199.11990404129028, 200.3119695186615, 201.50403499603271, 202.68818998336792, 203.87234497070312, 205.04237365722656, 206.21240234375, 207.39820003509521, 208.58399772644043, 209.77827382087708, 210.97254991531372, 212.16665625572205, 213.36076259613037, 214.5510401725769, 215.74131774902344, 216.93263936042786, 218.12396097183228, 219.31339859962463, 220.502836227417, 221.69197726249695, 222.8811182975769, 224.07802820205688, 225.27493810653687, 226.47678804397583, 227.6786379814148, 228.86549043655396, 230.05234289169312, 231.236230134964, 232.42011737823486, 233.61113834381104, 234.8021593093872, 235.99202632904053, 237.18189334869385, 238.36716651916504, 239.55243968963623, 241.9498336315155, 244.34722757339478]
[23.5325, 23.5325, 26.255, 26.255, 28.16, 28.16, 29.2125, 29.2125, 30.175, 30.175, 32.22, 32.22, 33.75, 33.75, 36.485, 36.485, 37.5475, 37.5475, 38.345, 38.345, 39.8525, 39.8525, 40.3225, 40.3225, 40.9725, 40.9725, 42.4, 42.4, 43.595, 43.595, 44.2275, 44.2275, 44.4025, 44.4025, 44.9575, 44.9575, 46.17, 46.17, 46.585, 46.585, 46.3175, 46.3175, 46.91, 46.91, 47.0725, 47.0725, 47.765, 47.765, 48.3625, 48.3625, 48.01, 48.01, 48.1, 48.1, 48.42, 48.42, 48.64, 48.64, 48.6025, 48.6025, 49.0975, 49.0975, 49.1525, 49.1525, 49.47, 49.47, 49.73, 49.73, 49.925, 49.925, 50.055, 50.055, 49.895, 49.895, 50.16, 50.16, 50.7725, 50.7725, 50.9375, 50.9375, 50.5675, 50.5675, 50.8075, 50.8075, 51.1175, 51.1175, 51.44, 51.44, 51.7775, 51.7775, 51.825, 51.825, 51.39, 51.39, 51.4675, 51.4675, 51.4225, 51.4225, 51.65, 51.65, 51.685, 51.685, 51.4175, 51.4175, 51.5875, 51.5875, 51.9925, 51.9925, 52.1325, 52.1325, 52.08, 52.08, 51.7225, 51.7225, 51.8, 51.8, 51.7, 51.7, 51.8525, 51.8525, 51.9225, 51.9225, 52.32, 52.32, 52.4025, 52.4025, 52.4075, 52.4075, 52.24, 52.24, 52.5025, 52.5025, 52.545, 52.545, 52.6925, 52.6925, 52.66, 52.66, 52.71, 52.71, 52.78, 52.78, 52.93, 52.93, 53.1525, 53.1525, 53.2175, 53.2175, 53.1825, 53.1825, 53.405, 53.405, 53.565, 53.565, 52.9425, 52.9425, 52.9075, 52.9075, 52.825, 52.825, 53.21, 53.21, 53.355, 53.355, 53.5325, 53.5325, 53.545, 53.545, 53.675, 53.675, 53.44, 53.44, 53.33, 53.33, 53.4425, 53.4425, 53.5025, 53.5025, 53.4225, 53.4225, 53.425, 53.425, 53.2825, 53.2825, 53.3425, 53.3425, 53.3125, 53.3125, 53.565, 53.565, 53.4125, 53.4125, 53.7425, 53.7425, 53.6125, 53.6125, 53.515, 53.515, 53.5325, 53.5325, 53.04, 53.04]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2075
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0770
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7285
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1955
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5770
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4780
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3285
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3005
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6910
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4175
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.585, Test loss: 1.860, Test accuracy: 53.79
Final Round, Global train loss: 0.585, Global test loss: 1.265, Global test accuracy: 61.91
Average accuracy final 10 rounds: 54.018 

Average global accuracy final 10 rounds: 61.902750000000005 

3861.4236676692963
[1.5139551162719727, 3.0279102325439453, 4.309161186218262, 5.590412139892578, 6.874286651611328, 8.158161163330078, 9.435220956802368, 10.712280750274658, 12.003278255462646, 13.294275760650635, 14.579126834869385, 15.863977909088135, 17.15173888206482, 18.439499855041504, 19.72748613357544, 21.015472412109375, 22.302653551101685, 23.589834690093994, 24.880720615386963, 26.17160654067993, 27.456088542938232, 28.740570545196533, 30.027918100357056, 31.315265655517578, 32.59809327125549, 33.88092088699341, 35.168819189071655, 36.4567174911499, 37.747565269470215, 39.03841304779053, 40.3184597492218, 41.598506450653076, 42.88242959976196, 44.16635274887085, 45.43176627159119, 46.69717979431152, 47.9623646736145, 49.22754955291748, 50.4921760559082, 51.756802558898926, 53.023515462875366, 54.29022836685181, 55.55310320854187, 56.815978050231934, 58.07348322868347, 59.33098840713501, 60.607611894607544, 61.88423538208008, 63.16551756858826, 64.44679975509644, 65.73217487335205, 67.01754999160767, 68.30655813217163, 69.5955662727356, 70.86854028701782, 72.14151430130005, 73.4882402420044, 74.83496618270874, 76.12589120864868, 77.41681623458862, 78.70363116264343, 79.99044609069824, 81.27274322509766, 82.55504035949707, 83.83634686470032, 85.11765336990356, 86.39568662643433, 87.67371988296509, 88.97125434875488, 90.26878881454468, 91.54564166069031, 92.82249450683594, 93.92183017730713, 95.02116584777832, 96.12023591995239, 97.21930599212646, 98.3209490776062, 99.42259216308594, 100.52755379676819, 101.63251543045044, 102.73154282569885, 103.83057022094727, 104.93416929244995, 106.03776836395264, 107.14398241043091, 108.25019645690918, 109.35472583770752, 110.45925521850586, 111.55852270126343, 112.657790184021, 113.76476311683655, 114.8717360496521, 115.97728824615479, 117.08284044265747, 118.19345164299011, 119.30406284332275, 120.40670323371887, 121.50934362411499, 122.61707282066345, 123.72480201721191, 124.8199782371521, 125.91515445709229, 127.02101302146912, 128.12687158584595, 129.22703385353088, 130.32719612121582, 131.4287121295929, 132.53022813796997, 133.63137865066528, 134.7325291633606, 135.83498978614807, 136.93745040893555, 138.05277252197266, 139.16809463500977, 140.275399684906, 141.38270473480225, 142.48961472511292, 143.59652471542358, 144.70113134384155, 145.80573797225952, 146.92093896865845, 148.03613996505737, 149.1484820842743, 150.2608242034912, 151.37091064453125, 152.4809970855713, 153.5909080505371, 154.70081901550293, 155.81336784362793, 156.92591667175293, 158.0331952571869, 159.14047384262085, 160.25477719306946, 161.36908054351807, 162.4745090007782, 163.57993745803833, 164.6878457069397, 165.79575395584106, 166.90696096420288, 168.0181679725647, 169.12484192848206, 170.2315158843994, 171.34138202667236, 172.4512481689453, 173.55856490135193, 174.66588163375854, 175.77743244171143, 176.8889832496643, 177.996506690979, 179.1040301322937, 180.21876907348633, 181.33350801467896, 182.4471137523651, 183.56071949005127, 184.66849040985107, 185.77626132965088, 186.8921172618866, 188.00797319412231, 189.11098766326904, 190.21400213241577, 191.33190536499023, 192.4498085975647, 193.55724143981934, 194.66467428207397, 195.77969074249268, 196.89470720291138, 197.99980187416077, 199.10489654541016, 200.2336187362671, 201.36234092712402, 202.46883583068848, 203.57533073425293, 204.67747449874878, 205.77961826324463, 206.89608502388, 208.01255178451538, 209.1291356086731, 210.2457194328308, 211.35477542877197, 212.46383142471313, 213.57229280471802, 214.6807541847229, 215.7884223461151, 216.89609050750732, 218.00267219543457, 219.10925388336182, 220.21593284606934, 221.32261180877686, 222.42742538452148, 223.5322389602661, 224.64135313034058, 225.75046730041504, 226.86061453819275, 227.97076177597046, 229.08404207229614, 230.19732236862183, 231.30741095542908, 232.41749954223633, 233.52473306655884, 234.63196659088135, 236.85924243927002, 239.0865182876587]
[19.985, 19.985, 25.5575, 25.5575, 26.2825, 26.2825, 28.1425, 28.1425, 28.985, 28.985, 31.1175, 31.1175, 32.4025, 32.4025, 34.43, 34.43, 36.2575, 36.2575, 37.4175, 37.4175, 38.6125, 38.6125, 38.725, 38.725, 39.425, 39.425, 40.66, 40.66, 42.9075, 42.9075, 43.33, 43.33, 44.0475, 44.0475, 44.625, 44.625, 45.5475, 45.5475, 45.9475, 45.9475, 46.7625, 46.7625, 47.0725, 47.0725, 46.7475, 46.7475, 47.1225, 47.1225, 47.3, 47.3, 47.5375, 47.5375, 47.9525, 47.9525, 48.8325, 48.8325, 49.135, 49.135, 48.7975, 48.7975, 48.6575, 48.6575, 49.2875, 49.2875, 49.4225, 49.4225, 49.92, 49.92, 50.2775, 50.2775, 50.0625, 50.0625, 50.485, 50.485, 50.855, 50.855, 51.4025, 51.4025, 51.96, 51.96, 52.1075, 52.1075, 52.1525, 52.1525, 52.0875, 52.0875, 52.5125, 52.5125, 52.11, 52.11, 52.03, 52.03, 52.53, 52.53, 52.5775, 52.5775, 52.4875, 52.4875, 52.9225, 52.9225, 52.8625, 52.8625, 52.64, 52.64, 52.48, 52.48, 52.315, 52.315, 52.57, 52.57, 51.6725, 51.6725, 51.745, 51.745, 51.9925, 51.9925, 51.9475, 51.9475, 52.27, 52.27, 52.4525, 52.4525, 52.715, 52.715, 52.11, 52.11, 52.35, 52.35, 52.0, 52.0, 52.1525, 52.1525, 52.275, 52.275, 52.6675, 52.6675, 52.81, 52.81, 53.1175, 53.1175, 53.0775, 53.0775, 53.51, 53.51, 53.8325, 53.8325, 53.8825, 53.8825, 53.6825, 53.6825, 53.785, 53.785, 53.655, 53.655, 53.695, 53.695, 53.36, 53.36, 53.0725, 53.0725, 52.98, 52.98, 53.2275, 53.2275, 53.25, 53.25, 53.5875, 53.5875, 53.575, 53.575, 53.675, 53.675, 53.645, 53.645, 53.2975, 53.2975, 53.8025, 53.8025, 54.0175, 54.0175, 54.185, 54.185, 53.915, 53.915, 54.0225, 54.0225, 54.3425, 54.3425, 54.235, 54.235, 54.0175, 54.0175, 53.8025, 53.8025, 53.9425, 53.9425, 53.9075, 53.9075, 53.81, 53.81, 53.7875, 53.7875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2135
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0795
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7285
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0660
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5860
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4645
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0060
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3330
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6670
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3875
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2055
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0780
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7235
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0665
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5715
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4610
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0730
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2780
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6800
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3835
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  28.0500
Round 1 global test acc  31.5600
Round 2 global test acc  40.8300
Round 3 global test acc  46.0500
Round 4 global test acc  49.0700
Round 5 global test acc  46.0200
Round 6 global test acc  50.8300
Round 7 global test acc  50.9100
Round 8 global test acc  53.8900
Round 9 global test acc  53.2900
Round 10 global test acc  55.3100
Round 11 global test acc  54.2500
Round 12 global test acc  55.8000
Round 13 global test acc  55.5300
Round 14 global test acc  56.1600
Round 15 global test acc  56.6200
Round 16 global test acc  58.6900
Round 17 global test acc  56.6800
Round 18 global test acc  59.3400
Round 19 global test acc  59.1800
Round 20 global test acc  59.4600
Round 21 global test acc  59.6700
Round 22 global test acc  59.3900
Round 23 global test acc  60.6600
Round 24 global test acc  59.7800
Round 25 global test acc  61.5300
Round 26 global test acc  61.5800
Round 27 global test acc  61.7900
Round 28 global test acc  61.6700
Round 29 global test acc  59.9900
Round 30 global test acc  63.1100
Round 31 global test acc  63.2200
Round 32 global test acc  61.7000
Round 33 global test acc  64.3000
Round 34 global test acc  62.6200
Round 35 global test acc  62.3900
Round 36 global test acc  63.7400
Round 37 global test acc  62.1000
Round 38 global test acc  63.7900
Round 39 global test acc  63.0500
Round 40 global test acc  63.9600
Round 41 global test acc  63.1700
Round 42 global test acc  64.4000
Round 43 global test acc  61.7200
Round 44 global test acc  63.6300
Round 45 global test acc  65.0900
Round 46 global test acc  63.9900
Round 47 global test acc  65.1000
Round 48 global test acc  64.9700
Round 49 global test acc  65.3800
Round 50 global test acc  64.8100
Round 51 global test acc  65.3300
Round 52 global test acc  64.8000
Round 53 global test acc  65.3300
Round 54 global test acc  63.7400
Round 55 global test acc  64.9800
Round 56 global test acc  66.4400
Round 57 global test acc  65.8200
Round 58 global test acc  64.7600
Round 59 global test acc  66.8700
Round 60 global test acc  65.8600
Round 61 global test acc  65.3700
Round 62 global test acc  65.5800
Round 63 global test acc  65.6400
Round 64 global test acc  66.1900
Round 65 global test acc  66.6200
Round 66 global test acc  66.2000
Round 67 global test acc  66.0400
Round 68 global test acc  67.1900
Round 69 global test acc  67.2900
Round 70 global test acc  66.2200
Round 71 global test acc  67.3100
Round 72 global test acc  67.7200
Round 73 global test acc  67.4400
Round 74 global test acc  67.3400
Round 75 global test acc  67.2100
Round 76 global test acc  66.3900
Round 77 global test acc  68.3700
Round 78 global test acc  67.0700
Round 79 global test acc  67.4500
Round 80 global test acc  67.0800
Round 81 global test acc  65.7500
Round 82 global test acc  64.0700
Round 83 global test acc  63.6500
Round 84 global test acc  62.9300
Round 85 global test acc  61.6700
Round 86 global test acc  61.8400
Round 87 global test acc  60.5600
Round 88 global test acc  60.3900
Round 89 global test acc  59.9400
Round 90 global test acc  59.8200
Round 91 global test acc  59.5600
Round 92 global test acc  59.1300
Round 93 global test acc  58.3000
Round 94 global test acc  58.5600
Round 95 global test acc  58.5800
Round 96 global test acc  57.9300
Round 97 global test acc  58.5800
Round 98 global test acc  58.4400
Round 99 global test acc  58.6800
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2100
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0790
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7190
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0605
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5945
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4385
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1775
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.4135
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6820
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4105
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.552, Test loss: 0.986, Test accuracy: 66.90
Average accuracy final 10 rounds: 66.63575
1869.511577129364
[1.7423474788665771, 3.162517547607422, 4.435385704040527, 5.685310363769531, 6.952738046646118, 8.21934962272644, 9.467183828353882, 10.733124732971191, 11.98501181602478, 13.23247504234314, 14.483497858047485, 15.727579832077026, 17.104257106781006, 18.49926447868347, 19.873652696609497, 21.269036531448364, 22.653103590011597, 24.040428161621094, 25.42329216003418, 26.809062957763672, 28.199949502944946, 29.578269243240356, 30.837002515792847, 32.086429834365845, 33.33882451057434, 34.6111741065979, 35.858168840408325, 37.10947346687317, 38.368226051330566, 39.62485647201538, 40.868415117263794, 42.11947560310364, 43.38572311401367, 44.64317798614502, 45.89656710624695, 47.1551787853241, 48.41575336456299, 49.659428119659424, 50.91221904754639, 52.17652773857117, 53.4413857460022, 54.694557666778564, 55.95835566520691, 57.22023153305054, 58.46779441833496, 59.74613857269287, 60.998164653778076, 62.29347348213196, 63.54596424102783, 64.79876923561096, 66.07346749305725, 67.31487464904785, 68.58641815185547, 69.83780026435852, 71.11228203773499, 72.39739441871643, 73.6495623588562, 74.90674090385437, 76.16186738014221, 77.4235725402832, 78.68636798858643, 79.96986794471741, 81.23411822319031, 82.51621294021606, 83.77753233909607, 85.01832890510559, 86.2989935874939, 87.54059195518494, 88.79354238510132, 90.06597328186035, 91.32444095611572, 92.6110463142395, 93.88211369514465, 95.13938975334167, 96.40032911300659, 97.65236496925354, 98.9043881893158, 100.18515729904175, 101.44061470031738, 102.69387936592102, 103.97050762176514, 105.2143816947937, 106.48816442489624, 107.74442958831787, 109.00535440444946, 110.27513074874878, 111.52843737602234, 112.7796516418457, 114.03550910949707, 115.30001306533813, 116.54886794090271, 117.80744767189026, 119.06805515289307, 120.32971930503845, 121.58805346488953, 122.84438705444336, 124.10354661941528, 125.34059453010559, 126.59731245040894, 127.85133790969849, 129.87142062187195]
[21.775, 26.8675, 30.85, 33.9, 35.955, 37.915, 39.9225, 41.555, 41.8725, 42.7175, 43.575, 45.0025, 45.875, 47.0175, 48.825, 49.3825, 49.8325, 50.6825, 51.4575, 52.14, 53.1125, 53.3675, 53.93, 54.3, 54.8675, 55.35, 56.1075, 54.93, 56.145, 55.835, 57.0225, 57.9975, 57.8425, 58.9775, 58.59, 58.9325, 59.3225, 59.7825, 60.7775, 60.7775, 60.945, 60.1875, 60.265, 61.2575, 60.91, 62.035, 62.11, 62.8075, 62.6675, 62.3875, 62.0925, 63.0925, 63.2725, 63.6725, 63.225, 63.5725, 64.035, 64.1725, 63.48, 63.2275, 63.935, 64.5875, 64.3275, 64.545, 64.845, 64.4225, 64.73, 64.53, 64.95, 65.435, 65.2125, 65.51, 65.175, 65.0225, 65.425, 65.72, 65.8875, 65.69, 65.665, 66.0975, 66.7625, 65.705, 66.18, 65.955, 66.245, 66.295, 66.45, 66.475, 66.3925, 66.1725, 66.2725, 66.4575, 66.6675, 67.05, 66.825, 66.8975, 66.635, 66.415, 66.5375, 66.6, 66.9]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2050
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0590
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7270
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.0645
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5930
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5010
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0750
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2260
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.6560
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3360
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5485
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4745
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8230
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4995
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7495
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7540
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5265
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5745
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8000
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6950
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.283, Test loss: 5.990, Test accuracy: 25.00
Final Round, Global train loss: 0.283, Global test loss: 1.830, Global test accuracy: 36.45
Average accuracy final 10 rounds: 24.735249999999997 

Average global accuracy final 10 rounds: 35.19375 

3844.011223554611
[1.439725637435913, 2.879451274871826, 4.069550275802612, 5.259649276733398, 6.449251890182495, 7.638854503631592, 8.810436964035034, 9.982019424438477, 11.19034719467163, 12.398674964904785, 13.603553771972656, 14.808432579040527, 16.002712726593018, 17.196992874145508, 18.240825414657593, 19.284657955169678, 20.315175533294678, 21.345693111419678, 22.376420736312866, 23.407148361206055, 24.442850589752197, 25.47855281829834, 26.510735034942627, 27.542917251586914, 28.568559408187866, 29.59420156478882, 30.62482213973999, 31.655442714691162, 32.678650856018066, 33.70185899734497, 34.72815704345703, 35.75445508956909, 36.78863596916199, 37.82281684875488, 38.8487446308136, 39.874672412872314, 40.89932346343994, 41.92397451400757, 42.948086738586426, 43.97219896316528, 44.993180990219116, 46.01416301727295, 47.04643154144287, 48.07870006561279, 49.119043588638306, 50.15938711166382, 51.183411836624146, 52.20743656158447, 53.239540338516235, 54.271644115448, 55.304386377334595, 56.33712863922119, 57.357848167419434, 58.378567695617676, 59.404016971588135, 60.429466247558594, 61.46508860588074, 62.50071096420288, 63.52528357505798, 64.54985618591309, 65.5836181640625, 66.61738014221191, 67.64953899383545, 68.68169784545898, 69.70917749404907, 70.73665714263916, 71.76197743415833, 72.78729772567749, 73.80107688903809, 74.81485605239868, 75.83833193778992, 76.86180782318115, 77.8890585899353, 78.91630935668945, 79.94494724273682, 80.97358512878418, 82.01705718040466, 83.06052923202515, 84.13341283798218, 85.20629644393921, 86.39470934867859, 87.58312225341797, 88.78271961212158, 89.9823169708252, 91.18224930763245, 92.3821816444397, 93.57313871383667, 94.76409578323364, 95.94784903526306, 97.13160228729248, 98.32764601707458, 99.52368974685669, 100.71986484527588, 101.91603994369507, 103.10935592651367, 104.30267190933228, 105.4999144077301, 106.69715690612793, 107.89261555671692, 109.08807420730591, 110.28314638137817, 111.47821855545044, 112.67131471633911, 113.86441087722778, 115.04990315437317, 116.23539543151855, 117.42558884620667, 118.61578226089478, 119.81217837333679, 121.00857448577881, 122.20113754272461, 123.39370059967041, 124.58801555633545, 125.78233051300049, 126.98315024375916, 128.18396997451782, 129.3811912536621, 130.5784125328064, 131.7749924659729, 132.9715723991394, 134.16538906097412, 135.35920572280884, 136.54899263381958, 137.73877954483032, 138.94151163101196, 140.1442437171936, 141.32968282699585, 142.5151219367981, 143.70558500289917, 144.89604806900024, 146.09105563163757, 147.2860631942749, 148.47177290916443, 149.65748262405396, 150.84513592720032, 152.03278923034668, 153.10561323165894, 154.1784372329712, 155.240581035614, 156.30272483825684, 157.3592174053192, 158.4157099723816, 159.43681597709656, 160.45792198181152, 161.48271298408508, 162.50750398635864, 163.53958106040955, 164.57165813446045, 165.5959222316742, 166.62018632888794, 167.64871621131897, 168.67724609375, 169.7024483680725, 170.72765064239502, 171.758159160614, 172.788667678833, 173.82192254066467, 174.85517740249634, 175.881178855896, 176.90718030929565, 177.93877363204956, 178.97036695480347, 180.00265908241272, 181.03495121002197, 182.06470584869385, 183.09446048736572, 184.13572216033936, 185.176983833313, 186.20624423027039, 187.23550462722778, 188.2703151702881, 189.3051257133484, 190.33474159240723, 191.36435747146606, 192.39018297195435, 193.41600847244263, 194.4424033164978, 195.46879816055298, 196.50396370887756, 197.53912925720215, 198.56634855270386, 199.59356784820557, 200.62445282936096, 201.65533781051636, 202.68672037124634, 203.71810293197632, 204.74539947509766, 205.772696018219, 206.8064422607422, 207.84018850326538, 208.86829137802124, 209.8963942527771, 210.92263984680176, 211.94888544082642, 212.98491740226746, 214.0209493637085, 215.04957389831543, 216.07819843292236, 217.11077189445496, 218.14334535598755, 220.22619819641113, 222.30905103683472]
[18.14, 18.14, 22.63, 22.63, 24.8275, 24.8275, 24.08, 24.08, 23.8425, 23.8425, 23.8425, 23.8425, 23.815, 23.815, 24.14, 24.14, 23.9825, 23.9825, 24.8425, 24.8425, 25.1825, 25.1825, 25.5525, 25.5525, 25.65, 25.65, 25.9575, 25.9575, 26.2275, 26.2275, 26.3425, 26.3425, 26.145, 26.145, 25.975, 25.975, 25.9125, 25.9125, 26.1175, 26.1175, 26.455, 26.455, 26.7725, 26.7725, 26.5675, 26.5675, 26.57, 26.57, 26.5925, 26.5925, 26.5225, 26.5225, 26.6975, 26.6975, 26.7925, 26.7925, 26.7275, 26.7275, 26.86, 26.86, 26.575, 26.575, 26.715, 26.715, 26.6275, 26.6275, 26.7575, 26.7575, 26.74, 26.74, 26.3075, 26.3075, 26.17, 26.17, 26.625, 26.625, 26.525, 26.525, 26.255, 26.255, 25.9875, 25.9875, 25.82, 25.82, 26.06, 26.06, 25.95, 25.95, 25.76, 25.76, 25.845, 25.845, 25.6175, 25.6175, 25.8925, 25.8925, 25.775, 25.775, 25.735, 25.735, 25.6425, 25.6425, 25.405, 25.405, 25.1575, 25.1575, 25.2075, 25.2075, 24.93, 24.93, 25.0025, 25.0025, 25.21, 25.21, 24.89, 24.89, 24.9475, 24.9475, 25.0125, 25.0125, 25.195, 25.195, 24.9425, 24.9425, 24.975, 24.975, 24.7275, 24.7275, 24.64, 24.64, 25.0575, 25.0575, 24.8425, 24.8425, 24.6175, 24.6175, 24.84, 24.84, 24.5575, 24.5575, 24.7875, 24.7875, 24.915, 24.915, 25.0925, 25.0925, 24.9575, 24.9575, 24.8975, 24.8975, 24.725, 24.725, 25.055, 25.055, 24.9625, 24.9625, 24.945, 24.945, 24.8375, 24.8375, 24.59, 24.59, 24.69, 24.69, 24.5025, 24.5025, 24.4275, 24.4275, 24.5725, 24.5725, 24.565, 24.565, 24.665, 24.665, 24.7625, 24.7625, 24.885, 24.885, 24.59, 24.59, 24.545, 24.545, 24.7075, 24.7075, 24.9175, 24.9175, 24.7675, 24.7675, 24.4525, 24.4525, 24.57, 24.57, 24.74, 24.74, 24.96, 24.96, 24.94, 24.94, 24.7525, 24.7525, 25.0, 25.0]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5450
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4800
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8150
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4985
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7705
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6810
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5105
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6335
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7920
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7570
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52622 is out of bounds for axis 0 with size 50000
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5480
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4920
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8285
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5345
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7640
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7120
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.6045
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5885
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7910
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6720
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 238, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52738 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5480
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5160
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8160
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5395
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7550
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6930
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.4835
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5535
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7990
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7095
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5565
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4830
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8245
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5085
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7550
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.6950
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.4805
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5915
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7800
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6110
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  16.8600
Round 1 global test acc  20.8100
Traceback (most recent call last):
  File "RFL.py", line 126, in <module>
    w_local, loss_local, f_k = local.train(copy.deepcopy(net_glob).to(args.device), copy.deepcopy(f_G).to(args.device),
  File "/home/ChenSM/code/FL_HLS/util/local_training.py", line 257, in train
    for batch_idx, (images, labels, idxs) in enumerate(self.ldr_train_tmp):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/util/local_training.py", line 48, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 53443 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5470
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4885
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8195
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5440
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7350
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7125
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5935
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.5825
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7875
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6470
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 52309 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5490
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5085
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8275
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.4685
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7665
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7180
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.4440
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6370
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7975
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6975
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8590
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1960
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.1590
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5770
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8000
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.3435
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6865
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7755
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1680
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6055
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5735
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8545
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3990
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.4205
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7190
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4550
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.293, Test loss: 6.339, Test accuracy: 22.28
Final Round, Global train loss: 0.293, Global test loss: 1.978, Global test accuracy: 29.01
Average accuracy final 10 rounds: 22.160750000000004 

Average global accuracy final 10 rounds: 28.644249999999996 

3826.2809324264526
[1.4464459419250488, 2.8928918838500977, 4.085153818130493, 5.277415752410889, 6.475033760070801, 7.672651767730713, 8.87558126449585, 10.078510761260986, 11.282162189483643, 12.485813617706299, 13.68700647354126, 14.88819932937622, 16.084317922592163, 17.280436515808105, 18.466741800308228, 19.65304708480835, 20.853047132492065, 22.05304718017578, 23.234498500823975, 24.415949821472168, 25.606696844100952, 26.797443866729736, 28.000561714172363, 29.20367956161499, 30.404908895492554, 31.606138229370117, 32.80012655258179, 33.99411487579346, 35.187758684158325, 36.38140249252319, 37.574437856674194, 38.767473220825195, 39.9666268825531, 41.165780544281006, 42.19328284263611, 43.22078514099121, 44.251548528671265, 45.28231191635132, 46.31514286994934, 47.34797382354736, 48.376192569732666, 49.40441131591797, 50.43646717071533, 51.468523025512695, 52.50281119346619, 53.53709936141968, 54.56498312950134, 55.59286689758301, 56.621416330337524, 57.64996576309204, 58.68178033828735, 59.713594913482666, 60.744229316711426, 61.774863719940186, 62.80884313583374, 63.842822551727295, 64.87239980697632, 65.90197706222534, 66.92852401733398, 67.95507097244263, 68.98311161994934, 70.01115226745605, 71.04491448402405, 72.07867670059204, 73.10623240470886, 74.13378810882568, 75.16181778907776, 76.18984746932983, 77.21890330314636, 78.24795913696289, 79.27304720878601, 80.29813528060913, 81.32480931282043, 82.35148334503174, 83.37952995300293, 84.40757656097412, 85.43534445762634, 86.46311235427856, 87.49085521697998, 88.5185980796814, 89.54364061355591, 90.56868314743042, 91.59391570091248, 92.61914825439453, 93.6488573551178, 94.67856645584106, 95.71154522895813, 96.7445240020752, 97.77138066291809, 98.79823732376099, 99.82846307754517, 100.85868883132935, 101.88886213302612, 102.9190354347229, 103.94536256790161, 104.97168970108032, 106.16253209114075, 107.35337448120117, 108.5448067188263, 109.73623895645142, 110.9290337562561, 112.12182855606079, 113.31933331489563, 114.51683807373047, 115.70842933654785, 116.90002059936523, 118.08940601348877, 119.2787914276123, 120.46606874465942, 121.65334606170654, 122.84237337112427, 124.03140068054199, 125.2289686203003, 126.4265365600586, 127.6153290271759, 128.8041214942932, 129.99852323532104, 131.19292497634888, 132.38421297073364, 133.5755009651184, 134.76689624786377, 135.95829153060913, 137.1516716480255, 138.3450517654419, 139.53408694267273, 140.72312211990356, 141.91483688354492, 143.10655164718628, 144.30113863945007, 145.49572563171387, 146.68932342529297, 147.88292121887207, 149.07564067840576, 150.26836013793945, 151.4722559452057, 152.67615175247192, 153.87274265289307, 155.0693335533142, 156.26287746429443, 157.45642137527466, 158.65050220489502, 159.84458303451538, 161.0416977405548, 162.23881244659424, 163.43494510650635, 164.63107776641846, 165.82725071907043, 167.0234236717224, 168.21656012535095, 169.4096965789795, 170.60240983963013, 171.79512310028076, 172.98788833618164, 174.18065357208252, 175.3699221611023, 176.55919075012207, 177.75316071510315, 178.94713068008423, 180.14244389533997, 181.3377571105957, 182.53581428527832, 183.73387145996094, 184.92892980575562, 186.1239881515503, 187.31460285186768, 188.50521755218506, 189.69925594329834, 190.89329433441162, 192.09009718894958, 193.28690004348755, 194.47978115081787, 195.6726622581482, 196.86415243148804, 198.05564260482788, 199.24739503860474, 200.4391474723816, 201.6341414451599, 202.82913541793823, 204.02361679077148, 205.21809816360474, 206.4153027534485, 207.61250734329224, 208.8046576976776, 209.996808052063, 211.27645564079285, 212.5561032295227, 213.80921173095703, 215.06232023239136, 216.31655931472778, 217.5707983970642, 218.83748865127563, 220.10417890548706, 221.36965012550354, 222.63512134552002, 223.90025997161865, 225.16539859771729, 226.44875693321228, 227.73211526870728, 229.00446343421936, 230.27681159973145, 232.91477465629578, 235.5527377128601]
[19.5175, 19.5175, 20.045, 20.045, 21.92, 21.92, 21.2, 21.2, 22.505, 22.505, 22.435, 22.435, 22.7225, 22.7225, 23.6025, 23.6025, 23.3775, 23.3775, 23.075, 23.075, 23.535, 23.535, 23.4325, 23.4325, 23.675, 23.675, 23.525, 23.525, 23.595, 23.595, 24.41, 24.41, 24.5175, 24.5175, 24.4475, 24.4475, 24.5275, 24.5275, 24.7875, 24.7875, 24.4825, 24.4825, 24.5275, 24.5275, 24.4925, 24.4925, 24.71, 24.71, 24.51, 24.51, 24.335, 24.335, 24.3825, 24.3825, 24.4925, 24.4925, 24.595, 24.595, 24.1975, 24.1975, 23.905, 23.905, 23.7875, 23.7875, 23.8675, 23.8675, 24.1375, 24.1375, 24.14, 24.14, 24.095, 24.095, 24.0225, 24.0225, 23.74, 23.74, 23.6975, 23.6975, 23.295, 23.295, 23.23, 23.23, 23.3, 23.3, 23.0625, 23.0625, 22.885, 22.885, 23.205, 23.205, 23.03, 23.03, 23.035, 23.035, 23.125, 23.125, 23.1975, 23.1975, 23.1375, 23.1375, 22.705, 22.705, 22.96, 22.96, 22.7, 22.7, 22.2125, 22.2125, 22.3075, 22.3075, 22.18, 22.18, 21.835, 21.835, 22.145, 22.145, 21.8725, 21.8725, 21.745, 21.745, 21.82, 21.82, 22.0275, 22.0275, 22.3925, 22.3925, 22.015, 22.015, 21.8775, 21.8775, 22.215, 22.215, 22.035, 22.035, 22.05, 22.05, 22.2525, 22.2525, 22.48, 22.48, 22.4475, 22.4475, 22.2375, 22.2375, 21.85, 21.85, 21.7275, 21.7275, 21.6775, 21.6775, 21.8275, 21.8275, 21.8375, 21.8375, 21.8525, 21.8525, 21.8275, 21.8275, 21.78, 21.78, 21.795, 21.795, 21.9725, 21.9725, 21.845, 21.845, 21.8175, 21.8175, 21.3875, 21.3875, 21.795, 21.795, 22.0, 22.0, 21.9275, 21.9275, 21.8875, 21.8875, 22.0925, 22.0925, 22.115, 22.115, 22.3325, 22.3325, 22.3125, 22.3125, 22.385, 22.385, 22.3675, 22.3675, 22.1275, 22.1275, 21.98, 21.98, 21.8775, 21.8775, 22.165, 22.165, 21.945, 21.945, 22.2825, 22.2825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8535
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2555
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0675
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5710
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7960
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1070
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6730
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7625
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2125
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5995
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5195
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8630
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.1830
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3335
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7160
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.5665
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.625, Test loss: 2.586, Test accuracy: 44.61
Final Round, Global train loss: 0.625, Global test loss: 1.469, Global test accuracy: 57.38
Average accuracy final 10 rounds: 45.870250000000006 

Average global accuracy final 10 rounds: 56.226 

3751.4729454517365
[1.5056233406066895, 3.011246681213379, 4.2501280307769775, 5.489009380340576, 6.735947847366333, 7.98288631439209, 9.227727890014648, 10.472569465637207, 11.714467763900757, 12.956366062164307, 14.20319414138794, 15.450022220611572, 16.694219827651978, 17.938417434692383, 19.1867356300354, 20.435053825378418, 21.679521322250366, 22.923988819122314, 24.17101001739502, 25.418031215667725, 26.670897006988525, 27.923762798309326, 29.166789293289185, 30.409815788269043, 31.65851378440857, 32.907211780548096, 34.15625524520874, 35.405298709869385, 36.65734934806824, 37.90939998626709, 39.16477823257446, 40.420156478881836, 41.67955684661865, 42.93895721435547, 44.19328832626343, 45.44761943817139, 46.693562269210815, 47.939505100250244, 49.193105697631836, 50.44670629501343, 51.70250082015991, 52.9582953453064, 54.205947399139404, 55.45359945297241, 56.706878423690796, 57.96015739440918, 59.20632553100586, 60.45249366760254, 61.69247817993164, 62.93246269226074, 64.1745834350586, 65.41670417785645, 66.66302752494812, 67.9093508720398, 69.15384078025818, 70.39833068847656, 71.64371705055237, 72.88910341262817, 74.1360297203064, 75.38295602798462, 76.62615752220154, 77.86935901641846, 79.11500239372253, 80.36064577102661, 81.60809803009033, 82.85555028915405, 84.0997896194458, 85.34402894973755, 86.59444665908813, 87.84486436843872, 89.09158134460449, 90.33829832077026, 91.58298063278198, 92.8276629447937, 94.07740259170532, 95.32714223861694, 96.57841277122498, 97.82968330383301, 99.07554793357849, 100.32141256332397, 101.57283139228821, 102.82425022125244, 104.06809163093567, 105.3119330406189, 106.56169009208679, 107.81144714355469, 109.06290078163147, 110.31435441970825, 111.56284165382385, 112.81132888793945, 114.06293535232544, 115.31454181671143, 116.5640082359314, 117.81347465515137, 119.05415201187134, 120.29482936859131, 121.53985357284546, 122.78487777709961, 124.02232122421265, 125.25976467132568, 126.49487161636353, 127.72997856140137, 128.97332215309143, 130.2166657447815, 131.4582531452179, 132.6998405456543, 133.95444297790527, 135.20904541015625, 136.45525979995728, 137.7014741897583, 138.95061230659485, 140.1997504234314, 141.44365072250366, 142.68755102157593, 143.93092322349548, 145.17429542541504, 146.4240484237671, 147.67380142211914, 148.92469930648804, 150.17559719085693, 151.42116332054138, 152.66672945022583, 153.91189336776733, 155.15705728530884, 156.40680766105652, 157.6565580368042, 158.90761804580688, 160.15867805480957, 161.40285968780518, 162.64704132080078, 163.89807677268982, 165.14911222457886, 166.39480233192444, 167.64049243927002, 168.88833475112915, 170.13617706298828, 171.39133405685425, 172.64649105072021, 173.89257955551147, 175.13866806030273, 176.3867952823639, 177.63492250442505, 178.8875710964203, 180.14021968841553, 181.38764762878418, 182.63507556915283, 183.88405537605286, 185.13303518295288, 186.38077640533447, 187.62851762771606, 188.86861753463745, 190.10871744155884, 191.35888767242432, 192.6090579032898, 193.8491563796997, 195.08925485610962, 196.32939314842224, 197.56953144073486, 198.8339238166809, 200.09831619262695, 201.340656042099, 202.58299589157104, 203.8247106075287, 205.06642532348633, 206.31218886375427, 207.55795240402222, 208.80708861351013, 210.05622482299805, 211.3053011894226, 212.55437755584717, 213.80289959907532, 215.05142164230347, 216.29005551338196, 217.52868938446045, 218.77653074264526, 220.02437210083008, 221.26443934440613, 222.50450658798218, 223.76151418685913, 225.01852178573608, 226.26521015167236, 227.51189851760864, 228.75262260437012, 229.9933466911316, 231.24717783927917, 232.50100898742676, 233.7472071647644, 234.99340534210205, 236.2454538345337, 237.49750232696533, 238.74273490905762, 239.9879674911499, 241.23038935661316, 242.47281122207642, 243.72796440124512, 244.98311758041382, 246.24123811721802, 247.49935865402222, 248.74109029769897, 249.98282194137573, 252.47224760055542, 254.9616732597351]
[20.575, 20.575, 25.43, 25.43, 27.0175, 27.0175, 30.9625, 30.9625, 31.45, 31.45, 32.9325, 32.9325, 33.21, 33.21, 34.3975, 34.3975, 35.2475, 35.2475, 35.0975, 35.0975, 35.68, 35.68, 36.6075, 36.6075, 37.9425, 37.9425, 38.825, 38.825, 39.2575, 39.2575, 39.97, 39.97, 39.9825, 39.9825, 40.7675, 40.7675, 41.0025, 41.0025, 41.51, 41.51, 41.5425, 41.5425, 41.8625, 41.8625, 42.8725, 42.8725, 42.51, 42.51, 43.1475, 43.1475, 43.31, 43.31, 43.2325, 43.2325, 43.165, 43.165, 43.115, 43.115, 43.6, 43.6, 43.7725, 43.7725, 44.29, 44.29, 44.71, 44.71, 44.9825, 44.9825, 45.415, 45.415, 45.205, 45.205, 45.1675, 45.1675, 45.1875, 45.1875, 45.36, 45.36, 45.285, 45.285, 45.2, 45.2, 45.38, 45.38, 45.0825, 45.0825, 45.3825, 45.3825, 45.4175, 45.4175, 45.28, 45.28, 45.58, 45.58, 45.2925, 45.2925, 45.5825, 45.5825, 45.155, 45.155, 45.0975, 45.0975, 45.2425, 45.2425, 45.295, 45.295, 45.295, 45.295, 45.375, 45.375, 45.3525, 45.3525, 45.6425, 45.6425, 45.7225, 45.7225, 45.845, 45.845, 45.9275, 45.9275, 46.0925, 46.0925, 45.6875, 45.6875, 45.52, 45.52, 45.5775, 45.5775, 45.5175, 45.5175, 45.7325, 45.7325, 46.0675, 46.0675, 45.63, 45.63, 45.675, 45.675, 45.74, 45.74, 45.36, 45.36, 45.4375, 45.4375, 45.1525, 45.1525, 45.61, 45.61, 45.835, 45.835, 45.9725, 45.9725, 45.375, 45.375, 45.2425, 45.2425, 45.2525, 45.2525, 45.1775, 45.1775, 45.6575, 45.6575, 45.5625, 45.5625, 45.7425, 45.7425, 45.3275, 45.3275, 45.2375, 45.2375, 45.05, 45.05, 45.3725, 45.3725, 45.52, 45.52, 45.495, 45.495, 45.59, 45.59, 45.775, 45.775, 45.875, 45.875, 45.6325, 45.6325, 45.8125, 45.8125, 45.8375, 45.8375, 45.57, 45.57, 45.925, 45.925, 46.0025, 46.0025, 46.095, 46.095, 46.1775, 46.1775, 44.6125, 44.6125]
/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8505
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2015
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0655
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5725
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7840
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1760
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7210
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7730
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.4495
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.5845
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4690
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8715
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.0875
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3400
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7320
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4405
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.767, Test loss: 2.417, Test accuracy: 42.45
Final Round, Global train loss: 0.767, Global test loss: 1.544, Global test accuracy: 53.17
Average accuracy final 10 rounds: 42.81425 

Average global accuracy final 10 rounds: 54.4665 

3809.850860595703
[1.5070743560791016, 3.014148712158203, 4.283578157424927, 5.55300760269165, 6.832696914672852, 8.112386226654053, 9.39608883857727, 10.679791450500488, 11.960596084594727, 13.241400718688965, 14.502647876739502, 15.763895034790039, 17.047094583511353, 18.330294132232666, 19.618821144104004, 20.907348155975342, 22.18991780281067, 23.472487449645996, 24.75062084197998, 26.028754234313965, 27.305274486541748, 28.58179473876953, 29.847382307052612, 31.112969875335693, 32.391521692276, 33.67007350921631, 34.94673442840576, 36.223395347595215, 37.5049262046814, 38.78645706176758, 40.060418128967285, 41.33437919616699, 42.60302948951721, 43.87167978286743, 45.13487672805786, 46.39807367324829, 47.68002724647522, 48.96198081970215, 50.24472260475159, 51.527464389801025, 52.79211163520813, 54.056758880615234, 55.275107622146606, 56.49345636367798, 57.71123814582825, 58.929019927978516, 60.160518646240234, 61.39201736450195, 62.61804723739624, 63.84407711029053, 65.07578873634338, 66.30750036239624, 67.54319405555725, 68.77888774871826, 70.0133113861084, 71.24773502349854, 72.51809978485107, 73.78846454620361, 75.06347370147705, 76.33848285675049, 77.61337423324585, 78.88826560974121, 80.18038010597229, 81.47249460220337, 82.74302744865417, 84.01356029510498, 85.30008268356323, 86.58660507202148, 87.86831045150757, 89.15001583099365, 90.43078064918518, 91.71154546737671, 92.99212884902954, 94.27271223068237, 95.54914045333862, 96.82556867599487, 98.1157398223877, 99.40591096878052, 100.69376182556152, 101.98161268234253, 103.2672872543335, 104.55296182632446, 105.7917251586914, 107.03048849105835, 108.26306176185608, 109.49563503265381, 110.73165559768677, 111.96767616271973, 113.23954629898071, 114.5114164352417, 115.79050517082214, 117.06959390640259, 118.30321478843689, 119.53683567047119, 120.80789875984192, 122.07896184921265, 123.30696058273315, 124.53495931625366, 125.76414442062378, 126.9933295249939, 128.26518654823303, 129.53704357147217, 130.7910716533661, 132.04509973526, 133.3200261592865, 134.594952583313, 135.6984429359436, 136.80193328857422, 137.9038107395172, 139.0056881904602, 140.11052823066711, 141.21536827087402, 142.32974791526794, 143.44412755966187, 144.54798865318298, 145.6518497467041, 146.75720524787903, 147.86256074905396, 148.97369694709778, 150.0848331451416, 151.19162130355835, 152.2984094619751, 153.40809035301208, 154.51777124404907, 155.6215205192566, 156.7252697944641, 157.8328094482422, 158.94034910202026, 160.04967522621155, 161.15900135040283, 162.26269149780273, 163.36638164520264, 164.47841691970825, 165.59045219421387, 166.69462656974792, 167.79880094528198, 168.90400290489197, 170.00920486450195, 171.13073420524597, 172.25226354599, 173.36391425132751, 174.47556495666504, 175.5892791748047, 176.70299339294434, 177.81810760498047, 178.9332218170166, 180.04486417770386, 181.1565065383911, 182.26931262016296, 183.38211870193481, 184.49903178215027, 185.61594486236572, 186.73481345176697, 187.8536820411682, 188.9634234905243, 190.07316493988037, 191.17935132980347, 192.28553771972656, 193.39895915985107, 194.5123805999756, 195.6292130947113, 196.74604558944702, 197.85779309272766, 198.9695405960083, 200.0767056941986, 201.18387079238892, 202.30864787101746, 203.433424949646, 204.54383730888367, 205.65424966812134, 206.76628279685974, 207.87831592559814, 208.99396800994873, 210.10962009429932, 211.22228503227234, 212.33494997024536, 213.44903707504272, 214.5631241798401, 215.6687092781067, 216.7742943763733, 217.88718962669373, 219.00008487701416, 220.11264181137085, 221.22519874572754, 222.33445858955383, 223.44371843338013, 224.5521228313446, 225.66052722930908, 226.77311182022095, 227.8856964111328, 228.99622178077698, 230.10674715042114, 231.22494196891785, 232.34313678741455, 233.46520519256592, 234.58727359771729, 235.70467400550842, 236.82207441329956, 237.93735933303833, 239.0526442527771, 241.2795853614807, 243.50652647018433]
[17.3825, 17.3825, 21.1325, 21.1325, 23.8, 23.8, 26.57, 26.57, 27.6425, 27.6425, 28.4175, 28.4175, 29.14, 29.14, 30.15, 30.15, 31.62, 31.62, 30.5025, 30.5025, 32.1475, 32.1475, 32.3225, 32.3225, 33.665, 33.665, 34.6625, 34.6625, 35.5825, 35.5825, 36.4375, 36.4375, 37.2025, 37.2025, 37.2725, 37.2725, 38.425, 38.425, 38.6375, 38.6375, 39.2525, 39.2525, 39.76, 39.76, 40.6075, 40.6075, 40.515, 40.515, 40.85, 40.85, 41.38, 41.38, 41.525, 41.525, 41.8525, 41.8525, 42.235, 42.235, 41.7825, 41.7825, 41.6775, 41.6775, 41.29, 41.29, 41.8, 41.8, 41.935, 41.935, 41.9575, 41.9575, 42.3925, 42.3925, 42.6875, 42.6875, 42.685, 42.685, 42.54, 42.54, 42.6925, 42.6925, 42.1025, 42.1025, 42.085, 42.085, 42.44, 42.44, 43.1075, 43.1075, 42.8675, 42.8675, 43.1225, 43.1225, 43.51, 43.51, 43.4525, 43.4525, 43.5175, 43.5175, 43.655, 43.655, 43.5575, 43.5575, 43.31, 43.31, 43.075, 43.075, 43.495, 43.495, 43.9125, 43.9125, 43.7425, 43.7425, 43.575, 43.575, 44.025, 44.025, 43.9, 43.9, 43.5375, 43.5375, 43.2625, 43.2625, 43.335, 43.335, 43.125, 43.125, 43.8675, 43.8675, 43.29, 43.29, 43.25, 43.25, 43.7125, 43.7125, 43.24, 43.24, 43.4125, 43.4125, 43.35, 43.35, 43.25, 43.25, 43.4875, 43.4875, 43.6075, 43.6075, 43.4575, 43.4575, 43.2225, 43.2225, 43.1525, 43.1525, 43.2075, 43.2075, 43.11, 43.11, 43.3525, 43.3525, 44.0, 44.0, 43.385, 43.385, 43.045, 43.045, 42.795, 42.795, 42.855, 42.855, 42.94, 42.94, 42.675, 42.675, 42.65, 42.65, 42.8225, 42.8225, 43.15, 43.15, 43.195, 43.195, 42.83, 42.83, 42.9175, 42.9175, 43.135, 43.135, 42.7975, 42.7975, 42.7275, 42.7275, 43.0125, 43.0125, 42.7075, 42.7075, 42.46, 42.46, 42.665, 42.665, 42.89, 42.89, 42.4525, 42.4525]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_co_teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8590
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.2015
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.2195
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6160
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8015
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0710
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6715
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7405
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1595
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6965
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.4980
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8680
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.2345
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2260
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7105
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4445
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_co_teaching.py", line 247, in <module>
    local.filter_data(net=net_local.to(args.device), net2=net_local2.to(args.device), concept_matrix_local = concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in filter_data
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1433, in <listcomp>
    filter_idxs1 = [sort_distance_tmp1[i][0] for i in range(math.floor(self.args.shard_per_user * self.args.nums_per_class * 0.9))]
IndexError: list index out of range
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8605
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1905
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.1430
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.6440
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.8065
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0930
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6775
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7620
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.3095
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6270
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5975
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8670
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3740
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2680
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7070
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.4130
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  29.5400
Round 1 global test acc  34.2300
Round 2 global test acc  38.2600
Round 3 global test acc  42.6600
Round 4 global test acc  45.2400
Round 5 global test acc  46.7000
Round 6 global test acc  47.1800
Round 7 global test acc  51.2700
Round 8 global test acc  49.4200
Round 9 global test acc  52.8900
Round 10 global test acc  48.6200
Round 11 global test acc  50.7300
Round 12 global test acc  49.7800
Round 13 global test acc  54.9900
Round 14 global test acc  49.2900
Round 15 global test acc  54.8500
Round 16 global test acc  56.0900
Round 17 global test acc  53.3100
Round 18 global test acc  56.0800
Round 19 global test acc  55.5600
Round 20 global test acc  54.9500
Round 21 global test acc  55.5300
Round 22 global test acc  56.8200
Round 23 global test acc  54.5600
Round 24 global test acc  56.2300
Round 25 global test acc  57.0200
Round 26 global test acc  58.3700
Round 27 global test acc  59.0400
Round 28 global test acc  56.8100
Round 29 global test acc  59.5500
Round 30 global test acc  58.2900
Round 31 global test acc  57.2200
Round 32 global test acc  57.3400
Round 33 global test acc  59.0700
Round 34 global test acc  59.1900
Round 35 global test acc  60.1600
Round 36 global test acc  58.3100
Round 37 global test acc  57.6000
Round 38 global test acc  57.4300
Round 39 global test acc  59.7400
Round 40 global test acc  60.0400
Round 41 global test acc  60.3400
Round 42 global test acc  60.0000
Round 43 global test acc  57.7800
Round 44 global test acc  55.3100
Round 45 global test acc  61.1700
Round 46 global test acc  57.8900
Round 47 global test acc  58.8100
Round 48 global test acc  60.3400
Round 49 global test acc  58.9600
Round 50 global test acc  60.4900
Round 51 global test acc  60.4000
Round 52 global test acc  61.6900
Round 53 global test acc  58.8800
Round 54 global test acc  62.6000
Round 55 global test acc  60.6700
Round 56 global test acc  59.4800
Round 57 global test acc  61.1200
Round 58 global test acc  60.2800
Round 59 global test acc  63.2200
Round 60 global test acc  60.5800
Round 61 global test acc  61.4100
Round 62 global test acc  60.8500
Round 63 global test acc  61.8800
Round 64 global test acc  63.5100
Round 65 global test acc  59.9600
Round 66 global test acc  63.7500
Round 67 global test acc  59.6600
Round 68 global test acc  62.3100
Round 69 global test acc  61.9800
Round 70 global test acc  62.4500
Round 71 global test acc  61.8000
Round 72 global test acc  60.4400
Round 73 global test acc  60.1200
Round 74 global test acc  62.1200
Round 75 global test acc  61.3300
Round 76 global test acc  60.8100
Round 77 global test acc  59.7200
Round 78 global test acc  60.8000
Round 79 global test acc  60.5600
Round 80 global test acc  62.7000
Round 81 global test acc  62.2600
Round 82 global test acc  61.2800
Round 83 global test acc  60.0900
Round 84 global test acc  60.0400
Round 85 global test acc  58.6900
Round 86 global test acc  58.2000
Round 87 global test acc  57.8400
Round 88 global test acc  57.6500
Round 89 global test acc  57.3200
Round 90 global test acc  56.5100
Round 91 global test acc  56.5400
Round 92 global test acc  56.7100
Round 93 global test acc  56.0700
Round 94 global test acc  55.0900
Round 95 global test acc  55.6500
Round 96 global test acc  55.4100
Round 97 global test acc  55.8500
Round 98 global test acc  55.0300
Round 99 global test acc  55.6000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8625
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1930
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.0650
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5760
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7915
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.0715
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.7275
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7660
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.1465
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6550
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5650
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8610
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.2260
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.3530
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7115
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.7165
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Final Round, Train loss: 0.542, Test loss: 1.027, Test accuracy: 66.37
Average accuracy final 10 rounds: 65.86949999999999
1896.489592075348
[1.718017578125, 3.102440357208252, 4.490554332733154, 5.894584894180298, 7.265548229217529, 8.647363185882568, 9.975440502166748, 11.277823448181152, 12.598231077194214, 13.90581226348877, 15.217212200164795, 16.59109902381897, 17.98841643333435, 19.334319829940796, 20.66502857208252, 22.05969738960266, 23.44347834587097, 24.832438230514526, 26.210824966430664, 27.59810161590576, 28.98257327079773, 30.37893319129944, 31.75857663154602, 33.15029311180115, 34.543145418167114, 35.92668628692627, 37.3211088180542, 38.716620445251465, 40.09744334220886, 41.472604751586914, 42.867063999176025, 44.24536681175232, 45.633132219314575, 47.03359389305115, 48.41504955291748, 49.806748151779175, 51.18897366523743, 52.58297634124756, 53.97079944610596, 55.35341119766235, 56.75467658042908, 58.13215446472168, 59.50946521759033, 60.90104699134827, 62.260497093200684, 63.63868546485901, 65.03589916229248, 66.4107882976532, 67.79169178009033, 69.16249108314514, 70.53726577758789, 71.90923595428467, 73.29409289360046, 74.66973948478699, 76.0322642326355, 77.42190766334534, 78.79668307304382, 80.17597150802612, 81.56504583358765, 82.94435477256775, 84.32497549057007, 85.71402668952942, 87.06541204452515, 88.44731044769287, 89.82366299629211, 91.18593239784241, 92.56446981430054, 93.96230626106262, 95.31320524215698, 96.69122457504272, 98.07763242721558, 99.43833661079407, 100.81082201004028, 102.19786810874939, 103.55367946624756, 104.9310929775238, 106.31539249420166, 107.69248747825623, 109.0683479309082, 110.44506669044495, 111.82139348983765, 113.19840717315674, 114.57862901687622, 115.95391488075256, 117.32475185394287, 118.72491884231567, 120.11146068572998, 121.48123717308044, 122.87042808532715, 124.26038932800293, 125.63611030578613, 127.02452325820923, 128.4011948108673, 129.78051471710205, 131.15917468070984, 132.54694628715515, 133.92468738555908, 135.31219005584717, 136.7039499282837, 138.07823538780212, 140.163494348526]
[19.715, 26.165, 30.97, 33.005, 35.555, 38.52, 39.4375, 41.7925, 42.86, 43.1525, 45.0275, 46.78, 47.0775, 48.02, 48.49, 50.4, 50.3225, 51.5375, 51.805, 52.2325, 52.86, 52.64, 52.6, 52.2525, 53.6575, 54.8575, 55.9775, 56.4925, 56.31, 56.4425, 56.865, 57.48, 58.0025, 58.4125, 57.68, 58.675, 59.25, 59.025, 59.6275, 59.79, 60.2875, 61.275, 61.28, 61.7525, 61.7925, 62.63, 62.79, 62.6725, 63.005, 62.8375, 63.0975, 63.1575, 62.9325, 63.1225, 63.305, 63.255, 62.94, 63.3425, 64.0425, 64.125, 64.41, 64.4175, 64.535, 64.2725, 63.99, 64.235, 64.0575, 64.24, 64.1075, 64.0025, 64.0875, 64.215, 64.6475, 64.5625, 64.6775, 64.8075, 64.505, 64.775, 64.775, 64.365, 64.935, 65.3975, 65.1625, 65.2825, 65.545, 65.885, 65.99, 65.86, 65.8, 65.51, 65.715, 65.6775, 65.8225, 65.8125, 65.97, 65.725, 65.7675, 66.235, 66.0725, 65.8975, 66.3725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9491 (0.8542), real noise ratio: 0.8485
Client 1, noise level: 0.2179 (0.1961), real noise ratio: 0.1955
Client 5, noise level: 0.0651 (0.0586), real noise ratio: 0.1880
Client 6, noise level: 0.6298 (0.5668), real noise ratio: 0.5760
Client 7, noise level: 0.8738 (0.7864), real noise ratio: 0.7955
Client 8, noise level: 0.0087 (0.0078), real noise ratio: 0.1895
Client 9, noise level: 0.7466 (0.6719), real noise ratio: 0.6915
Client 10, noise level: 0.8128 (0.7316), real noise ratio: 0.7500
Client 11, noise level: 0.0757 (0.0681), real noise ratio: 0.2165
Client 12, noise level: 0.6565 (0.5908), real noise ratio: 0.6175
Client 14, noise level: 0.4799 (0.4319), real noise ratio: 0.5780
Client 15, noise level: 0.9556 (0.8600), real noise ratio: 0.8415
Client 16, noise level: 0.0000 (0.0000), real noise ratio: 0.3795
Client 17, noise level: 0.2470 (0.2223), real noise ratio: 0.2810
Client 18, noise level: 0.7122 (0.6410), real noise ratio: 0.7430
Client 19, noise level: 0.3246 (0.2921), real noise ratio: 0.3505
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 63, in <module>
    rand_set_all = np.load('data/sample/rand_set_all.npy', allow_pickle=True)
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/numpy/lib/npyio.py", line 417, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'data/sample/rand_set_all.npy'
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8715
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5415
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.4665
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7330
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8460
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5570
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8120
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8165
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.5490
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7985
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7355
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.9015
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5530
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6115
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.8205
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.7585
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.379, Test loss: 6.937, Test accuracy: 16.21
Final Round, Global train loss: 0.379, Global test loss: 2.125, Global test accuracy: 21.86
Average accuracy final 10 rounds: 16.202249999999996 

Average global accuracy final 10 rounds: 19.78 

3863.186411857605
[1.501359224319458, 3.002718448638916, 4.2678444385528564, 5.532970428466797, 6.80104923248291, 8.069128036499023, 9.351171731948853, 10.633215427398682, 11.906213283538818, 13.179211139678955, 14.489233493804932, 15.799255847930908, 17.07257103919983, 18.34588623046875, 19.645201683044434, 20.944517135620117, 22.22214388847351, 23.499770641326904, 24.792673349380493, 26.085576057434082, 27.360023498535156, 28.63447093963623, 29.900633811950684, 31.166796684265137, 32.43725252151489, 33.70770835876465, 34.98573708534241, 36.263765811920166, 37.528847217559814, 38.79392862319946, 40.08108925819397, 41.36824989318848, 42.63888454437256, 43.90951919555664, 45.176559925079346, 46.44360065460205, 47.717214584350586, 48.99082851409912, 50.26288342475891, 51.5349383354187, 52.80485439300537, 54.07477045059204, 55.34996318817139, 56.62515592575073, 57.90046310424805, 59.17577028274536, 60.439603328704834, 61.70343637466431, 62.971585273742676, 64.23973417282104, 65.51290082931519, 66.78606748580933, 68.05672907829285, 69.32739067077637, 70.59079194068909, 71.8541932106018, 73.11678552627563, 74.37937784194946, 75.63745975494385, 76.89554166793823, 78.15060496330261, 79.40566825866699, 80.6624550819397, 81.9192419052124, 83.17176055908203, 84.42427921295166, 85.6790099143982, 86.93374061584473, 88.18996715545654, 89.44619369506836, 90.72586607933044, 92.00553846359253, 93.2650294303894, 94.52452039718628, 95.76984310150146, 97.01516580581665, 98.22157144546509, 99.42797708511353, 100.63357830047607, 101.83917951583862, 103.03963208198547, 104.24008464813232, 105.44083881378174, 106.64159297943115, 107.84186840057373, 109.04214382171631, 110.2451331615448, 111.44812250137329, 112.64840316772461, 113.84868383407593, 115.05640959739685, 116.26413536071777, 117.47135019302368, 118.67856502532959, 119.87728500366211, 121.07600498199463, 122.286376953125, 123.49674892425537, 124.70011878013611, 125.90348863601685, 127.10451459884644, 128.30554056167603, 129.51122307777405, 130.71690559387207, 131.92398762702942, 133.13106966018677, 134.33374691009521, 135.53642416000366, 136.74565601348877, 137.95488786697388, 139.15515851974487, 140.35542917251587, 141.56188678741455, 142.76834440231323, 143.97730040550232, 145.1862564086914, 146.40420842170715, 147.6221604347229, 148.8193163871765, 150.01647233963013, 151.232745885849, 152.44901943206787, 153.65776252746582, 154.86650562286377, 156.07118248939514, 157.2758593559265, 158.48380208015442, 159.69174480438232, 160.8910722732544, 162.09039974212646, 163.29542994499207, 164.50046014785767, 165.70417714118958, 166.90789413452148, 168.1086745262146, 169.30945491790771, 170.51805782318115, 171.7266607284546, 172.9327940940857, 174.1389274597168, 175.3439838886261, 176.5490403175354, 177.75605463981628, 178.96306896209717, 180.16934180259705, 181.37561464309692, 182.5827031135559, 183.7897915840149, 184.99969029426575, 186.2095890045166, 187.41315174102783, 188.61671447753906, 189.77497601509094, 190.93323755264282, 191.973623752594, 193.01400995254517, 194.05236387252808, 195.090717792511, 196.12876200675964, 197.1668062210083, 198.2009162902832, 199.2350263595581, 200.2656605243683, 201.29629468917847, 202.32914018630981, 203.36198568344116, 204.39152026176453, 205.4210548400879, 206.45739793777466, 207.49374103546143, 208.52541208267212, 209.5570831298828, 210.59666323661804, 211.63624334335327, 212.6730399131775, 213.7098364830017, 214.74079394340515, 215.7717514038086, 216.8112394809723, 217.850727558136, 218.88250637054443, 219.91428518295288, 220.9578821659088, 222.00147914886475, 223.04390144348145, 224.08632373809814, 225.12631487846375, 226.16630601882935, 227.21167516708374, 228.25704431533813, 229.28989362716675, 230.32274293899536, 231.3675239086151, 232.41230487823486, 233.44840812683105, 234.48451137542725, 235.5234785079956, 236.56244564056396, 237.60877346992493, 238.6551012992859, 240.7410490512848, 242.8269968032837]
[11.2575, 11.2575, 15.475, 15.475, 15.255, 15.255, 16.3425, 16.3425, 16.895, 16.895, 16.8475, 16.8475, 16.375, 16.375, 16.1225, 16.1225, 16.3975, 16.3975, 17.0825, 17.0825, 16.96, 16.96, 17.1375, 17.1375, 17.5725, 17.5725, 17.5925, 17.5925, 17.42, 17.42, 17.645, 17.645, 17.58, 17.58, 17.89, 17.89, 18.0725, 18.0725, 18.135, 18.135, 18.08, 18.08, 18.1525, 18.1525, 18.185, 18.185, 18.105, 18.105, 18.08, 18.08, 17.875, 17.875, 17.66, 17.66, 17.745, 17.745, 17.86, 17.86, 17.79, 17.79, 18.015, 18.015, 17.8325, 17.8325, 17.555, 17.555, 17.4675, 17.4675, 17.515, 17.515, 17.885, 17.885, 17.7925, 17.7925, 17.565, 17.565, 17.345, 17.345, 17.3925, 17.3925, 17.195, 17.195, 17.3775, 17.3775, 17.3075, 17.3075, 16.975, 16.975, 16.6625, 16.6625, 16.8875, 16.8875, 17.1775, 17.1775, 16.8875, 16.8875, 16.8375, 16.8375, 16.6625, 16.6625, 16.575, 16.575, 16.6225, 16.6225, 16.52, 16.52, 16.945, 16.945, 16.8825, 16.8825, 17.105, 17.105, 16.95, 16.95, 16.585, 16.585, 16.5275, 16.5275, 16.5575, 16.5575, 16.75, 16.75, 16.56, 16.56, 16.4875, 16.4875, 16.255, 16.255, 16.275, 16.275, 16.165, 16.165, 16.335, 16.335, 16.3825, 16.3825, 16.4175, 16.4175, 16.2825, 16.2825, 16.25, 16.25, 16.3875, 16.3875, 16.1675, 16.1675, 16.275, 16.275, 16.1375, 16.1375, 16.1925, 16.1925, 16.3025, 16.3025, 16.3875, 16.3875, 16.1975, 16.1975, 16.3, 16.3, 16.2975, 16.2975, 16.2725, 16.2725, 15.9175, 15.9175, 16.0575, 16.0575, 16.1975, 16.1975, 16.25, 16.25, 16.1975, 16.1975, 16.4475, 16.4475, 16.4675, 16.4675, 16.395, 16.395, 16.1525, 16.1525, 16.185, 16.185, 16.18, 16.18, 16.2475, 16.2475, 16.0275, 16.0275, 16.2825, 16.2825, 16.1125, 16.1125, 16.215, 16.215, 16.2075, 16.2075, 16.4125, 16.4125, 16.2075, 16.2075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
Client 0, noise level: 0.9745 (0.8771), real noise ratio: 0.8790
Client 1, noise level: 0.6089 (0.5481), real noise ratio: 0.5425
Client 5, noise level: 0.5325 (0.4793), real noise ratio: 0.5120
Client 6, noise level: 0.8149 (0.7334), real noise ratio: 0.7345
Client 7, noise level: 0.9369 (0.8432), real noise ratio: 0.8485
Client 8, noise level: 0.5044 (0.4539), real noise ratio: 0.5875
Client 9, noise level: 0.8733 (0.7860), real noise ratio: 0.8025
Client 10, noise level: 0.9064 (0.8158), real noise ratio: 0.8465
Client 11, noise level: 0.5379 (0.4841), real noise ratio: 0.6100
Client 12, noise level: 0.8282 (0.7454), real noise ratio: 0.7445
Client 14, noise level: 0.7399 (0.6659), real noise ratio: 0.7110
Client 15, noise level: 0.9778 (0.8800), real noise ratio: 0.8845
Client 16, noise level: 0.5000 (0.4500), real noise ratio: 0.5165
Client 17, noise level: 0.6235 (0.5611), real noise ratio: 0.6485
Client 18, noise level: 0.8561 (0.7705), real noise ratio: 0.7980
Client 19, noise level: 0.6623 (0.5961), real noise ratio: 0.6765
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Final Round, Train loss: 0.755, Test loss: 3.060, Test accuracy: 36.22
Final Round, Global train loss: 0.755, Global test loss: 1.673, Global test accuracy: 48.31
Average accuracy final 10 rounds: 37.690250000000006 

Average global accuracy final 10 rounds: 47.8865 

3662.0091438293457
[1.4006922245025635, 2.801384449005127, 3.968996047973633, 5.136607646942139, 6.309640407562256, 7.482673168182373, 8.653927326202393, 9.825181484222412, 10.997901678085327, 12.170621871948242, 13.34009337425232, 14.509564876556396, 15.674345970153809, 16.83912706375122, 18.00687575340271, 19.1746244430542, 20.346039533615112, 21.517454624176025, 22.69444179534912, 23.871428966522217, 25.041305541992188, 26.211182117462158, 27.395820140838623, 28.580458164215088, 29.577624559402466, 30.574790954589844, 31.5846107006073, 32.594430446624756, 33.60437750816345, 34.61432456970215, 35.61087465286255, 36.60742473602295, 37.60736131668091, 38.60729789733887, 39.61564540863037, 40.623992919921875, 41.620845794677734, 42.617698669433594, 43.619322776794434, 44.62094688415527, 45.62510347366333, 46.62926006317139, 47.62989163398743, 48.63052320480347, 49.62876582145691, 50.62700843811035, 51.62760090827942, 52.628193378448486, 53.63073801994324, 54.63328266143799, 55.64180636405945, 56.65033006668091, 57.65289664268494, 58.655463218688965, 59.67288947105408, 60.69031572341919, 61.69112491607666, 62.69193410873413, 63.70908832550049, 64.72624254226685, 65.72892880439758, 66.73161506652832, 67.72867774963379, 68.72574043273926, 69.73466777801514, 70.74359512329102, 71.75536823272705, 72.76714134216309, 73.77307486534119, 74.77900838851929, 75.7874481678009, 76.79588794708252, 77.80193138122559, 78.80797481536865, 79.80889058113098, 80.80980634689331, 81.81381750106812, 82.81782865524292, 83.8277473449707, 84.83766603469849, 85.85049939155579, 86.86333274841309, 87.87611699104309, 88.8889012336731, 89.88820481300354, 90.88750839233398, 91.89816331863403, 92.90881824493408, 93.91222405433655, 94.91562986373901, 95.92949891090393, 96.94336795806885, 97.95142149925232, 98.95947504043579, 99.95817255973816, 100.95687007904053, 101.96367526054382, 102.97048044204712, 103.97566056251526, 104.9808406829834, 105.98139023780823, 106.98193979263306, 107.97914505004883, 108.9763503074646, 109.98479199409485, 110.9932336807251, 111.99440622329712, 112.99557876586914, 114.0028076171875, 115.01003646850586, 116.01458430290222, 117.01913213729858, 118.0174548625946, 119.01577758789062, 120.02519249916077, 121.03460741043091, 122.0464129447937, 123.0582184791565, 124.06677174568176, 125.07532501220703, 126.07614874839783, 127.07697248458862, 128.08273124694824, 129.08849000930786, 130.09047389030457, 131.09245777130127, 132.09212493896484, 133.09179210662842, 134.09199023246765, 135.09218835830688, 136.09172701835632, 137.09126567840576, 138.0917489528656, 139.09223222732544, 140.09296202659607, 141.0936918258667, 142.09745073318481, 143.10120964050293, 144.10082960128784, 145.10044956207275, 146.10096549987793, 147.1014814376831, 148.0998513698578, 149.09822130203247, 150.10134625434875, 151.10447120666504, 152.10877656936646, 153.11308193206787, 154.11384320259094, 155.114604473114, 156.11623072624207, 157.11785697937012, 158.11976718902588, 159.12167739868164, 160.11830925941467, 161.1149411201477, 162.11330103874207, 163.11166095733643, 164.1152148246765, 165.1187686920166, 166.11791563034058, 167.11706256866455, 168.11709141731262, 169.1171202659607, 170.11814284324646, 171.11916542053223, 172.1164424419403, 173.1137194633484, 174.11624145507812, 175.11876344680786, 176.12069606781006, 177.12262868881226, 178.12362027168274, 179.12461185455322, 180.12753534317017, 181.1304588317871, 182.1313455104828, 183.13223218917847, 184.12868213653564, 185.12513208389282, 186.12742376327515, 187.12971544265747, 188.13150835037231, 189.13330125808716, 190.13216614723206, 191.13103103637695, 192.13062143325806, 193.13021183013916, 194.12876081466675, 195.12730979919434, 196.12620377540588, 197.12509775161743, 198.12161421775818, 199.11813068389893, 200.12006998062134, 201.12200927734375, 202.12493443489075, 203.12785959243774, 204.12909030914307, 205.1303210258484, 207.4441077709198, 209.7578945159912]
[19.4925, 19.4925, 20.185, 20.185, 24.3175, 24.3175, 25.6975, 25.6975, 27.2925, 27.2925, 28.2975, 28.2975, 30.59, 30.59, 31.0275, 31.0275, 31.7775, 31.7775, 31.9575, 31.9575, 32.3125, 32.3125, 32.855, 32.855, 33.875, 33.875, 34.59, 34.59, 34.095, 34.095, 34.775, 34.775, 35.1975, 35.1975, 35.2225, 35.2225, 35.5575, 35.5575, 37.2225, 37.2225, 37.76, 37.76, 38.265, 38.265, 38.3925, 38.3925, 38.59, 38.59, 38.675, 38.675, 39.035, 39.035, 38.8475, 38.8475, 38.9025, 38.9025, 38.64, 38.64, 38.8975, 38.8975, 38.63, 38.63, 39.0125, 39.0125, 38.465, 38.465, 38.7375, 38.7375, 38.605, 38.605, 38.1475, 38.1475, 38.225, 38.225, 38.4125, 38.4125, 39.21, 39.21, 38.95, 38.95, 38.4775, 38.4775, 38.7075, 38.7075, 38.7475, 38.7475, 39.0725, 39.0725, 38.93, 38.93, 38.8325, 38.8325, 38.6725, 38.6725, 38.685, 38.685, 38.3225, 38.3225, 38.2225, 38.2225, 38.165, 38.165, 38.12, 38.12, 38.16, 38.16, 37.84, 37.84, 37.82, 37.82, 37.95, 37.95, 38.0975, 38.0975, 37.775, 37.775, 37.89, 37.89, 37.6225, 37.6225, 37.7575, 37.7575, 38.04, 38.04, 37.9925, 37.9925, 38.175, 38.175, 38.085, 38.085, 38.2825, 38.2825, 38.1875, 38.1875, 38.2375, 38.2375, 38.185, 38.185, 38.02, 38.02, 38.135, 38.135, 38.475, 38.475, 38.23, 38.23, 37.86, 37.86, 37.7175, 37.7175, 37.8325, 37.8325, 37.85, 37.85, 37.815, 37.815, 37.5275, 37.5275, 37.315, 37.315, 37.44, 37.44, 37.975, 37.975, 37.7075, 37.7075, 37.53, 37.53, 37.7175, 37.7175, 37.755, 37.755, 37.6525, 37.6525, 37.2875, 37.2875, 37.4325, 37.4325, 37.845, 37.845, 37.9, 37.9, 37.8025, 37.8025, 37.38, 37.38, 37.7575, 37.7575, 37.615, 37.615, 37.6, 37.6, 37.4875, 37.4875, 37.8, 37.8, 37.78, 37.78, 37.78, 37.78, 36.22, 36.22]
