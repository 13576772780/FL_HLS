nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.000, Test loss: 1.748, Test accuracy: 36.77
Round   0, Global train loss: 2.000, Global test loss: 1.736, Global test accuracy: 37.97
Round   1, Train loss: 1.673, Test loss: 1.567, Test accuracy: 43.21
Round   1, Global train loss: 1.673, Global test loss: 1.475, Global test accuracy: 47.71
Round   2, Train loss: 1.522, Test loss: 1.529, Test accuracy: 44.34
Round   2, Global train loss: 1.522, Global test loss: 1.365, Global test accuracy: 51.81
Round   3, Train loss: 1.510, Test loss: 1.517, Test accuracy: 44.93
Round   3, Global train loss: 1.510, Global test loss: 1.346, Global test accuracy: 53.93
Round   4, Train loss: 1.458, Test loss: 1.493, Test accuracy: 45.83
Round   4, Global train loss: 1.458, Global test loss: 1.331, Global test accuracy: 54.18
Round   5, Train loss: 1.439, Test loss: 1.469, Test accuracy: 47.16
Round   5, Global train loss: 1.439, Global test loss: 1.378, Global test accuracy: 53.28
Round   6, Train loss: 1.451, Test loss: 1.452, Test accuracy: 48.05
Round   6, Global train loss: 1.451, Global test loss: 1.368, Global test accuracy: 53.27
Round   7, Train loss: 1.344, Test loss: 1.429, Test accuracy: 48.99
Round   7, Global train loss: 1.344, Global test loss: 1.300, Global test accuracy: 55.42
Round   8, Train loss: 1.243, Test loss: 1.426, Test accuracy: 49.32
Round   8, Global train loss: 1.243, Global test loss: 1.243, Global test accuracy: 56.52
Round   9, Train loss: 1.171, Test loss: 1.424, Test accuracy: 49.84
Round   9, Global train loss: 1.171, Global test loss: 1.180, Global test accuracy: 59.17
Round  10, Train loss: 1.240, Test loss: 1.406, Test accuracy: 50.73
Round  10, Global train loss: 1.240, Global test loss: 1.320, Global test accuracy: 55.15
Round  11, Train loss: 1.196, Test loss: 1.403, Test accuracy: 51.03
Round  11, Global train loss: 1.196, Global test loss: 1.277, Global test accuracy: 56.47
Round  12, Train loss: 1.111, Test loss: 1.416, Test accuracy: 50.97
Round  12, Global train loss: 1.111, Global test loss: 1.206, Global test accuracy: 58.61
Round  13, Train loss: 1.148, Test loss: 1.405, Test accuracy: 51.62
Round  13, Global train loss: 1.148, Global test loss: 1.208, Global test accuracy: 58.68
Round  14, Train loss: 1.015, Test loss: 1.402, Test accuracy: 52.13
Round  14, Global train loss: 1.015, Global test loss: 1.186, Global test accuracy: 59.51
Round  15, Train loss: 1.119, Test loss: 1.405, Test accuracy: 52.69
Round  15, Global train loss: 1.119, Global test loss: 1.199, Global test accuracy: 59.14
Round  16, Train loss: 1.003, Test loss: 1.421, Test accuracy: 52.91
Round  16, Global train loss: 1.003, Global test loss: 1.157, Global test accuracy: 59.45
Round  17, Train loss: 0.964, Test loss: 1.425, Test accuracy: 52.93
Round  17, Global train loss: 0.964, Global test loss: 1.208, Global test accuracy: 58.56
Round  18, Train loss: 0.987, Test loss: 1.431, Test accuracy: 53.13
Round  18, Global train loss: 0.987, Global test loss: 1.160, Global test accuracy: 59.71
Round  19, Train loss: 0.949, Test loss: 1.454, Test accuracy: 52.91
Round  19, Global train loss: 0.949, Global test loss: 1.206, Global test accuracy: 58.71
Round  20, Train loss: 0.797, Test loss: 1.470, Test accuracy: 53.00
Round  20, Global train loss: 0.797, Global test loss: 1.118, Global test accuracy: 61.06
Round  21, Train loss: 0.947, Test loss: 1.473, Test accuracy: 53.17
Round  21, Global train loss: 0.947, Global test loss: 1.180, Global test accuracy: 59.61
Round  22, Train loss: 0.760, Test loss: 1.481, Test accuracy: 53.34
Round  22, Global train loss: 0.760, Global test loss: 1.110, Global test accuracy: 61.53
Round  23, Train loss: 0.768, Test loss: 1.481, Test accuracy: 53.53
Round  23, Global train loss: 0.768, Global test loss: 1.150, Global test accuracy: 59.86
Round  24, Train loss: 0.833, Test loss: 1.479, Test accuracy: 54.18
Round  24, Global train loss: 0.833, Global test loss: 1.161, Global test accuracy: 60.01
Round  25, Train loss: 0.711, Test loss: 1.505, Test accuracy: 54.05
Round  25, Global train loss: 0.711, Global test loss: 1.184, Global test accuracy: 58.30
Round  26, Train loss: 0.686, Test loss: 1.516, Test accuracy: 53.98
Round  26, Global train loss: 0.686, Global test loss: 1.138, Global test accuracy: 60.96
Round  27, Train loss: 0.713, Test loss: 1.536, Test accuracy: 54.23
Round  27, Global train loss: 0.713, Global test loss: 1.206, Global test accuracy: 58.20
Round  28, Train loss: 0.554, Test loss: 1.576, Test accuracy: 54.20
Round  28, Global train loss: 0.554, Global test loss: 1.111, Global test accuracy: 61.87
Round  29, Train loss: 0.840, Test loss: 1.573, Test accuracy: 54.43
Round  29, Global train loss: 0.840, Global test loss: 1.284, Global test accuracy: 56.15
Round  30, Train loss: 0.666, Test loss: 1.592, Test accuracy: 54.59
Round  30, Global train loss: 0.666, Global test loss: 1.151, Global test accuracy: 60.13
Round  31, Train loss: 0.654, Test loss: 1.636, Test accuracy: 54.25
Round  31, Global train loss: 0.654, Global test loss: 1.163, Global test accuracy: 59.78
Round  32, Train loss: 0.805, Test loss: 1.646, Test accuracy: 54.23
Round  32, Global train loss: 0.805, Global test loss: 1.285, Global test accuracy: 55.80
Round  33, Train loss: 0.725, Test loss: 1.633, Test accuracy: 54.84
Round  33, Global train loss: 0.725, Global test loss: 1.131, Global test accuracy: 61.38
Round  34, Train loss: 0.649, Test loss: 1.670, Test accuracy: 54.68
Round  34, Global train loss: 0.649, Global test loss: 1.161, Global test accuracy: 59.68
Round  35, Train loss: 0.655, Test loss: 1.691, Test accuracy: 54.78
Round  35, Global train loss: 0.655, Global test loss: 1.135, Global test accuracy: 60.70
Round  36, Train loss: 0.528, Test loss: 1.686, Test accuracy: 54.85
Round  36, Global train loss: 0.528, Global test loss: 1.134, Global test accuracy: 61.16
Round  37, Train loss: 0.599, Test loss: 1.708, Test accuracy: 54.91
Round  37, Global train loss: 0.599, Global test loss: 1.182, Global test accuracy: 58.94
Round  38, Train loss: 0.501, Test loss: 1.717, Test accuracy: 55.16
Round  38, Global train loss: 0.501, Global test loss: 1.147, Global test accuracy: 61.42
Round  39, Train loss: 0.494, Test loss: 1.742, Test accuracy: 55.17
Round  39, Global train loss: 0.494, Global test loss: 1.137, Global test accuracy: 61.16
Round  40, Train loss: 0.496, Test loss: 1.800, Test accuracy: 54.94
Round  40, Global train loss: 0.496, Global test loss: 1.181, Global test accuracy: 58.83
Round  41, Train loss: 0.495, Test loss: 1.831, Test accuracy: 54.82
Round  41, Global train loss: 0.495, Global test loss: 1.160, Global test accuracy: 59.81
Round  42, Train loss: 0.458, Test loss: 1.817, Test accuracy: 55.09
Round  42, Global train loss: 0.458, Global test loss: 1.145, Global test accuracy: 60.98
Round  43, Train loss: 0.577, Test loss: 1.836, Test accuracy: 55.24
Round  43, Global train loss: 0.577, Global test loss: 1.192, Global test accuracy: 58.85
Round  44, Train loss: 0.480, Test loss: 1.859, Test accuracy: 55.19
Round  44, Global train loss: 0.480, Global test loss: 1.131, Global test accuracy: 62.05
Round  45, Train loss: 0.438, Test loss: 1.860, Test accuracy: 55.43
Round  45, Global train loss: 0.438, Global test loss: 1.173, Global test accuracy: 59.53
Round  46, Train loss: 0.488, Test loss: 1.890, Test accuracy: 55.38
Round  46, Global train loss: 0.488, Global test loss: 1.158, Global test accuracy: 61.02
Round  47, Train loss: 0.492, Test loss: 1.920, Test accuracy: 55.41
Round  47, Global train loss: 0.492, Global test loss: 1.133, Global test accuracy: 61.30
Round  48, Train loss: 0.381, Test loss: 1.953, Test accuracy: 55.05
Round  48, Global train loss: 0.381, Global test loss: 1.173, Global test accuracy: 61.68
Round  49, Train loss: 0.469, Test loss: 1.943, Test accuracy: 55.24
Round  49, Global train loss: 0.469, Global test loss: 1.198, Global test accuracy: 57.93
Round  50, Train loss: 0.489, Test loss: 1.959, Test accuracy: 55.16
Round  50, Global train loss: 0.489, Global test loss: 1.136, Global test accuracy: 61.51
Round  51, Train loss: 0.403, Test loss: 1.999, Test accuracy: 55.08
Round  51, Global train loss: 0.403, Global test loss: 1.164, Global test accuracy: 60.86
Round  52, Train loss: 0.385, Test loss: 2.011, Test accuracy: 55.38
Round  52, Global train loss: 0.385, Global test loss: 1.233, Global test accuracy: 56.69
Round  53, Train loss: 0.388, Test loss: 2.039, Test accuracy: 55.20
Round  53, Global train loss: 0.388, Global test loss: 1.228, Global test accuracy: 57.27
Round  54, Train loss: 0.409, Test loss: 2.030, Test accuracy: 55.35
Round  54, Global train loss: 0.409, Global test loss: 1.196, Global test accuracy: 58.85
Round  55, Train loss: 0.368, Test loss: 2.055, Test accuracy: 55.10
Round  55, Global train loss: 0.368, Global test loss: 1.217, Global test accuracy: 58.31
Round  56, Train loss: 0.390, Test loss: 2.070, Test accuracy: 55.05
Round  56, Global train loss: 0.390, Global test loss: 1.251, Global test accuracy: 56.06
Round  57, Train loss: 0.365, Test loss: 2.075, Test accuracy: 55.22
Round  57, Global train loss: 0.365, Global test loss: 1.172, Global test accuracy: 61.72
Round  58, Train loss: 0.399, Test loss: 2.083, Test accuracy: 55.34
Round  58, Global train loss: 0.399, Global test loss: 1.193, Global test accuracy: 58.66
Round  59, Train loss: 0.384, Test loss: 2.085, Test accuracy: 55.43
Round  59, Global train loss: 0.384, Global test loss: 1.237, Global test accuracy: 57.30
Round  60, Train loss: 0.359, Test loss: 2.116, Test accuracy: 55.53
Round  60, Global train loss: 0.359, Global test loss: 1.283, Global test accuracy: 54.95
Round  61, Train loss: 0.362, Test loss: 2.172, Test accuracy: 55.29
Round  61, Global train loss: 0.362, Global test loss: 1.314, Global test accuracy: 53.73
Round  62, Train loss: 0.363, Test loss: 2.145, Test accuracy: 55.56
Round  62, Global train loss: 0.363, Global test loss: 1.202, Global test accuracy: 58.54
Round  63, Train loss: 0.336, Test loss: 2.148, Test accuracy: 55.49
Round  63, Global train loss: 0.336, Global test loss: 1.170, Global test accuracy: 60.38
Round  64, Train loss: 0.357, Test loss: 2.164, Test accuracy: 55.50
Round  64, Global train loss: 0.357, Global test loss: 1.163, Global test accuracy: 62.16
Round  65, Train loss: 0.339, Test loss: 2.171, Test accuracy: 55.80
Round  65, Global train loss: 0.339, Global test loss: 1.207, Global test accuracy: 59.56
Round  66, Train loss: 0.317, Test loss: 2.154, Test accuracy: 56.19
Round  66, Global train loss: 0.317, Global test loss: 1.207, Global test accuracy: 59.35
Round  67, Train loss: 0.318, Test loss: 2.129, Test accuracy: 56.25
Round  67, Global train loss: 0.318, Global test loss: 1.186, Global test accuracy: 59.23
Round  68, Train loss: 0.374, Test loss: 2.166, Test accuracy: 55.95
Round  68, Global train loss: 0.374, Global test loss: 1.218, Global test accuracy: 58.11
Round  69, Train loss: 0.346, Test loss: 2.201, Test accuracy: 56.05
Round  69, Global train loss: 0.346, Global test loss: 1.201, Global test accuracy: 59.22
Round  70, Train loss: 0.317, Test loss: 2.231, Test accuracy: 55.99
Round  70, Global train loss: 0.317, Global test loss: 1.214, Global test accuracy: 58.27
Round  71, Train loss: 0.307, Test loss: 2.299, Test accuracy: 55.50
Round  71, Global train loss: 0.307, Global test loss: 1.244, Global test accuracy: 57.95
Round  72, Train loss: 0.268, Test loss: 2.275, Test accuracy: 55.66
Round  72, Global train loss: 0.268, Global test loss: 1.218, Global test accuracy: 58.98
Round  73, Train loss: 0.291, Test loss: 2.286, Test accuracy: 55.50
Round  73, Global train loss: 0.291, Global test loss: 1.263, Global test accuracy: 56.50
Round  74, Train loss: 0.288, Test loss: 2.304, Test accuracy: 55.47
Round  74, Global train loss: 0.288, Global test loss: 1.225, Global test accuracy: 58.45
Round  75, Train loss: 0.319, Test loss: 2.349, Test accuracy: 55.52
Round  75, Global train loss: 0.319, Global test loss: 1.220, Global test accuracy: 58.89
Round  76, Train loss: 0.287, Test loss: 2.382, Test accuracy: 55.21
Round  76, Global train loss: 0.287, Global test loss: 1.189, Global test accuracy: 60.77
Round  77, Train loss: 0.246, Test loss: 2.373, Test accuracy: 55.55
Round  77, Global train loss: 0.246, Global test loss: 1.239, Global test accuracy: 58.72
Round  78, Train loss: 0.279, Test loss: 2.382, Test accuracy: 55.66
Round  78, Global train loss: 0.279, Global test loss: 1.316, Global test accuracy: 54.93
Round  79, Train loss: 0.292, Test loss: 2.405, Test accuracy: 55.59
Round  79, Global train loss: 0.292, Global test loss: 1.315, Global test accuracy: 54.13
Round  80, Train loss: 0.302, Test loss: 2.390, Test accuracy: 55.90
Round  80, Global train loss: 0.302, Global test loss: 1.254, Global test accuracy: 58.18
Round  81, Train loss: 0.275, Test loss: 2.411, Test accuracy: 55.87
Round  81, Global train loss: 0.275, Global test loss: 1.233, Global test accuracy: 59.72
Round  82, Train loss: 0.255, Test loss: 2.441, Test accuracy: 55.61
Round  82, Global train loss: 0.255, Global test loss: 1.267, Global test accuracy: 58.85
Round  83, Train loss: 0.214, Test loss: 2.453, Test accuracy: 55.77
Round  83, Global train loss: 0.214, Global test loss: 1.266, Global test accuracy: 60.20
Round  84, Train loss: 0.265, Test loss: 2.486, Test accuracy: 55.56
Round  84, Global train loss: 0.265, Global test loss: 1.243, Global test accuracy: 58.30
Round  85, Train loss: 0.260, Test loss: 2.489, Test accuracy: 55.65
Round  85, Global train loss: 0.260, Global test loss: 1.275, Global test accuracy: 55.54
Round  86, Train loss: 0.249, Test loss: 2.495, Test accuracy: 55.79
Round  86, Global train loss: 0.249, Global test loss: 1.226, Global test accuracy: 58.51
Round  87, Train loss: 0.214, Test loss: 2.543, Test accuracy: 55.43
Round  87, Global train loss: 0.214, Global test loss: 1.304, Global test accuracy: 60.71
Round  88, Train loss: 0.249, Test loss: 2.562, Test accuracy: 55.58
Round  88, Global train loss: 0.249, Global test loss: 1.288, Global test accuracy: 55.53
Round  89, Train loss: 0.226, Test loss: 2.586, Test accuracy: 55.58
Round  89, Global train loss: 0.226, Global test loss: 1.271, Global test accuracy: 57.82
Round  90, Train loss: 0.263, Test loss: 2.606, Test accuracy: 55.62
Round  90, Global train loss: 0.263, Global test loss: 1.287, Global test accuracy: 55.13
Round  91, Train loss: 0.230, Test loss: 2.596, Test accuracy: 55.42
Round  91, Global train loss: 0.230, Global test loss: 1.333, Global test accuracy: 53.31
Round  92, Train loss: 0.218, Test loss: 2.653, Test accuracy: 55.45
Round  92, Global train loss: 0.218, Global test loss: 1.263, Global test accuracy: 56.04
Round  93, Train loss: 0.225, Test loss: 2.641, Test accuracy: 55.40
Round  93, Global train loss: 0.225, Global test loss: 1.271, Global test accuracy: 59.48
Round  94, Train loss: 0.248, Test loss: 2.639, Test accuracy: 55.51
Round  94, Global train loss: 0.248, Global test loss: 1.275, Global test accuracy: 56.04
Round  95, Train loss: 0.215, Test loss: 2.688, Test accuracy: 55.36
Round  95, Global train loss: 0.215, Global test loss: 1.262, Global test accuracy: 57.93
Round  96, Train loss: 0.230, Test loss: 2.690, Test accuracy: 55.57
Round  96, Global train loss: 0.230, Global test loss: 1.283, Global test accuracy: 55.53
Round  97, Train loss: 0.224, Test loss: 2.684, Test accuracy: 55.60
Round  97, Global train loss: 0.224, Global test loss: 1.293, Global test accuracy: 57.78
Round  98, Train loss: 0.231, Test loss: 2.644, Test accuracy: 56.15
Round  98, Global train loss: 0.231, Global test loss: 1.301, Global test accuracy: 56.21
Round  99, Train loss: 0.218, Test loss: 2.639, Test accuracy: 56.08
Round  99, Global train loss: 0.218, Global test loss: 1.318, Global test accuracy: 54.91
Final Round, Train loss: 0.143, Test loss: 2.886, Test accuracy: 55.47
Final Round, Global train loss: 0.143, Global test loss: 1.318, Global test accuracy: 54.91
Average accuracy final 10 rounds: 55.614999999999995 

Average global accuracy final 10 rounds: 56.236749999999994 

5686.760164260864
[4.593260765075684, 9.186521530151367, 13.570147037506104, 17.95377254486084, 22.28322458267212, 26.6126766204834, 30.961400747299194, 35.31012487411499, 39.69643235206604, 44.08273983001709, 48.42551231384277, 52.76828479766846, 57.11180567741394, 61.455326557159424, 65.79253697395325, 70.12974739074707, 74.47930860519409, 78.82886981964111, 83.14774441719055, 87.46661901473999, 91.79603981971741, 96.12546062469482, 100.42854261398315, 104.73162460327148, 109.04827523231506, 113.36492586135864, 117.67373991012573, 121.98255395889282, 126.30778527259827, 130.6330165863037, 134.94763588905334, 139.26225519180298, 143.6013195514679, 147.9403839111328, 152.3041388988495, 156.66789388656616, 160.99183821678162, 165.31578254699707, 169.63008046150208, 173.94437837600708, 178.26508688926697, 182.58579540252686, 186.90845608711243, 191.231116771698, 195.56630039215088, 199.90148401260376, 204.23888278007507, 208.5762815475464, 212.952378988266, 217.3284764289856, 221.68094992637634, 226.0334234237671, 230.36521887779236, 234.69701433181763, 239.04294419288635, 243.38887405395508, 247.79108333587646, 252.19329261779785, 256.56424045562744, 260.93518829345703, 265.28938817977905, 269.6435880661011, 274.0313792228699, 278.4191703796387, 282.8851249217987, 287.35107946395874, 291.7274250984192, 296.10377073287964, 300.4732120037079, 304.84265327453613, 309.1641912460327, 313.4857292175293, 317.7934195995331, 322.10110998153687, 326.4519872665405, 330.8028645515442, 335.13814997673035, 339.4734354019165, 343.8123605251312, 348.15128564834595, 352.590656042099, 357.03002643585205, 361.3530604839325, 365.67609453201294, 369.9990246295929, 374.32195472717285, 378.67891120910645, 383.03586769104004, 387.3384017944336, 391.64093589782715, 395.9547293186188, 400.2685227394104, 404.702996969223, 409.13747119903564, 413.5493595600128, 417.96124792099, 422.28091311454773, 426.60057830810547, 430.913311958313, 435.2260456085205, 439.537056684494, 443.84806776046753, 448.1576817035675, 452.4672956466675, 456.774658203125, 461.0820207595825, 465.4169476032257, 469.7518744468689, 474.0618824958801, 478.37189054489136, 482.68351554870605, 486.99514055252075, 491.3217821121216, 495.6484236717224, 499.97801780700684, 504.30761194229126, 508.69192934036255, 513.0762467384338, 517.414918422699, 521.7535901069641, 526.0690488815308, 530.3845076560974, 534.8379063606262, 539.291305065155, 543.607923746109, 547.924542427063, 552.3947570323944, 556.8649716377258, 561.2089326381683, 565.5528936386108, 569.8664350509644, 574.1799764633179, 578.4962377548218, 582.8124990463257, 587.1431128978729, 591.4737267494202, 595.80744099617, 600.1411552429199, 604.5417678356171, 608.9423804283142, 613.2759184837341, 617.609456539154, 621.9276833534241, 626.2459101676941, 630.5704276561737, 634.8949451446533, 639.2688221931458, 643.6426992416382, 648.0016267299652, 652.3605542182922, 656.6855494976044, 661.0105447769165, 665.3198714256287, 669.6291980743408, 673.9484858512878, 678.2677736282349, 682.5854561328888, 686.9031386375427, 691.2333567142487, 695.5635747909546, 699.8854842185974, 704.2073936462402, 708.5377724170685, 712.8681511878967, 717.1899552345276, 721.5117592811584, 725.8313574790955, 730.1509556770325, 734.4758393764496, 738.8007230758667, 743.108247756958, 747.4157724380493, 751.7365911006927, 756.0574097633362, 760.3769228458405, 764.6964359283447, 769.0867710113525, 773.4771060943604, 777.8279809951782, 782.1788558959961, 786.5380899906158, 790.8973240852356, 795.2940859794617, 799.6908478736877, 804.0683917999268, 808.4459357261658, 812.8075733184814, 817.1692109107971, 821.4950017929077, 825.8207926750183, 830.0928874015808, 834.3649821281433, 838.7003231048584, 843.0356640815735, 847.3689246177673, 851.7021851539612, 856.0247750282288, 860.3473649024963, 864.6670339107513, 868.9867029190063, 871.1543920040131, 873.3220810890198]
[36.775, 36.775, 43.21, 43.21, 44.345, 44.345, 44.9325, 44.9325, 45.8275, 45.8275, 47.16, 47.16, 48.045, 48.045, 48.9875, 48.9875, 49.3225, 49.3225, 49.845, 49.845, 50.7325, 50.7325, 51.0325, 51.0325, 50.9675, 50.9675, 51.615, 51.615, 52.1325, 52.1325, 52.685, 52.685, 52.9075, 52.9075, 52.93, 52.93, 53.13, 53.13, 52.905, 52.905, 52.9975, 52.9975, 53.175, 53.175, 53.345, 53.345, 53.5275, 53.5275, 54.1825, 54.1825, 54.05, 54.05, 53.985, 53.985, 54.2325, 54.2325, 54.195, 54.195, 54.43, 54.43, 54.595, 54.595, 54.2475, 54.2475, 54.23, 54.23, 54.84, 54.84, 54.6775, 54.6775, 54.785, 54.785, 54.855, 54.855, 54.91, 54.91, 55.155, 55.155, 55.17, 55.17, 54.94, 54.94, 54.8225, 54.8225, 55.09, 55.09, 55.24, 55.24, 55.1875, 55.1875, 55.4275, 55.4275, 55.375, 55.375, 55.4125, 55.4125, 55.045, 55.045, 55.245, 55.245, 55.165, 55.165, 55.0775, 55.0775, 55.3825, 55.3825, 55.2, 55.2, 55.3525, 55.3525, 55.105, 55.105, 55.0525, 55.0525, 55.22, 55.22, 55.3425, 55.3425, 55.43, 55.43, 55.53, 55.53, 55.2925, 55.2925, 55.565, 55.565, 55.49, 55.49, 55.4975, 55.4975, 55.7975, 55.7975, 56.1875, 56.1875, 56.2475, 56.2475, 55.9525, 55.9525, 56.045, 56.045, 55.9875, 55.9875, 55.5, 55.5, 55.6625, 55.6625, 55.5025, 55.5025, 55.465, 55.465, 55.525, 55.525, 55.21, 55.21, 55.55, 55.55, 55.6625, 55.6625, 55.5875, 55.5875, 55.895, 55.895, 55.8725, 55.8725, 55.61, 55.61, 55.7725, 55.7725, 55.56, 55.56, 55.645, 55.645, 55.79, 55.79, 55.4325, 55.4325, 55.5825, 55.5825, 55.5825, 55.5825, 55.6175, 55.6175, 55.42, 55.42, 55.4475, 55.4475, 55.3975, 55.3975, 55.505, 55.505, 55.3575, 55.3575, 55.57, 55.57, 55.605, 55.605, 56.15, 56.15, 56.08, 56.08, 55.47, 55.47]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.982, Test loss: 1.730, Test accuracy: 36.97
Round   0, Global train loss: 1.982, Global test loss: 1.718, Global test accuracy: 37.79
Round   1, Train loss: 1.675, Test loss: 1.577, Test accuracy: 42.76
Round   1, Global train loss: 1.675, Global test loss: 1.489, Global test accuracy: 46.75
Round   2, Train loss: 1.534, Test loss: 1.506, Test accuracy: 45.31
Round   2, Global train loss: 1.534, Global test loss: 1.341, Global test accuracy: 52.38
Round   3, Train loss: 1.430, Test loss: 1.413, Test accuracy: 49.03
Round   3, Global train loss: 1.430, Global test loss: 1.248, Global test accuracy: 56.28
Round   4, Train loss: 1.356, Test loss: 1.389, Test accuracy: 50.21
Round   4, Global train loss: 1.356, Global test loss: 1.194, Global test accuracy: 58.33
Round   5, Train loss: 1.262, Test loss: 1.353, Test accuracy: 51.51
Round   5, Global train loss: 1.262, Global test loss: 1.121, Global test accuracy: 61.01
Round   6, Train loss: 1.216, Test loss: 1.295, Test accuracy: 53.84
Round   6, Global train loss: 1.216, Global test loss: 1.068, Global test accuracy: 63.20
Round   7, Train loss: 1.152, Test loss: 1.245, Test accuracy: 56.02
Round   7, Global train loss: 1.152, Global test loss: 1.034, Global test accuracy: 63.53
Round   8, Train loss: 1.090, Test loss: 1.230, Test accuracy: 56.60
Round   8, Global train loss: 1.090, Global test loss: 1.006, Global test accuracy: 64.59
Round   9, Train loss: 1.055, Test loss: 1.209, Test accuracy: 57.64
Round   9, Global train loss: 1.055, Global test loss: 0.970, Global test accuracy: 66.39
Round  10, Train loss: 1.038, Test loss: 1.179, Test accuracy: 59.12
Round  10, Global train loss: 1.038, Global test loss: 0.938, Global test accuracy: 67.59
Round  11, Train loss: 0.975, Test loss: 1.155, Test accuracy: 59.82
Round  11, Global train loss: 0.975, Global test loss: 0.920, Global test accuracy: 67.72
Round  12, Train loss: 0.946, Test loss: 1.158, Test accuracy: 60.00
Round  12, Global train loss: 0.946, Global test loss: 0.903, Global test accuracy: 68.84
Round  13, Train loss: 0.917, Test loss: 1.149, Test accuracy: 60.65
Round  13, Global train loss: 0.917, Global test loss: 0.889, Global test accuracy: 69.41
Round  14, Train loss: 0.901, Test loss: 1.137, Test accuracy: 61.15
Round  14, Global train loss: 0.901, Global test loss: 0.865, Global test accuracy: 70.06
Round  15, Train loss: 0.877, Test loss: 1.105, Test accuracy: 62.48
Round  15, Global train loss: 0.877, Global test loss: 0.848, Global test accuracy: 70.97
Round  16, Train loss: 0.868, Test loss: 1.074, Test accuracy: 63.62
Round  16, Global train loss: 0.868, Global test loss: 0.826, Global test accuracy: 71.77
Round  17, Train loss: 0.830, Test loss: 1.061, Test accuracy: 64.33
Round  17, Global train loss: 0.830, Global test loss: 0.827, Global test accuracy: 71.56
Round  18, Train loss: 0.798, Test loss: 1.043, Test accuracy: 64.91
Round  18, Global train loss: 0.798, Global test loss: 0.803, Global test accuracy: 72.33
Round  19, Train loss: 0.807, Test loss: 1.047, Test accuracy: 65.12
Round  19, Global train loss: 0.807, Global test loss: 0.807, Global test accuracy: 72.58
Round  20, Train loss: 0.767, Test loss: 1.035, Test accuracy: 65.51
Round  20, Global train loss: 0.767, Global test loss: 0.789, Global test accuracy: 72.61
Round  21, Train loss: 0.765, Test loss: 1.005, Test accuracy: 66.62
Round  21, Global train loss: 0.765, Global test loss: 0.785, Global test accuracy: 73.74
Round  22, Train loss: 0.751, Test loss: 0.988, Test accuracy: 67.31
Round  22, Global train loss: 0.751, Global test loss: 0.784, Global test accuracy: 73.98
Round  23, Train loss: 0.722, Test loss: 0.983, Test accuracy: 67.77
Round  23, Global train loss: 0.722, Global test loss: 0.774, Global test accuracy: 74.28
Round  24, Train loss: 0.696, Test loss: 0.988, Test accuracy: 68.03
Round  24, Global train loss: 0.696, Global test loss: 0.777, Global test accuracy: 74.10
Round  25, Train loss: 0.714, Test loss: 0.997, Test accuracy: 67.81
Round  25, Global train loss: 0.714, Global test loss: 0.775, Global test accuracy: 74.36
Round  26, Train loss: 0.672, Test loss: 0.993, Test accuracy: 68.05
Round  26, Global train loss: 0.672, Global test loss: 0.779, Global test accuracy: 74.36
Round  27, Train loss: 0.672, Test loss: 0.991, Test accuracy: 68.32
Round  27, Global train loss: 0.672, Global test loss: 0.773, Global test accuracy: 74.89
Round  28, Train loss: 0.700, Test loss: 0.985, Test accuracy: 68.61
Round  28, Global train loss: 0.700, Global test loss: 0.767, Global test accuracy: 74.89
Round  29, Train loss: 0.653, Test loss: 0.990, Test accuracy: 68.58
Round  29, Global train loss: 0.653, Global test loss: 0.759, Global test accuracy: 75.25
Round  30, Train loss: 0.658, Test loss: 0.997, Test accuracy: 68.53
Round  30, Global train loss: 0.658, Global test loss: 0.760, Global test accuracy: 75.29
Round  31, Train loss: 0.677, Test loss: 0.993, Test accuracy: 68.99
Round  31, Global train loss: 0.677, Global test loss: 0.741, Global test accuracy: 75.41
Round  32, Train loss: 0.629, Test loss: 0.983, Test accuracy: 69.36
Round  32, Global train loss: 0.629, Global test loss: 0.744, Global test accuracy: 75.65
Round  33, Train loss: 0.601, Test loss: 0.993, Test accuracy: 69.29
Round  33, Global train loss: 0.601, Global test loss: 0.743, Global test accuracy: 76.06
Round  34, Train loss: 0.641, Test loss: 0.992, Test accuracy: 69.23
Round  34, Global train loss: 0.641, Global test loss: 0.736, Global test accuracy: 75.93
Round  35, Train loss: 0.597, Test loss: 0.985, Test accuracy: 69.50
Round  35, Global train loss: 0.597, Global test loss: 0.747, Global test accuracy: 75.78
Round  36, Train loss: 0.582, Test loss: 0.972, Test accuracy: 69.89
Round  36, Global train loss: 0.582, Global test loss: 0.744, Global test accuracy: 75.98
Round  37, Train loss: 0.592, Test loss: 0.968, Test accuracy: 69.87
Round  37, Global train loss: 0.592, Global test loss: 0.741, Global test accuracy: 75.91
Round  38, Train loss: 0.620, Test loss: 0.960, Test accuracy: 70.14
Round  38, Global train loss: 0.620, Global test loss: 0.727, Global test accuracy: 76.20
Round  39, Train loss: 0.577, Test loss: 0.963, Test accuracy: 70.13
Round  39, Global train loss: 0.577, Global test loss: 0.736, Global test accuracy: 76.20
Round  40, Train loss: 0.559, Test loss: 0.976, Test accuracy: 69.90
Round  40, Global train loss: 0.559, Global test loss: 0.737, Global test accuracy: 76.35
Round  41, Train loss: 0.559, Test loss: 0.977, Test accuracy: 70.02
Round  41, Global train loss: 0.559, Global test loss: 0.746, Global test accuracy: 76.32
Round  42, Train loss: 0.558, Test loss: 0.975, Test accuracy: 70.30
Round  42, Global train loss: 0.558, Global test loss: 0.731, Global test accuracy: 76.77
Round  43, Train loss: 0.555, Test loss: 0.978, Test accuracy: 70.42
Round  43, Global train loss: 0.555, Global test loss: 0.746, Global test accuracy: 76.31
Round  44, Train loss: 0.553, Test loss: 0.976, Test accuracy: 70.69
Round  44, Global train loss: 0.553, Global test loss: 0.744, Global test accuracy: 76.76
Round  45, Train loss: 0.521, Test loss: 0.964, Test accuracy: 70.93
Round  45, Global train loss: 0.521, Global test loss: 0.735, Global test accuracy: 76.46
Round  46, Train loss: 0.537, Test loss: 0.956, Test accuracy: 71.06
Round  46, Global train loss: 0.537, Global test loss: 0.734, Global test accuracy: 76.50
Round  47, Train loss: 0.527, Test loss: 0.961, Test accuracy: 71.01
Round  47, Global train loss: 0.527, Global test loss: 0.734, Global test accuracy: 77.04
Round  48, Train loss: 0.541, Test loss: 0.961, Test accuracy: 71.13
Round  48, Global train loss: 0.541, Global test loss: 0.727, Global test accuracy: 77.25
Round  49, Train loss: 0.534, Test loss: 0.960, Test accuracy: 71.02
Round  49, Global train loss: 0.534, Global test loss: 0.722, Global test accuracy: 76.58
Round  50, Train loss: 0.503, Test loss: 0.963, Test accuracy: 71.17
Round  50, Global train loss: 0.503, Global test loss: 0.735, Global test accuracy: 76.88
Round  51, Train loss: 0.491, Test loss: 0.962, Test accuracy: 71.13
Round  51, Global train loss: 0.491, Global test loss: 0.744, Global test accuracy: 76.53
Round  52, Train loss: 0.482, Test loss: 0.967, Test accuracy: 71.27
Round  52, Global train loss: 0.482, Global test loss: 0.746, Global test accuracy: 77.01
Round  53, Train loss: 0.528, Test loss: 0.968, Test accuracy: 71.43
Round  53, Global train loss: 0.528, Global test loss: 0.732, Global test accuracy: 77.04
Round  54, Train loss: 0.489, Test loss: 0.966, Test accuracy: 71.47
Round  54, Global train loss: 0.489, Global test loss: 0.739, Global test accuracy: 76.89
Round  55, Train loss: 0.502, Test loss: 0.963, Test accuracy: 71.67
Round  55, Global train loss: 0.502, Global test loss: 0.737, Global test accuracy: 77.08
Round  56, Train loss: 0.496, Test loss: 0.959, Test accuracy: 71.73
Round  56, Global train loss: 0.496, Global test loss: 0.742, Global test accuracy: 76.84
Round  57, Train loss: 0.486, Test loss: 0.953, Test accuracy: 72.00
Round  57, Global train loss: 0.486, Global test loss: 0.737, Global test accuracy: 77.19
Round  58, Train loss: 0.493, Test loss: 0.951, Test accuracy: 72.22
Round  58, Global train loss: 0.493, Global test loss: 0.736, Global test accuracy: 77.13
Round  59, Train loss: 0.478, Test loss: 0.948, Test accuracy: 72.25
Round  59, Global train loss: 0.478, Global test loss: 0.741, Global test accuracy: 76.86
Round  60, Train loss: 0.510, Test loss: 0.954, Test accuracy: 72.12
Round  60, Global train loss: 0.510, Global test loss: 0.730, Global test accuracy: 77.29
Round  61, Train loss: 0.500, Test loss: 0.965, Test accuracy: 71.80
Round  61, Global train loss: 0.500, Global test loss: 0.720, Global test accuracy: 77.39
Round  62, Train loss: 0.451, Test loss: 0.968, Test accuracy: 71.73
Round  62, Global train loss: 0.451, Global test loss: 0.738, Global test accuracy: 77.28
Round  63, Train loss: 0.489, Test loss: 0.964, Test accuracy: 71.98
Round  63, Global train loss: 0.489, Global test loss: 0.735, Global test accuracy: 77.48
Round  64, Train loss: 0.487, Test loss: 0.974, Test accuracy: 71.83
Round  64, Global train loss: 0.487, Global test loss: 0.727, Global test accuracy: 77.35
Round  65, Train loss: 0.464, Test loss: 0.952, Test accuracy: 72.42
Round  65, Global train loss: 0.464, Global test loss: 0.726, Global test accuracy: 77.09
Round  66, Train loss: 0.453, Test loss: 0.962, Test accuracy: 72.36
Round  66, Global train loss: 0.453, Global test loss: 0.735, Global test accuracy: 77.33
Round  67, Train loss: 0.430, Test loss: 0.969, Test accuracy: 72.26
Round  67, Global train loss: 0.430, Global test loss: 0.760, Global test accuracy: 76.89
Round  68, Train loss: 0.482, Test loss: 0.955, Test accuracy: 72.37
Round  68, Global train loss: 0.482, Global test loss: 0.729, Global test accuracy: 77.13
Round  69, Train loss: 0.457, Test loss: 0.949, Test accuracy: 72.32
Round  69, Global train loss: 0.457, Global test loss: 0.736, Global test accuracy: 77.12
Round  70, Train loss: 0.437, Test loss: 0.962, Test accuracy: 72.19
Round  70, Global train loss: 0.437, Global test loss: 0.749, Global test accuracy: 76.97
Round  71, Train loss: 0.444, Test loss: 0.957, Test accuracy: 72.39
Round  71, Global train loss: 0.444, Global test loss: 0.746, Global test accuracy: 77.37
Round  72, Train loss: 0.419, Test loss: 0.960, Test accuracy: 72.46
Round  72, Global train loss: 0.419, Global test loss: 0.754, Global test accuracy: 77.46
Round  73, Train loss: 0.452, Test loss: 0.960, Test accuracy: 72.42
Round  73, Global train loss: 0.452, Global test loss: 0.743, Global test accuracy: 77.38
Round  74, Train loss: 0.449, Test loss: 0.972, Test accuracy: 72.48
Round  74, Global train loss: 0.449, Global test loss: 0.753, Global test accuracy: 77.45
Round  75, Train loss: 0.441, Test loss: 0.970, Test accuracy: 72.50
Round  75, Global train loss: 0.441, Global test loss: 0.746, Global test accuracy: 77.36
Round  76, Train loss: 0.439, Test loss: 0.970, Test accuracy: 72.44
Round  76, Global train loss: 0.439, Global test loss: 0.749, Global test accuracy: 77.36
Round  77, Train loss: 0.424, Test loss: 0.977, Test accuracy: 72.39
Round  77, Global train loss: 0.424, Global test loss: 0.762, Global test accuracy: 77.24
Round  78, Train loss: 0.436, Test loss: 0.976, Test accuracy: 72.59
Round  78, Global train loss: 0.436, Global test loss: 0.750, Global test accuracy: 77.72
Round  79, Train loss: 0.466, Test loss: 0.976, Test accuracy: 72.65
Round  79, Global train loss: 0.466, Global test loss: 0.730, Global test accuracy: 77.71
Round  80, Train loss: 0.406, Test loss: 0.971, Test accuracy: 72.68
Round  80, Global train loss: 0.406, Global test loss: 0.753, Global test accuracy: 77.90
Round  81, Train loss: 0.399, Test loss: 0.969, Test accuracy: 72.80
Round  81, Global train loss: 0.399, Global test loss: 0.772, Global test accuracy: 77.39
Round  82, Train loss: 0.422, Test loss: 0.974, Test accuracy: 72.80
Round  82, Global train loss: 0.422, Global test loss: 0.760, Global test accuracy: 77.82
Round  83, Train loss: 0.421, Test loss: 0.980, Test accuracy: 72.70
Round  83, Global train loss: 0.421, Global test loss: 0.746, Global test accuracy: 77.22
Round  84, Train loss: 0.397, Test loss: 0.993, Test accuracy: 72.53
Round  84, Global train loss: 0.397, Global test loss: 0.768, Global test accuracy: 77.52
Round  85, Train loss: 0.467, Test loss: 0.996, Test accuracy: 72.67
Round  85, Global train loss: 0.467, Global test loss: 0.730, Global test accuracy: 78.11
Round  86, Train loss: 0.409, Test loss: 1.002, Test accuracy: 72.51
Round  86, Global train loss: 0.409, Global test loss: 0.753, Global test accuracy: 77.78
Round  87, Train loss: 0.398, Test loss: 0.996, Test accuracy: 72.67
Round  87, Global train loss: 0.398, Global test loss: 0.766, Global test accuracy: 77.62
Round  88, Train loss: 0.432, Test loss: 0.984, Test accuracy: 72.82
Round  88, Global train loss: 0.432, Global test loss: 0.739, Global test accuracy: 77.92
Round  89, Train loss: 0.424, Test loss: 0.981, Test accuracy: 72.96
Round  89, Global train loss: 0.424, Global test loss: 0.754, Global test accuracy: 77.59
Round  90, Train loss: 0.419, Test loss: 0.980, Test accuracy: 72.84
Round  90, Global train loss: 0.419, Global test loss: 0.744, Global test accuracy: 77.21
Round  91, Train loss: 0.388, Test loss: 0.984, Test accuracy: 72.82
Round  91, Global train loss: 0.388, Global test loss: 0.764, Global test accuracy: 77.51
Round  92, Train loss: 0.389, Test loss: 0.989, Test accuracy: 72.86
Round  92, Global train loss: 0.389, Global test loss: 0.756, Global test accuracy: 77.91
Round  93, Train loss: 0.376, Test loss: 0.987, Test accuracy: 73.08
Round  93, Global train loss: 0.376, Global test loss: 0.758, Global test accuracy: 77.24
Round  94, Train loss: 0.399, Test loss: 0.990, Test accuracy: 72.95
Round  94, Global train loss: 0.399, Global test loss: 0.754, Global test accuracy: 77.75
Round  95, Train loss: 0.402, Test loss: 0.981, Test accuracy: 73.36
Round  95, Global train loss: 0.402, Global test loss: 0.769, Global test accuracy: 77.44
Round  96, Train loss: 0.432, Test loss: 0.993, Test accuracy: 73.05
Round  96, Global train loss: 0.432, Global test loss: 0.736, Global test accuracy: 77.78
Round  97, Train loss: 0.393, Test loss: 1.000, Test accuracy: 72.99
Round  97, Global train loss: 0.393, Global test loss: 0.758, Global test accuracy: 77.73
Round  98, Train loss: 0.391, Test loss: 0.990, Test accuracy: 73.14
Round  98, Global train loss: 0.391, Global test loss: 0.764, Global test accuracy: 77.69
Round  99, Train loss: 0.398, Test loss: 0.990, Test accuracy: 73.30
Round  99, Global train loss: 0.398, Global test loss: 0.758, Global test accuracy: 77.91
Final Round, Train loss: 0.238, Test loss: 1.082, Test accuracy: 73.85
Final Round, Global train loss: 0.238, Global test loss: 0.758, Global test accuracy: 77.91
Average accuracy final 10 rounds: 73.03999999999999 

Average global accuracy final 10 rounds: 77.61824999999999 

5590.381016492844
[4.573133230209351, 9.146266460418701, 13.41658878326416, 17.68691110610962, 21.977622509002686, 26.268333911895752, 30.543285369873047, 34.81823682785034, 39.097599267959595, 43.37696170806885, 47.64836287498474, 51.919764041900635, 56.20458245277405, 60.48940086364746, 64.77055287361145, 69.05170488357544, 73.29708862304688, 77.54247236251831, 81.7798752784729, 86.01727819442749, 90.26620841026306, 94.51513862609863, 98.7605631351471, 103.00598764419556, 107.27392673492432, 111.54186582565308, 115.8041980266571, 120.06653022766113, 124.3193826675415, 128.57223510742188, 132.83747458457947, 137.10271406173706, 141.3443260192871, 145.58593797683716, 149.83115792274475, 154.07637786865234, 158.32496118545532, 162.5735445022583, 166.8285083770752, 171.0834722518921, 175.33529138565063, 179.58711051940918, 183.8438081741333, 188.10050582885742, 192.34845399856567, 196.59640216827393, 200.8452455997467, 205.09408903121948, 209.32101345062256, 213.54793787002563, 217.7860448360443, 222.024151802063, 226.27431440353394, 230.52447700500488, 234.76496505737305, 239.0054531097412, 243.25305461883545, 247.5006561279297, 251.75181698799133, 256.002977848053, 260.28881430625916, 264.57465076446533, 268.8362798690796, 273.09790897369385, 277.39211797714233, 281.6863269805908, 285.9658958911896, 290.24546480178833, 294.52226066589355, 298.7990565299988, 303.07701802253723, 307.3549795150757, 311.5879485607147, 315.82091760635376, 320.07714891433716, 324.33338022232056, 328.60589718818665, 332.87841415405273, 337.05876874923706, 341.2391233444214, 345.4263377189636, 349.61355209350586, 353.740740776062, 357.86792945861816, 361.9697299003601, 366.07153034210205, 370.2770564556122, 374.4825825691223, 378.6410083770752, 382.7994341850281, 387.0606143474579, 391.3217945098877, 395.4735543727875, 399.62531423568726, 403.7798502445221, 407.93438625335693, 412.1354429721832, 416.3364996910095, 420.84807682037354, 425.35965394973755, 429.6645658016205, 433.9694776535034, 438.21807885169983, 442.46668004989624, 446.9620273113251, 451.4573745727539, 455.78493785858154, 460.1125011444092, 464.44300603866577, 468.77351093292236, 472.99938440322876, 477.22525787353516, 481.69028639793396, 486.15531492233276, 490.5549190044403, 494.95452308654785, 499.47224593162537, 503.9899687767029, 508.3559341430664, 512.7218995094299, 517.0445384979248, 521.3671774864197, 525.4724040031433, 529.577630519867, 533.6330120563507, 537.6883935928345, 541.8588697910309, 546.0293459892273, 550.1720261573792, 554.314706325531, 558.3037662506104, 562.2928261756897, 566.8910484313965, 571.4892706871033, 576.0196096897125, 580.5499486923218, 584.8125283718109, 589.0751080513, 593.4253165721893, 597.7755250930786, 602.1039717197418, 606.432418346405, 610.726443529129, 615.020468711853, 619.2889382839203, 623.5574078559875, 627.8027908802032, 632.048173904419, 636.3231835365295, 640.5981931686401, 644.9104008674622, 649.2226085662842, 653.6586933135986, 658.0947780609131, 662.4845335483551, 666.8742890357971, 671.1962213516235, 675.51815366745, 679.7782413959503, 684.0383291244507, 688.2805144786835, 692.5226998329163, 696.778124332428, 701.0335488319397, 705.290118932724, 709.5466890335083, 713.8041229248047, 718.0615568161011, 722.3237438201904, 726.5859308242798, 730.8486223220825, 735.1113138198853, 739.3707118034363, 743.6301097869873, 747.895327091217, 752.1605443954468, 756.4015982151031, 760.6426520347595, 764.8712749481201, 769.0998978614807, 773.3350257873535, 777.5701537132263, 781.8069133758545, 786.0436730384827, 790.2829484939575, 794.5222239494324, 798.7721209526062, 803.02201795578, 807.2524180412292, 811.4828181266785, 815.7061040401459, 819.9293899536133, 824.183336019516, 828.4372820854187, 832.6822285652161, 836.9271750450134, 841.1761615276337, 845.4251480102539, 849.6715257167816, 853.9179034233093, 856.044417142868, 858.1709308624268]
[36.965, 36.965, 42.7575, 42.7575, 45.3075, 45.3075, 49.03, 49.03, 50.2075, 50.2075, 51.5075, 51.5075, 53.84, 53.84, 56.0225, 56.0225, 56.6, 56.6, 57.6375, 57.6375, 59.1225, 59.1225, 59.8225, 59.8225, 59.9975, 59.9975, 60.65, 60.65, 61.145, 61.145, 62.475, 62.475, 63.62, 63.62, 64.3325, 64.3325, 64.9075, 64.9075, 65.1225, 65.1225, 65.5125, 65.5125, 66.625, 66.625, 67.3075, 67.3075, 67.7675, 67.7675, 68.0275, 68.0275, 67.8125, 67.8125, 68.05, 68.05, 68.3225, 68.3225, 68.6125, 68.6125, 68.585, 68.585, 68.53, 68.53, 68.99, 68.99, 69.3625, 69.3625, 69.29, 69.29, 69.2325, 69.2325, 69.505, 69.505, 69.8875, 69.8875, 69.8725, 69.8725, 70.135, 70.135, 70.1275, 70.1275, 69.9025, 69.9025, 70.0225, 70.0225, 70.3025, 70.3025, 70.4175, 70.4175, 70.685, 70.685, 70.9275, 70.9275, 71.065, 71.065, 71.0075, 71.0075, 71.13, 71.13, 71.015, 71.015, 71.17, 71.17, 71.1325, 71.1325, 71.2675, 71.2675, 71.43, 71.43, 71.4675, 71.4675, 71.67, 71.67, 71.7325, 71.7325, 72.0025, 72.0025, 72.2175, 72.2175, 72.255, 72.255, 72.12, 72.12, 71.7975, 71.7975, 71.735, 71.735, 71.9825, 71.9825, 71.835, 71.835, 72.4175, 72.4175, 72.365, 72.365, 72.2625, 72.2625, 72.37, 72.37, 72.3175, 72.3175, 72.1875, 72.1875, 72.395, 72.395, 72.4575, 72.4575, 72.4175, 72.4175, 72.4775, 72.4775, 72.5025, 72.5025, 72.435, 72.435, 72.385, 72.385, 72.595, 72.595, 72.6525, 72.6525, 72.6825, 72.6825, 72.8, 72.8, 72.795, 72.795, 72.705, 72.705, 72.53, 72.53, 72.67, 72.67, 72.5075, 72.5075, 72.6725, 72.6725, 72.8175, 72.8175, 72.9575, 72.9575, 72.845, 72.845, 72.82, 72.82, 72.865, 72.865, 73.0775, 73.0775, 72.9475, 72.9475, 73.3625, 73.3625, 73.05, 73.05, 72.99, 72.99, 73.14, 73.14, 73.3025, 73.3025, 73.8475, 73.8475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.066, Test loss: 1.840, Test accuracy: 32.81
Round   0, Global train loss: 2.066, Global test loss: 1.833, Global test accuracy: 34.25
Round   1, Train loss: 1.795, Test loss: 1.683, Test accuracy: 38.03
Round   1, Global train loss: 1.795, Global test loss: 1.611, Global test accuracy: 41.74
Round   2, Train loss: 1.682, Test loss: 1.588, Test accuracy: 41.72
Round   2, Global train loss: 1.682, Global test loss: 1.522, Global test accuracy: 44.01
Round   3, Train loss: 1.622, Test loss: 1.548, Test accuracy: 43.77
Round   3, Global train loss: 1.622, Global test loss: 1.434, Global test accuracy: 48.80
Round   4, Train loss: 1.552, Test loss: 1.528, Test accuracy: 44.60
Round   4, Global train loss: 1.552, Global test loss: 1.368, Global test accuracy: 51.49
Round   5, Train loss: 1.494, Test loss: 1.509, Test accuracy: 44.94
Round   5, Global train loss: 1.494, Global test loss: 1.316, Global test accuracy: 52.64
Round   6, Train loss: 1.435, Test loss: 1.459, Test accuracy: 47.18
Round   6, Global train loss: 1.435, Global test loss: 1.274, Global test accuracy: 55.49
Round   7, Train loss: 1.394, Test loss: 1.417, Test accuracy: 48.66
Round   7, Global train loss: 1.394, Global test loss: 1.221, Global test accuracy: 57.26
Round   8, Train loss: 1.356, Test loss: 1.360, Test accuracy: 50.81
Round   8, Global train loss: 1.356, Global test loss: 1.176, Global test accuracy: 58.46
Round   9, Train loss: 1.311, Test loss: 1.316, Test accuracy: 52.84
Round   9, Global train loss: 1.311, Global test loss: 1.137, Global test accuracy: 60.12
Round  10, Train loss: 1.267, Test loss: 1.301, Test accuracy: 53.87
Round  10, Global train loss: 1.267, Global test loss: 1.098, Global test accuracy: 61.52
Round  11, Train loss: 1.232, Test loss: 1.296, Test accuracy: 54.00
Round  11, Global train loss: 1.232, Global test loss: 1.083, Global test accuracy: 62.38
Round  12, Train loss: 1.199, Test loss: 1.279, Test accuracy: 54.63
Round  12, Global train loss: 1.199, Global test loss: 1.051, Global test accuracy: 63.05
Round  13, Train loss: 1.185, Test loss: 1.243, Test accuracy: 55.97
Round  13, Global train loss: 1.185, Global test loss: 1.014, Global test accuracy: 64.62
Round  14, Train loss: 1.163, Test loss: 1.218, Test accuracy: 56.91
Round  14, Global train loss: 1.163, Global test loss: 1.009, Global test accuracy: 65.25
Round  15, Train loss: 1.135, Test loss: 1.196, Test accuracy: 57.89
Round  15, Global train loss: 1.135, Global test loss: 1.001, Global test accuracy: 65.72
Round  16, Train loss: 1.114, Test loss: 1.171, Test accuracy: 58.78
Round  16, Global train loss: 1.114, Global test loss: 0.985, Global test accuracy: 65.66
Round  17, Train loss: 1.086, Test loss: 1.156, Test accuracy: 59.46
Round  17, Global train loss: 1.086, Global test loss: 0.971, Global test accuracy: 66.58
Round  18, Train loss: 1.079, Test loss: 1.116, Test accuracy: 61.05
Round  18, Global train loss: 1.079, Global test loss: 0.948, Global test accuracy: 67.31
Round  19, Train loss: 1.051, Test loss: 1.106, Test accuracy: 61.49
Round  19, Global train loss: 1.051, Global test loss: 0.938, Global test accuracy: 68.31
Round  20, Train loss: 1.031, Test loss: 1.090, Test accuracy: 62.22
Round  20, Global train loss: 1.031, Global test loss: 0.916, Global test accuracy: 68.41
Round  21, Train loss: 1.006, Test loss: 1.075, Test accuracy: 62.70
Round  21, Global train loss: 1.006, Global test loss: 0.906, Global test accuracy: 68.98
Round  22, Train loss: 0.982, Test loss: 1.067, Test accuracy: 63.15
Round  22, Global train loss: 0.982, Global test loss: 0.893, Global test accuracy: 69.07
Round  23, Train loss: 0.958, Test loss: 1.067, Test accuracy: 63.05
Round  23, Global train loss: 0.958, Global test loss: 0.894, Global test accuracy: 69.74
Round  24, Train loss: 0.960, Test loss: 1.039, Test accuracy: 63.97
Round  24, Global train loss: 0.960, Global test loss: 0.874, Global test accuracy: 70.10
Round  25, Train loss: 0.949, Test loss: 1.038, Test accuracy: 64.05
Round  25, Global train loss: 0.949, Global test loss: 0.872, Global test accuracy: 69.94
Round  26, Train loss: 0.925, Test loss: 1.022, Test accuracy: 64.70
Round  26, Global train loss: 0.925, Global test loss: 0.858, Global test accuracy: 70.67
Round  27, Train loss: 0.912, Test loss: 1.002, Test accuracy: 65.52
Round  27, Global train loss: 0.912, Global test loss: 0.851, Global test accuracy: 70.92
Round  28, Train loss: 0.914, Test loss: 0.998, Test accuracy: 65.78
Round  28, Global train loss: 0.914, Global test loss: 0.837, Global test accuracy: 71.44
Round  29, Train loss: 0.896, Test loss: 0.993, Test accuracy: 66.00
Round  29, Global train loss: 0.896, Global test loss: 0.834, Global test accuracy: 71.59
Round  30, Train loss: 0.879, Test loss: 0.986, Test accuracy: 66.41
Round  30, Global train loss: 0.879, Global test loss: 0.817, Global test accuracy: 72.62
Round  31, Train loss: 0.874, Test loss: 0.988, Test accuracy: 66.48
Round  31, Global train loss: 0.874, Global test loss: 0.821, Global test accuracy: 72.02
Round  32, Train loss: 0.855, Test loss: 0.986, Test accuracy: 66.76
Round  32, Global train loss: 0.855, Global test loss: 0.803, Global test accuracy: 72.85
Round  33, Train loss: 0.829, Test loss: 0.966, Test accuracy: 67.35
Round  33, Global train loss: 0.829, Global test loss: 0.808, Global test accuracy: 72.53
Round  34, Train loss: 0.846, Test loss: 0.962, Test accuracy: 67.33
Round  34, Global train loss: 0.846, Global test loss: 0.819, Global test accuracy: 71.68
Round  35, Train loss: 0.818, Test loss: 0.948, Test accuracy: 67.71
Round  35, Global train loss: 0.818, Global test loss: 0.781, Global test accuracy: 73.04
Round  36, Train loss: 0.813, Test loss: 0.937, Test accuracy: 68.18
Round  36, Global train loss: 0.813, Global test loss: 0.785, Global test accuracy: 73.38
Round  37, Train loss: 0.818, Test loss: 0.937, Test accuracy: 68.16
Round  37, Global train loss: 0.818, Global test loss: 0.775, Global test accuracy: 73.49
Round  38, Train loss: 0.787, Test loss: 0.942, Test accuracy: 68.05
Round  38, Global train loss: 0.787, Global test loss: 0.782, Global test accuracy: 73.39
Round  39, Train loss: 0.801, Test loss: 0.930, Test accuracy: 68.58
Round  39, Global train loss: 0.801, Global test loss: 0.762, Global test accuracy: 73.75
Round  40, Train loss: 0.795, Test loss: 0.926, Test accuracy: 68.75
Round  40, Global train loss: 0.795, Global test loss: 0.768, Global test accuracy: 73.60
Round  41, Train loss: 0.759, Test loss: 0.918, Test accuracy: 69.00
Round  41, Global train loss: 0.759, Global test loss: 0.757, Global test accuracy: 74.28
Round  42, Train loss: 0.776, Test loss: 0.916, Test accuracy: 69.14
Round  42, Global train loss: 0.776, Global test loss: 0.760, Global test accuracy: 74.36
Round  43, Train loss: 0.742, Test loss: 0.916, Test accuracy: 69.13
Round  43, Global train loss: 0.742, Global test loss: 0.760, Global test accuracy: 73.86
Round  44, Train loss: 0.735, Test loss: 0.918, Test accuracy: 69.39
Round  44, Global train loss: 0.735, Global test loss: 0.767, Global test accuracy: 73.72
Round  45, Train loss: 0.731, Test loss: 0.913, Test accuracy: 69.50
Round  45, Global train loss: 0.731, Global test loss: 0.750, Global test accuracy: 74.69
Round  46, Train loss: 0.746, Test loss: 0.911, Test accuracy: 69.59
Round  46, Global train loss: 0.746, Global test loss: 0.759, Global test accuracy: 74.47
Round  47, Train loss: 0.736, Test loss: 0.901, Test accuracy: 69.97
Round  47, Global train loss: 0.736, Global test loss: 0.745, Global test accuracy: 74.73
Round  48, Train loss: 0.706, Test loss: 0.909, Test accuracy: 69.81
Round  48, Global train loss: 0.706, Global test loss: 0.746, Global test accuracy: 74.97
Round  49, Train loss: 0.691, Test loss: 0.899, Test accuracy: 70.06
Round  49, Global train loss: 0.691, Global test loss: 0.751, Global test accuracy: 74.64
Round  50, Train loss: 0.732, Test loss: 0.903, Test accuracy: 70.20
Round  50, Global train loss: 0.732, Global test loss: 0.740, Global test accuracy: 75.44
Round  51, Train loss: 0.713, Test loss: 0.889, Test accuracy: 70.71
Round  51, Global train loss: 0.713, Global test loss: 0.730, Global test accuracy: 75.64
Round  52, Train loss: 0.682, Test loss: 0.885, Test accuracy: 70.87
Round  52, Global train loss: 0.682, Global test loss: 0.732, Global test accuracy: 75.64
Round  53, Train loss: 0.695, Test loss: 0.882, Test accuracy: 70.97
Round  53, Global train loss: 0.695, Global test loss: 0.725, Global test accuracy: 75.61
Round  54, Train loss: 0.696, Test loss: 0.883, Test accuracy: 70.88
Round  54, Global train loss: 0.696, Global test loss: 0.731, Global test accuracy: 75.56
Round  55, Train loss: 0.687, Test loss: 0.883, Test accuracy: 70.86
Round  55, Global train loss: 0.687, Global test loss: 0.722, Global test accuracy: 76.11
Round  56, Train loss: 0.671, Test loss: 0.893, Test accuracy: 70.71
Round  56, Global train loss: 0.671, Global test loss: 0.741, Global test accuracy: 75.28
Round  57, Train loss: 0.670, Test loss: 0.886, Test accuracy: 71.05
Round  57, Global train loss: 0.670, Global test loss: 0.731, Global test accuracy: 75.73
Round  58, Train loss: 0.669, Test loss: 0.886, Test accuracy: 71.12
Round  58, Global train loss: 0.669, Global test loss: 0.722, Global test accuracy: 75.52
Round  59, Train loss: 0.645, Test loss: 0.885, Test accuracy: 71.18
Round  59, Global train loss: 0.645, Global test loss: 0.721, Global test accuracy: 75.81
Round  60, Train loss: 0.665, Test loss: 0.874, Test accuracy: 71.44
Round  60, Global train loss: 0.665, Global test loss: 0.708, Global test accuracy: 76.13
Round  61, Train loss: 0.654, Test loss: 0.883, Test accuracy: 71.17
Round  61, Global train loss: 0.654, Global test loss: 0.725, Global test accuracy: 75.38
Round  62, Train loss: 0.656, Test loss: 0.884, Test accuracy: 71.05
Round  62, Global train loss: 0.656, Global test loss: 0.723, Global test accuracy: 76.03
Round  63, Train loss: 0.607, Test loss: 0.879, Test accuracy: 71.15
Round  63, Global train loss: 0.607, Global test loss: 0.730, Global test accuracy: 75.63
Round  64, Train loss: 0.621, Test loss: 0.879, Test accuracy: 71.22
Round  64, Global train loss: 0.621, Global test loss: 0.721, Global test accuracy: 75.79
Round  65, Train loss: 0.643, Test loss: 0.872, Test accuracy: 71.43
Round  65, Global train loss: 0.643, Global test loss: 0.714, Global test accuracy: 76.05
Round  66, Train loss: 0.614, Test loss: 0.866, Test accuracy: 71.66
Round  66, Global train loss: 0.614, Global test loss: 0.711, Global test accuracy: 75.77
Round  67, Train loss: 0.617, Test loss: 0.875, Test accuracy: 71.47
Round  67, Global train loss: 0.617, Global test loss: 0.719, Global test accuracy: 76.07
Round  68, Train loss: 0.635, Test loss: 0.888, Test accuracy: 71.31
Round  68, Global train loss: 0.635, Global test loss: 0.720, Global test accuracy: 75.75
Round  69, Train loss: 0.610, Test loss: 0.884, Test accuracy: 71.58
Round  69, Global train loss: 0.610, Global test loss: 0.719, Global test accuracy: 75.64
Round  70, Train loss: 0.625, Test loss: 0.889, Test accuracy: 71.49
Round  70, Global train loss: 0.625, Global test loss: 0.715, Global test accuracy: 76.40
Round  71, Train loss: 0.599, Test loss: 0.869, Test accuracy: 71.98
Round  71, Global train loss: 0.599, Global test loss: 0.717, Global test accuracy: 75.86
Round  72, Train loss: 0.608, Test loss: 0.870, Test accuracy: 71.95
Round  72, Global train loss: 0.608, Global test loss: 0.712, Global test accuracy: 76.40
Round  73, Train loss: 0.600, Test loss: 0.880, Test accuracy: 71.88
Round  73, Global train loss: 0.600, Global test loss: 0.712, Global test accuracy: 76.09
Round  74, Train loss: 0.599, Test loss: 0.883, Test accuracy: 71.87
Round  74, Global train loss: 0.599, Global test loss: 0.717, Global test accuracy: 76.08
Round  75, Train loss: 0.579, Test loss: 0.879, Test accuracy: 71.87
Round  75, Global train loss: 0.579, Global test loss: 0.714, Global test accuracy: 76.01
Round  76, Train loss: 0.590, Test loss: 0.879, Test accuracy: 71.73
Round  76, Global train loss: 0.590, Global test loss: 0.710, Global test accuracy: 76.25
Round  77, Train loss: 0.574, Test loss: 0.869, Test accuracy: 72.17
Round  77, Global train loss: 0.574, Global test loss: 0.714, Global test accuracy: 76.30
Round  78, Train loss: 0.582, Test loss: 0.867, Test accuracy: 72.11
Round  78, Global train loss: 0.582, Global test loss: 0.707, Global test accuracy: 76.49
Round  79, Train loss: 0.583, Test loss: 0.874, Test accuracy: 72.13
Round  79, Global train loss: 0.583, Global test loss: 0.714, Global test accuracy: 76.51
Round  80, Train loss: 0.578, Test loss: 0.863, Test accuracy: 72.33
Round  80, Global train loss: 0.578, Global test loss: 0.717, Global test accuracy: 76.29
Round  81, Train loss: 0.577, Test loss: 0.872, Test accuracy: 72.23
Round  81, Global train loss: 0.577, Global test loss: 0.724, Global test accuracy: 76.41
Round  82, Train loss: 0.563, Test loss: 0.881, Test accuracy: 72.00
Round  82, Global train loss: 0.563, Global test loss: 0.717, Global test accuracy: 76.18
Round  83, Train loss: 0.563, Test loss: 0.870, Test accuracy: 72.31
Round  83, Global train loss: 0.563, Global test loss: 0.707, Global test accuracy: 76.77
Round  84, Train loss: 0.570, Test loss: 0.868, Test accuracy: 72.20
Round  84, Global train loss: 0.570, Global test loss: 0.706, Global test accuracy: 76.55
Round  85, Train loss: 0.559, Test loss: 0.864, Test accuracy: 72.45
Round  85, Global train loss: 0.559, Global test loss: 0.707, Global test accuracy: 76.28
Round  86, Train loss: 0.552, Test loss: 0.863, Test accuracy: 72.53
Round  86, Global train loss: 0.552, Global test loss: 0.717, Global test accuracy: 76.22
Round  87, Train loss: 0.567, Test loss: 0.866, Test accuracy: 72.48
Round  87, Global train loss: 0.567, Global test loss: 0.703, Global test accuracy: 76.33
Round  88, Train loss: 0.547, Test loss: 0.868, Test accuracy: 72.56
Round  88, Global train loss: 0.547, Global test loss: 0.708, Global test accuracy: 76.48
Round  89, Train loss: 0.529, Test loss: 0.876, Test accuracy: 72.53
Round  89, Global train loss: 0.529, Global test loss: 0.710, Global test accuracy: 76.56
Round  90, Train loss: 0.542, Test loss: 0.867, Test accuracy: 72.64
Round  90, Global train loss: 0.542, Global test loss: 0.708, Global test accuracy: 76.98
Round  91, Train loss: 0.573, Test loss: 0.867, Test accuracy: 72.75
Round  91, Global train loss: 0.573, Global test loss: 0.712, Global test accuracy: 76.61
Round  92, Train loss: 0.537, Test loss: 0.870, Test accuracy: 72.86
Round  92, Global train loss: 0.537, Global test loss: 0.709, Global test accuracy: 77.02
Round  93, Train loss: 0.537, Test loss: 0.868, Test accuracy: 72.78
Round  93, Global train loss: 0.537, Global test loss: 0.710, Global test accuracy: 76.67
Round  94, Train loss: 0.518, Test loss: 0.874, Test accuracy: 72.78
Round  94, Global train loss: 0.518, Global test loss: 0.710, Global test accuracy: 76.68
Round  95, Train loss: 0.525, Test loss: 0.872, Test accuracy: 73.06
Round  95, Global train loss: 0.525, Global test loss: 0.707, Global test accuracy: 76.98/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  96, Train loss: 0.505, Test loss: 0.884, Test accuracy: 72.78
Round  96, Global train loss: 0.505, Global test loss: 0.721, Global test accuracy: 76.41
Round  97, Train loss: 0.527, Test loss: 0.892, Test accuracy: 72.64
Round  97, Global train loss: 0.527, Global test loss: 0.715, Global test accuracy: 76.75
Round  98, Train loss: 0.514, Test loss: 0.885, Test accuracy: 72.76
Round  98, Global train loss: 0.514, Global test loss: 0.722, Global test accuracy: 76.57
Round  99, Train loss: 0.504, Test loss: 0.877, Test accuracy: 72.94
Round  99, Global train loss: 0.504, Global test loss: 0.723, Global test accuracy: 76.95
Final Round, Train loss: 0.329, Test loss: 0.972, Test accuracy: 72.99
Final Round, Global train loss: 0.329, Global test loss: 0.723, Global test accuracy: 76.95
Average accuracy final 10 rounds: 72.79899999999999 

Average global accuracy final 10 rounds: 76.76375000000002 

5874.595259904861
[4.9520323276519775, 9.904064655303955, 14.503034353256226, 19.102004051208496, 23.68638014793396, 28.270756244659424, 32.89557361602783, 37.52039098739624, 42.094061851501465, 46.66773271560669, 51.243892431259155, 55.82005214691162, 60.448535203933716, 65.07701826095581, 69.66893172264099, 74.26084518432617, 78.8120596408844, 83.36327409744263, 87.9114179611206, 92.45956182479858, 97.04533839225769, 101.6311149597168, 106.16321158409119, 110.69530820846558, 115.23192048072815, 119.76853275299072, 124.29112243652344, 128.81371212005615, 133.3484103679657, 137.88310861587524, 142.44929313659668, 147.01547765731812, 151.83692622184753, 156.65837478637695, 161.33154392242432, 166.00471305847168, 170.80063271522522, 175.59655237197876, 180.2075183391571, 184.81848430633545, 189.5124008655548, 194.20631742477417, 198.79909682273865, 203.39187622070312, 207.9774830341339, 212.5630898475647, 217.14190292358398, 221.72071599960327, 226.29654812812805, 230.87238025665283, 235.7204065322876, 240.56843280792236, 245.1743061542511, 249.78017950057983, 254.39164352416992, 259.00310754776, 263.6103768348694, 268.21764612197876, 272.88610219955444, 277.5545582771301, 282.3290128707886, 287.103467464447, 291.81680488586426, 296.5301423072815, 301.2515218257904, 305.9729013442993, 310.7943203449249, 315.61573934555054, 320.23533034324646, 324.8549213409424, 329.45641255378723, 334.0579037666321, 338.825395822525, 343.59288787841797, 348.19214725494385, 352.7914066314697, 357.4528591632843, 362.1143116950989, 366.975506067276, 371.8367004394531, 376.7535283565521, 381.6703562736511, 386.26579308509827, 390.8612298965454, 395.8780286312103, 400.89482736587524, 405.89664030075073, 410.8984532356262, 415.9550485610962, 421.01164388656616, 426.2094955444336, 431.407347202301, 436.40175914764404, 441.39617109298706, 446.38531279563904, 451.374454498291, 456.3300459384918, 461.2856373786926, 466.3502311706543, 471.41482496261597, 476.3877172470093, 481.3606095314026, 485.9628655910492, 490.5651216506958, 495.17151641845703, 499.77791118621826, 504.3909385204315, 509.0039658546448, 513.6114571094513, 518.2189483642578, 522.8225741386414, 527.4261999130249, 532.0340342521667, 536.6418685913086, 541.2309603691101, 545.8200521469116, 550.6935534477234, 555.5670547485352, 560.4037725925446, 565.240490436554, 569.9164695739746, 574.5924487113953, 579.218939781189, 583.8454308509827, 588.5029480457306, 593.1604652404785, 597.7731478214264, 602.3858304023743, 607.0015184879303, 611.6172065734863, 616.2104437351227, 620.803680896759, 625.383483171463, 629.963285446167, 634.5443434715271, 639.1254014968872, 643.6867911815643, 648.2481808662415, 652.8387489318848, 657.4293169975281, 662.0245907306671, 666.6198644638062, 671.209370136261, 675.7988758087158, 680.3958737850189, 684.992871761322, 689.5825219154358, 694.1721720695496, 698.7633380889893, 703.354504108429, 707.9563264846802, 712.5581488609314, 717.1445984840393, 721.7310481071472, 726.3159561157227, 730.9008641242981, 735.5008928775787, 740.1009216308594, 744.7003040313721, 749.2996864318848, 753.8921258449554, 758.4845652580261, 763.0512833595276, 767.618001461029, 772.2169215679169, 776.8158416748047, 781.4048407077789, 785.9938397407532, 790.5636744499207, 795.1335091590881, 799.7023503780365, 804.2711915969849, 808.8470549583435, 813.4229183197021, 817.9904160499573, 822.5579137802124, 827.1412045955658, 831.7244954109192, 836.3613038063049, 840.9981122016907, 845.600485086441, 850.2028579711914, 854.8400297164917, 859.477201461792, 864.1081671714783, 868.7391328811646, 873.3714823722839, 878.0038318634033, 882.5702016353607, 887.1365714073181, 891.7647426128387, 896.3929138183594, 901.0146386623383, 905.6363635063171, 910.2569897174835, 914.8776159286499, 919.5336337089539, 924.1896514892578, 928.8091192245483, 933.4285869598389, 935.7738306522369, 938.119074344635]
[32.815, 32.815, 38.035, 38.035, 41.715, 41.715, 43.775, 43.775, 44.5975, 44.5975, 44.9375, 44.9375, 47.1775, 47.1775, 48.66, 48.66, 50.8075, 50.8075, 52.8425, 52.8425, 53.865, 53.865, 54.0, 54.0, 54.6275, 54.6275, 55.9725, 55.9725, 56.91, 56.91, 57.8875, 57.8875, 58.7775, 58.7775, 59.4625, 59.4625, 61.045, 61.045, 61.4875, 61.4875, 62.215, 62.215, 62.6975, 62.6975, 63.1475, 63.1475, 63.0525, 63.0525, 63.97, 63.97, 64.05, 64.05, 64.7, 64.7, 65.5175, 65.5175, 65.775, 65.775, 66.0, 66.0, 66.405, 66.405, 66.48, 66.48, 66.7625, 66.7625, 67.3525, 67.3525, 67.335, 67.335, 67.7075, 67.7075, 68.1775, 68.1775, 68.1575, 68.1575, 68.0475, 68.0475, 68.5775, 68.5775, 68.745, 68.745, 68.9975, 68.9975, 69.145, 69.145, 69.1325, 69.1325, 69.3875, 69.3875, 69.5025, 69.5025, 69.59, 69.59, 69.97, 69.97, 69.805, 69.805, 70.0575, 70.0575, 70.2, 70.2, 70.7075, 70.7075, 70.8675, 70.8675, 70.97, 70.97, 70.8775, 70.8775, 70.855, 70.855, 70.7075, 70.7075, 71.0525, 71.0525, 71.125, 71.125, 71.18, 71.18, 71.44, 71.44, 71.175, 71.175, 71.0525, 71.0525, 71.1525, 71.1525, 71.22, 71.22, 71.4325, 71.4325, 71.6575, 71.6575, 71.4725, 71.4725, 71.3075, 71.3075, 71.585, 71.585, 71.49, 71.49, 71.9775, 71.9775, 71.9525, 71.9525, 71.8775, 71.8775, 71.8725, 71.8725, 71.87, 71.87, 71.7275, 71.7275, 72.17, 72.17, 72.11, 72.11, 72.1325, 72.1325, 72.3325, 72.3325, 72.235, 72.235, 72.005, 72.005, 72.31, 72.31, 72.2025, 72.2025, 72.4475, 72.4475, 72.53, 72.53, 72.48, 72.48, 72.565, 72.565, 72.535, 72.535, 72.6375, 72.6375, 72.75, 72.75, 72.8575, 72.8575, 72.78, 72.78, 72.785, 72.785, 73.0625, 73.0625, 72.7825, 72.7825, 72.6375, 72.6375, 72.7625, 72.7625, 72.935, 72.935, 72.99, 72.99]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.220, Test loss: 1.959, Test accuracy: 31.54
Round   1, Train loss: 1.874, Test loss: 1.657, Test accuracy: 41.35
Round   2, Train loss: 1.679, Test loss: 1.542, Test accuracy: 45.83
Round   3, Train loss: 1.579, Test loss: 1.467, Test accuracy: 49.23
Round   4, Train loss: 1.511, Test loss: 1.379, Test accuracy: 52.52
Round   5, Train loss: 1.426, Test loss: 1.339, Test accuracy: 55.47
Round   6, Train loss: 1.384, Test loss: 1.270, Test accuracy: 57.15
Round   7, Train loss: 1.316, Test loss: 1.209, Test accuracy: 59.62
Round   8, Train loss: 1.268, Test loss: 1.193, Test accuracy: 60.03
Round   9, Train loss: 1.228, Test loss: 1.149, Test accuracy: 61.49
Round  10, Train loss: 1.190, Test loss: 1.128, Test accuracy: 62.38
Round  11, Train loss: 1.144, Test loss: 1.103, Test accuracy: 63.23
Round  12, Train loss: 1.129, Test loss: 1.084, Test accuracy: 63.68
Round  13, Train loss: 1.096, Test loss: 1.032, Test accuracy: 64.91
Round  14, Train loss: 1.068, Test loss: 0.992, Test accuracy: 65.90
Round  15, Train loss: 1.033, Test loss: 0.988, Test accuracy: 66.34
Round  16, Train loss: 1.034, Test loss: 0.931, Test accuracy: 68.11
Round  17, Train loss: 0.983, Test loss: 0.913, Test accuracy: 68.83
Round  18, Train loss: 0.959, Test loss: 0.913, Test accuracy: 68.43
Round  19, Train loss: 0.953, Test loss: 0.903, Test accuracy: 69.06
Round  20, Train loss: 0.931, Test loss: 0.892, Test accuracy: 69.44
Round  21, Train loss: 0.913, Test loss: 0.888, Test accuracy: 69.29
Round  22, Train loss: 0.892, Test loss: 0.859, Test accuracy: 70.52
Round  23, Train loss: 0.893, Test loss: 0.862, Test accuracy: 70.44
Round  24, Train loss: 0.855, Test loss: 0.849, Test accuracy: 71.37
Round  25, Train loss: 0.846, Test loss: 0.839, Test accuracy: 71.25
Round  26, Train loss: 0.847, Test loss: 0.825, Test accuracy: 71.71
Round  27, Train loss: 0.828, Test loss: 0.827, Test accuracy: 71.71
Round  28, Train loss: 0.799, Test loss: 0.825, Test accuracy: 71.81
Round  29, Train loss: 0.809, Test loss: 0.817, Test accuracy: 72.14
Round  30, Train loss: 0.789, Test loss: 0.819, Test accuracy: 72.21
Round  31, Train loss: 0.775, Test loss: 0.818, Test accuracy: 72.07
Round  32, Train loss: 0.802, Test loss: 0.793, Test accuracy: 73.03
Round  33, Train loss: 0.762, Test loss: 0.792, Test accuracy: 73.36
Round  34, Train loss: 0.746, Test loss: 0.795, Test accuracy: 73.11
Round  35, Train loss: 0.738, Test loss: 0.788, Test accuracy: 73.36
Round  36, Train loss: 0.733, Test loss: 0.782, Test accuracy: 73.50
Round  37, Train loss: 0.718, Test loss: 0.795, Test accuracy: 73.13
Round  38, Train loss: 0.738, Test loss: 0.781, Test accuracy: 73.70
Round  39, Train loss: 0.737, Test loss: 0.780, Test accuracy: 73.45
Round  40, Train loss: 0.709, Test loss: 0.762, Test accuracy: 73.93
Round  41, Train loss: 0.700, Test loss: 0.762, Test accuracy: 74.00
Round  42, Train loss: 0.692, Test loss: 0.757, Test accuracy: 74.07
Round  43, Train loss: 0.678, Test loss: 0.751, Test accuracy: 74.47
Round  44, Train loss: 0.672, Test loss: 0.763, Test accuracy: 73.93
Round  45, Train loss: 0.679, Test loss: 0.747, Test accuracy: 74.66
Round  46, Train loss: 0.673, Test loss: 0.748, Test accuracy: 74.32
Round  47, Train loss: 0.660, Test loss: 0.752, Test accuracy: 74.63
Round  48, Train loss: 0.643, Test loss: 0.738, Test accuracy: 74.76
Round  49, Train loss: 0.621, Test loss: 0.741, Test accuracy: 74.98
Round  50, Train loss: 0.630, Test loss: 0.738, Test accuracy: 74.93
Round  51, Train loss: 0.629, Test loss: 0.735, Test accuracy: 74.88
Round  52, Train loss: 0.650, Test loss: 0.728, Test accuracy: 75.25
Round  53, Train loss: 0.657, Test loss: 0.721, Test accuracy: 75.66
Round  54, Train loss: 0.622, Test loss: 0.729, Test accuracy: 75.38
Round  55, Train loss: 0.610, Test loss: 0.736, Test accuracy: 74.88
Round  56, Train loss: 0.602, Test loss: 0.752, Test accuracy: 74.44
Round  57, Train loss: 0.589, Test loss: 0.744, Test accuracy: 74.57
Round  58, Train loss: 0.604, Test loss: 0.737, Test accuracy: 74.78
Round  59, Train loss: 0.604, Test loss: 0.728, Test accuracy: 75.22
Round  60, Train loss: 0.545, Test loss: 0.730, Test accuracy: 75.25
Round  61, Train loss: 0.594, Test loss: 0.728, Test accuracy: 75.16
Round  62, Train loss: 0.616, Test loss: 0.723, Test accuracy: 75.72
Round  63, Train loss: 0.602, Test loss: 0.734, Test accuracy: 75.30
Round  64, Train loss: 0.569, Test loss: 0.713, Test accuracy: 75.95
Round  65, Train loss: 0.563, Test loss: 0.722, Test accuracy: 75.75
Round  66, Train loss: 0.582, Test loss: 0.724, Test accuracy: 75.74
Round  67, Train loss: 0.587, Test loss: 0.723, Test accuracy: 75.62
Round  68, Train loss: 0.578, Test loss: 0.713, Test accuracy: 76.16
Round  69, Train loss: 0.546, Test loss: 0.720, Test accuracy: 75.94
Round  70, Train loss: 0.563, Test loss: 0.719, Test accuracy: 75.92
Round  71, Train loss: 0.575, Test loss: 0.719, Test accuracy: 75.97
Round  72, Train loss: 0.564, Test loss: 0.732, Test accuracy: 75.53
Round  73, Train loss: 0.534, Test loss: 0.720, Test accuracy: 76.06
Round  74, Train loss: 0.543, Test loss: 0.722, Test accuracy: 76.03
Round  75, Train loss: 0.590, Test loss: 0.710, Test accuracy: 76.41
Round  76, Train loss: 0.524, Test loss: 0.710, Test accuracy: 76.34
Round  77, Train loss: 0.531, Test loss: 0.715, Test accuracy: 76.39
Round  78, Train loss: 0.555, Test loss: 0.704, Test accuracy: 76.86
Round  79, Train loss: 0.529, Test loss: 0.708, Test accuracy: 76.60
Round  80, Train loss: 0.533, Test loss: 0.709, Test accuracy: 76.33
Round  81, Train loss: 0.535, Test loss: 0.712, Test accuracy: 76.69
Round  82, Train loss: 0.523, Test loss: 0.706, Test accuracy: 76.72
Round  83, Train loss: 0.499, Test loss: 0.697, Test accuracy: 76.93
Round  84, Train loss: 0.555, Test loss: 0.706, Test accuracy: 76.71
Round  85, Train loss: 0.493, Test loss: 0.711, Test accuracy: 76.88
Round  86, Train loss: 0.497, Test loss: 0.718, Test accuracy: 76.60
Round  87, Train loss: 0.513, Test loss: 0.720, Test accuracy: 76.76
Round  88, Train loss: 0.492, Test loss: 0.712, Test accuracy: 76.55
Round  89, Train loss: 0.493, Test loss: 0.718, Test accuracy: 76.36
Round  90, Train loss: 0.519, Test loss: 0.714, Test accuracy: 76.60
Round  91, Train loss: 0.474, Test loss: 0.711, Test accuracy: 76.53
Round  92, Train loss: 0.465, Test loss: 0.715, Test accuracy: 76.70
Round  93, Train loss: 0.467, Test loss: 0.724, Test accuracy: 76.41
Round  94, Train loss: 0.510, Test loss: 0.718, Test accuracy: 76.83
Round  95, Train loss: 0.525, Test loss: 0.723, Test accuracy: 76.56
Round  96, Train loss: 0.480, Test loss: 0.719, Test accuracy: 76.47
Round  97, Train loss: 0.509, Test loss: 0.721, Test accuracy: 76.41
Round  98, Train loss: 0.494, Test loss: 0.724, Test accuracy: 76.53
Round  99, Train loss: 0.469, Test loss: 0.708, Test accuracy: 77.00
Final Round, Train loss: 0.405, Test loss: 0.707, Test accuracy: 76.86
Average accuracy final 10 rounds: 76.60425
4170.665553331375
[5.836911916732788, 11.237217426300049, 16.50145149230957, 21.85591959953308, 27.006492137908936, 32.16070055961609, 37.30236840248108, 42.44975161552429, 47.61399483680725, 52.7761869430542, 57.88757658004761, 63.039424657821655, 68.1877601146698, 73.49644351005554, 78.79129147529602, 84.09754538536072, 89.260666847229, 94.41603326797485, 99.58218336105347, 104.72323155403137, 109.8894362449646, 115.04176926612854, 120.21392369270325, 125.37075400352478, 130.56657314300537, 135.74049544334412, 140.92821073532104, 146.1064248085022, 151.22032260894775, 156.34674882888794, 161.5451056957245, 166.72763013839722, 171.90538120269775, 177.13654327392578, 182.33736491203308, 187.54947185516357, 192.73725485801697, 197.9490773677826, 203.09355878829956, 208.28399753570557, 213.5717203617096, 218.8093945980072, 223.99254631996155, 229.20933151245117, 234.47918558120728, 239.6520173549652, 244.8348560333252, 249.9984314441681, 255.1976613998413, 260.37712240219116, 265.57224702835083, 270.85142707824707, 276.01770997047424, 281.18482875823975, 286.34890031814575, 291.5249180793762, 296.7207717895508, 301.8987662792206, 307.0806429386139, 312.2721173763275, 317.42587757110596, 322.6131646633148, 327.8029992580414, 332.9689464569092, 338.1305067539215, 343.27483201026917, 348.4278380870819, 353.6203625202179, 358.82037925720215, 363.9901485443115, 369.1661150455475, 374.33965849876404, 379.464231967926, 384.61280059814453, 389.84004974365234, 395.1992299556732, 400.5368404388428, 405.88154578208923, 411.08725476264954, 416.2190475463867, 421.3424563407898, 426.49652767181396, 431.6532418727875, 436.8298268318176, 442.081960439682, 447.20850443840027, 452.3406445980072, 457.4713304042816, 462.6066789627075, 467.7372176647186, 472.8569633960724, 478.0062277317047, 483.3509292602539, 488.62936449050903, 493.92920899391174, 499.1985864639282, 504.3482086658478, 509.47461199760437, 514.6002404689789, 519.7453014850616, 521.8945426940918]
[31.54, 41.35, 45.825, 49.225, 52.52, 55.465, 57.15, 59.625, 60.0275, 61.4875, 62.3825, 63.225, 63.6775, 64.9125, 65.9025, 66.3375, 68.1075, 68.8275, 68.43, 69.065, 69.435, 69.2925, 70.515, 70.4425, 71.37, 71.245, 71.7125, 71.7125, 71.805, 72.145, 72.2075, 72.0725, 73.03, 73.365, 73.1075, 73.3625, 73.5025, 73.1275, 73.705, 73.4525, 73.93, 74.0025, 74.07, 74.475, 73.93, 74.66, 74.3225, 74.6275, 74.7575, 74.9825, 74.9275, 74.875, 75.2525, 75.66, 75.375, 74.875, 74.435, 74.57, 74.78, 75.22, 75.2525, 75.1575, 75.725, 75.2975, 75.95, 75.745, 75.74, 75.6175, 76.16, 75.935, 75.925, 75.965, 75.53, 76.055, 76.0275, 76.4125, 76.3375, 76.395, 76.8625, 76.6, 76.325, 76.6875, 76.7225, 76.9275, 76.7075, 76.88, 76.6, 76.7625, 76.55, 76.36, 76.5975, 76.53, 76.7025, 76.4125, 76.8325, 76.5575, 76.47, 76.4125, 76.5325, 76.995, 76.855]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  31.2000
Round 1 global test acc  40.3700
Round 2 global test acc  46.4400
Round 3 global test acc  48.7500
Round 4 global test acc  51.7900
Round 5 global test acc  53.3900
Round 6 global test acc  53.3500
Round 7 global test acc  54.5200
Round 8 global test acc  55.5400
Round 9 global test acc  55.7200
Round 10 global test acc  57.5400
Round 11 global test acc  57.4200
Round 12 global test acc  58.2000
Round 13 global test acc  59.4700
Round 14 global test acc  58.2100
Round 15 global test acc  60.1300
Round 16 global test acc  59.5400
Round 17 global test acc  59.5100
Round 18 global test acc  61.5700
Round 19 global test acc  60.9700
Round 20 global test acc  61.6500
Round 21 global test acc  61.6300
Round 22 global test acc  61.5500
Round 23 global test acc  62.0000
Round 24 global test acc  61.8200
Round 25 global test acc  61.5200
Round 26 global test acc  62.0300
Round 27 global test acc  62.8900
Round 28 global test acc  63.3600
Round 29 global test acc  62.9500
Round 30 global test acc  63.2700
Round 31 global test acc  63.7500
Round 32 global test acc  62.5100
Round 33 global test acc  64.2900
Round 34 global test acc  63.1100
Round 35 global test acc  63.6200
Round 36 global test acc  64.1200
Round 37 global test acc  64.5600
Round 38 global test acc  64.5900
Round 39 global test acc  63.9300
Round 40 global test acc  64.0800
Round 41 global test acc  64.5100
Round 42 global test acc  64.5900
Round 43 global test acc  64.9000
Round 44 global test acc  64.0200
Round 45 global test acc  64.9400
Round 46 global test acc  64.5300
Round 47 global test acc  65.8300
Round 48 global test acc  64.9800
Round 49 global test acc  64.9500
Round 50 global test acc  65.7000
Round 51 global test acc  65.0800
Round 52 global test acc  66.0000
Round 53 global test acc  65.9300
Round 54 global test acc  65.9400
Round 55 global test acc  66.0200
Round 56 global test acc  66.1800
Round 57 global test acc  65.7900
Round 58 global test acc  66.0100
Round 59 global test acc  66.4400
Round 60 global test acc  66.6800
Round 61 global test acc  67.0300
Round 62 global test acc  66.9300
Round 63 global test acc  66.0700
Round 64 global test acc  66.6300
Round 65 global test acc  66.8800
Round 66 global test acc  66.8800
Round 67 global test acc  66.1500
Round 68 global test acc  67.0700
Round 69 global test acc  67.0300
Round 70 global test acc  67.6600
Round 71 global test acc  67.0100
Round 72 global test acc  67.1000
Round 73 global test acc  66.7000
Round 74 global test acc  67.6900
Round 75 global test acc  67.1800
Round 76 global test acc  67.2600
Round 77 global test acc  67.4100
Round 78 global test acc  67.6700
Round 79 global test acc  68.0500
Round 80 global test acc  65.5800
Round 81 global test acc  64.1800
Round 82 global test acc  62.8400
Round 83 global test acc  60.9700
Round 84 global test acc  60.5900
Round 85 global test acc  60.3700
Round 86 global test acc  60.2000
Round 87 global test acc  60.1200
Round 88 global test acc  59.9100
Round 89 global test acc  59.5000
Round 90 global test acc  59.2000
Round 91 global test acc  58.9700
Round 92 global test acc  58.6600
Round 93 global test acc  59.7500
Round 94 global test acc  60.0300
Round 95 global test acc  59.7800
Round 96 global test acc  58.6700
Round 97 global test acc  58.5700
Round 98 global test acc  58.1500
Round 99 global test acc  57.9500
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.222, Test loss: 1.948, Test accuracy: 30.70
Round   1, Train loss: 1.872, Test loss: 1.652, Test accuracy: 40.30
Round   2, Train loss: 1.679, Test loss: 1.552, Test accuracy: 45.00
Round   3, Train loss: 1.583, Test loss: 1.478, Test accuracy: 48.12
Round   4, Train loss: 1.501, Test loss: 1.390, Test accuracy: 51.02
Round   5, Train loss: 1.439, Test loss: 1.327, Test accuracy: 53.41
Round   6, Train loss: 1.392, Test loss: 1.266, Test accuracy: 55.81
Round   7, Train loss: 1.331, Test loss: 1.234, Test accuracy: 57.19
Round   8, Train loss: 1.284, Test loss: 1.180, Test accuracy: 59.51
Round   9, Train loss: 1.229, Test loss: 1.159, Test accuracy: 60.87
Round  10, Train loss: 1.195, Test loss: 1.127, Test accuracy: 61.52
Round  11, Train loss: 1.152, Test loss: 1.089, Test accuracy: 62.90
Round  12, Train loss: 1.145, Test loss: 1.050, Test accuracy: 63.62
Round  13, Train loss: 1.095, Test loss: 1.026, Test accuracy: 64.46
Round  14, Train loss: 1.053, Test loss: 1.015, Test accuracy: 65.38
Round  15, Train loss: 1.053, Test loss: 0.993, Test accuracy: 65.90
Round  16, Train loss: 1.022, Test loss: 0.962, Test accuracy: 66.76
Round  17, Train loss: 0.993, Test loss: 0.960, Test accuracy: 66.72
Round  18, Train loss: 0.983, Test loss: 0.958, Test accuracy: 67.28
Round  19, Train loss: 0.951, Test loss: 0.934, Test accuracy: 67.87
Round  20, Train loss: 0.938, Test loss: 0.924, Test accuracy: 68.41
Round  21, Train loss: 0.932, Test loss: 0.907, Test accuracy: 68.73
Round  22, Train loss: 0.909, Test loss: 0.885, Test accuracy: 69.52
Round  23, Train loss: 0.885, Test loss: 0.880, Test accuracy: 69.82
Round  24, Train loss: 0.886, Test loss: 0.856, Test accuracy: 70.78
Round  25, Train loss: 0.838, Test loss: 0.857, Test accuracy: 70.49
Round  26, Train loss: 0.866, Test loss: 0.845, Test accuracy: 70.98
Round  27, Train loss: 0.839, Test loss: 0.832, Test accuracy: 71.68
Round  28, Train loss: 0.830, Test loss: 0.828, Test accuracy: 71.89
Round  29, Train loss: 0.822, Test loss: 0.819, Test accuracy: 71.83
Round  30, Train loss: 0.818, Test loss: 0.814, Test accuracy: 72.23
Round  31, Train loss: 0.783, Test loss: 0.802, Test accuracy: 72.47
Round  32, Train loss: 0.779, Test loss: 0.807, Test accuracy: 72.28
Round  33, Train loss: 0.773, Test loss: 0.801, Test accuracy: 72.49
Round  34, Train loss: 0.757, Test loss: 0.788, Test accuracy: 73.24
Round  35, Train loss: 0.761, Test loss: 0.777, Test accuracy: 73.50
Round  36, Train loss: 0.745, Test loss: 0.787, Test accuracy: 73.29
Round  37, Train loss: 0.729, Test loss: 0.779, Test accuracy: 73.57
Round  38, Train loss: 0.704, Test loss: 0.777, Test accuracy: 73.90
Round  39, Train loss: 0.711, Test loss: 0.773, Test accuracy: 73.66
Round  40, Train loss: 0.725, Test loss: 0.770, Test accuracy: 73.95
Round  41, Train loss: 0.704, Test loss: 0.768, Test accuracy: 73.96
Round  42, Train loss: 0.710, Test loss: 0.751, Test accuracy: 74.59
Round  43, Train loss: 0.681, Test loss: 0.755, Test accuracy: 74.18
Round  44, Train loss: 0.683, Test loss: 0.747, Test accuracy: 74.38
Round  45, Train loss: 0.659, Test loss: 0.759, Test accuracy: 74.14
Round  46, Train loss: 0.679, Test loss: 0.752, Test accuracy: 74.48
Round  47, Train loss: 0.656, Test loss: 0.752, Test accuracy: 74.72
Round  48, Train loss: 0.680, Test loss: 0.754, Test accuracy: 74.60
Round  49, Train loss: 0.658, Test loss: 0.749, Test accuracy: 74.45
Round  50, Train loss: 0.631, Test loss: 0.744, Test accuracy: 74.78
Round  51, Train loss: 0.639, Test loss: 0.736, Test accuracy: 75.33
Round  52, Train loss: 0.620, Test loss: 0.743, Test accuracy: 75.05
Round  53, Train loss: 0.607, Test loss: 0.745, Test accuracy: 74.98
Round  54, Train loss: 0.616, Test loss: 0.752, Test accuracy: 74.54
Round  55, Train loss: 0.630, Test loss: 0.746, Test accuracy: 74.92
Round  56, Train loss: 0.615, Test loss: 0.739, Test accuracy: 75.04
Round  57, Train loss: 0.632, Test loss: 0.732, Test accuracy: 75.11
Round  58, Train loss: 0.603, Test loss: 0.728, Test accuracy: 75.33
Round  59, Train loss: 0.615, Test loss: 0.732, Test accuracy: 75.25
Round  60, Train loss: 0.577, Test loss: 0.742, Test accuracy: 74.78
Round  61, Train loss: 0.623, Test loss: 0.732, Test accuracy: 75.21
Round  62, Train loss: 0.635, Test loss: 0.724, Test accuracy: 75.39
Round  63, Train loss: 0.587, Test loss: 0.730, Test accuracy: 75.64
Round  64, Train loss: 0.581, Test loss: 0.724, Test accuracy: 75.61
Round  65, Train loss: 0.582, Test loss: 0.732, Test accuracy: 75.47
Round  66, Train loss: 0.577, Test loss: 0.730, Test accuracy: 75.44
Round  67, Train loss: 0.566, Test loss: 0.735, Test accuracy: 75.35
Round  68, Train loss: 0.563, Test loss: 0.744, Test accuracy: 75.30
Round  69, Train loss: 0.530, Test loss: 0.741, Test accuracy: 75.18
Round  70, Train loss: 0.555, Test loss: 0.729, Test accuracy: 75.41
Round  71, Train loss: 0.579, Test loss: 0.724, Test accuracy: 75.72
Round  72, Train loss: 0.555, Test loss: 0.740, Test accuracy: 75.58
Round  73, Train loss: 0.577, Test loss: 0.731, Test accuracy: 75.57
Round  74, Train loss: 0.540, Test loss: 0.740, Test accuracy: 75.55
Round  75, Train loss: 0.529, Test loss: 0.734, Test accuracy: 75.52
Round  76, Train loss: 0.532, Test loss: 0.741, Test accuracy: 75.42
Round  77, Train loss: 0.558, Test loss: 0.733, Test accuracy: 75.57
Round  78, Train loss: 0.511, Test loss: 0.734, Test accuracy: 75.98
Round  79, Train loss: 0.575, Test loss: 0.719, Test accuracy: 76.11
Round  80, Train loss: 0.517, Test loss: 0.726, Test accuracy: 76.01
Round  81, Train loss: 0.539, Test loss: 0.715, Test accuracy: 76.46
Round  82, Train loss: 0.517, Test loss: 0.726, Test accuracy: 76.24
Round  83, Train loss: 0.529, Test loss: 0.737, Test accuracy: 76.11
Round  84, Train loss: 0.545, Test loss: 0.731, Test accuracy: 75.71
Round  85, Train loss: 0.526, Test loss: 0.720, Test accuracy: 76.09
Round  86, Train loss: 0.521, Test loss: 0.715, Test accuracy: 76.36
Round  87, Train loss: 0.512, Test loss: 0.724, Test accuracy: 76.19
Round  88, Train loss: 0.499, Test loss: 0.728, Test accuracy: 76.20
Round  89, Train loss: 0.515, Test loss: 0.720, Test accuracy: 76.39
Round  90, Train loss: 0.483, Test loss: 0.727, Test accuracy: 76.08
Round  91, Train loss: 0.523, Test loss: 0.725, Test accuracy: 76.30
Round  92, Train loss: 0.534, Test loss: 0.720, Test accuracy: 76.40
Round  93, Train loss: 0.516, Test loss: 0.730, Test accuracy: 76.13
Round  94, Train loss: 0.506, Test loss: 0.727, Test accuracy: 76.24
Round  95, Train loss: 0.479, Test loss: 0.730, Test accuracy: 76.05
Round  96, Train loss: 0.507, Test loss: 0.713, Test accuracy: 76.72
Round  97, Train loss: 0.525, Test loss: 0.718, Test accuracy: 76.41
Round  98, Train loss: 0.493, Test loss: 0.720, Test accuracy: 76.34
Round  99, Train loss: 0.466, Test loss: 0.736, Test accuracy: 75.84
Final Round, Train loss: 0.407, Test loss: 0.729, Test accuracy: 76.18
Average accuracy final 10 rounds: 76.25050000000002
4113.769355773926
[5.454874753952026, 10.555379390716553, 15.651283025741577, 20.757880210876465, 25.85800290107727, 30.981015920639038, 36.07926368713379, 41.18157434463501, 46.28122925758362, 51.38980674743652, 56.491928577423096, 61.603963136672974, 66.7174346446991, 71.8403787612915, 76.94119191169739, 82.04925155639648, 87.29097986221313, 92.42995548248291, 98.15787267684937, 103.80151033401489, 108.95501804351807, 114.04534554481506, 119.12985014915466, 124.231938123703, 129.3165054321289, 134.4335913658142, 139.51328539848328, 144.57951760292053, 149.6563422679901, 154.746000289917, 159.8101978302002, 164.89379262924194, 170.02098059654236, 175.16208624839783, 180.2496497631073, 185.36102104187012, 190.47460627555847, 195.60441088676453, 200.8443443775177, 206.0030369758606, 211.09270215034485, 216.177090883255, 221.62486815452576, 226.99225449562073, 232.44640851020813, 237.9105682373047, 243.06315565109253, 248.16372203826904, 253.2374587059021, 258.3608078956604, 263.4471912384033, 268.7090446949005, 273.7862730026245, 279.0606529712677, 284.3700120449066, 289.57728838920593, 294.8592150211334, 300.2668011188507, 305.35895109176636, 310.5939679145813, 315.6852343082428, 320.9531853199005, 326.2526361942291, 331.6711084842682, 337.074524641037, 342.5017545223236, 347.7134675979614, 352.8123369216919, 357.9224388599396, 363.07744884490967, 368.18286085128784, 373.2932999134064, 378.4062833786011, 383.5220069885254, 388.61911606788635, 393.7025623321533, 398.7963898181915, 403.8996329307556, 408.9674491882324, 414.03755140304565, 419.10260915756226, 424.1755166053772, 429.2628700733185, 434.3692181110382, 439.44207406044006, 444.50069284439087, 449.5448634624481, 454.5814485549927, 459.6474657058716, 464.6891963481903, 469.715980052948, 474.764301776886, 479.8097803592682, 484.8563497066498, 489.9349436759949, 494.9728670120239, 500.0405766963959, 505.09076023101807, 510.31009316444397, 515.5977907180786, 517.6603763103485]
[30.6975, 40.305, 45.0, 48.12, 51.0175, 53.415, 55.81, 57.19, 59.5125, 60.8675, 61.5225, 62.8975, 63.615, 64.46, 65.3775, 65.9025, 66.7625, 66.725, 67.2775, 67.8725, 68.41, 68.735, 69.5175, 69.82, 70.785, 70.4875, 70.9775, 71.6775, 71.8925, 71.83, 72.2325, 72.4725, 72.2775, 72.4925, 73.2375, 73.5025, 73.29, 73.5675, 73.8975, 73.6625, 73.9475, 73.9625, 74.5925, 74.1775, 74.375, 74.135, 74.485, 74.72, 74.6, 74.4525, 74.7775, 75.3275, 75.0525, 74.9825, 74.54, 74.925, 75.0425, 75.1075, 75.335, 75.245, 74.7825, 75.21, 75.3925, 75.6425, 75.61, 75.475, 75.445, 75.3525, 75.2975, 75.1775, 75.4125, 75.7175, 75.58, 75.57, 75.5525, 75.515, 75.4175, 75.57, 75.9825, 76.1125, 76.0075, 76.46, 76.2375, 76.115, 75.7125, 76.0925, 76.365, 76.1875, 76.1975, 76.3875, 76.085, 76.2975, 76.3975, 76.13, 76.24, 76.045, 76.725, 76.405, 76.34, 75.84, 76.1775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.243, Test loss: 1.999, Test accuracy: 30.82
Round   1, Train loss: 1.898, Test loss: 1.669, Test accuracy: 40.62
Round   2, Train loss: 1.689, Test loss: 1.576, Test accuracy: 44.35
Round   3, Train loss: 1.587, Test loss: 1.502, Test accuracy: 48.06
Round   4, Train loss: 1.514, Test loss: 1.439, Test accuracy: 50.51
Round   5, Train loss: 1.458, Test loss: 1.340, Test accuracy: 53.78
Round   6, Train loss: 1.377, Test loss: 1.264, Test accuracy: 56.38
Round   7, Train loss: 1.320, Test loss: 1.233, Test accuracy: 58.62
Round   8, Train loss: 1.270, Test loss: 1.188, Test accuracy: 60.44
Round   9, Train loss: 1.236, Test loss: 1.127, Test accuracy: 61.60
Round  10, Train loss: 1.181, Test loss: 1.124, Test accuracy: 61.68
Round  11, Train loss: 1.163, Test loss: 1.054, Test accuracy: 63.81
Round  12, Train loss: 1.114, Test loss: 1.010, Test accuracy: 65.24
Round  13, Train loss: 1.073, Test loss: 1.000, Test accuracy: 65.50
Round  14, Train loss: 1.049, Test loss: 0.969, Test accuracy: 66.58
Round  15, Train loss: 1.012, Test loss: 0.968, Test accuracy: 67.16
Round  16, Train loss: 0.993, Test loss: 0.944, Test accuracy: 68.14
Round  17, Train loss: 0.962, Test loss: 0.929, Test accuracy: 68.55
Round  18, Train loss: 0.960, Test loss: 0.911, Test accuracy: 69.07
Round  19, Train loss: 0.923, Test loss: 0.894, Test accuracy: 69.81
Round  20, Train loss: 0.900, Test loss: 0.888, Test accuracy: 70.17
Round  21, Train loss: 0.915, Test loss: 0.873, Test accuracy: 70.50
Round  22, Train loss: 0.890, Test loss: 0.837, Test accuracy: 71.52
Round  23, Train loss: 0.861, Test loss: 0.842, Test accuracy: 71.46
Round  24, Train loss: 0.832, Test loss: 0.845, Test accuracy: 71.31
Round  25, Train loss: 0.841, Test loss: 0.826, Test accuracy: 71.98
Round  26, Train loss: 0.809, Test loss: 0.819, Test accuracy: 72.23
Round  27, Train loss: 0.820, Test loss: 0.808, Test accuracy: 72.64
Round  28, Train loss: 0.796, Test loss: 0.805, Test accuracy: 73.03
Round  29, Train loss: 0.782, Test loss: 0.797, Test accuracy: 72.63
Round  30, Train loss: 0.776, Test loss: 0.795, Test accuracy: 73.14
Round  31, Train loss: 0.786, Test loss: 0.774, Test accuracy: 73.67
Round  32, Train loss: 0.756, Test loss: 0.779, Test accuracy: 73.49
Round  33, Train loss: 0.759, Test loss: 0.767, Test accuracy: 74.09
Round  34, Train loss: 0.744, Test loss: 0.761, Test accuracy: 74.01
Round  35, Train loss: 0.740, Test loss: 0.766, Test accuracy: 73.80
Round  36, Train loss: 0.711, Test loss: 0.759, Test accuracy: 74.12
Round  37, Train loss: 0.708, Test loss: 0.767, Test accuracy: 74.18
Round  38, Train loss: 0.719, Test loss: 0.760, Test accuracy: 74.10
Round  39, Train loss: 0.700, Test loss: 0.759, Test accuracy: 73.90
Round  40, Train loss: 0.690, Test loss: 0.754, Test accuracy: 74.32
Round  41, Train loss: 0.712, Test loss: 0.758, Test accuracy: 74.37
Round  42, Train loss: 0.692, Test loss: 0.746, Test accuracy: 74.56
Round  43, Train loss: 0.676, Test loss: 0.751, Test accuracy: 74.50
Round  44, Train loss: 0.673, Test loss: 0.744, Test accuracy: 74.86
Round  45, Train loss: 0.645, Test loss: 0.737, Test accuracy: 75.23
Round  46, Train loss: 0.675, Test loss: 0.740, Test accuracy: 74.82
Round  47, Train loss: 0.659, Test loss: 0.740, Test accuracy: 74.70
Round  48, Train loss: 0.673, Test loss: 0.731, Test accuracy: 75.45
Round  49, Train loss: 0.653, Test loss: 0.730, Test accuracy: 75.07
Round  50, Train loss: 0.636, Test loss: 0.740, Test accuracy: 74.84
Round  51, Train loss: 0.634, Test loss: 0.727, Test accuracy: 75.47
Round  52, Train loss: 0.638, Test loss: 0.722, Test accuracy: 75.62
Round  53, Train loss: 0.616, Test loss: 0.739, Test accuracy: 75.13
Round  54, Train loss: 0.626, Test loss: 0.729, Test accuracy: 75.47
Round  55, Train loss: 0.623, Test loss: 0.723, Test accuracy: 75.77
Round  56, Train loss: 0.614, Test loss: 0.725, Test accuracy: 75.47
Round  57, Train loss: 0.619, Test loss: 0.708, Test accuracy: 75.94
Round  58, Train loss: 0.587, Test loss: 0.711, Test accuracy: 75.82
Round  59, Train loss: 0.588, Test loss: 0.713, Test accuracy: 75.95
Round  60, Train loss: 0.622, Test loss: 0.708, Test accuracy: 75.97
Round  61, Train loss: 0.580, Test loss: 0.707, Test accuracy: 76.42
Round  62, Train loss: 0.566, Test loss: 0.720, Test accuracy: 75.68
Round  63, Train loss: 0.557, Test loss: 0.718, Test accuracy: 75.94
Round  64, Train loss: 0.596, Test loss: 0.718, Test accuracy: 75.92
Round  65, Train loss: 0.581, Test loss: 0.719, Test accuracy: 75.87
Round  66, Train loss: 0.571, Test loss: 0.718, Test accuracy: 75.78
Round  67, Train loss: 0.556, Test loss: 0.712, Test accuracy: 75.88
Round  68, Train loss: 0.542, Test loss: 0.716, Test accuracy: 75.97
Round  69, Train loss: 0.565, Test loss: 0.705, Test accuracy: 76.21
Round  70, Train loss: 0.582, Test loss: 0.717, Test accuracy: 75.87
Round  71, Train loss: 0.566, Test loss: 0.718, Test accuracy: 75.64
Round  72, Train loss: 0.559, Test loss: 0.710, Test accuracy: 76.03
Round  73, Train loss: 0.546, Test loss: 0.719, Test accuracy: 75.87
Round  74, Train loss: 0.522, Test loss: 0.709, Test accuracy: 76.17
Round  75, Train loss: 0.544, Test loss: 0.720, Test accuracy: 75.89
Round  76, Train loss: 0.530, Test loss: 0.717, Test accuracy: 76.06
Round  77, Train loss: 0.583, Test loss: 0.704, Test accuracy: 76.58
Round  78, Train loss: 0.550, Test loss: 0.704, Test accuracy: 76.61
Round  79, Train loss: 0.527, Test loss: 0.716, Test accuracy: 76.39
Round  80, Train loss: 0.512, Test loss: 0.710, Test accuracy: 75.95
Round  81, Train loss: 0.528, Test loss: 0.719, Test accuracy: 76.09
Round  82, Train loss: 0.508, Test loss: 0.720, Test accuracy: 76.52
Round  83, Train loss: 0.515, Test loss: 0.725, Test accuracy: 76.27
Round  84, Train loss: 0.511, Test loss: 0.720, Test accuracy: 76.47
Round  85, Train loss: 0.528, Test loss: 0.709, Test accuracy: 76.16
Round  86, Train loss: 0.511, Test loss: 0.704, Test accuracy: 76.67
Round  87, Train loss: 0.509, Test loss: 0.705, Test accuracy: 76.70
Round  88, Train loss: 0.486, Test loss: 0.712, Test accuracy: 76.53
Round  89, Train loss: 0.514, Test loss: 0.708, Test accuracy: 76.55
Round  90, Train loss: 0.518, Test loss: 0.710, Test accuracy: 76.36
Round  91, Train loss: 0.517, Test loss: 0.707, Test accuracy: 76.68
Round  92, Train loss: 0.504, Test loss: 0.709, Test accuracy: 76.78
Round  93, Train loss: 0.498, Test loss: 0.713, Test accuracy: 76.81
Round  94, Train loss: 0.477, Test loss: 0.710, Test accuracy: 76.68
Round  95, Train loss: 0.462, Test loss: 0.715, Test accuracy: 76.33
Round  96, Train loss: 0.478, Test loss: 0.709, Test accuracy: 76.57
Round  97, Train loss: 0.469, Test loss: 0.706, Test accuracy: 76.69
Round  98, Train loss: 0.477, Test loss: 0.704, Test accuracy: 76.93
Round  99, Train loss: 0.505, Test loss: 0.710, Test accuracy: 76.59
Final Round, Train loss: 0.408, Test loss: 0.702, Test accuracy: 76.81
Average accuracy final 10 rounds: 76.64275
6261.9352860450745
[5.407973527908325, 10.966896772384644, 16.36032724380493, 22.05706763267517, 27.732532739639282, 33.44141626358032, 38.95633816719055, 44.29994988441467, 49.43379473686218, 54.56805896759033, 59.76322293281555, 64.94698691368103, 70.09373164176941, 75.26692342758179, 80.51024985313416, 85.66723847389221, 91.04189896583557, 96.21606302261353, 101.52059769630432, 106.75696730613708, 112.1546790599823, 121.84388542175293, 131.643230676651, 141.2466778755188, 150.9919788837433, 160.67174887657166, 170.37045121192932, 180.1746163368225, 189.79578495025635, 199.36019444465637, 208.91673636436462, 218.53154587745667, 228.15178442001343, 237.74982404708862, 247.45203948020935, 257.67177510261536, 268.3321006298065, 278.1382622718811, 287.8153347969055, 297.4157645702362, 307.071387052536, 316.72843408584595, 326.32727003097534, 336.00939202308655, 345.75996923446655, 356.17070150375366, 367.091894865036, 376.8246865272522, 386.5599687099457, 396.18921637535095, 405.85973834991455, 415.57382106781006, 425.27662467956543, 434.90616750717163, 444.6144254207611, 454.28471088409424, 463.89465141296387, 473.5218164920807, 483.1480624675751, 493.09908509254456, 502.93784070014954, 512.6037311553955, 522.2720220088959, 531.9313616752625, 541.5733375549316, 551.285257101059, 561.0058302879333, 570.6637451648712, 580.3311154842377, 589.982786655426, 599.7446863651276, 609.4639329910278, 619.1101748943329, 628.8547074794769, 638.4921445846558, 648.1193161010742, 657.7630965709686, 667.4103426933289, 677.1330244541168, 686.8371860980988, 696.6150758266449, 706.3128836154938, 716.1927988529205, 726.0412924289703, 735.7918589115143, 745.4596061706543, 755.304573059082, 765.0871422290802, 774.7440588474274, 784.3894891738892, 794.1600527763367, 803.8755781650543, 814.1441295146942, 823.992794752121, 833.8324112892151, 843.5340383052826, 853.6434001922607, 863.8511831760406, 874.0969936847687, 884.1900057792664, 886.4627454280853]
[30.825, 40.6175, 44.355, 48.0575, 50.5075, 53.7825, 56.3775, 58.625, 60.435, 61.6025, 61.6775, 63.815, 65.2375, 65.495, 66.585, 67.1625, 68.145, 68.55, 69.0675, 69.805, 70.1725, 70.5, 71.52, 71.4625, 71.31, 71.98, 72.23, 72.6375, 73.0325, 72.6325, 73.14, 73.67, 73.4875, 74.0875, 74.01, 73.7975, 74.125, 74.1825, 74.1025, 73.9025, 74.3225, 74.3675, 74.5575, 74.4975, 74.8625, 75.23, 74.8225, 74.7, 75.45, 75.0675, 74.84, 75.465, 75.6225, 75.13, 75.4675, 75.765, 75.465, 75.935, 75.82, 75.95, 75.975, 76.4175, 75.6775, 75.935, 75.9225, 75.8725, 75.775, 75.88, 75.97, 76.21, 75.8675, 75.6375, 76.025, 75.8675, 76.1675, 75.89, 76.055, 76.585, 76.6075, 76.39, 75.9525, 76.095, 76.515, 76.265, 76.47, 76.16, 76.675, 76.7, 76.53, 76.55, 76.3625, 76.6825, 76.775, 76.81, 76.6775, 76.335, 76.5725, 76.6925, 76.93, 76.59, 76.8075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 201, in get_data_from_file
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_co_teaching2.py", line 92, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "RFL.py", line 62, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 93, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 95, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 12, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.158, Test loss: 2.003, Test accuracy: 32.46
Round   0, Global train loss: 2.158, Global test loss: 2.012, Global test accuracy: 33.82
Round   1, Train loss: 2.046, Test loss: 1.839, Test accuracy: 36.82
Round   1, Global train loss: 2.046, Global test loss: 1.782, Global test accuracy: 41.29
Round   2, Train loss: 1.875, Test loss: 1.764, Test accuracy: 38.92
Round   2, Global train loss: 1.875, Global test loss: 1.610, Global test accuracy: 46.95
Round   3, Train loss: 1.939, Test loss: 1.771, Test accuracy: 39.80
Round   3, Global train loss: 1.939, Global test loss: 1.795, Global test accuracy: 45.24
Round   4, Train loss: 1.736, Test loss: 1.711, Test accuracy: 40.76
Round   4, Global train loss: 1.736, Global test loss: 1.595, Global test accuracy: 48.23
Round   5, Train loss: 1.732, Test loss: 1.676, Test accuracy: 42.12
Round   5, Global train loss: 1.732, Global test loss: 1.517, Global test accuracy: 49.56
Round   6, Train loss: 1.780, Test loss: 1.683, Test accuracy: 41.85
Round   6, Global train loss: 1.780, Global test loss: 1.511, Global test accuracy: 51.39
Round   7, Train loss: 1.787, Test loss: 1.682, Test accuracy: 42.73
Round   7, Global train loss: 1.787, Global test loss: 1.608, Global test accuracy: 51.99
Round   8, Train loss: 1.719, Test loss: 1.678, Test accuracy: 42.64
Round   8, Global train loss: 1.719, Global test loss: 1.572, Global test accuracy: 52.67
Round   9, Train loss: 1.805, Test loss: 1.670, Test accuracy: 43.02
Round   9, Global train loss: 1.805, Global test loss: 1.567, Global test accuracy: 52.65
Round  10, Train loss: 1.659, Test loss: 1.668, Test accuracy: 43.69
Round  10, Global train loss: 1.659, Global test loss: 1.503, Global test accuracy: 54.97
Round  11, Train loss: 1.846, Test loss: 1.657, Test accuracy: 44.23
Round  11, Global train loss: 1.846, Global test loss: 1.694, Global test accuracy: 50.11
Round  12, Train loss: 1.446, Test loss: 1.648, Test accuracy: 44.64
Round  12, Global train loss: 1.446, Global test loss: 1.405, Global test accuracy: 54.85
Round  13, Train loss: 1.572, Test loss: 1.653, Test accuracy: 44.53
Round  13, Global train loss: 1.572, Global test loss: 1.494, Global test accuracy: 53.40
Round  14, Train loss: 1.728, Test loss: 1.659, Test accuracy: 44.48
Round  14, Global train loss: 1.728, Global test loss: 1.658, Global test accuracy: 50.17
Round  15, Train loss: 1.514, Test loss: 1.659, Test accuracy: 44.66
Round  15, Global train loss: 1.514, Global test loss: 1.486, Global test accuracy: 53.45
Round  16, Train loss: 1.457, Test loss: 1.652, Test accuracy: 45.28
Round  16, Global train loss: 1.457, Global test loss: 1.489, Global test accuracy: 53.69
Round  17, Train loss: 1.242, Test loss: 1.649, Test accuracy: 45.48
Round  17, Global train loss: 1.242, Global test loss: 1.345, Global test accuracy: 56.49
Round  18, Train loss: 1.361, Test loss: 1.649, Test accuracy: 45.53
Round  18, Global train loss: 1.361, Global test loss: 1.407, Global test accuracy: 55.26
Round  19, Train loss: 1.625, Test loss: 1.666, Test accuracy: 45.01
Round  19, Global train loss: 1.625, Global test loss: 1.633, Global test accuracy: 51.80
Round  20, Train loss: 1.361, Test loss: 1.689, Test accuracy: 45.05
Round  20, Global train loss: 1.361, Global test loss: 1.471, Global test accuracy: 53.29
Round  21, Train loss: 1.615, Test loss: 1.700, Test accuracy: 45.22
Round  21, Global train loss: 1.615, Global test loss: 1.671, Global test accuracy: 52.00
Round  22, Train loss: 1.310, Test loss: 1.708, Test accuracy: 45.11
Round  22, Global train loss: 1.310, Global test loss: 1.418, Global test accuracy: 54.99
Round  23, Train loss: 1.464, Test loss: 1.718, Test accuracy: 44.86
Round  23, Global train loss: 1.464, Global test loss: 1.589, Global test accuracy: 52.12
Round  24, Train loss: 1.515, Test loss: 1.736, Test accuracy: 44.66
Round  24, Global train loss: 1.515, Global test loss: 1.594, Global test accuracy: 50.91
Round  25, Train loss: 1.339, Test loss: 1.743, Test accuracy: 44.18
Round  25, Global train loss: 1.339, Global test loss: 1.528, Global test accuracy: 53.42
Round  26, Train loss: 1.280, Test loss: 1.756, Test accuracy: 44.60
Round  26, Global train loss: 1.280, Global test loss: 1.490, Global test accuracy: 53.88
Round  27, Train loss: 1.281, Test loss: 1.788, Test accuracy: 44.20
Round  27, Global train loss: 1.281, Global test loss: 1.497, Global test accuracy: 53.12
Round  28, Train loss: 1.432, Test loss: 1.797, Test accuracy: 44.33
Round  28, Global train loss: 1.432, Global test loss: 1.583, Global test accuracy: 51.92
Round  29, Train loss: 1.339, Test loss: 1.827, Test accuracy: 43.87
Round  29, Global train loss: 1.339, Global test loss: 1.682, Global test accuracy: 46.34
Round  30, Train loss: 0.812, Test loss: 1.819, Test accuracy: 44.07
Round  30, Global train loss: 0.812, Global test loss: 1.279, Global test accuracy: 56.51
Round  31, Train loss: 1.220, Test loss: 1.849, Test accuracy: 43.68
Round  31, Global train loss: 1.220, Global test loss: 1.491, Global test accuracy: 51.75
Round  32, Train loss: 0.816, Test loss: 1.870, Test accuracy: 43.63
Round  32, Global train loss: 0.816, Global test loss: 1.286, Global test accuracy: 56.24
Round  33, Train loss: 1.159, Test loss: 1.886, Test accuracy: 43.29
Round  33, Global train loss: 1.159, Global test loss: 1.451, Global test accuracy: 54.52
Round  34, Train loss: 0.958, Test loss: 1.904, Test accuracy: 43.35
Round  34, Global train loss: 0.958, Global test loss: 1.371, Global test accuracy: 55.18
Round  35, Train loss: 0.952, Test loss: 1.925, Test accuracy: 43.41
Round  35, Global train loss: 0.952, Global test loss: 1.310, Global test accuracy: 56.00
Round  36, Train loss: 1.096, Test loss: 1.941, Test accuracy: 43.19
Round  36, Global train loss: 1.096, Global test loss: 1.642, Global test accuracy: 45.46
Round  37, Train loss: 0.915, Test loss: 1.983, Test accuracy: 43.09
Round  37, Global train loss: 0.915, Global test loss: 1.478, Global test accuracy: 52.24
Round  38, Train loss: 1.189, Test loss: 2.000, Test accuracy: 43.16
Round  38, Global train loss: 1.189, Global test loss: 1.566, Global test accuracy: 48.65
Round  39, Train loss: 0.889, Test loss: 2.031, Test accuracy: 43.12
Round  39, Global train loss: 0.889, Global test loss: 1.386, Global test accuracy: 54.97
Round  40, Train loss: 0.978, Test loss: 2.065, Test accuracy: 43.27
Round  40, Global train loss: 0.978, Global test loss: 1.546, Global test accuracy: 48.65
Round  41, Train loss: 1.091, Test loss: 2.109, Test accuracy: 43.01
Round  41, Global train loss: 1.091, Global test loss: 1.662, Global test accuracy: 47.05
Round  42, Train loss: 0.988, Test loss: 2.170, Test accuracy: 42.89
Round  42, Global train loss: 0.988, Global test loss: 1.549, Global test accuracy: 47.80
Round  43, Train loss: 0.974, Test loss: 2.195, Test accuracy: 42.94
Round  43, Global train loss: 0.974, Global test loss: 1.451, Global test accuracy: 52.13
Round  44, Train loss: 1.089, Test loss: 2.215, Test accuracy: 42.91
Round  44, Global train loss: 1.089, Global test loss: 1.569, Global test accuracy: 49.05
Round  45, Train loss: 0.890, Test loss: 2.239, Test accuracy: 42.92
Round  45, Global train loss: 0.890, Global test loss: 1.461, Global test accuracy: 50.80
Round  46, Train loss: 1.038, Test loss: 2.227, Test accuracy: 43.11
Round  46, Global train loss: 1.038, Global test loss: 1.763, Global test accuracy: 40.59
Round  47, Train loss: 0.660, Test loss: 2.261, Test accuracy: 42.96
Round  47, Global train loss: 0.660, Global test loss: 1.426, Global test accuracy: 51.48
Round  48, Train loss: 0.681, Test loss: 2.277, Test accuracy: 43.04
Round  48, Global train loss: 0.681, Global test loss: 1.329, Global test accuracy: 54.99
Round  49, Train loss: 1.020, Test loss: 2.296, Test accuracy: 42.71
Round  49, Global train loss: 1.020, Global test loss: 1.826, Global test accuracy: 36.81
Round  50, Train loss: 1.016, Test loss: 2.304, Test accuracy: 42.62
Round  50, Global train loss: 1.016, Global test loss: 1.592, Global test accuracy: 45.90
Round  51, Train loss: 0.755, Test loss: 2.321, Test accuracy: 42.48
Round  51, Global train loss: 0.755, Global test loss: 1.542, Global test accuracy: 48.12
Round  52, Train loss: 0.859, Test loss: 2.348, Test accuracy: 42.42
Round  52, Global train loss: 0.859, Global test loss: 1.643, Global test accuracy: 44.22
Round  53, Train loss: 0.711, Test loss: 2.396, Test accuracy: 42.34
Round  53, Global train loss: 0.711, Global test loss: 1.462, Global test accuracy: 50.83
Round  54, Train loss: 0.694, Test loss: 2.433, Test accuracy: 42.11
Round  54, Global train loss: 0.694, Global test loss: 1.417, Global test accuracy: 52.77
Round  55, Train loss: 1.054, Test loss: 2.435, Test accuracy: 41.88
Round  55, Global train loss: 1.054, Global test loss: 1.795, Global test accuracy: 36.97
Round  56, Train loss: 0.725, Test loss: 2.468, Test accuracy: 41.69
Round  56, Global train loss: 0.725, Global test loss: 1.496, Global test accuracy: 49.79
Round  57, Train loss: 0.948, Test loss: 2.497, Test accuracy: 41.76
Round  57, Global train loss: 0.948, Global test loss: 1.623, Global test accuracy: 44.88
Round  58, Train loss: 0.981, Test loss: 2.535, Test accuracy: 41.73
Round  58, Global train loss: 0.981, Global test loss: 1.674, Global test accuracy: 42.77
Round  59, Train loss: 0.863, Test loss: 2.576, Test accuracy: 41.50
Round  59, Global train loss: 0.863, Global test loss: 1.688, Global test accuracy: 42.60
Round  60, Train loss: 0.833, Test loss: 2.590, Test accuracy: 41.47
Round  60, Global train loss: 0.833, Global test loss: 1.642, Global test accuracy: 45.98
Round  61, Train loss: 0.773, Test loss: 2.610, Test accuracy: 41.22
Round  61, Global train loss: 0.773, Global test loss: 1.640, Global test accuracy: 46.41
Round  62, Train loss: 0.698, Test loss: 2.660, Test accuracy: 40.98
Round  62, Global train loss: 0.698, Global test loss: 1.620, Global test accuracy: 44.27
Round  63, Train loss: 0.624, Test loss: 2.707, Test accuracy: 41.20
Round  63, Global train loss: 0.624, Global test loss: 1.616, Global test accuracy: 46.42
Round  64, Train loss: 0.731, Test loss: 2.741, Test accuracy: 41.35
Round  64, Global train loss: 0.731, Global test loss: 1.568, Global test accuracy: 48.06
Round  65, Train loss: 0.859, Test loss: 2.744, Test accuracy: 41.58
Round  65, Global train loss: 0.859, Global test loss: 1.647, Global test accuracy: 46.14
Round  66, Train loss: 0.782, Test loss: 2.745, Test accuracy: 41.73
Round  66, Global train loss: 0.782, Global test loss: 1.646, Global test accuracy: 44.10
Round  67, Train loss: 0.841, Test loss: 2.782, Test accuracy: 41.44
Round  67, Global train loss: 0.841, Global test loss: 1.605, Global test accuracy: 46.73
Round  68, Train loss: 0.590, Test loss: 2.782, Test accuracy: 41.47
Round  68, Global train loss: 0.590, Global test loss: 1.497, Global test accuracy: 49.70
Round  69, Train loss: 0.724, Test loss: 2.799, Test accuracy: 41.01
Round  69, Global train loss: 0.724, Global test loss: 1.709, Global test accuracy: 41.24
Round  70, Train loss: 0.632, Test loss: 2.814, Test accuracy: 40.84
Round  70, Global train loss: 0.632, Global test loss: 1.591, Global test accuracy: 46.48
Round  71, Train loss: 0.719, Test loss: 2.837, Test accuracy: 41.04
Round  71, Global train loss: 0.719, Global test loss: 1.669, Global test accuracy: 43.25
Round  72, Train loss: 0.680, Test loss: 2.858, Test accuracy: 40.98
Round  72, Global train loss: 0.680, Global test loss: 1.455, Global test accuracy: 51.19
Round  73, Train loss: 0.563, Test loss: 2.937, Test accuracy: 40.34
Round  73, Global train loss: 0.563, Global test loss: 1.495, Global test accuracy: 50.50
Round  74, Train loss: 0.584, Test loss: 2.990, Test accuracy: 40.19
Round  74, Global train loss: 0.584, Global test loss: 1.507, Global test accuracy: 48.74
Round  75, Train loss: 0.593, Test loss: 2.994, Test accuracy: 40.34
Round  75, Global train loss: 0.593, Global test loss: 1.581, Global test accuracy: 46.86
Round  76, Train loss: 0.516, Test loss: 3.060, Test accuracy: 40.45
Round  76, Global train loss: 0.516, Global test loss: 1.551, Global test accuracy: 46.48
Round  77, Train loss: 0.559, Test loss: 3.079, Test accuracy: 40.07
Round  77, Global train loss: 0.559, Global test loss: 1.551, Global test accuracy: 47.16
Round  78, Train loss: 0.509, Test loss: 3.080, Test accuracy: 40.33
Round  78, Global train loss: 0.509, Global test loss: 1.500, Global test accuracy: 49.55
Round  79, Train loss: 0.536, Test loss: 3.112, Test accuracy: 40.38
Round  79, Global train loss: 0.536, Global test loss: 1.545, Global test accuracy: 46.62
Round  80, Train loss: 0.434, Test loss: 3.116, Test accuracy: 40.12
Round  80, Global train loss: 0.434, Global test loss: 1.477, Global test accuracy: 48.83
Round  81, Train loss: 0.635, Test loss: 3.093, Test accuracy: 40.25
Round  81, Global train loss: 0.635, Global test loss: 1.599, Global test accuracy: 46.48
Round  82, Train loss: 0.775, Test loss: 3.117, Test accuracy: 40.44
Round  82, Global train loss: 0.775, Global test loss: 1.853, Global test accuracy: 36.86
Round  83, Train loss: 0.532, Test loss: 3.112, Test accuracy: 40.42
Round  83, Global train loss: 0.532, Global test loss: 1.555, Global test accuracy: 47.03
Round  84, Train loss: 0.556, Test loss: 3.160, Test accuracy: 40.33
Round  84, Global train loss: 0.556, Global test loss: 1.732, Global test accuracy: 37.81
Round  85, Train loss: 0.655, Test loss: 3.168, Test accuracy: 40.63
Round  85, Global train loss: 0.655, Global test loss: 1.839, Global test accuracy: 36.09
Round  86, Train loss: 0.559, Test loss: 3.184, Test accuracy: 40.45
Round  86, Global train loss: 0.559, Global test loss: 1.522, Global test accuracy: 48.09
Round  87, Train loss: 0.319, Test loss: 3.192, Test accuracy: 40.62
Round  87, Global train loss: 0.319, Global test loss: 1.315, Global test accuracy: 57.16
Round  88, Train loss: 0.491, Test loss: 3.227, Test accuracy: 40.65
Round  88, Global train loss: 0.491, Global test loss: 1.503, Global test accuracy: 48.83
Round  89, Train loss: 0.467, Test loss: 3.245, Test accuracy: 40.43
Round  89, Global train loss: 0.467, Global test loss: 1.490, Global test accuracy: 49.53
Round  90, Train loss: 0.479, Test loss: 3.252, Test accuracy: 40.90
Round  90, Global train loss: 0.479, Global test loss: 1.610, Global test accuracy: 45.31
Round  91, Train loss: 0.505, Test loss: 3.237, Test accuracy: 40.65
Round  91, Global train loss: 0.505, Global test loss: 1.626, Global test accuracy: 45.09
Round  92, Train loss: 0.456, Test loss: 3.304, Test accuracy: 40.40
Round  92, Global train loss: 0.456, Global test loss: 1.457, Global test accuracy: 51.13
Round  93, Train loss: 0.527, Test loss: 3.337, Test accuracy: 40.64
Round  93, Global train loss: 0.527, Global test loss: 1.718, Global test accuracy: 41.78
Round  94, Train loss: 0.604, Test loss: 3.339, Test accuracy: 40.37
Round  94, Global train loss: 0.604, Global test loss: 1.780, Global test accuracy: 36.80
Round  95, Train loss: 0.467, Test loss: 3.368, Test accuracy: 40.53
Round  95, Global train loss: 0.467, Global test loss: 1.620, Global test accuracy: 43.02
Round  96, Train loss: 0.447, Test loss: 3.395, Test accuracy: 40.73
Round  96, Global train loss: 0.447, Global test loss: 1.579, Global test accuracy: 46.67
Round  97, Train loss: 0.493, Test loss: 3.374, Test accuracy: 40.89
Round  97, Global train loss: 0.493, Global test loss: 1.678, Global test accuracy: 43.72
Round  98, Train loss: 0.440, Test loss: 3.383, Test accuracy: 40.77
Round  98, Global train loss: 0.440, Global test loss: 1.537, Global test accuracy: 47.22
Round  99, Train loss: 0.476, Test loss: 3.445, Test accuracy: 40.70
Round  99, Global train loss: 0.476, Global test loss: 1.758, Global test accuracy: 35.91
Final Round, Train loss: 0.290, Test loss: 3.869, Test accuracy: 40.88
Final Round, Global train loss: 0.290, Global test loss: 1.758, Global test accuracy: 35.91
Average accuracy final 10 rounds: 40.65899999999999 

Average global accuracy final 10 rounds: 43.664500000000004 

5749.10884642601
[4.601930856704712, 9.203861713409424, 13.908807516098022, 18.61375331878662, 22.935617685317993, 27.257482051849365, 31.96799659729004, 36.67851114273071, 41.37480616569519, 46.07110118865967, 50.7659707069397, 55.46084022521973, 60.107961654663086, 64.75508308410645, 69.44348311424255, 74.13188314437866, 78.86026978492737, 83.58865642547607, 88.44218587875366, 93.29571533203125, 97.84953546524048, 102.4033555984497, 106.82702565193176, 111.25069570541382, 115.84008383750916, 120.42947196960449, 125.03644132614136, 129.64341068267822, 134.24903345108032, 138.85465621948242, 143.31622409820557, 147.7777919769287, 152.21312403678894, 156.64845609664917, 161.21722841262817, 165.78600072860718, 170.27984023094177, 174.77367973327637, 179.20401310920715, 183.63434648513794, 188.04593563079834, 192.45752477645874, 196.83101224899292, 201.2044997215271, 205.63429188728333, 210.06408405303955, 214.40868997573853, 218.7532958984375, 223.25448441505432, 227.75567293167114, 232.15120148658752, 236.5467300415039, 240.83341073989868, 245.12009143829346, 249.4416742324829, 253.76325702667236, 258.02900195121765, 262.29474687576294, 266.5114994049072, 270.7282519340515, 274.99079513549805, 279.2533383369446, 283.67508244514465, 288.0968265533447, 292.55396604537964, 297.01110553741455, 301.352281332016, 305.69345712661743, 310.1207163333893, 314.54797554016113, 318.8056766986847, 323.06337785720825, 327.40898394584656, 331.75459003448486, 336.0922842025757, 340.4299783706665, 344.77838349342346, 349.1267886161804, 353.4474837779999, 357.76817893981934, 362.1345212459564, 366.5008635520935, 370.79282784461975, 375.084792137146, 379.45149636268616, 383.8182005882263, 388.15873193740845, 392.4992632865906, 396.76853013038635, 401.03779697418213, 405.3363950252533, 409.63499307632446, 413.95568227767944, 418.2763714790344, 422.5838243961334, 426.8912773132324, 431.3660888671875, 435.8409004211426, 440.1599152088165, 444.4789299964905, 448.8048496246338, 453.1307692527771, 457.37598609924316, 461.62120294570923, 465.89739656448364, 470.17359018325806, 474.6651167869568, 479.1566433906555, 483.42972350120544, 487.70280361175537, 491.97226309776306, 496.24172258377075, 500.7952551841736, 505.3487877845764, 510.16822123527527, 514.9876546859741, 519.5701992511749, 524.1527438163757, 528.5263066291809, 532.8998694419861, 537.5219464302063, 542.1440234184265, 546.7760934829712, 551.4081635475159, 556.0487470626831, 560.6893305778503, 565.2601580619812, 569.8309855461121, 574.5232291221619, 579.2154726982117, 584.1001505851746, 588.9848284721375, 593.5414290428162, 598.0980296134949, 602.5679051876068, 607.0377807617188, 611.2941086292267, 615.5504364967346, 619.818389415741, 624.0863423347473, 628.6687433719635, 633.2511444091797, 637.5597763061523, 641.868408203125, 646.1649785041809, 650.4615488052368, 654.8447830677032, 659.2280173301697, 663.8137328624725, 668.3994483947754, 672.9772944450378, 677.5551404953003, 682.1554796695709, 686.7558188438416, 691.1336541175842, 695.5114893913269, 700.0959060192108, 704.6803226470947, 709.1944499015808, 713.7085771560669, 718.2539963722229, 722.7994155883789, 727.5109145641327, 732.2224135398865, 737.5822443962097, 742.942075252533, 747.653767824173, 752.365460395813, 756.7857558727264, 761.2060513496399, 767.1933922767639, 773.1807332038879, 777.4572284221649, 781.7337236404419, 786.3730397224426, 791.0123558044434, 795.4296562671661, 799.8469567298889, 804.3101925849915, 808.773428440094, 813.1766147613525, 817.5798010826111, 822.1571755409241, 826.7345499992371, 831.3014633655548, 835.8683767318726, 840.3752474784851, 844.8821182250977, 849.3928847312927, 853.9036512374878, 858.3064539432526, 862.7092566490173, 867.1633169651031, 871.617377281189, 876.1069025993347, 880.5964279174805, 885.1152935028076, 889.6341590881348, 894.1659972667694, 898.697835445404, 900.9988136291504, 903.2997918128967]
[32.4575, 32.4575, 36.8175, 36.8175, 38.9225, 38.9225, 39.805, 39.805, 40.76, 40.76, 42.125, 42.125, 41.8525, 41.8525, 42.735, 42.735, 42.6425, 42.6425, 43.0225, 43.0225, 43.6925, 43.6925, 44.225, 44.225, 44.64, 44.64, 44.5325, 44.5325, 44.4825, 44.4825, 44.6625, 44.6625, 45.2775, 45.2775, 45.4825, 45.4825, 45.53, 45.53, 45.0075, 45.0075, 45.0525, 45.0525, 45.22, 45.22, 45.1075, 45.1075, 44.8575, 44.8575, 44.66, 44.66, 44.1775, 44.1775, 44.6, 44.6, 44.2, 44.2, 44.3275, 44.3275, 43.8725, 43.8725, 44.0675, 44.0675, 43.6775, 43.6775, 43.6275, 43.6275, 43.2875, 43.2875, 43.355, 43.355, 43.4125, 43.4125, 43.1925, 43.1925, 43.0925, 43.0925, 43.1575, 43.1575, 43.115, 43.115, 43.265, 43.265, 43.01, 43.01, 42.8925, 42.8925, 42.935, 42.935, 42.91, 42.91, 42.9225, 42.9225, 43.11, 43.11, 42.9625, 42.9625, 43.0425, 43.0425, 42.7125, 42.7125, 42.62, 42.62, 42.48, 42.48, 42.4225, 42.4225, 42.335, 42.335, 42.1125, 42.1125, 41.875, 41.875, 41.69, 41.69, 41.7625, 41.7625, 41.73, 41.73, 41.5025, 41.5025, 41.47, 41.47, 41.2225, 41.2225, 40.975, 40.975, 41.2, 41.2, 41.3475, 41.3475, 41.575, 41.575, 41.7325, 41.7325, 41.435, 41.435, 41.47, 41.47, 41.01, 41.01, 40.84, 40.84, 41.0375, 41.0375, 40.9825, 40.9825, 40.335, 40.335, 40.19, 40.19, 40.3425, 40.3425, 40.45, 40.45, 40.07, 40.07, 40.3325, 40.3325, 40.38, 40.38, 40.125, 40.125, 40.2475, 40.2475, 40.4375, 40.4375, 40.4225, 40.4225, 40.33, 40.33, 40.6325, 40.6325, 40.4475, 40.4475, 40.615, 40.615, 40.6475, 40.6475, 40.4275, 40.4275, 40.9025, 40.9025, 40.65, 40.65, 40.4, 40.4, 40.64, 40.64, 40.3725, 40.3725, 40.535, 40.535, 40.7325, 40.7325, 40.8875, 40.8875, 40.775, 40.775, 40.695, 40.695, 40.875, 40.875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 2, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.094, Test loss: 1.871, Test accuracy: 34.10
Round   0, Global train loss: 2.094, Global test loss: 1.874, Global test accuracy: 35.44
Round   1, Train loss: 1.901, Test loss: 1.702, Test accuracy: 38.88
Round   1, Global train loss: 1.901, Global test loss: 1.605, Global test accuracy: 44.45
Round   2, Train loss: 1.677, Test loss: 1.649, Test accuracy: 40.00
Round   2, Global train loss: 1.677, Global test loss: 1.424, Global test accuracy: 49.42
Round   3, Train loss: 1.661, Test loss: 1.614, Test accuracy: 42.28
Round   3, Global train loss: 1.661, Global test loss: 1.388, Global test accuracy: 52.51
Round   4, Train loss: 1.604, Test loss: 1.542, Test accuracy: 45.92
Round   4, Global train loss: 1.604, Global test loss: 1.305, Global test accuracy: 55.87
Round   5, Train loss: 1.573, Test loss: 1.506, Test accuracy: 47.91
Round   5, Global train loss: 1.573, Global test loss: 1.275, Global test accuracy: 58.36
Round   6, Train loss: 1.552, Test loss: 1.468, Test accuracy: 49.25
Round   6, Global train loss: 1.552, Global test loss: 1.228, Global test accuracy: 60.93
Round   7, Train loss: 1.402, Test loss: 1.436, Test accuracy: 50.52
Round   7, Global train loss: 1.402, Global test loss: 1.155, Global test accuracy: 61.35
Round   8, Train loss: 1.345, Test loss: 1.408, Test accuracy: 51.51
Round   8, Global train loss: 1.345, Global test loss: 1.091, Global test accuracy: 64.10
Round   9, Train loss: 1.374, Test loss: 1.368, Test accuracy: 53.38
Round   9, Global train loss: 1.374, Global test loss: 1.085, Global test accuracy: 64.46
Round  10, Train loss: 1.291, Test loss: 1.361, Test accuracy: 53.80
Round  10, Global train loss: 1.291, Global test loss: 1.053, Global test accuracy: 65.63
Round  11, Train loss: 1.385, Test loss: 1.319, Test accuracy: 56.01
Round  11, Global train loss: 1.385, Global test loss: 1.064, Global test accuracy: 66.25
Round  12, Train loss: 1.282, Test loss: 1.302, Test accuracy: 56.37
Round  12, Global train loss: 1.282, Global test loss: 1.002, Global test accuracy: 66.80
Round  13, Train loss: 1.310, Test loss: 1.277, Test accuracy: 57.63
Round  13, Global train loss: 1.310, Global test loss: 1.011, Global test accuracy: 67.73
Round  14, Train loss: 1.257, Test loss: 1.263, Test accuracy: 58.29
Round  14, Global train loss: 1.257, Global test loss: 0.994, Global test accuracy: 67.84
Round  15, Train loss: 1.226, Test loss: 1.238, Test accuracy: 59.29
Round  15, Global train loss: 1.226, Global test loss: 0.994, Global test accuracy: 68.33
Round  16, Train loss: 1.227, Test loss: 1.233, Test accuracy: 59.56
Round  16, Global train loss: 1.227, Global test loss: 0.970, Global test accuracy: 68.12
Round  17, Train loss: 1.053, Test loss: 1.218, Test accuracy: 60.12
Round  17, Global train loss: 1.053, Global test loss: 0.924, Global test accuracy: 69.33
Round  18, Train loss: 1.150, Test loss: 1.215, Test accuracy: 60.33
Round  18, Global train loss: 1.150, Global test loss: 0.915, Global test accuracy: 70.38
Round  19, Train loss: 1.153, Test loss: 1.197, Test accuracy: 61.07
Round  19, Global train loss: 1.153, Global test loss: 0.909, Global test accuracy: 70.67
Round  20, Train loss: 1.096, Test loss: 1.199, Test accuracy: 60.96
Round  20, Global train loss: 1.096, Global test loss: 0.900, Global test accuracy: 70.89
Round  21, Train loss: 1.155, Test loss: 1.179, Test accuracy: 61.78
Round  21, Global train loss: 1.155, Global test loss: 0.898, Global test accuracy: 70.94
Round  22, Train loss: 1.101, Test loss: 1.174, Test accuracy: 62.12
Round  22, Global train loss: 1.101, Global test loss: 0.883, Global test accuracy: 71.67
Round  23, Train loss: 1.112, Test loss: 1.163, Test accuracy: 62.50
Round  23, Global train loss: 1.112, Global test loss: 0.881, Global test accuracy: 71.62
Round  24, Train loss: 1.182, Test loss: 1.154, Test accuracy: 62.74
Round  24, Global train loss: 1.182, Global test loss: 0.927, Global test accuracy: 71.23
Round  25, Train loss: 1.090, Test loss: 1.166, Test accuracy: 62.42
Round  25, Global train loss: 1.090, Global test loss: 0.880, Global test accuracy: 71.62
Round  26, Train loss: 1.008, Test loss: 1.170, Test accuracy: 62.45
Round  26, Global train loss: 1.008, Global test loss: 0.876, Global test accuracy: 71.17
Round  27, Train loss: 1.058, Test loss: 1.161, Test accuracy: 62.86
Round  27, Global train loss: 1.058, Global test loss: 0.867, Global test accuracy: 71.56
Round  28, Train loss: 1.085, Test loss: 1.162, Test accuracy: 63.07
Round  28, Global train loss: 1.085, Global test loss: 0.880, Global test accuracy: 71.87
Round  29, Train loss: 1.015, Test loss: 1.163, Test accuracy: 63.16
Round  29, Global train loss: 1.015, Global test loss: 0.845, Global test accuracy: 72.26
Round  30, Train loss: 0.893, Test loss: 1.158, Test accuracy: 63.27
Round  30, Global train loss: 0.893, Global test loss: 0.828, Global test accuracy: 72.75
Round  31, Train loss: 0.952, Test loss: 1.163, Test accuracy: 63.42
Round  31, Global train loss: 0.952, Global test loss: 0.822, Global test accuracy: 73.10
Round  32, Train loss: 0.993, Test loss: 1.149, Test accuracy: 63.91
Round  32, Global train loss: 0.993, Global test loss: 0.829, Global test accuracy: 73.58
Round  33, Train loss: 0.992, Test loss: 1.153, Test accuracy: 63.78
Round  33, Global train loss: 0.992, Global test loss: 0.830, Global test accuracy: 72.85
Round  34, Train loss: 0.901, Test loss: 1.164, Test accuracy: 63.76
Round  34, Global train loss: 0.901, Global test loss: 0.830, Global test accuracy: 73.03
Round  35, Train loss: 1.035, Test loss: 1.157, Test accuracy: 64.19
Round  35, Global train loss: 1.035, Global test loss: 0.854, Global test accuracy: 72.69
Round  36, Train loss: 1.003, Test loss: 1.167, Test accuracy: 64.04
Round  36, Global train loss: 1.003, Global test loss: 0.836, Global test accuracy: 73.04
Round  37, Train loss: 0.787, Test loss: 1.174, Test accuracy: 63.79
Round  37, Global train loss: 0.787, Global test loss: 0.825, Global test accuracy: 73.22
Round  38, Train loss: 1.037, Test loss: 1.170, Test accuracy: 63.95
Round  38, Global train loss: 1.037, Global test loss: 0.842, Global test accuracy: 73.07
Round  39, Train loss: 0.848, Test loss: 1.143, Test accuracy: 64.66
Round  39, Global train loss: 0.848, Global test loss: 0.833, Global test accuracy: 73.03
Round  40, Train loss: 0.950, Test loss: 1.153, Test accuracy: 64.43
Round  40, Global train loss: 0.950, Global test loss: 0.834, Global test accuracy: 73.55
Round  41, Train loss: 0.919, Test loss: 1.148, Test accuracy: 64.60
Round  41, Global train loss: 0.919, Global test loss: 0.811, Global test accuracy: 73.67
Round  42, Train loss: 1.038, Test loss: 1.152, Test accuracy: 64.63
Round  42, Global train loss: 1.038, Global test loss: 0.844, Global test accuracy: 73.84
Round  43, Train loss: 0.992, Test loss: 1.162, Test accuracy: 64.42
Round  43, Global train loss: 0.992, Global test loss: 0.861, Global test accuracy: 73.11
Round  44, Train loss: 1.043, Test loss: 1.159, Test accuracy: 64.41
Round  44, Global train loss: 1.043, Global test loss: 0.877, Global test accuracy: 72.67
Round  45, Train loss: 0.863, Test loss: 1.162, Test accuracy: 64.54
Round  45, Global train loss: 0.863, Global test loss: 0.839, Global test accuracy: 72.83
Round  46, Train loss: 0.918, Test loss: 1.178, Test accuracy: 64.07
Round  46, Global train loss: 0.918, Global test loss: 0.805, Global test accuracy: 73.81
Round  47, Train loss: 0.709, Test loss: 1.182, Test accuracy: 64.22
Round  47, Global train loss: 0.709, Global test loss: 0.813, Global test accuracy: 73.44
Round  48, Train loss: 0.873, Test loss: 1.175, Test accuracy: 64.42
Round  48, Global train loss: 0.873, Global test loss: 0.797, Global test accuracy: 74.41
Round  49, Train loss: 0.869, Test loss: 1.161, Test accuracy: 64.74
Round  49, Global train loss: 0.869, Global test loss: 0.811, Global test accuracy: 74.08
Round  50, Train loss: 0.919, Test loss: 1.156, Test accuracy: 65.03
Round  50, Global train loss: 0.919, Global test loss: 0.829, Global test accuracy: 73.43
Round  51, Train loss: 0.919, Test loss: 1.145, Test accuracy: 65.13
Round  51, Global train loss: 0.919, Global test loss: 0.824, Global test accuracy: 73.58
Round  52, Train loss: 0.839, Test loss: 1.144, Test accuracy: 65.16
Round  52, Global train loss: 0.839, Global test loss: 0.808, Global test accuracy: 73.77
Round  53, Train loss: 0.962, Test loss: 1.157, Test accuracy: 64.96
Round  53, Global train loss: 0.962, Global test loss: 0.828, Global test accuracy: 73.63
Round  54, Train loss: 0.817, Test loss: 1.153, Test accuracy: 64.97
Round  54, Global train loss: 0.817, Global test loss: 0.811, Global test accuracy: 74.17
Round  55, Train loss: 1.024, Test loss: 1.156, Test accuracy: 65.05
Round  55, Global train loss: 1.024, Global test loss: 0.837, Global test accuracy: 73.58
Round  56, Train loss: 0.834, Test loss: 1.167, Test accuracy: 64.97
Round  56, Global train loss: 0.834, Global test loss: 0.867, Global test accuracy: 72.14
Round  57, Train loss: 0.950, Test loss: 1.171, Test accuracy: 64.92
Round  57, Global train loss: 0.950, Global test loss: 0.836, Global test accuracy: 73.39
Round  58, Train loss: 0.988, Test loss: 1.171, Test accuracy: 65.09
Round  58, Global train loss: 0.988, Global test loss: 0.853, Global test accuracy: 73.20
Round  59, Train loss: 0.918, Test loss: 1.180, Test accuracy: 64.51
Round  59, Global train loss: 0.918, Global test loss: 0.862, Global test accuracy: 72.81
Round  60, Train loss: 0.876, Test loss: 1.177, Test accuracy: 64.92
Round  60, Global train loss: 0.876, Global test loss: 0.831, Global test accuracy: 73.19
Round  61, Train loss: 0.858, Test loss: 1.178, Test accuracy: 64.85
Round  61, Global train loss: 0.858, Global test loss: 0.832, Global test accuracy: 73.51
Round  62, Train loss: 0.785, Test loss: 1.167, Test accuracy: 65.13
Round  62, Global train loss: 0.785, Global test loss: 0.787, Global test accuracy: 74.35
Round  63, Train loss: 0.817, Test loss: 1.176, Test accuracy: 64.78
Round  63, Global train loss: 0.817, Global test loss: 0.840, Global test accuracy: 73.62
Round  64, Train loss: 0.838, Test loss: 1.171, Test accuracy: 65.04
Round  64, Global train loss: 0.838, Global test loss: 0.829, Global test accuracy: 73.22
Round  65, Train loss: 0.974, Test loss: 1.165, Test accuracy: 65.14
Round  65, Global train loss: 0.974, Global test loss: 0.859, Global test accuracy: 72.92
Round  66, Train loss: 0.863, Test loss: 1.170, Test accuracy: 64.92
Round  66, Global train loss: 0.863, Global test loss: 0.822, Global test accuracy: 73.92
Round  67, Train loss: 0.859, Test loss: 1.176, Test accuracy: 64.80
Round  67, Global train loss: 0.859, Global test loss: 0.840, Global test accuracy: 73.51
Round  68, Train loss: 0.792, Test loss: 1.167, Test accuracy: 65.31
Round  68, Global train loss: 0.792, Global test loss: 0.805, Global test accuracy: 74.50
Round  69, Train loss: 0.866, Test loss: 1.167, Test accuracy: 65.47
Round  69, Global train loss: 0.866, Global test loss: 0.829, Global test accuracy: 73.97
Round  70, Train loss: 0.713, Test loss: 1.182, Test accuracy: 65.11
Round  70, Global train loss: 0.713, Global test loss: 0.808, Global test accuracy: 74.38
Round  71, Train loss: 0.905, Test loss: 1.182, Test accuracy: 65.23
Round  71, Global train loss: 0.905, Global test loss: 0.851, Global test accuracy: 73.47
Round  72, Train loss: 0.852, Test loss: 1.176, Test accuracy: 65.17
Round  72, Global train loss: 0.852, Global test loss: 0.838, Global test accuracy: 73.47
Round  73, Train loss: 0.781, Test loss: 1.177, Test accuracy: 65.22
Round  73, Global train loss: 0.781, Global test loss: 0.831, Global test accuracy: 73.67
Round  74, Train loss: 0.835, Test loss: 1.185, Test accuracy: 64.92
Round  74, Global train loss: 0.835, Global test loss: 0.877, Global test accuracy: 72.61
Round  75, Train loss: 0.859, Test loss: 1.191, Test accuracy: 64.73
Round  75, Global train loss: 0.859, Global test loss: 0.853, Global test accuracy: 73.18
Round  76, Train loss: 0.811, Test loss: 1.189, Test accuracy: 65.00
Round  76, Global train loss: 0.811, Global test loss: 0.833, Global test accuracy: 73.50
Round  77, Train loss: 0.821, Test loss: 1.194, Test accuracy: 65.00
Round  77, Global train loss: 0.821, Global test loss: 0.815, Global test accuracy: 73.97
Round  78, Train loss: 0.749, Test loss: 1.207, Test accuracy: 64.61
Round  78, Global train loss: 0.749, Global test loss: 0.831, Global test accuracy: 73.84
Round  79, Train loss: 0.910, Test loss: 1.206, Test accuracy: 64.83
Round  79, Global train loss: 0.910, Global test loss: 0.893, Global test accuracy: 72.91
Round  80, Train loss: 0.763, Test loss: 1.194, Test accuracy: 64.82
Round  80, Global train loss: 0.763, Global test loss: 0.859, Global test accuracy: 73.07
Round  81, Train loss: 0.839, Test loss: 1.196, Test accuracy: 65.01
Round  81, Global train loss: 0.839, Global test loss: 0.847, Global test accuracy: 73.78
Round  82, Train loss: 0.783, Test loss: 1.198, Test accuracy: 65.06
Round  82, Global train loss: 0.783, Global test loss: 0.858, Global test accuracy: 73.28
Round  83, Train loss: 0.783, Test loss: 1.204, Test accuracy: 64.93
Round  83, Global train loss: 0.783, Global test loss: 0.825, Global test accuracy: 74.34
Round  84, Train loss: 0.867, Test loss: 1.213, Test accuracy: 64.82
Round  84, Global train loss: 0.867, Global test loss: 0.814, Global test accuracy: 74.08
Round  85, Train loss: 0.728, Test loss: 1.224, Test accuracy: 64.67
Round  85, Global train loss: 0.728, Global test loss: 0.801, Global test accuracy: 74.58
Round  86, Train loss: 0.787, Test loss: 1.205, Test accuracy: 64.83
Round  86, Global train loss: 0.787, Global test loss: 0.856, Global test accuracy: 72.96
Round  87, Train loss: 0.648, Test loss: 1.206, Test accuracy: 64.98
Round  87, Global train loss: 0.648, Global test loss: 0.830, Global test accuracy: 74.31
Round  88, Train loss: 0.746, Test loss: 1.216, Test accuracy: 64.88
Round  88, Global train loss: 0.746, Global test loss: 0.903, Global test accuracy: 72.71
Round  89, Train loss: 0.822, Test loss: 1.219, Test accuracy: 64.89
Round  89, Global train loss: 0.822, Global test loss: 0.850, Global test accuracy: 73.41
Round  90, Train loss: 0.794, Test loss: 1.230, Test accuracy: 64.84
Round  90, Global train loss: 0.794, Global test loss: 0.868, Global test accuracy: 73.24
Round  91, Train loss: 0.837, Test loss: 1.227, Test accuracy: 64.94
Round  91, Global train loss: 0.837, Global test loss: 0.877, Global test accuracy: 72.86
Round  92, Train loss: 0.764, Test loss: 1.235, Test accuracy: 64.72
Round  92, Global train loss: 0.764, Global test loss: 0.816, Global test accuracy: 74.43
Round  93, Train loss: 0.731, Test loss: 1.232, Test accuracy: 64.85
Round  93, Global train loss: 0.731, Global test loss: 0.828, Global test accuracy: 73.73
Round  94, Train loss: 0.812, Test loss: 1.222, Test accuracy: 64.81
Round  94, Global train loss: 0.812, Global test loss: 0.817, Global test accuracy: 73.85
Round  95, Train loss: 0.729, Test loss: 1.224, Test accuracy: 64.90
Round  95, Global train loss: 0.729, Global test loss: 0.835, Global test accuracy: 74.22
Round  96, Train loss: 0.767, Test loss: 1.203, Test accuracy: 65.35
Round  96, Global train loss: 0.767, Global test loss: 0.827, Global test accuracy: 74.13
Round  97, Train loss: 0.719, Test loss: 1.210, Test accuracy: 65.68
Round  97, Global train loss: 0.719, Global test loss: 0.839, Global test accuracy: 74.10
Round  98, Train loss: 0.788, Test loss: 1.216, Test accuracy: 65.63
Round  98, Global train loss: 0.788, Global test loss: 0.877, Global test accuracy: 73.11
Round  99, Train loss: 0.731, Test loss: 1.221, Test accuracy: 65.61
Round  99, Global train loss: 0.731, Global test loss: 0.822, Global test accuracy: 74.35
Final Round, Train loss: 0.482, Test loss: 1.344, Test accuracy: 65.70
Final Round, Global train loss: 0.482, Global test loss: 0.822, Global test accuracy: 74.35
Average accuracy final 10 rounds: 65.13425 

Average global accuracy final 10 rounds: 73.80299999999998 

5712.33495426178
[4.918663024902344, 9.837326049804688, 14.410980701446533, 18.98463535308838, 23.46156597137451, 27.938496589660645, 32.53910541534424, 37.13971424102783, 41.755125999450684, 46.370537757873535, 50.98115134239197, 55.5917649269104, 59.994771003723145, 64.39777708053589, 68.95763087272644, 73.51748466491699, 77.97160577774048, 82.42572689056396, 86.92907881736755, 91.43243074417114, 95.87991309165955, 100.32739543914795, 105.28660273551941, 110.24581003189087, 114.68024349212646, 119.11467695236206, 123.38470673561096, 127.65473651885986, 131.924320936203, 136.19390535354614, 140.47758221626282, 144.7612590789795, 149.02587389945984, 153.29048871994019, 157.56274914741516, 161.83500957489014, 166.24683141708374, 170.65865325927734, 174.9074046611786, 179.15615606307983, 183.38697791099548, 187.61779975891113, 191.8434135913849, 196.06902742385864, 200.30365228652954, 204.53827714920044, 209.15446853637695, 213.77065992355347, 218.36689949035645, 222.96313905715942, 227.7973394393921, 232.63153982162476, 237.11417627334595, 241.59681272506714, 246.28254961967468, 250.96828651428223, 255.58310770988464, 260.19792890548706, 264.95589876174927, 269.7138686180115, 274.33872985839844, 278.9635910987854, 283.6257646083832, 288.28793811798096, 292.9873950481415, 297.686851978302, 302.3453698158264, 307.00388765335083, 311.7720899581909, 316.540292263031, 321.1079115867615, 325.67553091049194, 330.2536871433258, 334.83184337615967, 339.573166847229, 344.31449031829834, 348.6227104663849, 352.93093061447144, 357.5302414894104, 362.12955236434937, 366.96085715293884, 371.7921619415283, 377.54935669898987, 383.3065514564514, 387.6352255344391, 391.96389961242676, 396.3728859424591, 400.78187227249146, 405.0912706851959, 409.4006690979004, 413.7156672477722, 418.03066539764404, 422.30714178085327, 426.5836181640625, 430.895316362381, 435.20701456069946, 439.56026101112366, 443.91350746154785, 448.23693895339966, 452.56037044525146, 456.8605844974518, 461.1607985496521, 465.4374384880066, 469.7140784263611, 474.01660919189453, 478.319139957428, 482.70428943634033, 487.0894389152527, 491.3969056606293, 495.70437240600586, 500.09394693374634, 504.4835214614868, 508.8685805797577, 513.2536396980286, 517.649251461029, 522.0448632240295, 526.3047144412994, 530.5645656585693, 534.8555943965912, 539.146623134613, 543.4312026500702, 547.7157821655273, 552.074221611023, 556.4326610565186, 560.7584054470062, 565.0841498374939, 569.4653487205505, 573.8465476036072, 578.1732711791992, 582.4999947547913, 586.9179239273071, 591.335853099823, 595.6236958503723, 599.9115386009216, 604.2190353870392, 608.5265321731567, 612.833354473114, 617.1401767730713, 621.4423635005951, 625.7445502281189, 630.0862348079681, 634.4279193878174, 638.7197406291962, 643.011561870575, 647.3002841472626, 651.5890064239502, 655.8633539676666, 660.137701511383, 664.4121806621552, 668.6866598129272, 672.982962846756, 677.2792658805847, 681.5749926567078, 685.8707194328308, 690.1685810089111, 694.4664425849915, 698.8803238868713, 703.2942051887512, 707.6178026199341, 711.941400051117, 716.4317746162415, 720.922149181366, 725.2903044223785, 729.6584596633911, 734.0519831180573, 738.4455065727234, 742.8862373828888, 747.3269681930542, 751.7392201423645, 756.1514720916748, 760.3913452625275, 764.6312184333801, 768.9013991355896, 773.1715798377991, 777.4301733970642, 781.6887669563293, 785.9833934307098, 790.2780199050903, 794.5057933330536, 798.7335667610168, 802.9998009204865, 807.266035079956, 811.5586225986481, 815.8512101173401, 820.2192640304565, 824.587317943573, 828.8602302074432, 833.1331424713135, 837.3899805545807, 841.6468186378479, 845.9086029529572, 850.1703872680664, 854.417690038681, 858.6649928092957, 862.9190437793732, 867.1730947494507, 871.4083652496338, 875.6436357498169, 879.8884687423706, 884.1333017349243, 886.2716715335846, 888.4100413322449]
[34.0975, 34.0975, 38.88, 38.88, 40.0, 40.0, 42.2775, 42.2775, 45.925, 45.925, 47.9125, 47.9125, 49.2525, 49.2525, 50.52, 50.52, 51.51, 51.51, 53.38, 53.38, 53.8025, 53.8025, 56.005, 56.005, 56.365, 56.365, 57.63, 57.63, 58.2925, 58.2925, 59.2875, 59.2875, 59.5575, 59.5575, 60.115, 60.115, 60.325, 60.325, 61.0725, 61.0725, 60.96, 60.96, 61.785, 61.785, 62.12, 62.12, 62.4975, 62.4975, 62.7425, 62.7425, 62.4175, 62.4175, 62.4475, 62.4475, 62.8575, 62.8575, 63.0725, 63.0725, 63.1625, 63.1625, 63.2725, 63.2725, 63.4175, 63.4175, 63.9125, 63.9125, 63.7775, 63.7775, 63.755, 63.755, 64.185, 64.185, 64.04, 64.04, 63.79, 63.79, 63.955, 63.955, 64.655, 64.655, 64.4325, 64.4325, 64.6025, 64.6025, 64.63, 64.63, 64.42, 64.42, 64.405, 64.405, 64.5375, 64.5375, 64.0725, 64.0725, 64.215, 64.215, 64.4175, 64.4175, 64.74, 64.74, 65.03, 65.03, 65.13, 65.13, 65.16, 65.16, 64.96, 64.96, 64.97, 64.97, 65.045, 65.045, 64.9725, 64.9725, 64.9225, 64.9225, 65.0925, 65.0925, 64.51, 64.51, 64.915, 64.915, 64.8475, 64.8475, 65.13, 65.13, 64.7775, 64.7775, 65.0375, 65.0375, 65.14, 65.14, 64.92, 64.92, 64.795, 64.795, 65.3125, 65.3125, 65.4675, 65.4675, 65.1075, 65.1075, 65.23, 65.23, 65.175, 65.175, 65.215, 65.215, 64.9225, 64.9225, 64.7275, 64.7275, 64.9975, 64.9975, 65.0, 65.0, 64.61, 64.61, 64.8275, 64.8275, 64.8225, 64.8225, 65.0125, 65.0125, 65.0625, 65.0625, 64.9275, 64.9275, 64.8175, 64.8175, 64.665, 64.665, 64.83, 64.83, 64.9825, 64.9825, 64.875, 64.875, 64.885, 64.885, 64.845, 64.845, 64.9425, 64.9425, 64.725, 64.725, 64.85, 64.85, 64.8125, 64.8125, 64.9025, 64.9025, 65.3525, 65.3525, 65.68, 65.68, 65.6275, 65.6275, 65.605, 65.605, 65.6975, 65.6975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 14, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.079, Test loss: 1.916, Test accuracy: 30.44
Round   0, Global train loss: 2.079, Global test loss: 1.902, Global test accuracy: 31.27
Round   1, Train loss: 1.913, Test loss: 1.795, Test accuracy: 34.75
Round   1, Global train loss: 1.913, Global test loss: 1.714, Global test accuracy: 38.48
Round   2, Train loss: 1.839, Test loss: 1.745, Test accuracy: 36.53
Round   2, Global train loss: 1.839, Global test loss: 1.596, Global test accuracy: 42.90
Round   3, Train loss: 1.747, Test loss: 1.708, Test accuracy: 37.83
Round   3, Global train loss: 1.747, Global test loss: 1.535, Global test accuracy: 45.00
Round   4, Train loss: 1.742, Test loss: 1.646, Test accuracy: 40.43
Round   4, Global train loss: 1.742, Global test loss: 1.478, Global test accuracy: 48.20
Round   5, Train loss: 1.680, Test loss: 1.605, Test accuracy: 42.22
Round   5, Global train loss: 1.680, Global test loss: 1.415, Global test accuracy: 50.95
Round   6, Train loss: 1.637, Test loss: 1.576, Test accuracy: 43.32
Round   6, Global train loss: 1.637, Global test loss: 1.385, Global test accuracy: 52.54
Round   7, Train loss: 1.644, Test loss: 1.551, Test accuracy: 44.50
Round   7, Global train loss: 1.644, Global test loss: 1.373, Global test accuracy: 54.04
Round   8, Train loss: 1.593, Test loss: 1.518, Test accuracy: 45.94
Round   8, Global train loss: 1.593, Global test loss: 1.313, Global test accuracy: 56.37
Round   9, Train loss: 1.565, Test loss: 1.477, Test accuracy: 47.84
Round   9, Global train loss: 1.565, Global test loss: 1.302, Global test accuracy: 57.73
Round  10, Train loss: 1.475, Test loss: 1.464, Test accuracy: 48.18
Round  10, Global train loss: 1.475, Global test loss: 1.221, Global test accuracy: 58.90
Round  11, Train loss: 1.487, Test loss: 1.408, Test accuracy: 50.80
Round  11, Global train loss: 1.487, Global test loss: 1.210, Global test accuracy: 59.71
Round  12, Train loss: 1.451, Test loss: 1.392, Test accuracy: 51.29
Round  12, Global train loss: 1.451, Global test loss: 1.181, Global test accuracy: 60.63
Round  13, Train loss: 1.391, Test loss: 1.350, Test accuracy: 53.08
Round  13, Global train loss: 1.391, Global test loss: 1.145, Global test accuracy: 60.78
Round  14, Train loss: 1.424, Test loss: 1.325, Test accuracy: 54.08
Round  14, Global train loss: 1.424, Global test loss: 1.139, Global test accuracy: 61.41
Round  15, Train loss: 1.400, Test loss: 1.302, Test accuracy: 55.30
Round  15, Global train loss: 1.400, Global test loss: 1.133, Global test accuracy: 62.44
Round  16, Train loss: 1.403, Test loss: 1.290, Test accuracy: 55.66
Round  16, Global train loss: 1.403, Global test loss: 1.133, Global test accuracy: 62.03
Round  17, Train loss: 1.517, Test loss: 1.298, Test accuracy: 55.57
Round  17, Global train loss: 1.517, Global test loss: 1.160, Global test accuracy: 62.76
Round  18, Train loss: 1.378, Test loss: 1.287, Test accuracy: 56.22
Round  18, Global train loss: 1.378, Global test loss: 1.108, Global test accuracy: 64.74
Round  19, Train loss: 1.334, Test loss: 1.254, Test accuracy: 57.52
Round  19, Global train loss: 1.334, Global test loss: 1.051, Global test accuracy: 65.03
Round  20, Train loss: 1.312, Test loss: 1.246, Test accuracy: 57.73
Round  20, Global train loss: 1.312, Global test loss: 1.050, Global test accuracy: 64.89
Round  21, Train loss: 1.264, Test loss: 1.226, Test accuracy: 58.70
Round  21, Global train loss: 1.264, Global test loss: 1.020, Global test accuracy: 65.97
Round  22, Train loss: 1.261, Test loss: 1.204, Test accuracy: 59.39
Round  22, Global train loss: 1.261, Global test loss: 1.011, Global test accuracy: 66.55
Round  23, Train loss: 1.120, Test loss: 1.198, Test accuracy: 59.55
Round  23, Global train loss: 1.120, Global test loss: 0.970, Global test accuracy: 66.93
Round  24, Train loss: 1.312, Test loss: 1.187, Test accuracy: 59.99
Round  24, Global train loss: 1.312, Global test loss: 1.019, Global test accuracy: 66.58
Round  25, Train loss: 1.193, Test loss: 1.168, Test accuracy: 60.89
Round  25, Global train loss: 1.193, Global test loss: 0.968, Global test accuracy: 67.94
Round  26, Train loss: 1.129, Test loss: 1.180, Test accuracy: 60.40
Round  26, Global train loss: 1.129, Global test loss: 0.962, Global test accuracy: 66.98
Round  27, Train loss: 1.285, Test loss: 1.154, Test accuracy: 61.20
Round  27, Global train loss: 1.285, Global test loss: 0.978, Global test accuracy: 67.69
Round  28, Train loss: 1.170, Test loss: 1.148, Test accuracy: 61.22
Round  28, Global train loss: 1.170, Global test loss: 0.949, Global test accuracy: 69.26
Round  29, Train loss: 1.183, Test loss: 1.136, Test accuracy: 61.83
Round  29, Global train loss: 1.183, Global test loss: 0.933, Global test accuracy: 68.84
Round  30, Train loss: 1.212, Test loss: 1.126, Test accuracy: 62.57
Round  30, Global train loss: 1.212, Global test loss: 0.947, Global test accuracy: 69.02
Round  31, Train loss: 1.168, Test loss: 1.121, Test accuracy: 62.70
Round  31, Global train loss: 1.168, Global test loss: 0.931, Global test accuracy: 69.25
Round  32, Train loss: 1.284, Test loss: 1.108, Test accuracy: 63.30
Round  32, Global train loss: 1.284, Global test loss: 0.964, Global test accuracy: 70.01
Round  33, Train loss: 1.260, Test loss: 1.104, Test accuracy: 63.50
Round  33, Global train loss: 1.260, Global test loss: 0.954, Global test accuracy: 69.77
Round  34, Train loss: 1.093, Test loss: 1.095, Test accuracy: 63.75
Round  34, Global train loss: 1.093, Global test loss: 0.896, Global test accuracy: 70.45
Round  35, Train loss: 1.225, Test loss: 1.099, Test accuracy: 63.74
Round  35, Global train loss: 1.225, Global test loss: 0.934, Global test accuracy: 70.26
Round  36, Train loss: 1.156, Test loss: 1.098, Test accuracy: 63.62
Round  36, Global train loss: 1.156, Global test loss: 0.920, Global test accuracy: 69.89
Round  37, Train loss: 1.129, Test loss: 1.098, Test accuracy: 63.88
Round  37, Global train loss: 1.129, Global test loss: 0.911, Global test accuracy: 70.83
Round  38, Train loss: 1.151, Test loss: 1.090, Test accuracy: 64.12
Round  38, Global train loss: 1.151, Global test loss: 0.918, Global test accuracy: 70.75
Round  39, Train loss: 1.191, Test loss: 1.096, Test accuracy: 64.08
Round  39, Global train loss: 1.191, Global test loss: 0.913, Global test accuracy: 71.50
Round  40, Train loss: 1.152, Test loss: 1.082, Test accuracy: 64.60
Round  40, Global train loss: 1.152, Global test loss: 0.889, Global test accuracy: 71.22
Round  41, Train loss: 1.002, Test loss: 1.091, Test accuracy: 64.45
Round  41, Global train loss: 1.002, Global test loss: 0.862, Global test accuracy: 71.57
Round  42, Train loss: 1.094, Test loss: 1.077, Test accuracy: 64.89
Round  42, Global train loss: 1.094, Global test loss: 0.866, Global test accuracy: 70.83
Round  43, Train loss: 1.095, Test loss: 1.075, Test accuracy: 64.84
Round  43, Global train loss: 1.095, Global test loss: 0.866, Global test accuracy: 71.72
Round  44, Train loss: 0.965, Test loss: 1.073, Test accuracy: 64.98
Round  44, Global train loss: 0.965, Global test loss: 0.844, Global test accuracy: 71.76
Round  45, Train loss: 0.962, Test loss: 1.074, Test accuracy: 64.97
Round  45, Global train loss: 0.962, Global test loss: 0.823, Global test accuracy: 72.20
Round  46, Train loss: 1.071, Test loss: 1.059, Test accuracy: 65.44
Round  46, Global train loss: 1.071, Global test loss: 0.849, Global test accuracy: 72.42
Round  47, Train loss: 1.070, Test loss: 1.057, Test accuracy: 65.46
Round  47, Global train loss: 1.070, Global test loss: 0.841, Global test accuracy: 72.64
Round  48, Train loss: 1.149, Test loss: 1.045, Test accuracy: 65.80
Round  48, Global train loss: 1.149, Global test loss: 0.882, Global test accuracy: 72.18
Round  49, Train loss: 1.002, Test loss: 1.032, Test accuracy: 66.14
Round  49, Global train loss: 1.002, Global test loss: 0.834, Global test accuracy: 72.38
Round  50, Train loss: 1.098, Test loss: 1.021, Test accuracy: 66.67
Round  50, Global train loss: 1.098, Global test loss: 0.843, Global test accuracy: 72.41
Round  51, Train loss: 1.060, Test loss: 1.027, Test accuracy: 66.61
Round  51, Global train loss: 1.060, Global test loss: 0.846, Global test accuracy: 72.15
Round  52, Train loss: 0.978, Test loss: 1.031, Test accuracy: 66.34
Round  52, Global train loss: 0.978, Global test loss: 0.829, Global test accuracy: 72.50
Round  53, Train loss: 1.085, Test loss: 1.033, Test accuracy: 66.38
Round  53, Global train loss: 1.085, Global test loss: 0.854, Global test accuracy: 72.52
Round  54, Train loss: 1.059, Test loss: 1.033, Test accuracy: 66.70
Round  54, Global train loss: 1.059, Global test loss: 0.840, Global test accuracy: 72.62
Round  55, Train loss: 0.962, Test loss: 1.027, Test accuracy: 66.89
Round  55, Global train loss: 0.962, Global test loss: 0.803, Global test accuracy: 72.59
Round  56, Train loss: 0.895, Test loss: 1.025, Test accuracy: 66.88
Round  56, Global train loss: 0.895, Global test loss: 0.801, Global test accuracy: 73.00
Round  57, Train loss: 1.046, Test loss: 1.027, Test accuracy: 66.84
Round  57, Global train loss: 1.046, Global test loss: 0.820, Global test accuracy: 73.12
Round  58, Train loss: 1.033, Test loss: 1.029, Test accuracy: 66.71
Round  58, Global train loss: 1.033, Global test loss: 0.821, Global test accuracy: 73.32
Round  59, Train loss: 0.985, Test loss: 1.022, Test accuracy: 67.16
Round  59, Global train loss: 0.985, Global test loss: 0.812, Global test accuracy: 72.96
Round  60, Train loss: 0.952, Test loss: 1.018, Test accuracy: 67.39
Round  60, Global train loss: 0.952, Global test loss: 0.809, Global test accuracy: 73.38
Round  61, Train loss: 0.961, Test loss: 1.026, Test accuracy: 67.03
Round  61, Global train loss: 0.961, Global test loss: 0.810, Global test accuracy: 73.26
Round  62, Train loss: 1.059, Test loss: 1.011, Test accuracy: 67.56
Round  62, Global train loss: 1.059, Global test loss: 0.823, Global test accuracy: 73.38
Round  63, Train loss: 0.875, Test loss: 1.007, Test accuracy: 67.64
Round  63, Global train loss: 0.875, Global test loss: 0.789, Global test accuracy: 73.22
Round  64, Train loss: 0.916, Test loss: 1.013, Test accuracy: 67.59
Round  64, Global train loss: 0.916, Global test loss: 0.794, Global test accuracy: 72.99
Round  65, Train loss: 0.937, Test loss: 1.011, Test accuracy: 67.53
Round  65, Global train loss: 0.937, Global test loss: 0.805, Global test accuracy: 73.38
Round  66, Train loss: 1.032, Test loss: 1.015, Test accuracy: 67.43
Round  66, Global train loss: 1.032, Global test loss: 0.822, Global test accuracy: 73.53
Round  67, Train loss: 1.056, Test loss: 1.005, Test accuracy: 67.78
Round  67, Global train loss: 1.056, Global test loss: 0.822, Global test accuracy: 73.34
Round  68, Train loss: 1.007, Test loss: 1.012, Test accuracy: 67.63
Round  68, Global train loss: 1.007, Global test loss: 0.821, Global test accuracy: 73.27
Round  69, Train loss: 1.022, Test loss: 1.011, Test accuracy: 67.46
Round  69, Global train loss: 1.022, Global test loss: 0.819, Global test accuracy: 73.12
Round  70, Train loss: 0.879, Test loss: 1.017, Test accuracy: 67.44
Round  70, Global train loss: 0.879, Global test loss: 0.796, Global test accuracy: 73.26
Round  71, Train loss: 0.899, Test loss: 1.003, Test accuracy: 67.95
Round  71, Global train loss: 0.899, Global test loss: 0.780, Global test accuracy: 73.84
Round  72, Train loss: 1.050, Test loss: 1.001, Test accuracy: 67.88
Round  72, Global train loss: 1.050, Global test loss: 0.814, Global test accuracy: 73.36
Round  73, Train loss: 0.989, Test loss: 1.004, Test accuracy: 67.92
Round  73, Global train loss: 0.989, Global test loss: 0.818, Global test accuracy: 73.00
Round  74, Train loss: 0.796, Test loss: 0.999, Test accuracy: 68.16
Round  74, Global train loss: 0.796, Global test loss: 0.776, Global test accuracy: 73.97
Round  75, Train loss: 0.970, Test loss: 0.999, Test accuracy: 68.00
Round  75, Global train loss: 0.970, Global test loss: 0.801, Global test accuracy: 73.22
Round  76, Train loss: 0.988, Test loss: 1.002, Test accuracy: 68.03
Round  76, Global train loss: 0.988, Global test loss: 0.808, Global test accuracy: 73.96
Round  77, Train loss: 1.023, Test loss: 0.997, Test accuracy: 68.31
Round  77, Global train loss: 1.023, Global test loss: 0.819, Global test accuracy: 73.83
Round  78, Train loss: 0.914, Test loss: 0.994, Test accuracy: 68.41
Round  78, Global train loss: 0.914, Global test loss: 0.807, Global test accuracy: 73.54
Round  79, Train loss: 0.927, Test loss: 0.989, Test accuracy: 68.46
Round  79, Global train loss: 0.927, Global test loss: 0.800, Global test accuracy: 73.72
Round  80, Train loss: 0.931, Test loss: 0.999, Test accuracy: 68.17
Round  80, Global train loss: 0.931, Global test loss: 0.798, Global test accuracy: 73.79
Round  81, Train loss: 0.893, Test loss: 0.999, Test accuracy: 68.16
Round  81, Global train loss: 0.893, Global test loss: 0.784, Global test accuracy: 74.17
Round  82, Train loss: 0.916, Test loss: 0.988, Test accuracy: 68.41
Round  82, Global train loss: 0.916, Global test loss: 0.786, Global test accuracy: 74.27
Round  83, Train loss: 0.845, Test loss: 0.992, Test accuracy: 68.31
Round  83, Global train loss: 0.845, Global test loss: 0.790, Global test accuracy: 73.35
Round  84, Train loss: 0.958, Test loss: 0.994, Test accuracy: 68.27
Round  84, Global train loss: 0.958, Global test loss: 0.799, Global test accuracy: 73.68
Round  85, Train loss: 0.874, Test loss: 0.997, Test accuracy: 68.27
Round  85, Global train loss: 0.874, Global test loss: 0.783, Global test accuracy: 74.08
Round  86, Train loss: 0.978, Test loss: 1.000, Test accuracy: 68.25
Round  86, Global train loss: 0.978, Global test loss: 0.815, Global test accuracy: 74.12
Round  87, Train loss: 0.977, Test loss: 1.001, Test accuracy: 68.29
Round  87, Global train loss: 0.977, Global test loss: 0.803, Global test accuracy: 74.29
Round  88, Train loss: 0.878, Test loss: 1.007, Test accuracy: 68.25
Round  88, Global train loss: 0.878, Global test loss: 0.800, Global test accuracy: 73.76
Round  89, Train loss: 0.981, Test loss: 1.008, Test accuracy: 68.34
Round  89, Global train loss: 0.981, Global test loss: 0.804, Global test accuracy: 74.26
Round  90, Train loss: 0.838, Test loss: 0.999, Test accuracy: 68.63
Round  90, Global train loss: 0.838, Global test loss: 0.784, Global test accuracy: 73.91
Round  91, Train loss: 0.884, Test loss: 1.001, Test accuracy: 68.67
Round  91, Global train loss: 0.884, Global test loss: 0.791, Global test accuracy: 73.91
Round  92, Train loss: 0.912, Test loss: 0.993, Test accuracy: 68.76
Round  92, Global train loss: 0.912, Global test loss: 0.793, Global test accuracy: 74.30
Round  93, Train loss: 0.864, Test loss: 0.990, Test accuracy: 68.86
Round  93, Global train loss: 0.864, Global test loss: 0.771, Global test accuracy: 74.69/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  94, Train loss: 0.929, Test loss: 0.997, Test accuracy: 68.48
Round  94, Global train loss: 0.929, Global test loss: 0.794, Global test accuracy: 74.01
Round  95, Train loss: 0.888, Test loss: 0.993, Test accuracy: 68.67
Round  95, Global train loss: 0.888, Global test loss: 0.774, Global test accuracy: 74.71
Round  96, Train loss: 0.934, Test loss: 0.994, Test accuracy: 68.60
Round  96, Global train loss: 0.934, Global test loss: 0.791, Global test accuracy: 73.97
Round  97, Train loss: 0.882, Test loss: 0.994, Test accuracy: 68.90
Round  97, Global train loss: 0.882, Global test loss: 0.793, Global test accuracy: 74.41
Round  98, Train loss: 0.906, Test loss: 0.994, Test accuracy: 68.73
Round  98, Global train loss: 0.906, Global test loss: 0.803, Global test accuracy: 74.03
Round  99, Train loss: 0.906, Test loss: 1.006, Test accuracy: 68.25
Round  99, Global train loss: 0.906, Global test loss: 0.793, Global test accuracy: 74.26
Final Round, Train loss: 0.609, Test loss: 1.117, Test accuracy: 66.81
Final Round, Global train loss: 0.609, Global test loss: 0.793, Global test accuracy: 74.26
Average accuracy final 10 rounds: 68.65575 

Average global accuracy final 10 rounds: 74.22 

5900.76736664772
[4.878308534622192, 9.756617069244385, 14.385633707046509, 19.014650344848633, 23.76319169998169, 28.511733055114746, 33.314088582992554, 38.11644411087036, 42.92012047767639, 47.72379684448242, 52.52761507034302, 57.33143329620361, 62.08183312416077, 66.83223295211792, 71.60600709915161, 76.3797812461853, 81.14573788642883, 85.91169452667236, 90.6353747844696, 95.35905504226685, 99.95986914634705, 104.56068325042725, 109.14828085899353, 113.73587846755981, 118.33574056625366, 122.93560266494751, 127.53225326538086, 132.1289038658142, 136.74643850326538, 141.36397314071655, 145.98123049736023, 150.5984878540039, 155.20221424102783, 159.80594062805176, 164.40192103385925, 168.99790143966675, 173.61266231536865, 178.22742319107056, 182.8434739112854, 187.45952463150024, 192.08211612701416, 196.70470762252808, 201.31343269348145, 205.92215776443481, 210.52364444732666, 215.1251311302185, 219.73602485656738, 224.34691858291626, 228.96593618392944, 233.58495378494263, 238.1904637813568, 242.795973777771, 247.40578651428223, 252.01559925079346, 256.6617739200592, 261.30794858932495, 265.9371716976166, 270.5663948059082, 275.2070472240448, 279.8476996421814, 284.53372597694397, 289.21975231170654, 293.8228237628937, 298.4258952140808, 303.02330017089844, 307.62070512771606, 312.23307156562805, 316.84543800354004, 321.47065472602844, 326.09587144851685, 330.73338055610657, 335.3708896636963, 339.9820394515991, 344.59318923950195, 349.1998887062073, 353.8065881729126, 358.4173963069916, 363.02820444107056, 367.6257424354553, 372.2232804298401, 376.8609163761139, 381.4985523223877, 386.12091994285583, 390.743287563324, 395.37391996383667, 400.00455236434937, 404.6338965892792, 409.263240814209, 413.8908178806305, 418.518394947052, 423.19162034988403, 427.86484575271606, 432.4792249202728, 437.0936040878296, 441.7093172073364, 446.32503032684326, 450.9396240711212, 455.55421781539917, 460.18890714645386, 464.82359647750854, 469.43593668937683, 474.0482769012451, 478.66650390625, 483.2847309112549, 487.8978807926178, 492.5110306739807, 497.1384787559509, 501.76592683792114, 506.39532494544983, 511.0247230529785, 515.6426239013672, 520.2605247497559, 524.9182062149048, 529.5758876800537, 534.2067649364471, 538.8376421928406, 543.4700791835785, 548.1025161743164, 552.7242670059204, 557.3460178375244, 561.9770159721375, 566.6080141067505, 571.2212822437286, 575.8345503807068, 580.4682931900024, 585.1020359992981, 589.7348659038544, 594.3676958084106, 599.0026185512543, 603.6375412940979, 608.261004447937, 612.8844676017761, 617.4929232597351, 622.1013789176941, 626.7273075580597, 631.3532361984253, 635.989086151123, 640.6249361038208, 645.271909236908, 649.9188823699951, 654.5807688236237, 659.2426552772522, 663.9007203578949, 668.5587854385376, 673.1947536468506, 677.8307218551636, 682.4679923057556, 687.1052627563477, 691.7571456432343, 696.4090285301208, 701.0508122444153, 705.6925959587097, 710.3407890796661, 714.9889822006226, 719.6268589496613, 724.2647356987, 728.9126257896423, 733.5605158805847, 738.3303134441376, 743.1001110076904, 747.9171767234802, 752.73424243927, 757.3572719097137, 761.9803013801575, 766.6117668151855, 771.2432322502136, 775.8965210914612, 780.5498099327087, 785.1917629241943, 789.8337159156799, 794.4972658157349, 799.1608157157898, 803.7988057136536, 808.4367957115173, 813.1031401157379, 817.7694845199585, 822.427750825882, 827.0860171318054, 831.7361526489258, 836.3862881660461, 841.0443432331085, 845.7023983001709, 850.3411843776703, 854.9799704551697, 859.6261157989502, 864.2722611427307, 868.9084074497223, 873.5445537567139, 878.1881411075592, 882.8317284584045, 887.4601736068726, 892.0886187553406, 896.731835603714, 901.3750524520874, 906.018238067627, 910.6614236831665, 915.2912218570709, 919.9210200309753, 924.5577352046967, 929.194450378418, 931.5267038345337, 933.8589572906494]
[30.4375, 30.4375, 34.75, 34.75, 36.5275, 36.5275, 37.8275, 37.8275, 40.43, 40.43, 42.2175, 42.2175, 43.3225, 43.3225, 44.5, 44.5, 45.935, 45.935, 47.8425, 47.8425, 48.1775, 48.1775, 50.8025, 50.8025, 51.2925, 51.2925, 53.08, 53.08, 54.08, 54.08, 55.2975, 55.2975, 55.665, 55.665, 55.57, 55.57, 56.2175, 56.2175, 57.5175, 57.5175, 57.7325, 57.7325, 58.7025, 58.7025, 59.3875, 59.3875, 59.55, 59.55, 59.9875, 59.9875, 60.89, 60.89, 60.4025, 60.4025, 61.195, 61.195, 61.2225, 61.2225, 61.83, 61.83, 62.5725, 62.5725, 62.705, 62.705, 63.3, 63.3, 63.5, 63.5, 63.7525, 63.7525, 63.7375, 63.7375, 63.6175, 63.6175, 63.885, 63.885, 64.12, 64.12, 64.0825, 64.0825, 64.6, 64.6, 64.4525, 64.4525, 64.89, 64.89, 64.845, 64.845, 64.98, 64.98, 64.9675, 64.9675, 65.44, 65.44, 65.4575, 65.4575, 65.8, 65.8, 66.145, 66.145, 66.6725, 66.6725, 66.6075, 66.6075, 66.34, 66.34, 66.3825, 66.3825, 66.705, 66.705, 66.895, 66.895, 66.88, 66.88, 66.84, 66.84, 66.71, 66.71, 67.155, 67.155, 67.3875, 67.3875, 67.03, 67.03, 67.555, 67.555, 67.64, 67.64, 67.5925, 67.5925, 67.535, 67.535, 67.4275, 67.4275, 67.7825, 67.7825, 67.63, 67.63, 67.4625, 67.4625, 67.435, 67.435, 67.9525, 67.9525, 67.875, 67.875, 67.925, 67.925, 68.1575, 68.1575, 68.0025, 68.0025, 68.0275, 68.0275, 68.305, 68.305, 68.4075, 68.4075, 68.4625, 68.4625, 68.1725, 68.1725, 68.155, 68.155, 68.41, 68.41, 68.3125, 68.3125, 68.27, 68.27, 68.27, 68.27, 68.255, 68.255, 68.29, 68.29, 68.245, 68.245, 68.3375, 68.3375, 68.6325, 68.6325, 68.6725, 68.6725, 68.7625, 68.7625, 68.8575, 68.8575, 68.48, 68.48, 68.675, 68.675, 68.5975, 68.5975, 68.8975, 68.8975, 68.7275, 68.7275, 68.255, 68.255, 66.8075, 66.8075]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 15, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.228, Test loss: 2.020, Test accuracy: 28.57
Round   1, Train loss: 1.988, Test loss: 1.789, Test accuracy: 36.66
Round   2, Train loss: 1.919, Test loss: 1.733, Test accuracy: 39.85
Round   3, Train loss: 1.778, Test loss: 1.642, Test accuracy: 42.68
Round   4, Train loss: 1.852, Test loss: 1.610, Test accuracy: 44.55
Round   5, Train loss: 1.789, Test loss: 1.565, Test accuracy: 46.66
Round   6, Train loss: 1.665, Test loss: 1.511, Test accuracy: 49.00
Round   7, Train loss: 1.607, Test loss: 1.462, Test accuracy: 50.98
Round   8, Train loss: 1.661, Test loss: 1.447, Test accuracy: 52.84
Round   9, Train loss: 1.545, Test loss: 1.400, Test accuracy: 53.65
Round  10, Train loss: 1.543, Test loss: 1.385, Test accuracy: 55.17
Round  11, Train loss: 1.467, Test loss: 1.317, Test accuracy: 56.70
Round  12, Train loss: 1.443, Test loss: 1.302, Test accuracy: 57.56
Round  13, Train loss: 1.522, Test loss: 1.262, Test accuracy: 58.75
Round  14, Train loss: 1.416, Test loss: 1.210, Test accuracy: 60.84
Round  15, Train loss: 1.426, Test loss: 1.202, Test accuracy: 60.82
Round  16, Train loss: 1.367, Test loss: 1.189, Test accuracy: 62.41
Round  17, Train loss: 1.416, Test loss: 1.185, Test accuracy: 62.90
Round  18, Train loss: 1.368, Test loss: 1.159, Test accuracy: 63.65
Round  19, Train loss: 1.396, Test loss: 1.151, Test accuracy: 64.00
Round  20, Train loss: 1.354, Test loss: 1.128, Test accuracy: 64.53
Round  21, Train loss: 1.356, Test loss: 1.108, Test accuracy: 64.92
Round  22, Train loss: 1.402, Test loss: 1.105, Test accuracy: 64.68
Round  23, Train loss: 1.371, Test loss: 1.089, Test accuracy: 65.67
Round  24, Train loss: 1.235, Test loss: 1.073, Test accuracy: 66.16
Round  25, Train loss: 1.249, Test loss: 1.062, Test accuracy: 66.17
Round  26, Train loss: 1.231, Test loss: 1.065, Test accuracy: 66.70
Round  27, Train loss: 1.223, Test loss: 1.048, Test accuracy: 66.95
Round  28, Train loss: 1.315, Test loss: 1.043, Test accuracy: 66.77
Round  29, Train loss: 1.285, Test loss: 1.044, Test accuracy: 67.22
Round  30, Train loss: 1.340, Test loss: 1.050, Test accuracy: 67.39
Round  31, Train loss: 1.216, Test loss: 1.037, Test accuracy: 67.42
Round  32, Train loss: 1.270, Test loss: 1.020, Test accuracy: 68.33
Round  33, Train loss: 1.220, Test loss: 1.015, Test accuracy: 68.56
Round  34, Train loss: 1.272, Test loss: 1.007, Test accuracy: 68.78
Round  35, Train loss: 1.197, Test loss: 1.004, Test accuracy: 68.89
Round  36, Train loss: 1.123, Test loss: 1.003, Test accuracy: 68.94
Round  37, Train loss: 1.329, Test loss: 1.007, Test accuracy: 69.31
Round  38, Train loss: 1.123, Test loss: 0.989, Test accuracy: 69.48
Round  39, Train loss: 1.140, Test loss: 0.985, Test accuracy: 69.28
Round  40, Train loss: 1.161, Test loss: 0.988, Test accuracy: 69.52
Round  41, Train loss: 1.050, Test loss: 0.971, Test accuracy: 69.80
Round  42, Train loss: 1.054, Test loss: 0.978, Test accuracy: 68.97
Round  43, Train loss: 1.079, Test loss: 0.967, Test accuracy: 69.53
Round  44, Train loss: 1.003, Test loss: 0.957, Test accuracy: 69.75
Round  45, Train loss: 1.121, Test loss: 0.979, Test accuracy: 69.21
Round  46, Train loss: 1.166, Test loss: 0.969, Test accuracy: 69.64
Round  47, Train loss: 1.251, Test loss: 0.977, Test accuracy: 69.67
Round  48, Train loss: 1.128, Test loss: 0.970, Test accuracy: 69.72
Round  49, Train loss: 1.060, Test loss: 0.963, Test accuracy: 69.74
Round  50, Train loss: 1.012, Test loss: 0.944, Test accuracy: 70.41
Round  51, Train loss: 1.062, Test loss: 0.937, Test accuracy: 70.75
Round  52, Train loss: 1.124, Test loss: 0.947, Test accuracy: 70.38
Round  53, Train loss: 1.135, Test loss: 0.940, Test accuracy: 71.16
Round  54, Train loss: 1.167, Test loss: 0.952, Test accuracy: 70.65
Round  55, Train loss: 0.968, Test loss: 0.937, Test accuracy: 71.11
Round  56, Train loss: 1.012, Test loss: 0.937, Test accuracy: 70.88
Round  57, Train loss: 0.989, Test loss: 0.933, Test accuracy: 70.77
Round  58, Train loss: 0.916, Test loss: 0.930, Test accuracy: 71.05
Round  59, Train loss: 0.944, Test loss: 0.930, Test accuracy: 70.71
Round  60, Train loss: 1.019, Test loss: 0.936, Test accuracy: 70.47
Round  61, Train loss: 0.964, Test loss: 0.934, Test accuracy: 70.64
Round  62, Train loss: 1.021, Test loss: 0.925, Test accuracy: 70.73
Round  63, Train loss: 1.024, Test loss: 0.943, Test accuracy: 70.84
Round  64, Train loss: 1.069, Test loss: 0.935, Test accuracy: 70.93
Round  65, Train loss: 0.937, Test loss: 0.927, Test accuracy: 70.98
Round  66, Train loss: 1.030, Test loss: 0.921, Test accuracy: 71.25
Round  67, Train loss: 0.933, Test loss: 0.911, Test accuracy: 71.57
Round  68, Train loss: 1.005, Test loss: 0.920, Test accuracy: 71.49
Round  69, Train loss: 0.969, Test loss: 0.923, Test accuracy: 71.79
Round  70, Train loss: 1.093, Test loss: 0.932, Test accuracy: 71.21
Round  71, Train loss: 0.959, Test loss: 0.931, Test accuracy: 70.95
Round  72, Train loss: 0.989, Test loss: 0.915, Test accuracy: 71.69
Round  73, Train loss: 1.119, Test loss: 0.928, Test accuracy: 71.34
Round  74, Train loss: 1.016, Test loss: 0.942, Test accuracy: 71.00
Round  75, Train loss: 1.015, Test loss: 0.936, Test accuracy: 71.12
Round  76, Train loss: 0.938, Test loss: 0.929, Test accuracy: 71.20
Round  77, Train loss: 1.005, Test loss: 0.919, Test accuracy: 71.35
Round  78, Train loss: 0.968, Test loss: 0.926, Test accuracy: 71.41
Round  79, Train loss: 0.958, Test loss: 0.929, Test accuracy: 71.62
Round  80, Train loss: 0.994, Test loss: 0.929, Test accuracy: 71.44
Round  81, Train loss: 0.901, Test loss: 0.918, Test accuracy: 71.36
Round  82, Train loss: 0.894, Test loss: 0.905, Test accuracy: 71.85
Round  83, Train loss: 1.059, Test loss: 0.927, Test accuracy: 71.29
Round  84, Train loss: 0.912, Test loss: 0.901, Test accuracy: 72.23
Round  85, Train loss: 0.941, Test loss: 0.912, Test accuracy: 71.79
Round  86, Train loss: 0.930, Test loss: 0.923, Test accuracy: 71.08
Round  87, Train loss: 1.056, Test loss: 0.915, Test accuracy: 71.86
Round  88, Train loss: 0.851, Test loss: 0.913, Test accuracy: 71.80
Round  89, Train loss: 0.987, Test loss: 0.925, Test accuracy: 71.54
Round  90, Train loss: 0.967, Test loss: 0.928, Test accuracy: 71.08
Round  91, Train loss: 0.944, Test loss: 0.920, Test accuracy: 71.21
Round  92, Train loss: 1.041, Test loss: 0.917, Test accuracy: 71.67
Round  93, Train loss: 0.911, Test loss: 0.921, Test accuracy: 71.63
Round  94, Train loss: 0.875, Test loss: 0.908, Test accuracy: 71.96
Round  95, Train loss: 1.011, Test loss: 0.916, Test accuracy: 71.65
Round  96, Train loss: 0.886, Test loss: 0.920, Test accuracy: 71.42
Round  97, Train loss: 0.942, Test loss: 0.919, Test accuracy: 71.74
Round  98, Train loss: 0.918, Test loss: 0.926, Test accuracy: 71.38
Round  99, Train loss: 1.015, Test loss: 0.919, Test accuracy: 71.67
Final Round, Train loss: 0.872, Test loss: 0.913, Test accuracy: 71.44
Average accuracy final 10 rounds: 71.53975
4044.4082016944885
[5.3332741260528564, 10.403390645980835, 15.449078559875488, 20.50724172592163, 25.666816234588623, 30.73103427886963, 35.806503772735596, 40.82546830177307, 45.8649845123291, 50.928343534469604, 55.99527835845947, 61.06895399093628, 66.12520861625671, 71.22097206115723, 76.29349732398987, 81.31974339485168, 86.33635592460632, 91.36425995826721, 96.37259936332703, 101.3792290687561, 106.45525765419006, 111.495934009552, 116.50109672546387, 121.50921154022217, 126.51082992553711, 131.5466673374176, 136.56293845176697, 141.5479016304016, 146.54419541358948, 151.53751277923584, 156.53760957717896, 161.51829743385315, 166.50625276565552, 171.5517041683197, 176.56878232955933, 181.56492495536804, 186.56804728507996, 191.56069993972778, 196.5798146724701, 201.6186385154724, 206.6815013885498, 211.70206880569458, 216.71572017669678, 221.81009674072266, 226.86787295341492, 231.98430585861206, 237.05019545555115, 242.149400472641, 247.1755175590515, 252.20271754264832, 257.1812641620636, 262.22606134414673, 267.21478366851807, 272.2045199871063, 277.1935040950775, 282.18815636634827, 287.22959089279175, 292.2180199623108, 297.21352791786194, 302.20823788642883, 307.22050380706787, 312.24230098724365, 317.2647559642792, 322.2817544937134, 327.29317903518677, 332.28016424179077, 337.26996994018555, 342.2598738670349, 347.29516196250916, 352.33291006088257, 357.3764851093292, 362.4413356781006, 367.47598934173584, 372.4964270591736, 377.51608395576477, 382.528361082077, 387.5438792705536, 392.53451228141785, 397.5273070335388, 402.69410252571106, 407.9359381198883, 412.9231767654419, 417.9088304042816, 422.9412500858307, 427.9649817943573, 433.017697095871, 438.07328271865845, 443.08828711509705, 448.0903329849243, 453.10448813438416, 458.13004517555237, 463.13857674598694, 468.1361267566681, 473.1534106731415, 478.1808030605316, 483.216735124588, 488.3737745285034, 493.4176092147827, 498.44531083106995, 503.48702597618103, 505.5460784435272]
[28.575, 36.665, 39.855, 42.6825, 44.5525, 46.665, 48.9975, 50.9825, 52.8425, 53.645, 55.17, 56.7025, 57.56, 58.75, 60.8375, 60.8225, 62.415, 62.8975, 63.6525, 64.005, 64.5275, 64.9175, 64.68, 65.665, 66.16, 66.175, 66.7, 66.9525, 66.77, 67.215, 67.39, 67.4225, 68.33, 68.56, 68.7775, 68.8925, 68.9425, 69.305, 69.4825, 69.2825, 69.515, 69.7975, 68.97, 69.5325, 69.7475, 69.2075, 69.6375, 69.6725, 69.7225, 69.7425, 70.405, 70.7475, 70.375, 71.1575, 70.65, 71.105, 70.8825, 70.765, 71.05, 70.7125, 70.465, 70.6375, 70.735, 70.8425, 70.9275, 70.98, 71.25, 71.5725, 71.49, 71.7925, 71.2075, 70.955, 71.685, 71.3375, 70.995, 71.1225, 71.1975, 71.35, 71.405, 71.6225, 71.44, 71.3575, 71.8475, 71.2875, 72.2325, 71.79, 71.0825, 71.8575, 71.8, 71.5425, 71.08, 71.21, 71.6675, 71.63, 71.9575, 71.6475, 71.4175, 71.7375, 71.38, 71.67, 71.4425]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 8, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  26.6400
Round 1 global test acc  40.1100
Round 2 global test acc  41.9500
Round 3 global test acc  44.3200
Round 4 global test acc  45.6200
Round 5 global test acc  48.1800
Round 6 global test acc  49.4400
Round 7 global test acc  51.0100
Round 8 global test acc  48.7500
Round 9 global test acc  53.7300
Round 10 global test acc  52.2900
Round 11 global test acc  53.3700
Round 12 global test acc  55.4900
Round 13 global test acc  54.2800
Round 14 global test acc  56.7100
Round 15 global test acc  56.5600
Round 16 global test acc  57.9200
Round 17 global test acc  57.6800
Round 18 global test acc  57.2700
Round 19 global test acc  58.8400
Round 20 global test acc  58.7600
Round 21 global test acc  58.8700
Round 22 global test acc  58.3200
Round 23 global test acc  59.3000
Round 24 global test acc  58.0400
Round 25 global test acc  60.1100
Round 26 global test acc  58.9100
Round 27 global test acc  60.4600
Round 28 global test acc  59.6600
Round 29 global test acc  60.9400
Round 30 global test acc  59.7300
Round 31 global test acc  60.6700
Round 32 global test acc  60.6500
Round 33 global test acc  60.9600
Round 34 global test acc  62.3700
Round 35 global test acc  62.7300
Round 36 global test acc  61.7300
Round 37 global test acc  61.8300
Round 38 global test acc  60.8800
Round 39 global test acc  61.7100
Round 40 global test acc  62.0500
Round 41 global test acc  61.4300
Round 42 global test acc  63.5800
Round 43 global test acc  62.9900
Round 44 global test acc  63.9600
Round 45 global test acc  62.9800
Round 46 global test acc  63.4600
Round 47 global test acc  62.7100
Round 48 global test acc  62.9500
Round 49 global test acc  64.4700
Round 50 global test acc  63.2000
Round 51 global test acc  63.8800
Round 52 global test acc  62.4200
Round 53 global test acc  64.5400
Round 54 global test acc  63.4500
Round 55 global test acc  63.8400
Round 56 global test acc  62.9800
Round 57 global test acc  64.0200
Round 58 global test acc  64.0600
Round 59 global test acc  64.0400
Round 60 global test acc  63.5600
Round 61 global test acc  64.5400
Round 62 global test acc  64.9100
Round 63 global test acc  65.5000
Round 64 global test acc  64.9300
Round 65 global test acc  64.8500
Round 66 global test acc  65.0800
Round 67 global test acc  65.2100
Round 68 global test acc  65.6100
Round 69 global test acc  65.3200
Round 70 global test acc  65.5800
Round 71 global test acc  65.9800
Round 72 global test acc  65.1600
Round 73 global test acc  65.0800
Round 74 global test acc  64.2900
Round 75 global test acc  65.4500
Round 76 global test acc  66.8100
Round 77 global test acc  63.8300
Round 78 global test acc  66.3600
Round 79 global test acc  66.9500
Round 80 global test acc  65.4700
Round 81 global test acc  64.3300
Round 82 global test acc  63.8100
Round 83 global test acc  63.1500
Round 84 global test acc  62.1800
Round 85 global test acc  61.1700
Round 86 global test acc  60.1600
Round 87 global test acc  59.7400
Round 88 global test acc  58.5200
Round 89 global test acc  58.0000
Round 90 global test acc  57.3000
Round 91 global test acc  56.5700
Round 92 global test acc  56.9300
Round 93 global test acc  57.1700
Round 94 global test acc  56.6800
Round 95 global test acc  56.3200
Round 96 global test acc  55.9200
Round 97 global test acc  56.9900
Round 98 global test acc  57.1600
Round 99 global test acc  57.1100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 14, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.217, Test loss: 2.024, Test accuracy: 29.69
Round   1, Train loss: 1.975, Test loss: 1.784, Test accuracy: 37.54
Round   2, Train loss: 1.794, Test loss: 1.656, Test accuracy: 42.59
Round   3, Train loss: 1.751, Test loss: 1.576, Test accuracy: 45.48
Round   4, Train loss: 1.730, Test loss: 1.516, Test accuracy: 47.91
Round   5, Train loss: 1.682, Test loss: 1.456, Test accuracy: 50.16
Round   6, Train loss: 1.585, Test loss: 1.428, Test accuracy: 52.25
Round   7, Train loss: 1.542, Test loss: 1.405, Test accuracy: 53.72
Round   8, Train loss: 1.500, Test loss: 1.358, Test accuracy: 55.05
Round   9, Train loss: 1.523, Test loss: 1.334, Test accuracy: 55.80
Round  10, Train loss: 1.407, Test loss: 1.300, Test accuracy: 57.12
Round  11, Train loss: 1.436, Test loss: 1.232, Test accuracy: 58.21
Round  12, Train loss: 1.349, Test loss: 1.232, Test accuracy: 58.94
Round  13, Train loss: 1.430, Test loss: 1.198, Test accuracy: 60.30
Round  14, Train loss: 1.287, Test loss: 1.169, Test accuracy: 61.38
Round  15, Train loss: 1.304, Test loss: 1.159, Test accuracy: 61.55
Round  16, Train loss: 1.333, Test loss: 1.155, Test accuracy: 62.15
Round  17, Train loss: 1.282, Test loss: 1.138, Test accuracy: 62.97
Round  18, Train loss: 1.243, Test loss: 1.140, Test accuracy: 63.16
Round  19, Train loss: 1.236, Test loss: 1.100, Test accuracy: 64.62
Round  20, Train loss: 1.215, Test loss: 1.079, Test accuracy: 64.76
Round  21, Train loss: 1.255, Test loss: 1.055, Test accuracy: 65.56
Round  22, Train loss: 1.322, Test loss: 1.059, Test accuracy: 65.63
Round  23, Train loss: 1.190, Test loss: 1.041, Test accuracy: 66.13
Round  24, Train loss: 1.253, Test loss: 1.027, Test accuracy: 66.42
Round  25, Train loss: 1.183, Test loss: 1.019, Test accuracy: 67.34
Round  26, Train loss: 1.138, Test loss: 1.027, Test accuracy: 67.17
Round  27, Train loss: 1.197, Test loss: 1.011, Test accuracy: 67.66
Round  28, Train loss: 1.131, Test loss: 0.977, Test accuracy: 68.40
Round  29, Train loss: 1.104, Test loss: 0.985, Test accuracy: 68.37
Round  30, Train loss: 1.097, Test loss: 1.000, Test accuracy: 68.36
Round  31, Train loss: 1.237, Test loss: 0.990, Test accuracy: 68.67
Round  32, Train loss: 1.109, Test loss: 0.985, Test accuracy: 68.97
Round  33, Train loss: 1.070, Test loss: 0.972, Test accuracy: 69.23
Round  34, Train loss: 1.165, Test loss: 0.960, Test accuracy: 69.31
Round  35, Train loss: 1.143, Test loss: 0.962, Test accuracy: 69.64
Round  36, Train loss: 1.149, Test loss: 0.947, Test accuracy: 69.88
Round  37, Train loss: 1.012, Test loss: 0.953, Test accuracy: 69.74
Round  38, Train loss: 1.119, Test loss: 0.940, Test accuracy: 70.03
Round  39, Train loss: 1.033, Test loss: 0.948, Test accuracy: 69.73
Round  40, Train loss: 1.112, Test loss: 0.945, Test accuracy: 70.17
Round  41, Train loss: 1.030, Test loss: 0.945, Test accuracy: 70.13
Round  42, Train loss: 1.150, Test loss: 0.951, Test accuracy: 70.05
Round  43, Train loss: 1.075, Test loss: 0.938, Test accuracy: 70.22
Round  44, Train loss: 1.041, Test loss: 0.938, Test accuracy: 70.10
Round  45, Train loss: 0.968, Test loss: 0.938, Test accuracy: 70.36
Round  46, Train loss: 1.039, Test loss: 0.927, Test accuracy: 70.16
Round  47, Train loss: 0.987, Test loss: 0.920, Test accuracy: 70.69
Round  48, Train loss: 1.049, Test loss: 0.923, Test accuracy: 70.56
Round  49, Train loss: 0.933, Test loss: 0.914, Test accuracy: 71.12
Round  50, Train loss: 0.962, Test loss: 0.904, Test accuracy: 71.67
Round  51, Train loss: 1.033, Test loss: 0.915, Test accuracy: 71.23
Round  52, Train loss: 1.024, Test loss: 0.915, Test accuracy: 71.47
Round  53, Train loss: 0.910, Test loss: 0.912, Test accuracy: 71.46
Round  54, Train loss: 0.962, Test loss: 0.913, Test accuracy: 71.36
Round  55, Train loss: 0.999, Test loss: 0.896, Test accuracy: 71.89
Round  56, Train loss: 0.993, Test loss: 0.903, Test accuracy: 71.63
Round  57, Train loss: 1.052, Test loss: 0.906, Test accuracy: 71.65
Round  58, Train loss: 0.994, Test loss: 0.901, Test accuracy: 72.00
Round  59, Train loss: 0.858, Test loss: 0.896, Test accuracy: 71.54
Round  60, Train loss: 1.015, Test loss: 0.893, Test accuracy: 71.96
Round  61, Train loss: 0.894, Test loss: 0.895, Test accuracy: 71.88
Round  62, Train loss: 1.033, Test loss: 0.887, Test accuracy: 72.15
Round  63, Train loss: 0.889, Test loss: 0.891, Test accuracy: 71.89
Round  64, Train loss: 0.869, Test loss: 0.885, Test accuracy: 71.90
Round  65, Train loss: 0.977, Test loss: 0.900, Test accuracy: 71.81
Round  66, Train loss: 1.006, Test loss: 0.889, Test accuracy: 72.12
Round  67, Train loss: 1.037, Test loss: 0.887, Test accuracy: 72.28
Round  68, Train loss: 0.869, Test loss: 0.889, Test accuracy: 72.09
Round  69, Train loss: 0.967, Test loss: 0.885, Test accuracy: 72.24
Round  70, Train loss: 0.917, Test loss: 0.881, Test accuracy: 72.23
Round  71, Train loss: 0.947, Test loss: 0.880, Test accuracy: 72.30
Round  72, Train loss: 0.948, Test loss: 0.880, Test accuracy: 72.14
Round  73, Train loss: 0.838, Test loss: 0.896, Test accuracy: 71.68
Round  74, Train loss: 0.963, Test loss: 0.890, Test accuracy: 72.04
Round  75, Train loss: 0.826, Test loss: 0.888, Test accuracy: 72.22
Round  76, Train loss: 0.840, Test loss: 0.889, Test accuracy: 71.89
Round  77, Train loss: 0.961, Test loss: 0.880, Test accuracy: 72.42
Round  78, Train loss: 0.781, Test loss: 0.881, Test accuracy: 72.43
Round  79, Train loss: 0.927, Test loss: 0.887, Test accuracy: 72.38
Round  80, Train loss: 0.810, Test loss: 0.886, Test accuracy: 72.22
Round  81, Train loss: 0.874, Test loss: 0.883, Test accuracy: 72.44
Round  82, Train loss: 0.886, Test loss: 0.893, Test accuracy: 72.17
Round  83, Train loss: 0.882, Test loss: 0.879, Test accuracy: 72.57
Round  84, Train loss: 0.963, Test loss: 0.881, Test accuracy: 72.14
Round  85, Train loss: 0.815, Test loss: 0.880, Test accuracy: 72.53
Round  86, Train loss: 0.827, Test loss: 0.885, Test accuracy: 72.35
Round  87, Train loss: 0.835, Test loss: 0.881, Test accuracy: 72.52
Round  88, Train loss: 0.891, Test loss: 0.893, Test accuracy: 72.01
Round  89, Train loss: 0.929, Test loss: 0.897, Test accuracy: 71.77
Round  90, Train loss: 0.925, Test loss: 0.898, Test accuracy: 71.89
Round  91, Train loss: 0.900, Test loss: 0.893, Test accuracy: 71.95
Round  92, Train loss: 0.938, Test loss: 0.886, Test accuracy: 72.58
Round  93, Train loss: 0.776, Test loss: 0.877, Test accuracy: 72.55
Round  94, Train loss: 0.906, Test loss: 0.883, Test accuracy: 72.32
Round  95, Train loss: 0.881, Test loss: 0.885, Test accuracy: 72.29
Round  96, Train loss: 0.808, Test loss: 0.878, Test accuracy: 72.66
Round  97, Train loss: 0.717, Test loss: 0.889, Test accuracy: 72.24
Round  98, Train loss: 0.807, Test loss: 0.881, Test accuracy: 72.56
Round  99, Train loss: 0.930, Test loss: 0.885, Test accuracy: 72.57
Final Round, Train loss: 0.778, Test loss: 0.887, Test accuracy: 72.47
Average accuracy final 10 rounds: 72.36
4157.710276842117
[5.75002908706665, 11.264784336090088, 16.714356184005737, 21.85017466545105, 26.98297619819641, 32.21085715293884, 37.39904832839966, 42.560776710510254, 47.718454360961914, 52.87989807128906, 58.062464475631714, 63.23271441459656, 68.36979794502258, 73.51317024230957, 78.65893959999084, 83.80153036117554, 88.92309784889221, 94.04779314994812, 99.18609547615051, 104.30314946174622, 109.43399119377136, 114.55311465263367, 119.66135263442993, 124.77599859237671, 129.93000745773315, 135.05124711990356, 140.1849913597107, 145.3184688091278, 150.46498465538025, 155.59197902679443, 160.72952032089233, 165.89906096458435, 171.0230746269226, 176.15267729759216, 181.2826943397522, 186.41359519958496, 191.57129049301147, 196.72213983535767, 201.93534660339355, 207.12711215019226, 212.27619862556458, 217.45130586624146, 222.6347782611847, 227.7960011959076, 232.95388627052307, 238.13280653953552, 243.26010966300964, 248.44240045547485, 253.6545970439911, 258.80117630958557, 263.93577551841736, 269.21288204193115, 274.359979391098, 279.48791885375977, 284.6124038696289, 289.7286057472229, 294.84844756126404, 299.9705767631531, 305.0915288925171, 310.2122731208801, 315.35836386680603, 320.48171186447144, 325.6048092842102, 330.7512595653534, 335.9383146762848, 341.1253433227539, 346.3091220855713, 351.4575364589691, 356.6286425590515, 361.8003568649292, 366.96857833862305, 372.18507075309753, 377.38507604599, 382.51946544647217, 387.6805830001831, 392.8270628452301, 397.9947683811188, 403.1691493988037, 408.32591485977173, 413.48585510253906, 418.6722276210785, 423.8277077674866, 429.003781080246, 434.1856255531311, 439.35367488861084, 444.5106055736542, 449.6700267791748, 454.83912897109985, 460.0312099456787, 465.2112498283386, 470.3711197376251, 475.5274622440338, 480.7010803222656, 485.862375497818, 491.0210976600647, 496.24675607681274, 501.4642689228058, 506.6667273044586, 511.8752362728119, 517.0966563224792, 519.2028915882111]
[29.6925, 37.5375, 42.59, 45.48, 47.9075, 50.1575, 52.2475, 53.7225, 55.0475, 55.7975, 57.1225, 58.2075, 58.935, 60.295, 61.38, 61.5475, 62.145, 62.9675, 63.16, 64.6225, 64.76, 65.56, 65.63, 66.1275, 66.4175, 67.34, 67.1725, 67.6575, 68.4025, 68.3725, 68.365, 68.665, 68.975, 69.2275, 69.315, 69.6425, 69.8825, 69.74, 70.03, 69.7325, 70.165, 70.13, 70.0475, 70.225, 70.1025, 70.3575, 70.1575, 70.6925, 70.5575, 71.1175, 71.6725, 71.2275, 71.475, 71.4575, 71.3625, 71.8875, 71.6325, 71.6525, 72.005, 71.5425, 71.9575, 71.88, 72.15, 71.8925, 71.8975, 71.815, 72.125, 72.285, 72.0875, 72.2375, 72.2325, 72.295, 72.14, 71.6775, 72.0425, 72.2225, 71.895, 72.425, 72.43, 72.375, 72.2225, 72.435, 72.1725, 72.57, 72.1425, 72.525, 72.35, 72.5175, 72.01, 71.7675, 71.8925, 71.9525, 72.58, 72.545, 72.32, 72.2875, 72.66, 72.2375, 72.555, 72.57, 72.4675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 14, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.233, Test loss: 2.040, Test accuracy: 29.42
Round   1, Train loss: 1.976, Test loss: 1.753, Test accuracy: 38.07
Round   2, Train loss: 1.853, Test loss: 1.681, Test accuracy: 41.59
Round   3, Train loss: 1.767, Test loss: 1.598, Test accuracy: 45.30
Round   4, Train loss: 1.819, Test loss: 1.524, Test accuracy: 48.09
Round   5, Train loss: 1.686, Test loss: 1.477, Test accuracy: 50.34
Round   6, Train loss: 1.565, Test loss: 1.415, Test accuracy: 52.46
Round   7, Train loss: 1.573, Test loss: 1.381, Test accuracy: 54.16
Round   8, Train loss: 1.608, Test loss: 1.358, Test accuracy: 55.86
Round   9, Train loss: 1.486, Test loss: 1.311, Test accuracy: 57.35
Round  10, Train loss: 1.504, Test loss: 1.304, Test accuracy: 58.02
Round  11, Train loss: 1.427, Test loss: 1.246, Test accuracy: 59.36
Round  12, Train loss: 1.395, Test loss: 1.237, Test accuracy: 59.83
Round  13, Train loss: 1.407, Test loss: 1.198, Test accuracy: 61.11
Round  14, Train loss: 1.439, Test loss: 1.188, Test accuracy: 61.67
Round  15, Train loss: 1.410, Test loss: 1.167, Test accuracy: 61.96
Round  16, Train loss: 1.296, Test loss: 1.131, Test accuracy: 62.99
Round  17, Train loss: 1.433, Test loss: 1.160, Test accuracy: 62.47
Round  18, Train loss: 1.309, Test loss: 1.145, Test accuracy: 63.30
Round  19, Train loss: 1.339, Test loss: 1.111, Test accuracy: 63.68
Round  20, Train loss: 1.348, Test loss: 1.090, Test accuracy: 63.94
Round  21, Train loss: 1.404, Test loss: 1.085, Test accuracy: 64.22
Round  22, Train loss: 1.299, Test loss: 1.057, Test accuracy: 65.62
Round  23, Train loss: 1.231, Test loss: 1.068, Test accuracy: 65.31
Round  24, Train loss: 1.146, Test loss: 1.046, Test accuracy: 65.92
Round  25, Train loss: 1.294, Test loss: 1.030, Test accuracy: 66.83
Round  26, Train loss: 1.224, Test loss: 1.040, Test accuracy: 66.34
Round  27, Train loss: 1.168, Test loss: 1.022, Test accuracy: 67.00
Round  28, Train loss: 1.223, Test loss: 1.011, Test accuracy: 67.94
Round  29, Train loss: 1.232, Test loss: 1.012, Test accuracy: 68.02
Round  30, Train loss: 1.213, Test loss: 0.999, Test accuracy: 68.10
Round  31, Train loss: 1.178, Test loss: 0.993, Test accuracy: 68.07
Round  32, Train loss: 1.188, Test loss: 0.990, Test accuracy: 68.91
Round  33, Train loss: 1.151, Test loss: 0.977, Test accuracy: 68.91
Round  34, Train loss: 1.257, Test loss: 0.983, Test accuracy: 69.11
Round  35, Train loss: 1.145, Test loss: 0.973, Test accuracy: 69.22
Round  36, Train loss: 1.131, Test loss: 0.959, Test accuracy: 69.40
Round  37, Train loss: 1.208, Test loss: 0.966, Test accuracy: 69.23
Round  38, Train loss: 1.063, Test loss: 0.954, Test accuracy: 69.12
Round  39, Train loss: 1.055, Test loss: 0.957, Test accuracy: 69.74
Round  40, Train loss: 1.118, Test loss: 0.954, Test accuracy: 69.92
Round  41, Train loss: 1.176, Test loss: 0.951, Test accuracy: 70.09
Round  42, Train loss: 1.008, Test loss: 0.944, Test accuracy: 69.86
Round  43, Train loss: 1.109, Test loss: 0.938, Test accuracy: 70.49
Round  44, Train loss: 0.973, Test loss: 0.936, Test accuracy: 70.42
Round  45, Train loss: 0.945, Test loss: 0.939, Test accuracy: 70.33
Round  46, Train loss: 1.215, Test loss: 0.926, Test accuracy: 70.86
Round  47, Train loss: 1.158, Test loss: 0.931, Test accuracy: 71.04
Round  48, Train loss: 1.068, Test loss: 0.922, Test accuracy: 71.28
Round  49, Train loss: 1.030, Test loss: 0.920, Test accuracy: 71.12
Round  50, Train loss: 0.947, Test loss: 0.901, Test accuracy: 71.59
Round  51, Train loss: 0.990, Test loss: 0.907, Test accuracy: 71.60
Round  52, Train loss: 1.090, Test loss: 0.926, Test accuracy: 70.87
Round  53, Train loss: 1.034, Test loss: 0.919, Test accuracy: 71.33
Round  54, Train loss: 1.068, Test loss: 0.923, Test accuracy: 71.05
Round  55, Train loss: 0.933, Test loss: 0.915, Test accuracy: 71.00
Round  56, Train loss: 0.937, Test loss: 0.911, Test accuracy: 71.37
Round  57, Train loss: 0.979, Test loss: 0.902, Test accuracy: 71.54
Round  58, Train loss: 0.956, Test loss: 0.904, Test accuracy: 71.46
Round  59, Train loss: 0.957, Test loss: 0.897, Test accuracy: 71.68
Round  60, Train loss: 1.055, Test loss: 0.901, Test accuracy: 71.75
Round  61, Train loss: 0.895, Test loss: 0.896, Test accuracy: 71.69
Round  62, Train loss: 1.017, Test loss: 0.891, Test accuracy: 72.00
Round  63, Train loss: 0.945, Test loss: 0.896, Test accuracy: 71.53
Round  64, Train loss: 1.002, Test loss: 0.886, Test accuracy: 72.27
Round  65, Train loss: 1.044, Test loss: 0.899, Test accuracy: 71.71
Round  66, Train loss: 0.998, Test loss: 0.892, Test accuracy: 72.20
Round  67, Train loss: 0.861, Test loss: 0.890, Test accuracy: 71.74
Round  68, Train loss: 0.896, Test loss: 0.895, Test accuracy: 71.79
Round  69, Train loss: 0.975, Test loss: 0.877, Test accuracy: 72.28
Round  70, Train loss: 0.960, Test loss: 0.881, Test accuracy: 72.21
Round  71, Train loss: 0.929, Test loss: 0.881, Test accuracy: 72.25
Round  72, Train loss: 0.888, Test loss: 0.889, Test accuracy: 72.10
Round  73, Train loss: 1.011, Test loss: 0.874, Test accuracy: 72.53
Round  74, Train loss: 0.885, Test loss: 0.885, Test accuracy: 72.06
Round  75, Train loss: 0.852, Test loss: 0.901, Test accuracy: 71.84
Round  76, Train loss: 0.916, Test loss: 0.887, Test accuracy: 72.22
Round  77, Train loss: 0.985, Test loss: 0.885, Test accuracy: 72.41
Round  78, Train loss: 0.994, Test loss: 0.883, Test accuracy: 72.62
Round  79, Train loss: 0.939, Test loss: 0.886, Test accuracy: 72.56
Round  80, Train loss: 1.009, Test loss: 0.879, Test accuracy: 72.72
Round  81, Train loss: 0.947, Test loss: 0.884, Test accuracy: 72.43
Round  82, Train loss: 0.794, Test loss: 0.885, Test accuracy: 72.50
Round  83, Train loss: 0.868, Test loss: 0.880, Test accuracy: 72.64
Round  84, Train loss: 0.867, Test loss: 0.884, Test accuracy: 72.53
Round  85, Train loss: 0.936, Test loss: 0.883, Test accuracy: 72.44
Round  86, Train loss: 0.851, Test loss: 0.877, Test accuracy: 72.72
Round  87, Train loss: 0.852, Test loss: 0.890, Test accuracy: 72.36
Round  88, Train loss: 0.803, Test loss: 0.884, Test accuracy: 72.51
Round  89, Train loss: 0.896, Test loss: 0.879, Test accuracy: 72.59
Round  90, Train loss: 0.960, Test loss: 0.889, Test accuracy: 72.41
Round  91, Train loss: 0.940, Test loss: 0.881, Test accuracy: 72.91
Round  92, Train loss: 0.910, Test loss: 0.880, Test accuracy: 72.87
Round  93, Train loss: 0.885, Test loss: 0.882, Test accuracy: 72.68
Round  94, Train loss: 0.837, Test loss: 0.883, Test accuracy: 72.68
Round  95, Train loss: 0.881, Test loss: 0.885, Test accuracy: 72.76
Round  96, Train loss: 0.958, Test loss: 0.881, Test accuracy: 72.80
Round  97, Train loss: 0.938, Test loss: 0.891, Test accuracy: 72.72
Round  98, Train loss: 0.800, Test loss: 0.883, Test accuracy: 72.75
Round  99, Train loss: 0.900, Test loss: 0.881, Test accuracy: 72.93
Final Round, Train loss: 0.809, Test loss: 0.884, Test accuracy: 72.86
Average accuracy final 10 rounds: 72.75175
6149.662060260773
[5.509664535522461, 10.671338081359863, 15.827162265777588, 20.991814851760864, 26.140119791030884, 31.296181678771973, 36.448657512664795, 41.63204503059387, 46.78471493721008, 51.92946910858154, 57.050010681152344, 62.14983940124512, 67.25379419326782, 72.35812020301819, 77.50550842285156, 82.79484438896179, 87.90503263473511, 93.01182436943054, 98.12132620811462, 103.23872566223145, 108.3413257598877, 117.8154513835907, 127.35354161262512, 136.82501482963562, 146.17699646949768, 155.57324147224426, 165.0109567642212, 174.48834776878357, 183.87206315994263, 193.37225651741028, 202.89420771598816, 212.4401307106018, 221.89210319519043, 231.30461049079895, 240.74248456954956, 250.05950546264648, 259.45038628578186, 268.8212010860443, 278.29379749298096, 287.6412727832794, 297.1770899295807, 306.7664804458618, 316.31957960128784, 325.85639238357544, 335.41210436820984, 344.9541594982147, 354.555783033371, 364.0885682106018, 373.61715030670166, 383.08176827430725, 392.61581921577454, 402.2067093849182, 411.77554726600647, 421.2967550754547, 430.94578886032104, 440.5544943809509, 450.1244559288025, 459.6416757106781, 469.2308506965637, 478.83072781562805, 488.45617604255676, 497.99197816848755, 507.544636964798, 517.0379552841187, 526.6171896457672, 536.1432511806488, 545.7644853591919, 555.1615707874298, 564.5683200359344, 573.9681804180145, 583.2151424884796, 592.5006790161133, 601.7901549339294, 611.1521549224854, 620.459130525589, 629.956440448761, 639.6437466144562, 648.9606301784515, 658.3826622962952, 667.6430854797363, 677.0818617343903, 686.3161897659302, 695.6308598518372, 704.9723200798035, 714.3110530376434, 723.6383440494537, 732.962441444397, 742.2892889976501, 751.7947022914886, 761.138331413269, 770.4957716464996, 780.0977053642273, 789.4089503288269, 798.6123034954071, 808.2579607963562, 817.97896027565, 827.8820779323578, 837.6814634799957, 847.4006721973419, 856.883469581604, 858.977237701416]
[29.4225, 38.07, 41.5875, 45.3025, 48.095, 50.3425, 52.4625, 54.165, 55.8575, 57.3525, 58.015, 59.3575, 59.8325, 61.11, 61.6725, 61.9625, 62.995, 62.47, 63.305, 63.68, 63.935, 64.225, 65.6175, 65.305, 65.9175, 66.83, 66.34, 67.005, 67.9425, 68.02, 68.1, 68.07, 68.91, 68.905, 69.1075, 69.2225, 69.4, 69.23, 69.125, 69.7375, 69.92, 70.09, 69.8575, 70.4925, 70.4225, 70.335, 70.8575, 71.0425, 71.275, 71.125, 71.59, 71.6, 70.8725, 71.3275, 71.0525, 70.9975, 71.3675, 71.5425, 71.4625, 71.68, 71.7525, 71.6875, 72.005, 71.5275, 72.27, 71.71, 72.1975, 71.7425, 71.7925, 72.285, 72.21, 72.2525, 72.0975, 72.53, 72.055, 71.84, 72.225, 72.4075, 72.6175, 72.5625, 72.725, 72.4325, 72.495, 72.645, 72.5325, 72.44, 72.715, 72.36, 72.5125, 72.5875, 72.4125, 72.9075, 72.87, 72.68, 72.6775, 72.7625, 72.8025, 72.7175, 72.755, 72.9325, 72.855]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 201, in get_data_from_file
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_co_teaching2.py", line 92, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "RFL.py", line 62, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 93, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 95, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 13, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.212, Test loss: 2.120, Test accuracy: 25.18
Round   0, Global train loss: 2.212, Global test loss: 2.131, Global test accuracy: 25.41
Round   1, Train loss: 2.105, Test loss: 1.982, Test accuracy: 33.55
Round   1, Global train loss: 2.105, Global test loss: 1.934, Global test accuracy: 39.01
Round   2, Train loss: 2.047, Test loss: 1.930, Test accuracy: 34.15
Round   2, Global train loss: 2.047, Global test loss: 1.835, Global test accuracy: 40.26
Round   3, Train loss: 2.061, Test loss: 1.936, Test accuracy: 34.61
Round   3, Global train loss: 2.061, Global test loss: 1.939, Global test accuracy: 41.94
Round   4, Train loss: 2.070, Test loss: 1.897, Test accuracy: 35.48
Round   4, Global train loss: 2.070, Global test loss: 1.849, Global test accuracy: 44.66
Round   5, Train loss: 1.946, Test loss: 1.873, Test accuracy: 35.98
Round   5, Global train loss: 1.946, Global test loss: 1.813, Global test accuracy: 45.28
Round   6, Train loss: 1.882, Test loss: 1.859, Test accuracy: 36.83
Round   6, Global train loss: 1.882, Global test loss: 1.751, Global test accuracy: 47.32
Round   7, Train loss: 1.931, Test loss: 1.854, Test accuracy: 36.69
Round   7, Global train loss: 1.931, Global test loss: 1.783, Global test accuracy: 46.51
Round   8, Train loss: 2.164, Test loss: 1.863, Test accuracy: 36.81
Round   8, Global train loss: 2.164, Global test loss: 2.045, Global test accuracy: 39.66
Round   9, Train loss: 1.946, Test loss: 1.835, Test accuracy: 37.62
Round   9, Global train loss: 1.946, Global test loss: 1.810, Global test accuracy: 47.88
Round  10, Train loss: 1.908, Test loss: 1.845, Test accuracy: 37.12
Round  10, Global train loss: 1.908, Global test loss: 1.901, Global test accuracy: 41.43
Round  11, Train loss: 2.027, Test loss: 1.832, Test accuracy: 37.98
Round  11, Global train loss: 2.027, Global test loss: 1.916, Global test accuracy: 47.17
Round  12, Train loss: 1.962, Test loss: 1.829, Test accuracy: 37.80
Round  12, Global train loss: 1.962, Global test loss: 1.833, Global test accuracy: 45.86
Round  13, Train loss: 1.589, Test loss: 1.813, Test accuracy: 38.27
Round  13, Global train loss: 1.589, Global test loss: 1.531, Global test accuracy: 50.63
Round  14, Train loss: 1.812, Test loss: 1.823, Test accuracy: 38.09
Round  14, Global train loss: 1.812, Global test loss: 1.781, Global test accuracy: 48.64
Round  15, Train loss: 1.663, Test loss: 1.835, Test accuracy: 38.09
Round  15, Global train loss: 1.663, Global test loss: 1.647, Global test accuracy: 50.35
Round  16, Train loss: 1.778, Test loss: 1.842, Test accuracy: 37.69
Round  16, Global train loss: 1.778, Global test loss: 1.869, Global test accuracy: 37.83
Round  17, Train loss: 1.742, Test loss: 1.832, Test accuracy: 38.25
Round  17, Global train loss: 1.742, Global test loss: 1.646, Global test accuracy: 50.71
Round  18, Train loss: 1.720, Test loss: 1.830, Test accuracy: 38.35
Round  18, Global train loss: 1.720, Global test loss: 1.786, Global test accuracy: 42.52
Round  19, Train loss: 1.617, Test loss: 1.843, Test accuracy: 38.31
Round  19, Global train loss: 1.617, Global test loss: 1.872, Global test accuracy: 38.35
Round  20, Train loss: 1.812, Test loss: 1.844, Test accuracy: 38.47
Round  20, Global train loss: 1.812, Global test loss: 1.956, Global test accuracy: 37.69
Round  21, Train loss: 1.725, Test loss: 1.868, Test accuracy: 38.01
Round  21, Global train loss: 1.725, Global test loss: 1.833, Global test accuracy: 45.01
Round  22, Train loss: 1.618, Test loss: 1.872, Test accuracy: 37.95
Round  22, Global train loss: 1.618, Global test loss: 1.772, Global test accuracy: 46.45
Round  23, Train loss: 1.816, Test loss: 1.883, Test accuracy: 38.19
Round  23, Global train loss: 1.816, Global test loss: 1.899, Global test accuracy: 40.55
Round  24, Train loss: 1.723, Test loss: 1.884, Test accuracy: 38.12
Round  24, Global train loss: 1.723, Global test loss: 1.770, Global test accuracy: 47.62
Round  25, Train loss: 1.432, Test loss: 1.902, Test accuracy: 38.27
Round  25, Global train loss: 1.432, Global test loss: 1.717, Global test accuracy: 43.70
Round  26, Train loss: 1.494, Test loss: 1.914, Test accuracy: 38.16
Round  26, Global train loss: 1.494, Global test loss: 1.617, Global test accuracy: 46.73
Round  27, Train loss: 1.342, Test loss: 1.928, Test accuracy: 37.78
Round  27, Global train loss: 1.342, Global test loss: 1.782, Global test accuracy: 36.47
Round  28, Train loss: 1.807, Test loss: 1.937, Test accuracy: 37.34
Round  28, Global train loss: 1.807, Global test loss: 1.898, Global test accuracy: 42.68
Round  29, Train loss: 1.555, Test loss: 1.944, Test accuracy: 37.30
Round  29, Global train loss: 1.555, Global test loss: 1.840, Global test accuracy: 38.61
Round  30, Train loss: 1.645, Test loss: 1.982, Test accuracy: 36.49
Round  30, Global train loss: 1.645, Global test loss: 1.963, Global test accuracy: 35.02
Round  31, Train loss: 1.300, Test loss: 2.007, Test accuracy: 36.49
Round  31, Global train loss: 1.300, Global test loss: 1.740, Global test accuracy: 41.28
Round  32, Train loss: 1.451, Test loss: 2.038, Test accuracy: 36.39
Round  32, Global train loss: 1.451, Global test loss: 1.666, Global test accuracy: 48.32
Round  33, Train loss: 1.240, Test loss: 2.052, Test accuracy: 36.37
Round  33, Global train loss: 1.240, Global test loss: 1.809, Global test accuracy: 36.16
Round  34, Train loss: 1.163, Test loss: 2.105, Test accuracy: 35.92
Round  34, Global train loss: 1.163, Global test loss: 1.653, Global test accuracy: 44.47
Round  35, Train loss: 1.592, Test loss: 2.126, Test accuracy: 35.94
Round  35, Global train loss: 1.592, Global test loss: 1.909, Global test accuracy: 38.66
Round  36, Train loss: 1.013, Test loss: 2.144, Test accuracy: 35.48
Round  36, Global train loss: 1.013, Global test loss: 1.698, Global test accuracy: 38.75
Round  37, Train loss: 1.378, Test loss: 2.183, Test accuracy: 35.66
Round  37, Global train loss: 1.378, Global test loss: 1.938, Global test accuracy: 33.62
Round  38, Train loss: 1.461, Test loss: 2.208, Test accuracy: 35.23
Round  38, Global train loss: 1.461, Global test loss: 1.764, Global test accuracy: 41.85
Round  39, Train loss: 1.338, Test loss: 2.226, Test accuracy: 35.25
Round  39, Global train loss: 1.338, Global test loss: 1.753, Global test accuracy: 40.95
Round  40, Train loss: 1.110, Test loss: 2.253, Test accuracy: 34.93
Round  40, Global train loss: 1.110, Global test loss: 1.720, Global test accuracy: 41.45
Round  41, Train loss: 0.871, Test loss: 2.280, Test accuracy: 34.70
Round  41, Global train loss: 0.871, Global test loss: 1.544, Global test accuracy: 45.55
Round  42, Train loss: 1.079, Test loss: 2.326, Test accuracy: 34.55
Round  42, Global train loss: 1.079, Global test loss: 1.842, Global test accuracy: 34.72
Round  43, Train loss: 1.196, Test loss: 2.336, Test accuracy: 34.58
Round  43, Global train loss: 1.196, Global test loss: 1.615, Global test accuracy: 46.65
Round  44, Train loss: 1.263, Test loss: 2.364, Test accuracy: 34.69
Round  44, Global train loss: 1.263, Global test loss: 1.722, Global test accuracy: 42.68
Round  45, Train loss: 1.164, Test loss: 2.366, Test accuracy: 34.81
Round  45, Global train loss: 1.164, Global test loss: 1.763, Global test accuracy: 40.05
Round  46, Train loss: 1.106, Test loss: 2.396, Test accuracy: 34.77
Round  46, Global train loss: 1.106, Global test loss: 1.782, Global test accuracy: 38.38
Round  47, Train loss: 1.081, Test loss: 2.418, Test accuracy: 34.63
Round  47, Global train loss: 1.081, Global test loss: 1.757, Global test accuracy: 39.30
Round  48, Train loss: 1.026, Test loss: 2.482, Test accuracy: 34.27
Round  48, Global train loss: 1.026, Global test loss: 1.662, Global test accuracy: 43.40
Round  49, Train loss: 1.016, Test loss: 2.506, Test accuracy: 34.19
Round  49, Global train loss: 1.016, Global test loss: 1.755, Global test accuracy: 39.54
Round  50, Train loss: 1.100, Test loss: 2.539, Test accuracy: 33.97
Round  50, Global train loss: 1.100, Global test loss: 1.754, Global test accuracy: 42.52
Round  51, Train loss: 1.062, Test loss: 2.577, Test accuracy: 33.68
Round  51, Global train loss: 1.062, Global test loss: 1.841, Global test accuracy: 35.18
Round  52, Train loss: 1.070, Test loss: 2.588, Test accuracy: 33.83
Round  52, Global train loss: 1.070, Global test loss: 1.942, Global test accuracy: 31.39
Round  53, Train loss: 1.139, Test loss: 2.658, Test accuracy: 33.72
Round  53, Global train loss: 1.139, Global test loss: 1.918, Global test accuracy: 35.83
Round  54, Train loss: 0.938, Test loss: 2.698, Test accuracy: 33.42
Round  54, Global train loss: 0.938, Global test loss: 1.810, Global test accuracy: 39.37
Round  55, Train loss: 0.925, Test loss: 2.690, Test accuracy: 33.73
Round  55, Global train loss: 0.925, Global test loss: 1.717, Global test accuracy: 40.55
Round  56, Train loss: 1.064, Test loss: 2.709, Test accuracy: 33.62
Round  56, Global train loss: 1.064, Global test loss: 1.864, Global test accuracy: 35.26
Round  57, Train loss: 1.072, Test loss: 2.754, Test accuracy: 33.14
Round  57, Global train loss: 1.072, Global test loss: 1.890, Global test accuracy: 34.42
Round  58, Train loss: 1.125, Test loss: 2.786, Test accuracy: 33.34
Round  58, Global train loss: 1.125, Global test loss: 2.072, Global test accuracy: 23.98
Round  59, Train loss: 1.040, Test loss: 2.807, Test accuracy: 33.49
Round  59, Global train loss: 1.040, Global test loss: 2.019, Global test accuracy: 25.56
Round  60, Train loss: 1.018, Test loss: 2.847, Test accuracy: 33.19
Round  60, Global train loss: 1.018, Global test loss: 1.915, Global test accuracy: 34.11
Round  61, Train loss: 0.990, Test loss: 2.857, Test accuracy: 33.23
Round  61, Global train loss: 0.990, Global test loss: 1.926, Global test accuracy: 33.55
Round  62, Train loss: 0.868, Test loss: 2.898, Test accuracy: 33.16
Round  62, Global train loss: 0.868, Global test loss: 1.801, Global test accuracy: 37.74
Round  63, Train loss: 0.682, Test loss: 2.935, Test accuracy: 33.06
Round  63, Global train loss: 0.682, Global test loss: 1.711, Global test accuracy: 38.77
Round  64, Train loss: 0.877, Test loss: 2.992, Test accuracy: 32.84
Round  64, Global train loss: 0.877, Global test loss: 1.900, Global test accuracy: 33.68
Round  65, Train loss: 0.830, Test loss: 3.033, Test accuracy: 32.77
Round  65, Global train loss: 0.830, Global test loss: 2.049, Global test accuracy: 22.84
Round  66, Train loss: 0.853, Test loss: 3.092, Test accuracy: 32.45
Round  66, Global train loss: 0.853, Global test loss: 1.882, Global test accuracy: 33.98
Round  67, Train loss: 0.834, Test loss: 3.122, Test accuracy: 32.81
Round  67, Global train loss: 0.834, Global test loss: 1.950, Global test accuracy: 30.54
Round  68, Train loss: 0.717, Test loss: 3.111, Test accuracy: 32.95
Round  68, Global train loss: 0.717, Global test loss: 1.779, Global test accuracy: 36.92
Round  69, Train loss: 0.837, Test loss: 3.145, Test accuracy: 32.94
Round  69, Global train loss: 0.837, Global test loss: 1.994, Global test accuracy: 29.41
Round  70, Train loss: 0.795, Test loss: 3.155, Test accuracy: 33.10
Round  70, Global train loss: 0.795, Global test loss: 1.908, Global test accuracy: 33.63
Round  71, Train loss: 0.691, Test loss: 3.220, Test accuracy: 32.92
Round  71, Global train loss: 0.691, Global test loss: 1.761, Global test accuracy: 37.57
Round  72, Train loss: 0.668, Test loss: 3.205, Test accuracy: 33.11
Round  72, Global train loss: 0.668, Global test loss: 1.841, Global test accuracy: 36.02
Round  73, Train loss: 0.738, Test loss: 3.243, Test accuracy: 32.66
Round  73, Global train loss: 0.738, Global test loss: 1.952, Global test accuracy: 30.12
Round  74, Train loss: 0.797, Test loss: 3.305, Test accuracy: 32.42
Round  74, Global train loss: 0.797, Global test loss: 1.888, Global test accuracy: 33.20
Round  75, Train loss: 0.714, Test loss: 3.319, Test accuracy: 32.55
Round  75, Global train loss: 0.714, Global test loss: 1.804, Global test accuracy: 36.23
Round  76, Train loss: 0.744, Test loss: 3.375, Test accuracy: 32.27
Round  76, Global train loss: 0.744, Global test loss: 1.821, Global test accuracy: 36.37
Round  77, Train loss: 0.749, Test loss: 3.405, Test accuracy: 32.41
Round  77, Global train loss: 0.749, Global test loss: 1.920, Global test accuracy: 33.66
Round  78, Train loss: 0.599, Test loss: 3.404, Test accuracy: 32.65
Round  78, Global train loss: 0.599, Global test loss: 1.718, Global test accuracy: 40.21
Round  79, Train loss: 0.790, Test loss: 3.425, Test accuracy: 32.41
Round  79, Global train loss: 0.790, Global test loss: 1.789, Global test accuracy: 38.60
Round  80, Train loss: 0.734, Test loss: 3.476, Test accuracy: 32.53
Round  80, Global train loss: 0.734, Global test loss: 1.871, Global test accuracy: 35.56
Round  81, Train loss: 0.690, Test loss: 3.490, Test accuracy: 32.58
Round  81, Global train loss: 0.690, Global test loss: 1.863, Global test accuracy: 34.08
Round  82, Train loss: 0.717, Test loss: 3.505, Test accuracy: 32.65
Round  82, Global train loss: 0.717, Global test loss: 1.718, Global test accuracy: 40.85
Round  83, Train loss: 0.830, Test loss: 3.495, Test accuracy: 32.70
Round  83, Global train loss: 0.830, Global test loss: 1.898, Global test accuracy: 36.32
Round  84, Train loss: 0.666, Test loss: 3.573, Test accuracy: 32.39
Round  84, Global train loss: 0.666, Global test loss: 1.906, Global test accuracy: 33.47
Round  85, Train loss: 0.580, Test loss: 3.606, Test accuracy: 32.38
Round  85, Global train loss: 0.580, Global test loss: 1.737, Global test accuracy: 38.13
Round  86, Train loss: 0.701, Test loss: 3.635, Test accuracy: 32.20
Round  86, Global train loss: 0.701, Global test loss: 1.776, Global test accuracy: 39.55
Round  87, Train loss: 0.604, Test loss: 3.635, Test accuracy: 32.18
Round  87, Global train loss: 0.604, Global test loss: 1.738, Global test accuracy: 40.73
Round  88, Train loss: 0.508, Test loss: 3.613, Test accuracy: 32.01
Round  88, Global train loss: 0.508, Global test loss: 1.680, Global test accuracy: 40.14
Round  89, Train loss: 0.715, Test loss: 3.629, Test accuracy: 32.01
Round  89, Global train loss: 0.715, Global test loss: 1.943, Global test accuracy: 32.57
Round  90, Train loss: 0.538, Test loss: 3.669, Test accuracy: 32.00
Round  90, Global train loss: 0.538, Global test loss: 1.734, Global test accuracy: 39.74
Round  91, Train loss: 0.598, Test loss: 3.646, Test accuracy: 32.46
Round  91, Global train loss: 0.598, Global test loss: 2.010, Global test accuracy: 25.88
Round  92, Train loss: 0.536, Test loss: 3.691, Test accuracy: 32.82
Round  92, Global train loss: 0.536, Global test loss: 1.767, Global test accuracy: 37.97
Round  93, Train loss: 0.654, Test loss: 3.742, Test accuracy: 32.64
Round  93, Global train loss: 0.654, Global test loss: 1.838, Global test accuracy: 35.37
Round  94, Train loss: 0.577, Test loss: 3.794, Test accuracy: 32.60
Round  94, Global train loss: 0.577, Global test loss: 1.828, Global test accuracy: 33.98
Round  95, Train loss: 0.709, Test loss: 3.809, Test accuracy: 32.64
Round  95, Global train loss: 0.709, Global test loss: 1.976, Global test accuracy: 32.31
Round  96, Train loss: 0.476, Test loss: 3.862, Test accuracy: 32.08
Round  96, Global train loss: 0.476, Global test loss: 1.896, Global test accuracy: 30.31
Round  97, Train loss: 0.657, Test loss: 3.860, Test accuracy: 32.13
Round  97, Global train loss: 0.657, Global test loss: 2.030, Global test accuracy: 24.88
Round  98, Train loss: 0.668, Test loss: 3.869, Test accuracy: 32.03
Round  98, Global train loss: 0.668, Global test loss: 1.962, Global test accuracy: 31.24
Round  99, Train loss: 0.549, Test loss: 3.866, Test accuracy: 32.20
Round  99, Global train loss: 0.549, Global test loss: 1.902, Global test accuracy: 30.89
Final Round, Train loss: 0.357, Test loss: 4.479, Test accuracy: 32.58
Final Round, Global train loss: 0.357, Global test loss: 1.902, Global test accuracy: 30.89
Average accuracy final 10 rounds: 32.36 

Average global accuracy final 10 rounds: 32.25575 

5661.037020206451
[4.65700364112854, 9.31400728225708, 13.620152235031128, 17.926297187805176, 22.19723081588745, 26.468164443969727, 30.799187421798706, 35.130210399627686, 39.455005168914795, 43.779799938201904, 48.09658622741699, 52.41337251663208, 56.731818199157715, 61.05026388168335, 65.34596395492554, 69.64166402816772, 73.96628522872925, 78.29090642929077, 82.59013032913208, 86.88935422897339, 91.20193648338318, 95.51451873779297, 99.86207723617554, 104.2096357345581, 108.54782152175903, 112.88600730895996, 117.21862292289734, 121.55123853683472, 125.82892179489136, 130.106605052948, 134.4471938610077, 138.78778266906738, 143.1021921634674, 147.41660165786743, 151.70948314666748, 156.00236463546753, 160.27702474594116, 164.5516848564148, 168.83651399612427, 173.12134313583374, 177.43026089668274, 181.73917865753174, 186.03571701049805, 190.33225536346436, 194.6094024181366, 198.88654947280884, 203.16752314567566, 207.44849681854248, 211.7121422290802, 215.97578763961792, 220.2346260547638, 224.49346446990967, 228.76745700836182, 233.04144954681396, 237.30797696113586, 241.57450437545776, 245.83094358444214, 250.0873827934265, 254.34958863258362, 258.6117944717407, 262.8813955783844, 267.1509966850281, 271.43047285079956, 275.70994901657104, 279.97266817092896, 284.23538732528687, 288.52044463157654, 292.8055019378662, 297.1082422733307, 301.41098260879517, 305.8769829273224, 310.3429832458496, 314.78768515586853, 319.23238706588745, 323.5128128528595, 327.79323863983154, 332.1058442592621, 336.4184498786926, 340.68525862693787, 344.9520673751831, 349.2089955806732, 353.46592378616333, 357.73075556755066, 361.995587348938, 366.27235436439514, 370.5491213798523, 374.8088319301605, 379.06854248046875, 383.35072898864746, 387.6329154968262, 391.89969539642334, 396.1664752960205, 400.478476524353, 404.79047775268555, 409.0692346096039, 413.3479914665222, 417.6786000728607, 422.0092086791992, 426.31146574020386, 430.6137228012085, 434.88768577575684, 439.1616487503052, 443.4320456981659, 447.7024426460266, 452.033221244812, 456.3639998435974, 460.70357036590576, 465.0431408882141, 469.44908833503723, 473.85503578186035, 478.11176109313965, 482.36848640441895, 486.64625239372253, 490.9240183830261, 495.17971873283386, 499.4354190826416, 503.68391942977905, 507.9324197769165, 512.2317044734955, 516.5309891700745, 520.835942029953, 525.1408948898315, 529.4881064891815, 533.8353180885315, 538.1009585857391, 542.3665990829468, 546.671716928482, 550.9768347740173, 555.3070893287659, 559.6373438835144, 563.9290452003479, 568.2207465171814, 572.5001270771027, 576.7795076370239, 581.0755269527435, 585.3715462684631, 589.6609902381897, 593.9504342079163, 598.2422161102295, 602.5339980125427, 606.8343162536621, 611.1346344947815, 615.435067653656, 619.7355008125305, 624.030647277832, 628.3257937431335, 632.6279947757721, 636.9301958084106, 641.2121901512146, 645.4941844940186, 649.7625303268433, 654.030876159668, 658.329626083374, 662.6283760070801, 666.9132468700409, 671.1981177330017, 675.5099017620087, 679.8216857910156, 684.1344730854034, 688.4472603797913, 692.7154273986816, 696.983594417572, 701.307612657547, 705.631630897522, 709.9234097003937, 714.2151885032654, 718.5143527984619, 722.8135170936584, 727.1036489009857, 731.393780708313, 735.6936745643616, 739.9935684204102, 744.2690443992615, 748.5445203781128, 752.8384182453156, 757.1323161125183, 761.4044442176819, 765.6765723228455, 769.9746108055115, 774.2726492881775, 778.5578994750977, 782.8431496620178, 787.1664912700653, 791.4898328781128, 795.7533938884735, 800.0169548988342, 804.2946317195892, 808.5723085403442, 812.8437035083771, 817.1150984764099, 821.424453496933, 825.733808517456, 830.0457026958466, 834.3575968742371, 838.6489856243134, 842.9403743743896, 847.2065467834473, 851.4727191925049, 855.7451705932617, 860.0176219940186, 862.1564166545868, 864.295211315155]
[25.175, 25.175, 33.5525, 33.5525, 34.1525, 34.1525, 34.6125, 34.6125, 35.48, 35.48, 35.975, 35.975, 36.83, 36.83, 36.6875, 36.6875, 36.8075, 36.8075, 37.625, 37.625, 37.1225, 37.1225, 37.9825, 37.9825, 37.8025, 37.8025, 38.27, 38.27, 38.0875, 38.0875, 38.085, 38.085, 37.6875, 37.6875, 38.2525, 38.2525, 38.355, 38.355, 38.3125, 38.3125, 38.465, 38.465, 38.005, 38.005, 37.95, 37.95, 38.185, 38.185, 38.1225, 38.1225, 38.265, 38.265, 38.1575, 38.1575, 37.785, 37.785, 37.34, 37.34, 37.3025, 37.3025, 36.4875, 36.4875, 36.4925, 36.4925, 36.3875, 36.3875, 36.3675, 36.3675, 35.9225, 35.9225, 35.9375, 35.9375, 35.48, 35.48, 35.6625, 35.6625, 35.2325, 35.2325, 35.2525, 35.2525, 34.9275, 34.9275, 34.7025, 34.7025, 34.5475, 34.5475, 34.5825, 34.5825, 34.685, 34.685, 34.8125, 34.8125, 34.765, 34.765, 34.6325, 34.6325, 34.27, 34.27, 34.19, 34.19, 33.9725, 33.9725, 33.68, 33.68, 33.83, 33.83, 33.72, 33.72, 33.4175, 33.4175, 33.73, 33.73, 33.615, 33.615, 33.14, 33.14, 33.345, 33.345, 33.49, 33.49, 33.185, 33.185, 33.225, 33.225, 33.1575, 33.1575, 33.0575, 33.0575, 32.845, 32.845, 32.7675, 32.7675, 32.4525, 32.4525, 32.81, 32.81, 32.95, 32.95, 32.9375, 32.9375, 33.1025, 33.1025, 32.9175, 32.9175, 33.1125, 33.1125, 32.6625, 32.6625, 32.4225, 32.4225, 32.5475, 32.5475, 32.275, 32.275, 32.4075, 32.4075, 32.6475, 32.6475, 32.415, 32.415, 32.53, 32.53, 32.58, 32.58, 32.6475, 32.6475, 32.695, 32.695, 32.3925, 32.3925, 32.385, 32.385, 32.1975, 32.1975, 32.1775, 32.1775, 32.0075, 32.0075, 32.0125, 32.0125, 32.0, 32.0, 32.4575, 32.4575, 32.8225, 32.8225, 32.64, 32.64, 32.605, 32.605, 32.6375, 32.6375, 32.0825, 32.0825, 32.1275, 32.1275, 32.0325, 32.0325, 32.195, 32.195, 32.58, 32.58]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 3, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.085, Test loss: 1.898, Test accuracy: 34.21
Round   0, Global train loss: 2.085, Global test loss: 1.899, Global test accuracy: 35.41
Round   1, Train loss: 1.913, Test loss: 1.739, Test accuracy: 39.03
Round   1, Global train loss: 1.913, Global test loss: 1.638, Global test accuracy: 44.41
Round   2, Train loss: 1.821, Test loss: 1.692, Test accuracy: 40.24
Round   2, Global train loss: 1.821, Global test loss: 1.541, Global test accuracy: 47.43
Round   3, Train loss: 1.721, Test loss: 1.635, Test accuracy: 42.09
Round   3, Global train loss: 1.721, Global test loss: 1.431, Global test accuracy: 51.61
Round   4, Train loss: 1.668, Test loss: 1.593, Test accuracy: 43.88
Round   4, Global train loss: 1.668, Global test loss: 1.376, Global test accuracy: 53.88
Round   5, Train loss: 1.631, Test loss: 1.548, Test accuracy: 45.88
Round   5, Global train loss: 1.631, Global test loss: 1.297, Global test accuracy: 57.06
Round   6, Train loss: 1.609, Test loss: 1.523, Test accuracy: 46.87
Round   6, Global train loss: 1.609, Global test loss: 1.264, Global test accuracy: 58.64
Round   7, Train loss: 1.553, Test loss: 1.493, Test accuracy: 48.26
Round   7, Global train loss: 1.553, Global test loss: 1.211, Global test accuracy: 60.83
Round   8, Train loss: 1.566, Test loss: 1.433, Test accuracy: 50.95
Round   8, Global train loss: 1.566, Global test loss: 1.229, Global test accuracy: 62.40
Round   9, Train loss: 1.530, Test loss: 1.422, Test accuracy: 51.61
Round   9, Global train loss: 1.530, Global test loss: 1.201, Global test accuracy: 63.22
Round  10, Train loss: 1.464, Test loss: 1.382, Test accuracy: 53.41
Round  10, Global train loss: 1.464, Global test loss: 1.139, Global test accuracy: 64.14
Round  11, Train loss: 1.434, Test loss: 1.364, Test accuracy: 54.08
Round  11, Global train loss: 1.434, Global test loss: 1.112, Global test accuracy: 64.62
Round  12, Train loss: 1.496, Test loss: 1.358, Test accuracy: 54.60
Round  12, Global train loss: 1.496, Global test loss: 1.171, Global test accuracy: 64.82
Round  13, Train loss: 1.431, Test loss: 1.343, Test accuracy: 55.45
Round  13, Global train loss: 1.431, Global test loss: 1.080, Global test accuracy: 65.79
Round  14, Train loss: 1.330, Test loss: 1.325, Test accuracy: 56.19
Round  14, Global train loss: 1.330, Global test loss: 1.062, Global test accuracy: 66.11
Round  15, Train loss: 1.378, Test loss: 1.316, Test accuracy: 56.52
Round  15, Global train loss: 1.378, Global test loss: 1.068, Global test accuracy: 66.69
Round  16, Train loss: 1.387, Test loss: 1.272, Test accuracy: 58.68
Round  16, Global train loss: 1.387, Global test loss: 1.032, Global test accuracy: 67.78
Round  17, Train loss: 1.411, Test loss: 1.276, Test accuracy: 58.52
Round  17, Global train loss: 1.411, Global test loss: 1.091, Global test accuracy: 66.47
Round  18, Train loss: 1.268, Test loss: 1.268, Test accuracy: 58.87
Round  18, Global train loss: 1.268, Global test loss: 1.016, Global test accuracy: 68.18
Round  19, Train loss: 1.218, Test loss: 1.269, Test accuracy: 59.02
Round  19, Global train loss: 1.218, Global test loss: 0.967, Global test accuracy: 68.34
Round  20, Train loss: 1.306, Test loss: 1.258, Test accuracy: 59.38
Round  20, Global train loss: 1.306, Global test loss: 1.029, Global test accuracy: 68.10
Round  21, Train loss: 1.223, Test loss: 1.253, Test accuracy: 59.39
Round  21, Global train loss: 1.223, Global test loss: 0.953, Global test accuracy: 69.59
Round  22, Train loss: 1.252, Test loss: 1.255, Test accuracy: 59.38
Round  22, Global train loss: 1.252, Global test loss: 0.971, Global test accuracy: 69.06
Round  23, Train loss: 1.279, Test loss: 1.254, Test accuracy: 59.51
Round  23, Global train loss: 1.279, Global test loss: 0.988, Global test accuracy: 69.46
Round  24, Train loss: 1.326, Test loss: 1.248, Test accuracy: 60.21
Round  24, Global train loss: 1.326, Global test loss: 1.002, Global test accuracy: 70.48
Round  25, Train loss: 1.281, Test loss: 1.250, Test accuracy: 60.26
Round  25, Global train loss: 1.281, Global test loss: 0.991, Global test accuracy: 70.00
Round  26, Train loss: 1.226, Test loss: 1.243, Test accuracy: 60.47
Round  26, Global train loss: 1.226, Global test loss: 0.963, Global test accuracy: 70.27
Round  27, Train loss: 1.190, Test loss: 1.237, Test accuracy: 60.65
Round  27, Global train loss: 1.190, Global test loss: 0.946, Global test accuracy: 70.46
Round  28, Train loss: 1.122, Test loss: 1.224, Test accuracy: 61.24
Round  28, Global train loss: 1.122, Global test loss: 0.893, Global test accuracy: 71.11
Round  29, Train loss: 1.152, Test loss: 1.220, Test accuracy: 61.10
Round  29, Global train loss: 1.152, Global test loss: 0.915, Global test accuracy: 70.73
Round  30, Train loss: 1.108, Test loss: 1.225, Test accuracy: 60.95
Round  30, Global train loss: 1.108, Global test loss: 0.895, Global test accuracy: 71.42
Round  31, Train loss: 1.200, Test loss: 1.212, Test accuracy: 61.48
Round  31, Global train loss: 1.200, Global test loss: 0.933, Global test accuracy: 70.85
Round  32, Train loss: 1.142, Test loss: 1.207, Test accuracy: 61.95
Round  32, Global train loss: 1.142, Global test loss: 0.894, Global test accuracy: 71.53
Round  33, Train loss: 1.080, Test loss: 1.204, Test accuracy: 62.18
Round  33, Global train loss: 1.080, Global test loss: 0.896, Global test accuracy: 71.30
Round  34, Train loss: 1.206, Test loss: 1.193, Test accuracy: 62.46
Round  34, Global train loss: 1.206, Global test loss: 0.956, Global test accuracy: 70.63
Round  35, Train loss: 1.154, Test loss: 1.191, Test accuracy: 62.50
Round  35, Global train loss: 1.154, Global test loss: 0.914, Global test accuracy: 71.68
Round  36, Train loss: 1.105, Test loss: 1.199, Test accuracy: 62.14
Round  36, Global train loss: 1.105, Global test loss: 0.919, Global test accuracy: 71.27
Round  37, Train loss: 1.070, Test loss: 1.196, Test accuracy: 62.28
Round  37, Global train loss: 1.070, Global test loss: 0.880, Global test accuracy: 71.78
Round  38, Train loss: 1.116, Test loss: 1.187, Test accuracy: 62.65
Round  38, Global train loss: 1.116, Global test loss: 0.922, Global test accuracy: 71.23
Round  39, Train loss: 1.028, Test loss: 1.185, Test accuracy: 62.73
Round  39, Global train loss: 1.028, Global test loss: 0.866, Global test accuracy: 71.92
Round  40, Train loss: 1.035, Test loss: 1.181, Test accuracy: 62.79
Round  40, Global train loss: 1.035, Global test loss: 0.866, Global test accuracy: 72.73
Round  41, Train loss: 1.031, Test loss: 1.194, Test accuracy: 62.47
Round  41, Global train loss: 1.031, Global test loss: 0.879, Global test accuracy: 72.54
Round  42, Train loss: 1.084, Test loss: 1.180, Test accuracy: 62.99
Round  42, Global train loss: 1.084, Global test loss: 0.911, Global test accuracy: 71.33
Round  43, Train loss: 1.077, Test loss: 1.192, Test accuracy: 62.77
Round  43, Global train loss: 1.077, Global test loss: 0.917, Global test accuracy: 71.58
Round  44, Train loss: 1.034, Test loss: 1.185, Test accuracy: 62.79
Round  44, Global train loss: 1.034, Global test loss: 0.941, Global test accuracy: 70.37
Round  45, Train loss: 1.116, Test loss: 1.192, Test accuracy: 62.90
Round  45, Global train loss: 1.116, Global test loss: 0.940, Global test accuracy: 71.17
Round  46, Train loss: 1.053, Test loss: 1.193, Test accuracy: 62.76
Round  46, Global train loss: 1.053, Global test loss: 0.886, Global test accuracy: 71.67
Round  47, Train loss: 0.945, Test loss: 1.180, Test accuracy: 63.05
Round  47, Global train loss: 0.945, Global test loss: 0.829, Global test accuracy: 72.68
Round  48, Train loss: 1.006, Test loss: 1.184, Test accuracy: 62.97
Round  48, Global train loss: 1.006, Global test loss: 0.874, Global test accuracy: 72.03
Round  49, Train loss: 1.094, Test loss: 1.200, Test accuracy: 62.57
Round  49, Global train loss: 1.094, Global test loss: 0.920, Global test accuracy: 71.88
Round  50, Train loss: 0.999, Test loss: 1.206, Test accuracy: 62.27
Round  50, Global train loss: 0.999, Global test loss: 0.885, Global test accuracy: 71.91
Round  51, Train loss: 1.001, Test loss: 1.213, Test accuracy: 62.23
Round  51, Global train loss: 1.001, Global test loss: 0.887, Global test accuracy: 71.50
Round  52, Train loss: 0.971, Test loss: 1.209, Test accuracy: 62.57
Round  52, Global train loss: 0.971, Global test loss: 0.883, Global test accuracy: 72.15
Round  53, Train loss: 1.035, Test loss: 1.213, Test accuracy: 62.49
Round  53, Global train loss: 1.035, Global test loss: 0.896, Global test accuracy: 71.48
Round  54, Train loss: 1.009, Test loss: 1.207, Test accuracy: 62.73
Round  54, Global train loss: 1.009, Global test loss: 0.885, Global test accuracy: 72.27
Round  55, Train loss: 1.114, Test loss: 1.205, Test accuracy: 62.78
Round  55, Global train loss: 1.114, Global test loss: 0.900, Global test accuracy: 72.72
Round  56, Train loss: 0.954, Test loss: 1.198, Test accuracy: 63.04
Round  56, Global train loss: 0.954, Global test loss: 0.849, Global test accuracy: 72.73
Round  57, Train loss: 0.980, Test loss: 1.209, Test accuracy: 62.86
Round  57, Global train loss: 0.980, Global test loss: 0.887, Global test accuracy: 71.98
Round  58, Train loss: 1.024, Test loss: 1.216, Test accuracy: 62.56
Round  58, Global train loss: 1.024, Global test loss: 0.916, Global test accuracy: 71.56
Round  59, Train loss: 0.894, Test loss: 1.228, Test accuracy: 62.37
Round  59, Global train loss: 0.894, Global test loss: 0.875, Global test accuracy: 72.02
Round  60, Train loss: 0.988, Test loss: 1.237, Test accuracy: 62.07
Round  60, Global train loss: 0.988, Global test loss: 0.902, Global test accuracy: 71.81
Round  61, Train loss: 1.013, Test loss: 1.241, Test accuracy: 61.81
Round  61, Global train loss: 1.013, Global test loss: 0.919, Global test accuracy: 71.19
Round  62, Train loss: 1.073, Test loss: 1.247, Test accuracy: 61.80
Round  62, Global train loss: 1.073, Global test loss: 0.910, Global test accuracy: 72.02
Round  63, Train loss: 0.926, Test loss: 1.249, Test accuracy: 61.83
Round  63, Global train loss: 0.926, Global test loss: 0.869, Global test accuracy: 72.34
Round  64, Train loss: 0.965, Test loss: 1.256, Test accuracy: 61.70
Round  64, Global train loss: 0.965, Global test loss: 0.923, Global test accuracy: 71.56
Round  65, Train loss: 0.936, Test loss: 1.264, Test accuracy: 61.81
Round  65, Global train loss: 0.936, Global test loss: 0.873, Global test accuracy: 72.84
Round  66, Train loss: 0.994, Test loss: 1.251, Test accuracy: 62.20
Round  66, Global train loss: 0.994, Global test loss: 0.918, Global test accuracy: 71.53
Round  67, Train loss: 0.979, Test loss: 1.241, Test accuracy: 62.26
Round  67, Global train loss: 0.979, Global test loss: 0.923, Global test accuracy: 70.97
Round  68, Train loss: 0.854, Test loss: 1.247, Test accuracy: 62.19
Round  68, Global train loss: 0.854, Global test loss: 0.893, Global test accuracy: 71.98
Round  69, Train loss: 0.918, Test loss: 1.243, Test accuracy: 62.44
Round  69, Global train loss: 0.918, Global test loss: 0.894, Global test accuracy: 72.18
Round  70, Train loss: 0.859, Test loss: 1.256, Test accuracy: 61.87
Round  70, Global train loss: 0.859, Global test loss: 0.897, Global test accuracy: 71.81
Round  71, Train loss: 0.882, Test loss: 1.250, Test accuracy: 62.22
Round  71, Global train loss: 0.882, Global test loss: 0.895, Global test accuracy: 71.63
Round  72, Train loss: 0.842, Test loss: 1.246, Test accuracy: 62.51
Round  72, Global train loss: 0.842, Global test loss: 0.881, Global test accuracy: 72.03
Round  73, Train loss: 0.913, Test loss: 1.261, Test accuracy: 62.17
Round  73, Global train loss: 0.913, Global test loss: 0.882, Global test accuracy: 71.92
Round  74, Train loss: 0.984, Test loss: 1.273, Test accuracy: 61.84
Round  74, Global train loss: 0.984, Global test loss: 0.930, Global test accuracy: 70.58
Round  75, Train loss: 0.944, Test loss: 1.266, Test accuracy: 61.94
Round  75, Global train loss: 0.944, Global test loss: 0.942, Global test accuracy: 70.59
Round  76, Train loss: 0.834, Test loss: 1.269, Test accuracy: 62.03
Round  76, Global train loss: 0.834, Global test loss: 0.910, Global test accuracy: 71.75
Round  77, Train loss: 0.855, Test loss: 1.266, Test accuracy: 62.32
Round  77, Global train loss: 0.855, Global test loss: 0.899, Global test accuracy: 71.73
Round  78, Train loss: 0.940, Test loss: 1.273, Test accuracy: 62.08
Round  78, Global train loss: 0.940, Global test loss: 0.933, Global test accuracy: 71.03
Round  79, Train loss: 0.964, Test loss: 1.279, Test accuracy: 61.97
Round  79, Global train loss: 0.964, Global test loss: 0.911, Global test accuracy: 71.53
Round  80, Train loss: 0.898, Test loss: 1.269, Test accuracy: 62.13
Round  80, Global train loss: 0.898, Global test loss: 0.908, Global test accuracy: 71.75
Round  81, Train loss: 0.984, Test loss: 1.274, Test accuracy: 62.10
Round  81, Global train loss: 0.984, Global test loss: 0.963, Global test accuracy: 69.97
Round  82, Train loss: 0.912, Test loss: 1.288, Test accuracy: 62.00
Round  82, Global train loss: 0.912, Global test loss: 0.883, Global test accuracy: 71.66
Round  83, Train loss: 0.960, Test loss: 1.277, Test accuracy: 62.30
Round  83, Global train loss: 0.960, Global test loss: 0.939, Global test accuracy: 70.97
Round  84, Train loss: 0.853, Test loss: 1.281, Test accuracy: 62.08
Round  84, Global train loss: 0.853, Global test loss: 0.907, Global test accuracy: 71.80
Round  85, Train loss: 0.827, Test loss: 1.289, Test accuracy: 62.02
Round  85, Global train loss: 0.827, Global test loss: 0.891, Global test accuracy: 72.01
Round  86, Train loss: 0.922, Test loss: 1.300, Test accuracy: 61.72
Round  86, Global train loss: 0.922, Global test loss: 0.905, Global test accuracy: 71.57
Round  87, Train loss: 0.817, Test loss: 1.308, Test accuracy: 61.81
Round  87, Global train loss: 0.817, Global test loss: 0.938, Global test accuracy: 70.94
Round  88, Train loss: 0.933, Test loss: 1.309, Test accuracy: 61.95
Round  88, Global train loss: 0.933, Global test loss: 0.914, Global test accuracy: 71.63
Round  89, Train loss: 0.936, Test loss: 1.312, Test accuracy: 62.01
Round  89, Global train loss: 0.936, Global test loss: 0.945, Global test accuracy: 71.19
Round  90, Train loss: 0.838, Test loss: 1.304, Test accuracy: 61.98
Round  90, Global train loss: 0.838, Global test loss: 0.914, Global test accuracy: 71.30
Round  91, Train loss: 0.818, Test loss: 1.305, Test accuracy: 61.83
Round  91, Global train loss: 0.818, Global test loss: 0.897, Global test accuracy: 71.72
Round  92, Train loss: 0.804, Test loss: 1.297, Test accuracy: 62.15
Round  92, Global train loss: 0.804, Global test loss: 0.886, Global test accuracy: 72.25
Round  93, Train loss: 0.873, Test loss: 1.305, Test accuracy: 61.95
Round  93, Global train loss: 0.873, Global test loss: 0.889, Global test accuracy: 72.02
Round  94, Train loss: 0.863, Test loss: 1.298, Test accuracy: 62.05
Round  94, Global train loss: 0.863, Global test loss: 0.878, Global test accuracy: 72.46
Round  95, Train loss: 0.837, Test loss: 1.291, Test accuracy: 62.32
Round  95, Global train loss: 0.837, Global test loss: 0.912, Global test accuracy: 71.82
Round  96, Train loss: 0.923, Test loss: 1.302, Test accuracy: 62.03
Round  96, Global train loss: 0.923, Global test loss: 0.929, Global test accuracy: 71.37
Round  97, Train loss: 0.914, Test loss: 1.284, Test accuracy: 62.61
Round  97, Global train loss: 0.914, Global test loss: 0.922, Global test accuracy: 72.09
Round  98, Train loss: 0.855, Test loss: 1.291, Test accuracy: 62.45
Round  98, Global train loss: 0.855, Global test loss: 0.899, Global test accuracy: 72.56
Round  99, Train loss: 0.835, Test loss: 1.281, Test accuracy: 62.73
Round  99, Global train loss: 0.835, Global test loss: 0.902, Global test accuracy: 72.38
Final Round, Train loss: 0.563, Test loss: 1.478, Test accuracy: 61.82
Final Round, Global train loss: 0.563, Global test loss: 0.902, Global test accuracy: 72.38
Average accuracy final 10 rounds: 62.20975 

Average global accuracy final 10 rounds: 71.99725000000001 

5721.992073774338
[4.901287078857422, 9.802574157714844, 14.492152214050293, 19.181730270385742, 23.967307090759277, 28.752883911132812, 33.09008765220642, 37.42729139328003, 41.71355581283569, 45.99982023239136, 50.31642389297485, 54.63302755355835, 58.95416331291199, 63.275299072265625, 67.63615942001343, 71.99701976776123, 76.27513003349304, 80.55324029922485, 84.82381629943848, 89.0943922996521, 93.36011934280396, 97.62584638595581, 101.90291547775269, 106.17998456954956, 110.46571135520935, 114.75143814086914, 119.0518684387207, 123.35229873657227, 127.66430306434631, 131.97630739212036, 136.2666802406311, 140.55705308914185, 144.8493947982788, 149.14173650741577, 153.44631910324097, 157.75090169906616, 162.07575178146362, 166.40060186386108, 170.68525409698486, 174.96990633010864, 179.3333592414856, 183.69681215286255, 188.01203799247742, 192.32726383209229, 196.62052536010742, 200.91378688812256, 205.27657270431519, 209.6393585205078, 213.9487452507019, 218.258131980896, 222.52335405349731, 226.78857612609863, 231.06755828857422, 235.3465404510498, 239.63235712051392, 243.91817378997803, 248.19616985321045, 252.47416591644287, 256.77129101753235, 261.0684161186218, 265.75933027267456, 270.4502444267273, 275.2239248752594, 279.9976053237915, 284.7900264263153, 289.5824475288391, 294.2420582771301, 298.90166902542114, 303.6335005760193, 308.36533212661743, 313.24045848846436, 318.1155848503113, 322.86837363243103, 327.6211624145508, 332.502210855484, 337.38325929641724, 341.6686029434204, 345.9539465904236, 350.2221806049347, 354.4904146194458, 358.78431701660156, 363.0782194137573, 367.34490275382996, 371.6115860939026, 375.8948838710785, 380.1781816482544, 384.4501016139984, 388.72202157974243, 393.1139397621155, 397.5058579444885, 401.83179450035095, 406.1577310562134, 410.47009348869324, 414.7824559211731, 419.07876801490784, 423.3750801086426, 427.68366718292236, 431.99225425720215, 436.3870565891266, 440.781858921051, 445.09500527381897, 449.4081516265869, 453.7234263420105, 458.0387010574341, 462.3627154827118, 466.6867299079895, 471.00262546539307, 475.31852102279663, 479.60744738578796, 483.8963737487793, 488.2592098712921, 492.62204599380493, 496.9729645252228, 501.3238830566406, 505.6783995628357, 510.03291606903076, 514.437620639801, 518.8423252105713, 523.0927958488464, 527.3432664871216, 531.5954339504242, 535.8476014137268, 540.1023647785187, 544.3571281433105, 548.6472792625427, 552.9374303817749, 557.2364585399628, 561.5354866981506, 565.8078100681305, 570.0801334381104, 574.3960552215576, 578.7119770050049, 582.9959583282471, 587.2799396514893, 591.5944311618805, 595.9089226722717, 600.1979792118073, 604.4870357513428, 608.7807531356812, 613.0744705200195, 617.3874959945679, 621.7005214691162, 626.0062561035156, 630.311990737915, 634.5907974243164, 638.8696041107178, 643.1408596038818, 647.4121150970459, 651.7020182609558, 655.9919214248657, 660.2965979576111, 664.6012744903564, 668.873884677887, 673.1464948654175, 677.4202077388763, 681.6939206123352, 685.9593508243561, 690.224781036377, 694.5142242908478, 698.8036675453186, 703.0846862792969, 707.3657050132751, 711.7979657649994, 716.2302265167236, 720.6484184265137, 725.0666103363037, 729.6327381134033, 734.1988658905029, 738.6363832950592, 743.0739006996155, 747.4836585521698, 751.8934164047241, 756.2086946964264, 760.5239729881287, 765.0305349826813, 769.5370969772339, 773.8646969795227, 778.1922969818115, 782.5024893283844, 786.8126816749573, 791.0800609588623, 795.3474402427673, 799.6628737449646, 803.9783072471619, 808.2496385574341, 812.5209698677063, 816.7965183258057, 821.072066783905, 825.3596880435944, 829.6473093032837, 833.9158327579498, 838.184356212616, 842.777708530426, 847.3710608482361, 851.6809177398682, 855.9907746315002, 860.3602628707886, 864.7297511100769, 869.3016622066498, 873.8735733032227, 876.1928699016571, 878.5121665000916]
[34.2075, 34.2075, 39.0275, 39.0275, 40.24, 40.24, 42.0925, 42.0925, 43.8775, 43.8775, 45.88, 45.88, 46.865, 46.865, 48.255, 48.255, 50.955, 50.955, 51.6125, 51.6125, 53.4125, 53.4125, 54.075, 54.075, 54.6025, 54.6025, 55.45, 55.45, 56.185, 56.185, 56.525, 56.525, 58.68, 58.68, 58.525, 58.525, 58.8675, 58.8675, 59.0175, 59.0175, 59.38, 59.38, 59.3875, 59.3875, 59.38, 59.38, 59.5125, 59.5125, 60.2075, 60.2075, 60.255, 60.255, 60.4675, 60.4675, 60.6525, 60.6525, 61.2425, 61.2425, 61.0975, 61.0975, 60.9525, 60.9525, 61.48, 61.48, 61.945, 61.945, 62.1825, 62.1825, 62.46, 62.46, 62.4975, 62.4975, 62.1375, 62.1375, 62.2825, 62.2825, 62.65, 62.65, 62.725, 62.725, 62.7875, 62.7875, 62.47, 62.47, 62.9875, 62.9875, 62.7675, 62.7675, 62.7875, 62.7875, 62.9, 62.9, 62.7575, 62.7575, 63.045, 63.045, 62.965, 62.965, 62.5725, 62.5725, 62.27, 62.27, 62.23, 62.23, 62.5675, 62.5675, 62.4875, 62.4875, 62.7275, 62.7275, 62.785, 62.785, 63.0375, 63.0375, 62.8575, 62.8575, 62.5625, 62.5625, 62.365, 62.365, 62.0675, 62.0675, 61.81, 61.81, 61.7975, 61.7975, 61.8275, 61.8275, 61.7025, 61.7025, 61.815, 61.815, 62.2, 62.2, 62.2575, 62.2575, 62.185, 62.185, 62.435, 62.435, 61.8675, 61.8675, 62.2175, 62.2175, 62.5075, 62.5075, 62.1725, 62.1725, 61.8425, 61.8425, 61.9425, 61.9425, 62.0275, 62.0275, 62.32, 62.32, 62.0775, 62.0775, 61.9675, 61.9675, 62.13, 62.13, 62.0975, 62.0975, 62.0025, 62.0025, 62.2975, 62.2975, 62.075, 62.075, 62.025, 62.025, 61.7225, 61.7225, 61.81, 61.81, 61.9525, 61.9525, 62.01, 62.01, 61.9825, 61.9825, 61.8325, 61.8325, 62.145, 62.145, 61.95, 61.95, 62.05, 62.05, 62.3175, 62.3175, 62.0275, 62.0275, 62.6075, 62.6075, 62.45, 62.45, 62.735, 62.735, 61.8225, 61.8225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 5, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.135, Test loss: 2.004, Test accuracy: 28.07
Round   0, Global train loss: 2.135, Global test loss: 2.001, Global test accuracy: 29.06
Round   1, Train loss: 1.993, Test loss: 1.876, Test accuracy: 33.44
Round   1, Global train loss: 1.993, Global test loss: 1.800, Global test accuracy: 38.10
Round   2, Train loss: 1.902, Test loss: 1.806, Test accuracy: 34.73
Round   2, Global train loss: 1.902, Global test loss: 1.664, Global test accuracy: 40.12
Round   3, Train loss: 1.849, Test loss: 1.739, Test accuracy: 37.05
Round   3, Global train loss: 1.849, Global test loss: 1.572, Global test accuracy: 44.27
Round   4, Train loss: 1.774, Test loss: 1.689, Test accuracy: 39.55
Round   4, Global train loss: 1.774, Global test loss: 1.511, Global test accuracy: 48.57
Round   5, Train loss: 1.853, Test loss: 1.660, Test accuracy: 41.20
Round   5, Global train loss: 1.853, Global test loss: 1.536, Global test accuracy: 48.20
Round   6, Train loss: 1.795, Test loss: 1.637, Test accuracy: 42.34
Round   6, Global train loss: 1.795, Global test loss: 1.479, Global test accuracy: 50.98
Round   7, Train loss: 1.733, Test loss: 1.622, Test accuracy: 42.85
Round   7, Global train loss: 1.733, Global test loss: 1.448, Global test accuracy: 50.28
Round   8, Train loss: 1.649, Test loss: 1.552, Test accuracy: 45.87
Round   8, Global train loss: 1.649, Global test loss: 1.360, Global test accuracy: 53.57
Round   9, Train loss: 1.564, Test loss: 1.545, Test accuracy: 46.02
Round   9, Global train loss: 1.564, Global test loss: 1.299, Global test accuracy: 55.42
Round  10, Train loss: 1.656, Test loss: 1.521, Test accuracy: 47.03
Round  10, Global train loss: 1.656, Global test loss: 1.313, Global test accuracy: 55.23
Round  11, Train loss: 1.644, Test loss: 1.506, Test accuracy: 47.72
Round  11, Global train loss: 1.644, Global test loss: 1.309, Global test accuracy: 57.22
Round  12, Train loss: 1.588, Test loss: 1.487, Test accuracy: 48.44
Round  12, Global train loss: 1.588, Global test loss: 1.293, Global test accuracy: 56.73
Round  13, Train loss: 1.607, Test loss: 1.460, Test accuracy: 49.67
Round  13, Global train loss: 1.607, Global test loss: 1.281, Global test accuracy: 58.81
Round  14, Train loss: 1.575, Test loss: 1.426, Test accuracy: 51.22
Round  14, Global train loss: 1.575, Global test loss: 1.245, Global test accuracy: 59.88
Round  15, Train loss: 1.514, Test loss: 1.409, Test accuracy: 51.95
Round  15, Global train loss: 1.514, Global test loss: 1.196, Global test accuracy: 61.31
Round  16, Train loss: 1.522, Test loss: 1.363, Test accuracy: 53.80
Round  16, Global train loss: 1.522, Global test loss: 1.211, Global test accuracy: 60.73
Round  17, Train loss: 1.568, Test loss: 1.360, Test accuracy: 54.32
Round  17, Global train loss: 1.568, Global test loss: 1.223, Global test accuracy: 61.84
Round  18, Train loss: 1.611, Test loss: 1.356, Test accuracy: 54.49
Round  18, Global train loss: 1.611, Global test loss: 1.238, Global test accuracy: 62.01
Round  19, Train loss: 1.523, Test loss: 1.352, Test accuracy: 54.70
Round  19, Global train loss: 1.523, Global test loss: 1.173, Global test accuracy: 63.72
Round  20, Train loss: 1.444, Test loss: 1.342, Test accuracy: 55.10
Round  20, Global train loss: 1.444, Global test loss: 1.151, Global test accuracy: 62.53
Round  21, Train loss: 1.456, Test loss: 1.334, Test accuracy: 55.44
Round  21, Global train loss: 1.456, Global test loss: 1.136, Global test accuracy: 63.84
Round  22, Train loss: 1.466, Test loss: 1.322, Test accuracy: 56.10
Round  22, Global train loss: 1.466, Global test loss: 1.141, Global test accuracy: 64.41
Round  23, Train loss: 1.441, Test loss: 1.309, Test accuracy: 56.38
Round  23, Global train loss: 1.441, Global test loss: 1.135, Global test accuracy: 63.51
Round  24, Train loss: 1.491, Test loss: 1.291, Test accuracy: 57.12
Round  24, Global train loss: 1.491, Global test loss: 1.118, Global test accuracy: 65.06
Round  25, Train loss: 1.468, Test loss: 1.287, Test accuracy: 57.04
Round  25, Global train loss: 1.468, Global test loss: 1.126, Global test accuracy: 64.03
Round  26, Train loss: 1.367, Test loss: 1.275, Test accuracy: 57.70
Round  26, Global train loss: 1.367, Global test loss: 1.077, Global test accuracy: 65.15
Round  27, Train loss: 1.426, Test loss: 1.272, Test accuracy: 58.10
Round  27, Global train loss: 1.426, Global test loss: 1.109, Global test accuracy: 65.53
Round  28, Train loss: 1.412, Test loss: 1.270, Test accuracy: 58.33
Round  28, Global train loss: 1.412, Global test loss: 1.086, Global test accuracy: 66.25
Round  29, Train loss: 1.457, Test loss: 1.263, Test accuracy: 58.65
Round  29, Global train loss: 1.457, Global test loss: 1.117, Global test accuracy: 65.48
Round  30, Train loss: 1.355, Test loss: 1.257, Test accuracy: 58.92
Round  30, Global train loss: 1.355, Global test loss: 1.067, Global test accuracy: 66.25
Round  31, Train loss: 1.368, Test loss: 1.241, Test accuracy: 59.60
Round  31, Global train loss: 1.368, Global test loss: 1.058, Global test accuracy: 67.17
Round  32, Train loss: 1.334, Test loss: 1.229, Test accuracy: 60.37
Round  32, Global train loss: 1.334, Global test loss: 1.022, Global test accuracy: 68.06
Round  33, Train loss: 1.378, Test loss: 1.224, Test accuracy: 60.59
Round  33, Global train loss: 1.378, Global test loss: 1.047, Global test accuracy: 67.92
Round  34, Train loss: 1.264, Test loss: 1.216, Test accuracy: 60.83
Round  34, Global train loss: 1.264, Global test loss: 1.003, Global test accuracy: 68.69
Round  35, Train loss: 1.317, Test loss: 1.209, Test accuracy: 61.06
Round  35, Global train loss: 1.317, Global test loss: 1.017, Global test accuracy: 67.77
Round  36, Train loss: 1.321, Test loss: 1.208, Test accuracy: 60.95
Round  36, Global train loss: 1.321, Global test loss: 1.028, Global test accuracy: 67.17
Round  37, Train loss: 1.342, Test loss: 1.195, Test accuracy: 61.60
Round  37, Global train loss: 1.342, Global test loss: 0.994, Global test accuracy: 69.27
Round  38, Train loss: 1.235, Test loss: 1.189, Test accuracy: 61.94
Round  38, Global train loss: 1.235, Global test loss: 0.976, Global test accuracy: 68.59
Round  39, Train loss: 1.279, Test loss: 1.200, Test accuracy: 61.69
Round  39, Global train loss: 1.279, Global test loss: 0.973, Global test accuracy: 69.70
Round  40, Train loss: 1.347, Test loss: 1.206, Test accuracy: 61.30
Round  40, Global train loss: 1.347, Global test loss: 1.007, Global test accuracy: 69.46
Round  41, Train loss: 1.389, Test loss: 1.197, Test accuracy: 61.68
Round  41, Global train loss: 1.389, Global test loss: 1.040, Global test accuracy: 69.52
Round  42, Train loss: 1.264, Test loss: 1.195, Test accuracy: 61.96
Round  42, Global train loss: 1.264, Global test loss: 0.996, Global test accuracy: 69.92
Round  43, Train loss: 1.227, Test loss: 1.180, Test accuracy: 62.15
Round  43, Global train loss: 1.227, Global test loss: 0.949, Global test accuracy: 70.44
Round  44, Train loss: 1.166, Test loss: 1.172, Test accuracy: 62.46
Round  44, Global train loss: 1.166, Global test loss: 0.920, Global test accuracy: 71.03
Round  45, Train loss: 1.230, Test loss: 1.166, Test accuracy: 62.72
Round  45, Global train loss: 1.230, Global test loss: 0.959, Global test accuracy: 70.18
Round  46, Train loss: 1.245, Test loss: 1.172, Test accuracy: 62.59
Round  46, Global train loss: 1.245, Global test loss: 0.958, Global test accuracy: 70.40
Round  47, Train loss: 1.286, Test loss: 1.175, Test accuracy: 62.62
Round  47, Global train loss: 1.286, Global test loss: 0.976, Global test accuracy: 70.58
Round  48, Train loss: 1.254, Test loss: 1.167, Test accuracy: 62.93
Round  48, Global train loss: 1.254, Global test loss: 0.962, Global test accuracy: 70.56
Round  49, Train loss: 1.299, Test loss: 1.167, Test accuracy: 62.81
Round  49, Global train loss: 1.299, Global test loss: 0.981, Global test accuracy: 69.83
Round  50, Train loss: 1.296, Test loss: 1.175, Test accuracy: 62.38
Round  50, Global train loss: 1.296, Global test loss: 1.006, Global test accuracy: 69.95
Round  51, Train loss: 1.182, Test loss: 1.174, Test accuracy: 62.67
Round  51, Global train loss: 1.182, Global test loss: 0.952, Global test accuracy: 70.42
Round  52, Train loss: 1.205, Test loss: 1.186, Test accuracy: 62.40
Round  52, Global train loss: 1.205, Global test loss: 0.958, Global test accuracy: 71.14
Round  53, Train loss: 1.208, Test loss: 1.167, Test accuracy: 62.77
Round  53, Global train loss: 1.208, Global test loss: 0.934, Global test accuracy: 70.81
Round  54, Train loss: 1.262, Test loss: 1.156, Test accuracy: 63.25
Round  54, Global train loss: 1.262, Global test loss: 0.963, Global test accuracy: 71.58
Round  55, Train loss: 1.198, Test loss: 1.137, Test accuracy: 63.98
Round  55, Global train loss: 1.198, Global test loss: 0.912, Global test accuracy: 72.19
Round  56, Train loss: 1.191, Test loss: 1.132, Test accuracy: 63.95
Round  56, Global train loss: 1.191, Global test loss: 0.926, Global test accuracy: 71.47
Round  57, Train loss: 1.265, Test loss: 1.129, Test accuracy: 64.05
Round  57, Global train loss: 1.265, Global test loss: 0.959, Global test accuracy: 71.39
Round  58, Train loss: 1.198, Test loss: 1.140, Test accuracy: 63.66
Round  58, Global train loss: 1.198, Global test loss: 0.946, Global test accuracy: 71.64
Round  59, Train loss: 1.128, Test loss: 1.144, Test accuracy: 63.72
Round  59, Global train loss: 1.128, Global test loss: 0.952, Global test accuracy: 70.43
Round  60, Train loss: 1.187, Test loss: 1.139, Test accuracy: 63.84
Round  60, Global train loss: 1.187, Global test loss: 0.959, Global test accuracy: 70.94
Round  61, Train loss: 1.249, Test loss: 1.147, Test accuracy: 63.63
Round  61, Global train loss: 1.249, Global test loss: 0.959, Global test accuracy: 71.48
Round  62, Train loss: 1.234, Test loss: 1.140, Test accuracy: 63.94
Round  62, Global train loss: 1.234, Global test loss: 0.944, Global test accuracy: 71.50
Round  63, Train loss: 1.119, Test loss: 1.135, Test accuracy: 64.03
Round  63, Global train loss: 1.119, Global test loss: 0.893, Global test accuracy: 72.58
Round  64, Train loss: 1.069, Test loss: 1.142, Test accuracy: 63.77
Round  64, Global train loss: 1.069, Global test loss: 0.873, Global test accuracy: 72.26
Round  65, Train loss: 1.108, Test loss: 1.140, Test accuracy: 63.96
Round  65, Global train loss: 1.108, Global test loss: 0.906, Global test accuracy: 72.03
Round  66, Train loss: 1.164, Test loss: 1.140, Test accuracy: 64.01
Round  66, Global train loss: 1.164, Global test loss: 0.900, Global test accuracy: 72.73
Round  67, Train loss: 1.251, Test loss: 1.135, Test accuracy: 64.34
Round  67, Global train loss: 1.251, Global test loss: 0.996, Global test accuracy: 70.25
Round  68, Train loss: 1.154, Test loss: 1.128, Test accuracy: 64.41
Round  68, Global train loss: 1.154, Global test loss: 0.913, Global test accuracy: 71.97
Round  69, Train loss: 1.179, Test loss: 1.122, Test accuracy: 64.47
Round  69, Global train loss: 1.179, Global test loss: 0.930, Global test accuracy: 71.93
Round  70, Train loss: 1.049, Test loss: 1.129, Test accuracy: 64.21
Round  70, Global train loss: 1.049, Global test loss: 0.888, Global test accuracy: 71.93
Round  71, Train loss: 1.032, Test loss: 1.133, Test accuracy: 64.26
Round  71, Global train loss: 1.032, Global test loss: 0.874, Global test accuracy: 72.73
Round  72, Train loss: 1.149, Test loss: 1.139, Test accuracy: 64.00
Round  72, Global train loss: 1.149, Global test loss: 0.927, Global test accuracy: 72.05
Round  73, Train loss: 1.159, Test loss: 1.122, Test accuracy: 64.81
Round  73, Global train loss: 1.159, Global test loss: 0.969, Global test accuracy: 71.04
Round  74, Train loss: 1.030, Test loss: 1.132, Test accuracy: 64.35
Round  74, Global train loss: 1.030, Global test loss: 0.903, Global test accuracy: 71.44
Round  75, Train loss: 1.125, Test loss: 1.127, Test accuracy: 64.69
Round  75, Global train loss: 1.125, Global test loss: 0.905, Global test accuracy: 72.17
Round  76, Train loss: 0.959, Test loss: 1.119, Test accuracy: 64.78
Round  76, Global train loss: 0.959, Global test loss: 0.848, Global test accuracy: 73.07
Round  77, Train loss: 1.116, Test loss: 1.125, Test accuracy: 64.51
Round  77, Global train loss: 1.116, Global test loss: 0.897, Global test accuracy: 71.91
Round  78, Train loss: 1.150, Test loss: 1.120, Test accuracy: 64.72
Round  78, Global train loss: 1.150, Global test loss: 0.918, Global test accuracy: 72.07
Round  79, Train loss: 1.184, Test loss: 1.132, Test accuracy: 64.12
Round  79, Global train loss: 1.184, Global test loss: 0.920, Global test accuracy: 72.03
Round  80, Train loss: 1.086, Test loss: 1.138, Test accuracy: 64.08
Round  80, Global train loss: 1.086, Global test loss: 0.915, Global test accuracy: 71.56
Round  81, Train loss: 1.044, Test loss: 1.125, Test accuracy: 64.73
Round  81, Global train loss: 1.044, Global test loss: 0.888, Global test accuracy: 71.97
Round  82, Train loss: 1.098, Test loss: 1.120, Test accuracy: 64.86
Round  82, Global train loss: 1.098, Global test loss: 0.900, Global test accuracy: 72.63
Round  83, Train loss: 1.024, Test loss: 1.125, Test accuracy: 64.86
Round  83, Global train loss: 1.024, Global test loss: 0.876, Global test accuracy: 72.43
Round  84, Train loss: 1.075, Test loss: 1.124, Test accuracy: 65.02
Round  84, Global train loss: 1.075, Global test loss: 0.883, Global test accuracy: 72.44
Round  85, Train loss: 1.135, Test loss: 1.128, Test accuracy: 64.64
Round  85, Global train loss: 1.135, Global test loss: 0.926, Global test accuracy: 71.88
Round  86, Train loss: 1.099, Test loss: 1.135, Test accuracy: 64.56
Round  86, Global train loss: 1.099, Global test loss: 0.903, Global test accuracy: 71.80
Round  87, Train loss: 1.141, Test loss: 1.132, Test accuracy: 64.50
Round  87, Global train loss: 1.141, Global test loss: 0.936, Global test accuracy: 71.58
Round  88, Train loss: 1.086, Test loss: 1.136, Test accuracy: 64.28
Round  88, Global train loss: 1.086, Global test loss: 0.893, Global test accuracy: 72.08
Round  89, Train loss: 1.016, Test loss: 1.133, Test accuracy: 64.24
Round  89, Global train loss: 1.016, Global test loss: 0.863, Global test accuracy: 72.72
Round  90, Train loss: 1.024, Test loss: 1.131, Test accuracy: 64.52
Round  90, Global train loss: 1.024, Global test loss: 0.861, Global test accuracy: 72.76
Round  91, Train loss: 1.026, Test loss: 1.130, Test accuracy: 64.64
Round  91, Global train loss: 1.026, Global test loss: 0.886, Global test accuracy: 72.26
Round  92, Train loss: 1.040, Test loss: 1.122, Test accuracy: 64.99
Round  92, Global train loss: 1.040, Global test loss: 0.864, Global test accuracy: 72.75/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  93, Train loss: 1.120, Test loss: 1.117, Test accuracy: 64.94
Round  93, Global train loss: 1.120, Global test loss: 0.895, Global test accuracy: 72.97
Round  94, Train loss: 1.145, Test loss: 1.108, Test accuracy: 65.13
Round  94, Global train loss: 1.145, Global test loss: 0.894, Global test accuracy: 72.89
Round  95, Train loss: 0.937, Test loss: 1.113, Test accuracy: 65.03
Round  95, Global train loss: 0.937, Global test loss: 0.855, Global test accuracy: 73.42
Round  96, Train loss: 1.018, Test loss: 1.105, Test accuracy: 65.36
Round  96, Global train loss: 1.018, Global test loss: 0.859, Global test accuracy: 73.23
Round  97, Train loss: 1.108, Test loss: 1.100, Test accuracy: 65.45
Round  97, Global train loss: 1.108, Global test loss: 0.889, Global test accuracy: 72.72
Round  98, Train loss: 0.987, Test loss: 1.107, Test accuracy: 65.21
Round  98, Global train loss: 0.987, Global test loss: 0.854, Global test accuracy: 73.17
Round  99, Train loss: 0.975, Test loss: 1.115, Test accuracy: 65.02
Round  99, Global train loss: 0.975, Global test loss: 0.851, Global test accuracy: 72.52
Final Round, Train loss: 0.753, Test loss: 1.249, Test accuracy: 63.12
Final Round, Global train loss: 0.753, Global test loss: 0.851, Global test accuracy: 72.52
Average accuracy final 10 rounds: 65.0295 

Average global accuracy final 10 rounds: 72.86725000000001 

6157.989908456802
[5.013404846191406, 10.026809692382812, 14.986503839492798, 19.946197986602783, 24.602174997329712, 29.25815200805664, 34.26491641998291, 39.27168083190918, 44.03577542304993, 48.799870014190674, 53.64798378944397, 58.496097564697266, 63.40117931365967, 68.30626106262207, 73.17162990570068, 78.0369987487793, 82.9842517375946, 87.93150472640991, 92.76448225975037, 97.59745979309082, 102.43839812278748, 107.27933645248413, 112.41052913665771, 117.5417218208313, 122.31710648536682, 127.09249114990234, 131.85871148109436, 136.62493181228638, 141.46568059921265, 146.30642938613892, 151.2444519996643, 156.1824746131897, 161.11992812156677, 166.05738162994385, 171.03996682167053, 176.02255201339722, 181.03495383262634, 186.04735565185547, 191.11939597129822, 196.19143629074097, 201.15488290786743, 206.1183295249939, 211.13006329536438, 216.14179706573486, 220.76461815834045, 225.38743925094604, 230.5593500137329, 235.73126077651978, 240.98117780685425, 246.23109483718872, 251.39308667182922, 256.5550785064697, 261.6580424308777, 266.76100635528564, 271.68255591392517, 276.6041054725647, 281.63335704803467, 286.66260862350464, 291.71847128868103, 296.7743339538574, 301.8672995567322, 306.96026515960693, 312.01374983787537, 317.0672345161438, 321.9186623096466, 326.7700901031494, 331.3740191459656, 335.97794818878174, 340.5810761451721, 345.1842041015625, 350.19071888923645, 355.1972336769104, 360.1464276313782, 365.09562158584595, 370.02811908721924, 374.96061658859253, 379.93337392807007, 384.9061312675476, 389.9242293834686, 394.94232749938965, 399.76943135261536, 404.59653520584106, 409.7039952278137, 414.8114552497864, 419.9956548213959, 425.17985439300537, 430.1998336315155, 435.21981287002563, 440.08972001075745, 444.95962715148926, 450.08315205574036, 455.20667695999146, 460.0636341571808, 464.9205913543701, 469.82108330726624, 474.72157526016235, 479.72405195236206, 484.72652864456177, 489.53960394859314, 494.3526792526245, 499.3002634048462, 504.24784755706787, 509.09213948249817, 513.9364314079285, 518.7723970413208, 523.6083626747131, 528.5171065330505, 533.4258503913879, 538.3485064506531, 543.2711625099182, 548.138911485672, 553.0066604614258, 557.8580622673035, 562.7094640731812, 567.3405210971832, 571.9715781211853, 576.941525220871, 581.9114723205566, 586.6251158714294, 591.3387594223022, 595.966459274292, 600.5941591262817, 605.2425725460052, 609.8909859657288, 614.5845968723297, 619.2782077789307, 623.9117319583893, 628.5452561378479, 633.1978721618652, 637.8504881858826, 642.9279320240021, 648.0053758621216, 653.0811972618103, 658.157018661499, 663.1851763725281, 668.2133340835571, 673.2430984973907, 678.2728629112244, 683.3172190189362, 688.361575126648, 693.0141975879669, 697.6668200492859, 702.3112297058105, 706.9556393623352, 711.9616649150848, 716.9676904678345, 721.8415648937225, 726.7154393196106, 731.6286025047302, 736.5417656898499, 741.5434522628784, 746.545138835907, 751.4147415161133, 756.2843441963196, 761.0519132614136, 765.8194823265076, 770.6355948448181, 775.4517073631287, 780.3242709636688, 785.196834564209, 791.0470674037933, 796.8973002433777, 802.7624380588531, 808.6275758743286, 814.486494064331, 820.3454122543335, 825.0820562839508, 829.8187003135681, 835.7526469230652, 841.6865935325623, 847.5706436634064, 853.4546937942505, 859.2506184577942, 865.0465431213379, 870.9176564216614, 876.7887697219849, 882.6273417472839, 888.465913772583, 894.4166233539581, 900.3673329353333, 906.0170855522156, 911.6668381690979, 917.4548089504242, 923.2427797317505, 929.2010746002197, 935.159369468689, 941.0320625305176, 946.9047555923462, 952.7985093593597, 958.6922631263733, 964.434323310852, 970.1763834953308, 976.0445559024811, 981.9127283096313, 987.9042737483978, 993.8958191871643, 999.6716113090515, 1005.4474034309387, 1011.1152300834656, 1016.7830567359924, 1019.6076557636261, 1022.4322547912598]
[28.075, 28.075, 33.44, 33.44, 34.73, 34.73, 37.0525, 37.0525, 39.545, 39.545, 41.195, 41.195, 42.3375, 42.3375, 42.8525, 42.8525, 45.8675, 45.8675, 46.015, 46.015, 47.0325, 47.0325, 47.72, 47.72, 48.4425, 48.4425, 49.675, 49.675, 51.215, 51.215, 51.9475, 51.9475, 53.8025, 53.8025, 54.3225, 54.3225, 54.4925, 54.4925, 54.695, 54.695, 55.1025, 55.1025, 55.4425, 55.4425, 56.0975, 56.0975, 56.3825, 56.3825, 57.115, 57.115, 57.04, 57.04, 57.7025, 57.7025, 58.105, 58.105, 58.33, 58.33, 58.65, 58.65, 58.9175, 58.9175, 59.6025, 59.6025, 60.3675, 60.3675, 60.5925, 60.5925, 60.8325, 60.8325, 61.065, 61.065, 60.945, 60.945, 61.605, 61.605, 61.94, 61.94, 61.6875, 61.6875, 61.295, 61.295, 61.6775, 61.6775, 61.96, 61.96, 62.15, 62.15, 62.46, 62.46, 62.715, 62.715, 62.585, 62.585, 62.615, 62.615, 62.93, 62.93, 62.815, 62.815, 62.38, 62.38, 62.67, 62.67, 62.395, 62.395, 62.77, 62.77, 63.2525, 63.2525, 63.9775, 63.9775, 63.95, 63.95, 64.05, 64.05, 63.66, 63.66, 63.715, 63.715, 63.835, 63.835, 63.635, 63.635, 63.94, 63.94, 64.025, 64.025, 63.7675, 63.7675, 63.9575, 63.9575, 64.0075, 64.0075, 64.345, 64.345, 64.4075, 64.4075, 64.4675, 64.4675, 64.21, 64.21, 64.26, 64.26, 64.005, 64.005, 64.815, 64.815, 64.3475, 64.3475, 64.695, 64.695, 64.7775, 64.7775, 64.51, 64.51, 64.72, 64.72, 64.1225, 64.1225, 64.0825, 64.0825, 64.73, 64.73, 64.855, 64.855, 64.86, 64.86, 65.0225, 65.0225, 64.635, 64.635, 64.56, 64.56, 64.495, 64.495, 64.2825, 64.2825, 64.2375, 64.2375, 64.5225, 64.5225, 64.64, 64.64, 64.99, 64.99, 64.9425, 64.9425, 65.13, 65.13, 65.0325, 65.0325, 65.36, 65.36, 65.4475, 65.4475, 65.21, 65.21, 65.02, 65.02, 63.115, 63.115]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 13, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.220, Test loss: 2.009, Test accuracy: 28.20
Round   1, Train loss: 2.042, Test loss: 1.864, Test accuracy: 34.10
Round   2, Train loss: 1.901, Test loss: 1.747, Test accuracy: 39.15
Round   3, Train loss: 1.824, Test loss: 1.677, Test accuracy: 41.80
Round   4, Train loss: 1.807, Test loss: 1.611, Test accuracy: 45.37
Round   5, Train loss: 1.707, Test loss: 1.537, Test accuracy: 47.11
Round   6, Train loss: 1.650, Test loss: 1.535, Test accuracy: 48.90
Round   7, Train loss: 1.621, Test loss: 1.462, Test accuracy: 50.97
Round   8, Train loss: 1.675, Test loss: 1.418, Test accuracy: 52.74
Round   9, Train loss: 1.631, Test loss: 1.408, Test accuracy: 53.77
Round  10, Train loss: 1.630, Test loss: 1.405, Test accuracy: 54.40
Round  11, Train loss: 1.529, Test loss: 1.358, Test accuracy: 56.02
Round  12, Train loss: 1.545, Test loss: 1.328, Test accuracy: 57.29
Round  13, Train loss: 1.493, Test loss: 1.289, Test accuracy: 58.77
Round  14, Train loss: 1.426, Test loss: 1.267, Test accuracy: 59.35
Round  15, Train loss: 1.544, Test loss: 1.264, Test accuracy: 60.42
Round  16, Train loss: 1.453, Test loss: 1.215, Test accuracy: 60.92
Round  17, Train loss: 1.436, Test loss: 1.189, Test accuracy: 62.05
Round  18, Train loss: 1.381, Test loss: 1.169, Test accuracy: 62.34
Round  19, Train loss: 1.376, Test loss: 1.172, Test accuracy: 63.08
Round  20, Train loss: 1.527, Test loss: 1.180, Test accuracy: 62.98
Round  21, Train loss: 1.473, Test loss: 1.158, Test accuracy: 63.95
Round  22, Train loss: 1.321, Test loss: 1.124, Test accuracy: 64.72
Round  23, Train loss: 1.462, Test loss: 1.126, Test accuracy: 64.66
Round  24, Train loss: 1.359, Test loss: 1.119, Test accuracy: 64.81
Round  25, Train loss: 1.337, Test loss: 1.128, Test accuracy: 64.98
Round  26, Train loss: 1.305, Test loss: 1.103, Test accuracy: 65.47
Round  27, Train loss: 1.351, Test loss: 1.105, Test accuracy: 65.25
Round  28, Train loss: 1.187, Test loss: 1.059, Test accuracy: 66.60
Round  29, Train loss: 1.217, Test loss: 1.069, Test accuracy: 66.39
Round  30, Train loss: 1.193, Test loss: 1.060, Test accuracy: 66.63
Round  31, Train loss: 1.389, Test loss: 1.067, Test accuracy: 66.77
Round  32, Train loss: 1.249, Test loss: 1.049, Test accuracy: 67.21
Round  33, Train loss: 1.234, Test loss: 1.049, Test accuracy: 67.32
Round  34, Train loss: 1.372, Test loss: 1.043, Test accuracy: 67.81
Round  35, Train loss: 1.292, Test loss: 1.049, Test accuracy: 67.17
Round  36, Train loss: 1.190, Test loss: 1.036, Test accuracy: 67.92
Round  37, Train loss: 1.178, Test loss: 1.024, Test accuracy: 68.43
Round  38, Train loss: 1.265, Test loss: 1.016, Test accuracy: 68.58
Round  39, Train loss: 1.145, Test loss: 1.004, Test accuracy: 68.75
Round  40, Train loss: 1.098, Test loss: 1.015, Test accuracy: 68.83
Round  41, Train loss: 1.115, Test loss: 1.014, Test accuracy: 68.25
Round  42, Train loss: 1.301, Test loss: 1.037, Test accuracy: 67.69
Round  43, Train loss: 1.184, Test loss: 1.016, Test accuracy: 68.61
Round  44, Train loss: 1.247, Test loss: 1.004, Test accuracy: 68.77
Round  45, Train loss: 1.214, Test loss: 1.003, Test accuracy: 69.07
Round  46, Train loss: 1.058, Test loss: 1.005, Test accuracy: 68.97
Round  47, Train loss: 1.024, Test loss: 0.995, Test accuracy: 69.25
Round  48, Train loss: 1.145, Test loss: 1.007, Test accuracy: 68.81
Round  49, Train loss: 1.054, Test loss: 0.994, Test accuracy: 69.53
Round  50, Train loss: 1.183, Test loss: 0.997, Test accuracy: 69.33
Round  51, Train loss: 1.135, Test loss: 0.992, Test accuracy: 69.50
Round  52, Train loss: 1.185, Test loss: 0.994, Test accuracy: 68.97
Round  53, Train loss: 1.143, Test loss: 0.984, Test accuracy: 69.61
Round  54, Train loss: 1.105, Test loss: 0.992, Test accuracy: 69.21
Round  55, Train loss: 1.199, Test loss: 0.977, Test accuracy: 69.89
Round  56, Train loss: 1.069, Test loss: 0.980, Test accuracy: 69.80
Round  57, Train loss: 1.203, Test loss: 0.980, Test accuracy: 70.15
Round  58, Train loss: 1.123, Test loss: 0.986, Test accuracy: 69.77
Round  59, Train loss: 1.049, Test loss: 0.984, Test accuracy: 69.64
Round  60, Train loss: 1.168, Test loss: 0.964, Test accuracy: 70.50
Round  61, Train loss: 1.115, Test loss: 0.967, Test accuracy: 70.34
Round  62, Train loss: 1.097, Test loss: 0.965, Test accuracy: 70.29
Round  63, Train loss: 1.046, Test loss: 0.965, Test accuracy: 70.27
Round  64, Train loss: 0.954, Test loss: 0.962, Test accuracy: 70.71
Round  65, Train loss: 1.171, Test loss: 0.971, Test accuracy: 70.37
Round  66, Train loss: 1.180, Test loss: 0.981, Test accuracy: 70.06
Round  67, Train loss: 1.025, Test loss: 0.974, Test accuracy: 70.36
Round  68, Train loss: 1.069, Test loss: 0.967, Test accuracy: 70.36
Round  69, Train loss: 1.012, Test loss: 0.978, Test accuracy: 70.04
Round  70, Train loss: 1.001, Test loss: 0.956, Test accuracy: 70.47
Round  71, Train loss: 1.065, Test loss: 0.957, Test accuracy: 70.43
Round  72, Train loss: 1.073, Test loss: 0.976, Test accuracy: 70.12
Round  73, Train loss: 1.003, Test loss: 0.971, Test accuracy: 70.33
Round  74, Train loss: 1.150, Test loss: 0.966, Test accuracy: 70.53
Round  75, Train loss: 1.106, Test loss: 0.971, Test accuracy: 70.11
Round  76, Train loss: 0.991, Test loss: 0.961, Test accuracy: 70.72
Round  77, Train loss: 0.976, Test loss: 0.973, Test accuracy: 70.00
Round  78, Train loss: 1.062, Test loss: 0.962, Test accuracy: 70.65
Round  79, Train loss: 0.955, Test loss: 0.944, Test accuracy: 71.30
Round  80, Train loss: 1.072, Test loss: 0.960, Test accuracy: 70.70
Round  81, Train loss: 1.041, Test loss: 0.960, Test accuracy: 70.80
Round  82, Train loss: 0.970, Test loss: 0.948, Test accuracy: 71.33
Round  83, Train loss: 1.057, Test loss: 0.959, Test accuracy: 70.99
Round  84, Train loss: 0.895, Test loss: 0.957, Test accuracy: 70.87
Round  85, Train loss: 0.974, Test loss: 0.959, Test accuracy: 71.11
Round  86, Train loss: 1.083, Test loss: 0.964, Test accuracy: 70.77
Round  87, Train loss: 0.860, Test loss: 0.960, Test accuracy: 70.74
Round  88, Train loss: 1.048, Test loss: 0.955, Test accuracy: 70.78
Round  89, Train loss: 1.071, Test loss: 0.967, Test accuracy: 70.44
Round  90, Train loss: 0.868, Test loss: 0.967, Test accuracy: 70.25
Round  91, Train loss: 0.862, Test loss: 0.955, Test accuracy: 70.68
Round  92, Train loss: 0.931, Test loss: 0.958, Test accuracy: 70.52
Round  93, Train loss: 1.090, Test loss: 0.946, Test accuracy: 71.12
Round  94, Train loss: 0.986, Test loss: 0.954, Test accuracy: 70.82
Round  95, Train loss: 1.048, Test loss: 0.968, Test accuracy: 70.35
Round  96, Train loss: 0.979, Test loss: 0.949, Test accuracy: 71.31
Round  97, Train loss: 1.013, Test loss: 0.952, Test accuracy: 70.77
Round  98, Train loss: 0.964, Test loss: 0.962, Test accuracy: 70.48
Round  99, Train loss: 0.866, Test loss: 0.956, Test accuracy: 70.83
Final Round, Train loss: 0.904, Test loss: 0.959, Test accuracy: 70.82
Average accuracy final 10 rounds: 70.7125
4110.941289663315
[5.997757196426392, 11.18958067893982, 16.317408084869385, 21.622581720352173, 27.069194078445435, 32.42086720466614, 37.53923511505127, 42.74678063392639, 47.91651725769043, 53.21123647689819, 58.445188999176025, 63.7094190120697, 68.92514300346375, 74.06257438659668, 79.21369504928589, 84.35737800598145, 89.78814387321472, 95.20163536071777, 100.35448598861694, 105.5298547744751, 110.6794753074646, 115.86357545852661, 121.02762961387634, 126.19118738174438, 131.35472083091736, 136.548570394516, 141.90085077285767, 147.06466507911682, 152.24011516571045, 157.36352372169495, 162.5538170337677, 167.77107644081116, 173.00958442687988, 178.26707553863525, 183.42128467559814, 188.65096735954285, 193.8566837310791, 199.00416684150696, 204.15538430213928, 209.3067982196808, 214.57924056053162, 219.76331233978271, 224.96065258979797, 230.17364716529846, 235.37327575683594, 240.6521234512329, 245.80357241630554, 250.97378969192505, 256.1590025424957, 261.27180767059326, 266.42981910705566, 271.6793842315674, 276.79418563842773, 281.9253890514374, 287.0549249649048, 292.18398547172546, 297.34669518470764, 302.5855152606964, 307.79227924346924, 312.9569237232208, 318.1328432559967, 323.38801765441895, 328.6968138217926, 333.93649220466614, 339.1985080242157, 344.3469786643982, 349.6341350078583, 354.902898311615, 360.31365036964417, 365.52237701416016, 370.6635239124298, 375.81337428092957, 380.9606602191925, 386.26099729537964, 391.404887676239, 396.5519669055939, 401.6688652038574, 406.9799566268921, 412.10608863830566, 417.18217039108276, 422.27767848968506, 427.448801279068, 432.50429105758667, 437.5919635295868, 442.6684548854828, 447.81684470176697, 452.9937119483948, 458.14704155921936, 463.43463468551636, 468.5151267051697, 473.68273067474365, 478.822468996048, 483.93412375450134, 489.0461654663086, 494.12734746932983, 499.2041275501251, 504.27166295051575, 509.3388946056366, 514.4683802127838, 519.7342398166656, 521.7972366809845]
[28.195, 34.1025, 39.145, 41.7975, 45.37, 47.1075, 48.8975, 50.97, 52.7425, 53.765, 54.3975, 56.015, 57.2925, 58.7725, 59.3525, 60.425, 60.925, 62.055, 62.345, 63.075, 62.9775, 63.955, 64.72, 64.6625, 64.815, 64.985, 65.475, 65.245, 66.5975, 66.39, 66.6325, 66.7725, 67.2125, 67.3225, 67.81, 67.165, 67.9225, 68.4325, 68.585, 68.7525, 68.835, 68.25, 67.685, 68.61, 68.7725, 69.0725, 68.9675, 69.2525, 68.815, 69.525, 69.325, 69.5, 68.975, 69.605, 69.2075, 69.89, 69.8, 70.1525, 69.7675, 69.64, 70.5025, 70.3375, 70.2875, 70.265, 70.71, 70.3675, 70.06, 70.36, 70.3575, 70.0375, 70.47, 70.4275, 70.1175, 70.335, 70.53, 70.1125, 70.715, 69.995, 70.65, 71.3, 70.7, 70.795, 71.3325, 70.9925, 70.8675, 71.1075, 70.7725, 70.7425, 70.7775, 70.445, 70.2525, 70.68, 70.5175, 71.1175, 70.8175, 70.3525, 71.3125, 70.7675, 70.4775, 70.83, 70.8225]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 9, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  27.3800
Round 1 global test acc  36.9400
Round 2 global test acc  42.9400
Round 3 global test acc  45.6900
Round 4 global test acc  47.9300
Round 5 global test acc  47.9500
Round 6 global test acc  50.9600
Round 7 global test acc  52.2100
Round 8 global test acc  51.3000
Round 9 global test acc  54.8000
Round 10 global test acc  53.9400
Round 11 global test acc  55.3000
Round 12 global test acc  54.9300
Round 13 global test acc  54.4000
Round 14 global test acc  54.6200
Round 15 global test acc  56.9800
Round 16 global test acc  55.3400
Round 17 global test acc  56.5000
Round 18 global test acc  56.9900
Round 19 global test acc  58.6700
Round 20 global test acc  58.7000
Round 21 global test acc  55.6700
Round 22 global test acc  57.0500
Round 23 global test acc  59.4800
Round 24 global test acc  60.0100
Round 25 global test acc  58.8400
Round 26 global test acc  55.9700
Round 27 global test acc  58.2800
Round 28 global test acc  60.4700
Round 29 global test acc  57.0100
Round 30 global test acc  56.6600
Round 31 global test acc  60.0500
Round 32 global test acc  60.3400
Round 33 global test acc  61.1300
Round 34 global test acc  60.7100
Round 35 global test acc  60.6500
Round 36 global test acc  61.4200
Round 37 global test acc  60.6100
Round 38 global test acc  62.2100
Round 39 global test acc  60.9700
Round 40 global test acc  59.9900
Round 41 global test acc  60.7600
Round 42 global test acc  57.1800
Round 43 global test acc  60.3200
Round 44 global test acc  62.9000
Round 45 global test acc  61.6600
Round 46 global test acc  62.5100
Round 47 global test acc  60.0800
Round 48 global test acc  62.1300
Round 49 global test acc  61.6600
Round 50 global test acc  61.8500
Round 51 global test acc  62.6200
Round 52 global test acc  63.1900
Round 53 global test acc  61.4200
Round 54 global test acc  61.6300
Round 55 global test acc  62.7100
Round 56 global test acc  62.4000
Round 57 global test acc  63.2000
Round 58 global test acc  62.5900
Round 59 global test acc  63.7400
Round 60 global test acc  60.4100
Round 61 global test acc  63.6100
Round 62 global test acc  63.7200
Round 63 global test acc  63.8900
Round 64 global test acc  63.4500
Round 65 global test acc  60.7600
Round 66 global test acc  62.4100
Round 67 global test acc  65.3800
Round 68 global test acc  64.0900
Round 69 global test acc  63.7900
Round 70 global test acc  64.0300
Round 71 global test acc  61.8500
Round 72 global test acc  62.9100
Round 73 global test acc  64.0600
Round 74 global test acc  63.7000
Round 75 global test acc  63.7300
Round 76 global test acc  62.6700
Round 77 global test acc  64.2100
Round 78 global test acc  64.8400
Round 79 global test acc  64.4500
Round 80 global test acc  64.1000
Round 81 global test acc  62.3800
Round 82 global test acc  62.3600
Round 83 global test acc  60.8100
Round 84 global test acc  59.3900
Round 85 global test acc  59.4300
Round 86 global test acc  58.9600
Round 87 global test acc  58.8900
Round 88 global test acc  57.4600
Round 89 global test acc  57.5400
Round 90 global test acc  56.5500
Round 91 global test acc  56.8100
Round 92 global test acc  56.8000
Round 93 global test acc  57.1100
Round 94 global test acc  57.0500
Round 95 global test acc  56.5600
Round 96 global test acc  56.0700
Round 97 global test acc  56.5600
Round 98 global test acc  56.2900
Round 99 global test acc  55.4100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 9, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.224, Test loss: 2.057, Test accuracy: 24.93
Round   1, Train loss: 2.078, Test loss: 1.887, Test accuracy: 34.09
Round   2, Train loss: 1.905, Test loss: 1.754, Test accuracy: 40.01
Round   3, Train loss: 1.887, Test loss: 1.707, Test accuracy: 42.78
Round   4, Train loss: 1.882, Test loss: 1.658, Test accuracy: 44.88
Round   5, Train loss: 1.794, Test loss: 1.584, Test accuracy: 47.27
Round   6, Train loss: 1.788, Test loss: 1.558, Test accuracy: 48.67
Round   7, Train loss: 1.675, Test loss: 1.532, Test accuracy: 49.78
Round   8, Train loss: 1.708, Test loss: 1.456, Test accuracy: 51.61
Round   9, Train loss: 1.696, Test loss: 1.454, Test accuracy: 52.03
Round  10, Train loss: 1.636, Test loss: 1.415, Test accuracy: 54.07
Round  11, Train loss: 1.630, Test loss: 1.428, Test accuracy: 53.94
Round  12, Train loss: 1.656, Test loss: 1.404, Test accuracy: 54.95
Round  13, Train loss: 1.526, Test loss: 1.372, Test accuracy: 55.79
Round  14, Train loss: 1.692, Test loss: 1.334, Test accuracy: 57.69
Round  15, Train loss: 1.633, Test loss: 1.345, Test accuracy: 58.06
Round  16, Train loss: 1.538, Test loss: 1.273, Test accuracy: 59.40
Round  17, Train loss: 1.592, Test loss: 1.283, Test accuracy: 59.43
Round  18, Train loss: 1.497, Test loss: 1.265, Test accuracy: 60.34
Round  19, Train loss: 1.539, Test loss: 1.240, Test accuracy: 60.51
Round  20, Train loss: 1.523, Test loss: 1.241, Test accuracy: 61.59
Round  21, Train loss: 1.529, Test loss: 1.222, Test accuracy: 62.30
Round  22, Train loss: 1.406, Test loss: 1.212, Test accuracy: 62.34
Round  23, Train loss: 1.498, Test loss: 1.190, Test accuracy: 63.09
Round  24, Train loss: 1.390, Test loss: 1.148, Test accuracy: 64.07
Round  25, Train loss: 1.315, Test loss: 1.141, Test accuracy: 64.58
Round  26, Train loss: 1.523, Test loss: 1.148, Test accuracy: 64.97
Round  27, Train loss: 1.381, Test loss: 1.142, Test accuracy: 65.00
Round  28, Train loss: 1.359, Test loss: 1.119, Test accuracy: 65.42
Round  29, Train loss: 1.426, Test loss: 1.114, Test accuracy: 65.77
Round  30, Train loss: 1.432, Test loss: 1.112, Test accuracy: 65.75
Round  31, Train loss: 1.368, Test loss: 1.130, Test accuracy: 65.69
Round  32, Train loss: 1.386, Test loss: 1.116, Test accuracy: 65.91
Round  33, Train loss: 1.279, Test loss: 1.108, Test accuracy: 66.09
Round  34, Train loss: 1.417, Test loss: 1.107, Test accuracy: 66.58
Round  35, Train loss: 1.330, Test loss: 1.103, Test accuracy: 66.67
Round  36, Train loss: 1.314, Test loss: 1.096, Test accuracy: 66.88
Round  37, Train loss: 1.269, Test loss: 1.079, Test accuracy: 67.00
Round  38, Train loss: 1.328, Test loss: 1.071, Test accuracy: 67.50
Round  39, Train loss: 1.288, Test loss: 1.069, Test accuracy: 67.59
Round  40, Train loss: 1.212, Test loss: 1.069, Test accuracy: 68.02
Round  41, Train loss: 1.107, Test loss: 1.050, Test accuracy: 67.89
Round  42, Train loss: 1.271, Test loss: 1.063, Test accuracy: 68.17
Round  43, Train loss: 1.353, Test loss: 1.057, Test accuracy: 68.15
Round  44, Train loss: 1.310, Test loss: 1.057, Test accuracy: 68.11
Round  45, Train loss: 1.324, Test loss: 1.059, Test accuracy: 68.11
Round  46, Train loss: 1.266, Test loss: 1.052, Test accuracy: 68.39
Round  47, Train loss: 1.201, Test loss: 1.042, Test accuracy: 68.35
Round  48, Train loss: 1.103, Test loss: 1.040, Test accuracy: 68.48
Round  49, Train loss: 1.301, Test loss: 1.046, Test accuracy: 68.36
Round  50, Train loss: 1.291, Test loss: 1.057, Test accuracy: 68.41
Round  51, Train loss: 1.261, Test loss: 1.053, Test accuracy: 68.21
Round  52, Train loss: 1.343, Test loss: 1.060, Test accuracy: 68.19
Round  53, Train loss: 1.286, Test loss: 1.045, Test accuracy: 68.55
Round  54, Train loss: 1.266, Test loss: 1.050, Test accuracy: 68.15
Round  55, Train loss: 1.300, Test loss: 1.035, Test accuracy: 68.57
Round  56, Train loss: 1.047, Test loss: 1.018, Test accuracy: 68.84
Round  57, Train loss: 1.275, Test loss: 1.023, Test accuracy: 68.89
Round  58, Train loss: 1.173, Test loss: 1.016, Test accuracy: 69.29
Round  59, Train loss: 1.160, Test loss: 1.032, Test accuracy: 68.70
Round  60, Train loss: 1.169, Test loss: 1.034, Test accuracy: 68.50
Round  61, Train loss: 1.329, Test loss: 1.029, Test accuracy: 68.62
Round  62, Train loss: 1.114, Test loss: 1.014, Test accuracy: 68.95
Round  63, Train loss: 1.111, Test loss: 1.016, Test accuracy: 69.18
Round  64, Train loss: 1.186, Test loss: 1.029, Test accuracy: 68.47
Round  65, Train loss: 1.126, Test loss: 1.021, Test accuracy: 69.16
Round  66, Train loss: 1.321, Test loss: 1.035, Test accuracy: 68.48
Round  67, Train loss: 1.176, Test loss: 1.038, Test accuracy: 68.23
Round  68, Train loss: 1.078, Test loss: 1.021, Test accuracy: 68.66
Round  69, Train loss: 1.174, Test loss: 1.035, Test accuracy: 68.36
Round  70, Train loss: 1.096, Test loss: 1.042, Test accuracy: 68.10
Round  71, Train loss: 1.125, Test loss: 1.040, Test accuracy: 68.24
Round  72, Train loss: 1.228, Test loss: 1.051, Test accuracy: 68.00
Round  73, Train loss: 1.068, Test loss: 1.029, Test accuracy: 68.81
Round  74, Train loss: 1.146, Test loss: 1.033, Test accuracy: 68.41
Round  75, Train loss: 1.234, Test loss: 1.038, Test accuracy: 68.44
Round  76, Train loss: 1.066, Test loss: 1.037, Test accuracy: 68.12
Round  77, Train loss: 1.004, Test loss: 1.023, Test accuracy: 68.52
Round  78, Train loss: 1.084, Test loss: 1.021, Test accuracy: 68.61
Round  79, Train loss: 1.169, Test loss: 1.016, Test accuracy: 68.92
Round  80, Train loss: 1.072, Test loss: 1.030, Test accuracy: 68.59
Round  81, Train loss: 1.111, Test loss: 1.023, Test accuracy: 68.55
Round  82, Train loss: 1.001, Test loss: 1.003, Test accuracy: 69.42
Round  83, Train loss: 1.075, Test loss: 1.014, Test accuracy: 69.02
Round  84, Train loss: 1.036, Test loss: 1.025, Test accuracy: 68.75
Round  85, Train loss: 1.040, Test loss: 1.004, Test accuracy: 69.08
Round  86, Train loss: 1.193, Test loss: 1.011, Test accuracy: 69.18
Round  87, Train loss: 1.107, Test loss: 1.024, Test accuracy: 68.86
Round  88, Train loss: 1.218, Test loss: 1.015, Test accuracy: 69.19
Round  89, Train loss: 1.180, Test loss: 1.029, Test accuracy: 68.51
Round  90, Train loss: 1.123, Test loss: 1.024, Test accuracy: 68.73
Round  91, Train loss: 1.011, Test loss: 1.017, Test accuracy: 68.90
Round  92, Train loss: 1.161, Test loss: 1.029, Test accuracy: 68.58
Round  93, Train loss: 1.122, Test loss: 1.026, Test accuracy: 68.67
Round  94, Train loss: 1.021, Test loss: 1.023, Test accuracy: 68.72
Round  95, Train loss: 1.027, Test loss: 1.038, Test accuracy: 68.14
Round  96, Train loss: 1.102, Test loss: 1.026, Test accuracy: 68.22
Round  97, Train loss: 1.102, Test loss: 1.045, Test accuracy: 68.15
Round  98, Train loss: 0.984, Test loss: 1.024, Test accuracy: 68.81
Round  99, Train loss: 1.030, Test loss: 1.036, Test accuracy: 68.22
Final Round, Train loss: 0.979, Test loss: 1.038, Test accuracy: 67.90
Average accuracy final 10 rounds: 68.51225
4032.7188651561737
[5.406609296798706, 10.483823776245117, 15.627828359603882, 20.71056079864502, 25.7337908744812, 30.740856647491455, 35.754894733428955, 40.76444387435913, 45.783377170562744, 50.8068368434906, 55.79350399971008, 60.79851484298706, 65.8174958229065, 70.8387098312378, 75.86893010139465, 80.87966799736023, 85.8894910812378, 90.87818264961243, 95.88466930389404, 100.90715384483337, 105.91614103317261, 110.9265022277832, 115.96665167808533, 120.9853847026825, 125.95166492462158, 130.9644045829773, 135.99237060546875, 141.03120136260986, 146.02810263633728, 151.04175281524658, 156.05700993537903, 161.06439971923828, 166.08645367622375, 171.10125756263733, 176.12618160247803, 181.1346468925476, 186.1623775959015, 191.20861530303955, 196.21255254745483, 201.27814149856567, 206.53004693984985, 211.86835289001465, 217.00408959388733, 222.19453287124634, 227.4064748287201, 232.47815918922424, 237.55712723731995, 242.60487008094788, 247.67534041404724, 252.73527026176453, 257.8073925971985, 262.8885486125946, 267.93960976600647, 273.0878064632416, 278.3917145729065, 283.6279001235962, 288.8778500556946, 294.1267158985138, 299.21547293663025, 304.32211422920227, 309.44869327545166, 314.52619075775146, 319.59933519363403, 324.6637740135193, 329.8028509616852, 334.9378447532654, 340.0034248828888, 345.0935561656952, 350.12933230400085, 355.2552902698517, 360.4423816204071, 365.51747393608093, 370.59675216674805, 375.7009263038635, 380.79043102264404, 385.9035806655884, 390.9748146533966, 396.09108090400696, 401.1831259727478, 406.2206017971039, 411.29617261886597, 416.4354546070099, 421.5425052642822, 426.578595161438, 431.6634225845337, 436.73798155784607, 441.866308927536, 446.972286939621, 452.05182361602783, 457.268905878067, 462.32239151000977, 467.34957551956177, 472.3995532989502, 477.459276676178, 482.51220202445984, 487.5997049808502, 492.6811022758484, 497.75946521759033, 503.00367641448975, 508.234139919281, 510.33548951148987]
[24.9275, 34.085, 40.0125, 42.7825, 44.88, 47.2725, 48.6675, 49.7825, 51.61, 52.03, 54.07, 53.9375, 54.9475, 55.7875, 57.6875, 58.06, 59.395, 59.4275, 60.3375, 60.5125, 61.595, 62.3, 62.3375, 63.095, 64.0675, 64.5825, 64.9675, 65.005, 65.415, 65.77, 65.745, 65.685, 65.9125, 66.0925, 66.585, 66.6725, 66.8775, 67.0025, 67.505, 67.5875, 68.0225, 67.8875, 68.17, 68.1475, 68.1075, 68.11, 68.395, 68.3475, 68.485, 68.3625, 68.41, 68.21, 68.195, 68.545, 68.1475, 68.57, 68.845, 68.885, 69.2925, 68.7, 68.505, 68.62, 68.955, 69.18, 68.475, 69.1575, 68.4825, 68.23, 68.6575, 68.36, 68.1025, 68.24, 68.0, 68.805, 68.41, 68.4425, 68.12, 68.52, 68.615, 68.9175, 68.59, 68.5475, 69.4225, 69.015, 68.7525, 69.0775, 69.1775, 68.8625, 69.19, 68.5125, 68.7325, 68.8975, 68.575, 68.665, 68.715, 68.135, 68.22, 68.1475, 68.8125, 68.2225, 67.9025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 15, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.208, Test loss: 1.991, Test accuracy: 28.43
Round   1, Train loss: 1.987, Test loss: 1.782, Test accuracy: 37.25
Round   2, Train loss: 1.858, Test loss: 1.709, Test accuracy: 40.74
Round   3, Train loss: 1.819, Test loss: 1.628, Test accuracy: 44.25
Round   4, Train loss: 1.743, Test loss: 1.567, Test accuracy: 46.62
Round   5, Train loss: 1.769, Test loss: 1.539, Test accuracy: 48.35
Round   6, Train loss: 1.721, Test loss: 1.517, Test accuracy: 50.00
Round   7, Train loss: 1.723, Test loss: 1.470, Test accuracy: 51.13
Round   8, Train loss: 1.608, Test loss: 1.373, Test accuracy: 53.29
Round   9, Train loss: 1.508, Test loss: 1.354, Test accuracy: 54.46
Round  10, Train loss: 1.569, Test loss: 1.331, Test accuracy: 55.38
Round  11, Train loss: 1.577, Test loss: 1.345, Test accuracy: 55.09
Round  12, Train loss: 1.539, Test loss: 1.321, Test accuracy: 56.90
Round  13, Train loss: 1.515, Test loss: 1.282, Test accuracy: 58.27
Round  14, Train loss: 1.497, Test loss: 1.258, Test accuracy: 59.38
Round  15, Train loss: 1.415, Test loss: 1.247, Test accuracy: 59.55
Round  16, Train loss: 1.406, Test loss: 1.191, Test accuracy: 61.10
Round  17, Train loss: 1.476, Test loss: 1.190, Test accuracy: 61.89
Round  18, Train loss: 1.441, Test loss: 1.170, Test accuracy: 62.40
Round  19, Train loss: 1.405, Test loss: 1.172, Test accuracy: 62.53
Round  20, Train loss: 1.335, Test loss: 1.163, Test accuracy: 63.43
Round  21, Train loss: 1.333, Test loss: 1.148, Test accuracy: 63.84
Round  22, Train loss: 1.374, Test loss: 1.128, Test accuracy: 64.30
Round  23, Train loss: 1.352, Test loss: 1.135, Test accuracy: 64.22
Round  24, Train loss: 1.372, Test loss: 1.103, Test accuracy: 64.51
Round  25, Train loss: 1.387, Test loss: 1.099, Test accuracy: 65.66
Round  26, Train loss: 1.363, Test loss: 1.083, Test accuracy: 65.58
Round  27, Train loss: 1.281, Test loss: 1.092, Test accuracy: 65.30
Round  28, Train loss: 1.333, Test loss: 1.067, Test accuracy: 65.88
Round  29, Train loss: 1.363, Test loss: 1.076, Test accuracy: 66.25
Round  30, Train loss: 1.258, Test loss: 1.065, Test accuracy: 66.14
Round  31, Train loss: 1.262, Test loss: 1.077, Test accuracy: 65.95
Round  32, Train loss: 1.245, Test loss: 1.048, Test accuracy: 66.66
Round  33, Train loss: 1.213, Test loss: 1.048, Test accuracy: 66.69
Round  34, Train loss: 1.217, Test loss: 1.043, Test accuracy: 67.60
Round  35, Train loss: 1.272, Test loss: 1.030, Test accuracy: 68.02
Round  36, Train loss: 1.250, Test loss: 1.035, Test accuracy: 67.62
Round  37, Train loss: 1.237, Test loss: 1.033, Test accuracy: 67.92
Round  38, Train loss: 1.252, Test loss: 1.045, Test accuracy: 67.19
Round  39, Train loss: 1.179, Test loss: 1.043, Test accuracy: 67.54
Round  40, Train loss: 1.303, Test loss: 1.038, Test accuracy: 67.35
Round  41, Train loss: 1.225, Test loss: 1.045, Test accuracy: 67.44
Round  42, Train loss: 1.220, Test loss: 1.044, Test accuracy: 67.35
Round  43, Train loss: 1.186, Test loss: 1.010, Test accuracy: 68.09
Round  44, Train loss: 1.078, Test loss: 1.005, Test accuracy: 67.97
Round  45, Train loss: 1.198, Test loss: 0.995, Test accuracy: 68.46
Round  46, Train loss: 1.186, Test loss: 0.994, Test accuracy: 68.67
Round  47, Train loss: 1.128, Test loss: 1.002, Test accuracy: 68.70
Round  48, Train loss: 1.140, Test loss: 1.000, Test accuracy: 69.00
Round  49, Train loss: 1.152, Test loss: 0.991, Test accuracy: 68.62
Round  50, Train loss: 1.199, Test loss: 0.988, Test accuracy: 69.33
Round  51, Train loss: 1.109, Test loss: 0.974, Test accuracy: 69.62
Round  52, Train loss: 1.145, Test loss: 0.983, Test accuracy: 69.33
Round  53, Train loss: 1.158, Test loss: 0.979, Test accuracy: 69.41
Round  54, Train loss: 1.138, Test loss: 0.987, Test accuracy: 69.09
Round  55, Train loss: 1.095, Test loss: 0.986, Test accuracy: 69.32
Round  56, Train loss: 1.052, Test loss: 0.964, Test accuracy: 69.77
Round  57, Train loss: 1.206, Test loss: 0.972, Test accuracy: 69.97
Round  58, Train loss: 1.073, Test loss: 0.971, Test accuracy: 70.03
Round  59, Train loss: 1.057, Test loss: 0.966, Test accuracy: 69.96
Round  60, Train loss: 1.129, Test loss: 0.975, Test accuracy: 69.73
Round  61, Train loss: 1.208, Test loss: 0.969, Test accuracy: 69.72
Round  62, Train loss: 1.122, Test loss: 0.963, Test accuracy: 70.27
Round  63, Train loss: 1.005, Test loss: 0.946, Test accuracy: 70.87
Round  64, Train loss: 1.004, Test loss: 0.956, Test accuracy: 70.35
Round  65, Train loss: 1.093, Test loss: 0.958, Test accuracy: 70.18
Round  66, Train loss: 1.063, Test loss: 0.966, Test accuracy: 69.89
Round  67, Train loss: 1.145, Test loss: 0.969, Test accuracy: 69.77
Round  68, Train loss: 1.025, Test loss: 0.960, Test accuracy: 70.37
Round  69, Train loss: 1.079, Test loss: 0.968, Test accuracy: 69.77
Round  70, Train loss: 0.951, Test loss: 0.955, Test accuracy: 70.11
Round  71, Train loss: 1.016, Test loss: 0.960, Test accuracy: 69.98
Round  72, Train loss: 1.064, Test loss: 0.970, Test accuracy: 69.60
Round  73, Train loss: 1.097, Test loss: 0.975, Test accuracy: 69.79
Round  74, Train loss: 1.022, Test loss: 0.956, Test accuracy: 70.22
Round  75, Train loss: 1.121, Test loss: 0.959, Test accuracy: 70.21
Round  76, Train loss: 0.959, Test loss: 0.953, Test accuracy: 70.46
Round  77, Train loss: 0.999, Test loss: 0.951, Test accuracy: 70.38
Round  78, Train loss: 1.004, Test loss: 0.955, Test accuracy: 70.45
Round  79, Train loss: 1.106, Test loss: 0.948, Test accuracy: 70.89
Round  80, Train loss: 0.979, Test loss: 0.953, Test accuracy: 70.65
Round  81, Train loss: 1.021, Test loss: 0.934, Test accuracy: 71.37
Round  82, Train loss: 1.116, Test loss: 0.950, Test accuracy: 70.58
Round  83, Train loss: 0.953, Test loss: 0.942, Test accuracy: 70.96
Round  84, Train loss: 0.935, Test loss: 0.952, Test accuracy: 70.55
Round  85, Train loss: 1.104, Test loss: 0.945, Test accuracy: 70.91
Round  86, Train loss: 1.083, Test loss: 0.948, Test accuracy: 71.06
Round  87, Train loss: 1.063, Test loss: 0.954, Test accuracy: 70.73
Round  88, Train loss: 1.054, Test loss: 0.966, Test accuracy: 70.48
Round  89, Train loss: 0.964, Test loss: 0.962, Test accuracy: 70.52
Round  90, Train loss: 0.971, Test loss: 0.971, Test accuracy: 70.22
Round  91, Train loss: 0.974, Test loss: 0.959, Test accuracy: 70.39
Round  92, Train loss: 0.978, Test loss: 0.950, Test accuracy: 70.64
Round  93, Train loss: 0.996, Test loss: 0.937, Test accuracy: 71.29
Round  94, Train loss: 0.962, Test loss: 0.941, Test accuracy: 70.93
Round  95, Train loss: 0.932, Test loss: 0.947, Test accuracy: 70.74
Round  96, Train loss: 0.909, Test loss: 0.956, Test accuracy: 70.53
Round  97, Train loss: 1.029, Test loss: 0.940, Test accuracy: 71.14
Round  98, Train loss: 0.886, Test loss: 0.937, Test accuracy: 71.23
Round  99, Train loss: 0.907, Test loss: 0.956, Test accuracy: 70.58
Final Round, Train loss: 0.927, Test loss: 0.960, Test accuracy: 70.60
Average accuracy final 10 rounds: 70.76875000000001
6172.660845518112
[5.506922483444214, 10.605288982391357, 15.647997379302979, 21.347353219985962, 26.438544750213623, 31.53029227256775, 36.60250401496887, 41.749109506607056, 47.18082809448242, 52.25772786140442, 57.330952167510986, 62.435059785842896, 68.92098379135132, 74.85125494003296, 79.90861105918884, 84.97325944900513, 90.38481450080872, 95.6835196018219, 100.77905201911926, 105.8362603187561, 110.90167331695557, 119.85964107513428, 130.68975973129272, 141.9104962348938, 150.75807452201843, 162.1242413520813, 170.97325110435486, 179.767986536026, 188.61652660369873, 199.1709852218628, 209.8411705493927, 220.29031443595886, 229.1796476840973, 239.75116229057312, 250.39962100982666, 261.2946226596832, 272.042982339859, 282.7636671066284, 291.77404165267944, 300.92621779441833, 309.8695578575134, 318.7652041912079, 327.85389709472656, 337.0243430137634, 346.14195466041565, 355.2938709259033, 365.96833205223083, 376.6362714767456, 385.7804660797119, 395.96332240104675, 405.21919107437134, 414.4708204269409, 423.68692111968994, 432.9677047729492, 442.04531621932983, 451.3375017642975, 460.55486392974854, 469.8218123912811, 480.94920015335083, 492.0490233898163, 502.74052810668945, 512.0302295684814, 521.5596323013306, 530.8105068206787, 540.3570883274078, 549.8471610546112, 559.1269991397858, 568.6771881580353, 579.4878604412079, 590.2164726257324, 601.2035324573517, 612.2015836238861, 623.0308063030243, 633.9867722988129, 645.1622459888458, 654.2379665374756, 663.3463253974915, 672.9244117736816, 682.57683634758, 691.7906935214996, 701.1742730140686, 710.59335064888, 720.3299105167389, 729.907744884491, 739.0256774425507, 748.1459863185883, 757.2429716587067, 766.3718073368073, 775.5227732658386, 784.6816036701202, 793.8533508777618, 802.9744057655334, 812.1122007369995, 821.3310186862946, 830.4684097766876, 839.6372046470642, 848.7812352180481, 857.8577437400818, 867.1392242908478, 876.28733253479, 878.3760166168213]
[28.43, 37.2525, 40.7425, 44.2475, 46.6225, 48.35, 50.0, 51.1275, 53.2875, 54.4625, 55.385, 55.0925, 56.895, 58.2725, 59.385, 59.555, 61.0975, 61.89, 62.395, 62.535, 63.4275, 63.84, 64.2975, 64.2225, 64.51, 65.655, 65.5825, 65.2975, 65.88, 66.2525, 66.1425, 65.95, 66.655, 66.6925, 67.6025, 68.02, 67.6225, 67.9225, 67.185, 67.54, 67.3525, 67.435, 67.3525, 68.095, 67.965, 68.46, 68.6675, 68.7, 68.9975, 68.6225, 69.325, 69.625, 69.325, 69.4075, 69.0875, 69.3225, 69.7675, 69.9675, 70.03, 69.96, 69.73, 69.7175, 70.2725, 70.87, 70.3525, 70.1825, 69.89, 69.77, 70.3725, 69.765, 70.1075, 69.98, 69.5975, 69.7875, 70.225, 70.2075, 70.4625, 70.3775, 70.4525, 70.895, 70.6525, 71.3675, 70.575, 70.9625, 70.55, 70.91, 71.055, 70.7325, 70.4825, 70.5225, 70.225, 70.3875, 70.645, 71.29, 70.93, 70.7375, 70.525, 71.135, 71.2275, 70.585, 70.5975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 201, in get_data_from_file
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_co_teaching2.py", line 92, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "RFL.py", line 62, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 93, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 95, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 295, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 0, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.194, Test loss: 2.091, Test accuracy: 27.75
Round   0, Global train loss: 2.194, Global test loss: 2.101, Global test accuracy: 28.84
Round   1, Train loss: 2.152, Test loss: 2.020, Test accuracy: 34.15
Round   1, Global train loss: 2.152, Global test loss: 2.027, Global test accuracy: 37.82
Round   2, Train loss: 2.082, Test loss: 1.965, Test accuracy: 34.74
Round   2, Global train loss: 2.082, Global test loss: 1.938, Global test accuracy: 38.94
Round   3, Train loss: 2.120, Test loss: 1.910, Test accuracy: 36.15
Round   3, Global train loss: 2.120, Global test loss: 1.838, Global test accuracy: 42.61
Round   4, Train loss: 2.083, Test loss: 1.920, Test accuracy: 36.77
Round   4, Global train loss: 2.083, Global test loss: 1.904, Global test accuracy: 42.17
Round   5, Train loss: 2.052, Test loss: 1.901, Test accuracy: 36.94
Round   5, Global train loss: 2.052, Global test loss: 1.889, Global test accuracy: 42.60
Round   6, Train loss: 2.144, Test loss: 1.902, Test accuracy: 35.95
Round   6, Global train loss: 2.144, Global test loss: 1.892, Global test accuracy: 41.20
Round   7, Train loss: 2.021, Test loss: 1.878, Test accuracy: 36.93
Round   7, Global train loss: 2.021, Global test loss: 1.817, Global test accuracy: 45.00
Round   8, Train loss: 1.901, Test loss: 1.863, Test accuracy: 37.58
Round   8, Global train loss: 1.901, Global test loss: 1.759, Global test accuracy: 47.47
Round   9, Train loss: 1.967, Test loss: 1.858, Test accuracy: 37.52
Round   9, Global train loss: 1.967, Global test loss: 1.750, Global test accuracy: 47.84
Round  10, Train loss: 2.125, Test loss: 1.865, Test accuracy: 37.71
Round  10, Global train loss: 2.125, Global test loss: 2.029, Global test accuracy: 41.29
Round  11, Train loss: 2.010, Test loss: 1.866, Test accuracy: 36.90
Round  11, Global train loss: 2.010, Global test loss: 1.899, Global test accuracy: 42.45
Round  12, Train loss: 1.965, Test loss: 1.856, Test accuracy: 37.16
Round  12, Global train loss: 1.965, Global test loss: 1.840, Global test accuracy: 44.09
Round  13, Train loss: 2.002, Test loss: 1.853, Test accuracy: 37.30
Round  13, Global train loss: 2.002, Global test loss: 1.891, Global test accuracy: 44.38
Round  14, Train loss: 1.929, Test loss: 1.852, Test accuracy: 37.18
Round  14, Global train loss: 1.929, Global test loss: 1.884, Global test accuracy: 45.30
Round  15, Train loss: 1.970, Test loss: 1.851, Test accuracy: 37.53
Round  15, Global train loss: 1.970, Global test loss: 1.869, Global test accuracy: 47.26
Round  16, Train loss: 1.995, Test loss: 1.852, Test accuracy: 37.59
Round  16, Global train loss: 1.995, Global test loss: 1.838, Global test accuracy: 44.64
Round  17, Train loss: 1.999, Test loss: 1.866, Test accuracy: 37.06
Round  17, Global train loss: 1.999, Global test loss: 1.959, Global test accuracy: 42.38
Round  18, Train loss: 1.893, Test loss: 1.861, Test accuracy: 37.06
Round  18, Global train loss: 1.893, Global test loss: 2.015, Global test accuracy: 39.45
Round  19, Train loss: 1.766, Test loss: 1.868, Test accuracy: 36.68
Round  19, Global train loss: 1.766, Global test loss: 1.722, Global test accuracy: 48.48
Round  20, Train loss: 1.897, Test loss: 1.865, Test accuracy: 36.73
Round  20, Global train loss: 1.897, Global test loss: 1.856, Global test accuracy: 45.16
Round  21, Train loss: 1.702, Test loss: 1.885, Test accuracy: 36.33
Round  21, Global train loss: 1.702, Global test loss: 1.676, Global test accuracy: 48.57
Round  22, Train loss: 1.872, Test loss: 1.894, Test accuracy: 36.32
Round  22, Global train loss: 1.872, Global test loss: 1.942, Global test accuracy: 41.98
Round  23, Train loss: 1.846, Test loss: 1.932, Test accuracy: 35.51
Round  23, Global train loss: 1.846, Global test loss: 1.972, Global test accuracy: 41.16
Round  24, Train loss: 1.764, Test loss: 1.937, Test accuracy: 35.53
Round  24, Global train loss: 1.764, Global test loss: 1.939, Global test accuracy: 40.04
Round  25, Train loss: 1.658, Test loss: 1.950, Test accuracy: 34.84
Round  25, Global train loss: 1.658, Global test loss: 1.879, Global test accuracy: 41.63
Round  26, Train loss: 1.673, Test loss: 1.967, Test accuracy: 34.84
Round  26, Global train loss: 1.673, Global test loss: 1.854, Global test accuracy: 42.83
Round  27, Train loss: 1.739, Test loss: 1.990, Test accuracy: 34.32
Round  27, Global train loss: 1.739, Global test loss: 1.825, Global test accuracy: 44.70
Round  28, Train loss: 1.645, Test loss: 2.014, Test accuracy: 34.12
Round  28, Global train loss: 1.645, Global test loss: 1.901, Global test accuracy: 42.02
Round  29, Train loss: 1.582, Test loss: 2.042, Test accuracy: 33.91
Round  29, Global train loss: 1.582, Global test loss: 1.820, Global test accuracy: 43.77
Round  30, Train loss: 1.582, Test loss: 2.057, Test accuracy: 33.69
Round  30, Global train loss: 1.582, Global test loss: 1.882, Global test accuracy: 43.15
Round  31, Train loss: 1.516, Test loss: 2.083, Test accuracy: 33.51
Round  31, Global train loss: 1.516, Global test loss: 1.816, Global test accuracy: 44.59
Round  32, Train loss: 1.578, Test loss: 2.122, Test accuracy: 32.96
Round  32, Global train loss: 1.578, Global test loss: 1.813, Global test accuracy: 42.83
Round  33, Train loss: 1.638, Test loss: 2.148, Test accuracy: 32.64
Round  33, Global train loss: 1.638, Global test loss: 1.896, Global test accuracy: 40.25
Round  34, Train loss: 1.439, Test loss: 2.178, Test accuracy: 32.14
Round  34, Global train loss: 1.439, Global test loss: 1.836, Global test accuracy: 43.81
Round  35, Train loss: 1.403, Test loss: 2.206, Test accuracy: 31.82
Round  35, Global train loss: 1.403, Global test loss: 1.810, Global test accuracy: 44.16
Round  36, Train loss: 1.355, Test loss: 2.242, Test accuracy: 31.72
Round  36, Global train loss: 1.355, Global test loss: 1.936, Global test accuracy: 35.90
Round  37, Train loss: 1.483, Test loss: 2.275, Test accuracy: 31.97
Round  37, Global train loss: 1.483, Global test loss: 1.954, Global test accuracy: 40.83
Round  38, Train loss: 1.374, Test loss: 2.322, Test accuracy: 31.61
Round  38, Global train loss: 1.374, Global test loss: 1.828, Global test accuracy: 42.37
Round  39, Train loss: 1.369, Test loss: 2.353, Test accuracy: 31.31
Round  39, Global train loss: 1.369, Global test loss: 1.691, Global test accuracy: 46.40
Round  40, Train loss: 1.263, Test loss: 2.368, Test accuracy: 31.40
Round  40, Global train loss: 1.263, Global test loss: 1.716, Global test accuracy: 43.89
Round  41, Train loss: 1.392, Test loss: 2.409, Test accuracy: 31.09
Round  41, Global train loss: 1.392, Global test loss: 1.797, Global test accuracy: 41.73
Round  42, Train loss: 1.209, Test loss: 2.465, Test accuracy: 30.95
Round  42, Global train loss: 1.209, Global test loss: 1.793, Global test accuracy: 42.52
Round  43, Train loss: 1.308, Test loss: 2.488, Test accuracy: 30.64
Round  43, Global train loss: 1.308, Global test loss: 1.954, Global test accuracy: 36.48
Round  44, Train loss: 1.268, Test loss: 2.513, Test accuracy: 30.33
Round  44, Global train loss: 1.268, Global test loss: 1.859, Global test accuracy: 39.30
Round  45, Train loss: 1.130, Test loss: 2.542, Test accuracy: 30.18
Round  45, Global train loss: 1.130, Global test loss: 1.827, Global test accuracy: 42.26
Round  46, Train loss: 1.254, Test loss: 2.587, Test accuracy: 30.50
Round  46, Global train loss: 1.254, Global test loss: 1.804, Global test accuracy: 40.33
Round  47, Train loss: 0.977, Test loss: 2.601, Test accuracy: 30.32
Round  47, Global train loss: 0.977, Global test loss: 1.785, Global test accuracy: 39.62
Round  48, Train loss: 1.243, Test loss: 2.633, Test accuracy: 30.13
Round  48, Global train loss: 1.243, Global test loss: 1.837, Global test accuracy: 41.16
Round  49, Train loss: 1.057, Test loss: 2.657, Test accuracy: 30.03
Round  49, Global train loss: 1.057, Global test loss: 1.828, Global test accuracy: 40.64
Round  50, Train loss: 0.998, Test loss: 2.693, Test accuracy: 30.14
Round  50, Global train loss: 0.998, Global test loss: 1.899, Global test accuracy: 36.98
Round  51, Train loss: 1.202, Test loss: 2.744, Test accuracy: 29.80
Round  51, Global train loss: 1.202, Global test loss: 1.890, Global test accuracy: 37.97
Round  52, Train loss: 0.977, Test loss: 2.783, Test accuracy: 29.39
Round  52, Global train loss: 0.977, Global test loss: 1.802, Global test accuracy: 40.11
Round  53, Train loss: 1.038, Test loss: 2.843, Test accuracy: 28.99
Round  53, Global train loss: 1.038, Global test loss: 1.894, Global test accuracy: 37.11
Round  54, Train loss: 1.060, Test loss: 2.899, Test accuracy: 28.92
Round  54, Global train loss: 1.060, Global test loss: 1.826, Global test accuracy: 39.83
Round  55, Train loss: 1.015, Test loss: 2.892, Test accuracy: 29.05
Round  55, Global train loss: 1.015, Global test loss: 1.789, Global test accuracy: 41.18
Round  56, Train loss: 0.995, Test loss: 2.913, Test accuracy: 29.00
Round  56, Global train loss: 0.995, Global test loss: 1.868, Global test accuracy: 38.90
Round  57, Train loss: 1.037, Test loss: 2.984, Test accuracy: 28.85
Round  57, Global train loss: 1.037, Global test loss: 1.794, Global test accuracy: 41.59
Round  58, Train loss: 1.007, Test loss: 3.012, Test accuracy: 28.84
Round  58, Global train loss: 1.007, Global test loss: 1.927, Global test accuracy: 36.23
Round  59, Train loss: 0.959, Test loss: 3.038, Test accuracy: 28.82
Round  59, Global train loss: 0.959, Global test loss: 1.910, Global test accuracy: 36.46
Round  60, Train loss: 0.969, Test loss: 3.059, Test accuracy: 28.93
Round  60, Global train loss: 0.969, Global test loss: 1.911, Global test accuracy: 37.37
Round  61, Train loss: 1.007, Test loss: 3.080, Test accuracy: 28.74
Round  61, Global train loss: 1.007, Global test loss: 1.884, Global test accuracy: 38.27
Round  62, Train loss: 1.018, Test loss: 3.148, Test accuracy: 28.38
Round  62, Global train loss: 1.018, Global test loss: 1.977, Global test accuracy: 34.84
Round  63, Train loss: 1.018, Test loss: 3.145, Test accuracy: 28.71
Round  63, Global train loss: 1.018, Global test loss: 1.924, Global test accuracy: 35.32
Round  64, Train loss: 0.865, Test loss: 3.185, Test accuracy: 28.46
Round  64, Global train loss: 0.865, Global test loss: 1.782, Global test accuracy: 40.77
Round  65, Train loss: 0.766, Test loss: 3.202, Test accuracy: 28.26
Round  65, Global train loss: 0.766, Global test loss: 1.720, Global test accuracy: 42.74
Round  66, Train loss: 1.027, Test loss: 3.262, Test accuracy: 28.15
Round  66, Global train loss: 1.027, Global test loss: 1.979, Global test accuracy: 34.26
Round  67, Train loss: 0.752, Test loss: 3.310, Test accuracy: 28.33
Round  67, Global train loss: 0.752, Global test loss: 1.810, Global test accuracy: 39.13
Round  68, Train loss: 0.971, Test loss: 3.354, Test accuracy: 27.98
Round  68, Global train loss: 0.971, Global test loss: 1.925, Global test accuracy: 36.19
Round  69, Train loss: 0.723, Test loss: 3.364, Test accuracy: 27.95
Round  69, Global train loss: 0.723, Global test loss: 1.779, Global test accuracy: 41.59
Round  70, Train loss: 0.746, Test loss: 3.377, Test accuracy: 28.35
Round  70, Global train loss: 0.746, Global test loss: 1.777, Global test accuracy: 39.56
Round  71, Train loss: 0.842, Test loss: 3.447, Test accuracy: 28.12
Round  71, Global train loss: 0.842, Global test loss: 1.918, Global test accuracy: 36.36
Round  72, Train loss: 0.908, Test loss: 3.492, Test accuracy: 28.14
Round  72, Global train loss: 0.908, Global test loss: 2.003, Global test accuracy: 29.71
Round  73, Train loss: 0.882, Test loss: 3.533, Test accuracy: 28.26
Round  73, Global train loss: 0.882, Global test loss: 1.881, Global test accuracy: 36.36
Round  74, Train loss: 0.855, Test loss: 3.571, Test accuracy: 28.26
Round  74, Global train loss: 0.855, Global test loss: 1.931, Global test accuracy: 35.62
Round  75, Train loss: 0.822, Test loss: 3.621, Test accuracy: 28.50
Round  75, Global train loss: 0.822, Global test loss: 1.858, Global test accuracy: 37.52
Round  76, Train loss: 0.728, Test loss: 3.638, Test accuracy: 28.46
Round  76, Global train loss: 0.728, Global test loss: 1.943, Global test accuracy: 34.15
Round  77, Train loss: 0.790, Test loss: 3.645, Test accuracy: 28.65
Round  77, Global train loss: 0.790, Global test loss: 1.926, Global test accuracy: 37.10
Round  78, Train loss: 0.849, Test loss: 3.642, Test accuracy: 28.31
Round  78, Global train loss: 0.849, Global test loss: 1.933, Global test accuracy: 36.50
Round  79, Train loss: 0.832, Test loss: 3.689, Test accuracy: 28.24
Round  79, Global train loss: 0.832, Global test loss: 2.012, Global test accuracy: 30.32
Round  80, Train loss: 0.647, Test loss: 3.735, Test accuracy: 28.61
Round  80, Global train loss: 0.647, Global test loss: 1.903, Global test accuracy: 38.24
Round  81, Train loss: 0.734, Test loss: 3.748, Test accuracy: 28.38
Round  81, Global train loss: 0.734, Global test loss: 2.030, Global test accuracy: 29.43
Round  82, Train loss: 0.769, Test loss: 3.744, Test accuracy: 28.42
Round  82, Global train loss: 0.769, Global test loss: 1.848, Global test accuracy: 35.85
Round  83, Train loss: 0.651, Test loss: 3.750, Test accuracy: 28.32
Round  83, Global train loss: 0.651, Global test loss: 1.917, Global test accuracy: 35.74
Round  84, Train loss: 0.667, Test loss: 3.817, Test accuracy: 28.38
Round  84, Global train loss: 0.667, Global test loss: 1.953, Global test accuracy: 34.95
Round  85, Train loss: 0.635, Test loss: 3.837, Test accuracy: 28.33
Round  85, Global train loss: 0.635, Global test loss: 1.884, Global test accuracy: 36.33
Round  86, Train loss: 0.765, Test loss: 3.838, Test accuracy: 28.50
Round  86, Global train loss: 0.765, Global test loss: 1.970, Global test accuracy: 32.41
Round  87, Train loss: 0.764, Test loss: 3.877, Test accuracy: 28.40
Round  87, Global train loss: 0.764, Global test loss: 1.978, Global test accuracy: 30.55
Round  88, Train loss: 0.737, Test loss: 3.905, Test accuracy: 28.30
Round  88, Global train loss: 0.737, Global test loss: 2.033, Global test accuracy: 32.60
Round  89, Train loss: 0.653, Test loss: 3.916, Test accuracy: 28.37
Round  89, Global train loss: 0.653, Global test loss: 1.844, Global test accuracy: 38.06
Round  90, Train loss: 0.727, Test loss: 3.956, Test accuracy: 27.97
Round  90, Global train loss: 0.727, Global test loss: 2.021, Global test accuracy: 31.30
Round  91, Train loss: 0.777, Test loss: 3.985, Test accuracy: 28.17
Round  91, Global train loss: 0.777, Global test loss: 1.969, Global test accuracy: 33.90
Round  92, Train loss: 0.531, Test loss: 3.990, Test accuracy: 28.17
Round  92, Global train loss: 0.531, Global test loss: 1.737, Global test accuracy: 40.77
Round  93, Train loss: 0.703, Test loss: 4.031, Test accuracy: 27.82
Round  93, Global train loss: 0.703, Global test loss: 1.847, Global test accuracy: 38.86
Round  94, Train loss: 0.699, Test loss: 4.070, Test accuracy: 28.03
Round  94, Global train loss: 0.699, Global test loss: 2.001, Global test accuracy: 30.38
Round  95, Train loss: 0.626, Test loss: 4.060, Test accuracy: 28.24
Round  95, Global train loss: 0.626, Global test loss: 1.924, Global test accuracy: 35.05
Round  96, Train loss: 0.662, Test loss: 4.151, Test accuracy: 28.38
Round  96, Global train loss: 0.662, Global test loss: 1.991, Global test accuracy: 32.85
Round  97, Train loss: 0.611, Test loss: 4.195, Test accuracy: 27.97
Round  97, Global train loss: 0.611, Global test loss: 2.039, Global test accuracy: 29.29
Round  98, Train loss: 0.591, Test loss: 4.210, Test accuracy: 27.81
Round  98, Global train loss: 0.591, Global test loss: 1.829, Global test accuracy: 37.98
Round  99, Train loss: 0.531, Test loss: 4.206, Test accuracy: 27.61
Round  99, Global train loss: 0.531, Global test loss: 1.807, Global test accuracy: 36.16
Final Round, Train loss: 0.407, Test loss: 4.900, Test accuracy: 28.10
Final Round, Global train loss: 0.407, Global test loss: 1.807, Global test accuracy: 36.16
Average accuracy final 10 rounds: 28.01775 

Average global accuracy final 10 rounds: 34.65425 

5366.780542850494
[4.80289101600647, 9.60578203201294, 13.847595453262329, 18.08940887451172, 22.300288200378418, 26.511167526245117, 31.251643657684326, 35.992119789123535, 40.453075647354126, 44.91403150558472, 49.2051146030426, 53.49619770050049, 57.82624101638794, 62.15628433227539, 66.72477412223816, 71.29326391220093, 75.70689630508423, 80.12052869796753, 84.48945903778076, 88.858389377594, 93.08509969711304, 97.31181001663208, 101.66215181350708, 106.01249361038208, 110.2396411895752, 114.46678876876831, 118.97558832168579, 123.48438787460327, 128.18141794204712, 132.87844800949097, 137.86099410057068, 142.8435401916504, 147.57938480377197, 152.31522941589355, 156.9283151626587, 161.54140090942383, 166.1515974998474, 170.761794090271, 174.99208664894104, 179.22237920761108, 183.41809582710266, 187.61381244659424, 191.78703379631042, 195.9602551460266, 200.53942203521729, 205.11858892440796, 209.98226189613342, 214.8459348678589, 219.46653699874878, 224.08713912963867, 228.7161259651184, 233.34511280059814, 237.96680808067322, 242.5885033607483, 247.2757728099823, 251.9630422592163, 256.15718626976013, 260.35133028030396, 264.5234067440033, 268.69548320770264, 273.1420302391052, 277.5885772705078, 281.97684812545776, 286.3651189804077, 290.63162636756897, 294.8981337547302, 299.23382210731506, 303.5695104598999, 307.941908121109, 312.3143057823181, 316.5539472103119, 320.79358863830566, 325.12879753112793, 329.4640064239502, 333.7034595012665, 337.94291257858276, 342.30976605415344, 346.6766195297241, 350.8680274486542, 355.05943536758423, 359.21952867507935, 363.37962198257446, 367.56426215171814, 371.7489023208618, 375.97436237335205, 380.1998224258423, 384.4700577259064, 388.74029302597046, 392.9415807723999, 397.14286851882935, 401.23060178756714, 405.31833505630493, 409.38491582870483, 413.45149660110474, 417.4913535118103, 421.53121042251587, 425.56199169158936, 429.59277296066284, 433.671781539917, 437.75079011917114, 441.86232233047485, 445.97385454177856, 450.17245864868164, 454.3710627555847, 458.5300920009613, 462.6891212463379, 466.8018317222595, 470.91454219818115, 475.03621006011963, 479.1578779220581, 483.2858006954193, 487.4137234687805, 491.5661518573761, 495.7185802459717, 499.8856153488159, 504.05265045166016, 508.2787573337555, 512.5048642158508, 516.723846912384, 520.9428296089172, 525.0852847099304, 529.2277398109436, 533.5401747226715, 537.8526096343994, 541.9142625331879, 545.9759154319763, 550.0372197628021, 554.0985240936279, 558.569452047348, 563.0403800010681, 567.0353543758392, 571.0303287506104, 575.0294580459595, 579.0285873413086, 583.0223581790924, 587.0161290168762, 591.0117428302765, 595.0073566436768, 598.9968047142029, 602.986252784729, 606.9794325828552, 610.9726123809814, 614.9608867168427, 618.9491610527039, 622.9292068481445, 626.9092526435852, 630.8996818065643, 634.8901109695435, 638.8815846443176, 642.8730583190918, 646.8733267784119, 650.8735952377319, 654.8523387908936, 658.8310823440552, 662.792097568512, 666.7531127929688, 670.7510826587677, 674.7490525245667, 678.702887058258, 682.6567215919495, 686.6269021034241, 690.5970826148987, 694.5587449073792, 698.5204071998596, 702.485880613327, 706.4513540267944, 710.4407904148102, 714.4302268028259, 718.4133439064026, 722.3964610099792, 726.3763027191162, 730.3561444282532, 734.3476033210754, 738.3390622138977, 742.3341073989868, 746.3291525840759, 750.1870129108429, 754.0448732376099, 757.9007127285004, 761.7565522193909, 765.6282193660736, 769.4998865127563, 773.9961607456207, 778.4924349784851, 782.3484799861908, 786.2045249938965, 790.0555984973907, 793.906672000885, 797.7774755954742, 801.6482791900635, 805.5035607814789, 809.3588423728943, 813.2316932678223, 817.1045441627502, 820.9671084880829, 824.8296728134155, 828.6979494094849, 832.5662260055542, 836.4309256076813, 840.2956252098083, 842.2407028675079, 844.1857805252075]
[27.755, 27.755, 34.1475, 34.1475, 34.745, 34.745, 36.1525, 36.1525, 36.775, 36.775, 36.9375, 36.9375, 35.95, 35.95, 36.93, 36.93, 37.5775, 37.5775, 37.5175, 37.5175, 37.7125, 37.7125, 36.895, 36.895, 37.165, 37.165, 37.295, 37.295, 37.1775, 37.1775, 37.535, 37.535, 37.5875, 37.5875, 37.0625, 37.0625, 37.06, 37.06, 36.68, 36.68, 36.7325, 36.7325, 36.33, 36.33, 36.3225, 36.3225, 35.5075, 35.5075, 35.5325, 35.5325, 34.84, 34.84, 34.845, 34.845, 34.32, 34.32, 34.1225, 34.1225, 33.915, 33.915, 33.685, 33.685, 33.5125, 33.5125, 32.9625, 32.9625, 32.64, 32.64, 32.1375, 32.1375, 31.8225, 31.8225, 31.72, 31.72, 31.9675, 31.9675, 31.61, 31.61, 31.3075, 31.3075, 31.4, 31.4, 31.09, 31.09, 30.95, 30.95, 30.6375, 30.6375, 30.3325, 30.3325, 30.1825, 30.1825, 30.505, 30.505, 30.3225, 30.3225, 30.1275, 30.1275, 30.03, 30.03, 30.14, 30.14, 29.8, 29.8, 29.3875, 29.3875, 28.99, 28.99, 28.9225, 28.9225, 29.0475, 29.0475, 28.995, 28.995, 28.85, 28.85, 28.845, 28.845, 28.8225, 28.8225, 28.9275, 28.9275, 28.7425, 28.7425, 28.3825, 28.3825, 28.7125, 28.7125, 28.4625, 28.4625, 28.2575, 28.2575, 28.15, 28.15, 28.33, 28.33, 27.9775, 27.9775, 27.945, 27.945, 28.3475, 28.3475, 28.125, 28.125, 28.145, 28.145, 28.2625, 28.2625, 28.2575, 28.2575, 28.495, 28.495, 28.4625, 28.4625, 28.6475, 28.6475, 28.3125, 28.3125, 28.24, 28.24, 28.6125, 28.6125, 28.375, 28.375, 28.4225, 28.4225, 28.3175, 28.3175, 28.3825, 28.3825, 28.3275, 28.3275, 28.5, 28.5, 28.4, 28.4, 28.305, 28.305, 28.37, 28.37, 27.97, 27.97, 28.1675, 28.1675, 28.1725, 28.1725, 27.825, 27.825, 28.0325, 28.0325, 28.24, 28.24, 28.38, 28.38, 27.9725, 27.9725, 27.8125, 27.8125, 27.605, 27.605, 28.1025, 28.1025]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 18, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.156, Test loss: 2.056, Test accuracy: 29.42
Round   0, Global train loss: 2.156, Global test loss: 2.059, Global test accuracy: 31.00
Round   1, Train loss: 2.019, Test loss: 1.850, Test accuracy: 35.37
Round   1, Global train loss: 2.019, Global test loss: 1.776, Global test accuracy: 39.63
Round   2, Train loss: 1.982, Test loss: 1.779, Test accuracy: 39.57
Round   2, Global train loss: 1.982, Global test loss: 1.710, Global test accuracy: 45.89
Round   3, Train loss: 1.878, Test loss: 1.709, Test accuracy: 40.28
Round   3, Global train loss: 1.878, Global test loss: 1.576, Global test accuracy: 47.40
Round   4, Train loss: 1.837, Test loss: 1.674, Test accuracy: 42.22
Round   4, Global train loss: 1.837, Global test loss: 1.516, Global test accuracy: 50.47
Round   5, Train loss: 1.800, Test loss: 1.630, Test accuracy: 44.59
Round   5, Global train loss: 1.800, Global test loss: 1.451, Global test accuracy: 53.54
Round   6, Train loss: 1.771, Test loss: 1.628, Test accuracy: 44.62
Round   6, Global train loss: 1.771, Global test loss: 1.448, Global test accuracy: 55.30
Round   7, Train loss: 1.749, Test loss: 1.598, Test accuracy: 46.00
Round   7, Global train loss: 1.749, Global test loss: 1.393, Global test accuracy: 58.05
Round   8, Train loss: 1.642, Test loss: 1.564, Test accuracy: 47.78
Round   8, Global train loss: 1.642, Global test loss: 1.320, Global test accuracy: 59.32
Round   9, Train loss: 1.608, Test loss: 1.534, Test accuracy: 49.34
Round   9, Global train loss: 1.608, Global test loss: 1.258, Global test accuracy: 61.79
Round  10, Train loss: 1.728, Test loss: 1.477, Test accuracy: 52.47
Round  10, Global train loss: 1.728, Global test loss: 1.338, Global test accuracy: 61.35
Round  11, Train loss: 1.548, Test loss: 1.476, Test accuracy: 52.26
Round  11, Global train loss: 1.548, Global test loss: 1.191, Global test accuracy: 63.23
Round  12, Train loss: 1.619, Test loss: 1.459, Test accuracy: 52.80
Round  12, Global train loss: 1.619, Global test loss: 1.223, Global test accuracy: 63.86
Round  13, Train loss: 1.624, Test loss: 1.447, Test accuracy: 53.21
Round  13, Global train loss: 1.624, Global test loss: 1.249, Global test accuracy: 64.05
Round  14, Train loss: 1.593, Test loss: 1.437, Test accuracy: 53.23
Round  14, Global train loss: 1.593, Global test loss: 1.216, Global test accuracy: 65.08
Round  15, Train loss: 1.570, Test loss: 1.429, Test accuracy: 53.78
Round  15, Global train loss: 1.570, Global test loss: 1.207, Global test accuracy: 65.62
Round  16, Train loss: 1.503, Test loss: 1.418, Test accuracy: 54.15
Round  16, Global train loss: 1.503, Global test loss: 1.117, Global test accuracy: 66.22
Round  17, Train loss: 1.469, Test loss: 1.409, Test accuracy: 54.58
Round  17, Global train loss: 1.469, Global test loss: 1.130, Global test accuracy: 66.37
Round  18, Train loss: 1.537, Test loss: 1.393, Test accuracy: 55.03
Round  18, Global train loss: 1.537, Global test loss: 1.160, Global test accuracy: 65.78
Round  19, Train loss: 1.440, Test loss: 1.395, Test accuracy: 55.16
Round  19, Global train loss: 1.440, Global test loss: 1.105, Global test accuracy: 66.84
Round  20, Train loss: 1.393, Test loss: 1.394, Test accuracy: 55.49
Round  20, Global train loss: 1.393, Global test loss: 1.082, Global test accuracy: 67.58
Round  21, Train loss: 1.434, Test loss: 1.391, Test accuracy: 55.48
Round  21, Global train loss: 1.434, Global test loss: 1.090, Global test accuracy: 67.49
Round  22, Train loss: 1.442, Test loss: 1.377, Test accuracy: 55.91
Round  22, Global train loss: 1.442, Global test loss: 1.100, Global test accuracy: 66.97
Round  23, Train loss: 1.436, Test loss: 1.381, Test accuracy: 55.89
Round  23, Global train loss: 1.436, Global test loss: 1.105, Global test accuracy: 67.17
Round  24, Train loss: 1.436, Test loss: 1.379, Test accuracy: 55.85
Round  24, Global train loss: 1.436, Global test loss: 1.083, Global test accuracy: 68.22
Round  25, Train loss: 1.395, Test loss: 1.395, Test accuracy: 55.71
Round  25, Global train loss: 1.395, Global test loss: 1.088, Global test accuracy: 67.11
Round  26, Train loss: 1.372, Test loss: 1.396, Test accuracy: 55.62
Round  26, Global train loss: 1.372, Global test loss: 1.097, Global test accuracy: 67.44
Round  27, Train loss: 1.352, Test loss: 1.369, Test accuracy: 56.61
Round  27, Global train loss: 1.352, Global test loss: 1.020, Global test accuracy: 68.58
Round  28, Train loss: 1.276, Test loss: 1.370, Test accuracy: 56.71
Round  28, Global train loss: 1.276, Global test loss: 1.003, Global test accuracy: 69.05
Round  29, Train loss: 1.205, Test loss: 1.357, Test accuracy: 57.23
Round  29, Global train loss: 1.205, Global test loss: 0.971, Global test accuracy: 69.40
Round  30, Train loss: 1.321, Test loss: 1.350, Test accuracy: 57.47
Round  30, Global train loss: 1.321, Global test loss: 1.029, Global test accuracy: 69.39
Round  31, Train loss: 1.392, Test loss: 1.347, Test accuracy: 57.87
Round  31, Global train loss: 1.392, Global test loss: 1.079, Global test accuracy: 69.25
Round  32, Train loss: 1.320, Test loss: 1.347, Test accuracy: 57.92
Round  32, Global train loss: 1.320, Global test loss: 1.020, Global test accuracy: 69.72
Round  33, Train loss: 1.292, Test loss: 1.339, Test accuracy: 58.24
Round  33, Global train loss: 1.292, Global test loss: 1.000, Global test accuracy: 69.78
Round  34, Train loss: 1.224, Test loss: 1.344, Test accuracy: 57.98
Round  34, Global train loss: 1.224, Global test loss: 0.992, Global test accuracy: 69.77
Round  35, Train loss: 1.252, Test loss: 1.363, Test accuracy: 57.34
Round  35, Global train loss: 1.252, Global test loss: 0.979, Global test accuracy: 69.89
Round  36, Train loss: 1.202, Test loss: 1.358, Test accuracy: 57.53
Round  36, Global train loss: 1.202, Global test loss: 0.983, Global test accuracy: 69.48
Round  37, Train loss: 1.311, Test loss: 1.360, Test accuracy: 57.56
Round  37, Global train loss: 1.311, Global test loss: 1.027, Global test accuracy: 69.50
Round  38, Train loss: 1.272, Test loss: 1.359, Test accuracy: 57.71
Round  38, Global train loss: 1.272, Global test loss: 1.047, Global test accuracy: 68.88
Round  39, Train loss: 1.193, Test loss: 1.361, Test accuracy: 57.73
Round  39, Global train loss: 1.193, Global test loss: 0.955, Global test accuracy: 70.42
Round  40, Train loss: 1.202, Test loss: 1.376, Test accuracy: 57.39
Round  40, Global train loss: 1.202, Global test loss: 0.987, Global test accuracy: 70.33
Round  41, Train loss: 1.216, Test loss: 1.396, Test accuracy: 56.77
Round  41, Global train loss: 1.216, Global test loss: 1.037, Global test accuracy: 68.94
Round  42, Train loss: 1.230, Test loss: 1.382, Test accuracy: 57.34
Round  42, Global train loss: 1.230, Global test loss: 1.017, Global test accuracy: 69.65
Round  43, Train loss: 1.416, Test loss: 1.368, Test accuracy: 57.72
Round  43, Global train loss: 1.416, Global test loss: 1.110, Global test accuracy: 68.49
Round  44, Train loss: 1.256, Test loss: 1.368, Test accuracy: 57.76
Round  44, Global train loss: 1.256, Global test loss: 1.050, Global test accuracy: 68.37
Round  45, Train loss: 1.364, Test loss: 1.381, Test accuracy: 57.30
Round  45, Global train loss: 1.364, Global test loss: 1.104, Global test accuracy: 67.96
Round  46, Train loss: 1.135, Test loss: 1.378, Test accuracy: 57.35
Round  46, Global train loss: 1.135, Global test loss: 0.978, Global test accuracy: 69.71
Round  47, Train loss: 1.140, Test loss: 1.370, Test accuracy: 57.81
Round  47, Global train loss: 1.140, Global test loss: 0.987, Global test accuracy: 69.83
Round  48, Train loss: 1.254, Test loss: 1.380, Test accuracy: 57.66
Round  48, Global train loss: 1.254, Global test loss: 1.015, Global test accuracy: 69.17
Round  49, Train loss: 1.128, Test loss: 1.380, Test accuracy: 57.63
Round  49, Global train loss: 1.128, Global test loss: 0.997, Global test accuracy: 69.11
Round  50, Train loss: 1.081, Test loss: 1.393, Test accuracy: 57.42
Round  50, Global train loss: 1.081, Global test loss: 0.974, Global test accuracy: 69.87
Round  51, Train loss: 1.190, Test loss: 1.401, Test accuracy: 57.35
Round  51, Global train loss: 1.190, Global test loss: 0.987, Global test accuracy: 70.20
Round  52, Train loss: 1.154, Test loss: 1.415, Test accuracy: 56.94
Round  52, Global train loss: 1.154, Global test loss: 0.992, Global test accuracy: 69.54
Round  53, Train loss: 1.057, Test loss: 1.395, Test accuracy: 57.47
Round  53, Global train loss: 1.057, Global test loss: 0.966, Global test accuracy: 70.42
Round  54, Train loss: 1.306, Test loss: 1.391, Test accuracy: 57.52
Round  54, Global train loss: 1.306, Global test loss: 1.096, Global test accuracy: 68.31
Round  55, Train loss: 1.212, Test loss: 1.393, Test accuracy: 57.43
Round  55, Global train loss: 1.212, Global test loss: 1.004, Global test accuracy: 69.73
Round  56, Train loss: 1.114, Test loss: 1.388, Test accuracy: 57.47
Round  56, Global train loss: 1.114, Global test loss: 0.963, Global test accuracy: 69.76
Round  57, Train loss: 1.122, Test loss: 1.386, Test accuracy: 57.52
Round  57, Global train loss: 1.122, Global test loss: 0.979, Global test accuracy: 70.19
Round  58, Train loss: 1.206, Test loss: 1.389, Test accuracy: 57.53
Round  58, Global train loss: 1.206, Global test loss: 1.017, Global test accuracy: 69.11
Round  59, Train loss: 1.069, Test loss: 1.390, Test accuracy: 57.36
Round  59, Global train loss: 1.069, Global test loss: 0.985, Global test accuracy: 69.61
Round  60, Train loss: 1.148, Test loss: 1.386, Test accuracy: 57.55
Round  60, Global train loss: 1.148, Global test loss: 0.980, Global test accuracy: 69.55
Round  61, Train loss: 1.021, Test loss: 1.382, Test accuracy: 57.85
Round  61, Global train loss: 1.021, Global test loss: 1.006, Global test accuracy: 68.39
Round  62, Train loss: 1.215, Test loss: 1.414, Test accuracy: 57.11
Round  62, Global train loss: 1.215, Global test loss: 1.079, Global test accuracy: 67.42
Round  63, Train loss: 1.118, Test loss: 1.421, Test accuracy: 57.17
Round  63, Global train loss: 1.118, Global test loss: 1.006, Global test accuracy: 69.06
Round  64, Train loss: 1.118, Test loss: 1.432, Test accuracy: 57.32
Round  64, Global train loss: 1.118, Global test loss: 1.004, Global test accuracy: 69.23
Round  65, Train loss: 1.103, Test loss: 1.440, Test accuracy: 57.10
Round  65, Global train loss: 1.103, Global test loss: 0.997, Global test accuracy: 69.70
Round  66, Train loss: 1.125, Test loss: 1.429, Test accuracy: 57.47
Round  66, Global train loss: 1.125, Global test loss: 1.012, Global test accuracy: 68.81
Round  67, Train loss: 1.211, Test loss: 1.429, Test accuracy: 57.34
Round  67, Global train loss: 1.211, Global test loss: 1.039, Global test accuracy: 68.80
Round  68, Train loss: 1.091, Test loss: 1.431, Test accuracy: 57.22
Round  68, Global train loss: 1.091, Global test loss: 1.001, Global test accuracy: 69.14
Round  69, Train loss: 1.044, Test loss: 1.443, Test accuracy: 56.99
Round  69, Global train loss: 1.044, Global test loss: 0.971, Global test accuracy: 69.62
Round  70, Train loss: 1.128, Test loss: 1.438, Test accuracy: 57.31
Round  70, Global train loss: 1.128, Global test loss: 1.054, Global test accuracy: 68.63
Round  71, Train loss: 1.045, Test loss: 1.438, Test accuracy: 57.05
Round  71, Global train loss: 1.045, Global test loss: 1.021, Global test accuracy: 68.23
Round  72, Train loss: 1.048, Test loss: 1.434, Test accuracy: 57.08
Round  72, Global train loss: 1.048, Global test loss: 1.029, Global test accuracy: 68.09
Round  73, Train loss: 1.300, Test loss: 1.436, Test accuracy: 56.87
Round  73, Global train loss: 1.300, Global test loss: 1.102, Global test accuracy: 68.03
Round  74, Train loss: 1.106, Test loss: 1.442, Test accuracy: 56.79
Round  74, Global train loss: 1.106, Global test loss: 1.085, Global test accuracy: 66.45
Round  75, Train loss: 1.014, Test loss: 1.446, Test accuracy: 56.69
Round  75, Global train loss: 1.014, Global test loss: 0.966, Global test accuracy: 69.94
Round  76, Train loss: 1.076, Test loss: 1.433, Test accuracy: 56.80
Round  76, Global train loss: 1.076, Global test loss: 0.986, Global test accuracy: 69.66
Round  77, Train loss: 1.050, Test loss: 1.447, Test accuracy: 56.67
Round  77, Global train loss: 1.050, Global test loss: 1.010, Global test accuracy: 68.92
Round  78, Train loss: 0.997, Test loss: 1.455, Test accuracy: 56.56
Round  78, Global train loss: 0.997, Global test loss: 0.998, Global test accuracy: 68.94
Round  79, Train loss: 0.955, Test loss: 1.469, Test accuracy: 56.27
Round  79, Global train loss: 0.955, Global test loss: 1.012, Global test accuracy: 68.88
Round  80, Train loss: 1.018, Test loss: 1.476, Test accuracy: 56.18
Round  80, Global train loss: 1.018, Global test loss: 1.036, Global test accuracy: 68.32
Round  81, Train loss: 1.183, Test loss: 1.491, Test accuracy: 55.86
Round  81, Global train loss: 1.183, Global test loss: 1.094, Global test accuracy: 67.31
Round  82, Train loss: 1.086, Test loss: 1.496, Test accuracy: 55.84
Round  82, Global train loss: 1.086, Global test loss: 1.087, Global test accuracy: 67.01
Round  83, Train loss: 1.131, Test loss: 1.505, Test accuracy: 55.71
Round  83, Global train loss: 1.131, Global test loss: 1.062, Global test accuracy: 67.75
Round  84, Train loss: 1.054, Test loss: 1.494, Test accuracy: 56.19
Round  84, Global train loss: 1.054, Global test loss: 1.064, Global test accuracy: 67.65
Round  85, Train loss: 1.173, Test loss: 1.493, Test accuracy: 56.22
Round  85, Global train loss: 1.173, Global test loss: 1.084, Global test accuracy: 67.96
Round  86, Train loss: 1.067, Test loss: 1.500, Test accuracy: 56.24
Round  86, Global train loss: 1.067, Global test loss: 1.056, Global test accuracy: 67.70
Round  87, Train loss: 1.053, Test loss: 1.496, Test accuracy: 56.55
Round  87, Global train loss: 1.053, Global test loss: 1.048, Global test accuracy: 67.68
Round  88, Train loss: 1.083, Test loss: 1.493, Test accuracy: 56.49
Round  88, Global train loss: 1.083, Global test loss: 1.116, Global test accuracy: 66.06
Round  89, Train loss: 0.940, Test loss: 1.495, Test accuracy: 56.37
Round  89, Global train loss: 0.940, Global test loss: 1.010, Global test accuracy: 68.84
Round  90, Train loss: 1.052, Test loss: 1.473, Test accuracy: 57.02
Round  90, Global train loss: 1.052, Global test loss: 1.034, Global test accuracy: 68.11
Round  91, Train loss: 1.030, Test loss: 1.478, Test accuracy: 57.01
Round  91, Global train loss: 1.030, Global test loss: 1.073, Global test accuracy: 67.53
Round  92, Train loss: 1.047, Test loss: 1.480, Test accuracy: 56.92
Round  92, Global train loss: 1.047, Global test loss: 1.055, Global test accuracy: 67.82
Round  93, Train loss: 1.090, Test loss: 1.504, Test accuracy: 56.58
Round  93, Global train loss: 1.090, Global test loss: 1.097, Global test accuracy: 65.84
Round  94, Train loss: 1.110, Test loss: 1.516, Test accuracy: 56.19
Round  94, Global train loss: 1.110, Global test loss: 1.038, Global test accuracy: 67.73
Round  95, Train loss: 1.108, Test loss: 1.519, Test accuracy: 56.13
Round  95, Global train loss: 1.108, Global test loss: 1.102, Global test accuracy: 66.78
Round  96, Train loss: 1.097, Test loss: 1.530, Test accuracy: 55.85
Round  96, Global train loss: 1.097, Global test loss: 1.104, Global test accuracy: 66.85
Round  97, Train loss: 1.151, Test loss: 1.527, Test accuracy: 55.97
Round  97, Global train loss: 1.151, Global test loss: 1.164, Global test accuracy: 64.30
Round  98, Train loss: 0.934, Test loss: 1.520, Test accuracy: 56.20
Round  98, Global train loss: 0.934, Global test loss: 1.012, Global test accuracy: 69.30
Round  99, Train loss: 0.908, Test loss: 1.527, Test accuracy: 56.09
Round  99, Global train loss: 0.908, Global test loss: 1.034, Global test accuracy: 68.52
Final Round, Train loss: 0.685, Test loss: 1.741, Test accuracy: 55.57
Final Round, Global train loss: 0.685, Global test loss: 1.034, Global test accuracy: 68.52
Average accuracy final 10 rounds: 56.396750000000004 

Average global accuracy final 10 rounds: 67.2775 

5382.275552034378
[4.48708701133728, 8.97417402267456, 13.029908657073975, 17.08564329147339, 21.135106086730957, 25.184568881988525, 29.223387241363525, 33.262205600738525, 37.3952910900116, 41.52837657928467, 45.7088897228241, 49.889402866363525, 54.109277963638306, 58.329153060913086, 62.44525337219238, 66.56135368347168, 70.69646000862122, 74.83156633377075, 78.96364736557007, 83.09572839736938, 87.14379906654358, 91.19186973571777, 95.33719992637634, 99.48253011703491, 103.55067682266235, 107.6188235282898, 111.70172333717346, 115.78462314605713, 119.85531234741211, 123.92600154876709, 127.980384349823, 132.0347671508789, 136.10442638397217, 140.17408561706543, 144.26589846611023, 148.35771131515503, 152.42967557907104, 156.50163984298706, 161.1072018146515, 165.71276378631592, 169.77454209327698, 173.83632040023804, 177.99034070968628, 182.14436101913452, 186.14832019805908, 190.15227937698364, 194.21062707901, 198.26897478103638, 202.8705644607544, 207.4721541404724, 211.54638266563416, 215.6206111907959, 219.74160170555115, 223.8625922203064, 227.96753454208374, 232.07247686386108, 236.18718647956848, 240.30189609527588, 244.4186475276947, 248.53539896011353, 252.64374589920044, 256.75209283828735, 260.8168160915375, 264.8815393447876, 268.9642629623413, 273.046986579895, 277.1454679965973, 281.24394941329956, 285.3469157218933, 289.44988203048706, 293.5195298194885, 297.58917760849, 301.67299818992615, 305.7568187713623, 309.9174482822418, 314.07807779312134, 318.2181701660156, 322.3582625389099, 326.95024847984314, 331.54223442077637, 335.7673177719116, 339.9924011230469, 344.47733426094055, 348.96226739883423, 353.1766936779022, 357.3911199569702, 361.78326058387756, 366.1754012107849, 370.83618903160095, 375.496976852417, 379.7833254337311, 384.06967401504517, 388.3472623825073, 392.6248507499695, 396.92569947242737, 401.22654819488525, 405.54648756980896, 409.86642694473267, 414.1845455169678, 418.5026640892029, 422.78062176704407, 427.05857944488525, 431.29971718788147, 435.5408549308777, 439.8422989845276, 444.1437430381775, 448.4615330696106, 452.7793231010437, 457.1077811717987, 461.4362392425537, 465.79944014549255, 470.1626410484314, 474.60609006881714, 479.0495390892029, 483.3509941101074, 487.65244913101196, 491.9406156539917, 496.22878217697144, 500.50478863716125, 504.7807950973511, 509.08040976524353, 513.380024433136, 517.6567010879517, 521.9333777427673, 526.1010456085205, 530.2687134742737, 534.469654083252, 538.6705946922302, 542.9569683074951, 547.24334192276, 551.5624098777771, 555.8814778327942, 560.214816570282, 564.5481553077698, 568.8606016635895, 573.1730480194092, 577.4101531505585, 581.6472582817078, 585.8992300033569, 590.1512017250061, 594.3879458904266, 598.6246900558472, 602.8643431663513, 607.1039962768555, 611.4146115779877, 615.7252268791199, 620.0436930656433, 624.3621592521667, 628.681069612503, 632.9999799728394, 637.2991998195648, 641.5984196662903, 645.8657922744751, 650.1331648826599, 654.3640522956848, 658.5949397087097, 662.8653061389923, 667.1356725692749, 671.4651000499725, 675.7945275306702, 680.14186835289, 684.4892091751099, 688.7898848056793, 693.0905604362488, 697.4238798618317, 701.7571992874146, 706.0958099365234, 710.4344205856323, 714.7792601585388, 719.1240997314453, 723.4687359333038, 727.8133721351624, 732.1363565921783, 736.4593410491943, 740.8079514503479, 745.1565618515015, 749.838196516037, 754.5198311805725, 759.2981503009796, 764.0764694213867, 768.4042253494263, 772.7319812774658, 777.0366897583008, 781.3413982391357, 785.6108543872833, 789.8803105354309, 794.1522922515869, 798.4242739677429, 802.7023746967316, 806.9804754257202, 811.2573189735413, 815.5341625213623, 819.8294801712036, 824.1247978210449, 828.3916413784027, 832.6584849357605, 836.9253873825073, 841.1922898292542, 845.3648488521576, 849.537407875061, 851.6510517597198, 853.7646956443787]
[29.4225, 29.4225, 35.37, 35.37, 39.57, 39.57, 40.28, 40.28, 42.2175, 42.2175, 44.5875, 44.5875, 44.625, 44.625, 46.0, 46.0, 47.7825, 47.7825, 49.345, 49.345, 52.4725, 52.4725, 52.2575, 52.2575, 52.8, 52.8, 53.2125, 53.2125, 53.23, 53.23, 53.785, 53.785, 54.145, 54.145, 54.5775, 54.5775, 55.0325, 55.0325, 55.1575, 55.1575, 55.4875, 55.4875, 55.485, 55.485, 55.9075, 55.9075, 55.8925, 55.8925, 55.8525, 55.8525, 55.7075, 55.7075, 55.6175, 55.6175, 56.6125, 56.6125, 56.7125, 56.7125, 57.225, 57.225, 57.4725, 57.4725, 57.87, 57.87, 57.925, 57.925, 58.245, 58.245, 57.9775, 57.9775, 57.3425, 57.3425, 57.5275, 57.5275, 57.5575, 57.5575, 57.7075, 57.7075, 57.725, 57.725, 57.3875, 57.3875, 56.765, 56.765, 57.34, 57.34, 57.7225, 57.7225, 57.7625, 57.7625, 57.3, 57.3, 57.355, 57.355, 57.8125, 57.8125, 57.655, 57.655, 57.6275, 57.6275, 57.42, 57.42, 57.35, 57.35, 56.935, 56.935, 57.465, 57.465, 57.515, 57.515, 57.4325, 57.4325, 57.4675, 57.4675, 57.5225, 57.5225, 57.53, 57.53, 57.3625, 57.3625, 57.55, 57.55, 57.8525, 57.8525, 57.11, 57.11, 57.1675, 57.1675, 57.3225, 57.3225, 57.105, 57.105, 57.4725, 57.4725, 57.3425, 57.3425, 57.22, 57.22, 56.9925, 56.9925, 57.3075, 57.3075, 57.05, 57.05, 57.0775, 57.0775, 56.8675, 56.8675, 56.7925, 56.7925, 56.6875, 56.6875, 56.805, 56.805, 56.6725, 56.6725, 56.5625, 56.5625, 56.2675, 56.2675, 56.1825, 56.1825, 55.8575, 55.8575, 55.84, 55.84, 55.7075, 55.7075, 56.19, 56.19, 56.2175, 56.2175, 56.2375, 56.2375, 56.545, 56.545, 56.495, 56.495, 56.3675, 56.3675, 57.02, 57.02, 57.0075, 57.0075, 56.9225, 56.9225, 56.58, 56.58, 56.1925, 56.1925, 56.135, 56.135, 55.855, 55.855, 55.9675, 55.9675, 56.2, 56.2, 56.0875, 56.0875, 55.5725, 55.5725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 16, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.179, Test loss: 2.121, Test accuracy: 22.28
Round   0, Global train loss: 2.179, Global test loss: 2.121, Global test accuracy: 22.74
Round   1, Train loss: 2.062, Test loss: 1.959, Test accuracy: 29.69
Round   1, Global train loss: 2.062, Global test loss: 1.890, Global test accuracy: 33.70
Round   2, Train loss: 2.011, Test loss: 1.885, Test accuracy: 32.37
Round   2, Global train loss: 2.011, Global test loss: 1.796, Global test accuracy: 36.27
Round   3, Train loss: 1.948, Test loss: 1.825, Test accuracy: 35.78
Round   3, Global train loss: 1.948, Global test loss: 1.695, Global test accuracy: 43.76
Round   4, Train loss: 1.921, Test loss: 1.788, Test accuracy: 36.26
Round   4, Global train loss: 1.921, Global test loss: 1.652, Global test accuracy: 43.18
Round   5, Train loss: 1.932, Test loss: 1.742, Test accuracy: 38.55
Round   5, Global train loss: 1.932, Global test loss: 1.617, Global test accuracy: 46.50
Round   6, Train loss: 1.912, Test loss: 1.743, Test accuracy: 38.20
Round   6, Global train loss: 1.912, Global test loss: 1.605, Global test accuracy: 46.30
Round   7, Train loss: 1.863, Test loss: 1.725, Test accuracy: 39.28
Round   7, Global train loss: 1.863, Global test loss: 1.555, Global test accuracy: 49.45
Round   8, Train loss: 1.875, Test loss: 1.713, Test accuracy: 39.66
Round   8, Global train loss: 1.875, Global test loss: 1.548, Global test accuracy: 49.05
Round   9, Train loss: 1.784, Test loss: 1.679, Test accuracy: 41.29
Round   9, Global train loss: 1.784, Global test loss: 1.472, Global test accuracy: 52.65
Round  10, Train loss: 1.811, Test loss: 1.631, Test accuracy: 44.12
Round  10, Global train loss: 1.811, Global test loss: 1.469, Global test accuracy: 54.57
Round  11, Train loss: 1.810, Test loss: 1.624, Test accuracy: 44.39
Round  11, Global train loss: 1.810, Global test loss: 1.478, Global test accuracy: 54.35
Round  12, Train loss: 1.817, Test loss: 1.601, Test accuracy: 45.65
Round  12, Global train loss: 1.817, Global test loss: 1.465, Global test accuracy: 56.67
Round  13, Train loss: 1.724, Test loss: 1.579, Test accuracy: 46.77
Round  13, Global train loss: 1.724, Global test loss: 1.385, Global test accuracy: 57.03
Round  14, Train loss: 1.710, Test loss: 1.576, Test accuracy: 46.90
Round  14, Global train loss: 1.710, Global test loss: 1.366, Global test accuracy: 56.97
Round  15, Train loss: 1.695, Test loss: 1.572, Test accuracy: 47.16
Round  15, Global train loss: 1.695, Global test loss: 1.331, Global test accuracy: 59.50
Round  16, Train loss: 1.657, Test loss: 1.541, Test accuracy: 48.62
Round  16, Global train loss: 1.657, Global test loss: 1.310, Global test accuracy: 60.19
Round  17, Train loss: 1.685, Test loss: 1.539, Test accuracy: 48.81
Round  17, Global train loss: 1.685, Global test loss: 1.332, Global test accuracy: 60.86
Round  18, Train loss: 1.730, Test loss: 1.527, Test accuracy: 49.27
Round  18, Global train loss: 1.730, Global test loss: 1.324, Global test accuracy: 60.90
Round  19, Train loss: 1.781, Test loss: 1.520, Test accuracy: 49.95
Round  19, Global train loss: 1.781, Global test loss: 1.403, Global test accuracy: 60.74
Round  20, Train loss: 1.714, Test loss: 1.485, Test accuracy: 51.74
Round  20, Global train loss: 1.714, Global test loss: 1.304, Global test accuracy: 61.52
Round  21, Train loss: 1.789, Test loss: 1.475, Test accuracy: 52.26
Round  21, Global train loss: 1.789, Global test loss: 1.380, Global test accuracy: 62.51
Round  22, Train loss: 1.611, Test loss: 1.469, Test accuracy: 52.77
Round  22, Global train loss: 1.611, Global test loss: 1.259, Global test accuracy: 62.71
Round  23, Train loss: 1.604, Test loss: 1.465, Test accuracy: 52.82
Round  23, Global train loss: 1.604, Global test loss: 1.247, Global test accuracy: 63.22
Round  24, Train loss: 1.662, Test loss: 1.448, Test accuracy: 53.41
Round  24, Global train loss: 1.662, Global test loss: 1.283, Global test accuracy: 62.76
Round  25, Train loss: 1.699, Test loss: 1.441, Test accuracy: 53.68
Round  25, Global train loss: 1.699, Global test loss: 1.279, Global test accuracy: 62.28
Round  26, Train loss: 1.578, Test loss: 1.438, Test accuracy: 53.88
Round  26, Global train loss: 1.578, Global test loss: 1.241, Global test accuracy: 63.56
Round  27, Train loss: 1.521, Test loss: 1.423, Test accuracy: 54.66
Round  27, Global train loss: 1.521, Global test loss: 1.183, Global test accuracy: 63.50
Round  28, Train loss: 1.576, Test loss: 1.414, Test accuracy: 54.88
Round  28, Global train loss: 1.576, Global test loss: 1.212, Global test accuracy: 64.42
Round  29, Train loss: 1.636, Test loss: 1.421, Test accuracy: 54.81
Round  29, Global train loss: 1.636, Global test loss: 1.275, Global test accuracy: 64.81
Round  30, Train loss: 1.492, Test loss: 1.416, Test accuracy: 54.73
Round  30, Global train loss: 1.492, Global test loss: 1.174, Global test accuracy: 65.01
Round  31, Train loss: 1.663, Test loss: 1.395, Test accuracy: 55.35
Round  31, Global train loss: 1.663, Global test loss: 1.241, Global test accuracy: 65.00
Round  32, Train loss: 1.469, Test loss: 1.392, Test accuracy: 55.84
Round  32, Global train loss: 1.469, Global test loss: 1.173, Global test accuracy: 65.71
Round  33, Train loss: 1.563, Test loss: 1.379, Test accuracy: 56.12
Round  33, Global train loss: 1.563, Global test loss: 1.179, Global test accuracy: 66.00
Round  34, Train loss: 1.577, Test loss: 1.371, Test accuracy: 56.79
Round  34, Global train loss: 1.577, Global test loss: 1.207, Global test accuracy: 66.04
Round  35, Train loss: 1.468, Test loss: 1.368, Test accuracy: 56.77
Round  35, Global train loss: 1.468, Global test loss: 1.148, Global test accuracy: 66.53
Round  36, Train loss: 1.482, Test loss: 1.373, Test accuracy: 56.50
Round  36, Global train loss: 1.482, Global test loss: 1.185, Global test accuracy: 66.00
Round  37, Train loss: 1.539, Test loss: 1.366, Test accuracy: 56.72
Round  37, Global train loss: 1.539, Global test loss: 1.161, Global test accuracy: 67.10
Round  38, Train loss: 1.509, Test loss: 1.364, Test accuracy: 57.23
Round  38, Global train loss: 1.509, Global test loss: 1.145, Global test accuracy: 66.88
Round  39, Train loss: 1.628, Test loss: 1.367, Test accuracy: 57.02
Round  39, Global train loss: 1.628, Global test loss: 1.201, Global test accuracy: 67.15
Round  40, Train loss: 1.453, Test loss: 1.363, Test accuracy: 57.30
Round  40, Global train loss: 1.453, Global test loss: 1.130, Global test accuracy: 67.06
Round  41, Train loss: 1.562, Test loss: 1.355, Test accuracy: 57.38
Round  41, Global train loss: 1.562, Global test loss: 1.172, Global test accuracy: 66.73
Round  42, Train loss: 1.344, Test loss: 1.347, Test accuracy: 57.74
Round  42, Global train loss: 1.344, Global test loss: 1.079, Global test accuracy: 67.56
Round  43, Train loss: 1.538, Test loss: 1.344, Test accuracy: 57.83
Round  43, Global train loss: 1.538, Global test loss: 1.127, Global test accuracy: 67.83
Round  44, Train loss: 1.508, Test loss: 1.347, Test accuracy: 57.55
Round  44, Global train loss: 1.508, Global test loss: 1.142, Global test accuracy: 67.43
Round  45, Train loss: 1.496, Test loss: 1.336, Test accuracy: 57.91
Round  45, Global train loss: 1.496, Global test loss: 1.143, Global test accuracy: 67.53
Round  46, Train loss: 1.466, Test loss: 1.333, Test accuracy: 58.30
Round  46, Global train loss: 1.466, Global test loss: 1.129, Global test accuracy: 66.92
Round  47, Train loss: 1.359, Test loss: 1.328, Test accuracy: 58.31
Round  47, Global train loss: 1.359, Global test loss: 1.082, Global test accuracy: 67.81
Round  48, Train loss: 1.499, Test loss: 1.330, Test accuracy: 58.26
Round  48, Global train loss: 1.499, Global test loss: 1.118, Global test accuracy: 68.45
Round  49, Train loss: 1.357, Test loss: 1.329, Test accuracy: 58.37
Round  49, Global train loss: 1.357, Global test loss: 1.073, Global test accuracy: 67.84
Round  50, Train loss: 1.364, Test loss: 1.321, Test accuracy: 58.60
Round  50, Global train loss: 1.364, Global test loss: 1.061, Global test accuracy: 68.71
Round  51, Train loss: 1.539, Test loss: 1.312, Test accuracy: 59.03
Round  51, Global train loss: 1.539, Global test loss: 1.162, Global test accuracy: 67.42
Round  52, Train loss: 1.380, Test loss: 1.311, Test accuracy: 59.38
Round  52, Global train loss: 1.380, Global test loss: 1.074, Global test accuracy: 68.28
Round  53, Train loss: 1.358, Test loss: 1.309, Test accuracy: 59.67
Round  53, Global train loss: 1.358, Global test loss: 1.074, Global test accuracy: 67.73
Round  54, Train loss: 1.463, Test loss: 1.302, Test accuracy: 59.89
Round  54, Global train loss: 1.463, Global test loss: 1.117, Global test accuracy: 67.94
Round  55, Train loss: 1.496, Test loss: 1.291, Test accuracy: 60.47
Round  55, Global train loss: 1.496, Global test loss: 1.108, Global test accuracy: 68.42
Round  56, Train loss: 1.445, Test loss: 1.305, Test accuracy: 59.76
Round  56, Global train loss: 1.445, Global test loss: 1.088, Global test accuracy: 69.05
Round  57, Train loss: 1.449, Test loss: 1.299, Test accuracy: 59.93
Round  57, Global train loss: 1.449, Global test loss: 1.104, Global test accuracy: 67.84
Round  58, Train loss: 1.344, Test loss: 1.303, Test accuracy: 59.82
Round  58, Global train loss: 1.344, Global test loss: 1.041, Global test accuracy: 69.39
Round  59, Train loss: 1.363, Test loss: 1.300, Test accuracy: 59.80
Round  59, Global train loss: 1.363, Global test loss: 1.060, Global test accuracy: 68.19
Round  60, Train loss: 1.466, Test loss: 1.286, Test accuracy: 60.22
Round  60, Global train loss: 1.466, Global test loss: 1.120, Global test accuracy: 68.18
Round  61, Train loss: 1.361, Test loss: 1.301, Test accuracy: 59.62
Round  61, Global train loss: 1.361, Global test loss: 1.121, Global test accuracy: 67.29
Round  62, Train loss: 1.329, Test loss: 1.301, Test accuracy: 59.60
Round  62, Global train loss: 1.329, Global test loss: 1.048, Global test accuracy: 69.69
Round  63, Train loss: 1.404, Test loss: 1.302, Test accuracy: 59.57
Round  63, Global train loss: 1.404, Global test loss: 1.044, Global test accuracy: 69.49
Round  64, Train loss: 1.460, Test loss: 1.298, Test accuracy: 59.84
Round  64, Global train loss: 1.460, Global test loss: 1.127, Global test accuracy: 68.66
Round  65, Train loss: 1.314, Test loss: 1.300, Test accuracy: 59.79
Round  65, Global train loss: 1.314, Global test loss: 1.060, Global test accuracy: 68.31
Round  66, Train loss: 1.449, Test loss: 1.299, Test accuracy: 59.66
Round  66, Global train loss: 1.449, Global test loss: 1.100, Global test accuracy: 68.83
Round  67, Train loss: 1.267, Test loss: 1.293, Test accuracy: 59.86
Round  67, Global train loss: 1.267, Global test loss: 1.021, Global test accuracy: 69.16
Round  68, Train loss: 1.415, Test loss: 1.308, Test accuracy: 59.42
Round  68, Global train loss: 1.415, Global test loss: 1.081, Global test accuracy: 68.97
Round  69, Train loss: 1.218, Test loss: 1.304, Test accuracy: 59.62
Round  69, Global train loss: 1.218, Global test loss: 1.010, Global test accuracy: 69.33
Round  70, Train loss: 1.336, Test loss: 1.296, Test accuracy: 59.76
Round  70, Global train loss: 1.336, Global test loss: 1.036, Global test accuracy: 69.59
Round  71, Train loss: 1.296, Test loss: 1.290, Test accuracy: 60.01
Round  71, Global train loss: 1.296, Global test loss: 1.032, Global test accuracy: 69.41
Round  72, Train loss: 1.413, Test loss: 1.280, Test accuracy: 60.31
Round  72, Global train loss: 1.413, Global test loss: 1.066, Global test accuracy: 68.49
Round  73, Train loss: 1.531, Test loss: 1.285, Test accuracy: 60.26
Round  73, Global train loss: 1.531, Global test loss: 1.174, Global test accuracy: 67.36
Round  74, Train loss: 1.332, Test loss: 1.308, Test accuracy: 59.53
Round  74, Global train loss: 1.332, Global test loss: 1.077, Global test accuracy: 68.28
Round  75, Train loss: 1.415, Test loss: 1.321, Test accuracy: 59.09
Round  75, Global train loss: 1.415, Global test loss: 1.111, Global test accuracy: 68.77
Round  76, Train loss: 1.358, Test loss: 1.304, Test accuracy: 59.56
Round  76, Global train loss: 1.358, Global test loss: 1.067, Global test accuracy: 68.80
Round  77, Train loss: 1.390, Test loss: 1.285, Test accuracy: 60.18
Round  77, Global train loss: 1.390, Global test loss: 1.097, Global test accuracy: 67.17
Round  78, Train loss: 1.433, Test loss: 1.291, Test accuracy: 59.81
Round  78, Global train loss: 1.433, Global test loss: 1.127, Global test accuracy: 67.89
Round  79, Train loss: 1.368, Test loss: 1.292, Test accuracy: 59.94
Round  79, Global train loss: 1.368, Global test loss: 1.107, Global test accuracy: 67.20
Round  80, Train loss: 1.280, Test loss: 1.305, Test accuracy: 59.33
Round  80, Global train loss: 1.280, Global test loss: 1.073, Global test accuracy: 68.01
Round  81, Train loss: 1.266, Test loss: 1.303, Test accuracy: 59.56
Round  81, Global train loss: 1.266, Global test loss: 1.044, Global test accuracy: 68.29
Round  82, Train loss: 1.410, Test loss: 1.309, Test accuracy: 59.36
Round  82, Global train loss: 1.410, Global test loss: 1.097, Global test accuracy: 68.04
Round  83, Train loss: 1.242, Test loss: 1.311, Test accuracy: 59.17
Round  83, Global train loss: 1.242, Global test loss: 1.022, Global test accuracy: 69.37
Round  84, Train loss: 1.266, Test loss: 1.317, Test accuracy: 59.21
Round  84, Global train loss: 1.266, Global test loss: 1.060, Global test accuracy: 68.70
Round  85, Train loss: 1.405, Test loss: 1.320, Test accuracy: 59.01
Round  85, Global train loss: 1.405, Global test loss: 1.119, Global test accuracy: 68.36
Round  86, Train loss: 1.322, Test loss: 1.321, Test accuracy: 59.03
Round  86, Global train loss: 1.322, Global test loss: 1.045, Global test accuracy: 68.66
Round  87, Train loss: 1.259, Test loss: 1.318, Test accuracy: 58.94
Round  87, Global train loss: 1.259, Global test loss: 1.040, Global test accuracy: 68.56
Round  88, Train loss: 1.247, Test loss: 1.306, Test accuracy: 59.21
Round  88, Global train loss: 1.247, Global test loss: 1.010, Global test accuracy: 69.22
Round  89, Train loss: 1.310, Test loss: 1.306, Test accuracy: 59.09
Round  89, Global train loss: 1.310, Global test loss: 1.055, Global test accuracy: 68.31
Round  90, Train loss: 1.339, Test loss: 1.312, Test accuracy: 58.99
Round  90, Global train loss: 1.339, Global test loss: 1.069, Global test accuracy: 68.41
Round  91, Train loss: 1.312, Test loss: 1.312, Test accuracy: 59.09
Round  91, Global train loss: 1.312, Global test loss: 1.042, Global test accuracy: 68.81/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  92, Train loss: 1.272, Test loss: 1.320, Test accuracy: 58.95
Round  92, Global train loss: 1.272, Global test loss: 1.036, Global test accuracy: 69.14
Round  93, Train loss: 1.247, Test loss: 1.319, Test accuracy: 59.09
Round  93, Global train loss: 1.247, Global test loss: 1.023, Global test accuracy: 69.31
Round  94, Train loss: 1.323, Test loss: 1.328, Test accuracy: 58.95
Round  94, Global train loss: 1.323, Global test loss: 1.041, Global test accuracy: 69.49
Round  95, Train loss: 1.285, Test loss: 1.324, Test accuracy: 59.19
Round  95, Global train loss: 1.285, Global test loss: 1.045, Global test accuracy: 69.19
Round  96, Train loss: 1.242, Test loss: 1.322, Test accuracy: 59.16
Round  96, Global train loss: 1.242, Global test loss: 1.050, Global test accuracy: 69.08
Round  97, Train loss: 1.193, Test loss: 1.324, Test accuracy: 59.04
Round  97, Global train loss: 1.193, Global test loss: 1.008, Global test accuracy: 69.18
Round  98, Train loss: 1.338, Test loss: 1.333, Test accuracy: 58.67
Round  98, Global train loss: 1.338, Global test loss: 1.096, Global test accuracy: 67.84
Round  99, Train loss: 1.241, Test loss: 1.323, Test accuracy: 58.90
Round  99, Global train loss: 1.241, Global test loss: 1.080, Global test accuracy: 67.78
Final Round, Train loss: 0.942, Test loss: 1.483, Test accuracy: 56.04
Final Round, Global train loss: 0.942, Global test loss: 1.080, Global test accuracy: 67.78
Average accuracy final 10 rounds: 59.001999999999995 

Average global accuracy final 10 rounds: 68.823 

5286.835159301758
[4.866243362426758, 9.732486724853516, 14.095948696136475, 18.459410667419434, 22.82512855529785, 27.19084644317627, 31.907866954803467, 36.624887466430664, 40.922346115112305, 45.219804763793945, 49.54221224784851, 53.864619731903076, 58.18403339385986, 62.50344705581665, 66.83044695854187, 71.15744686126709, 75.49912142753601, 79.84079599380493, 84.17699217796326, 88.51318836212158, 92.6866626739502, 96.86013698577881, 101.05184721946716, 105.24355745315552, 109.44610142707825, 113.64864540100098, 117.85676789283752, 122.06489038467407, 126.32786750793457, 130.59084463119507, 134.7723422050476, 138.95383977890015, 143.15335869789124, 147.35287761688232, 151.71538186073303, 156.07788610458374, 160.28318047523499, 164.48847484588623, 168.75637650489807, 173.0242781639099, 177.3068609237671, 181.58944368362427, 185.91605043411255, 190.24265718460083, 194.5518696308136, 198.86108207702637, 203.0852165222168, 207.30935096740723, 211.5306384563446, 215.75192594528198, 219.95156335830688, 224.1512007713318, 228.3164505958557, 232.48170042037964, 236.70300197601318, 240.92430353164673, 245.07768082618713, 249.23105812072754, 253.49304747581482, 257.7550368309021, 261.9189772605896, 266.0829176902771, 270.26217103004456, 274.441424369812, 278.650808095932, 282.860191822052, 287.0703854560852, 291.2805790901184, 295.6147389411926, 299.94889879226685, 304.25190234184265, 308.55490589141846, 312.8275508880615, 317.1001958847046, 321.45915389060974, 325.8181118965149, 330.0604028701782, 334.30269384384155, 338.5161635875702, 342.7296333312988, 346.9221251010895, 351.1146168708801, 355.28575801849365, 359.4568991661072, 363.6332232952118, 367.8095474243164, 371.9802396297455, 376.15093183517456, 380.3218262195587, 384.49272060394287, 388.6511218547821, 392.80952310562134, 396.9644215106964, 401.1193199157715, 405.2636694908142, 409.40801906585693, 413.6118459701538, 417.8156728744507, 421.9868462085724, 426.1580195426941, 430.33638548851013, 434.5147514343262, 438.68110394477844, 442.8474564552307, 447.0125267505646, 451.17759704589844, 455.34161710739136, 459.5056371688843, 463.67824053764343, 467.8508439064026, 472.0304412841797, 476.2100386619568, 480.42908120155334, 484.6481237411499, 488.83889722824097, 493.02967071533203, 497.25830483436584, 501.48693895339966, 505.717059135437, 509.94717931747437, 514.1528582572937, 518.358537197113, 522.5449066162109, 526.7312760353088, 530.9492826461792, 535.1672892570496, 539.3498468399048, 543.53240442276, 547.6847364902496, 551.8370685577393, 556.0127732753754, 560.1884779930115, 564.3782389163971, 568.5679998397827, 572.7430291175842, 576.9180583953857, 581.1406650543213, 585.3632717132568, 589.5523474216461, 593.7414231300354, 597.9466233253479, 602.1518235206604, 606.3431293964386, 610.5344352722168, 614.7558887004852, 618.9773421287537, 623.2089612483978, 627.440580368042, 631.6527876853943, 635.8649950027466, 640.0859484672546, 644.3069019317627, 648.542058467865, 652.7772150039673, 656.9852919578552, 661.1933689117432, 665.3736417293549, 669.5539145469666, 673.8116517066956, 678.0693888664246, 682.3145658969879, 686.5597429275513, 690.7980990409851, 695.036455154419, 699.2767255306244, 703.5169959068298, 707.761477470398, 712.0059590339661, 716.2601552009583, 720.5143513679504, 724.7182033061981, 728.9220552444458, 733.1224355697632, 737.3228158950806, 741.5031349658966, 745.6834540367126, 749.8845918178558, 754.085729598999, 758.3233556747437, 762.5609817504883, 766.8147940635681, 771.068606376648, 775.3134746551514, 779.5583429336548, 783.6400146484375, 787.7216863632202, 791.8055367469788, 795.8893871307373, 799.9875435829163, 804.0857000350952, 808.1725573539734, 812.2594146728516, 816.3904013633728, 820.521388053894, 824.6374151706696, 828.7534422874451, 832.8594336509705, 836.9654250144958, 841.0320112705231, 845.0985975265503, 847.1451990604401, 849.1918005943298]
[22.28, 22.28, 29.69, 29.69, 32.365, 32.365, 35.7775, 35.7775, 36.26, 36.26, 38.55, 38.55, 38.1975, 38.1975, 39.2775, 39.2775, 39.6625, 39.6625, 41.2925, 41.2925, 44.1175, 44.1175, 44.3875, 44.3875, 45.65, 45.65, 46.77, 46.77, 46.8975, 46.8975, 47.16, 47.16, 48.615, 48.615, 48.81, 48.81, 49.2675, 49.2675, 49.9475, 49.9475, 51.7425, 51.7425, 52.2575, 52.2575, 52.7725, 52.7725, 52.8175, 52.8175, 53.4075, 53.4075, 53.6775, 53.6775, 53.885, 53.885, 54.655, 54.655, 54.8775, 54.8775, 54.815, 54.815, 54.73, 54.73, 55.3525, 55.3525, 55.8425, 55.8425, 56.1225, 56.1225, 56.79, 56.79, 56.765, 56.765, 56.5025, 56.5025, 56.7225, 56.7225, 57.2275, 57.2275, 57.0175, 57.0175, 57.3, 57.3, 57.375, 57.375, 57.7375, 57.7375, 57.825, 57.825, 57.555, 57.555, 57.915, 57.915, 58.2975, 58.2975, 58.3125, 58.3125, 58.255, 58.255, 58.365, 58.365, 58.6025, 58.6025, 59.0325, 59.0325, 59.3825, 59.3825, 59.67, 59.67, 59.8925, 59.8925, 60.465, 60.465, 59.7625, 59.7625, 59.9275, 59.9275, 59.82, 59.82, 59.795, 59.795, 60.215, 60.215, 59.6175, 59.6175, 59.5975, 59.5975, 59.5675, 59.5675, 59.8425, 59.8425, 59.79, 59.79, 59.665, 59.665, 59.8575, 59.8575, 59.4175, 59.4175, 59.62, 59.62, 59.755, 59.755, 60.0075, 60.0075, 60.31, 60.31, 60.2625, 60.2625, 59.5275, 59.5275, 59.085, 59.085, 59.5575, 59.5575, 60.1775, 60.1775, 59.815, 59.815, 59.935, 59.935, 59.325, 59.325, 59.56, 59.56, 59.3575, 59.3575, 59.17, 59.17, 59.2125, 59.2125, 59.005, 59.005, 59.035, 59.035, 58.9425, 58.9425, 59.2075, 59.2075, 59.0925, 59.0925, 58.9925, 58.9925, 59.0925, 59.0925, 58.955, 58.955, 59.085, 59.085, 58.9475, 58.9475, 59.1875, 59.1875, 59.155, 59.155, 59.0425, 59.0425, 58.6675, 58.6675, 58.895, 58.895, 56.04, 56.04]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 7, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.240, Test loss: 2.124, Test accuracy: 27.41
Round   1, Train loss: 2.081, Test loss: 1.934, Test accuracy: 34.53
Round   2, Train loss: 2.014, Test loss: 1.822, Test accuracy: 37.52
Round   3, Train loss: 1.945, Test loss: 1.771, Test accuracy: 39.76
Round   4, Train loss: 1.896, Test loss: 1.704, Test accuracy: 42.45
Round   5, Train loss: 1.849, Test loss: 1.635, Test accuracy: 44.33
Round   6, Train loss: 1.836, Test loss: 1.606, Test accuracy: 45.90
Round   7, Train loss: 1.830, Test loss: 1.598, Test accuracy: 46.67
Round   8, Train loss: 1.798, Test loss: 1.583, Test accuracy: 48.11
Round   9, Train loss: 1.751, Test loss: 1.542, Test accuracy: 49.70
Round  10, Train loss: 1.767, Test loss: 1.483, Test accuracy: 50.95
Round  11, Train loss: 1.666, Test loss: 1.447, Test accuracy: 52.05
Round  12, Train loss: 1.624, Test loss: 1.419, Test accuracy: 53.58
Round  13, Train loss: 1.680, Test loss: 1.384, Test accuracy: 54.85
Round  14, Train loss: 1.685, Test loss: 1.394, Test accuracy: 55.79
Round  15, Train loss: 1.689, Test loss: 1.351, Test accuracy: 57.05
Round  16, Train loss: 1.589, Test loss: 1.329, Test accuracy: 57.77
Round  17, Train loss: 1.594, Test loss: 1.333, Test accuracy: 58.42
Round  18, Train loss: 1.584, Test loss: 1.331, Test accuracy: 58.42
Round  19, Train loss: 1.497, Test loss: 1.296, Test accuracy: 59.02
Round  20, Train loss: 1.556, Test loss: 1.288, Test accuracy: 60.09
Round  21, Train loss: 1.502, Test loss: 1.272, Test accuracy: 60.55
Round  22, Train loss: 1.530, Test loss: 1.263, Test accuracy: 60.71
Round  23, Train loss: 1.582, Test loss: 1.266, Test accuracy: 61.62
Round  24, Train loss: 1.481, Test loss: 1.265, Test accuracy: 61.33
Round  25, Train loss: 1.563, Test loss: 1.242, Test accuracy: 61.53
Round  26, Train loss: 1.574, Test loss: 1.266, Test accuracy: 61.87
Round  27, Train loss: 1.519, Test loss: 1.223, Test accuracy: 62.83
Round  28, Train loss: 1.461, Test loss: 1.202, Test accuracy: 63.27
Round  29, Train loss: 1.401, Test loss: 1.194, Test accuracy: 62.97
Round  30, Train loss: 1.502, Test loss: 1.197, Test accuracy: 63.28
Round  31, Train loss: 1.335, Test loss: 1.192, Test accuracy: 63.55
Round  32, Train loss: 1.421, Test loss: 1.175, Test accuracy: 64.13
Round  33, Train loss: 1.425, Test loss: 1.188, Test accuracy: 63.44
Round  34, Train loss: 1.392, Test loss: 1.166, Test accuracy: 64.71
Round  35, Train loss: 1.375, Test loss: 1.172, Test accuracy: 64.63
Round  36, Train loss: 1.346, Test loss: 1.173, Test accuracy: 64.41
Round  37, Train loss: 1.407, Test loss: 1.161, Test accuracy: 64.69
Round  38, Train loss: 1.409, Test loss: 1.149, Test accuracy: 65.31
Round  39, Train loss: 1.275, Test loss: 1.138, Test accuracy: 65.23
Round  40, Train loss: 1.383, Test loss: 1.136, Test accuracy: 65.81
Round  41, Train loss: 1.304, Test loss: 1.125, Test accuracy: 65.42
Round  42, Train loss: 1.362, Test loss: 1.137, Test accuracy: 65.42
Round  43, Train loss: 1.374, Test loss: 1.126, Test accuracy: 66.00
Round  44, Train loss: 1.297, Test loss: 1.117, Test accuracy: 66.07
Round  45, Train loss: 1.341, Test loss: 1.118, Test accuracy: 66.22
Round  46, Train loss: 1.379, Test loss: 1.114, Test accuracy: 66.01
Round  47, Train loss: 1.330, Test loss: 1.121, Test accuracy: 66.17
Round  48, Train loss: 1.381, Test loss: 1.111, Test accuracy: 66.28
Round  49, Train loss: 1.281, Test loss: 1.112, Test accuracy: 66.17
Round  50, Train loss: 1.271, Test loss: 1.115, Test accuracy: 66.23
Round  51, Train loss: 1.324, Test loss: 1.102, Test accuracy: 66.86
Round  52, Train loss: 1.333, Test loss: 1.094, Test accuracy: 66.89
Round  53, Train loss: 1.262, Test loss: 1.089, Test accuracy: 66.89
Round  54, Train loss: 1.305, Test loss: 1.083, Test accuracy: 66.89
Round  55, Train loss: 1.291, Test loss: 1.085, Test accuracy: 67.33
Round  56, Train loss: 1.246, Test loss: 1.089, Test accuracy: 67.16
Round  57, Train loss: 1.290, Test loss: 1.096, Test accuracy: 66.72
Round  58, Train loss: 1.297, Test loss: 1.099, Test accuracy: 66.75
Round  59, Train loss: 1.250, Test loss: 1.099, Test accuracy: 66.74
Round  60, Train loss: 1.324, Test loss: 1.097, Test accuracy: 66.87
Round  61, Train loss: 1.197, Test loss: 1.086, Test accuracy: 67.10
Round  62, Train loss: 1.330, Test loss: 1.086, Test accuracy: 67.48
Round  63, Train loss: 1.317, Test loss: 1.079, Test accuracy: 67.27
Round  64, Train loss: 1.210, Test loss: 1.083, Test accuracy: 67.11
Round  65, Train loss: 1.258, Test loss: 1.089, Test accuracy: 66.89
Round  66, Train loss: 1.259, Test loss: 1.079, Test accuracy: 67.02
Round  67, Train loss: 1.225, Test loss: 1.069, Test accuracy: 67.51
Round  68, Train loss: 1.202, Test loss: 1.070, Test accuracy: 67.25
Round  69, Train loss: 1.243, Test loss: 1.071, Test accuracy: 67.26
Round  70, Train loss: 1.162, Test loss: 1.079, Test accuracy: 67.13
Round  71, Train loss: 1.190, Test loss: 1.069, Test accuracy: 67.30
Round  72, Train loss: 1.253, Test loss: 1.076, Test accuracy: 67.38
Round  73, Train loss: 1.260, Test loss: 1.046, Test accuracy: 68.20
Round  74, Train loss: 1.223, Test loss: 1.067, Test accuracy: 67.55
Round  75, Train loss: 1.206, Test loss: 1.066, Test accuracy: 67.81
Round  76, Train loss: 1.206, Test loss: 1.086, Test accuracy: 67.22
Round  77, Train loss: 1.216, Test loss: 1.079, Test accuracy: 67.44
Round  78, Train loss: 1.096, Test loss: 1.057, Test accuracy: 67.78
Round  79, Train loss: 1.204, Test loss: 1.077, Test accuracy: 67.11
Round  80, Train loss: 1.148, Test loss: 1.073, Test accuracy: 67.31
Round  81, Train loss: 1.249, Test loss: 1.063, Test accuracy: 68.08
Round  82, Train loss: 1.278, Test loss: 1.058, Test accuracy: 68.08
Round  83, Train loss: 1.258, Test loss: 1.061, Test accuracy: 67.53
Round  84, Train loss: 1.161, Test loss: 1.069, Test accuracy: 67.57
Round  85, Train loss: 1.130, Test loss: 1.065, Test accuracy: 67.75
Round  86, Train loss: 1.178, Test loss: 1.060, Test accuracy: 67.94
Round  87, Train loss: 1.188, Test loss: 1.064, Test accuracy: 67.73
Round  88, Train loss: 1.204, Test loss: 1.071, Test accuracy: 67.64
Round  89, Train loss: 1.125, Test loss: 1.064, Test accuracy: 67.70
Round  90, Train loss: 1.175, Test loss: 1.071, Test accuracy: 67.40
Round  91, Train loss: 1.222, Test loss: 1.069, Test accuracy: 67.73
Round  92, Train loss: 1.161, Test loss: 1.071, Test accuracy: 67.76
Round  93, Train loss: 1.229, Test loss: 1.062, Test accuracy: 68.18
Round  94, Train loss: 1.141, Test loss: 1.047, Test accuracy: 68.67
Round  95, Train loss: 1.160, Test loss: 1.061, Test accuracy: 67.94
Round  96, Train loss: 1.148, Test loss: 1.069, Test accuracy: 67.81
Round  97, Train loss: 1.192, Test loss: 1.078, Test accuracy: 67.27
Round  98, Train loss: 1.073, Test loss: 1.053, Test accuracy: 68.03
Round  99, Train loss: 1.162, Test loss: 1.064, Test accuracy: 67.77
Final Round, Train loss: 1.095, Test loss: 1.070, Test accuracy: 67.33
Average accuracy final 10 rounds: 67.85575
3693.8018746376038
[5.319250583648682, 10.382739305496216, 15.440393924713135, 20.48735475540161, 25.513853311538696, 30.51498031616211, 35.509299755096436, 40.51302909851074, 45.52202010154724, 50.508272886276245, 55.50851321220398, 60.5137722492218, 65.514817237854, 70.52734613418579, 75.54219627380371, 80.5618417263031, 85.59835577011108, 90.63647961616516, 95.67328000068665, 100.69659614562988, 105.71914744377136, 110.74018383026123, 115.77158236503601, 120.81939220428467, 125.88200879096985, 130.93356156349182, 135.9824411869049, 141.0186948776245, 146.05861926078796, 151.07529211044312, 156.09114289283752, 161.10358095169067, 165.59398579597473, 170.08373188972473, 174.5683629512787, 179.06639432907104, 183.5795979499817, 188.09049582481384, 192.56953859329224, 197.05890321731567, 201.5454204082489, 206.01774954795837, 210.51023387908936, 215.03197598457336, 219.54553890228271, 224.06067943572998, 228.56378412246704, 233.06240248680115, 237.55921411514282, 242.02995586395264, 246.53849267959595, 251.1372787952423, 255.6348090171814, 260.13433933258057, 264.62582325935364, 269.13117146492004, 273.63578176498413, 278.1052348613739, 282.5775761604309, 287.0547707080841, 291.5286543369293, 296.0004949569702, 300.4736132621765, 304.95903062820435, 309.4312262535095, 313.8998064994812, 318.3894863128662, 322.8728678226471, 327.35738945007324, 331.82432985305786, 336.31499099731445, 340.8090660572052, 345.3085370063782, 349.82214403152466, 354.32380056381226, 358.83841586112976, 363.351021528244, 367.8531632423401, 372.33246326446533, 376.81920766830444, 381.29526686668396, 385.76959204673767, 390.50882387161255, 395.50141406059265, 400.51997232437134, 405.51847410202026, 410.52540135383606, 415.55575251579285, 420.57694482803345, 425.358505487442, 429.8787443637848, 434.3919975757599, 438.9082815647125, 443.4163730144501, 447.93130254745483, 452.44222044944763, 456.9709825515747, 461.47854924201965, 465.98554015159607, 470.5169892311096, 472.3622741699219]
[27.41, 34.535, 37.5225, 39.7625, 42.4525, 44.3325, 45.895, 46.6675, 48.1125, 49.695, 50.9475, 52.045, 53.58, 54.85, 55.7875, 57.045, 57.775, 58.4175, 58.425, 59.0175, 60.0875, 60.555, 60.71, 61.615, 61.33, 61.53, 61.8725, 62.83, 63.265, 62.97, 63.2825, 63.5525, 64.13, 63.44, 64.7075, 64.6275, 64.405, 64.695, 65.305, 65.235, 65.8075, 65.425, 65.4175, 66.0, 66.07, 66.225, 66.01, 66.1725, 66.2775, 66.165, 66.235, 66.855, 66.885, 66.895, 66.8875, 67.325, 67.1575, 66.72, 66.745, 66.74, 66.8675, 67.1, 67.4825, 67.2725, 67.115, 66.885, 67.0175, 67.51, 67.2525, 67.26, 67.1275, 67.3, 67.375, 68.2, 67.5475, 67.805, 67.225, 67.4375, 67.775, 67.1125, 67.31, 68.0825, 68.0775, 67.5325, 67.57, 67.75, 67.935, 67.7325, 67.6375, 67.7, 67.3975, 67.735, 67.7575, 68.18, 68.67, 67.935, 67.81, 67.27, 68.035, 67.7675, 67.33]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 8, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  22.4500
Round 1 global test acc  35.0300
Round 2 global test acc  42.2400
Round 3 global test acc  42.2400
Round 4 global test acc  46.0600
Round 5 global test acc  46.2400
Round 6 global test acc  49.6700
Round 7 global test acc  50.1500
Round 8 global test acc  52.1300
Round 9 global test acc  52.2300
Round 10 global test acc  52.2500
Round 11 global test acc  53.1100
Round 12 global test acc  52.6000
Round 13 global test acc  52.8400
Round 14 global test acc  54.6300
Round 15 global test acc  53.4100
Round 16 global test acc  54.6000
Round 17 global test acc  55.2100
Round 18 global test acc  56.2700
Round 19 global test acc  55.1900
Round 20 global test acc  56.0700
Round 21 global test acc  57.0100
Round 22 global test acc  57.4900
Round 23 global test acc  57.2200
Round 24 global test acc  57.0000
Round 25 global test acc  57.7700
Round 26 global test acc  58.9700
Round 27 global test acc  57.1300
Round 28 global test acc  59.4800
Round 29 global test acc  58.5400
Round 30 global test acc  58.7500
Round 31 global test acc  59.3000
Round 32 global test acc  58.3300
Round 33 global test acc  57.5900
Round 34 global test acc  57.8900
Round 35 global test acc  60.3300
Round 36 global test acc  59.9100
Round 37 global test acc  59.8900
Round 38 global test acc  61.2400
Round 39 global test acc  58.9100
Round 40 global test acc  61.3900
Round 41 global test acc  59.7100
Round 42 global test acc  61.7300
Round 43 global test acc  61.8100
Round 44 global test acc  60.6300
Round 45 global test acc  59.6000
Round 46 global test acc  60.5000
Round 47 global test acc  62.0600
Round 48 global test acc  60.7200
Round 49 global test acc  61.1900
Round 50 global test acc  61.2900
Round 51 global test acc  61.7800
Round 52 global test acc  60.7600
Round 53 global test acc  62.3000
Round 54 global test acc  61.8100
Round 55 global test acc  62.4400
Round 56 global test acc  62.3200
Round 57 global test acc  59.8100
Round 58 global test acc  62.2000
Round 59 global test acc  61.0600
Round 60 global test acc  62.5900
Round 61 global test acc  63.0600
Round 62 global test acc  62.0900
Round 63 global test acc  62.5200
Round 64 global test acc  63.2500
Round 65 global test acc  62.6800
Round 66 global test acc  63.6200
Round 67 global test acc  61.1700
Round 68 global test acc  63.1500
Round 69 global test acc  61.4000
Round 70 global test acc  63.5100
Round 71 global test acc  63.3900
Round 72 global test acc  61.0600
Round 73 global test acc  61.4300
Round 74 global test acc  63.0200
Round 75 global test acc  61.3200
Round 76 global test acc  62.5800
Round 77 global test acc  60.9400
Round 78 global test acc  60.2600
Round 79 global test acc  62.4500
Round 80 global test acc  63.1600
Round 81 global test acc  61.7400
Round 82 global test acc  61.6500
Round 83 global test acc  60.1300
Round 84 global test acc  58.7700
Round 85 global test acc  58.6300
Round 86 global test acc  57.1700
Round 87 global test acc  56.3300
Round 88 global test acc  56.1300
Round 89 global test acc  56.0400
Round 90 global test acc  56.0600
Round 91 global test acc  54.9800
Round 92 global test acc  54.3300
Round 93 global test acc  54.8500
Round 94 global test acc  54.3300
Round 95 global test acc  52.9300
Round 96 global test acc  53.1000
Round 97 global test acc  53.6700
Round 98 global test acc  52.6600
Round 99 global test acc  52.1600
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 3, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.244, Test loss: 2.143, Test accuracy: 25.09
Round   1, Train loss: 2.117, Test loss: 1.948, Test accuracy: 32.66
Round   2, Train loss: 2.032, Test loss: 1.853, Test accuracy: 36.26
Round   3, Train loss: 1.982, Test loss: 1.793, Test accuracy: 38.80
Round   4, Train loss: 1.969, Test loss: 1.750, Test accuracy: 41.26
Round   5, Train loss: 1.904, Test loss: 1.671, Test accuracy: 43.10
Round   6, Train loss: 1.777, Test loss: 1.637, Test accuracy: 44.70
Round   7, Train loss: 1.834, Test loss: 1.626, Test accuracy: 46.41
Round   8, Train loss: 1.886, Test loss: 1.610, Test accuracy: 47.70
Round   9, Train loss: 1.896, Test loss: 1.584, Test accuracy: 48.85
Round  10, Train loss: 1.732, Test loss: 1.504, Test accuracy: 51.23
Round  11, Train loss: 1.723, Test loss: 1.483, Test accuracy: 52.26
Round  12, Train loss: 1.628, Test loss: 1.475, Test accuracy: 52.69
Round  13, Train loss: 1.696, Test loss: 1.446, Test accuracy: 54.40
Round  14, Train loss: 1.717, Test loss: 1.423, Test accuracy: 54.93
Round  15, Train loss: 1.619, Test loss: 1.375, Test accuracy: 56.10
Round  16, Train loss: 1.621, Test loss: 1.369, Test accuracy: 56.90
Round  17, Train loss: 1.557, Test loss: 1.365, Test accuracy: 57.28
Round  18, Train loss: 1.593, Test loss: 1.365, Test accuracy: 57.27
Round  19, Train loss: 1.585, Test loss: 1.322, Test accuracy: 58.82
Round  20, Train loss: 1.566, Test loss: 1.294, Test accuracy: 59.88
Round  21, Train loss: 1.649, Test loss: 1.311, Test accuracy: 60.08
Round  22, Train loss: 1.515, Test loss: 1.310, Test accuracy: 60.21
Round  23, Train loss: 1.469, Test loss: 1.288, Test accuracy: 60.80
Round  24, Train loss: 1.554, Test loss: 1.268, Test accuracy: 61.81
Round  25, Train loss: 1.573, Test loss: 1.261, Test accuracy: 61.69
Round  26, Train loss: 1.546, Test loss: 1.250, Test accuracy: 62.23
Round  27, Train loss: 1.575, Test loss: 1.238, Test accuracy: 62.66
Round  28, Train loss: 1.563, Test loss: 1.237, Test accuracy: 63.19
Round  29, Train loss: 1.490, Test loss: 1.225, Test accuracy: 63.00
Round  30, Train loss: 1.473, Test loss: 1.211, Test accuracy: 63.38
Round  31, Train loss: 1.468, Test loss: 1.186, Test accuracy: 63.57
Round  32, Train loss: 1.449, Test loss: 1.178, Test accuracy: 64.56
Round  33, Train loss: 1.389, Test loss: 1.169, Test accuracy: 64.62
Round  34, Train loss: 1.619, Test loss: 1.204, Test accuracy: 63.98
Round  35, Train loss: 1.467, Test loss: 1.183, Test accuracy: 64.83
Round  36, Train loss: 1.460, Test loss: 1.178, Test accuracy: 64.85
Round  37, Train loss: 1.432, Test loss: 1.176, Test accuracy: 65.41
Round  38, Train loss: 1.308, Test loss: 1.156, Test accuracy: 65.51
Round  39, Train loss: 1.449, Test loss: 1.158, Test accuracy: 65.36
Round  40, Train loss: 1.380, Test loss: 1.158, Test accuracy: 65.67
Round  41, Train loss: 1.341, Test loss: 1.137, Test accuracy: 65.66
Round  42, Train loss: 1.323, Test loss: 1.136, Test accuracy: 65.89
Round  43, Train loss: 1.437, Test loss: 1.131, Test accuracy: 66.50
Round  44, Train loss: 1.300, Test loss: 1.139, Test accuracy: 65.74
Round  45, Train loss: 1.460, Test loss: 1.119, Test accuracy: 66.37
Round  46, Train loss: 1.411, Test loss: 1.122, Test accuracy: 66.19
Round  47, Train loss: 1.316, Test loss: 1.124, Test accuracy: 66.61
Round  48, Train loss: 1.328, Test loss: 1.113, Test accuracy: 67.14
Round  49, Train loss: 1.410, Test loss: 1.114, Test accuracy: 66.74
Round  50, Train loss: 1.353, Test loss: 1.120, Test accuracy: 66.65
Round  51, Train loss: 1.293, Test loss: 1.091, Test accuracy: 67.64
Round  52, Train loss: 1.290, Test loss: 1.097, Test accuracy: 67.28
Round  53, Train loss: 1.277, Test loss: 1.109, Test accuracy: 66.77
Round  54, Train loss: 1.420, Test loss: 1.089, Test accuracy: 67.70
Round  55, Train loss: 1.298, Test loss: 1.092, Test accuracy: 67.58
Round  56, Train loss: 1.345, Test loss: 1.074, Test accuracy: 68.46
Round  57, Train loss: 1.299, Test loss: 1.073, Test accuracy: 68.07
Round  58, Train loss: 1.259, Test loss: 1.066, Test accuracy: 68.22
Round  59, Train loss: 1.283, Test loss: 1.094, Test accuracy: 67.47
Round  60, Train loss: 1.451, Test loss: 1.084, Test accuracy: 67.85
Round  61, Train loss: 1.326, Test loss: 1.091, Test accuracy: 67.34
Round  62, Train loss: 1.212, Test loss: 1.078, Test accuracy: 67.74
Round  63, Train loss: 1.260, Test loss: 1.085, Test accuracy: 67.60
Round  64, Train loss: 1.329, Test loss: 1.088, Test accuracy: 67.56
Round  65, Train loss: 1.276, Test loss: 1.080, Test accuracy: 67.65
Round  66, Train loss: 1.215, Test loss: 1.080, Test accuracy: 67.75
Round  67, Train loss: 1.265, Test loss: 1.080, Test accuracy: 67.88
Round  68, Train loss: 1.183, Test loss: 1.073, Test accuracy: 68.22
Round  69, Train loss: 1.208, Test loss: 1.075, Test accuracy: 68.49
Round  70, Train loss: 1.322, Test loss: 1.095, Test accuracy: 67.56
Round  71, Train loss: 1.164, Test loss: 1.084, Test accuracy: 67.58
Round  72, Train loss: 1.226, Test loss: 1.091, Test accuracy: 67.17
Round  73, Train loss: 1.216, Test loss: 1.072, Test accuracy: 67.64
Round  74, Train loss: 1.109, Test loss: 1.079, Test accuracy: 67.55
Round  75, Train loss: 1.390, Test loss: 1.071, Test accuracy: 68.24
Round  76, Train loss: 1.244, Test loss: 1.080, Test accuracy: 67.86
Round  77, Train loss: 1.311, Test loss: 1.082, Test accuracy: 67.42
Round  78, Train loss: 1.273, Test loss: 1.082, Test accuracy: 67.42
Round  79, Train loss: 1.161, Test loss: 1.108, Test accuracy: 66.22
Round  80, Train loss: 1.319, Test loss: 1.112, Test accuracy: 66.11
Round  81, Train loss: 1.260, Test loss: 1.090, Test accuracy: 67.01
Round  82, Train loss: 1.228, Test loss: 1.077, Test accuracy: 67.58
Round  83, Train loss: 1.265, Test loss: 1.085, Test accuracy: 67.13
Round  84, Train loss: 1.238, Test loss: 1.081, Test accuracy: 67.57
Round  85, Train loss: 1.190, Test loss: 1.072, Test accuracy: 68.26
Round  86, Train loss: 1.262, Test loss: 1.077, Test accuracy: 68.16
Round  87, Train loss: 1.102, Test loss: 1.066, Test accuracy: 68.17
Round  88, Train loss: 1.111, Test loss: 1.057, Test accuracy: 68.68
Round  89, Train loss: 1.218, Test loss: 1.069, Test accuracy: 68.03
Round  90, Train loss: 1.082, Test loss: 1.068, Test accuracy: 67.88
Round  91, Train loss: 1.248, Test loss: 1.082, Test accuracy: 67.40
Round  92, Train loss: 1.260, Test loss: 1.078, Test accuracy: 67.64
Round  93, Train loss: 1.223, Test loss: 1.076, Test accuracy: 67.95
Round  94, Train loss: 1.133, Test loss: 1.073, Test accuracy: 68.10
Round  95, Train loss: 1.090, Test loss: 1.063, Test accuracy: 68.12
Round  96, Train loss: 1.179, Test loss: 1.056, Test accuracy: 68.33
Round  97, Train loss: 1.305, Test loss: 1.082, Test accuracy: 67.41
Round  98, Train loss: 1.248, Test loss: 1.069, Test accuracy: 68.14
Round  99, Train loss: 1.162, Test loss: 1.061, Test accuracy: 68.21
Final Round, Train loss: 1.115, Test loss: 1.079, Test accuracy: 66.97
Average accuracy final 10 rounds: 67.91975000000001
3667.8702940940857
[5.007670640945435, 9.69098973274231, 14.400330305099487, 19.23203182220459, 23.81448745727539, 28.293562650680542, 32.77944564819336, 37.59846258163452, 42.60261249542236, 47.393455505371094, 52.41536021232605, 57.24701809883118, 61.73354244232178, 66.20949339866638, 70.69128656387329, 75.18874263763428, 79.6691403388977, 84.15840888023376, 88.61782789230347, 93.12054181098938, 97.64451479911804, 102.11601066589355, 106.66641926765442, 111.16614866256714, 115.65644454956055, 120.15618681907654, 124.67618584632874, 129.1698579788208, 133.6846694946289, 138.19261765480042, 142.69459056854248, 147.17132353782654, 151.64898681640625, 156.15102243423462, 160.6837568283081, 165.23786163330078, 169.7679386138916, 174.31644773483276, 178.84750247001648, 183.404878616333, 187.9154977798462, 192.40808939933777, 196.97925543785095, 201.51858258247375, 206.07971811294556, 210.6553921699524, 215.24991869926453, 220.12414503097534, 225.1345477104187, 230.1693959236145, 234.6854214668274, 239.22901153564453, 243.7504940032959, 248.2485249042511, 252.75079989433289, 257.27271366119385, 261.7922852039337, 266.2791256904602, 270.94543194770813, 275.4586625099182, 280.00257563591003, 284.54187297821045, 289.07211542129517, 293.59251070022583, 298.109742641449, 302.7875473499298, 307.38048696517944, 311.93122696876526, 316.446414232254, 321.53640246391296, 326.50724720954895, 331.6686873435974, 336.70369935035706, 341.62653398513794, 346.75863814353943, 351.8275442123413, 356.5020010471344, 361.1916182041168, 365.9449577331543, 370.65629863739014, 375.3353018760681, 380.0013921260834, 384.64050579071045, 389.31026434898376, 393.9624083042145, 398.6701076030731, 403.3831031322479, 408.0781364440918, 412.6376111507416, 417.1927046775818, 422.3845462799072, 427.14384174346924, 431.96829652786255, 436.83630418777466, 441.68755984306335, 446.7673382759094, 452.0698821544647, 457.3327958583832, 462.6053557395935, 467.54910945892334, 469.57195019721985]
[25.095, 32.665, 36.255, 38.8025, 41.255, 43.1, 44.7025, 46.41, 47.695, 48.8525, 51.2325, 52.2625, 52.685, 54.3975, 54.9275, 56.1, 56.895, 57.2825, 57.2725, 58.82, 59.8825, 60.08, 60.2075, 60.805, 61.81, 61.69, 62.235, 62.66, 63.1875, 62.9975, 63.385, 63.57, 64.555, 64.625, 63.9775, 64.8325, 64.8525, 65.405, 65.5075, 65.3575, 65.67, 65.66, 65.8925, 66.5, 65.7425, 66.3675, 66.1925, 66.6125, 67.145, 66.74, 66.6475, 67.6425, 67.285, 66.7725, 67.7025, 67.575, 68.4625, 68.0725, 68.215, 67.475, 67.8475, 67.3375, 67.7425, 67.5975, 67.56, 67.6475, 67.7525, 67.8775, 68.2175, 68.4875, 67.5625, 67.5825, 67.1675, 67.6375, 67.5475, 68.2425, 67.8625, 67.4175, 67.42, 66.215, 66.115, 67.0125, 67.575, 67.1325, 67.5675, 68.26, 68.155, 68.17, 68.68, 68.0325, 67.8825, 67.3975, 67.6375, 67.9525, 68.1025, 68.125, 68.335, 67.4125, 68.14, 68.2125, 66.9725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 14, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.242, Test loss: 2.099, Test accuracy: 27.66
Round   1, Train loss: 2.065, Test loss: 1.870, Test accuracy: 34.93
Round   2, Train loss: 1.996, Test loss: 1.790, Test accuracy: 38.90
Round   3, Train loss: 1.947, Test loss: 1.734, Test accuracy: 41.18
Round   4, Train loss: 1.888, Test loss: 1.717, Test accuracy: 43.07
Round   5, Train loss: 1.880, Test loss: 1.635, Test accuracy: 45.73
Round   6, Train loss: 1.863, Test loss: 1.627, Test accuracy: 46.98
Round   7, Train loss: 1.778, Test loss: 1.582, Test accuracy: 48.40
Round   8, Train loss: 1.819, Test loss: 1.570, Test accuracy: 49.57
Round   9, Train loss: 1.758, Test loss: 1.527, Test accuracy: 51.38
Round  10, Train loss: 1.735, Test loss: 1.452, Test accuracy: 53.33
Round  11, Train loss: 1.742, Test loss: 1.465, Test accuracy: 53.98
Round  12, Train loss: 1.719, Test loss: 1.429, Test accuracy: 54.97
Round  13, Train loss: 1.632, Test loss: 1.374, Test accuracy: 56.06
Round  14, Train loss: 1.649, Test loss: 1.354, Test accuracy: 57.47
Round  15, Train loss: 1.645, Test loss: 1.335, Test accuracy: 58.45
Round  16, Train loss: 1.682, Test loss: 1.326, Test accuracy: 58.70
Round  17, Train loss: 1.632, Test loss: 1.317, Test accuracy: 59.11
Round  18, Train loss: 1.535, Test loss: 1.322, Test accuracy: 59.42
Round  19, Train loss: 1.498, Test loss: 1.288, Test accuracy: 60.28
Round  20, Train loss: 1.616, Test loss: 1.262, Test accuracy: 61.23
Round  21, Train loss: 1.513, Test loss: 1.259, Test accuracy: 61.77
Round  22, Train loss: 1.530, Test loss: 1.259, Test accuracy: 61.86
Round  23, Train loss: 1.585, Test loss: 1.232, Test accuracy: 62.85
Round  24, Train loss: 1.422, Test loss: 1.228, Test accuracy: 62.81
Round  25, Train loss: 1.459, Test loss: 1.207, Test accuracy: 63.55
Round  26, Train loss: 1.556, Test loss: 1.204, Test accuracy: 64.11
Round  27, Train loss: 1.619, Test loss: 1.208, Test accuracy: 64.30
Round  28, Train loss: 1.488, Test loss: 1.184, Test accuracy: 64.50
Round  29, Train loss: 1.441, Test loss: 1.191, Test accuracy: 64.56
Round  30, Train loss: 1.488, Test loss: 1.177, Test accuracy: 64.85
Round  31, Train loss: 1.419, Test loss: 1.152, Test accuracy: 65.36
Round  32, Train loss: 1.500, Test loss: 1.170, Test accuracy: 64.98
Round  33, Train loss: 1.451, Test loss: 1.138, Test accuracy: 66.04
Round  34, Train loss: 1.352, Test loss: 1.148, Test accuracy: 65.59
Round  35, Train loss: 1.478, Test loss: 1.165, Test accuracy: 65.17
Round  36, Train loss: 1.476, Test loss: 1.165, Test accuracy: 65.70
Round  37, Train loss: 1.349, Test loss: 1.128, Test accuracy: 66.03
Round  38, Train loss: 1.422, Test loss: 1.138, Test accuracy: 65.83
Round  39, Train loss: 1.421, Test loss: 1.112, Test accuracy: 66.71
Round  40, Train loss: 1.431, Test loss: 1.115, Test accuracy: 66.59
Round  41, Train loss: 1.426, Test loss: 1.107, Test accuracy: 66.87
Round  42, Train loss: 1.356, Test loss: 1.097, Test accuracy: 67.10
Round  43, Train loss: 1.327, Test loss: 1.074, Test accuracy: 67.55
Round  44, Train loss: 1.376, Test loss: 1.087, Test accuracy: 67.18
Round  45, Train loss: 1.335, Test loss: 1.084, Test accuracy: 67.68
Round  46, Train loss: 1.469, Test loss: 1.094, Test accuracy: 67.41
Round  47, Train loss: 1.497, Test loss: 1.107, Test accuracy: 67.10
Round  48, Train loss: 1.303, Test loss: 1.073, Test accuracy: 67.78
Round  49, Train loss: 1.336, Test loss: 1.070, Test accuracy: 67.90
Round  50, Train loss: 1.353, Test loss: 1.083, Test accuracy: 67.51
Round  51, Train loss: 1.281, Test loss: 1.064, Test accuracy: 68.42
Round  52, Train loss: 1.319, Test loss: 1.074, Test accuracy: 67.94
Round  53, Train loss: 1.372, Test loss: 1.089, Test accuracy: 67.36
Round  54, Train loss: 1.243, Test loss: 1.069, Test accuracy: 67.93
Round  55, Train loss: 1.296, Test loss: 1.068, Test accuracy: 68.07
Round  56, Train loss: 1.349, Test loss: 1.075, Test accuracy: 68.00
Round  57, Train loss: 1.324, Test loss: 1.066, Test accuracy: 68.36
Round  58, Train loss: 1.257, Test loss: 1.057, Test accuracy: 68.18
Round  59, Train loss: 1.321, Test loss: 1.068, Test accuracy: 68.27
Round  60, Train loss: 1.241, Test loss: 1.053, Test accuracy: 68.53
Round  61, Train loss: 1.345, Test loss: 1.075, Test accuracy: 67.81
Round  62, Train loss: 1.211, Test loss: 1.046, Test accuracy: 68.83
Round  63, Train loss: 1.225, Test loss: 1.040, Test accuracy: 68.85
Round  64, Train loss: 1.216, Test loss: 1.045, Test accuracy: 68.56
Round  65, Train loss: 1.338, Test loss: 1.061, Test accuracy: 68.58
Round  66, Train loss: 1.178, Test loss: 1.035, Test accuracy: 69.00
Round  67, Train loss: 1.362, Test loss: 1.030, Test accuracy: 69.17
Round  68, Train loss: 1.210, Test loss: 1.034, Test accuracy: 68.85
Round  69, Train loss: 1.242, Test loss: 1.049, Test accuracy: 68.26
Round  70, Train loss: 1.210, Test loss: 1.041, Test accuracy: 68.59
Round  71, Train loss: 1.172, Test loss: 1.041, Test accuracy: 68.51
Round  72, Train loss: 1.233, Test loss: 1.047, Test accuracy: 68.75
Round  73, Train loss: 1.247, Test loss: 1.041, Test accuracy: 68.81
Round  74, Train loss: 1.170, Test loss: 1.053, Test accuracy: 68.42
Round  75, Train loss: 1.206, Test loss: 1.056, Test accuracy: 68.61
Round  76, Train loss: 1.162, Test loss: 1.053, Test accuracy: 68.34
Round  77, Train loss: 1.158, Test loss: 1.034, Test accuracy: 69.14
Round  78, Train loss: 1.194, Test loss: 1.057, Test accuracy: 68.46
Round  79, Train loss: 1.199, Test loss: 1.059, Test accuracy: 68.30
Round  80, Train loss: 1.152, Test loss: 1.058, Test accuracy: 68.38
Round  81, Train loss: 1.333, Test loss: 1.045, Test accuracy: 68.71
Round  82, Train loss: 1.203, Test loss: 1.043, Test accuracy: 68.77
Round  83, Train loss: 1.198, Test loss: 1.041, Test accuracy: 68.78
Round  84, Train loss: 1.124, Test loss: 1.044, Test accuracy: 68.72
Round  85, Train loss: 1.121, Test loss: 1.030, Test accuracy: 69.01
Round  86, Train loss: 1.136, Test loss: 1.035, Test accuracy: 68.88
Round  87, Train loss: 1.266, Test loss: 1.064, Test accuracy: 67.89
Round  88, Train loss: 1.207, Test loss: 1.045, Test accuracy: 68.66
Round  89, Train loss: 1.190, Test loss: 1.038, Test accuracy: 68.81
Round  90, Train loss: 1.060, Test loss: 1.031, Test accuracy: 68.73
Round  91, Train loss: 1.188, Test loss: 1.046, Test accuracy: 68.53
Round  92, Train loss: 1.130, Test loss: 1.056, Test accuracy: 68.22
Round  93, Train loss: 1.144, Test loss: 1.035, Test accuracy: 69.08
Round  94, Train loss: 1.088, Test loss: 1.034, Test accuracy: 69.00
Round  95, Train loss: 1.165, Test loss: 1.038, Test accuracy: 68.88
Round  96, Train loss: 1.163, Test loss: 1.051, Test accuracy: 68.64
Round  97, Train loss: 1.065, Test loss: 1.027, Test accuracy: 69.11
Round  98, Train loss: 1.085, Test loss: 1.046, Test accuracy: 68.69
Round  99, Train loss: 1.203, Test loss: 1.059, Test accuracy: 68.21
Final Round, Train loss: 1.083, Test loss: 1.053, Test accuracy: 68.13
Average accuracy final 10 rounds: 68.7085
6080.394614696503
[5.848273038864136, 10.749822616577148, 15.85139536857605, 21.203661918640137, 26.143808603286743, 31.019699573516846, 36.37524747848511, 41.63385581970215, 46.586806297302246, 51.525532722473145, 56.41939187049866, 61.32214689254761, 66.19523739814758, 71.08305239677429, 76.17667746543884, 81.08077883720398, 85.98706555366516, 90.89033508300781, 95.77151656150818, 100.63207411766052, 105.59930300712585, 114.13174962997437, 122.3515374660492, 130.5684094429016, 138.7033188343048, 147.01815509796143, 155.22511649131775, 163.47013664245605, 171.6438558101654, 180.0878849029541, 188.9288785457611, 197.54108810424805, 206.1669418811798, 214.76895308494568, 223.38091659545898, 231.99326467514038, 240.60307216644287, 249.22254276275635, 257.9008421897888, 266.61261010169983, 275.2888367176056, 284.2691082954407, 293.28573727607727, 302.25063967704773, 311.22648310661316, 320.2045257091522, 329.5302104949951, 338.8819947242737, 348.41887044906616, 357.76868057250977, 367.5941767692566, 377.3905222415924, 387.1296694278717, 397.08972787857056, 406.97071075439453, 416.8205542564392, 426.6359965801239, 436.4312243461609, 446.14306926727295, 455.9729905128479, 465.75077962875366, 475.55064272880554, 485.31949949264526, 495.1148245334625, 504.8799443244934, 514.8204047679901, 524.7136881351471, 534.4489216804504, 544.3301255702972, 554.1540727615356, 563.8724637031555, 573.6675486564636, 583.4919326305389, 593.4039974212646, 603.2885131835938, 613.1020822525024, 623.0265054702759, 632.8548586368561, 642.6698842048645, 652.4113202095032, 662.1105434894562, 671.8868360519409, 681.6997056007385, 691.4678893089294, 701.2724001407623, 710.979926109314, 720.8653216362, 730.7011437416077, 740.4499177932739, 750.1737306118011, 759.9080245494843, 769.613765001297, 779.3825924396515, 789.1558611392975, 798.885448217392, 808.6045064926147, 818.4199736118317, 828.190452337265, 837.892938375473, 847.5774364471436, 849.8509502410889]
[27.6575, 34.93, 38.9025, 41.1825, 43.07, 45.7325, 46.9775, 48.395, 49.5725, 51.3775, 53.325, 53.98, 54.965, 56.06, 57.4675, 58.455, 58.6975, 59.11, 59.42, 60.285, 61.235, 61.775, 61.86, 62.8475, 62.815, 63.5475, 64.11, 64.295, 64.5, 64.555, 64.8525, 65.355, 64.9825, 66.04, 65.59, 65.17, 65.7025, 66.035, 65.825, 66.7125, 66.5875, 66.87, 67.1025, 67.5475, 67.1775, 67.68, 67.41, 67.1, 67.7775, 67.8975, 67.5125, 68.4225, 67.935, 67.3625, 67.9325, 68.0675, 67.9975, 68.3575, 68.1775, 68.27, 68.535, 67.81, 68.8275, 68.8475, 68.5625, 68.58, 69.005, 69.165, 68.85, 68.2575, 68.5875, 68.5075, 68.7475, 68.815, 68.415, 68.605, 68.3375, 69.14, 68.4625, 68.3, 68.38, 68.7125, 68.7725, 68.7775, 68.7175, 69.01, 68.8825, 67.8875, 68.6625, 68.815, 68.7325, 68.53, 68.2225, 69.0775, 69.0025, 68.8775, 68.635, 69.1075, 68.6875, 68.2125, 68.1325]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.093, Test loss: 1.972, Test accuracy: 25.45
Round   0, Global train loss: 1.093, Global test loss: 2.334, Global test accuracy: 16.02
Round   1, Train loss: 0.895, Test loss: 1.378, Test accuracy: 47.29
Round   1, Global train loss: 0.895, Global test loss: 2.121, Global test accuracy: 25.87
Round   2, Train loss: 0.782, Test loss: 1.135, Test accuracy: 53.29
Round   2, Global train loss: 0.782, Global test loss: 2.112, Global test accuracy: 24.94
Round   3, Train loss: 0.708, Test loss: 0.952, Test accuracy: 59.63
Round   3, Global train loss: 0.708, Global test loss: 2.075, Global test accuracy: 28.46
Round   4, Train loss: 0.656, Test loss: 0.880, Test accuracy: 62.11
Round   4, Global train loss: 0.656, Global test loss: 2.206, Global test accuracy: 13.23
Round   5, Train loss: 0.581, Test loss: 0.875, Test accuracy: 64.78
Round   5, Global train loss: 0.581, Global test loss: 2.733, Global test accuracy: 21.29
Round   6, Train loss: 0.777, Test loss: 0.738, Test accuracy: 67.78
Round   6, Global train loss: 0.777, Global test loss: 2.101, Global test accuracy: 20.67
Round   7, Train loss: 0.588, Test loss: 0.745, Test accuracy: 68.33
Round   7, Global train loss: 0.588, Global test loss: 2.039, Global test accuracy: 28.66
Round   8, Train loss: 0.673, Test loss: 0.661, Test accuracy: 70.01
Round   8, Global train loss: 0.673, Global test loss: 2.029, Global test accuracy: 28.01
Round   9, Train loss: 0.681, Test loss: 0.655, Test accuracy: 70.52
Round   9, Global train loss: 0.681, Global test loss: 2.049, Global test accuracy: 29.97
Round  10, Train loss: 0.539, Test loss: 0.673, Test accuracy: 70.42
Round  10, Global train loss: 0.539, Global test loss: 2.102, Global test accuracy: 24.73
Round  11, Train loss: 0.608, Test loss: 0.648, Test accuracy: 71.51
Round  11, Global train loss: 0.608, Global test loss: 2.112, Global test accuracy: 26.40
Round  12, Train loss: 0.585, Test loss: 0.641, Test accuracy: 71.80
Round  12, Global train loss: 0.585, Global test loss: 2.124, Global test accuracy: 28.90
Round  13, Train loss: 0.627, Test loss: 0.631, Test accuracy: 72.72
Round  13, Global train loss: 0.627, Global test loss: 2.049, Global test accuracy: 31.04
Round  14, Train loss: 0.549, Test loss: 0.647, Test accuracy: 72.28
Round  14, Global train loss: 0.549, Global test loss: 2.079, Global test accuracy: 25.77
Round  15, Train loss: 0.476, Test loss: 0.636, Test accuracy: 72.93
Round  15, Global train loss: 0.476, Global test loss: 2.466, Global test accuracy: 21.78
Round  16, Train loss: 0.577, Test loss: 0.627, Test accuracy: 73.63
Round  16, Global train loss: 0.577, Global test loss: 2.039, Global test accuracy: 26.81
Round  17, Train loss: 0.494, Test loss: 0.602, Test accuracy: 74.64
Round  17, Global train loss: 0.494, Global test loss: 1.978, Global test accuracy: 28.89
Round  18, Train loss: 0.475, Test loss: 0.617, Test accuracy: 74.35
Round  18, Global train loss: 0.475, Global test loss: 2.030, Global test accuracy: 31.07
Round  19, Train loss: 0.551, Test loss: 0.616, Test accuracy: 74.71
Round  19, Global train loss: 0.551, Global test loss: 2.129, Global test accuracy: 28.54
Round  20, Train loss: 0.430, Test loss: 0.611, Test accuracy: 74.94
Round  20, Global train loss: 0.430, Global test loss: 2.106, Global test accuracy: 24.82
Round  21, Train loss: 0.513, Test loss: 0.612, Test accuracy: 75.27
Round  21, Global train loss: 0.513, Global test loss: 2.056, Global test accuracy: 34.82
Round  22, Train loss: 0.464, Test loss: 0.599, Test accuracy: 75.82
Round  22, Global train loss: 0.464, Global test loss: 2.142, Global test accuracy: 23.70
Round  23, Train loss: 0.483, Test loss: 0.598, Test accuracy: 76.04
Round  23, Global train loss: 0.483, Global test loss: 2.049, Global test accuracy: 30.23
Round  24, Train loss: 0.469, Test loss: 0.594, Test accuracy: 76.09
Round  24, Global train loss: 0.469, Global test loss: 2.164, Global test accuracy: 16.27
Round  25, Train loss: 0.405, Test loss: 0.598, Test accuracy: 76.04
Round  25, Global train loss: 0.405, Global test loss: 2.066, Global test accuracy: 27.58
Round  26, Train loss: 0.351, Test loss: 0.610, Test accuracy: 75.86
Round  26, Global train loss: 0.351, Global test loss: 2.013, Global test accuracy: 28.12
Round  27, Train loss: 0.348, Test loss: 0.609, Test accuracy: 76.14
Round  27, Global train loss: 0.348, Global test loss: 2.076, Global test accuracy: 25.17
Round  28, Train loss: 0.477, Test loss: 0.597, Test accuracy: 76.72
Round  28, Global train loss: 0.477, Global test loss: 2.182, Global test accuracy: 20.39
Round  29, Train loss: 0.419, Test loss: 0.595, Test accuracy: 76.62
Round  29, Global train loss: 0.419, Global test loss: 2.179, Global test accuracy: 17.32
Round  30, Train loss: 0.364, Test loss: 0.589, Test accuracy: 76.83
Round  30, Global train loss: 0.364, Global test loss: 2.154, Global test accuracy: 25.67
Round  31, Train loss: 0.336, Test loss: 0.588, Test accuracy: 77.05
Round  31, Global train loss: 0.336, Global test loss: 1.966, Global test accuracy: 30.29
Round  32, Train loss: 0.406, Test loss: 0.589, Test accuracy: 77.25
Round  32, Global train loss: 0.406, Global test loss: 2.004, Global test accuracy: 29.98
Round  33, Train loss: 0.342, Test loss: 0.601, Test accuracy: 77.38
Round  33, Global train loss: 0.342, Global test loss: 2.075, Global test accuracy: 25.05
Round  34, Train loss: 0.353, Test loss: 0.598, Test accuracy: 77.62
Round  34, Global train loss: 0.353, Global test loss: 2.065, Global test accuracy: 22.26
Round  35, Train loss: 0.288, Test loss: 0.587, Test accuracy: 77.76
Round  35, Global train loss: 0.288, Global test loss: 2.108, Global test accuracy: 20.77
Round  36, Train loss: 0.391, Test loss: 0.593, Test accuracy: 78.08
Round  36, Global train loss: 0.391, Global test loss: 2.012, Global test accuracy: 29.12
Round  37, Train loss: 0.395, Test loss: 0.604, Test accuracy: 77.84
Round  37, Global train loss: 0.395, Global test loss: 2.190, Global test accuracy: 19.98
Round  38, Train loss: 0.390, Test loss: 0.629, Test accuracy: 77.51
Round  38, Global train loss: 0.390, Global test loss: 2.130, Global test accuracy: 24.77
Round  39, Train loss: 0.292, Test loss: 0.626, Test accuracy: 77.80
Round  39, Global train loss: 0.292, Global test loss: 2.333, Global test accuracy: 20.10
Round  40, Train loss: 0.273, Test loss: 0.652, Test accuracy: 77.19
Round  40, Global train loss: 0.273, Global test loss: 2.021, Global test accuracy: 30.81
Round  41, Train loss: 0.206, Test loss: 0.647, Test accuracy: 77.25
Round  41, Global train loss: 0.206, Global test loss: 2.124, Global test accuracy: 22.70
Round  42, Train loss: 0.225, Test loss: 0.650, Test accuracy: 77.40
Round  42, Global train loss: 0.225, Global test loss: 2.115, Global test accuracy: 27.84
Round  43, Train loss: 0.327, Test loss: 0.645, Test accuracy: 77.85
Round  43, Global train loss: 0.327, Global test loss: 2.110, Global test accuracy: 23.15
Round  44, Train loss: 0.245, Test loss: 0.664, Test accuracy: 77.71
Round  44, Global train loss: 0.245, Global test loss: 2.141, Global test accuracy: 18.35
Round  45, Train loss: 0.231, Test loss: 0.680, Test accuracy: 77.65
Round  45, Global train loss: 0.231, Global test loss: 2.224, Global test accuracy: 16.38
Round  46, Train loss: 0.264, Test loss: 0.686, Test accuracy: 77.79
Round  46, Global train loss: 0.264, Global test loss: 2.026, Global test accuracy: 28.48
Round  47, Train loss: 0.266, Test loss: 0.707, Test accuracy: 77.89
Round  47, Global train loss: 0.266, Global test loss: 2.050, Global test accuracy: 32.77
Round  48, Train loss: 0.284, Test loss: 0.700, Test accuracy: 78.00
Round  48, Global train loss: 0.284, Global test loss: 2.031, Global test accuracy: 30.93
Round  49, Train loss: 0.247, Test loss: 0.697, Test accuracy: 77.78
Round  49, Global train loss: 0.247, Global test loss: 2.090, Global test accuracy: 28.05
Round  50, Train loss: 0.181, Test loss: 0.697, Test accuracy: 77.91
Round  50, Global train loss: 0.181, Global test loss: 2.111, Global test accuracy: 25.50
Round  51, Train loss: 0.188, Test loss: 0.710, Test accuracy: 78.12
Round  51, Global train loss: 0.188, Global test loss: 2.103, Global test accuracy: 32.20
Round  52, Train loss: 0.195, Test loss: 0.715, Test accuracy: 78.14
Round  52, Global train loss: 0.195, Global test loss: 2.338, Global test accuracy: 20.58
Round  53, Train loss: 0.212, Test loss: 0.722, Test accuracy: 78.04
Round  53, Global train loss: 0.212, Global test loss: 2.033, Global test accuracy: 28.22
Round  54, Train loss: 0.193, Test loss: 0.745, Test accuracy: 77.72
Round  54, Global train loss: 0.193, Global test loss: 2.217, Global test accuracy: 16.67
Round  55, Train loss: 0.156, Test loss: 0.746, Test accuracy: 78.12
Round  55, Global train loss: 0.156, Global test loss: 1.968, Global test accuracy: 32.83
Round  56, Train loss: 0.157, Test loss: 0.758, Test accuracy: 77.85
Round  56, Global train loss: 0.157, Global test loss: 2.025, Global test accuracy: 34.71
Round  57, Train loss: 0.133, Test loss: 0.751, Test accuracy: 78.26
Round  57, Global train loss: 0.133, Global test loss: 2.061, Global test accuracy: 25.68
Round  58, Train loss: 0.229, Test loss: 0.763, Test accuracy: 78.32
Round  58, Global train loss: 0.229, Global test loss: 2.087, Global test accuracy: 24.66
Round  59, Train loss: 0.184, Test loss: 0.776, Test accuracy: 78.01
Round  59, Global train loss: 0.184, Global test loss: 1.971, Global test accuracy: 27.14
Round  60, Train loss: 0.143, Test loss: 0.781, Test accuracy: 78.25
Round  60, Global train loss: 0.143, Global test loss: 2.124, Global test accuracy: 20.17
Round  61, Train loss: 0.131, Test loss: 0.798, Test accuracy: 77.95
Round  61, Global train loss: 0.131, Global test loss: 1.999, Global test accuracy: 30.73
Round  62, Train loss: 0.190, Test loss: 0.809, Test accuracy: 78.00
Round  62, Global train loss: 0.190, Global test loss: 2.106, Global test accuracy: 23.19
Round  63, Train loss: 0.184, Test loss: 0.806, Test accuracy: 78.25
Round  63, Global train loss: 0.184, Global test loss: 2.038, Global test accuracy: 32.39
Round  64, Train loss: 0.147, Test loss: 0.817, Test accuracy: 78.22
Round  64, Global train loss: 0.147, Global test loss: 1.991, Global test accuracy: 30.12
Round  65, Train loss: 0.166, Test loss: 0.798, Test accuracy: 78.53
Round  65, Global train loss: 0.166, Global test loss: 2.066, Global test accuracy: 25.72
Round  66, Train loss: 0.161, Test loss: 0.805, Test accuracy: 78.50
Round  66, Global train loss: 0.161, Global test loss: 2.099, Global test accuracy: 28.53
Round  67, Train loss: 0.166, Test loss: 0.843, Test accuracy: 78.16
Round  67, Global train loss: 0.166, Global test loss: 2.105, Global test accuracy: 26.72
Round  68, Train loss: 0.134, Test loss: 0.856, Test accuracy: 78.45
Round  68, Global train loss: 0.134, Global test loss: 2.091, Global test accuracy: 30.96
Round  69, Train loss: 0.127, Test loss: 0.844, Test accuracy: 78.64
Round  69, Global train loss: 0.127, Global test loss: 2.127, Global test accuracy: 22.26
Round  70, Train loss: 0.163, Test loss: 0.868, Test accuracy: 78.47
Round  70, Global train loss: 0.163, Global test loss: 2.196, Global test accuracy: 16.89
Round  71, Train loss: 0.091, Test loss: 0.883, Test accuracy: 78.28
Round  71, Global train loss: 0.091, Global test loss: 2.080, Global test accuracy: 26.62
Round  72, Train loss: 0.106, Test loss: 0.883, Test accuracy: 78.15
Round  72, Global train loss: 0.106, Global test loss: 2.016, Global test accuracy: 25.94
Round  73, Train loss: 0.124, Test loss: 0.876, Test accuracy: 78.28
Round  73, Global train loss: 0.124, Global test loss: 2.103, Global test accuracy: 23.41
Round  74, Train loss: 0.163, Test loss: 0.878, Test accuracy: 78.49
Round  74, Global train loss: 0.163, Global test loss: 2.189, Global test accuracy: 21.92
Round  75, Train loss: 0.096, Test loss: 0.900, Test accuracy: 78.17
Round  75, Global train loss: 0.096, Global test loss: 2.245, Global test accuracy: 18.10
Round  76, Train loss: 0.114, Test loss: 0.884, Test accuracy: 78.71
Round  76, Global train loss: 0.114, Global test loss: 2.073, Global test accuracy: 30.02
Round  77, Train loss: 0.116, Test loss: 0.892, Test accuracy: 78.96
Round  77, Global train loss: 0.116, Global test loss: 2.104, Global test accuracy: 25.79
Round  78, Train loss: 0.102, Test loss: 0.911, Test accuracy: 78.78
Round  78, Global train loss: 0.102, Global test loss: 2.135, Global test accuracy: 21.68
Round  79, Train loss: 0.093, Test loss: 0.933, Test accuracy: 78.32
Round  79, Global train loss: 0.093, Global test loss: 2.048, Global test accuracy: 28.23
Round  80, Train loss: 0.112, Test loss: 0.926, Test accuracy: 78.57
Round  80, Global train loss: 0.112, Global test loss: 2.043, Global test accuracy: 23.73
Round  81, Train loss: 0.078, Test loss: 0.947, Test accuracy: 78.46
Round  81, Global train loss: 0.078, Global test loss: 2.235, Global test accuracy: 10.34
Round  82, Train loss: 0.087, Test loss: 0.937, Test accuracy: 78.46
Round  82, Global train loss: 0.087, Global test loss: 2.067, Global test accuracy: 28.68
Round  83, Train loss: 0.116, Test loss: 0.948, Test accuracy: 78.02
Round  83, Global train loss: 0.116, Global test loss: 2.137, Global test accuracy: 26.24
Round  84, Train loss: 0.083, Test loss: 0.966, Test accuracy: 78.16
Round  84, Global train loss: 0.083, Global test loss: 2.018, Global test accuracy: 32.79
Round  85, Train loss: 0.087, Test loss: 0.962, Test accuracy: 77.98
Round  85, Global train loss: 0.087, Global test loss: 2.396, Global test accuracy: 23.94
Round  86, Train loss: 0.080, Test loss: 0.966, Test accuracy: 78.39
Round  86, Global train loss: 0.080, Global test loss: 2.150, Global test accuracy: 17.84
Round  87, Train loss: 0.072, Test loss: 0.973, Test accuracy: 78.72
Round  87, Global train loss: 0.072, Global test loss: 2.021, Global test accuracy: 29.27
Round  88, Train loss: 0.067, Test loss: 1.003, Test accuracy: 78.57
Round  88, Global train loss: 0.067, Global test loss: 2.162, Global test accuracy: 25.87
Round  89, Train loss: 0.083, Test loss: 0.990, Test accuracy: 78.87
Round  89, Global train loss: 0.083, Global test loss: 2.168, Global test accuracy: 16.01
Round  90, Train loss: 0.078, Test loss: 0.993, Test accuracy: 79.17
Round  90, Global train loss: 0.078, Global test loss: 2.111, Global test accuracy: 27.20
Round  91, Train loss: 0.056, Test loss: 1.017, Test accuracy: 78.76
Round  91, Global train loss: 0.056, Global test loss: 2.157, Global test accuracy: 17.73
Round  92, Train loss: 0.064, Test loss: 1.031, Test accuracy: 78.53
Round  92, Global train loss: 0.064, Global test loss: 2.558, Global test accuracy: 25.83
Round  93, Train loss: 0.070, Test loss: 1.034, Test accuracy: 78.72
Round  93, Global train loss: 0.070, Global test loss: 2.071, Global test accuracy: 20.73
Round  94, Train loss: 0.080, Test loss: 1.070, Test accuracy: 78.50
Round  94, Global train loss: 0.080, Global test loss: 2.095, Global test accuracy: 25.83
Round  95, Train loss: 0.072, Test loss: 1.071, Test accuracy: 78.82
Round  95, Global train loss: 0.072, Global test loss: 2.314, Global test accuracy: 23.52
Round  96, Train loss: 0.090, Test loss: 1.065, Test accuracy: 78.55
Round  96, Global train loss: 0.090, Global test loss: 2.166, Global test accuracy: 18.83
Round  97, Train loss: 0.065, Test loss: 1.030, Test accuracy: 79.16
Round  97, Global train loss: 0.065, Global test loss: 2.084, Global test accuracy: 25.18
Round  98, Train loss: 0.073, Test loss: 1.045, Test accuracy: 79.17
Round  98, Global train loss: 0.073, Global test loss: 2.085, Global test accuracy: 27.04
Round  99, Train loss: 0.061, Test loss: 1.001, Test accuracy: 79.37
Round  99, Global train loss: 0.061, Global test loss: 2.149, Global test accuracy: 24.22
Final Round, Train loss: 0.062, Test loss: 1.072, Test accuracy: 78.92
Final Round, Global train loss: 0.062, Global test loss: 2.149, Global test accuracy: 24.22
Average accuracy final 10 rounds: 78.87333333333333 

Average global accuracy final 10 rounds: 23.610833333333332 

1847.946459054947
[1.5834565162658691, 3.1669130325317383, 4.6345055103302, 6.102097988128662, 7.486349582672119, 8.870601177215576, 10.233997583389282, 11.597393989562988, 13.001781940460205, 14.406169891357422, 15.802618026733398, 17.199066162109375, 18.593493700027466, 19.987921237945557, 21.38764977455139, 22.787378311157227, 24.223522663116455, 25.659667015075684, 27.121293306350708, 28.582919597625732, 29.997329473495483, 31.411739349365234, 32.90027070045471, 34.38880205154419, 35.75079965591431, 37.112797260284424, 38.496164083480835, 39.879530906677246, 41.27423143386841, 42.66893196105957, 44.05313992500305, 45.43734788894653, 46.80294346809387, 48.16853904724121, 49.56766653060913, 50.96679401397705, 52.36625027656555, 53.76570653915405, 55.18911647796631, 56.612526416778564, 58.010629653930664, 59.408732891082764, 60.812721252441406, 62.21670961380005, 63.66382575035095, 65.11094188690186, 66.51096868515015, 67.91099548339844, 69.29289937019348, 70.67480325698853, 72.06764125823975, 73.46047925949097, 74.84725046157837, 76.23402166366577, 77.60916304588318, 78.98430442810059, 80.36962604522705, 81.75494766235352, 83.18010139465332, 84.60525512695312, 86.05279874801636, 87.50034236907959, 88.92099380493164, 90.34164524078369, 91.71407699584961, 93.08650875091553, 94.49500799179077, 95.90350723266602, 97.3016300201416, 98.69975280761719, 100.08514428138733, 101.47053575515747, 102.84940958023071, 104.22828340530396, 105.60769581794739, 106.98710823059082, 108.37890887260437, 109.77070951461792, 111.189612865448, 112.60851621627808, 114.02090787887573, 115.43329954147339, 116.86271095275879, 118.29212236404419, 119.69818472862244, 121.10424709320068, 122.5212676525116, 123.93828821182251, 125.3738784790039, 126.8094687461853, 128.18917536735535, 129.5688819885254, 130.96199083328247, 132.35509967803955, 133.7274558544159, 135.09981203079224, 136.47535276412964, 137.85089349746704, 139.2751965522766, 140.69949960708618, 142.11452674865723, 143.52955389022827, 144.95457410812378, 146.3795943260193, 147.79422044754028, 149.20884656906128, 150.6034436225891, 151.99804067611694, 153.3777196407318, 154.75739860534668, 156.185542345047, 157.61368608474731, 159.0094771385193, 160.40526819229126, 161.78721809387207, 163.16916799545288, 164.54407238960266, 165.91897678375244, 167.32374596595764, 168.72851514816284, 170.09387016296387, 171.4592251777649, 172.8377227783203, 174.21622037887573, 175.59148144721985, 176.96674251556396, 178.34484696388245, 179.72295141220093, 181.11544108390808, 182.50793075561523, 183.91053104400635, 185.31313133239746, 186.74703216552734, 188.18093299865723, 189.62093353271484, 191.06093406677246, 192.45659375190735, 193.85225343704224, 195.23412132263184, 196.61598920822144, 197.99564599990845, 199.37530279159546, 200.73830437660217, 202.1013059616089, 203.46650958061218, 204.83171319961548, 206.19740104675293, 207.56308889389038, 208.95747661590576, 210.35186433792114, 211.72771167755127, 213.1035590171814, 214.4969084262848, 215.89025783538818, 217.3410460948944, 218.79183435440063, 220.21985459327698, 221.64787483215332, 223.0373764038086, 224.42687797546387, 225.80005860328674, 227.17323923110962, 228.5758295059204, 229.9784197807312, 231.36995267868042, 232.76148557662964, 234.15372395515442, 235.5459623336792, 236.91637778282166, 238.2867932319641, 239.66320872306824, 241.03962421417236, 242.407329082489, 243.77503395080566, 245.19043374061584, 246.60583353042603, 248.04825687408447, 249.49068021774292, 251.06284093856812, 252.6350016593933, 254.03553676605225, 255.43607187271118, 256.8064684867859, 258.1768651008606, 259.57047271728516, 260.9640803337097, 262.37068581581116, 263.7772912979126, 265.2688663005829, 266.7604413032532, 268.27730679512024, 269.7941722869873, 271.2483251094818, 272.7024779319763, 274.0672242641449, 275.4319705963135, 276.8164396286011, 278.2009086608887, 279.62193417549133, 281.042959690094, 283.3554217815399, 285.66788387298584]
[25.45, 25.45, 47.291666666666664, 47.291666666666664, 53.291666666666664, 53.291666666666664, 59.63333333333333, 59.63333333333333, 62.108333333333334, 62.108333333333334, 64.775, 64.775, 67.78333333333333, 67.78333333333333, 68.33333333333333, 68.33333333333333, 70.00833333333334, 70.00833333333334, 70.51666666666667, 70.51666666666667, 70.41666666666667, 70.41666666666667, 71.50833333333334, 71.50833333333334, 71.8, 71.8, 72.71666666666667, 72.71666666666667, 72.28333333333333, 72.28333333333333, 72.93333333333334, 72.93333333333334, 73.63333333333334, 73.63333333333334, 74.64166666666667, 74.64166666666667, 74.35, 74.35, 74.70833333333333, 74.70833333333333, 74.94166666666666, 74.94166666666666, 75.26666666666667, 75.26666666666667, 75.81666666666666, 75.81666666666666, 76.04166666666667, 76.04166666666667, 76.09166666666667, 76.09166666666667, 76.04166666666667, 76.04166666666667, 75.85833333333333, 75.85833333333333, 76.14166666666667, 76.14166666666667, 76.725, 76.725, 76.625, 76.625, 76.825, 76.825, 77.05, 77.05, 77.25, 77.25, 77.375, 77.375, 77.61666666666666, 77.61666666666666, 77.75833333333334, 77.75833333333334, 78.08333333333333, 78.08333333333333, 77.84166666666667, 77.84166666666667, 77.50833333333334, 77.50833333333334, 77.8, 77.8, 77.19166666666666, 77.19166666666666, 77.25, 77.25, 77.4, 77.4, 77.85, 77.85, 77.70833333333333, 77.70833333333333, 77.65, 77.65, 77.79166666666667, 77.79166666666667, 77.89166666666667, 77.89166666666667, 78.0, 78.0, 77.78333333333333, 77.78333333333333, 77.90833333333333, 77.90833333333333, 78.11666666666666, 78.11666666666666, 78.14166666666667, 78.14166666666667, 78.04166666666667, 78.04166666666667, 77.71666666666667, 77.71666666666667, 78.11666666666666, 78.11666666666666, 77.85, 77.85, 78.25833333333334, 78.25833333333334, 78.31666666666666, 78.31666666666666, 78.00833333333334, 78.00833333333334, 78.25, 78.25, 77.95, 77.95, 78.0, 78.0, 78.25, 78.25, 78.21666666666667, 78.21666666666667, 78.525, 78.525, 78.5, 78.5, 78.15833333333333, 78.15833333333333, 78.45, 78.45, 78.64166666666667, 78.64166666666667, 78.475, 78.475, 78.275, 78.275, 78.15, 78.15, 78.275, 78.275, 78.49166666666666, 78.49166666666666, 78.175, 78.175, 78.70833333333333, 78.70833333333333, 78.95833333333333, 78.95833333333333, 78.775, 78.775, 78.31666666666666, 78.31666666666666, 78.56666666666666, 78.56666666666666, 78.45833333333333, 78.45833333333333, 78.45833333333333, 78.45833333333333, 78.01666666666667, 78.01666666666667, 78.15833333333333, 78.15833333333333, 77.98333333333333, 77.98333333333333, 78.39166666666667, 78.39166666666667, 78.725, 78.725, 78.56666666666666, 78.56666666666666, 78.86666666666666, 78.86666666666666, 79.16666666666667, 79.16666666666667, 78.75833333333334, 78.75833333333334, 78.525, 78.525, 78.725, 78.725, 78.5, 78.5, 78.81666666666666, 78.81666666666666, 78.55, 78.55, 79.15833333333333, 79.15833333333333, 79.16666666666667, 79.16666666666667, 79.36666666666666, 79.36666666666666, 78.925, 78.925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.014, Test loss: 1.876, Test accuracy: 28.32
Round   0, Global train loss: 1.014, Global test loss: 2.239, Global test accuracy: 20.19
Round   1, Train loss: 0.902, Test loss: 1.672, Test accuracy: 37.71
Round   1, Global train loss: 0.902, Global test loss: 2.184, Global test accuracy: 22.71
Round   2, Train loss: 0.895, Test loss: 1.372, Test accuracy: 47.38
Round   2, Global train loss: 0.895, Global test loss: 2.055, Global test accuracy: 28.77
Round   3, Train loss: 0.840, Test loss: 1.400, Test accuracy: 45.66
Round   3, Global train loss: 0.840, Global test loss: 2.266, Global test accuracy: 20.08
Round   4, Train loss: 0.769, Test loss: 0.912, Test accuracy: 62.41
Round   4, Global train loss: 0.769, Global test loss: 1.796, Global test accuracy: 36.59
Round   5, Train loss: 0.828, Test loss: 0.803, Test accuracy: 65.62
Round   5, Global train loss: 0.828, Global test loss: 1.781, Global test accuracy: 36.83
Round   6, Train loss: 0.686, Test loss: 0.820, Test accuracy: 64.63
Round   6, Global train loss: 0.686, Global test loss: 1.840, Global test accuracy: 33.26
Round   7, Train loss: 0.683, Test loss: 0.810, Test accuracy: 66.42
Round   7, Global train loss: 0.683, Global test loss: 1.935, Global test accuracy: 34.42
Round   8, Train loss: 0.636, Test loss: 0.703, Test accuracy: 69.47
Round   8, Global train loss: 0.636, Global test loss: 1.635, Global test accuracy: 42.12
Round   9, Train loss: 0.697, Test loss: 0.654, Test accuracy: 71.54
Round   9, Global train loss: 0.697, Global test loss: 1.642, Global test accuracy: 40.19
Round  10, Train loss: 0.614, Test loss: 0.640, Test accuracy: 72.66
Round  10, Global train loss: 0.614, Global test loss: 1.650, Global test accuracy: 39.14
Round  11, Train loss: 0.639, Test loss: 0.616, Test accuracy: 73.84
Round  11, Global train loss: 0.639, Global test loss: 1.703, Global test accuracy: 40.41
Round  12, Train loss: 0.616, Test loss: 0.626, Test accuracy: 73.03
Round  12, Global train loss: 0.616, Global test loss: 1.850, Global test accuracy: 35.04
Round  13, Train loss: 0.647, Test loss: 0.606, Test accuracy: 73.97
Round  13, Global train loss: 0.647, Global test loss: 1.720, Global test accuracy: 39.83
Round  14, Train loss: 0.593, Test loss: 0.624, Test accuracy: 73.21
Round  14, Global train loss: 0.593, Global test loss: 1.695, Global test accuracy: 41.33
Round  15, Train loss: 0.692, Test loss: 0.594, Test accuracy: 75.23
Round  15, Global train loss: 0.692, Global test loss: 1.735, Global test accuracy: 36.31
Round  16, Train loss: 0.640, Test loss: 0.592, Test accuracy: 75.21
Round  16, Global train loss: 0.640, Global test loss: 1.441, Global test accuracy: 48.98
Round  17, Train loss: 0.639, Test loss: 0.585, Test accuracy: 75.54
Round  17, Global train loss: 0.639, Global test loss: 1.680, Global test accuracy: 43.17
Round  18, Train loss: 0.549, Test loss: 0.573, Test accuracy: 76.26
Round  18, Global train loss: 0.549, Global test loss: 1.641, Global test accuracy: 40.48
Round  19, Train loss: 0.583, Test loss: 0.557, Test accuracy: 76.77
Round  19, Global train loss: 0.583, Global test loss: 1.603, Global test accuracy: 44.13
Round  20, Train loss: 0.576, Test loss: 0.547, Test accuracy: 77.21
Round  20, Global train loss: 0.576, Global test loss: 1.482, Global test accuracy: 48.68
Round  21, Train loss: 0.489, Test loss: 0.540, Test accuracy: 77.71
Round  21, Global train loss: 0.489, Global test loss: 1.454, Global test accuracy: 51.38
Round  22, Train loss: 0.462, Test loss: 0.542, Test accuracy: 77.72
Round  22, Global train loss: 0.462, Global test loss: 1.525, Global test accuracy: 51.81
Round  23, Train loss: 0.583, Test loss: 0.541, Test accuracy: 77.83
Round  23, Global train loss: 0.583, Global test loss: 1.361, Global test accuracy: 52.11
Round  24, Train loss: 0.532, Test loss: 0.558, Test accuracy: 77.37
Round  24, Global train loss: 0.532, Global test loss: 1.294, Global test accuracy: 53.68
Round  25, Train loss: 0.466, Test loss: 0.541, Test accuracy: 78.22
Round  25, Global train loss: 0.466, Global test loss: 1.409, Global test accuracy: 51.76
Round  26, Train loss: 0.430, Test loss: 0.538, Test accuracy: 78.17
Round  26, Global train loss: 0.430, Global test loss: 1.350, Global test accuracy: 52.92
Round  27, Train loss: 0.556, Test loss: 0.525, Test accuracy: 79.04
Round  27, Global train loss: 0.556, Global test loss: 1.431, Global test accuracy: 50.19
Round  28, Train loss: 0.498, Test loss: 0.528, Test accuracy: 79.10
Round  28, Global train loss: 0.498, Global test loss: 1.650, Global test accuracy: 44.79
Round  29, Train loss: 0.492, Test loss: 0.508, Test accuracy: 79.89
Round  29, Global train loss: 0.492, Global test loss: 1.518, Global test accuracy: 47.54
Round  30, Train loss: 0.370, Test loss: 0.525, Test accuracy: 79.46
Round  30, Global train loss: 0.370, Global test loss: 1.588, Global test accuracy: 48.37
Round  31, Train loss: 0.506, Test loss: 0.518, Test accuracy: 79.74
Round  31, Global train loss: 0.506, Global test loss: 1.637, Global test accuracy: 45.65
Round  32, Train loss: 0.421, Test loss: 0.521, Test accuracy: 79.77
Round  32, Global train loss: 0.421, Global test loss: 1.329, Global test accuracy: 54.40
Round  33, Train loss: 0.408, Test loss: 0.528, Test accuracy: 79.70
Round  33, Global train loss: 0.408, Global test loss: 1.584, Global test accuracy: 48.97
Round  34, Train loss: 0.376, Test loss: 0.501, Test accuracy: 80.53
Round  34, Global train loss: 0.376, Global test loss: 1.279, Global test accuracy: 56.38
Round  35, Train loss: 0.416, Test loss: 0.486, Test accuracy: 80.96
Round  35, Global train loss: 0.416, Global test loss: 1.470, Global test accuracy: 50.90
Round  36, Train loss: 0.512, Test loss: 0.482, Test accuracy: 81.04
Round  36, Global train loss: 0.512, Global test loss: 1.211, Global test accuracy: 58.67
Round  37, Train loss: 0.326, Test loss: 0.493, Test accuracy: 80.62
Round  37, Global train loss: 0.326, Global test loss: 1.371, Global test accuracy: 57.05
Round  38, Train loss: 0.377, Test loss: 0.508, Test accuracy: 80.53
Round  38, Global train loss: 0.377, Global test loss: 1.392, Global test accuracy: 53.08
Round  39, Train loss: 0.485, Test loss: 0.516, Test accuracy: 80.41
Round  39, Global train loss: 0.485, Global test loss: 1.272, Global test accuracy: 56.56
Round  40, Train loss: 0.401, Test loss: 0.505, Test accuracy: 80.68
Round  40, Global train loss: 0.401, Global test loss: 1.334, Global test accuracy: 55.42
Round  41, Train loss: 0.322, Test loss: 0.512, Test accuracy: 80.55
Round  41, Global train loss: 0.322, Global test loss: 1.501, Global test accuracy: 54.66
Round  42, Train loss: 0.492, Test loss: 0.536, Test accuracy: 79.71
Round  42, Global train loss: 0.492, Global test loss: 1.428, Global test accuracy: 52.46
Round  43, Train loss: 0.337, Test loss: 0.530, Test accuracy: 80.16
Round  43, Global train loss: 0.337, Global test loss: 1.367, Global test accuracy: 53.92
Round  44, Train loss: 0.325, Test loss: 0.521, Test accuracy: 80.32
Round  44, Global train loss: 0.325, Global test loss: 1.308, Global test accuracy: 56.88
Round  45, Train loss: 0.416, Test loss: 0.528, Test accuracy: 80.05
Round  45, Global train loss: 0.416, Global test loss: 1.307, Global test accuracy: 56.42
Round  46, Train loss: 0.370, Test loss: 0.517, Test accuracy: 80.62
Round  46, Global train loss: 0.370, Global test loss: 1.213, Global test accuracy: 58.90
Round  47, Train loss: 0.361, Test loss: 0.505, Test accuracy: 81.09
Round  47, Global train loss: 0.361, Global test loss: 1.341, Global test accuracy: 55.79
Round  48, Train loss: 0.411, Test loss: 0.511, Test accuracy: 80.91
Round  48, Global train loss: 0.411, Global test loss: 1.508, Global test accuracy: 52.94
Round  49, Train loss: 0.236, Test loss: 0.511, Test accuracy: 80.98
Round  49, Global train loss: 0.236, Global test loss: 1.926, Global test accuracy: 46.68
Round  50, Train loss: 0.397, Test loss: 0.502, Test accuracy: 81.69
Round  50, Global train loss: 0.397, Global test loss: 1.183, Global test accuracy: 61.05
Round  51, Train loss: 0.301, Test loss: 0.515, Test accuracy: 81.58
Round  51, Global train loss: 0.301, Global test loss: 1.369, Global test accuracy: 56.64
Round  52, Train loss: 0.357, Test loss: 0.512, Test accuracy: 81.42
Round  52, Global train loss: 0.357, Global test loss: 1.101, Global test accuracy: 63.59
Round  53, Train loss: 0.341, Test loss: 0.499, Test accuracy: 81.81
Round  53, Global train loss: 0.341, Global test loss: 1.386, Global test accuracy: 56.08
Round  54, Train loss: 0.367, Test loss: 0.498, Test accuracy: 81.75
Round  54, Global train loss: 0.367, Global test loss: 1.314, Global test accuracy: 56.20
Round  55, Train loss: 0.331, Test loss: 0.503, Test accuracy: 81.57
Round  55, Global train loss: 0.331, Global test loss: 1.256, Global test accuracy: 59.03
Round  56, Train loss: 0.298, Test loss: 0.494, Test accuracy: 81.78
Round  56, Global train loss: 0.298, Global test loss: 1.247, Global test accuracy: 58.92
Round  57, Train loss: 0.245, Test loss: 0.494, Test accuracy: 81.97
Round  57, Global train loss: 0.245, Global test loss: 1.289, Global test accuracy: 58.95
Round  58, Train loss: 0.322, Test loss: 0.492, Test accuracy: 81.74
Round  58, Global train loss: 0.322, Global test loss: 1.116, Global test accuracy: 63.61
Round  59, Train loss: 0.320, Test loss: 0.508, Test accuracy: 81.52
Round  59, Global train loss: 0.320, Global test loss: 1.253, Global test accuracy: 60.27
Round  60, Train loss: 0.214, Test loss: 0.485, Test accuracy: 82.12
Round  60, Global train loss: 0.214, Global test loss: 1.483, Global test accuracy: 56.90
Round  61, Train loss: 0.369, Test loss: 0.475, Test accuracy: 82.33
Round  61, Global train loss: 0.369, Global test loss: 1.112, Global test accuracy: 62.95
Round  62, Train loss: 0.312, Test loss: 0.473, Test accuracy: 82.61
Round  62, Global train loss: 0.312, Global test loss: 1.128, Global test accuracy: 62.17
Round  63, Train loss: 0.231, Test loss: 0.472, Test accuracy: 82.55
Round  63, Global train loss: 0.231, Global test loss: 1.296, Global test accuracy: 59.77
Round  64, Train loss: 0.327, Test loss: 0.488, Test accuracy: 82.39
Round  64, Global train loss: 0.327, Global test loss: 1.290, Global test accuracy: 58.19
Round  65, Train loss: 0.343, Test loss: 0.488, Test accuracy: 82.50
Round  65, Global train loss: 0.343, Global test loss: 1.227, Global test accuracy: 60.90
Round  66, Train loss: 0.288, Test loss: 0.495, Test accuracy: 82.54
Round  66, Global train loss: 0.288, Global test loss: 1.274, Global test accuracy: 59.74
Round  67, Train loss: 0.336, Test loss: 0.492, Test accuracy: 82.64
Round  67, Global train loss: 0.336, Global test loss: 1.298, Global test accuracy: 57.53
Round  68, Train loss: 0.373, Test loss: 0.481, Test accuracy: 82.77
Round  68, Global train loss: 0.373, Global test loss: 1.453, Global test accuracy: 52.27
Round  69, Train loss: 0.329, Test loss: 0.488, Test accuracy: 82.58
Round  69, Global train loss: 0.329, Global test loss: 1.586, Global test accuracy: 53.68
Round  70, Train loss: 0.219, Test loss: 0.488, Test accuracy: 82.81
Round  70, Global train loss: 0.219, Global test loss: 1.409, Global test accuracy: 57.75
Round  71, Train loss: 0.265, Test loss: 0.502, Test accuracy: 82.40
Round  71, Global train loss: 0.265, Global test loss: 1.150, Global test accuracy: 62.46
Round  72, Train loss: 0.285, Test loss: 0.500, Test accuracy: 82.54
Round  72, Global train loss: 0.285, Global test loss: 1.465, Global test accuracy: 57.04
Round  73, Train loss: 0.376, Test loss: 0.491, Test accuracy: 82.78
Round  73, Global train loss: 0.376, Global test loss: 1.444, Global test accuracy: 54.90
Round  74, Train loss: 0.254, Test loss: 0.509, Test accuracy: 82.46
Round  74, Global train loss: 0.254, Global test loss: 1.105, Global test accuracy: 64.18
Round  75, Train loss: 0.287, Test loss: 0.505, Test accuracy: 82.34
Round  75, Global train loss: 0.287, Global test loss: 1.293, Global test accuracy: 58.77
Round  76, Train loss: 0.282, Test loss: 0.506, Test accuracy: 82.50
Round  76, Global train loss: 0.282, Global test loss: 1.157, Global test accuracy: 62.73
Round  77, Train loss: 0.168, Test loss: 0.505, Test accuracy: 82.65
Round  77, Global train loss: 0.168, Global test loss: 1.352, Global test accuracy: 60.66
Round  78, Train loss: 0.294, Test loss: 0.514, Test accuracy: 82.38
Round  78, Global train loss: 0.294, Global test loss: 1.259, Global test accuracy: 59.69
Round  79, Train loss: 0.203, Test loss: 0.509, Test accuracy: 82.38
Round  79, Global train loss: 0.203, Global test loss: 1.485, Global test accuracy: 56.89
Round  80, Train loss: 0.308, Test loss: 0.503, Test accuracy: 82.84
Round  80, Global train loss: 0.308, Global test loss: 1.142, Global test accuracy: 62.51
Round  81, Train loss: 0.169, Test loss: 0.506, Test accuracy: 82.76
Round  81, Global train loss: 0.169, Global test loss: 1.677, Global test accuracy: 52.23
Round  82, Train loss: 0.233, Test loss: 0.521, Test accuracy: 82.58
Round  82, Global train loss: 0.233, Global test loss: 1.297, Global test accuracy: 59.56
Round  83, Train loss: 0.253, Test loss: 0.522, Test accuracy: 82.42
Round  83, Global train loss: 0.253, Global test loss: 1.199, Global test accuracy: 62.12
Round  84, Train loss: 0.178, Test loss: 0.525, Test accuracy: 82.51
Round  84, Global train loss: 0.178, Global test loss: 1.370, Global test accuracy: 61.62
Round  85, Train loss: 0.215, Test loss: 0.522, Test accuracy: 82.78
Round  85, Global train loss: 0.215, Global test loss: 1.426, Global test accuracy: 59.02
Round  86, Train loss: 0.215, Test loss: 0.538, Test accuracy: 82.65
Round  86, Global train loss: 0.215, Global test loss: 1.450, Global test accuracy: 58.49
Round  87, Train loss: 0.192, Test loss: 0.535, Test accuracy: 82.68
Round  87, Global train loss: 0.192, Global test loss: 1.666, Global test accuracy: 54.23
Round  88, Train loss: 0.213, Test loss: 0.520, Test accuracy: 82.88
Round  88, Global train loss: 0.213, Global test loss: 1.583, Global test accuracy: 55.24
Round  89, Train loss: 0.295, Test loss: 0.527, Test accuracy: 82.87
Round  89, Global train loss: 0.295, Global test loss: 1.328, Global test accuracy: 59.12
Round  90, Train loss: 0.271, Test loss: 0.548, Test accuracy: 82.43
Round  90, Global train loss: 0.271, Global test loss: 1.319, Global test accuracy: 59.72
Round  91, Train loss: 0.262, Test loss: 0.547, Test accuracy: 82.46
Round  91, Global train loss: 0.262, Global test loss: 1.530, Global test accuracy: 56.88
Round  92, Train loss: 0.310, Test loss: 0.553, Test accuracy: 82.58
Round  92, Global train loss: 0.310, Global test loss: 1.174, Global test accuracy: 61.83
Round  93, Train loss: 0.215, Test loss: 0.575, Test accuracy: 82.24
Round  93, Global train loss: 0.215, Global test loss: 1.210, Global test accuracy: 62.17
Round  94, Train loss: 0.273, Test loss: 0.563, Test accuracy: 82.83
Round  94, Global train loss: 0.273, Global test loss: 1.466, Global test accuracy: 56.89
Round  95, Train loss: 0.285, Test loss: 0.553, Test accuracy: 83.00
Round  95, Global train loss: 0.285, Global test loss: 1.308, Global test accuracy: 59.86
Round  96, Train loss: 0.196, Test loss: 0.563, Test accuracy: 82.51
Round  96, Global train loss: 0.196, Global test loss: 1.324, Global test accuracy: 60.24
Round  97, Train loss: 0.203, Test loss: 0.548, Test accuracy: 82.83
Round  97, Global train loss: 0.203, Global test loss: 1.481, Global test accuracy: 59.38
Round  98, Train loss: 0.262, Test loss: 0.533, Test accuracy: 83.24
Round  98, Global train loss: 0.262, Global test loss: 1.179, Global test accuracy: 62.92
Round  99, Train loss: 0.239, Test loss: 0.530, Test accuracy: 83.34
Round  99, Global train loss: 0.239, Global test loss: 1.317, Global test accuracy: 60.65
Final Round, Train loss: 0.174, Test loss: 0.604, Test accuracy: 82.97
Final Round, Global train loss: 0.174, Global test loss: 1.317, Global test accuracy: 60.65
Average accuracy final 10 rounds: 82.74583333333332 

Average global accuracy final 10 rounds: 60.05499999999999 

1845.5120737552643
[1.6806519031524658, 3.3613038063049316, 4.759857892990112, 6.158411979675293, 7.512953758239746, 8.8674955368042, 10.25396990776062, 11.640444278717041, 13.023863554000854, 14.407282829284668, 15.79460620880127, 17.18192958831787, 18.587669610977173, 19.993409633636475, 21.414446592330933, 22.83548355102539, 24.22668766975403, 25.617891788482666, 26.993892431259155, 28.369893074035645, 29.76168966293335, 31.153486251831055, 32.5647349357605, 33.97598361968994, 35.38153672218323, 36.787089824676514, 38.19087243080139, 39.59465503692627, 40.99231433868408, 42.389973640441895, 43.77922821044922, 45.16848278045654, 46.60915994644165, 48.04983711242676, 49.43724775314331, 50.82465839385986, 52.23715257644653, 53.6496467590332, 55.06666326522827, 56.48367977142334, 57.87275457382202, 59.2618293762207, 60.640188694000244, 62.018548011779785, 63.3976833820343, 64.77681875228882, 66.14925527572632, 67.52169179916382, 68.90089964866638, 70.28010749816895, 71.6972279548645, 73.11434841156006, 74.50534152984619, 75.89633464813232, 77.3068597316742, 78.71738481521606, 80.08498048782349, 81.45257616043091, 82.85083603858948, 84.24909591674805, 85.6670253276825, 87.08495473861694, 88.48061060905457, 89.87626647949219, 91.27948045730591, 92.68269443511963, 94.09793829917908, 95.51318216323853, 96.89979648590088, 98.28641080856323, 99.68048238754272, 101.07455396652222, 102.46541428565979, 103.85627460479736, 105.239905834198, 106.62353706359863, 108.0069146156311, 109.39029216766357, 110.75826573371887, 112.12623929977417, 113.50977778434753, 114.8933162689209, 116.3210723400116, 117.7488284111023, 119.14890813827515, 120.548987865448, 121.94927978515625, 123.3495717048645, 124.75138902664185, 126.15320634841919, 127.54201626777649, 128.9308261871338, 130.33026361465454, 131.7297010421753, 133.16472935676575, 134.5997576713562, 136.03107142448425, 137.4623851776123, 138.88309955596924, 140.30381393432617, 141.68395709991455, 143.06410026550293, 144.43946361541748, 145.81482696533203, 147.2256190776825, 148.63641119003296, 150.0104615688324, 151.38451194763184, 152.77710270881653, 154.16969347000122, 155.58302903175354, 156.99636459350586, 158.4878077507019, 159.97925090789795, 161.40909099578857, 162.8389310836792, 164.20394253730774, 165.56895399093628, 166.95108914375305, 168.33322429656982, 169.75009989738464, 171.16697549819946, 172.56878089904785, 173.97058629989624, 175.3591284751892, 176.74767065048218, 178.13677835464478, 179.52588605880737, 180.90838408470154, 182.2908821105957, 183.6599144935608, 185.02894687652588, 186.4149887561798, 187.80103063583374, 189.19108152389526, 190.5811324119568, 191.96918320655823, 193.35723400115967, 194.73994541168213, 196.1226568222046, 197.48149585723877, 198.84033489227295, 200.23354291915894, 201.62675094604492, 203.0424292087555, 204.45810747146606, 205.91082787513733, 207.3635482788086, 208.8106768131256, 210.25780534744263, 211.62523865699768, 212.99267196655273, 214.39048624038696, 215.7883005142212, 217.16846179962158, 218.54862308502197, 219.90021920204163, 221.25181531906128, 222.62051582336426, 223.98921632766724, 225.3544898033142, 226.71976327896118, 228.08795404434204, 229.4561448097229, 230.82112526893616, 232.1861057281494, 233.54987955093384, 234.91365337371826, 236.31978511810303, 237.7259168624878, 239.1460349559784, 240.566153049469, 241.9776647090912, 243.38917636871338, 244.812180519104, 246.23518466949463, 247.59349846839905, 248.95181226730347, 250.30926156044006, 251.66671085357666, 253.0702772140503, 254.47384357452393, 255.8801610469818, 257.2864785194397, 258.75383734703064, 260.2211961746216, 261.59478068351746, 262.96836519241333, 264.33808422088623, 265.70780324935913, 267.0911293029785, 268.4744553565979, 269.83652782440186, 271.1986002922058, 272.57405734062195, 273.9495143890381, 275.36373567581177, 276.77795696258545, 278.18407917022705, 279.59020137786865, 281.88195967674255, 284.17371797561646]
[28.316666666666666, 28.316666666666666, 37.708333333333336, 37.708333333333336, 47.375, 47.375, 45.65833333333333, 45.65833333333333, 62.40833333333333, 62.40833333333333, 65.61666666666666, 65.61666666666666, 64.63333333333334, 64.63333333333334, 66.425, 66.425, 69.46666666666667, 69.46666666666667, 71.54166666666667, 71.54166666666667, 72.65833333333333, 72.65833333333333, 73.84166666666667, 73.84166666666667, 73.03333333333333, 73.03333333333333, 73.96666666666667, 73.96666666666667, 73.20833333333333, 73.20833333333333, 75.23333333333333, 75.23333333333333, 75.20833333333333, 75.20833333333333, 75.54166666666667, 75.54166666666667, 76.25833333333334, 76.25833333333334, 76.76666666666667, 76.76666666666667, 77.20833333333333, 77.20833333333333, 77.70833333333333, 77.70833333333333, 77.71666666666667, 77.71666666666667, 77.825, 77.825, 77.36666666666666, 77.36666666666666, 78.225, 78.225, 78.175, 78.175, 79.04166666666667, 79.04166666666667, 79.1, 79.1, 79.89166666666667, 79.89166666666667, 79.45833333333333, 79.45833333333333, 79.74166666666666, 79.74166666666666, 79.76666666666667, 79.76666666666667, 79.7, 79.7, 80.53333333333333, 80.53333333333333, 80.95833333333333, 80.95833333333333, 81.04166666666667, 81.04166666666667, 80.61666666666666, 80.61666666666666, 80.525, 80.525, 80.40833333333333, 80.40833333333333, 80.68333333333334, 80.68333333333334, 80.55, 80.55, 79.70833333333333, 79.70833333333333, 80.15833333333333, 80.15833333333333, 80.31666666666666, 80.31666666666666, 80.05, 80.05, 80.625, 80.625, 81.09166666666667, 81.09166666666667, 80.90833333333333, 80.90833333333333, 80.98333333333333, 80.98333333333333, 81.69166666666666, 81.69166666666666, 81.575, 81.575, 81.41666666666667, 81.41666666666667, 81.80833333333334, 81.80833333333334, 81.75, 81.75, 81.56666666666666, 81.56666666666666, 81.78333333333333, 81.78333333333333, 81.975, 81.975, 81.74166666666666, 81.74166666666666, 81.51666666666667, 81.51666666666667, 82.11666666666666, 82.11666666666666, 82.325, 82.325, 82.60833333333333, 82.60833333333333, 82.55, 82.55, 82.39166666666667, 82.39166666666667, 82.5, 82.5, 82.54166666666667, 82.54166666666667, 82.64166666666667, 82.64166666666667, 82.76666666666667, 82.76666666666667, 82.58333333333333, 82.58333333333333, 82.80833333333334, 82.80833333333334, 82.4, 82.4, 82.54166666666667, 82.54166666666667, 82.775, 82.775, 82.45833333333333, 82.45833333333333, 82.34166666666667, 82.34166666666667, 82.5, 82.5, 82.65, 82.65, 82.375, 82.375, 82.375, 82.375, 82.84166666666667, 82.84166666666667, 82.75833333333334, 82.75833333333334, 82.575, 82.575, 82.41666666666667, 82.41666666666667, 82.50833333333334, 82.50833333333334, 82.775, 82.775, 82.65, 82.65, 82.68333333333334, 82.68333333333334, 82.88333333333334, 82.88333333333334, 82.86666666666666, 82.86666666666666, 82.43333333333334, 82.43333333333334, 82.45833333333333, 82.45833333333333, 82.575, 82.575, 82.24166666666666, 82.24166666666666, 82.825, 82.825, 83.0, 83.0, 82.50833333333334, 82.50833333333334, 82.83333333333333, 82.83333333333333, 83.24166666666666, 83.24166666666666, 83.34166666666667, 83.34166666666667, 82.975, 82.975]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.136, Test loss: 2.170, Test accuracy: 19.58
Round   0, Global train loss: 1.136, Global test loss: 2.514, Global test accuracy: 10.00
Round   1, Train loss: 0.982, Test loss: 1.768, Test accuracy: 27.66
Round   1, Global train loss: 0.982, Global test loss: 2.341, Global test accuracy: 11.11
Round   2, Train loss: 0.895, Test loss: 1.514, Test accuracy: 40.87
Round   2, Global train loss: 0.895, Global test loss: 2.155, Global test accuracy: 25.05
Round   3, Train loss: 0.813, Test loss: 1.353, Test accuracy: 51.92
Round   3, Global train loss: 0.813, Global test loss: 2.275, Global test accuracy: 31.04
Round   4, Train loss: 0.777, Test loss: 1.093, Test accuracy: 57.77
Round   4, Global train loss: 0.777, Global test loss: 1.928, Global test accuracy: 35.73
Round   5, Train loss: 0.693, Test loss: 1.002, Test accuracy: 58.96
Round   5, Global train loss: 0.693, Global test loss: 1.986, Global test accuracy: 29.83
Round   6, Train loss: 0.827, Test loss: 0.970, Test accuracy: 60.52
Round   6, Global train loss: 0.827, Global test loss: 2.212, Global test accuracy: 18.65
Round   7, Train loss: 0.689, Test loss: 0.942, Test accuracy: 63.69
Round   7, Global train loss: 0.689, Global test loss: 1.952, Global test accuracy: 38.19
Round   8, Train loss: 0.791, Test loss: 0.878, Test accuracy: 63.64
Round   8, Global train loss: 0.791, Global test loss: 1.731, Global test accuracy: 34.73
Round   9, Train loss: 0.704, Test loss: 0.815, Test accuracy: 65.58
Round   9, Global train loss: 0.704, Global test loss: 1.700, Global test accuracy: 35.92
Round  10, Train loss: 0.827, Test loss: 0.799, Test accuracy: 68.58
Round  10, Global train loss: 0.827, Global test loss: 1.812, Global test accuracy: 36.22
Round  11, Train loss: 0.654, Test loss: 0.727, Test accuracy: 69.99
Round  11, Global train loss: 0.654, Global test loss: 1.832, Global test accuracy: 33.73
Round  12, Train loss: 0.588, Test loss: 0.694, Test accuracy: 71.09
Round  12, Global train loss: 0.588, Global test loss: 1.737, Global test accuracy: 39.32
Round  13, Train loss: 0.706, Test loss: 0.707, Test accuracy: 71.20
Round  13, Global train loss: 0.706, Global test loss: 1.536, Global test accuracy: 45.68
Round  14, Train loss: 0.594, Test loss: 0.627, Test accuracy: 73.53
Round  14, Global train loss: 0.594, Global test loss: 1.593, Global test accuracy: 44.27
Round  15, Train loss: 0.648, Test loss: 0.609, Test accuracy: 74.01
Round  15, Global train loss: 0.648, Global test loss: 1.629, Global test accuracy: 42.82
Round  16, Train loss: 0.637, Test loss: 0.588, Test accuracy: 74.78
Round  16, Global train loss: 0.637, Global test loss: 1.432, Global test accuracy: 48.12
Round  17, Train loss: 0.711, Test loss: 0.597, Test accuracy: 74.52
Round  17, Global train loss: 0.711, Global test loss: 1.672, Global test accuracy: 42.67
Round  18, Train loss: 0.649, Test loss: 0.594, Test accuracy: 74.83
Round  18, Global train loss: 0.649, Global test loss: 1.469, Global test accuracy: 47.98
Round  19, Train loss: 0.696, Test loss: 0.590, Test accuracy: 75.29
Round  19, Global train loss: 0.696, Global test loss: 1.525, Global test accuracy: 47.07
Round  20, Train loss: 0.532, Test loss: 0.594, Test accuracy: 75.49
Round  20, Global train loss: 0.532, Global test loss: 1.500, Global test accuracy: 49.27
Round  21, Train loss: 0.557, Test loss: 0.596, Test accuracy: 75.09
Round  21, Global train loss: 0.557, Global test loss: 1.636, Global test accuracy: 43.32
Round  22, Train loss: 0.570, Test loss: 0.584, Test accuracy: 75.53
Round  22, Global train loss: 0.570, Global test loss: 1.347, Global test accuracy: 52.33
Round  23, Train loss: 0.599, Test loss: 0.559, Test accuracy: 76.98
Round  23, Global train loss: 0.599, Global test loss: 1.514, Global test accuracy: 47.03
Round  24, Train loss: 0.523, Test loss: 0.565, Test accuracy: 76.96
Round  24, Global train loss: 0.523, Global test loss: 1.614, Global test accuracy: 46.77
Round  25, Train loss: 0.519, Test loss: 0.568, Test accuracy: 76.86
Round  25, Global train loss: 0.519, Global test loss: 1.469, Global test accuracy: 49.33
Round  26, Train loss: 0.514, Test loss: 0.546, Test accuracy: 77.93
Round  26, Global train loss: 0.514, Global test loss: 1.473, Global test accuracy: 49.42
Round  27, Train loss: 0.609, Test loss: 0.547, Test accuracy: 78.05
Round  27, Global train loss: 0.609, Global test loss: 1.680, Global test accuracy: 40.48
Round  28, Train loss: 0.439, Test loss: 0.547, Test accuracy: 78.14
Round  28, Global train loss: 0.439, Global test loss: 1.662, Global test accuracy: 43.37
Round  29, Train loss: 0.487, Test loss: 0.552, Test accuracy: 77.90
Round  29, Global train loss: 0.487, Global test loss: 1.627, Global test accuracy: 42.35
Round  30, Train loss: 0.395, Test loss: 0.550, Test accuracy: 77.56
Round  30, Global train loss: 0.395, Global test loss: 1.471, Global test accuracy: 50.58
Round  31, Train loss: 0.533, Test loss: 0.530, Test accuracy: 78.22
Round  31, Global train loss: 0.533, Global test loss: 1.431, Global test accuracy: 52.83
Round  32, Train loss: 0.476, Test loss: 0.527, Test accuracy: 78.15
Round  32, Global train loss: 0.476, Global test loss: 1.382, Global test accuracy: 52.71
Round  33, Train loss: 0.518, Test loss: 0.525, Test accuracy: 78.56
Round  33, Global train loss: 0.518, Global test loss: 1.492, Global test accuracy: 48.07
Round  34, Train loss: 0.415, Test loss: 0.516, Test accuracy: 79.04
Round  34, Global train loss: 0.415, Global test loss: 1.392, Global test accuracy: 50.37
Round  35, Train loss: 0.479, Test loss: 0.508, Test accuracy: 79.17
Round  35, Global train loss: 0.479, Global test loss: 1.479, Global test accuracy: 50.31
Round  36, Train loss: 0.531, Test loss: 0.513, Test accuracy: 79.31
Round  36, Global train loss: 0.531, Global test loss: 1.670, Global test accuracy: 47.92
Round  37, Train loss: 0.481, Test loss: 0.507, Test accuracy: 79.60
Round  37, Global train loss: 0.481, Global test loss: 1.217, Global test accuracy: 57.39
Round  38, Train loss: 0.456, Test loss: 0.501, Test accuracy: 79.93
Round  38, Global train loss: 0.456, Global test loss: 1.405, Global test accuracy: 53.93
Round  39, Train loss: 0.483, Test loss: 0.511, Test accuracy: 79.58
Round  39, Global train loss: 0.483, Global test loss: 1.634, Global test accuracy: 48.01
Round  40, Train loss: 0.409, Test loss: 0.498, Test accuracy: 79.92
Round  40, Global train loss: 0.409, Global test loss: 1.280, Global test accuracy: 56.56
Round  41, Train loss: 0.474, Test loss: 0.489, Test accuracy: 80.32
Round  41, Global train loss: 0.474, Global test loss: 1.343, Global test accuracy: 53.49
Round  42, Train loss: 0.488, Test loss: 0.509, Test accuracy: 79.81
Round  42, Global train loss: 0.488, Global test loss: 1.353, Global test accuracy: 54.14
Round  43, Train loss: 0.404, Test loss: 0.527, Test accuracy: 79.39
Round  43, Global train loss: 0.404, Global test loss: 1.363, Global test accuracy: 52.24
Round  44, Train loss: 0.476, Test loss: 0.538, Test accuracy: 79.15
Round  44, Global train loss: 0.476, Global test loss: 1.404, Global test accuracy: 51.73
Round  45, Train loss: 0.421, Test loss: 0.531, Test accuracy: 79.36
Round  45, Global train loss: 0.421, Global test loss: 1.190, Global test accuracy: 58.23
Round  46, Train loss: 0.371, Test loss: 0.538, Test accuracy: 79.19
Round  46, Global train loss: 0.371, Global test loss: 1.335, Global test accuracy: 56.40
Round  47, Train loss: 0.387, Test loss: 0.524, Test accuracy: 79.58
Round  47, Global train loss: 0.387, Global test loss: 1.381, Global test accuracy: 52.59
Round  48, Train loss: 0.460, Test loss: 0.521, Test accuracy: 79.58
Round  48, Global train loss: 0.460, Global test loss: 1.187, Global test accuracy: 58.64
Round  49, Train loss: 0.334, Test loss: 0.507, Test accuracy: 79.93
Round  49, Global train loss: 0.334, Global test loss: 1.248, Global test accuracy: 58.01
Round  50, Train loss: 0.332, Test loss: 0.499, Test accuracy: 80.21
Round  50, Global train loss: 0.332, Global test loss: 1.563, Global test accuracy: 48.53
Round  51, Train loss: 0.416, Test loss: 0.506, Test accuracy: 80.27
Round  51, Global train loss: 0.416, Global test loss: 1.397, Global test accuracy: 53.67
Round  52, Train loss: 0.532, Test loss: 0.504, Test accuracy: 80.47
Round  52, Global train loss: 0.532, Global test loss: 1.347, Global test accuracy: 53.40
Round  53, Train loss: 0.408, Test loss: 0.501, Test accuracy: 80.61
Round  53, Global train loss: 0.408, Global test loss: 1.240, Global test accuracy: 56.54
Round  54, Train loss: 0.374, Test loss: 0.503, Test accuracy: 80.41
Round  54, Global train loss: 0.374, Global test loss: 1.178, Global test accuracy: 59.94
Round  55, Train loss: 0.388, Test loss: 0.500, Test accuracy: 80.52
Round  55, Global train loss: 0.388, Global test loss: 1.286, Global test accuracy: 55.33
Round  56, Train loss: 0.474, Test loss: 0.506, Test accuracy: 80.09
Round  56, Global train loss: 0.474, Global test loss: 1.475, Global test accuracy: 51.54
Round  57, Train loss: 0.422, Test loss: 0.499, Test accuracy: 80.47
Round  57, Global train loss: 0.422, Global test loss: 1.223, Global test accuracy: 56.99
Round  58, Train loss: 0.375, Test loss: 0.512, Test accuracy: 80.30
Round  58, Global train loss: 0.375, Global test loss: 1.182, Global test accuracy: 60.33
Round  59, Train loss: 0.351, Test loss: 0.516, Test accuracy: 80.20
Round  59, Global train loss: 0.351, Global test loss: 1.272, Global test accuracy: 57.02
Round  60, Train loss: 0.293, Test loss: 0.508, Test accuracy: 80.61
Round  60, Global train loss: 0.293, Global test loss: 1.471, Global test accuracy: 57.12
Round  61, Train loss: 0.341, Test loss: 0.514, Test accuracy: 80.46
Round  61, Global train loss: 0.341, Global test loss: 1.330, Global test accuracy: 58.33
Round  62, Train loss: 0.352, Test loss: 0.501, Test accuracy: 80.94
Round  62, Global train loss: 0.352, Global test loss: 1.228, Global test accuracy: 59.35
Round  63, Train loss: 0.402, Test loss: 0.514, Test accuracy: 80.69
Round  63, Global train loss: 0.402, Global test loss: 1.238, Global test accuracy: 58.54
Round  64, Train loss: 0.345, Test loss: 0.522, Test accuracy: 80.58
Round  64, Global train loss: 0.345, Global test loss: 1.173, Global test accuracy: 60.48
Round  65, Train loss: 0.246, Test loss: 0.531, Test accuracy: 80.15
Round  65, Global train loss: 0.246, Global test loss: 1.404, Global test accuracy: 56.67
Round  66, Train loss: 0.434, Test loss: 0.519, Test accuracy: 80.55
Round  66, Global train loss: 0.434, Global test loss: 1.266, Global test accuracy: 58.42
Round  67, Train loss: 0.403, Test loss: 0.515, Test accuracy: 80.86
Round  67, Global train loss: 0.403, Global test loss: 1.339, Global test accuracy: 55.67
Round  68, Train loss: 0.349, Test loss: 0.526, Test accuracy: 80.84
Round  68, Global train loss: 0.349, Global test loss: 1.366, Global test accuracy: 55.66
Round  69, Train loss: 0.343, Test loss: 0.527, Test accuracy: 80.61
Round  69, Global train loss: 0.343, Global test loss: 1.324, Global test accuracy: 57.03
Round  70, Train loss: 0.311, Test loss: 0.518, Test accuracy: 81.24
Round  70, Global train loss: 0.311, Global test loss: 1.214, Global test accuracy: 59.72
Round  71, Train loss: 0.256, Test loss: 0.507, Test accuracy: 81.46
Round  71, Global train loss: 0.256, Global test loss: 1.270, Global test accuracy: 59.42
Round  72, Train loss: 0.387, Test loss: 0.500, Test accuracy: 81.49
Round  72, Global train loss: 0.387, Global test loss: 1.558, Global test accuracy: 53.93
Round  73, Train loss: 0.349, Test loss: 0.481, Test accuracy: 82.10
Round  73, Global train loss: 0.349, Global test loss: 1.257, Global test accuracy: 58.86
Round  74, Train loss: 0.297, Test loss: 0.474, Test accuracy: 82.22
Round  74, Global train loss: 0.297, Global test loss: 1.235, Global test accuracy: 59.67
Round  75, Train loss: 0.240, Test loss: 0.478, Test accuracy: 82.21
Round  75, Global train loss: 0.240, Global test loss: 1.427, Global test accuracy: 55.83
Round  76, Train loss: 0.288, Test loss: 0.488, Test accuracy: 82.07
Round  76, Global train loss: 0.288, Global test loss: 1.233, Global test accuracy: 60.20
Round  77, Train loss: 0.320, Test loss: 0.486, Test accuracy: 82.18
Round  77, Global train loss: 0.320, Global test loss: 1.365, Global test accuracy: 56.61
Round  78, Train loss: 0.324, Test loss: 0.471, Test accuracy: 82.57
Round  78, Global train loss: 0.324, Global test loss: 1.393, Global test accuracy: 55.77
Round  79, Train loss: 0.274, Test loss: 0.472, Test accuracy: 82.71
Round  79, Global train loss: 0.274, Global test loss: 1.535, Global test accuracy: 52.85
Round  80, Train loss: 0.264, Test loss: 0.488, Test accuracy: 82.53
Round  80, Global train loss: 0.264, Global test loss: 1.416, Global test accuracy: 57.26
Round  81, Train loss: 0.336, Test loss: 0.488, Test accuracy: 82.38
Round  81, Global train loss: 0.336, Global test loss: 1.320, Global test accuracy: 56.70
Round  82, Train loss: 0.275, Test loss: 0.490, Test accuracy: 82.33
Round  82, Global train loss: 0.275, Global test loss: 1.344, Global test accuracy: 57.62
Round  83, Train loss: 0.356, Test loss: 0.496, Test accuracy: 82.06
Round  83, Global train loss: 0.356, Global test loss: 1.210, Global test accuracy: 60.83
Round  84, Train loss: 0.265, Test loss: 0.500, Test accuracy: 82.35
Round  84, Global train loss: 0.265, Global test loss: 1.162, Global test accuracy: 62.77
Round  85, Train loss: 0.314, Test loss: 0.502, Test accuracy: 82.17
Round  85, Global train loss: 0.314, Global test loss: 1.562, Global test accuracy: 55.88
Round  86, Train loss: 0.296, Test loss: 0.511, Test accuracy: 82.05
Round  86, Global train loss: 0.296, Global test loss: 1.525, Global test accuracy: 56.18
Round  87, Train loss: 0.299, Test loss: 0.496, Test accuracy: 82.22
Round  87, Global train loss: 0.299, Global test loss: 1.221, Global test accuracy: 60.76
Round  88, Train loss: 0.330, Test loss: 0.506, Test accuracy: 82.03
Round  88, Global train loss: 0.330, Global test loss: 1.157, Global test accuracy: 61.70
Round  89, Train loss: 0.324, Test loss: 0.500, Test accuracy: 82.21
Round  89, Global train loss: 0.324, Global test loss: 1.119, Global test accuracy: 63.09
Round  90, Train loss: 0.317, Test loss: 0.505, Test accuracy: 81.97
Round  90, Global train loss: 0.317, Global test loss: 1.153, Global test accuracy: 62.53
Round  91, Train loss: 0.302, Test loss: 0.494, Test accuracy: 82.17
Round  91, Global train loss: 0.302, Global test loss: 1.144, Global test accuracy: 62.44
Round  92, Train loss: 0.266, Test loss: 0.491, Test accuracy: 82.33
Round  92, Global train loss: 0.266, Global test loss: 1.121, Global test accuracy: 63.69
Round  93, Train loss: 0.238, Test loss: 0.499, Test accuracy: 82.51
Round  93, Global train loss: 0.238, Global test loss: 1.284, Global test accuracy: 61.33
Round  94, Train loss: 0.291, Test loss: 0.519, Test accuracy: 82.22
Round  94, Global train loss: 0.291, Global test loss: 1.261, Global test accuracy: 60.15
Round  95, Train loss: 0.233, Test loss: 0.505, Test accuracy: 82.55
Round  95, Global train loss: 0.233, Global test loss: 1.344, Global test accuracy: 60.47/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  96, Train loss: 0.302, Test loss: 0.506, Test accuracy: 82.68
Round  96, Global train loss: 0.302, Global test loss: 1.161, Global test accuracy: 62.06
Round  97, Train loss: 0.346, Test loss: 0.498, Test accuracy: 82.78
Round  97, Global train loss: 0.346, Global test loss: 1.281, Global test accuracy: 59.98
Round  98, Train loss: 0.310, Test loss: 0.499, Test accuracy: 82.89
Round  98, Global train loss: 0.310, Global test loss: 1.348, Global test accuracy: 58.46
Round  99, Train loss: 0.303, Test loss: 0.511, Test accuracy: 82.77
Round  99, Global train loss: 0.303, Global test loss: 1.551, Global test accuracy: 55.19
Final Round, Train loss: 0.225, Test loss: 0.555, Test accuracy: 82.58
Final Round, Global train loss: 0.225, Global test loss: 1.551, Global test accuracy: 55.19
Average accuracy final 10 rounds: 82.48750000000001 

Average global accuracy final 10 rounds: 60.62916666666666 

1889.428349494934
[1.8059608936309814, 3.611921787261963, 5.1009745597839355, 6.590027332305908, 8.105516910552979, 9.621006488800049, 11.147824764251709, 12.67464303970337, 14.21180009841919, 15.74895715713501, 17.204381704330444, 18.65980625152588, 20.117960214614868, 21.576114177703857, 23.04478669166565, 24.51345920562744, 25.982002019882202, 27.450544834136963, 28.8973970413208, 30.34424924850464, 31.800316333770752, 33.256383419036865, 34.74098992347717, 36.22559642791748, 37.76805138587952, 39.31050634384155, 40.79210543632507, 42.273704528808594, 43.829917430877686, 45.38613033294678, 46.84816789627075, 48.31020545959473, 49.77674841880798, 51.24329137802124, 52.70280981063843, 54.162328243255615, 55.626704454422, 57.09108066558838, 58.55491328239441, 60.01874589920044, 61.46952748298645, 62.92030906677246, 64.39987635612488, 65.8794436454773, 67.33832359313965, 68.797203540802, 70.27136254310608, 71.74552154541016, 73.25018215179443, 74.75484275817871, 76.25730776786804, 77.75977277755737, 79.24101877212524, 80.72226476669312, 82.1846866607666, 83.64710855484009, 85.10697078704834, 86.56683301925659, 88.02021098136902, 89.47358894348145, 90.97942614555359, 92.48526334762573, 93.94638156890869, 95.40749979019165, 96.8495261669159, 98.29155254364014, 99.7444269657135, 101.19730138778687, 102.65646481513977, 104.11562824249268, 105.58685326576233, 107.05807828903198, 108.66298007965088, 110.26788187026978, 111.7407808303833, 113.21367979049683, 114.71961522102356, 116.2255506515503, 117.72827672958374, 119.23100280761719, 120.68689203262329, 122.1427812576294, 123.59368777275085, 125.04459428787231, 126.51802849769592, 127.99146270751953, 129.46760988235474, 130.94375705718994, 132.40196704864502, 133.8601770401001, 135.32466912269592, 136.78916120529175, 138.25957226753235, 139.72998332977295, 141.19589066505432, 142.6617980003357, 144.13447046279907, 145.60714292526245, 147.11095666885376, 148.61477041244507, 150.0838747024536, 151.55297899246216, 153.03336572647095, 154.51375246047974, 155.97987914085388, 157.44600582122803, 158.92415571212769, 160.40230560302734, 161.8792827129364, 163.35625982284546, 164.84899592399597, 166.34173202514648, 167.82695531845093, 169.31217861175537, 170.80918145179749, 172.3061842918396, 173.8140480518341, 175.3219118118286, 176.81649136543274, 178.31107091903687, 179.787992477417, 181.26491403579712, 182.7409529685974, 184.2169919013977, 185.72424602508545, 187.2315001487732, 188.71133542060852, 190.19117069244385, 191.65519547462463, 193.11922025680542, 194.59056782722473, 196.06191539764404, 197.55642199516296, 199.05092859268188, 200.5116937160492, 201.9724588394165, 203.45549988746643, 204.93854093551636, 206.41472458839417, 207.89090824127197, 209.38235712051392, 210.87380599975586, 212.36290097236633, 213.8519959449768, 215.35578274726868, 216.85956954956055, 218.31625604629517, 219.77294254302979, 221.22334361076355, 222.67374467849731, 224.13998866081238, 225.60623264312744, 227.06428909301758, 228.52234554290771, 230.01010751724243, 231.49786949157715, 233.0778408050537, 234.65781211853027, 236.16112613677979, 237.6644401550293, 239.19779872894287, 240.73115730285645, 242.20979261398315, 243.68842792510986, 245.14800691604614, 246.60758590698242, 248.06003308296204, 249.51248025894165, 250.96568989753723, 252.4188995361328, 253.88187384605408, 255.34484815597534, 256.8047239780426, 258.26459980010986, 259.7317764759064, 261.1989531517029, 262.6745636463165, 264.1501741409302, 265.5959084033966, 267.04164266586304, 268.50733065605164, 269.97301864624023, 271.45208168029785, 272.93114471435547, 274.42286372184753, 275.9145827293396, 277.3972809314728, 278.87997913360596, 280.3506746292114, 281.8213701248169, 283.2862033843994, 284.75103664398193, 286.27652406692505, 287.80201148986816, 289.2546625137329, 290.70731353759766, 292.2449588775635, 293.7826042175293, 295.2776143550873, 296.77262449264526, 299.22425150871277, 301.6758785247803]
[19.583333333333332, 19.583333333333332, 27.658333333333335, 27.658333333333335, 40.86666666666667, 40.86666666666667, 51.916666666666664, 51.916666666666664, 57.775, 57.775, 58.958333333333336, 58.958333333333336, 60.516666666666666, 60.516666666666666, 63.69166666666667, 63.69166666666667, 63.641666666666666, 63.641666666666666, 65.575, 65.575, 68.58333333333333, 68.58333333333333, 69.99166666666666, 69.99166666666666, 71.09166666666667, 71.09166666666667, 71.2, 71.2, 73.525, 73.525, 74.00833333333334, 74.00833333333334, 74.775, 74.775, 74.51666666666667, 74.51666666666667, 74.83333333333333, 74.83333333333333, 75.29166666666667, 75.29166666666667, 75.49166666666666, 75.49166666666666, 75.09166666666667, 75.09166666666667, 75.525, 75.525, 76.98333333333333, 76.98333333333333, 76.95833333333333, 76.95833333333333, 76.85833333333333, 76.85833333333333, 77.93333333333334, 77.93333333333334, 78.05, 78.05, 78.14166666666667, 78.14166666666667, 77.9, 77.9, 77.55833333333334, 77.55833333333334, 78.225, 78.225, 78.15, 78.15, 78.55833333333334, 78.55833333333334, 79.04166666666667, 79.04166666666667, 79.16666666666667, 79.16666666666667, 79.30833333333334, 79.30833333333334, 79.6, 79.6, 79.93333333333334, 79.93333333333334, 79.58333333333333, 79.58333333333333, 79.91666666666667, 79.91666666666667, 80.31666666666666, 80.31666666666666, 79.80833333333334, 79.80833333333334, 79.39166666666667, 79.39166666666667, 79.15, 79.15, 79.35833333333333, 79.35833333333333, 79.19166666666666, 79.19166666666666, 79.575, 79.575, 79.58333333333333, 79.58333333333333, 79.93333333333334, 79.93333333333334, 80.20833333333333, 80.20833333333333, 80.26666666666667, 80.26666666666667, 80.475, 80.475, 80.60833333333333, 80.60833333333333, 80.40833333333333, 80.40833333333333, 80.51666666666667, 80.51666666666667, 80.09166666666667, 80.09166666666667, 80.475, 80.475, 80.3, 80.3, 80.2, 80.2, 80.60833333333333, 80.60833333333333, 80.45833333333333, 80.45833333333333, 80.94166666666666, 80.94166666666666, 80.69166666666666, 80.69166666666666, 80.58333333333333, 80.58333333333333, 80.15, 80.15, 80.55, 80.55, 80.85833333333333, 80.85833333333333, 80.84166666666667, 80.84166666666667, 80.60833333333333, 80.60833333333333, 81.24166666666666, 81.24166666666666, 81.45833333333333, 81.45833333333333, 81.49166666666666, 81.49166666666666, 82.1, 82.1, 82.225, 82.225, 82.20833333333333, 82.20833333333333, 82.06666666666666, 82.06666666666666, 82.18333333333334, 82.18333333333334, 82.56666666666666, 82.56666666666666, 82.70833333333333, 82.70833333333333, 82.525, 82.525, 82.38333333333334, 82.38333333333334, 82.325, 82.325, 82.05833333333334, 82.05833333333334, 82.35, 82.35, 82.175, 82.175, 82.05, 82.05, 82.225, 82.225, 82.025, 82.025, 82.20833333333333, 82.20833333333333, 81.975, 81.975, 82.175, 82.175, 82.33333333333333, 82.33333333333333, 82.50833333333334, 82.50833333333334, 82.21666666666667, 82.21666666666667, 82.55, 82.55, 82.68333333333334, 82.68333333333334, 82.775, 82.775, 82.89166666666667, 82.89166666666667, 82.76666666666667, 82.76666666666667, 82.58333333333333, 82.58333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.515, Test loss: 2.106, Test accuracy: 17.06
Round   1, Train loss: 1.051, Test loss: 1.832, Test accuracy: 28.38
Round   2, Train loss: 0.933, Test loss: 1.541, Test accuracy: 44.55
Round   3, Train loss: 0.770, Test loss: 1.214, Test accuracy: 54.58
Round   4, Train loss: 0.719, Test loss: 1.263, Test accuracy: 56.63
Round   5, Train loss: 0.763, Test loss: 1.064, Test accuracy: 57.17
Round   6, Train loss: 0.809, Test loss: 0.951, Test accuracy: 62.24
Round   7, Train loss: 0.763, Test loss: 0.837, Test accuracy: 65.72
Round   8, Train loss: 0.812, Test loss: 0.939, Test accuracy: 65.05
Round   9, Train loss: 0.707, Test loss: 0.800, Test accuracy: 68.60
Round  10, Train loss: 0.661, Test loss: 0.848, Test accuracy: 67.53
Round  11, Train loss: 0.604, Test loss: 0.685, Test accuracy: 72.03
Round  12, Train loss: 0.636, Test loss: 0.683, Test accuracy: 72.48
Round  13, Train loss: 0.731, Test loss: 0.602, Test accuracy: 74.86
Round  14, Train loss: 0.756, Test loss: 0.601, Test accuracy: 75.06
Round  15, Train loss: 0.670, Test loss: 0.596, Test accuracy: 75.52
Round  16, Train loss: 0.730, Test loss: 0.591, Test accuracy: 75.92
Round  17, Train loss: 0.599, Test loss: 0.589, Test accuracy: 75.81
Round  18, Train loss: 0.660, Test loss: 0.568, Test accuracy: 77.29
Round  19, Train loss: 0.599, Test loss: 0.562, Test accuracy: 77.05
Round  20, Train loss: 0.699, Test loss: 0.567, Test accuracy: 77.40
Round  21, Train loss: 0.596, Test loss: 0.548, Test accuracy: 78.05
Round  22, Train loss: 0.544, Test loss: 0.547, Test accuracy: 77.67
Round  23, Train loss: 0.565, Test loss: 0.525, Test accuracy: 78.18
Round  24, Train loss: 0.524, Test loss: 0.526, Test accuracy: 78.06
Round  25, Train loss: 0.502, Test loss: 0.504, Test accuracy: 79.15
Round  26, Train loss: 0.566, Test loss: 0.508, Test accuracy: 79.26
Round  27, Train loss: 0.586, Test loss: 0.536, Test accuracy: 78.14
Round  28, Train loss: 0.481, Test loss: 0.513, Test accuracy: 78.65
Round  29, Train loss: 0.557, Test loss: 0.506, Test accuracy: 78.97
Round  30, Train loss: 0.568, Test loss: 0.491, Test accuracy: 79.88
Round  31, Train loss: 0.513, Test loss: 0.485, Test accuracy: 79.67
Round  32, Train loss: 0.550, Test loss: 0.490, Test accuracy: 80.06
Round  33, Train loss: 0.443, Test loss: 0.487, Test accuracy: 80.29
Round  34, Train loss: 0.465, Test loss: 0.483, Test accuracy: 80.42
Round  35, Train loss: 0.452, Test loss: 0.483, Test accuracy: 80.35
Round  36, Train loss: 0.453, Test loss: 0.462, Test accuracy: 80.84
Round  37, Train loss: 0.391, Test loss: 0.454, Test accuracy: 81.22
Round  38, Train loss: 0.421, Test loss: 0.458, Test accuracy: 81.13
Round  39, Train loss: 0.478, Test loss: 0.465, Test accuracy: 80.95
Round  40, Train loss: 0.348, Test loss: 0.454, Test accuracy: 81.39
Round  41, Train loss: 0.477, Test loss: 0.460, Test accuracy: 81.22
Round  42, Train loss: 0.433, Test loss: 0.445, Test accuracy: 81.63
Round  43, Train loss: 0.405, Test loss: 0.460, Test accuracy: 81.08
Round  44, Train loss: 0.363, Test loss: 0.448, Test accuracy: 81.44
Round  45, Train loss: 0.467, Test loss: 0.452, Test accuracy: 81.31
Round  46, Train loss: 0.374, Test loss: 0.443, Test accuracy: 81.58
Round  47, Train loss: 0.337, Test loss: 0.443, Test accuracy: 81.92
Round  48, Train loss: 0.500, Test loss: 0.439, Test accuracy: 82.02
Round  49, Train loss: 0.387, Test loss: 0.439, Test accuracy: 81.39
Round  50, Train loss: 0.377, Test loss: 0.429, Test accuracy: 81.97
Round  51, Train loss: 0.413, Test loss: 0.432, Test accuracy: 82.14
Round  52, Train loss: 0.424, Test loss: 0.428, Test accuracy: 82.44
Round  53, Train loss: 0.405, Test loss: 0.424, Test accuracy: 82.50
Round  54, Train loss: 0.342, Test loss: 0.419, Test accuracy: 82.60
Round  55, Train loss: 0.350, Test loss: 0.418, Test accuracy: 82.72
Round  56, Train loss: 0.455, Test loss: 0.420, Test accuracy: 82.72
Round  57, Train loss: 0.400, Test loss: 0.422, Test accuracy: 82.65
Round  58, Train loss: 0.322, Test loss: 0.416, Test accuracy: 82.61
Round  59, Train loss: 0.391, Test loss: 0.417, Test accuracy: 82.83
Round  60, Train loss: 0.264, Test loss: 0.417, Test accuracy: 82.58
Round  61, Train loss: 0.308, Test loss: 0.412, Test accuracy: 83.11
Round  62, Train loss: 0.467, Test loss: 0.408, Test accuracy: 83.38
Round  63, Train loss: 0.425, Test loss: 0.414, Test accuracy: 82.97
Round  64, Train loss: 0.395, Test loss: 0.413, Test accuracy: 82.86
Round  65, Train loss: 0.274, Test loss: 0.413, Test accuracy: 82.84
Round  66, Train loss: 0.324, Test loss: 0.406, Test accuracy: 83.58
Round  67, Train loss: 0.304, Test loss: 0.404, Test accuracy: 83.54
Round  68, Train loss: 0.252, Test loss: 0.406, Test accuracy: 83.46
Round  69, Train loss: 0.348, Test loss: 0.408, Test accuracy: 83.33
Round  70, Train loss: 0.248, Test loss: 0.413, Test accuracy: 83.20
Round  71, Train loss: 0.327, Test loss: 0.400, Test accuracy: 83.67
Round  72, Train loss: 0.254, Test loss: 0.406, Test accuracy: 83.62
Round  73, Train loss: 0.235, Test loss: 0.398, Test accuracy: 83.82
Round  74, Train loss: 0.386, Test loss: 0.399, Test accuracy: 83.73
Round  75, Train loss: 0.393, Test loss: 0.395, Test accuracy: 83.85
Round  76, Train loss: 0.288, Test loss: 0.395, Test accuracy: 83.67
Round  77, Train loss: 0.261, Test loss: 0.400, Test accuracy: 83.53
Round  78, Train loss: 0.386, Test loss: 0.395, Test accuracy: 83.63
Round  79, Train loss: 0.286, Test loss: 0.396, Test accuracy: 83.65
Round  80, Train loss: 0.246, Test loss: 0.395, Test accuracy: 83.72
Round  81, Train loss: 0.272, Test loss: 0.400, Test accuracy: 83.73
Round  82, Train loss: 0.256, Test loss: 0.401, Test accuracy: 83.82
Round  83, Train loss: 0.253, Test loss: 0.396, Test accuracy: 83.81
Round  84, Train loss: 0.360, Test loss: 0.397, Test accuracy: 84.06
Round  85, Train loss: 0.292, Test loss: 0.397, Test accuracy: 83.87
Round  86, Train loss: 0.348, Test loss: 0.389, Test accuracy: 84.17
Round  87, Train loss: 0.240, Test loss: 0.390, Test accuracy: 84.33
Round  88, Train loss: 0.193, Test loss: 0.387, Test accuracy: 84.37
Round  89, Train loss: 0.220, Test loss: 0.390, Test accuracy: 83.99
Round  90, Train loss: 0.328, Test loss: 0.387, Test accuracy: 84.29
Round  91, Train loss: 0.340, Test loss: 0.388, Test accuracy: 84.25
Round  92, Train loss: 0.320, Test loss: 0.387, Test accuracy: 84.33
Round  93, Train loss: 0.305, Test loss: 0.384, Test accuracy: 84.17
Round  94, Train loss: 0.271, Test loss: 0.395, Test accuracy: 84.20
Round  95, Train loss: 0.304, Test loss: 0.385, Test accuracy: 84.39
Round  96, Train loss: 0.276, Test loss: 0.386, Test accuracy: 84.47
Round  97, Train loss: 0.256, Test loss: 0.383, Test accuracy: 84.35
Round  98, Train loss: 0.176, Test loss: 0.390, Test accuracy: 84.29
Round  99, Train loss: 0.292, Test loss: 0.389, Test accuracy: 84.17
Final Round, Train loss: 0.242, Test loss: 0.384, Test accuracy: 84.65
Average accuracy final 10 rounds: 84.2925
1359.8465056419373
[1.9732024669647217, 3.6477737426757812, 5.329066753387451, 7.00944709777832, 8.752430438995361, 10.56031847000122, 12.322131633758545, 14.098281860351562, 15.852624416351318, 17.49621272087097, 19.17083477973938, 20.86876368522644, 22.526678800582886, 24.19996452331543, 25.870328903198242, 27.545669555664062, 29.257419109344482, 30.97991967201233, 32.685649394989014, 34.360522747039795, 36.01843547821045, 37.78252363204956, 39.55275225639343, 41.31342053413391, 43.014625787734985, 44.695292711257935, 46.37561631202698, 48.06537652015686, 49.75403904914856, 51.43058967590332, 53.10546612739563, 54.776283979415894, 56.50984787940979, 58.23885440826416, 59.91465663909912, 61.58153510093689, 63.26243329048157, 64.98390340805054, 66.69543504714966, 68.41955590248108, 70.11206269264221, 71.83917546272278, 73.58544111251831, 75.32058238983154, 77.0421736240387, 78.73636269569397, 80.43170857429504, 82.11818671226501, 83.83082842826843, 85.54895806312561, 87.30680990219116, 88.96814680099487, 90.67001605033875, 92.34280753135681, 94.03352665901184, 95.71575260162354, 97.40180993080139, 99.08396673202515, 100.807199716568, 102.5636043548584, 104.31946086883545, 106.07048273086548, 107.77689981460571, 109.52815556526184, 111.19377374649048, 112.87261939048767, 114.58117318153381, 116.26458024978638, 117.95735335350037, 119.64359784126282, 121.31746530532837, 123.01387095451355, 124.68993186950684, 126.37465834617615, 128.0900194644928, 129.8110339641571, 131.53890132904053, 133.27554416656494, 135.0070023536682, 136.69475674629211, 138.37246680259705, 140.0647792816162, 141.80130696296692, 143.46810054779053, 145.1597442626953, 146.84605169296265, 148.53251838684082, 150.25263690948486, 152.0164258480072, 153.75404024124146, 155.48228359222412, 157.16705179214478, 158.85669708251953, 160.517085313797, 162.1930594444275, 163.86346411705017, 165.54503345489502, 167.22802996635437, 168.89304304122925, 170.57298684120178, 172.87863326072693]
[17.058333333333334, 28.375, 44.55, 54.583333333333336, 56.63333333333333, 57.175, 62.24166666666667, 65.725, 65.05, 68.6, 67.53333333333333, 72.03333333333333, 72.48333333333333, 74.85833333333333, 75.05833333333334, 75.51666666666667, 75.925, 75.80833333333334, 77.29166666666667, 77.05, 77.4, 78.05, 77.66666666666667, 78.18333333333334, 78.05833333333334, 79.15, 79.25833333333334, 78.14166666666667, 78.65, 78.975, 79.88333333333334, 79.675, 80.05833333333334, 80.29166666666667, 80.41666666666667, 80.35, 80.84166666666667, 81.21666666666667, 81.13333333333334, 80.95, 81.39166666666667, 81.225, 81.63333333333334, 81.08333333333333, 81.44166666666666, 81.30833333333334, 81.575, 81.91666666666667, 82.01666666666667, 81.39166666666667, 81.975, 82.14166666666667, 82.44166666666666, 82.5, 82.6, 82.71666666666667, 82.725, 82.65, 82.60833333333333, 82.83333333333333, 82.575, 83.10833333333333, 83.38333333333334, 82.96666666666667, 82.85833333333333, 82.84166666666667, 83.58333333333333, 83.54166666666667, 83.45833333333333, 83.33333333333333, 83.2, 83.66666666666667, 83.61666666666666, 83.81666666666666, 83.73333333333333, 83.85, 83.66666666666667, 83.53333333333333, 83.63333333333334, 83.65, 83.71666666666667, 83.73333333333333, 83.81666666666666, 83.80833333333334, 84.05833333333334, 83.86666666666666, 84.16666666666667, 84.33333333333333, 84.36666666666666, 83.99166666666666, 84.29166666666667, 84.25, 84.33333333333333, 84.16666666666667, 84.2, 84.39166666666667, 84.475, 84.35, 84.29166666666667, 84.175, 84.65]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  18.8800
Round 1 global test acc  11.5100
Round 2 global test acc  12.6100
Round 3 global test acc  24.6600
Round 4 global test acc  16.6100
Round 5 global test acc  20.7600
Round 6 global test acc  19.3100
Round 7 global test acc  24.9600
Round 8 global test acc  31.8400
Round 9 global test acc  27.4600
Round 10 global test acc  24.0700
Round 11 global test acc  26.3300
Round 12 global test acc  26.3700
Round 13 global test acc  30.8300
Round 14 global test acc  36.1700
Round 15 global test acc  35.4400
Round 16 global test acc  35.7500
Round 17 global test acc  34.5800
Round 18 global test acc  28.2700
Round 19 global test acc  22.0400
Round 20 global test acc  30.6700
Round 21 global test acc  22.0800
Round 22 global test acc  30.9300
Round 23 global test acc  31.5300
Round 24 global test acc  24.6400
Round 25 global test acc  26.9300
Round 26 global test acc  30.7500
Round 27 global test acc  28.1300
Round 28 global test acc  25.6100
Round 29 global test acc  23.9600
Round 30 global test acc  24.3300
Round 31 global test acc  29.5000
Round 32 global test acc  28.2500
Round 33 global test acc  28.1200
Round 34 global test acc  35.5000
Round 35 global test acc  36.6200
Round 36 global test acc  32.9600
Round 37 global test acc  32.6600
Round 38 global test acc  31.3400
Round 39 global test acc  36.9500
Round 40 global test acc  36.5500
Round 41 global test acc  36.9600
Round 42 global test acc  28.4900
Round 43 global test acc  39.3200
Round 44 global test acc  24.5300
Round 45 global test acc  32.7500
Round 46 global test acc  23.4600
Round 47 global test acc  28.3000
Round 48 global test acc  32.3000
Round 49 global test acc  39.3600
Round 50 global test acc  39.9500
Round 51 global test acc  36.8800
Round 52 global test acc  30.6400
Round 53 global test acc  36.4700
Round 54 global test acc  29.1100
Round 55 global test acc  35.9600
Round 56 global test acc  35.4200
Round 57 global test acc  27.9100
Round 58 global test acc  38.7400
Round 59 global test acc  39.9000
Round 60 global test acc  37.7400
Round 61 global test acc  29.3700
Round 62 global test acc  32.4300
Round 63 global test acc  39.2800
Round 64 global test acc  40.3200
Round 65 global test acc  27.4800
Round 66 global test acc  21.3400
Round 67 global test acc  25.2500
Round 68 global test acc  27.6900
Round 69 global test acc  35.6800
Round 70 global test acc  27.3800
Round 71 global test acc  30.7500
Round 72 global test acc  32.5600
Round 73 global test acc  32.0400
Round 74 global test acc  30.3200
Round 75 global test acc  29.2100
Round 76 global test acc  41.2700
Round 77 global test acc  34.6500
Round 78 global test acc  43.5300
Round 79 global test acc  32.4800
Round 80 global test acc  28.5000
Round 81 global test acc  26.7100
Round 82 global test acc  24.4600
Round 83 global test acc  23.4200
Round 84 global test acc  22.6900
Round 85 global test acc  21.3900
Round 86 global test acc  20.8500
Round 87 global test acc  20.2500
Round 88 global test acc  19.7600
Round 89 global test acc  19.1700
Round 90 global test acc  19.0800
Round 91 global test acc  18.7100
Round 92 global test acc  18.5400
Round 93 global test acc  18.8200
Round 94 global test acc  19.6100
Round 95 global test acc  19.2000
Round 96 global test acc  18.8500
Round 97 global test acc  18.5700
Round 98 global test acc  18.6600
Round 99 global test acc  18.6400
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.508, Test loss: 2.153, Test accuracy: 20.32
Round   1, Train loss: 1.051, Test loss: 1.653, Test accuracy: 37.72
Round   2, Train loss: 0.860, Test loss: 1.485, Test accuracy: 43.74
Round   3, Train loss: 0.916, Test loss: 1.166, Test accuracy: 54.40
Round   4, Train loss: 0.807, Test loss: 1.015, Test accuracy: 58.08
Round   5, Train loss: 0.799, Test loss: 0.822, Test accuracy: 63.69
Round   6, Train loss: 0.762, Test loss: 0.770, Test accuracy: 66.08
Round   7, Train loss: 0.679, Test loss: 0.727, Test accuracy: 68.23
Round   8, Train loss: 0.640, Test loss: 0.695, Test accuracy: 69.80
Round   9, Train loss: 0.631, Test loss: 0.675, Test accuracy: 70.77
Round  10, Train loss: 0.669, Test loss: 0.665, Test accuracy: 71.43
Round  11, Train loss: 0.685, Test loss: 0.636, Test accuracy: 73.03
Round  12, Train loss: 0.554, Test loss: 0.621, Test accuracy: 73.32
Round  13, Train loss: 0.611, Test loss: 0.615, Test accuracy: 74.83
Round  14, Train loss: 0.621, Test loss: 0.605, Test accuracy: 74.92
Round  15, Train loss: 0.552, Test loss: 0.592, Test accuracy: 75.12
Round  16, Train loss: 0.630, Test loss: 0.593, Test accuracy: 75.23
Round  17, Train loss: 0.531, Test loss: 0.576, Test accuracy: 76.06
Round  18, Train loss: 0.631, Test loss: 0.580, Test accuracy: 76.19
Round  19, Train loss: 0.510, Test loss: 0.569, Test accuracy: 76.08
Round  20, Train loss: 0.582, Test loss: 0.558, Test accuracy: 76.47
Round  21, Train loss: 0.530, Test loss: 0.557, Test accuracy: 76.42
Round  22, Train loss: 0.527, Test loss: 0.538, Test accuracy: 77.33
Round  23, Train loss: 0.521, Test loss: 0.535, Test accuracy: 77.22
Round  24, Train loss: 0.478, Test loss: 0.529, Test accuracy: 77.70
Round  25, Train loss: 0.529, Test loss: 0.534, Test accuracy: 77.96
Round  26, Train loss: 0.551, Test loss: 0.524, Test accuracy: 78.07
Round  27, Train loss: 0.562, Test loss: 0.507, Test accuracy: 78.71
Round  28, Train loss: 0.345, Test loss: 0.496, Test accuracy: 79.03
Round  29, Train loss: 0.545, Test loss: 0.497, Test accuracy: 79.28
Round  30, Train loss: 0.466, Test loss: 0.494, Test accuracy: 79.22
Round  31, Train loss: 0.469, Test loss: 0.483, Test accuracy: 80.01
Round  32, Train loss: 0.604, Test loss: 0.481, Test accuracy: 80.01
Round  33, Train loss: 0.404, Test loss: 0.490, Test accuracy: 79.82
Round  34, Train loss: 0.525, Test loss: 0.479, Test accuracy: 80.13
Round  35, Train loss: 0.518, Test loss: 0.472, Test accuracy: 80.11
Round  36, Train loss: 0.469, Test loss: 0.474, Test accuracy: 80.03
Round  37, Train loss: 0.398, Test loss: 0.467, Test accuracy: 80.32
Round  38, Train loss: 0.462, Test loss: 0.456, Test accuracy: 80.76
Round  39, Train loss: 0.480, Test loss: 0.458, Test accuracy: 80.75
Round  40, Train loss: 0.426, Test loss: 0.454, Test accuracy: 81.08
Round  41, Train loss: 0.330, Test loss: 0.457, Test accuracy: 81.17
Round  42, Train loss: 0.506, Test loss: 0.445, Test accuracy: 81.45
Round  43, Train loss: 0.348, Test loss: 0.450, Test accuracy: 81.42
Round  44, Train loss: 0.480, Test loss: 0.451, Test accuracy: 81.33
Round  45, Train loss: 0.505, Test loss: 0.450, Test accuracy: 81.36
Round  46, Train loss: 0.420, Test loss: 0.446, Test accuracy: 81.47
Round  47, Train loss: 0.445, Test loss: 0.442, Test accuracy: 81.81
Round  48, Train loss: 0.390, Test loss: 0.435, Test accuracy: 82.09
Round  49, Train loss: 0.444, Test loss: 0.432, Test accuracy: 82.01
Round  50, Train loss: 0.410, Test loss: 0.433, Test accuracy: 82.42
Round  51, Train loss: 0.402, Test loss: 0.430, Test accuracy: 82.06
Round  52, Train loss: 0.499, Test loss: 0.428, Test accuracy: 83.04
Round  53, Train loss: 0.374, Test loss: 0.429, Test accuracy: 82.28
Round  54, Train loss: 0.445, Test loss: 0.425, Test accuracy: 82.71
Round  55, Train loss: 0.440, Test loss: 0.428, Test accuracy: 82.28
Round  56, Train loss: 0.387, Test loss: 0.426, Test accuracy: 82.35
Round  57, Train loss: 0.338, Test loss: 0.423, Test accuracy: 82.77
Round  58, Train loss: 0.381, Test loss: 0.418, Test accuracy: 82.89
Round  59, Train loss: 0.442, Test loss: 0.423, Test accuracy: 82.69
Round  60, Train loss: 0.341, Test loss: 0.415, Test accuracy: 82.72
Round  61, Train loss: 0.431, Test loss: 0.415, Test accuracy: 82.81
Round  62, Train loss: 0.380, Test loss: 0.415, Test accuracy: 82.88
Round  63, Train loss: 0.365, Test loss: 0.424, Test accuracy: 82.64
Round  64, Train loss: 0.482, Test loss: 0.420, Test accuracy: 82.68
Round  65, Train loss: 0.306, Test loss: 0.414, Test accuracy: 83.28
Round  66, Train loss: 0.287, Test loss: 0.415, Test accuracy: 82.97
Round  67, Train loss: 0.325, Test loss: 0.407, Test accuracy: 83.45
Round  68, Train loss: 0.333, Test loss: 0.404, Test accuracy: 83.50
Round  69, Train loss: 0.445, Test loss: 0.410, Test accuracy: 83.36
Round  70, Train loss: 0.306, Test loss: 0.407, Test accuracy: 83.45
Round  71, Train loss: 0.361, Test loss: 0.409, Test accuracy: 83.40
Round  72, Train loss: 0.324, Test loss: 0.408, Test accuracy: 83.53
Round  73, Train loss: 0.303, Test loss: 0.409, Test accuracy: 83.33
Round  74, Train loss: 0.376, Test loss: 0.404, Test accuracy: 83.66
Round  75, Train loss: 0.313, Test loss: 0.404, Test accuracy: 83.42
Round  76, Train loss: 0.356, Test loss: 0.399, Test accuracy: 84.03
Round  77, Train loss: 0.280, Test loss: 0.398, Test accuracy: 83.89
Round  78, Train loss: 0.357, Test loss: 0.404, Test accuracy: 83.78
Round  79, Train loss: 0.248, Test loss: 0.407, Test accuracy: 83.41
Round  80, Train loss: 0.402, Test loss: 0.406, Test accuracy: 83.65
Round  81, Train loss: 0.296, Test loss: 0.401, Test accuracy: 83.72
Round  82, Train loss: 0.384, Test loss: 0.398, Test accuracy: 83.92
Round  83, Train loss: 0.336, Test loss: 0.399, Test accuracy: 83.95
Round  84, Train loss: 0.260, Test loss: 0.398, Test accuracy: 83.87
Round  85, Train loss: 0.413, Test loss: 0.398, Test accuracy: 83.93
Round  86, Train loss: 0.324, Test loss: 0.400, Test accuracy: 83.51
Round  87, Train loss: 0.344, Test loss: 0.397, Test accuracy: 84.31
Round  88, Train loss: 0.252, Test loss: 0.394, Test accuracy: 84.24
Round  89, Train loss: 0.206, Test loss: 0.393, Test accuracy: 84.29
Round  90, Train loss: 0.243, Test loss: 0.400, Test accuracy: 84.41
Round  91, Train loss: 0.287, Test loss: 0.391, Test accuracy: 84.37
Round  92, Train loss: 0.240, Test loss: 0.395, Test accuracy: 84.13
Round  93, Train loss: 0.232, Test loss: 0.394, Test accuracy: 84.18
Round  94, Train loss: 0.261, Test loss: 0.397, Test accuracy: 84.10
Round  95, Train loss: 0.300, Test loss: 0.397, Test accuracy: 84.05
Round  96, Train loss: 0.222, Test loss: 0.395, Test accuracy: 84.07
Round  97, Train loss: 0.289, Test loss: 0.397, Test accuracy: 83.83
Round  98, Train loss: 0.279, Test loss: 0.400, Test accuracy: 83.79
Round  99, Train loss: 0.257, Test loss: 0.397, Test accuracy: 84.06
Final Round, Train loss: 0.237, Test loss: 0.396, Test accuracy: 84.31
Average accuracy final 10 rounds: 84.09833333333333
1328.0385959148407
[2.030087947845459, 3.658116102218628, 5.293462038040161, 6.9131200313568115, 8.53981876373291, 10.197973012924194, 11.86130166053772, 13.503721952438354, 15.155487060546875, 16.794420957565308, 18.426682949066162, 20.067421436309814, 21.710551738739014, 23.39213252067566, 25.12611222267151, 26.804237365722656, 28.478163242340088, 30.143706798553467, 31.81707787513733, 33.46558904647827, 35.09177374839783, 36.73016357421875, 38.382508516311646, 40.002607345581055, 41.652480602264404, 43.32306218147278, 45.0209527015686, 46.73838138580322, 48.42947173118591, 50.1250422000885, 51.8135187625885, 53.46773076057434, 55.09803366661072, 56.74945640563965, 58.40245246887207, 60.06793284416199, 61.69283485412598, 63.33330154418945, 65.03068327903748, 66.74576616287231, 68.47689604759216, 70.19008469581604, 71.81472873687744, 73.46203637123108, 75.18552803993225, 76.87471079826355, 78.59221601486206, 80.31925749778748, 81.98388409614563, 83.63160872459412, 85.28536677360535, 86.92107439041138, 88.53581547737122, 90.18097710609436, 91.80503249168396, 93.44684839248657, 95.0956678390503, 96.73536849021912, 98.364337682724, 99.98878240585327, 101.61665868759155, 103.25428199768066, 104.8866331577301, 106.5325756072998, 108.1905448436737, 109.9038770198822, 111.59610676765442, 113.28695511817932, 114.97116303443909, 116.67578387260437, 118.35394787788391, 119.98872351646423, 121.61411786079407, 123.25494289398193, 124.91524267196655, 126.60871195793152, 128.27061557769775, 129.95253729820251, 131.63676595687866, 133.3076572418213, 134.99505639076233, 136.67302227020264, 138.310955286026, 139.93359804153442, 141.61574363708496, 143.28039741516113, 144.93654823303223, 146.55828094482422, 148.17931747436523, 149.839022397995, 151.52469635009766, 153.1866705417633, 154.90461111068726, 156.57939887046814, 158.21111488342285, 159.83248472213745, 161.45804476737976, 163.07932567596436, 164.71822595596313, 166.34112930297852, 168.56258535385132]
[20.316666666666666, 37.71666666666667, 43.74166666666667, 54.4, 58.083333333333336, 63.69166666666667, 66.075, 68.23333333333333, 69.8, 70.76666666666667, 71.43333333333334, 73.025, 73.31666666666666, 74.83333333333333, 74.925, 75.11666666666666, 75.23333333333333, 76.05833333333334, 76.19166666666666, 76.08333333333333, 76.46666666666667, 76.41666666666667, 77.33333333333333, 77.225, 77.7, 77.95833333333333, 78.06666666666666, 78.70833333333333, 79.025, 79.275, 79.225, 80.00833333333334, 80.00833333333334, 79.81666666666666, 80.13333333333334, 80.10833333333333, 80.025, 80.31666666666666, 80.75833333333334, 80.75, 81.08333333333333, 81.16666666666667, 81.45, 81.425, 81.325, 81.35833333333333, 81.46666666666667, 81.80833333333334, 82.09166666666667, 82.00833333333334, 82.41666666666667, 82.05833333333334, 83.04166666666667, 82.28333333333333, 82.70833333333333, 82.275, 82.35, 82.76666666666667, 82.89166666666667, 82.69166666666666, 82.71666666666667, 82.80833333333334, 82.875, 82.64166666666667, 82.68333333333334, 83.28333333333333, 82.975, 83.45, 83.5, 83.35833333333333, 83.45, 83.4, 83.525, 83.325, 83.65833333333333, 83.41666666666667, 84.03333333333333, 83.89166666666667, 83.78333333333333, 83.40833333333333, 83.65, 83.71666666666667, 83.925, 83.95, 83.86666666666666, 83.93333333333334, 83.50833333333334, 84.30833333333334, 84.24166666666666, 84.29166666666667, 84.40833333333333, 84.36666666666666, 84.13333333333334, 84.18333333333334, 84.1, 84.05, 84.06666666666666, 83.825, 83.79166666666667, 84.05833333333334, 84.30833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.427, Test loss: 2.299, Test accuracy: 25.29
Round   1, Train loss: 0.979, Test loss: 1.816, Test accuracy: 37.99
Round   2, Train loss: 0.897, Test loss: 1.319, Test accuracy: 51.36
Round   3, Train loss: 0.849, Test loss: 1.195, Test accuracy: 53.35
Round   4, Train loss: 0.776, Test loss: 1.068, Test accuracy: 56.87
Round   5, Train loss: 0.654, Test loss: 1.048, Test accuracy: 61.79
Round   6, Train loss: 0.713, Test loss: 0.990, Test accuracy: 61.42
Round   7, Train loss: 0.678, Test loss: 0.970, Test accuracy: 64.64
Round   8, Train loss: 0.740, Test loss: 0.905, Test accuracy: 65.82
Round   9, Train loss: 0.626, Test loss: 0.706, Test accuracy: 71.39
Round  10, Train loss: 0.657, Test loss: 0.683, Test accuracy: 71.34
Round  11, Train loss: 0.626, Test loss: 0.706, Test accuracy: 72.06
Round  12, Train loss: 0.624, Test loss: 0.670, Test accuracy: 72.99
Round  13, Train loss: 0.703, Test loss: 0.593, Test accuracy: 75.43
Round  14, Train loss: 0.559, Test loss: 0.582, Test accuracy: 75.83
Round  15, Train loss: 0.629, Test loss: 0.582, Test accuracy: 76.24
Round  16, Train loss: 0.591, Test loss: 0.592, Test accuracy: 76.19
Round  17, Train loss: 0.557, Test loss: 0.574, Test accuracy: 76.64
Round  18, Train loss: 0.608, Test loss: 0.563, Test accuracy: 77.00
Round  19, Train loss: 0.525, Test loss: 0.549, Test accuracy: 77.49
Round  20, Train loss: 0.529, Test loss: 0.531, Test accuracy: 77.78
Round  21, Train loss: 0.542, Test loss: 0.531, Test accuracy: 77.56
Round  22, Train loss: 0.503, Test loss: 0.525, Test accuracy: 77.39
Round  23, Train loss: 0.521, Test loss: 0.523, Test accuracy: 78.04
Round  24, Train loss: 0.599, Test loss: 0.514, Test accuracy: 78.62
Round  25, Train loss: 0.653, Test loss: 0.520, Test accuracy: 78.46
Round  26, Train loss: 0.513, Test loss: 0.518, Test accuracy: 78.77
Round  27, Train loss: 0.579, Test loss: 0.518, Test accuracy: 79.04
Round  28, Train loss: 0.541, Test loss: 0.505, Test accuracy: 79.33
Round  29, Train loss: 0.400, Test loss: 0.493, Test accuracy: 79.47
Round  30, Train loss: 0.621, Test loss: 0.487, Test accuracy: 79.91
Round  31, Train loss: 0.561, Test loss: 0.492, Test accuracy: 79.94
Round  32, Train loss: 0.564, Test loss: 0.483, Test accuracy: 80.39
Round  33, Train loss: 0.461, Test loss: 0.476, Test accuracy: 80.52
Round  34, Train loss: 0.505, Test loss: 0.479, Test accuracy: 80.25
Round  35, Train loss: 0.547, Test loss: 0.475, Test accuracy: 80.42
Round  36, Train loss: 0.484, Test loss: 0.475, Test accuracy: 80.57
Round  37, Train loss: 0.356, Test loss: 0.469, Test accuracy: 80.51
Round  38, Train loss: 0.380, Test loss: 0.464, Test accuracy: 80.79
Round  39, Train loss: 0.499, Test loss: 0.459, Test accuracy: 81.22
Round  40, Train loss: 0.431, Test loss: 0.453, Test accuracy: 81.44
Round  41, Train loss: 0.491, Test loss: 0.460, Test accuracy: 80.87
Round  42, Train loss: 0.486, Test loss: 0.446, Test accuracy: 81.78
Round  43, Train loss: 0.439, Test loss: 0.452, Test accuracy: 81.61
Round  44, Train loss: 0.516, Test loss: 0.457, Test accuracy: 81.55
Round  45, Train loss: 0.372, Test loss: 0.457, Test accuracy: 81.53
Round  46, Train loss: 0.428, Test loss: 0.453, Test accuracy: 81.46
Round  47, Train loss: 0.454, Test loss: 0.457, Test accuracy: 81.32
Round  48, Train loss: 0.443, Test loss: 0.450, Test accuracy: 81.62
Round  49, Train loss: 0.314, Test loss: 0.445, Test accuracy: 82.07
Round  50, Train loss: 0.395, Test loss: 0.443, Test accuracy: 82.34
Round  51, Train loss: 0.437, Test loss: 0.432, Test accuracy: 82.59
Round  52, Train loss: 0.359, Test loss: 0.439, Test accuracy: 82.19
Round  53, Train loss: 0.354, Test loss: 0.439, Test accuracy: 82.20
Round  54, Train loss: 0.375, Test loss: 0.432, Test accuracy: 82.36
Round  55, Train loss: 0.386, Test loss: 0.432, Test accuracy: 82.22
Round  56, Train loss: 0.398, Test loss: 0.433, Test accuracy: 82.28
Round  57, Train loss: 0.417, Test loss: 0.428, Test accuracy: 82.83
Round  58, Train loss: 0.407, Test loss: 0.424, Test accuracy: 82.91
Round  59, Train loss: 0.329, Test loss: 0.427, Test accuracy: 83.00
Round  60, Train loss: 0.369, Test loss: 0.432, Test accuracy: 82.61
Round  61, Train loss: 0.346, Test loss: 0.423, Test accuracy: 82.77
Round  62, Train loss: 0.295, Test loss: 0.425, Test accuracy: 82.86
Round  63, Train loss: 0.334, Test loss: 0.416, Test accuracy: 82.97
Round  64, Train loss: 0.380, Test loss: 0.414, Test accuracy: 82.96
Round  65, Train loss: 0.356, Test loss: 0.417, Test accuracy: 83.03
Round  66, Train loss: 0.305, Test loss: 0.418, Test accuracy: 83.05
Round  67, Train loss: 0.395, Test loss: 0.414, Test accuracy: 83.17
Round  68, Train loss: 0.399, Test loss: 0.417, Test accuracy: 83.06
Round  69, Train loss: 0.379, Test loss: 0.410, Test accuracy: 83.23
Round  70, Train loss: 0.316, Test loss: 0.410, Test accuracy: 83.28
Round  71, Train loss: 0.247, Test loss: 0.409, Test accuracy: 83.29
Round  72, Train loss: 0.461, Test loss: 0.410, Test accuracy: 83.58
Round  73, Train loss: 0.338, Test loss: 0.415, Test accuracy: 83.02
Round  74, Train loss: 0.280, Test loss: 0.403, Test accuracy: 83.62
Round  75, Train loss: 0.370, Test loss: 0.409, Test accuracy: 83.52
Round  76, Train loss: 0.265, Test loss: 0.406, Test accuracy: 83.51
Round  77, Train loss: 0.330, Test loss: 0.410, Test accuracy: 83.66
Round  78, Train loss: 0.328, Test loss: 0.419, Test accuracy: 83.17
Round  79, Train loss: 0.351, Test loss: 0.417, Test accuracy: 83.47
Round  80, Train loss: 0.290, Test loss: 0.405, Test accuracy: 83.57
Round  81, Train loss: 0.284, Test loss: 0.406, Test accuracy: 83.92
Round  82, Train loss: 0.347, Test loss: 0.409, Test accuracy: 83.50
Round  83, Train loss: 0.332, Test loss: 0.408, Test accuracy: 83.46
Round  84, Train loss: 0.237, Test loss: 0.410, Test accuracy: 83.58
Round  85, Train loss: 0.337, Test loss: 0.412, Test accuracy: 83.37
Round  86, Train loss: 0.406, Test loss: 0.409, Test accuracy: 83.51
Round  87, Train loss: 0.260, Test loss: 0.403, Test accuracy: 83.73
Round  88, Train loss: 0.229, Test loss: 0.401, Test accuracy: 83.88
Round  89, Train loss: 0.333, Test loss: 0.407, Test accuracy: 83.55
Round  90, Train loss: 0.295, Test loss: 0.402, Test accuracy: 83.76
Round  91, Train loss: 0.241, Test loss: 0.403, Test accuracy: 83.92
Round  92, Train loss: 0.337, Test loss: 0.399, Test accuracy: 84.10
Round  93, Train loss: 0.235, Test loss: 0.390, Test accuracy: 84.05
Round  94, Train loss: 0.218, Test loss: 0.399, Test accuracy: 83.94
Round  95, Train loss: 0.300, Test loss: 0.398, Test accuracy: 83.62
Round  96, Train loss: 0.228, Test loss: 0.396, Test accuracy: 83.82
Round  97, Train loss: 0.289, Test loss: 0.398, Test accuracy: 84.08
Round  98, Train loss: 0.312, Test loss: 0.398, Test accuracy: 84.22
Round  99, Train loss: 0.310, Test loss: 0.387, Test accuracy: 84.38
Final Round, Train loss: 0.244, Test loss: 0.389, Test accuracy: 84.48
Average accuracy final 10 rounds: 83.98916666666668
1871.4560027122498
[2.02103853225708, 3.6636016368865967, 5.342033863067627, 7.049038648605347, 8.761602401733398, 10.490497589111328, 12.186388969421387, 13.8844633102417, 15.570824384689331, 17.22690439224243, 18.930617809295654, 20.641460180282593, 22.303561687469482, 23.920095205307007, 25.57781982421875, 27.25540852546692, 28.96916127204895, 30.626108407974243, 32.31620931625366, 34.00859475135803, 35.69675302505493, 38.62425398826599, 41.548288106918335, 44.44876408576965, 47.364113330841064, 50.26155233383179, 53.06454873085022, 55.827290058135986, 58.66355633735657, 61.51147389411926, 64.295649766922, 67.11543488502502, 69.91538429260254, 72.68573832511902, 75.54652976989746, 78.51575899124146, 81.4573073387146, 84.35892939567566, 87.14754962921143, 90.00356101989746, 92.88085436820984, 95.69801712036133, 98.58362555503845, 101.47538661956787, 104.3810133934021, 107.1959342956543, 109.99369096755981, 112.89491605758667, 115.67129588127136, 118.44756031036377, 121.31565737724304, 124.0874445438385, 126.86913394927979, 129.76043725013733, 132.70791721343994, 135.73430061340332, 138.58862853050232, 141.35490250587463, 144.14187479019165, 147.00276494026184, 149.8734290599823, 152.6858742237091, 155.55750846862793, 158.38645768165588, 161.34529638290405, 164.2769434452057, 167.1627368927002, 169.94649243354797, 172.7885866165161, 175.64659094810486, 178.4548020362854, 181.26584768295288, 184.07963371276855, 186.91293740272522, 189.73288416862488, 192.56254935264587, 195.42305755615234, 198.33750581741333, 201.14708018302917, 203.96857810020447, 206.75916862487793, 209.54745221138, 212.42134833335876, 215.27771663665771, 218.20507836341858, 221.03992557525635, 223.85232830047607, 226.69328212738037, 229.607075214386, 232.4706416130066, 235.33141803741455, 238.13876700401306, 240.97364139556885, 243.78735184669495, 246.57021975517273, 249.41484236717224, 252.22100639343262, 255.04015493392944, 257.9527220726013, 260.8031373023987, 263.01602602005005]
[25.291666666666668, 37.99166666666667, 51.358333333333334, 53.35, 56.86666666666667, 61.791666666666664, 61.425, 64.64166666666667, 65.81666666666666, 71.39166666666667, 71.34166666666667, 72.05833333333334, 72.99166666666666, 75.43333333333334, 75.83333333333333, 76.24166666666666, 76.19166666666666, 76.64166666666667, 77.0, 77.49166666666666, 77.775, 77.55833333333334, 77.39166666666667, 78.04166666666667, 78.61666666666666, 78.45833333333333, 78.76666666666667, 79.04166666666667, 79.325, 79.475, 79.90833333333333, 79.94166666666666, 80.39166666666667, 80.51666666666667, 80.25, 80.41666666666667, 80.56666666666666, 80.50833333333334, 80.79166666666667, 81.225, 81.44166666666666, 80.86666666666666, 81.78333333333333, 81.60833333333333, 81.55, 81.525, 81.45833333333333, 81.31666666666666, 81.61666666666666, 82.06666666666666, 82.34166666666667, 82.59166666666667, 82.19166666666666, 82.2, 82.35833333333333, 82.21666666666667, 82.28333333333333, 82.825, 82.90833333333333, 83.0, 82.60833333333333, 82.76666666666667, 82.85833333333333, 82.96666666666667, 82.95833333333333, 83.03333333333333, 83.05, 83.16666666666667, 83.05833333333334, 83.23333333333333, 83.275, 83.29166666666667, 83.58333333333333, 83.01666666666667, 83.625, 83.51666666666667, 83.50833333333334, 83.65833333333333, 83.175, 83.475, 83.56666666666666, 83.925, 83.5, 83.45833333333333, 83.575, 83.36666666666666, 83.50833333333334, 83.73333333333333, 83.875, 83.55, 83.75833333333334, 83.925, 84.1, 84.05, 83.94166666666666, 83.61666666666666, 83.81666666666666, 84.08333333333333, 84.225, 84.375, 84.48333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 201, in get_data_from_file
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_co_teaching2.py", line 92, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "RFL.py", line 62, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 93, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 95, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 6, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.210, Test loss: 1.849, Test accuracy: 25.77
Round   0, Global train loss: 1.210, Global test loss: 2.203, Global test accuracy: 16.36
Round   1, Train loss: 1.097, Test loss: 1.661, Test accuracy: 38.10
Round   1, Global train loss: 1.097, Global test loss: 2.238, Global test accuracy: 22.68
Round   2, Train loss: 1.034, Test loss: 1.656, Test accuracy: 40.93
Round   2, Global train loss: 1.034, Global test loss: 2.378, Global test accuracy: 21.43
Round   3, Train loss: 0.919, Test loss: 1.255, Test accuracy: 53.50
Round   3, Global train loss: 0.919, Global test loss: 2.104, Global test accuracy: 28.66
Round   4, Train loss: 0.858, Test loss: 1.224, Test accuracy: 54.26
Round   4, Global train loss: 0.858, Global test loss: 2.138, Global test accuracy: 24.73
Round   5, Train loss: 0.805, Test loss: 1.107, Test accuracy: 55.06
Round   5, Global train loss: 0.805, Global test loss: 2.114, Global test accuracy: 18.25
Round   6, Train loss: 0.862, Test loss: 1.134, Test accuracy: 55.52
Round   6, Global train loss: 0.862, Global test loss: 2.239, Global test accuracy: 23.03
Round   7, Train loss: 0.921, Test loss: 1.026, Test accuracy: 58.42
Round   7, Global train loss: 0.921, Global test loss: 2.156, Global test accuracy: 23.32
Round   8, Train loss: 0.812, Test loss: 0.961, Test accuracy: 61.83
Round   8, Global train loss: 0.812, Global test loss: 2.193, Global test accuracy: 20.16
Round   9, Train loss: 0.819, Test loss: 0.955, Test accuracy: 61.25
Round   9, Global train loss: 0.819, Global test loss: 2.232, Global test accuracy: 22.96
Round  10, Train loss: 0.846, Test loss: 0.836, Test accuracy: 64.22
Round  10, Global train loss: 0.846, Global test loss: 2.112, Global test accuracy: 19.30
Round  11, Train loss: 0.880, Test loss: 0.854, Test accuracy: 63.76
Round  11, Global train loss: 0.880, Global test loss: 2.261, Global test accuracy: 23.55
Round  12, Train loss: 0.904, Test loss: 0.842, Test accuracy: 65.33
Round  12, Global train loss: 0.904, Global test loss: 2.075, Global test accuracy: 29.65
Round  13, Train loss: 0.795, Test loss: 0.765, Test accuracy: 68.30
Round  13, Global train loss: 0.795, Global test loss: 2.148, Global test accuracy: 21.73
Round  14, Train loss: 0.853, Test loss: 0.770, Test accuracy: 67.65
Round  14, Global train loss: 0.853, Global test loss: 2.113, Global test accuracy: 21.57
Round  15, Train loss: 0.665, Test loss: 0.755, Test accuracy: 68.66
Round  15, Global train loss: 0.665, Global test loss: 2.091, Global test accuracy: 23.88
Round  16, Train loss: 0.733, Test loss: 0.765, Test accuracy: 68.74
Round  16, Global train loss: 0.733, Global test loss: 2.042, Global test accuracy: 28.38
Round  17, Train loss: 0.776, Test loss: 0.754, Test accuracy: 69.31
Round  17, Global train loss: 0.776, Global test loss: 2.073, Global test accuracy: 28.49
Round  18, Train loss: 0.796, Test loss: 0.771, Test accuracy: 68.99
Round  18, Global train loss: 0.796, Global test loss: 2.154, Global test accuracy: 14.86
Round  19, Train loss: 0.730, Test loss: 0.759, Test accuracy: 70.16
Round  19, Global train loss: 0.730, Global test loss: 2.116, Global test accuracy: 22.98
Round  20, Train loss: 0.563, Test loss: 0.759, Test accuracy: 70.12
Round  20, Global train loss: 0.563, Global test loss: 2.086, Global test accuracy: 25.68
Round  21, Train loss: 0.624, Test loss: 0.756, Test accuracy: 70.12
Round  21, Global train loss: 0.624, Global test loss: 2.060, Global test accuracy: 32.62
Round  22, Train loss: 0.746, Test loss: 0.750, Test accuracy: 70.33
Round  22, Global train loss: 0.746, Global test loss: 2.131, Global test accuracy: 26.82
Round  23, Train loss: 0.707, Test loss: 0.742, Test accuracy: 70.42
Round  23, Global train loss: 0.707, Global test loss: 2.079, Global test accuracy: 23.33
Round  24, Train loss: 0.722, Test loss: 0.743, Test accuracy: 70.16
Round  24, Global train loss: 0.722, Global test loss: 2.102, Global test accuracy: 24.55
Round  25, Train loss: 0.690, Test loss: 0.719, Test accuracy: 71.34
Round  25, Global train loss: 0.690, Global test loss: 2.097, Global test accuracy: 21.52
Round  26, Train loss: 0.750, Test loss: 0.739, Test accuracy: 70.68
Round  26, Global train loss: 0.750, Global test loss: 2.255, Global test accuracy: 19.37
Round  27, Train loss: 0.550, Test loss: 0.747, Test accuracy: 71.13
Round  27, Global train loss: 0.550, Global test loss: 2.094, Global test accuracy: 30.27
Round  28, Train loss: 0.553, Test loss: 0.745, Test accuracy: 70.93
Round  28, Global train loss: 0.553, Global test loss: 2.111, Global test accuracy: 29.02
Round  29, Train loss: 0.599, Test loss: 0.746, Test accuracy: 71.04
Round  29, Global train loss: 0.599, Global test loss: 2.313, Global test accuracy: 14.31
Round  30, Train loss: 0.586, Test loss: 0.741, Test accuracy: 71.15
Round  30, Global train loss: 0.586, Global test loss: 2.174, Global test accuracy: 19.22
Round  31, Train loss: 0.627, Test loss: 0.738, Test accuracy: 71.10
Round  31, Global train loss: 0.627, Global test loss: 2.099, Global test accuracy: 23.82
Round  32, Train loss: 0.468, Test loss: 0.755, Test accuracy: 70.62
Round  32, Global train loss: 0.468, Global test loss: 2.086, Global test accuracy: 30.08
Round  33, Train loss: 0.662, Test loss: 0.758, Test accuracy: 71.17
Round  33, Global train loss: 0.662, Global test loss: 2.256, Global test accuracy: 22.60
Round  34, Train loss: 0.678, Test loss: 0.772, Test accuracy: 71.18
Round  34, Global train loss: 0.678, Global test loss: 2.088, Global test accuracy: 30.72
Round  35, Train loss: 0.646, Test loss: 0.786, Test accuracy: 70.60
Round  35, Global train loss: 0.646, Global test loss: 2.166, Global test accuracy: 30.77
Round  36, Train loss: 0.613, Test loss: 0.816, Test accuracy: 68.83
Round  36, Global train loss: 0.613, Global test loss: 2.086, Global test accuracy: 35.99
Round  37, Train loss: 0.525, Test loss: 0.798, Test accuracy: 69.97
Round  37, Global train loss: 0.525, Global test loss: 2.160, Global test accuracy: 21.62
Round  38, Train loss: 0.679, Test loss: 0.811, Test accuracy: 69.31
Round  38, Global train loss: 0.679, Global test loss: 2.109, Global test accuracy: 26.89
Round  39, Train loss: 0.519, Test loss: 0.802, Test accuracy: 69.10
Round  39, Global train loss: 0.519, Global test loss: 2.135, Global test accuracy: 17.21
Round  40, Train loss: 0.620, Test loss: 0.820, Test accuracy: 68.86
Round  40, Global train loss: 0.620, Global test loss: 2.063, Global test accuracy: 23.61
Round  41, Train loss: 0.618, Test loss: 0.842, Test accuracy: 68.41
Round  41, Global train loss: 0.618, Global test loss: 2.116, Global test accuracy: 23.77
Round  42, Train loss: 0.507, Test loss: 0.846, Test accuracy: 68.28
Round  42, Global train loss: 0.507, Global test loss: 2.300, Global test accuracy: 17.49
Round  43, Train loss: 0.418, Test loss: 0.834, Test accuracy: 69.22
Round  43, Global train loss: 0.418, Global test loss: 2.042, Global test accuracy: 33.33
Round  44, Train loss: 0.320, Test loss: 0.809, Test accuracy: 69.93
Round  44, Global train loss: 0.320, Global test loss: 2.123, Global test accuracy: 25.94
Round  45, Train loss: 0.460, Test loss: 0.797, Test accuracy: 70.17
Round  45, Global train loss: 0.460, Global test loss: 2.213, Global test accuracy: 22.81
Round  46, Train loss: 0.355, Test loss: 0.815, Test accuracy: 69.41
Round  46, Global train loss: 0.355, Global test loss: 2.102, Global test accuracy: 23.17
Round  47, Train loss: 0.627, Test loss: 0.826, Test accuracy: 68.94
Round  47, Global train loss: 0.627, Global test loss: 2.092, Global test accuracy: 26.94
Round  48, Train loss: 0.417, Test loss: 0.848, Test accuracy: 68.77
Round  48, Global train loss: 0.417, Global test loss: 2.051, Global test accuracy: 24.43
Round  49, Train loss: 0.392, Test loss: 0.855, Test accuracy: 68.77
Round  49, Global train loss: 0.392, Global test loss: 2.108, Global test accuracy: 23.82
Round  50, Train loss: 0.425, Test loss: 0.872, Test accuracy: 69.08
Round  50, Global train loss: 0.425, Global test loss: 2.052, Global test accuracy: 26.94
Round  51, Train loss: 0.348, Test loss: 0.864, Test accuracy: 69.88
Round  51, Global train loss: 0.348, Global test loss: 2.143, Global test accuracy: 23.43
Round  52, Train loss: 0.405, Test loss: 0.900, Test accuracy: 69.46
Round  52, Global train loss: 0.405, Global test loss: 2.129, Global test accuracy: 23.77
Round  53, Train loss: 0.434, Test loss: 0.925, Test accuracy: 68.74
Round  53, Global train loss: 0.434, Global test loss: 2.135, Global test accuracy: 21.93
Round  54, Train loss: 0.464, Test loss: 0.927, Test accuracy: 68.81
Round  54, Global train loss: 0.464, Global test loss: 2.125, Global test accuracy: 25.77
Round  55, Train loss: 0.365, Test loss: 0.920, Test accuracy: 69.43
Round  55, Global train loss: 0.365, Global test loss: 2.114, Global test accuracy: 23.88
Round  56, Train loss: 0.489, Test loss: 0.937, Test accuracy: 68.56
Round  56, Global train loss: 0.489, Global test loss: 2.189, Global test accuracy: 22.83
Round  57, Train loss: 0.513, Test loss: 0.960, Test accuracy: 68.01
Round  57, Global train loss: 0.513, Global test loss: 2.155, Global test accuracy: 24.45
Round  58, Train loss: 0.327, Test loss: 1.010, Test accuracy: 67.47
Round  58, Global train loss: 0.327, Global test loss: 2.105, Global test accuracy: 30.66
Round  59, Train loss: 0.491, Test loss: 0.993, Test accuracy: 68.33
Round  59, Global train loss: 0.491, Global test loss: 2.143, Global test accuracy: 23.18
Round  60, Train loss: 0.312, Test loss: 0.984, Test accuracy: 68.67
Round  60, Global train loss: 0.312, Global test loss: 2.105, Global test accuracy: 27.24
Round  61, Train loss: 0.203, Test loss: 1.010, Test accuracy: 68.30
Round  61, Global train loss: 0.203, Global test loss: 2.072, Global test accuracy: 21.38
Round  62, Train loss: 0.320, Test loss: 1.005, Test accuracy: 68.31
Round  62, Global train loss: 0.320, Global test loss: 2.012, Global test accuracy: 29.19
Round  63, Train loss: 0.359, Test loss: 1.002, Test accuracy: 68.32
Round  63, Global train loss: 0.359, Global test loss: 2.175, Global test accuracy: 20.57
Round  64, Train loss: 0.214, Test loss: 1.004, Test accuracy: 67.80
Round  64, Global train loss: 0.214, Global test loss: 2.161, Global test accuracy: 23.12
Round  65, Train loss: 0.269, Test loss: 1.019, Test accuracy: 67.47
Round  65, Global train loss: 0.269, Global test loss: 2.147, Global test accuracy: 20.76
Round  66, Train loss: 0.261, Test loss: 1.009, Test accuracy: 67.79
Round  66, Global train loss: 0.261, Global test loss: 2.130, Global test accuracy: 23.02
Round  67, Train loss: 0.335, Test loss: 1.036, Test accuracy: 67.79
Round  67, Global train loss: 0.335, Global test loss: 2.228, Global test accuracy: 16.13
Round  68, Train loss: 0.372, Test loss: 1.079, Test accuracy: 67.28
Round  68, Global train loss: 0.372, Global test loss: 2.068, Global test accuracy: 31.77
Round  69, Train loss: 0.364, Test loss: 1.095, Test accuracy: 67.52
Round  69, Global train loss: 0.364, Global test loss: 2.210, Global test accuracy: 24.10
Round  70, Train loss: 0.170, Test loss: 1.140, Test accuracy: 67.28
Round  70, Global train loss: 0.170, Global test loss: 2.208, Global test accuracy: 18.43
Round  71, Train loss: 0.304, Test loss: 1.121, Test accuracy: 67.52
Round  71, Global train loss: 0.304, Global test loss: 2.114, Global test accuracy: 26.58
Round  72, Train loss: 0.233, Test loss: 1.123, Test accuracy: 67.54
Round  72, Global train loss: 0.233, Global test loss: 2.074, Global test accuracy: 27.44
Round  73, Train loss: 0.383, Test loss: 1.117, Test accuracy: 67.62
Round  73, Global train loss: 0.383, Global test loss: 2.163, Global test accuracy: 21.47
Round  74, Train loss: 0.363, Test loss: 1.124, Test accuracy: 67.81
Round  74, Global train loss: 0.363, Global test loss: 2.145, Global test accuracy: 29.67
Round  75, Train loss: 0.298, Test loss: 1.156, Test accuracy: 67.26
Round  75, Global train loss: 0.298, Global test loss: 2.140, Global test accuracy: 25.08
Round  76, Train loss: 0.251, Test loss: 1.152, Test accuracy: 67.27
Round  76, Global train loss: 0.251, Global test loss: 2.140, Global test accuracy: 24.60
Round  77, Train loss: 0.277, Test loss: 1.157, Test accuracy: 67.38
Round  77, Global train loss: 0.277, Global test loss: 2.174, Global test accuracy: 26.82
Round  78, Train loss: 0.174, Test loss: 1.146, Test accuracy: 67.62
Round  78, Global train loss: 0.174, Global test loss: 2.231, Global test accuracy: 21.95
Round  79, Train loss: 0.277, Test loss: 1.158, Test accuracy: 67.86
Round  79, Global train loss: 0.277, Global test loss: 2.256, Global test accuracy: 17.60
Round  80, Train loss: 0.298, Test loss: 1.157, Test accuracy: 68.44
Round  80, Global train loss: 0.298, Global test loss: 2.181, Global test accuracy: 25.68
Round  81, Train loss: 0.193, Test loss: 1.141, Test accuracy: 68.77
Round  81, Global train loss: 0.193, Global test loss: 2.169, Global test accuracy: 21.34
Round  82, Train loss: 0.241, Test loss: 1.150, Test accuracy: 68.23
Round  82, Global train loss: 0.241, Global test loss: 2.079, Global test accuracy: 31.68
Round  83, Train loss: 0.188, Test loss: 1.202, Test accuracy: 67.61
Round  83, Global train loss: 0.188, Global test loss: 2.150, Global test accuracy: 25.81
Round  84, Train loss: 0.154, Test loss: 1.189, Test accuracy: 67.92
Round  84, Global train loss: 0.154, Global test loss: 2.057, Global test accuracy: 24.66
Round  85, Train loss: 0.211, Test loss: 1.205, Test accuracy: 67.44
Round  85, Global train loss: 0.211, Global test loss: 2.263, Global test accuracy: 13.90
Round  86, Train loss: 0.248, Test loss: 1.225, Test accuracy: 67.37
Round  86, Global train loss: 0.248, Global test loss: 2.268, Global test accuracy: 19.60
Round  87, Train loss: 0.124, Test loss: 1.240, Test accuracy: 67.88
Round  87, Global train loss: 0.124, Global test loss: 2.110, Global test accuracy: 28.23
Round  88, Train loss: 0.107, Test loss: 1.269, Test accuracy: 67.54
Round  88, Global train loss: 0.107, Global test loss: 2.122, Global test accuracy: 26.51
Round  89, Train loss: 0.137, Test loss: 1.290, Test accuracy: 67.30
Round  89, Global train loss: 0.137, Global test loss: 2.137, Global test accuracy: 25.11
Round  90, Train loss: 0.196, Test loss: 1.291, Test accuracy: 67.57
Round  90, Global train loss: 0.196, Global test loss: 2.117, Global test accuracy: 23.22
Round  91, Train loss: 0.194, Test loss: 1.282, Test accuracy: 67.90
Round  91, Global train loss: 0.194, Global test loss: 2.254, Global test accuracy: 23.02
Round  92, Train loss: 0.172, Test loss: 1.303, Test accuracy: 67.92
Round  92, Global train loss: 0.172, Global test loss: 2.085, Global test accuracy: 29.40
Round  93, Train loss: 0.269, Test loss: 1.315, Test accuracy: 68.08
Round  93, Global train loss: 0.269, Global test loss: 2.174, Global test accuracy: 15.77
Round  94, Train loss: 0.202, Test loss: 1.322, Test accuracy: 67.67
Round  94, Global train loss: 0.202, Global test loss: 2.165, Global test accuracy: 19.30
Round  95, Train loss: 0.115, Test loss: 1.355, Test accuracy: 66.88
Round  95, Global train loss: 0.115, Global test loss: 2.206, Global test accuracy: 17.44
Round  96, Train loss: 0.196, Test loss: 1.351, Test accuracy: 68.07
Round  96, Global train loss: 0.196, Global test loss: 2.153, Global test accuracy: 26.52
Round  97, Train loss: 0.123, Test loss: 1.354, Test accuracy: 67.94
Round  97, Global train loss: 0.123, Global test loss: 2.109, Global test accuracy: 23.77
Round  98, Train loss: 0.127, Test loss: 1.400, Test accuracy: 67.83
Round  98, Global train loss: 0.127, Global test loss: 2.062, Global test accuracy: 28.76
Round  99, Train loss: 0.120, Test loss: 1.376, Test accuracy: 68.07
Round  99, Global train loss: 0.120, Global test loss: 2.157, Global test accuracy: 21.19
Final Round, Train loss: 0.145, Test loss: 1.470, Test accuracy: 67.49
Final Round, Global train loss: 0.145, Global test loss: 2.157, Global test accuracy: 21.19
Average accuracy final 10 rounds: 67.79166666666667 

Average global accuracy final 10 rounds: 22.83833333333333 

1855.9064004421234
[1.9309706687927246, 3.861941337585449, 5.292006254196167, 6.722071170806885, 8.174015760421753, 9.625960350036621, 11.024303913116455, 12.422647476196289, 13.825996398925781, 15.229345321655273, 16.613003492355347, 17.99666166305542, 19.381219387054443, 20.765777111053467, 22.140694618225098, 23.51561212539673, 24.88822603225708, 26.26083993911743, 27.64182448387146, 29.02280902862549, 30.416864156723022, 31.810919284820557, 33.20311665534973, 34.595314025878906, 36.02007269859314, 37.44483137130737, 38.86654281616211, 40.288254261016846, 41.72973370552063, 43.171213150024414, 44.64560294151306, 46.11999273300171, 47.51732015609741, 48.914647579193115, 50.32183885574341, 51.7290301322937, 53.10308361053467, 54.477137088775635, 55.864280462265015, 57.251423835754395, 58.67277479171753, 60.094125747680664, 61.51198744773865, 62.92984914779663, 64.3114242553711, 65.69299936294556, 67.0656418800354, 68.43828439712524, 69.83144521713257, 71.22460603713989, 72.60387349128723, 73.98314094543457, 75.37871861457825, 76.77429628372192, 78.15875792503357, 79.54321956634521, 80.93705725669861, 82.330894947052, 83.7158215045929, 85.10074806213379, 86.4752893447876, 87.8498306274414, 89.24440932273865, 90.63898801803589, 92.01109600067139, 93.38320398330688, 94.81590127944946, 96.24859857559204, 97.6551456451416, 99.06169271469116, 100.45959234237671, 101.85749197006226, 103.244469165802, 104.63144636154175, 106.11690354347229, 107.60236072540283, 108.99816012382507, 110.39395952224731, 111.82160019874573, 113.24924087524414, 114.64355039596558, 116.03785991668701, 117.41165590286255, 118.78545188903809, 120.176584482193, 121.5677170753479, 122.9574134349823, 124.3471097946167, 125.75637221336365, 127.1656346321106, 128.55344009399414, 129.94124555587769, 131.32309293746948, 132.70494031906128, 134.09314560890198, 135.48135089874268, 136.8504354953766, 138.2195200920105, 139.61797451972961, 141.01642894744873, 142.40720510482788, 143.79798126220703, 145.1804747581482, 146.56296825408936, 147.9601607322693, 149.35735321044922, 150.81120610237122, 152.2650589942932, 153.67382049560547, 155.08258199691772, 156.4860942363739, 157.88960647583008, 159.332852602005, 160.77609872817993, 162.17876386642456, 163.5814290046692, 164.99158358573914, 166.40173816680908, 167.78707218170166, 169.17240619659424, 170.5484004020691, 171.92439460754395, 173.32368731498718, 174.72298002243042, 176.1089379787445, 177.4948959350586, 178.9214162826538, 180.34793663024902, 181.7598171234131, 183.17169761657715, 184.63530659675598, 186.09891557693481, 187.48258328437805, 188.8662509918213, 190.23457646369934, 191.6029019355774, 192.98466515541077, 194.36642837524414, 195.76324701309204, 197.16006565093994, 198.57896494865417, 199.9978642463684, 201.4097752571106, 202.82168626785278, 204.23589730262756, 205.65010833740234, 207.05630111694336, 208.46249389648438, 209.86179542541504, 211.2610969543457, 212.67491936683655, 214.0887417793274, 215.46201181411743, 216.83528184890747, 218.2286570072174, 219.62203216552734, 221.03262066841125, 222.44320917129517, 223.9240529537201, 225.40489673614502, 226.82197403907776, 228.2390513420105, 229.64811992645264, 231.05718851089478, 232.44470953941345, 233.83223056793213, 235.21590375900269, 236.59957695007324, 237.97308230400085, 239.34658765792847, 240.74950337409973, 242.152419090271, 243.54045605659485, 244.9284930229187, 246.30848360061646, 247.6884741783142, 249.09353828430176, 250.4986023902893, 251.90150713920593, 253.30441188812256, 254.70569372177124, 256.1069755554199, 257.51310443878174, 258.91923332214355, 260.30892848968506, 261.69862365722656, 263.08984184265137, 264.4810600280762, 265.8871865272522, 267.2933130264282, 268.6752200126648, 270.05712699890137, 271.4495527744293, 272.8419785499573, 274.2175862789154, 275.59319400787354, 276.9677309989929, 278.3422679901123, 279.736031293869, 281.12979459762573, 283.48400235176086, 285.838210105896]
[25.766666666666666, 25.766666666666666, 38.1, 38.1, 40.93333333333333, 40.93333333333333, 53.5, 53.5, 54.25833333333333, 54.25833333333333, 55.05833333333333, 55.05833333333333, 55.516666666666666, 55.516666666666666, 58.416666666666664, 58.416666666666664, 61.833333333333336, 61.833333333333336, 61.25, 61.25, 64.225, 64.225, 63.75833333333333, 63.75833333333333, 65.33333333333333, 65.33333333333333, 68.3, 68.3, 67.65, 67.65, 68.65833333333333, 68.65833333333333, 68.74166666666666, 68.74166666666666, 69.30833333333334, 69.30833333333334, 68.99166666666666, 68.99166666666666, 70.15833333333333, 70.15833333333333, 70.11666666666666, 70.11666666666666, 70.125, 70.125, 70.33333333333333, 70.33333333333333, 70.41666666666667, 70.41666666666667, 70.15833333333333, 70.15833333333333, 71.34166666666667, 71.34166666666667, 70.68333333333334, 70.68333333333334, 71.13333333333334, 71.13333333333334, 70.93333333333334, 70.93333333333334, 71.04166666666667, 71.04166666666667, 71.15, 71.15, 71.1, 71.1, 70.61666666666666, 70.61666666666666, 71.16666666666667, 71.16666666666667, 71.18333333333334, 71.18333333333334, 70.6, 70.6, 68.83333333333333, 68.83333333333333, 69.975, 69.975, 69.30833333333334, 69.30833333333334, 69.1, 69.1, 68.85833333333333, 68.85833333333333, 68.40833333333333, 68.40833333333333, 68.28333333333333, 68.28333333333333, 69.21666666666667, 69.21666666666667, 69.93333333333334, 69.93333333333334, 70.16666666666667, 70.16666666666667, 69.40833333333333, 69.40833333333333, 68.94166666666666, 68.94166666666666, 68.76666666666667, 68.76666666666667, 68.76666666666667, 68.76666666666667, 69.075, 69.075, 69.875, 69.875, 69.45833333333333, 69.45833333333333, 68.74166666666666, 68.74166666666666, 68.80833333333334, 68.80833333333334, 69.43333333333334, 69.43333333333334, 68.55833333333334, 68.55833333333334, 68.00833333333334, 68.00833333333334, 67.46666666666667, 67.46666666666667, 68.33333333333333, 68.33333333333333, 68.66666666666667, 68.66666666666667, 68.3, 68.3, 68.30833333333334, 68.30833333333334, 68.31666666666666, 68.31666666666666, 67.8, 67.8, 67.46666666666667, 67.46666666666667, 67.79166666666667, 67.79166666666667, 67.79166666666667, 67.79166666666667, 67.275, 67.275, 67.51666666666667, 67.51666666666667, 67.28333333333333, 67.28333333333333, 67.51666666666667, 67.51666666666667, 67.54166666666667, 67.54166666666667, 67.625, 67.625, 67.80833333333334, 67.80833333333334, 67.25833333333334, 67.25833333333334, 67.26666666666667, 67.26666666666667, 67.38333333333334, 67.38333333333334, 67.61666666666666, 67.61666666666666, 67.85833333333333, 67.85833333333333, 68.44166666666666, 68.44166666666666, 68.76666666666667, 68.76666666666667, 68.23333333333333, 68.23333333333333, 67.60833333333333, 67.60833333333333, 67.925, 67.925, 67.44166666666666, 67.44166666666666, 67.36666666666666, 67.36666666666666, 67.875, 67.875, 67.54166666666667, 67.54166666666667, 67.3, 67.3, 67.56666666666666, 67.56666666666666, 67.9, 67.9, 67.925, 67.925, 68.075, 68.075, 67.66666666666667, 67.66666666666667, 66.875, 66.875, 68.06666666666666, 68.06666666666666, 67.94166666666666, 67.94166666666666, 67.83333333333333, 67.83333333333333, 68.06666666666666, 68.06666666666666, 67.49166666666666, 67.49166666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 13, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.181, Test loss: 1.855, Test accuracy: 29.32
Round   0, Global train loss: 1.181, Global test loss: 2.207, Global test accuracy: 20.64
Round   1, Train loss: 0.982, Test loss: 1.608, Test accuracy: 41.76
Round   1, Global train loss: 0.982, Global test loss: 2.145, Global test accuracy: 29.49
Round   2, Train loss: 0.795, Test loss: 1.688, Test accuracy: 43.53
Round   2, Global train loss: 0.795, Global test loss: 2.373, Global test accuracy: 28.86
Round   3, Train loss: 0.885, Test loss: 1.225, Test accuracy: 55.38
Round   3, Global train loss: 0.885, Global test loss: 2.046, Global test accuracy: 31.32
Round   4, Train loss: 0.799, Test loss: 1.181, Test accuracy: 57.91
Round   4, Global train loss: 0.799, Global test loss: 1.969, Global test accuracy: 31.67
Round   5, Train loss: 0.791, Test loss: 1.078, Test accuracy: 61.35
Round   5, Global train loss: 0.791, Global test loss: 1.843, Global test accuracy: 37.29
Round   6, Train loss: 0.723, Test loss: 1.088, Test accuracy: 60.50
Round   6, Global train loss: 0.723, Global test loss: 1.962, Global test accuracy: 36.29
Round   7, Train loss: 0.804, Test loss: 0.951, Test accuracy: 63.27
Round   7, Global train loss: 0.804, Global test loss: 1.912, Global test accuracy: 36.95
Round   8, Train loss: 0.730, Test loss: 0.896, Test accuracy: 67.09
Round   8, Global train loss: 0.730, Global test loss: 1.970, Global test accuracy: 36.40
Round   9, Train loss: 0.744, Test loss: 0.821, Test accuracy: 67.77
Round   9, Global train loss: 0.744, Global test loss: 1.702, Global test accuracy: 40.62
Round  10, Train loss: 0.873, Test loss: 0.755, Test accuracy: 70.00
Round  10, Global train loss: 0.873, Global test loss: 1.788, Global test accuracy: 33.80
Round  11, Train loss: 0.654, Test loss: 0.743, Test accuracy: 70.65
Round  11, Global train loss: 0.654, Global test loss: 1.755, Global test accuracy: 39.24
Round  12, Train loss: 0.766, Test loss: 0.732, Test accuracy: 71.15
Round  12, Global train loss: 0.766, Global test loss: 1.496, Global test accuracy: 49.27
Round  13, Train loss: 0.628, Test loss: 0.653, Test accuracy: 73.80
Round  13, Global train loss: 0.628, Global test loss: 1.545, Global test accuracy: 47.03
Round  14, Train loss: 0.637, Test loss: 0.643, Test accuracy: 74.22
Round  14, Global train loss: 0.637, Global test loss: 1.464, Global test accuracy: 50.44
Round  15, Train loss: 0.605, Test loss: 0.657, Test accuracy: 73.63
Round  15, Global train loss: 0.605, Global test loss: 1.387, Global test accuracy: 52.21
Round  16, Train loss: 0.639, Test loss: 0.629, Test accuracy: 74.68
Round  16, Global train loss: 0.639, Global test loss: 1.339, Global test accuracy: 54.50
Round  17, Train loss: 0.688, Test loss: 0.596, Test accuracy: 75.97
Round  17, Global train loss: 0.688, Global test loss: 1.497, Global test accuracy: 46.02
Round  18, Train loss: 0.689, Test loss: 0.594, Test accuracy: 76.12
Round  18, Global train loss: 0.689, Global test loss: 1.577, Global test accuracy: 43.10
Round  19, Train loss: 0.682, Test loss: 0.590, Test accuracy: 76.26
Round  19, Global train loss: 0.682, Global test loss: 1.429, Global test accuracy: 49.38
Round  20, Train loss: 0.507, Test loss: 0.586, Test accuracy: 76.62
Round  20, Global train loss: 0.507, Global test loss: 1.526, Global test accuracy: 47.41
Round  21, Train loss: 0.539, Test loss: 0.578, Test accuracy: 77.10
Round  21, Global train loss: 0.539, Global test loss: 1.297, Global test accuracy: 52.82
Round  22, Train loss: 0.627, Test loss: 0.579, Test accuracy: 77.37
Round  22, Global train loss: 0.627, Global test loss: 1.312, Global test accuracy: 53.47
Round  23, Train loss: 0.638, Test loss: 0.567, Test accuracy: 78.04
Round  23, Global train loss: 0.638, Global test loss: 1.336, Global test accuracy: 55.16
Round  24, Train loss: 0.558, Test loss: 0.566, Test accuracy: 78.43
Round  24, Global train loss: 0.558, Global test loss: 1.306, Global test accuracy: 55.96
Round  25, Train loss: 0.620, Test loss: 0.570, Test accuracy: 78.49
Round  25, Global train loss: 0.620, Global test loss: 1.345, Global test accuracy: 55.41
Round  26, Train loss: 0.522, Test loss: 0.554, Test accuracy: 78.95
Round  26, Global train loss: 0.522, Global test loss: 1.674, Global test accuracy: 45.83
Round  27, Train loss: 0.539, Test loss: 0.560, Test accuracy: 78.62
Round  27, Global train loss: 0.539, Global test loss: 1.351, Global test accuracy: 55.25
Round  28, Train loss: 0.550, Test loss: 0.568, Test accuracy: 78.07
Round  28, Global train loss: 0.550, Global test loss: 1.377, Global test accuracy: 52.58
Round  29, Train loss: 0.622, Test loss: 0.566, Test accuracy: 78.36
Round  29, Global train loss: 0.622, Global test loss: 1.607, Global test accuracy: 45.49
Round  30, Train loss: 0.450, Test loss: 0.563, Test accuracy: 78.66
Round  30, Global train loss: 0.450, Global test loss: 1.550, Global test accuracy: 50.68
Round  31, Train loss: 0.573, Test loss: 0.548, Test accuracy: 78.87
Round  31, Global train loss: 0.573, Global test loss: 1.391, Global test accuracy: 55.00
Round  32, Train loss: 0.511, Test loss: 0.547, Test accuracy: 79.25
Round  32, Global train loss: 0.511, Global test loss: 1.418, Global test accuracy: 54.58
Round  33, Train loss: 0.530, Test loss: 0.540, Test accuracy: 79.72
Round  33, Global train loss: 0.530, Global test loss: 1.578, Global test accuracy: 46.21
Round  34, Train loss: 0.427, Test loss: 0.532, Test accuracy: 80.01
Round  34, Global train loss: 0.427, Global test loss: 1.346, Global test accuracy: 54.75
Round  35, Train loss: 0.546, Test loss: 0.528, Test accuracy: 80.17
Round  35, Global train loss: 0.546, Global test loss: 1.345, Global test accuracy: 54.11
Round  36, Train loss: 0.407, Test loss: 0.530, Test accuracy: 80.00
Round  36, Global train loss: 0.407, Global test loss: 1.217, Global test accuracy: 60.41
Round  37, Train loss: 0.339, Test loss: 0.541, Test accuracy: 79.68
Round  37, Global train loss: 0.339, Global test loss: 1.556, Global test accuracy: 52.66
Round  38, Train loss: 0.512, Test loss: 0.547, Test accuracy: 79.34
Round  38, Global train loss: 0.512, Global test loss: 1.246, Global test accuracy: 58.72
Round  39, Train loss: 0.517, Test loss: 0.547, Test accuracy: 79.31
Round  39, Global train loss: 0.517, Global test loss: 1.225, Global test accuracy: 56.35
Round  40, Train loss: 0.490, Test loss: 0.546, Test accuracy: 79.82
Round  40, Global train loss: 0.490, Global test loss: 1.240, Global test accuracy: 58.48
Round  41, Train loss: 0.545, Test loss: 0.529, Test accuracy: 80.02
Round  41, Global train loss: 0.545, Global test loss: 1.204, Global test accuracy: 59.31
Round  42, Train loss: 0.436, Test loss: 0.531, Test accuracy: 80.07
Round  42, Global train loss: 0.436, Global test loss: 1.626, Global test accuracy: 46.86
Round  43, Train loss: 0.346, Test loss: 0.519, Test accuracy: 80.39
Round  43, Global train loss: 0.346, Global test loss: 1.236, Global test accuracy: 58.41
Round  44, Train loss: 0.455, Test loss: 0.530, Test accuracy: 80.24
Round  44, Global train loss: 0.455, Global test loss: 1.266, Global test accuracy: 56.42
Round  45, Train loss: 0.322, Test loss: 0.530, Test accuracy: 80.50
Round  45, Global train loss: 0.322, Global test loss: 1.557, Global test accuracy: 53.07
Round  46, Train loss: 0.395, Test loss: 0.517, Test accuracy: 80.86
Round  46, Global train loss: 0.395, Global test loss: 1.360, Global test accuracy: 58.23
Round  47, Train loss: 0.477, Test loss: 0.516, Test accuracy: 80.90
Round  47, Global train loss: 0.477, Global test loss: 1.377, Global test accuracy: 53.99
Round  48, Train loss: 0.411, Test loss: 0.520, Test accuracy: 80.73
Round  48, Global train loss: 0.411, Global test loss: 1.286, Global test accuracy: 58.71
Round  49, Train loss: 0.429, Test loss: 0.513, Test accuracy: 81.03
Round  49, Global train loss: 0.429, Global test loss: 1.225, Global test accuracy: 61.34
Round  50, Train loss: 0.394, Test loss: 0.505, Test accuracy: 81.30
Round  50, Global train loss: 0.394, Global test loss: 1.100, Global test accuracy: 62.59
Round  51, Train loss: 0.376, Test loss: 0.534, Test accuracy: 81.09
Round  51, Global train loss: 0.376, Global test loss: 1.240, Global test accuracy: 59.64
Round  52, Train loss: 0.355, Test loss: 0.520, Test accuracy: 81.23
Round  52, Global train loss: 0.355, Global test loss: 1.151, Global test accuracy: 62.24
Round  53, Train loss: 0.426, Test loss: 0.521, Test accuracy: 81.08
Round  53, Global train loss: 0.426, Global test loss: 1.130, Global test accuracy: 62.66
Round  54, Train loss: 0.412, Test loss: 0.530, Test accuracy: 80.97
Round  54, Global train loss: 0.412, Global test loss: 1.048, Global test accuracy: 64.47
Round  55, Train loss: 0.437, Test loss: 0.521, Test accuracy: 81.13
Round  55, Global train loss: 0.437, Global test loss: 1.184, Global test accuracy: 60.02
Round  56, Train loss: 0.446, Test loss: 0.532, Test accuracy: 80.72
Round  56, Global train loss: 0.446, Global test loss: 1.266, Global test accuracy: 59.40
Round  57, Train loss: 0.441, Test loss: 0.527, Test accuracy: 80.92
Round  57, Global train loss: 0.441, Global test loss: 1.281, Global test accuracy: 57.82
Round  58, Train loss: 0.401, Test loss: 0.519, Test accuracy: 81.38
Round  58, Global train loss: 0.401, Global test loss: 1.162, Global test accuracy: 61.35
Round  59, Train loss: 0.455, Test loss: 0.521, Test accuracy: 81.24
Round  59, Global train loss: 0.455, Global test loss: 1.429, Global test accuracy: 53.60
Round  60, Train loss: 0.301, Test loss: 0.538, Test accuracy: 80.78
Round  60, Global train loss: 0.301, Global test loss: 1.285, Global test accuracy: 59.89
Round  61, Train loss: 0.417, Test loss: 0.543, Test accuracy: 80.77
Round  61, Global train loss: 0.417, Global test loss: 1.150, Global test accuracy: 61.34
Round  62, Train loss: 0.282, Test loss: 0.534, Test accuracy: 81.16
Round  62, Global train loss: 0.282, Global test loss: 1.233, Global test accuracy: 61.25
Round  63, Train loss: 0.389, Test loss: 0.548, Test accuracy: 80.98
Round  63, Global train loss: 0.389, Global test loss: 1.160, Global test accuracy: 62.09
Round  64, Train loss: 0.325, Test loss: 0.554, Test accuracy: 80.71
Round  64, Global train loss: 0.325, Global test loss: 1.419, Global test accuracy: 55.71
Round  65, Train loss: 0.273, Test loss: 0.533, Test accuracy: 81.24
Round  65, Global train loss: 0.273, Global test loss: 1.280, Global test accuracy: 62.35
Round  66, Train loss: 0.361, Test loss: 0.522, Test accuracy: 81.58
Round  66, Global train loss: 0.361, Global test loss: 1.195, Global test accuracy: 62.27
Round  67, Train loss: 0.430, Test loss: 0.529, Test accuracy: 81.28
Round  67, Global train loss: 0.430, Global test loss: 1.383, Global test accuracy: 54.55
Round  68, Train loss: 0.285, Test loss: 0.529, Test accuracy: 81.28
Round  68, Global train loss: 0.285, Global test loss: 1.319, Global test accuracy: 60.76
Round  69, Train loss: 0.365, Test loss: 0.524, Test accuracy: 81.94
Round  69, Global train loss: 0.365, Global test loss: 1.024, Global test accuracy: 65.32
Round  70, Train loss: 0.366, Test loss: 0.508, Test accuracy: 82.21
Round  70, Global train loss: 0.366, Global test loss: 1.420, Global test accuracy: 53.06
Round  71, Train loss: 0.312, Test loss: 0.510, Test accuracy: 82.23
Round  71, Global train loss: 0.312, Global test loss: 1.162, Global test accuracy: 61.89
Round  72, Train loss: 0.296, Test loss: 0.496, Test accuracy: 82.72
Round  72, Global train loss: 0.296, Global test loss: 1.335, Global test accuracy: 59.23
Round  73, Train loss: 0.323, Test loss: 0.498, Test accuracy: 82.65
Round  73, Global train loss: 0.323, Global test loss: 1.157, Global test accuracy: 62.99
Round  74, Train loss: 0.354, Test loss: 0.492, Test accuracy: 83.20
Round  74, Global train loss: 0.354, Global test loss: 1.248, Global test accuracy: 63.47
Round  75, Train loss: 0.313, Test loss: 0.503, Test accuracy: 83.29
Round  75, Global train loss: 0.313, Global test loss: 1.219, Global test accuracy: 63.72
Round  76, Train loss: 0.360, Test loss: 0.502, Test accuracy: 83.31
Round  76, Global train loss: 0.360, Global test loss: 1.277, Global test accuracy: 60.88
Round  77, Train loss: 0.304, Test loss: 0.521, Test accuracy: 82.77
Round  77, Global train loss: 0.304, Global test loss: 1.299, Global test accuracy: 63.08
Round  78, Train loss: 0.220, Test loss: 0.510, Test accuracy: 83.15
Round  78, Global train loss: 0.220, Global test loss: 1.634, Global test accuracy: 56.47
Round  79, Train loss: 0.397, Test loss: 0.502, Test accuracy: 83.04
Round  79, Global train loss: 0.397, Global test loss: 1.502, Global test accuracy: 56.08
Round  80, Train loss: 0.290, Test loss: 0.526, Test accuracy: 82.12
Round  80, Global train loss: 0.290, Global test loss: 1.285, Global test accuracy: 60.65
Round  81, Train loss: 0.329, Test loss: 0.530, Test accuracy: 81.94
Round  81, Global train loss: 0.329, Global test loss: 1.110, Global test accuracy: 64.03
Round  82, Train loss: 0.325, Test loss: 0.518, Test accuracy: 82.54
Round  82, Global train loss: 0.325, Global test loss: 1.150, Global test accuracy: 62.83
Round  83, Train loss: 0.316, Test loss: 0.536, Test accuracy: 82.39
Round  83, Global train loss: 0.316, Global test loss: 1.278, Global test accuracy: 62.75
Round  84, Train loss: 0.297, Test loss: 0.553, Test accuracy: 81.98
Round  84, Global train loss: 0.297, Global test loss: 1.148, Global test accuracy: 63.89
Round  85, Train loss: 0.359, Test loss: 0.538, Test accuracy: 82.65
Round  85, Global train loss: 0.359, Global test loss: 1.271, Global test accuracy: 59.52
Round  86, Train loss: 0.357, Test loss: 0.567, Test accuracy: 82.44
Round  86, Global train loss: 0.357, Global test loss: 1.524, Global test accuracy: 55.42
Round  87, Train loss: 0.218, Test loss: 0.559, Test accuracy: 82.47
Round  87, Global train loss: 0.218, Global test loss: 1.578, Global test accuracy: 57.48
Round  88, Train loss: 0.222, Test loss: 0.557, Test accuracy: 82.62
Round  88, Global train loss: 0.222, Global test loss: 1.098, Global test accuracy: 66.20
Round  89, Train loss: 0.289, Test loss: 0.564, Test accuracy: 82.11
Round  89, Global train loss: 0.289, Global test loss: 1.258, Global test accuracy: 62.62
Round  90, Train loss: 0.274, Test loss: 0.535, Test accuracy: 82.69
Round  90, Global train loss: 0.274, Global test loss: 1.130, Global test accuracy: 63.97
Round  91, Train loss: 0.247, Test loss: 0.553, Test accuracy: 82.11
Round  91, Global train loss: 0.247, Global test loss: 1.520, Global test accuracy: 57.41
Round  92, Train loss: 0.263, Test loss: 0.554, Test accuracy: 81.85
Round  92, Global train loss: 0.263, Global test loss: 1.266, Global test accuracy: 63.34
Round  93, Train loss: 0.344, Test loss: 0.563, Test accuracy: 81.74
Round  93, Global train loss: 0.344, Global test loss: 1.167, Global test accuracy: 63.76
Round  94, Train loss: 0.316, Test loss: 0.547, Test accuracy: 82.57
Round  94, Global train loss: 0.316, Global test loss: 1.141, Global test accuracy: 63.44
Round  95, Train loss: 0.299, Test loss: 0.551, Test accuracy: 82.60
Round  95, Global train loss: 0.299, Global test loss: 1.378, Global test accuracy: 58.02
Round  96, Train loss: 0.251, Test loss: 0.546, Test accuracy: 82.64
Round  96, Global train loss: 0.251, Global test loss: 1.199, Global test accuracy: 64.03
Round  97, Train loss: 0.267, Test loss: 0.529, Test accuracy: 83.10
Round  97, Global train loss: 0.267, Global test loss: 1.182, Global test accuracy: 64.56
Round  98, Train loss: 0.234, Test loss: 0.543, Test accuracy: 82.72
Round  98, Global train loss: 0.234, Global test loss: 1.365, Global test accuracy: 60.96
Round  99, Train loss: 0.291, Test loss: 0.541, Test accuracy: 82.98
Round  99, Global train loss: 0.291, Global test loss: 1.336, Global test accuracy: 61.40
Final Round, Train loss: 0.216, Test loss: 0.593, Test accuracy: 82.67
Final Round, Global train loss: 0.216, Global test loss: 1.336, Global test accuracy: 61.40
Average accuracy final 10 rounds: 82.50083333333332 

Average global accuracy final 10 rounds: 62.089166666666664 

1814.3477754592896
[1.8102357387542725, 3.620471477508545, 5.004445791244507, 6.388420104980469, 7.758052110671997, 9.127684116363525, 10.565592527389526, 12.003500938415527, 13.376502752304077, 14.749504566192627, 16.119844675064087, 17.490184783935547, 18.856398582458496, 20.222612380981445, 21.583805322647095, 22.944998264312744, 24.430843353271484, 25.916688442230225, 27.370932817459106, 28.82517719268799, 30.210233449935913, 31.595289707183838, 33.004932165145874, 34.41457462310791, 35.78073024749756, 37.14688587188721, 38.50983238220215, 39.87277889251709, 41.24113464355469, 42.609490394592285, 43.980672121047974, 45.35185384750366, 46.72522735595703, 48.0986008644104, 49.44835877418518, 50.79811668395996, 52.170170068740845, 53.54222345352173, 54.90827989578247, 56.27433633804321, 57.648974657058716, 59.02361297607422, 60.39162015914917, 61.75962734222412, 63.145381689071655, 64.53113603591919, 65.91575503349304, 67.3003740310669, 68.68100166320801, 70.06162929534912, 71.41310334205627, 72.76457738876343, 74.13139939308167, 75.4982213973999, 76.84395146369934, 78.18968152999878, 79.61282062530518, 81.03595972061157, 82.41591429710388, 83.79586887359619, 85.19169354438782, 86.58751821517944, 87.95338869094849, 89.31925916671753, 90.69708323478699, 92.07490730285645, 93.42858934402466, 94.78227138519287, 96.25795865058899, 97.73364591598511, 99.10130167007446, 100.46895742416382, 101.8414568901062, 103.21395635604858, 104.57717180252075, 105.94038724899292, 107.3273057937622, 108.7142243385315, 110.09661340713501, 111.47900247573853, 112.88212776184082, 114.28525304794312, 115.68700551986694, 117.08875799179077, 118.45265626907349, 119.8165545463562, 121.18088555335999, 122.54521656036377, 123.90779614448547, 125.27037572860718, 126.66259741783142, 128.05481910705566, 129.41508388519287, 130.77534866333008, 132.12595963478088, 133.4765706062317, 134.8487904071808, 136.22101020812988, 137.56927180290222, 138.91753339767456, 140.27514600753784, 141.63275861740112, 143.04919481277466, 144.4656310081482, 145.89510941505432, 147.32458782196045, 148.73444366455078, 150.1442995071411, 151.51541924476624, 152.88653898239136, 154.25002598762512, 155.6135129928589, 156.9726586341858, 158.3318042755127, 159.6791639328003, 161.0265235900879, 162.40467929840088, 163.78283500671387, 165.1399474143982, 166.49705982208252, 167.86535787582397, 169.23365592956543, 170.64949131011963, 172.06532669067383, 173.49316883087158, 174.92101097106934, 176.26643633842468, 177.61186170578003, 178.9784553050995, 180.34504890441895, 181.72663593292236, 183.10822296142578, 184.49104857444763, 185.87387418746948, 187.24391984939575, 188.61396551132202, 189.97608304023743, 191.33820056915283, 192.73007941246033, 194.12195825576782, 195.51259088516235, 196.90322351455688, 198.27295184135437, 199.64268016815186, 200.99903225898743, 202.355384349823, 203.7561752796173, 205.15696620941162, 206.5269799232483, 207.89699363708496, 209.2636103630066, 210.63022708892822, 211.9945559501648, 213.35888481140137, 214.72577905654907, 216.09267330169678, 217.43997764587402, 218.78728199005127, 220.1623718738556, 221.5374617576599, 222.99483847618103, 224.45221519470215, 225.89214181900024, 227.33206844329834, 228.7338318824768, 230.13559532165527, 231.48548793792725, 232.83538055419922, 234.19411182403564, 235.55284309387207, 236.91150617599487, 238.27016925811768, 239.64457511901855, 241.01898097991943, 242.4288010597229, 243.83862113952637, 245.20956897735596, 246.58051681518555, 247.9386487007141, 249.29678058624268, 250.64964270591736, 252.00250482559204, 253.34936141967773, 254.69621801376343, 256.1064488887787, 257.51667976379395, 258.8820164203644, 260.2473530769348, 261.6404538154602, 263.0335545539856, 264.43254590034485, 265.8315372467041, 267.21420669555664, 268.5968761444092, 269.95437955856323, 271.3118829727173, 272.6664433479309, 274.02100372314453, 275.3800015449524, 276.73899936676025, 279.03580713272095, 281.33261489868164]
[29.325, 29.325, 41.75833333333333, 41.75833333333333, 43.53333333333333, 43.53333333333333, 55.38333333333333, 55.38333333333333, 57.90833333333333, 57.90833333333333, 61.35, 61.35, 60.5, 60.5, 63.275, 63.275, 67.09166666666667, 67.09166666666667, 67.76666666666667, 67.76666666666667, 70.0, 70.0, 70.65, 70.65, 71.15, 71.15, 73.8, 73.8, 74.21666666666667, 74.21666666666667, 73.63333333333334, 73.63333333333334, 74.68333333333334, 74.68333333333334, 75.96666666666667, 75.96666666666667, 76.11666666666666, 76.11666666666666, 76.25833333333334, 76.25833333333334, 76.61666666666666, 76.61666666666666, 77.1, 77.1, 77.36666666666666, 77.36666666666666, 78.04166666666667, 78.04166666666667, 78.43333333333334, 78.43333333333334, 78.49166666666666, 78.49166666666666, 78.95, 78.95, 78.625, 78.625, 78.06666666666666, 78.06666666666666, 78.35833333333333, 78.35833333333333, 78.65833333333333, 78.65833333333333, 78.86666666666666, 78.86666666666666, 79.25, 79.25, 79.71666666666667, 79.71666666666667, 80.00833333333334, 80.00833333333334, 80.16666666666667, 80.16666666666667, 80.0, 80.0, 79.68333333333334, 79.68333333333334, 79.34166666666667, 79.34166666666667, 79.30833333333334, 79.30833333333334, 79.81666666666666, 79.81666666666666, 80.01666666666667, 80.01666666666667, 80.06666666666666, 80.06666666666666, 80.39166666666667, 80.39166666666667, 80.24166666666666, 80.24166666666666, 80.5, 80.5, 80.85833333333333, 80.85833333333333, 80.9, 80.9, 80.73333333333333, 80.73333333333333, 81.025, 81.025, 81.3, 81.3, 81.09166666666667, 81.09166666666667, 81.23333333333333, 81.23333333333333, 81.08333333333333, 81.08333333333333, 80.96666666666667, 80.96666666666667, 81.13333333333334, 81.13333333333334, 80.725, 80.725, 80.925, 80.925, 81.375, 81.375, 81.24166666666666, 81.24166666666666, 80.78333333333333, 80.78333333333333, 80.76666666666667, 80.76666666666667, 81.15833333333333, 81.15833333333333, 80.98333333333333, 80.98333333333333, 80.70833333333333, 80.70833333333333, 81.24166666666666, 81.24166666666666, 81.575, 81.575, 81.275, 81.275, 81.275, 81.275, 81.94166666666666, 81.94166666666666, 82.20833333333333, 82.20833333333333, 82.23333333333333, 82.23333333333333, 82.71666666666667, 82.71666666666667, 82.65, 82.65, 83.2, 83.2, 83.29166666666667, 83.29166666666667, 83.30833333333334, 83.30833333333334, 82.76666666666667, 82.76666666666667, 83.15, 83.15, 83.04166666666667, 83.04166666666667, 82.11666666666666, 82.11666666666666, 81.94166666666666, 81.94166666666666, 82.54166666666667, 82.54166666666667, 82.39166666666667, 82.39166666666667, 81.98333333333333, 81.98333333333333, 82.65, 82.65, 82.44166666666666, 82.44166666666666, 82.475, 82.475, 82.61666666666666, 82.61666666666666, 82.10833333333333, 82.10833333333333, 82.69166666666666, 82.69166666666666, 82.10833333333333, 82.10833333333333, 81.85, 81.85, 81.74166666666666, 81.74166666666666, 82.56666666666666, 82.56666666666666, 82.6, 82.6, 82.64166666666667, 82.64166666666667, 83.1, 83.1, 82.725, 82.725, 82.98333333333333, 82.98333333333333, 82.66666666666667, 82.66666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 18, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.156, Test loss: 1.858, Test accuracy: 31.05
Round   0, Global train loss: 1.156, Global test loss: 2.214, Global test accuracy: 21.52
Round   1, Train loss: 1.034, Test loss: 1.750, Test accuracy: 39.80
Round   1, Global train loss: 1.034, Global test loss: 2.293, Global test accuracy: 27.60
Round   2, Train loss: 0.824, Test loss: 1.724, Test accuracy: 42.51
Round   2, Global train loss: 0.824, Global test loss: 2.426, Global test accuracy: 27.43
Round   3, Train loss: 0.981, Test loss: 1.256, Test accuracy: 49.08
Round   3, Global train loss: 0.981, Global test loss: 2.032, Global test accuracy: 27.25
Round   4, Train loss: 0.835, Test loss: 1.236, Test accuracy: 52.85
Round   4, Global train loss: 0.835, Global test loss: 2.046, Global test accuracy: 33.18
Round   5, Train loss: 0.851, Test loss: 1.124, Test accuracy: 57.00
Round   5, Global train loss: 0.851, Global test loss: 1.837, Global test accuracy: 40.32
Round   6, Train loss: 0.798, Test loss: 1.115, Test accuracy: 56.31
Round   6, Global train loss: 0.798, Global test loss: 2.087, Global test accuracy: 30.68
Round   7, Train loss: 0.955, Test loss: 0.934, Test accuracy: 61.92
Round   7, Global train loss: 0.955, Global test loss: 1.788, Global test accuracy: 41.27
Round   8, Train loss: 0.783, Test loss: 0.920, Test accuracy: 65.65
Round   8, Global train loss: 0.783, Global test loss: 2.076, Global test accuracy: 34.37
Round   9, Train loss: 0.774, Test loss: 0.848, Test accuracy: 65.89
Round   9, Global train loss: 0.774, Global test loss: 1.798, Global test accuracy: 35.04
Round  10, Train loss: 0.933, Test loss: 0.755, Test accuracy: 68.72
Round  10, Global train loss: 0.933, Global test loss: 1.661, Global test accuracy: 40.17
Round  11, Train loss: 0.744, Test loss: 0.750, Test accuracy: 68.99
Round  11, Global train loss: 0.744, Global test loss: 1.792, Global test accuracy: 37.17
Round  12, Train loss: 0.808, Test loss: 0.735, Test accuracy: 69.96
Round  12, Global train loss: 0.808, Global test loss: 1.519, Global test accuracy: 47.04
Round  13, Train loss: 0.775, Test loss: 0.682, Test accuracy: 72.04
Round  13, Global train loss: 0.775, Global test loss: 1.553, Global test accuracy: 45.34
Round  14, Train loss: 0.757, Test loss: 0.655, Test accuracy: 73.62
Round  14, Global train loss: 0.757, Global test loss: 1.537, Global test accuracy: 45.12
Round  15, Train loss: 0.713, Test loss: 0.651, Test accuracy: 73.51
Round  15, Global train loss: 0.713, Global test loss: 1.413, Global test accuracy: 50.29
Round  16, Train loss: 0.613, Test loss: 0.653, Test accuracy: 73.92
Round  16, Global train loss: 0.613, Global test loss: 1.415, Global test accuracy: 50.74
Round  17, Train loss: 0.703, Test loss: 0.650, Test accuracy: 74.20
Round  17, Global train loss: 0.703, Global test loss: 1.512, Global test accuracy: 45.43
Round  18, Train loss: 0.763, Test loss: 0.639, Test accuracy: 74.78
Round  18, Global train loss: 0.763, Global test loss: 1.571, Global test accuracy: 40.79
Round  19, Train loss: 0.709, Test loss: 0.622, Test accuracy: 75.60
Round  19, Global train loss: 0.709, Global test loss: 1.553, Global test accuracy: 43.01
Round  20, Train loss: 0.528, Test loss: 0.618, Test accuracy: 75.88
Round  20, Global train loss: 0.528, Global test loss: 1.634, Global test accuracy: 44.43
Round  21, Train loss: 0.555, Test loss: 0.604, Test accuracy: 76.45
Round  21, Global train loss: 0.555, Global test loss: 1.405, Global test accuracy: 48.12
Round  22, Train loss: 0.650, Test loss: 0.598, Test accuracy: 76.32
Round  22, Global train loss: 0.650, Global test loss: 1.382, Global test accuracy: 50.40
Round  23, Train loss: 0.683, Test loss: 0.595, Test accuracy: 76.65
Round  23, Global train loss: 0.683, Global test loss: 1.366, Global test accuracy: 52.09
Round  24, Train loss: 0.631, Test loss: 0.589, Test accuracy: 76.77
Round  24, Global train loss: 0.631, Global test loss: 1.382, Global test accuracy: 52.17
Round  25, Train loss: 0.676, Test loss: 0.587, Test accuracy: 77.00
Round  25, Global train loss: 0.676, Global test loss: 1.377, Global test accuracy: 53.08
Round  26, Train loss: 0.497, Test loss: 0.584, Test accuracy: 77.12
Round  26, Global train loss: 0.497, Global test loss: 1.763, Global test accuracy: 44.42
Round  27, Train loss: 0.529, Test loss: 0.580, Test accuracy: 77.30
Round  27, Global train loss: 0.529, Global test loss: 1.363, Global test accuracy: 55.14
Round  28, Train loss: 0.650, Test loss: 0.588, Test accuracy: 77.06
Round  28, Global train loss: 0.650, Global test loss: 1.378, Global test accuracy: 53.68
Round  29, Train loss: 0.705, Test loss: 0.600, Test accuracy: 76.97
Round  29, Global train loss: 0.705, Global test loss: 1.449, Global test accuracy: 49.77
Round  30, Train loss: 0.389, Test loss: 0.599, Test accuracy: 77.19
Round  30, Global train loss: 0.389, Global test loss: 1.576, Global test accuracy: 50.41
Round  31, Train loss: 0.755, Test loss: 0.574, Test accuracy: 78.22
Round  31, Global train loss: 0.755, Global test loss: 1.392, Global test accuracy: 54.18
Round  32, Train loss: 0.578, Test loss: 0.567, Test accuracy: 78.43
Round  32, Global train loss: 0.578, Global test loss: 1.319, Global test accuracy: 54.16
Round  33, Train loss: 0.595, Test loss: 0.560, Test accuracy: 78.70
Round  33, Global train loss: 0.595, Global test loss: 1.461, Global test accuracy: 48.07
Round  34, Train loss: 0.449, Test loss: 0.556, Test accuracy: 78.91
Round  34, Global train loss: 0.449, Global test loss: 1.327, Global test accuracy: 53.89
Round  35, Train loss: 0.628, Test loss: 0.561, Test accuracy: 79.06
Round  35, Global train loss: 0.628, Global test loss: 1.376, Global test accuracy: 52.25
Round  36, Train loss: 0.560, Test loss: 0.559, Test accuracy: 79.40
Round  36, Global train loss: 0.560, Global test loss: 1.213, Global test accuracy: 58.73
Round  37, Train loss: 0.496, Test loss: 0.555, Test accuracy: 79.45
Round  37, Global train loss: 0.496, Global test loss: 1.422, Global test accuracy: 52.87
Round  38, Train loss: 0.512, Test loss: 0.562, Test accuracy: 79.21
Round  38, Global train loss: 0.512, Global test loss: 1.330, Global test accuracy: 54.88
Round  39, Train loss: 0.583, Test loss: 0.554, Test accuracy: 79.47
Round  39, Global train loss: 0.583, Global test loss: 1.282, Global test accuracy: 55.19
Round  40, Train loss: 0.632, Test loss: 0.561, Test accuracy: 78.94
Round  40, Global train loss: 0.632, Global test loss: 1.238, Global test accuracy: 57.19
Round  41, Train loss: 0.620, Test loss: 0.566, Test accuracy: 78.62
Round  41, Global train loss: 0.620, Global test loss: 1.178, Global test accuracy: 60.06
Round  42, Train loss: 0.621, Test loss: 0.557, Test accuracy: 79.01
Round  42, Global train loss: 0.621, Global test loss: 1.528, Global test accuracy: 46.59
Round  43, Train loss: 0.408, Test loss: 0.569, Test accuracy: 78.61
Round  43, Global train loss: 0.408, Global test loss: 1.349, Global test accuracy: 56.06
Round  44, Train loss: 0.447, Test loss: 0.556, Test accuracy: 78.93
Round  44, Global train loss: 0.447, Global test loss: 1.348, Global test accuracy: 54.08
Round  45, Train loss: 0.369, Test loss: 0.569, Test accuracy: 79.12
Round  45, Global train loss: 0.369, Global test loss: 1.652, Global test accuracy: 51.45
Round  46, Train loss: 0.501, Test loss: 0.548, Test accuracy: 79.90
Round  46, Global train loss: 0.501, Global test loss: 1.594, Global test accuracy: 52.91
Round  47, Train loss: 0.538, Test loss: 0.551, Test accuracy: 79.35
Round  47, Global train loss: 0.538, Global test loss: 1.171, Global test accuracy: 59.97
Round  48, Train loss: 0.509, Test loss: 0.546, Test accuracy: 79.84
Round  48, Global train loss: 0.509, Global test loss: 1.251, Global test accuracy: 58.96
Round  49, Train loss: 0.487, Test loss: 0.554, Test accuracy: 79.62
Round  49, Global train loss: 0.487, Global test loss: 1.226, Global test accuracy: 59.84
Round  50, Train loss: 0.448, Test loss: 0.553, Test accuracy: 80.08
Round  50, Global train loss: 0.448, Global test loss: 1.237, Global test accuracy: 59.04
Round  51, Train loss: 0.444, Test loss: 0.558, Test accuracy: 80.30
Round  51, Global train loss: 0.444, Global test loss: 1.242, Global test accuracy: 57.67
Round  52, Train loss: 0.416, Test loss: 0.547, Test accuracy: 80.46
Round  52, Global train loss: 0.416, Global test loss: 1.223, Global test accuracy: 60.63
Round  53, Train loss: 0.452, Test loss: 0.541, Test accuracy: 80.87
Round  53, Global train loss: 0.452, Global test loss: 1.119, Global test accuracy: 61.23
Round  54, Train loss: 0.522, Test loss: 0.557, Test accuracy: 80.46
Round  54, Global train loss: 0.522, Global test loss: 1.119, Global test accuracy: 62.09
Round  55, Train loss: 0.602, Test loss: 0.545, Test accuracy: 80.91
Round  55, Global train loss: 0.602, Global test loss: 1.154, Global test accuracy: 60.54
Round  56, Train loss: 0.505, Test loss: 0.557, Test accuracy: 80.07
Round  56, Global train loss: 0.505, Global test loss: 1.337, Global test accuracy: 55.63
Round  57, Train loss: 0.455, Test loss: 0.557, Test accuracy: 80.08
Round  57, Global train loss: 0.455, Global test loss: 1.338, Global test accuracy: 55.08
Round  58, Train loss: 0.456, Test loss: 0.548, Test accuracy: 80.54
Round  58, Global train loss: 0.456, Global test loss: 1.134, Global test accuracy: 61.37
Round  59, Train loss: 0.591, Test loss: 0.539, Test accuracy: 80.72
Round  59, Global train loss: 0.591, Global test loss: 1.282, Global test accuracy: 55.62
Round  60, Train loss: 0.475, Test loss: 0.546, Test accuracy: 80.26
Round  60, Global train loss: 0.475, Global test loss: 1.303, Global test accuracy: 56.56
Round  61, Train loss: 0.512, Test loss: 0.555, Test accuracy: 80.16
Round  61, Global train loss: 0.512, Global test loss: 1.095, Global test accuracy: 62.59
Round  62, Train loss: 0.336, Test loss: 0.534, Test accuracy: 80.62
Round  62, Global train loss: 0.336, Global test loss: 1.224, Global test accuracy: 59.58
Round  63, Train loss: 0.451, Test loss: 0.537, Test accuracy: 80.61
Round  63, Global train loss: 0.451, Global test loss: 1.213, Global test accuracy: 58.58
Round  64, Train loss: 0.322, Test loss: 0.536, Test accuracy: 80.57
Round  64, Global train loss: 0.322, Global test loss: 1.296, Global test accuracy: 58.98
Round  65, Train loss: 0.325, Test loss: 0.520, Test accuracy: 81.09
Round  65, Global train loss: 0.325, Global test loss: 1.320, Global test accuracy: 58.85
Round  66, Train loss: 0.471, Test loss: 0.532, Test accuracy: 80.78
Round  66, Global train loss: 0.471, Global test loss: 1.133, Global test accuracy: 62.84
Round  67, Train loss: 0.450, Test loss: 0.534, Test accuracy: 80.88
Round  67, Global train loss: 0.450, Global test loss: 1.336, Global test accuracy: 54.92
Round  68, Train loss: 0.409, Test loss: 0.544, Test accuracy: 80.64
Round  68, Global train loss: 0.409, Global test loss: 1.245, Global test accuracy: 61.27
Round  69, Train loss: 0.396, Test loss: 0.547, Test accuracy: 80.62
Round  69, Global train loss: 0.396, Global test loss: 1.100, Global test accuracy: 62.83
Round  70, Train loss: 0.436, Test loss: 0.546, Test accuracy: 80.69
Round  70, Global train loss: 0.436, Global test loss: 1.330, Global test accuracy: 54.98
Round  71, Train loss: 0.407, Test loss: 0.540, Test accuracy: 80.65
Round  71, Global train loss: 0.407, Global test loss: 1.194, Global test accuracy: 59.89
Round  72, Train loss: 0.425, Test loss: 0.529, Test accuracy: 80.93
Round  72, Global train loss: 0.425, Global test loss: 1.249, Global test accuracy: 58.91
Round  73, Train loss: 0.335, Test loss: 0.541, Test accuracy: 80.64
Round  73, Global train loss: 0.335, Global test loss: 1.194, Global test accuracy: 61.65
Round  74, Train loss: 0.375, Test loss: 0.529, Test accuracy: 81.10
Round  74, Global train loss: 0.375, Global test loss: 1.184, Global test accuracy: 62.13
Round  75, Train loss: 0.345, Test loss: 0.529, Test accuracy: 81.13
Round  75, Global train loss: 0.345, Global test loss: 1.246, Global test accuracy: 61.36
Round  76, Train loss: 0.423, Test loss: 0.537, Test accuracy: 81.17
Round  76, Global train loss: 0.423, Global test loss: 1.265, Global test accuracy: 60.44
Round  77, Train loss: 0.366, Test loss: 0.528, Test accuracy: 81.47
Round  77, Global train loss: 0.366, Global test loss: 1.311, Global test accuracy: 59.62
Round  78, Train loss: 0.270, Test loss: 0.518, Test accuracy: 81.78
Round  78, Global train loss: 0.270, Global test loss: 1.760, Global test accuracy: 53.27
Round  79, Train loss: 0.478, Test loss: 0.523, Test accuracy: 81.50
Round  79, Global train loss: 0.478, Global test loss: 1.347, Global test accuracy: 56.90
Round  80, Train loss: 0.377, Test loss: 0.508, Test accuracy: 81.97
Round  80, Global train loss: 0.377, Global test loss: 1.353, Global test accuracy: 57.55
Round  81, Train loss: 0.372, Test loss: 0.516, Test accuracy: 81.93
Round  81, Global train loss: 0.372, Global test loss: 1.225, Global test accuracy: 60.23
Round  82, Train loss: 0.479, Test loss: 0.535, Test accuracy: 81.33
Round  82, Global train loss: 0.479, Global test loss: 1.176, Global test accuracy: 61.27
Round  83, Train loss: 0.370, Test loss: 0.521, Test accuracy: 81.83
Round  83, Global train loss: 0.370, Global test loss: 1.317, Global test accuracy: 59.40
Round  84, Train loss: 0.395, Test loss: 0.517, Test accuracy: 82.01
Round  84, Global train loss: 0.395, Global test loss: 1.107, Global test accuracy: 63.36
Round  85, Train loss: 0.463, Test loss: 0.543, Test accuracy: 81.45
Round  85, Global train loss: 0.463, Global test loss: 1.197, Global test accuracy: 60.46
Round  86, Train loss: 0.456, Test loss: 0.525, Test accuracy: 81.66
Round  86, Global train loss: 0.456, Global test loss: 1.379, Global test accuracy: 55.54
Round  87, Train loss: 0.333, Test loss: 0.534, Test accuracy: 81.35
Round  87, Global train loss: 0.333, Global test loss: 1.389, Global test accuracy: 57.95
Round  88, Train loss: 0.307, Test loss: 0.536, Test accuracy: 81.18
Round  88, Global train loss: 0.307, Global test loss: 1.153, Global test accuracy: 63.24
Round  89, Train loss: 0.324, Test loss: 0.551, Test accuracy: 81.09
Round  89, Global train loss: 0.324, Global test loss: 1.361, Global test accuracy: 59.37
Round  90, Train loss: 0.381, Test loss: 0.535, Test accuracy: 81.80
Round  90, Global train loss: 0.381, Global test loss: 1.207, Global test accuracy: 61.13
Round  91, Train loss: 0.322, Test loss: 0.534, Test accuracy: 81.67
Round  91, Global train loss: 0.322, Global test loss: 1.486, Global test accuracy: 55.27
Round  92, Train loss: 0.382, Test loss: 0.540, Test accuracy: 81.66
Round  92, Global train loss: 0.382, Global test loss: 1.279, Global test accuracy: 60.94
Round  93, Train loss: 0.411, Test loss: 0.554, Test accuracy: 81.49
Round  93, Global train loss: 0.411, Global test loss: 1.182, Global test accuracy: 62.31/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

Round  94, Train loss: 0.410, Test loss: 0.545, Test accuracy: 81.75
Round  94, Global train loss: 0.410, Global test loss: 1.200, Global test accuracy: 60.79
Round  95, Train loss: 0.418, Test loss: 0.556, Test accuracy: 81.30
Round  95, Global train loss: 0.418, Global test loss: 1.208, Global test accuracy: 60.09
Round  96, Train loss: 0.297, Test loss: 0.556, Test accuracy: 81.28
Round  96, Global train loss: 0.297, Global test loss: 1.139, Global test accuracy: 63.66
Round  97, Train loss: 0.324, Test loss: 0.550, Test accuracy: 81.28
Round  97, Global train loss: 0.324, Global test loss: 1.188, Global test accuracy: 62.77
Round  98, Train loss: 0.376, Test loss: 0.538, Test accuracy: 81.91
Round  98, Global train loss: 0.376, Global test loss: 1.211, Global test accuracy: 62.87
Round  99, Train loss: 0.371, Test loss: 0.533, Test accuracy: 82.12
Round  99, Global train loss: 0.371, Global test loss: 1.289, Global test accuracy: 60.58
Final Round, Train loss: 0.278, Test loss: 0.566, Test accuracy: 82.15
Final Round, Global train loss: 0.278, Global test loss: 1.289, Global test accuracy: 60.58
Average accuracy final 10 rounds: 81.62583333333333 

Average global accuracy final 10 rounds: 61.04 

1892.0593371391296
[1.8447630405426025, 3.689526081085205, 5.140935897827148, 6.592345714569092, 8.074891567230225, 9.557437419891357, 11.019573211669922, 12.481709003448486, 13.952754497528076, 15.423799991607666, 16.876490831375122, 18.329181671142578, 19.81724715232849, 21.305312633514404, 22.80201482772827, 24.29871702194214, 25.83858323097229, 27.37844944000244, 28.89718222618103, 30.41591501235962, 31.88498330116272, 33.35405158996582, 34.825095653533936, 36.29613971710205, 37.76972436904907, 39.243309020996094, 40.708845376968384, 42.174381732940674, 43.64050483703613, 45.10662794113159, 46.577943086624146, 48.0492582321167, 49.52889323234558, 51.00852823257446, 52.468350410461426, 53.92817258834839, 55.38139724731445, 56.83462190628052, 58.31798481941223, 59.801347732543945, 61.32366228103638, 62.84597682952881, 64.36705279350281, 65.8881287574768, 67.41987371444702, 68.95161867141724, 70.40448355674744, 71.85734844207764, 73.3119010925293, 74.76645374298096, 76.26872706413269, 77.77100038528442, 79.27008295059204, 80.76916551589966, 82.25297045707703, 83.7367753982544, 85.23355841636658, 86.73034143447876, 88.1884355545044, 89.64652967453003, 91.10239672660828, 92.55826377868652, 94.0816559791565, 95.60504817962646, 97.06529211997986, 98.52553606033325, 100.06805610656738, 101.61057615280151, 103.09623527526855, 104.5818943977356, 106.20818257331848, 107.83447074890137, 109.30358242988586, 110.77269411087036, 112.23331069946289, 113.69392728805542, 115.19080710411072, 116.68768692016602, 118.14818072319031, 119.6086745262146, 121.1029691696167, 122.5972638130188, 124.08215475082397, 125.56704568862915, 127.07047367095947, 128.5739016532898, 130.03752541542053, 131.50114917755127, 132.9742624759674, 134.44737577438354, 135.92646503448486, 137.40555429458618, 138.9224820137024, 140.4394097328186, 141.92713046073914, 143.41485118865967, 144.898535490036, 146.38221979141235, 147.85304284095764, 149.32386589050293, 150.89826798439026, 152.4726700782776, 153.9969837665558, 155.52129745483398, 156.9898808002472, 158.4584641456604, 159.917231798172, 161.3759994506836, 162.83110117912292, 164.28620290756226, 165.74530410766602, 167.20440530776978, 168.68894267082214, 170.1734800338745, 171.70231103897095, 173.23114204406738, 174.77610397338867, 176.32106590270996, 177.79629611968994, 179.27152633666992, 180.72625136375427, 182.18097639083862, 183.63386631011963, 185.08675622940063, 186.54897117614746, 188.0111861228943, 189.46281814575195, 190.91445016860962, 192.38572072982788, 193.85699129104614, 195.31003379821777, 196.7630763053894, 198.20533680915833, 199.64759731292725, 201.0952115058899, 202.54282569885254, 204.01755046844482, 205.4922752380371, 206.99465608596802, 208.49703693389893, 209.99628496170044, 211.49553298950195, 212.97437167167664, 214.45321035385132, 215.90661096572876, 217.3600115776062, 218.82587671279907, 220.29174184799194, 221.75685262680054, 223.22196340560913, 224.6874177455902, 226.1528720855713, 227.64510107040405, 229.13733005523682, 230.61452388763428, 232.09171772003174, 233.5506649017334, 235.00961208343506, 236.46207761764526, 237.91454315185547, 239.37219858169556, 240.82985401153564, 242.28874039649963, 243.74762678146362, 245.19762659072876, 246.6476263999939, 248.1162624359131, 249.58489847183228, 251.08256363868713, 252.580228805542, 254.07371306419373, 255.56719732284546, 257.0275068283081, 258.48781633377075, 259.9637408256531, 261.4396653175354, 262.90968132019043, 264.37969732284546, 265.8660776615143, 267.3524580001831, 268.85424065589905, 270.356023311615, 271.84773683547974, 273.3394503593445, 274.8415596485138, 276.3436689376831, 277.8203032016754, 279.2969374656677, 280.7569682598114, 282.2169990539551, 283.6799476146698, 285.1428961753845, 286.6246407032013, 288.10638523101807, 289.57639336586, 291.0464015007019, 292.53474378585815, 294.0230860710144, 295.4993906021118, 296.97569513320923, 299.4508616924286, 301.92602825164795]
[31.05, 31.05, 39.8, 39.8, 42.50833333333333, 42.50833333333333, 49.075, 49.075, 52.85, 52.85, 57.0, 57.0, 56.30833333333333, 56.30833333333333, 61.916666666666664, 61.916666666666664, 65.65, 65.65, 65.89166666666667, 65.89166666666667, 68.725, 68.725, 68.99166666666666, 68.99166666666666, 69.95833333333333, 69.95833333333333, 72.04166666666667, 72.04166666666667, 73.625, 73.625, 73.50833333333334, 73.50833333333334, 73.925, 73.925, 74.2, 74.2, 74.775, 74.775, 75.6, 75.6, 75.88333333333334, 75.88333333333334, 76.45, 76.45, 76.31666666666666, 76.31666666666666, 76.65, 76.65, 76.76666666666667, 76.76666666666667, 77.0, 77.0, 77.11666666666666, 77.11666666666666, 77.3, 77.3, 77.05833333333334, 77.05833333333334, 76.975, 76.975, 77.19166666666666, 77.19166666666666, 78.225, 78.225, 78.43333333333334, 78.43333333333334, 78.7, 78.7, 78.90833333333333, 78.90833333333333, 79.05833333333334, 79.05833333333334, 79.4, 79.4, 79.45, 79.45, 79.20833333333333, 79.20833333333333, 79.46666666666667, 79.46666666666667, 78.94166666666666, 78.94166666666666, 78.61666666666666, 78.61666666666666, 79.00833333333334, 79.00833333333334, 78.60833333333333, 78.60833333333333, 78.93333333333334, 78.93333333333334, 79.11666666666666, 79.11666666666666, 79.9, 79.9, 79.35, 79.35, 79.84166666666667, 79.84166666666667, 79.61666666666666, 79.61666666666666, 80.075, 80.075, 80.3, 80.3, 80.45833333333333, 80.45833333333333, 80.86666666666666, 80.86666666666666, 80.45833333333333, 80.45833333333333, 80.90833333333333, 80.90833333333333, 80.06666666666666, 80.06666666666666, 80.075, 80.075, 80.54166666666667, 80.54166666666667, 80.71666666666667, 80.71666666666667, 80.25833333333334, 80.25833333333334, 80.15833333333333, 80.15833333333333, 80.61666666666666, 80.61666666666666, 80.60833333333333, 80.60833333333333, 80.56666666666666, 80.56666666666666, 81.09166666666667, 81.09166666666667, 80.775, 80.775, 80.88333333333334, 80.88333333333334, 80.64166666666667, 80.64166666666667, 80.61666666666666, 80.61666666666666, 80.69166666666666, 80.69166666666666, 80.65, 80.65, 80.93333333333334, 80.93333333333334, 80.64166666666667, 80.64166666666667, 81.1, 81.1, 81.13333333333334, 81.13333333333334, 81.175, 81.175, 81.46666666666667, 81.46666666666667, 81.78333333333333, 81.78333333333333, 81.5, 81.5, 81.975, 81.975, 81.93333333333334, 81.93333333333334, 81.33333333333333, 81.33333333333333, 81.83333333333333, 81.83333333333333, 82.00833333333334, 82.00833333333334, 81.45, 81.45, 81.65833333333333, 81.65833333333333, 81.35, 81.35, 81.18333333333334, 81.18333333333334, 81.09166666666667, 81.09166666666667, 81.8, 81.8, 81.66666666666667, 81.66666666666667, 81.65833333333333, 81.65833333333333, 81.49166666666666, 81.49166666666666, 81.75, 81.75, 81.3, 81.3, 81.275, 81.275, 81.28333333333333, 81.28333333333333, 81.90833333333333, 81.90833333333333, 82.125, 82.125, 82.15, 82.15]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 4, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.561, Test loss: 1.967, Test accuracy: 25.32
Round   1, Train loss: 1.049, Test loss: 1.882, Test accuracy: 36.63
Round   2, Train loss: 0.805, Test loss: 1.895, Test accuracy: 40.57
Round   3, Train loss: 0.884, Test loss: 1.303, Test accuracy: 51.48
Round   4, Train loss: 0.765, Test loss: 1.237, Test accuracy: 55.83
Round   5, Train loss: 0.799, Test loss: 1.087, Test accuracy: 57.42
Round   6, Train loss: 0.653, Test loss: 1.298, Test accuracy: 56.47
Round   7, Train loss: 0.744, Test loss: 1.003, Test accuracy: 62.53
Round   8, Train loss: 0.750, Test loss: 0.959, Test accuracy: 65.77
Round   9, Train loss: 0.748, Test loss: 0.848, Test accuracy: 65.44
Round  10, Train loss: 0.823, Test loss: 0.738, Test accuracy: 69.45
Round  11, Train loss: 0.626, Test loss: 0.745, Test accuracy: 70.22
Round  12, Train loss: 0.711, Test loss: 0.702, Test accuracy: 72.01
Round  13, Train loss: 0.716, Test loss: 0.626, Test accuracy: 74.88
Round  14, Train loss: 0.689, Test loss: 0.609, Test accuracy: 74.87
Round  15, Train loss: 0.687, Test loss: 0.626, Test accuracy: 75.34
Round  16, Train loss: 0.542, Test loss: 0.603, Test accuracy: 75.47
Round  17, Train loss: 0.744, Test loss: 0.596, Test accuracy: 75.85
Round  18, Train loss: 0.652, Test loss: 0.581, Test accuracy: 76.68
Round  19, Train loss: 0.772, Test loss: 0.575, Test accuracy: 76.90
Round  20, Train loss: 0.542, Test loss: 0.577, Test accuracy: 77.11
Round  21, Train loss: 0.522, Test loss: 0.568, Test accuracy: 76.77
Round  22, Train loss: 0.657, Test loss: 0.558, Test accuracy: 77.14
Round  23, Train loss: 0.544, Test loss: 0.547, Test accuracy: 76.96
Round  24, Train loss: 0.513, Test loss: 0.521, Test accuracy: 78.13
Round  25, Train loss: 0.597, Test loss: 0.517, Test accuracy: 78.72
Round  26, Train loss: 0.501, Test loss: 0.528, Test accuracy: 78.47
Round  27, Train loss: 0.499, Test loss: 0.514, Test accuracy: 79.53
Round  28, Train loss: 0.674, Test loss: 0.523, Test accuracy: 79.31
Round  29, Train loss: 0.646, Test loss: 0.515, Test accuracy: 79.44
Round  30, Train loss: 0.365, Test loss: 0.520, Test accuracy: 78.64
Round  31, Train loss: 0.539, Test loss: 0.501, Test accuracy: 79.89
Round  32, Train loss: 0.545, Test loss: 0.488, Test accuracy: 80.39
Round  33, Train loss: 0.561, Test loss: 0.489, Test accuracy: 80.28
Round  34, Train loss: 0.438, Test loss: 0.493, Test accuracy: 80.03
Round  35, Train loss: 0.566, Test loss: 0.503, Test accuracy: 79.41
Round  36, Train loss: 0.496, Test loss: 0.480, Test accuracy: 80.65
Round  37, Train loss: 0.373, Test loss: 0.473, Test accuracy: 80.79
Round  38, Train loss: 0.455, Test loss: 0.466, Test accuracy: 81.07
Round  39, Train loss: 0.565, Test loss: 0.459, Test accuracy: 81.59
Round  40, Train loss: 0.492, Test loss: 0.458, Test accuracy: 81.51
Round  41, Train loss: 0.502, Test loss: 0.463, Test accuracy: 81.47
Round  42, Train loss: 0.488, Test loss: 0.445, Test accuracy: 82.14
Round  43, Train loss: 0.340, Test loss: 0.447, Test accuracy: 81.91
Round  44, Train loss: 0.510, Test loss: 0.453, Test accuracy: 82.06
Round  45, Train loss: 0.362, Test loss: 0.455, Test accuracy: 81.76
Round  46, Train loss: 0.406, Test loss: 0.449, Test accuracy: 81.64
Round  47, Train loss: 0.458, Test loss: 0.444, Test accuracy: 82.11
Round  48, Train loss: 0.384, Test loss: 0.446, Test accuracy: 82.03
Round  49, Train loss: 0.459, Test loss: 0.438, Test accuracy: 82.38
Round  50, Train loss: 0.398, Test loss: 0.451, Test accuracy: 82.06
Round  51, Train loss: 0.485, Test loss: 0.438, Test accuracy: 82.48
Round  52, Train loss: 0.457, Test loss: 0.441, Test accuracy: 82.28
Round  53, Train loss: 0.385, Test loss: 0.434, Test accuracy: 82.72
Round  54, Train loss: 0.476, Test loss: 0.438, Test accuracy: 82.50
Round  55, Train loss: 0.465, Test loss: 0.433, Test accuracy: 82.84
Round  56, Train loss: 0.375, Test loss: 0.441, Test accuracy: 82.61
Round  57, Train loss: 0.399, Test loss: 0.432, Test accuracy: 83.08
Round  58, Train loss: 0.471, Test loss: 0.422, Test accuracy: 83.50
Round  59, Train loss: 0.446, Test loss: 0.432, Test accuracy: 83.11
Round  60, Train loss: 0.372, Test loss: 0.419, Test accuracy: 83.47
Round  61, Train loss: 0.444, Test loss: 0.424, Test accuracy: 83.35
Round  62, Train loss: 0.293, Test loss: 0.418, Test accuracy: 83.62
Round  63, Train loss: 0.370, Test loss: 0.429, Test accuracy: 83.52
Round  64, Train loss: 0.333, Test loss: 0.413, Test accuracy: 84.01
Round  65, Train loss: 0.321, Test loss: 0.411, Test accuracy: 83.71
Round  66, Train loss: 0.424, Test loss: 0.411, Test accuracy: 84.37
Round  67, Train loss: 0.419, Test loss: 0.411, Test accuracy: 84.20
Round  68, Train loss: 0.326, Test loss: 0.414, Test accuracy: 84.08
Round  69, Train loss: 0.386, Test loss: 0.414, Test accuracy: 84.23
Round  70, Train loss: 0.451, Test loss: 0.416, Test accuracy: 83.85
Round  71, Train loss: 0.358, Test loss: 0.411, Test accuracy: 84.02
Round  72, Train loss: 0.359, Test loss: 0.410, Test accuracy: 84.12
Round  73, Train loss: 0.317, Test loss: 0.409, Test accuracy: 84.33
Round  74, Train loss: 0.364, Test loss: 0.408, Test accuracy: 84.34
Round  75, Train loss: 0.285, Test loss: 0.402, Test accuracy: 84.67
Round  76, Train loss: 0.346, Test loss: 0.409, Test accuracy: 84.23
Round  77, Train loss: 0.319, Test loss: 0.408, Test accuracy: 84.45
Round  78, Train loss: 0.266, Test loss: 0.406, Test accuracy: 84.38
Round  79, Train loss: 0.397, Test loss: 0.412, Test accuracy: 84.28
Round  80, Train loss: 0.304, Test loss: 0.404, Test accuracy: 84.37
Round  81, Train loss: 0.303, Test loss: 0.404, Test accuracy: 84.56
Round  82, Train loss: 0.351, Test loss: 0.402, Test accuracy: 84.40
Round  83, Train loss: 0.350, Test loss: 0.404, Test accuracy: 84.65
Round  84, Train loss: 0.318, Test loss: 0.409, Test accuracy: 84.34
Round  85, Train loss: 0.362, Test loss: 0.403, Test accuracy: 84.37
Round  86, Train loss: 0.335, Test loss: 0.404, Test accuracy: 84.55
Round  87, Train loss: 0.271, Test loss: 0.402, Test accuracy: 84.39
Round  88, Train loss: 0.321, Test loss: 0.409, Test accuracy: 84.44
Round  89, Train loss: 0.315, Test loss: 0.412, Test accuracy: 84.25
Round  90, Train loss: 0.285, Test loss: 0.417, Test accuracy: 84.17
Round  91, Train loss: 0.276, Test loss: 0.412, Test accuracy: 83.98
Round  92, Train loss: 0.287, Test loss: 0.417, Test accuracy: 84.21
Round  93, Train loss: 0.340, Test loss: 0.404, Test accuracy: 84.77
Round  94, Train loss: 0.442, Test loss: 0.397, Test accuracy: 84.90
Round  95, Train loss: 0.377, Test loss: 0.401, Test accuracy: 84.78
Round  96, Train loss: 0.307, Test loss: 0.399, Test accuracy: 84.76
Round  97, Train loss: 0.302, Test loss: 0.399, Test accuracy: 84.92
Round  98, Train loss: 0.311, Test loss: 0.410, Test accuracy: 84.53
Round  99, Train loss: 0.329, Test loss: 0.405, Test accuracy: 84.62
Final Round, Train loss: 0.265, Test loss: 0.402, Test accuracy: 84.92
Average accuracy final 10 rounds: 84.56333333333335
1342.7439641952515
[2.1670753955841064, 3.859412431716919, 5.521503210067749, 7.200584888458252, 8.863879203796387, 10.513205766677856, 12.186749696731567, 13.868466138839722, 15.51714563369751, 17.17188835144043, 18.84233522415161, 20.482980251312256, 22.191380977630615, 23.914509534835815, 25.634482622146606, 27.388187885284424, 29.123332977294922, 30.857590675354004, 32.49936389923096, 34.1481499671936, 35.848143339157104, 37.552634716033936, 39.27629041671753, 40.973244428634644, 42.69754886627197, 44.38584327697754, 46.070770025253296, 47.780200481414795, 49.462202310562134, 51.14422011375427, 52.80934953689575, 54.472874879837036, 56.193352460861206, 57.92810249328613, 59.65581512451172, 61.394038915634155, 63.06524991989136, 64.70444869995117, 66.3461537361145, 68.00292539596558, 69.64590644836426, 71.29809880256653, 72.94361543655396, 74.660085439682, 76.34371423721313, 78.0270664691925, 79.67522954940796, 81.30244898796082, 82.96005368232727, 84.63559865951538, 86.33368444442749, 88.00066709518433, 89.64738535881042, 91.28554606437683, 92.95267105102539, 94.61834049224854, 96.27148461341858, 97.94921827316284, 99.62416124343872, 101.35378694534302, 103.00425720214844, 104.67238306999207, 106.35524988174438, 108.02149891853333, 109.65835690498352, 111.31347918510437, 112.95487308502197, 114.63266587257385, 116.2874505519867, 118.03308296203613, 119.74578976631165, 121.4321870803833, 123.12228512763977, 124.80940628051758, 126.50436639785767, 128.15260648727417, 129.78034090995789, 131.4133529663086, 133.10441064834595, 134.77882504463196, 136.4153552055359, 138.05464601516724, 139.68748259544373, 141.42801213264465, 143.05309176445007, 144.70599222183228, 146.34545588493347, 147.9977889060974, 149.66348028182983, 151.3679175376892, 153.10270643234253, 154.78147411346436, 156.43460178375244, 158.06713342666626, 159.725971698761, 161.3735227584839, 163.05924773216248, 164.8082435131073, 166.5405352115631, 168.24549984931946, 170.4829843044281]
[25.325, 36.63333333333333, 40.56666666666667, 51.483333333333334, 55.825, 57.416666666666664, 56.46666666666667, 62.53333333333333, 65.76666666666667, 65.44166666666666, 69.45, 70.21666666666667, 72.00833333333334, 74.88333333333334, 74.86666666666666, 75.34166666666667, 75.475, 75.85, 76.68333333333334, 76.9, 77.10833333333333, 76.76666666666667, 77.14166666666667, 76.95833333333333, 78.13333333333334, 78.725, 78.475, 79.525, 79.30833333333334, 79.44166666666666, 78.64166666666667, 79.89166666666667, 80.39166666666667, 80.275, 80.03333333333333, 79.40833333333333, 80.65, 80.79166666666667, 81.06666666666666, 81.59166666666667, 81.50833333333334, 81.475, 82.14166666666667, 81.90833333333333, 82.05833333333334, 81.75833333333334, 81.64166666666667, 82.10833333333333, 82.03333333333333, 82.38333333333334, 82.05833333333334, 82.48333333333333, 82.275, 82.71666666666667, 82.5, 82.84166666666667, 82.60833333333333, 83.075, 83.5, 83.10833333333333, 83.475, 83.35, 83.61666666666666, 83.51666666666667, 84.00833333333334, 83.70833333333333, 84.36666666666666, 84.2, 84.08333333333333, 84.23333333333333, 83.85, 84.01666666666667, 84.125, 84.33333333333333, 84.34166666666667, 84.66666666666667, 84.23333333333333, 84.45, 84.38333333333334, 84.275, 84.36666666666666, 84.55833333333334, 84.4, 84.65, 84.34166666666667, 84.36666666666666, 84.55, 84.39166666666667, 84.44166666666666, 84.25, 84.16666666666667, 83.98333333333333, 84.20833333333333, 84.76666666666667, 84.9, 84.775, 84.75833333333334, 84.925, 84.525, 84.625, 84.91666666666667]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 11, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  10.0000
Round 1 global test acc  15.0500
Round 2 global test acc  14.5500
Round 3 global test acc  18.8700
Round 4 global test acc  21.0500
Round 5 global test acc  25.1500
Round 6 global test acc  12.2400
Round 7 global test acc  25.5800
Round 8 global test acc  17.9400
Round 9 global test acc  26.1600
Round 10 global test acc  21.2400
Round 11 global test acc  24.0200
Round 12 global test acc  26.3600
Round 13 global test acc  31.3400
Round 14 global test acc  31.5400
Round 15 global test acc  22.9800
Round 16 global test acc  23.2700
Round 17 global test acc  28.5300
Round 18 global test acc  24.0300
Round 19 global test acc  23.2900
Round 20 global test acc  25.1700
Round 21 global test acc  30.5000
Round 22 global test acc  36.0700
Round 23 global test acc  31.2600
Round 24 global test acc  25.0300
Round 25 global test acc  29.7100
Round 26 global test acc  27.2000
Round 27 global test acc  33.3400
Round 28 global test acc  35.5500
Round 29 global test acc  23.5600
Round 30 global test acc  32.9300
Round 31 global test acc  33.7300
Round 32 global test acc  20.7500
Round 33 global test acc  33.2800
Round 34 global test acc  30.4900
Round 35 global test acc  37.5200
Round 36 global test acc  36.1900
Round 37 global test acc  24.3400
Round 38 global test acc  31.1000
Round 39 global test acc  24.6500
Round 40 global test acc  27.1000
Round 41 global test acc  36.9800
Round 42 global test acc  25.3700
Round 43 global test acc  39.8500
Round 44 global test acc  31.3600
Round 45 global test acc  34.1100
Round 46 global test acc  40.4000
Round 47 global test acc  32.5000
Round 48 global test acc  28.0700
Round 49 global test acc  29.8300
Round 50 global test acc  39.7800
Round 51 global test acc  32.4700
Round 52 global test acc  33.9100
Round 53 global test acc  33.7100
Round 54 global test acc  25.5000
Round 55 global test acc  39.8100
Round 56 global test acc  38.5400
Round 57 global test acc  30.2900
Round 58 global test acc  34.6500
Round 59 global test acc  37.4000
Round 60 global test acc  28.3600
Round 61 global test acc  28.2500
Round 62 global test acc  35.1300
Round 63 global test acc  25.0000
Round 64 global test acc  31.8600
Round 65 global test acc  26.9200
Round 66 global test acc  34.5900
Round 67 global test acc  38.8200
Round 68 global test acc  30.5000
Round 69 global test acc  32.9300
Round 70 global test acc  31.0700
Round 71 global test acc  29.9400
Round 72 global test acc  37.7000
Round 73 global test acc  32.8500
Round 74 global test acc  33.0900
Round 75 global test acc  28.9500
Round 76 global test acc  23.5100
Round 77 global test acc  21.8800
Round 78 global test acc  38.7900
Round 79 global test acc  27.3200
Round 80 global test acc  24.2000
Round 81 global test acc  20.0400
Round 82 global test acc  16.6900
Round 83 global test acc  14.8400
Round 84 global test acc  12.8800
Round 85 global test acc  11.5700
Round 86 global test acc  13.6600
Round 87 global test acc  10.2200
Round 88 global test acc  13.6800
Round 89 global test acc  11.7200
Round 90 global test acc  15.4500
Round 91 global test acc  16.6100
Round 92 global test acc  16.1800
Round 93 global test acc  15.6500
Round 94 global test acc  15.1700
Round 95 global test acc  14.3300
Round 96 global test acc  13.6000
Round 97 global test acc  13.4000
Round 98 global test acc  13.3400
Round 99 global test acc  16.4200
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 3, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.635, Test loss: 1.954, Test accuracy: 27.91
Round   1, Train loss: 1.117, Test loss: 1.917, Test accuracy: 34.23
Round   2, Train loss: 0.956, Test loss: 1.996, Test accuracy: 37.69
Round   3, Train loss: 0.992, Test loss: 1.295, Test accuracy: 49.14
Round   4, Train loss: 0.877, Test loss: 1.293, Test accuracy: 50.48
Round   5, Train loss: 0.970, Test loss: 1.152, Test accuracy: 54.38
Round   6, Train loss: 0.751, Test loss: 1.276, Test accuracy: 56.12
Round   7, Train loss: 0.810, Test loss: 1.087, Test accuracy: 59.85
Round   8, Train loss: 0.815, Test loss: 1.022, Test accuracy: 64.61
Round   9, Train loss: 0.860, Test loss: 0.956, Test accuracy: 63.71
Round  10, Train loss: 0.998, Test loss: 0.806, Test accuracy: 68.08
Round  11, Train loss: 0.687, Test loss: 0.818, Test accuracy: 68.00
Round  12, Train loss: 0.926, Test loss: 0.761, Test accuracy: 69.38
Round  13, Train loss: 0.768, Test loss: 0.654, Test accuracy: 74.00
Round  14, Train loss: 0.794, Test loss: 0.654, Test accuracy: 74.35
Round  15, Train loss: 0.840, Test loss: 0.658, Test accuracy: 74.74
Round  16, Train loss: 0.682, Test loss: 0.651, Test accuracy: 74.38
Round  17, Train loss: 0.845, Test loss: 0.638, Test accuracy: 75.44
Round  18, Train loss: 0.765, Test loss: 0.637, Test accuracy: 75.49
Round  19, Train loss: 0.782, Test loss: 0.624, Test accuracy: 76.56
Round  20, Train loss: 0.648, Test loss: 0.605, Test accuracy: 76.28
Round  21, Train loss: 0.669, Test loss: 0.612, Test accuracy: 76.78
Round  22, Train loss: 0.802, Test loss: 0.601, Test accuracy: 76.90
Round  23, Train loss: 0.644, Test loss: 0.591, Test accuracy: 76.73
Round  24, Train loss: 0.562, Test loss: 0.579, Test accuracy: 76.86
Round  25, Train loss: 0.711, Test loss: 0.571, Test accuracy: 77.75
Round  26, Train loss: 0.605, Test loss: 0.571, Test accuracy: 77.79
Round  27, Train loss: 0.647, Test loss: 0.566, Test accuracy: 77.92
Round  28, Train loss: 0.781, Test loss: 0.557, Test accuracy: 78.19
Round  29, Train loss: 0.743, Test loss: 0.562, Test accuracy: 78.70
Round  30, Train loss: 0.458, Test loss: 0.565, Test accuracy: 78.19
Round  31, Train loss: 0.635, Test loss: 0.562, Test accuracy: 78.47
Round  32, Train loss: 0.691, Test loss: 0.547, Test accuracy: 79.58
Round  33, Train loss: 0.711, Test loss: 0.547, Test accuracy: 79.70
Round  34, Train loss: 0.494, Test loss: 0.544, Test accuracy: 80.32
Round  35, Train loss: 0.650, Test loss: 0.549, Test accuracy: 80.17
Round  36, Train loss: 0.494, Test loss: 0.530, Test accuracy: 81.05
Round  37, Train loss: 0.488, Test loss: 0.520, Test accuracy: 80.89
Round  38, Train loss: 0.661, Test loss: 0.521, Test accuracy: 81.15
Round  39, Train loss: 0.723, Test loss: 0.515, Test accuracy: 81.49
Round  40, Train loss: 0.547, Test loss: 0.519, Test accuracy: 81.37
Round  41, Train loss: 0.620, Test loss: 0.504, Test accuracy: 81.92
Round  42, Train loss: 0.580, Test loss: 0.511, Test accuracy: 81.58
Round  43, Train loss: 0.487, Test loss: 0.511, Test accuracy: 81.76
Round  44, Train loss: 0.654, Test loss: 0.509, Test accuracy: 81.99
Round  45, Train loss: 0.418, Test loss: 0.513, Test accuracy: 81.92
Round  46, Train loss: 0.459, Test loss: 0.504, Test accuracy: 81.67
Round  47, Train loss: 0.560, Test loss: 0.499, Test accuracy: 82.22
Round  48, Train loss: 0.391, Test loss: 0.501, Test accuracy: 81.98
Round  49, Train loss: 0.573, Test loss: 0.490, Test accuracy: 82.56
Round  50, Train loss: 0.639, Test loss: 0.493, Test accuracy: 82.55
Round  51, Train loss: 0.579, Test loss: 0.488, Test accuracy: 82.88
Round  52, Train loss: 0.605, Test loss: 0.487, Test accuracy: 82.96
Round  53, Train loss: 0.500, Test loss: 0.495, Test accuracy: 82.68
Round  54, Train loss: 0.521, Test loss: 0.493, Test accuracy: 82.45
Round  55, Train loss: 0.607, Test loss: 0.495, Test accuracy: 81.97
Round  56, Train loss: 0.536, Test loss: 0.491, Test accuracy: 82.03
Round  57, Train loss: 0.553, Test loss: 0.484, Test accuracy: 82.68
Round  58, Train loss: 0.615, Test loss: 0.477, Test accuracy: 82.95
Round  59, Train loss: 0.495, Test loss: 0.491, Test accuracy: 82.35
Round  60, Train loss: 0.419, Test loss: 0.486, Test accuracy: 82.89
Round  61, Train loss: 0.598, Test loss: 0.480, Test accuracy: 83.26
Round  62, Train loss: 0.437, Test loss: 0.478, Test accuracy: 82.83
Round  63, Train loss: 0.464, Test loss: 0.473, Test accuracy: 83.22
Round  64, Train loss: 0.436, Test loss: 0.462, Test accuracy: 83.64
Round  65, Train loss: 0.370, Test loss: 0.463, Test accuracy: 83.67
Round  66, Train loss: 0.567, Test loss: 0.464, Test accuracy: 83.47
Round  67, Train loss: 0.507, Test loss: 0.461, Test accuracy: 83.87
Round  68, Train loss: 0.382, Test loss: 0.462, Test accuracy: 84.00
Round  69, Train loss: 0.480, Test loss: 0.455, Test accuracy: 84.12
Round  70, Train loss: 0.582, Test loss: 0.455, Test accuracy: 84.17
Round  71, Train loss: 0.454, Test loss: 0.457, Test accuracy: 84.12
Round  72, Train loss: 0.447, Test loss: 0.458, Test accuracy: 83.74
Round  73, Train loss: 0.462, Test loss: 0.467, Test accuracy: 83.53
Round  74, Train loss: 0.420, Test loss: 0.471, Test accuracy: 83.39
Round  75, Train loss: 0.378, Test loss: 0.455, Test accuracy: 84.21
Round  76, Train loss: 0.487, Test loss: 0.467, Test accuracy: 83.91
Round  77, Train loss: 0.425, Test loss: 0.459, Test accuracy: 84.31
Round  78, Train loss: 0.319, Test loss: 0.453, Test accuracy: 84.20
Round  79, Train loss: 0.549, Test loss: 0.457, Test accuracy: 84.50
Round  80, Train loss: 0.400, Test loss: 0.456, Test accuracy: 84.17
Round  81, Train loss: 0.500, Test loss: 0.456, Test accuracy: 84.36
Round  82, Train loss: 0.438, Test loss: 0.452, Test accuracy: 84.42
Round  83, Train loss: 0.400, Test loss: 0.459, Test accuracy: 84.13
Round  84, Train loss: 0.500, Test loss: 0.456, Test accuracy: 84.16
Round  85, Train loss: 0.465, Test loss: 0.450, Test accuracy: 84.41
Round  86, Train loss: 0.433, Test loss: 0.449, Test accuracy: 84.27
Round  87, Train loss: 0.278, Test loss: 0.446, Test accuracy: 84.59
Round  88, Train loss: 0.452, Test loss: 0.448, Test accuracy: 84.85
Round  89, Train loss: 0.454, Test loss: 0.452, Test accuracy: 84.53
Round  90, Train loss: 0.385, Test loss: 0.454, Test accuracy: 84.58
Round  91, Train loss: 0.364, Test loss: 0.458, Test accuracy: 84.08
Round  92, Train loss: 0.282, Test loss: 0.453, Test accuracy: 84.51
Round  93, Train loss: 0.522, Test loss: 0.452, Test accuracy: 84.78
Round  94, Train loss: 0.540, Test loss: 0.454, Test accuracy: 84.52
Round  95, Train loss: 0.514, Test loss: 0.459, Test accuracy: 84.38
Round  96, Train loss: 0.385, Test loss: 0.452, Test accuracy: 84.59
Round  97, Train loss: 0.333, Test loss: 0.446, Test accuracy: 84.73
Round  98, Train loss: 0.307, Test loss: 0.453, Test accuracy: 84.45
Round  99, Train loss: 0.463, Test loss: 0.454, Test accuracy: 84.32
Final Round, Train loss: 0.355, Test loss: 0.448, Test accuracy: 84.78
Average accuracy final 10 rounds: 84.49333333333334
1341.5180277824402
[2.090285301208496, 3.7337229251861572, 5.381031513214111, 7.0377655029296875, 8.799813508987427, 10.517343521118164, 12.155697107315063, 13.798635005950928, 15.457761764526367, 17.184348344802856, 18.887102127075195, 20.603657722473145, 22.3015775680542, 23.972634315490723, 25.617072105407715, 27.26929497718811, 28.928646326065063, 30.616604328155518, 32.28588938713074, 33.93262600898743, 35.58950352668762, 37.23252606391907, 38.87344670295715, 40.5168194770813, 42.164549350738525, 43.82042694091797, 45.49098181724548, 47.20234966278076, 48.92650055885315, 50.72180533409119, 52.36668872833252, 54.011314392089844, 55.68248677253723, 57.3576238155365, 59.020179748535156, 60.750298738479614, 62.48515844345093, 64.15907049179077, 65.80164694786072, 67.4520616531372, 69.09233236312866, 70.72071623802185, 72.37045788764954, 74.02087044715881, 75.66282653808594, 77.31278395652771, 78.98884081840515, 80.65045237541199, 82.28420424461365, 83.93772268295288, 85.58508658409119, 87.31383156776428, 89.05071187019348, 90.84056115150452, 92.56505179405212, 94.22483420372009, 95.86700701713562, 97.52001237869263, 99.24506139755249, 100.94309425354004, 102.59125685691833, 104.26089930534363, 105.91902112960815, 107.58771300315857, 109.26434850692749, 110.99252772331238, 112.75685119628906, 114.43148183822632, 116.09012293815613, 117.79118776321411, 119.47711706161499, 121.16283369064331, 122.80621790885925, 124.43904089927673, 126.08184623718262, 127.75696516036987, 129.44464492797852, 131.0971450805664, 132.76378893852234, 134.44313192367554, 136.13391733169556, 137.83887004852295, 139.5989191532135, 141.3573932647705, 143.06411266326904, 144.71182298660278, 146.35043597221375, 148.02133893966675, 149.69150686264038, 151.37142372131348, 153.05916380882263, 154.7315137386322, 156.40894722938538, 158.05963969230652, 159.70214796066284, 161.34530878067017, 163.01968598365784, 164.6693298816681, 166.33202648162842, 167.9961974620819, 170.22609162330627]
[27.908333333333335, 34.225, 37.69166666666667, 49.141666666666666, 50.483333333333334, 54.375, 56.125, 59.85, 64.60833333333333, 63.708333333333336, 68.075, 68.0, 69.38333333333334, 74.0, 74.35, 74.74166666666666, 74.375, 75.44166666666666, 75.49166666666666, 76.55833333333334, 76.275, 76.775, 76.9, 76.73333333333333, 76.85833333333333, 77.75, 77.79166666666667, 77.925, 78.19166666666666, 78.7, 78.19166666666666, 78.46666666666667, 79.58333333333333, 79.7, 80.31666666666666, 80.175, 81.05, 80.89166666666667, 81.15, 81.49166666666666, 81.36666666666666, 81.91666666666667, 81.58333333333333, 81.75833333333334, 81.99166666666666, 81.91666666666667, 81.66666666666667, 82.21666666666667, 81.98333333333333, 82.55833333333334, 82.55, 82.88333333333334, 82.95833333333333, 82.68333333333334, 82.45, 81.96666666666667, 82.03333333333333, 82.68333333333334, 82.95, 82.35, 82.89166666666667, 83.25833333333334, 82.825, 83.21666666666667, 83.64166666666667, 83.675, 83.475, 83.86666666666666, 84.0, 84.125, 84.16666666666667, 84.125, 83.74166666666666, 83.525, 83.39166666666667, 84.20833333333333, 83.90833333333333, 84.30833333333334, 84.2, 84.5, 84.175, 84.35833333333333, 84.41666666666667, 84.13333333333334, 84.15833333333333, 84.40833333333333, 84.26666666666667, 84.59166666666667, 84.85, 84.53333333333333, 84.58333333333333, 84.08333333333333, 84.50833333333334, 84.775, 84.51666666666667, 84.375, 84.59166666666667, 84.73333333333333, 84.45, 84.31666666666666, 84.775]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.4 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 3, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.540, Test loss: 1.945, Test accuracy: 26.45
Round   1, Train loss: 1.022, Test loss: 1.925, Test accuracy: 35.51
Round   2, Train loss: 0.847, Test loss: 1.905, Test accuracy: 40.12
Round   3, Train loss: 0.897, Test loss: 1.325, Test accuracy: 50.43
Round   4, Train loss: 0.784, Test loss: 1.269, Test accuracy: 53.14
Round   5, Train loss: 0.784, Test loss: 1.156, Test accuracy: 55.35
Round   6, Train loss: 0.713, Test loss: 1.269, Test accuracy: 56.14
Round   7, Train loss: 0.794, Test loss: 1.046, Test accuracy: 59.53
Round   8, Train loss: 0.747, Test loss: 0.993, Test accuracy: 64.49
Round   9, Train loss: 0.723, Test loss: 0.937, Test accuracy: 64.58
Round  10, Train loss: 0.873, Test loss: 0.738, Test accuracy: 69.30
Round  11, Train loss: 0.634, Test loss: 0.762, Test accuracy: 69.97
Round  12, Train loss: 0.790, Test loss: 0.705, Test accuracy: 71.05
Round  13, Train loss: 0.663, Test loss: 0.632, Test accuracy: 73.77
Round  14, Train loss: 0.704, Test loss: 0.625, Test accuracy: 74.24
Round  15, Train loss: 0.640, Test loss: 0.610, Test accuracy: 74.99
Round  16, Train loss: 0.622, Test loss: 0.611, Test accuracy: 74.93
Round  17, Train loss: 0.702, Test loss: 0.595, Test accuracy: 75.50
Round  18, Train loss: 0.656, Test loss: 0.590, Test accuracy: 75.82
Round  19, Train loss: 0.702, Test loss: 0.577, Test accuracy: 76.38
Round  20, Train loss: 0.518, Test loss: 0.572, Test accuracy: 76.58
Round  21, Train loss: 0.575, Test loss: 0.586, Test accuracy: 76.11
Round  22, Train loss: 0.695, Test loss: 0.566, Test accuracy: 76.88
Round  23, Train loss: 0.592, Test loss: 0.558, Test accuracy: 77.49
Round  24, Train loss: 0.569, Test loss: 0.547, Test accuracy: 77.59
Round  25, Train loss: 0.664, Test loss: 0.545, Test accuracy: 77.73
Round  26, Train loss: 0.568, Test loss: 0.545, Test accuracy: 77.89
Round  27, Train loss: 0.555, Test loss: 0.536, Test accuracy: 78.68
Round  28, Train loss: 0.653, Test loss: 0.530, Test accuracy: 78.95
Round  29, Train loss: 0.656, Test loss: 0.535, Test accuracy: 78.32
Round  30, Train loss: 0.469, Test loss: 0.524, Test accuracy: 78.84
Round  31, Train loss: 0.574, Test loss: 0.530, Test accuracy: 78.66
Round  32, Train loss: 0.513, Test loss: 0.515, Test accuracy: 79.71
Round  33, Train loss: 0.578, Test loss: 0.513, Test accuracy: 79.45
Round  34, Train loss: 0.451, Test loss: 0.508, Test accuracy: 79.46
Round  35, Train loss: 0.600, Test loss: 0.520, Test accuracy: 78.92
Round  36, Train loss: 0.457, Test loss: 0.502, Test accuracy: 79.38
Round  37, Train loss: 0.389, Test loss: 0.487, Test accuracy: 80.22
Round  38, Train loss: 0.595, Test loss: 0.494, Test accuracy: 79.85
Round  39, Train loss: 0.588, Test loss: 0.490, Test accuracy: 80.40
Round  40, Train loss: 0.505, Test loss: 0.489, Test accuracy: 80.61
Round  41, Train loss: 0.560, Test loss: 0.492, Test accuracy: 80.52
Round  42, Train loss: 0.538, Test loss: 0.484, Test accuracy: 80.76
Round  43, Train loss: 0.395, Test loss: 0.476, Test accuracy: 80.77
Round  44, Train loss: 0.486, Test loss: 0.469, Test accuracy: 81.42
Round  45, Train loss: 0.373, Test loss: 0.474, Test accuracy: 81.14
Round  46, Train loss: 0.413, Test loss: 0.464, Test accuracy: 81.65
Round  47, Train loss: 0.546, Test loss: 0.462, Test accuracy: 81.50
Round  48, Train loss: 0.393, Test loss: 0.466, Test accuracy: 81.42
Round  49, Train loss: 0.424, Test loss: 0.455, Test accuracy: 81.72
Round  50, Train loss: 0.485, Test loss: 0.453, Test accuracy: 81.82
Round  51, Train loss: 0.447, Test loss: 0.454, Test accuracy: 82.28
Round  52, Train loss: 0.422, Test loss: 0.455, Test accuracy: 82.20
Round  53, Train loss: 0.445, Test loss: 0.453, Test accuracy: 82.03
Round  54, Train loss: 0.477, Test loss: 0.455, Test accuracy: 81.84
Round  55, Train loss: 0.507, Test loss: 0.454, Test accuracy: 82.28
Round  56, Train loss: 0.424, Test loss: 0.462, Test accuracy: 81.67
Round  57, Train loss: 0.492, Test loss: 0.448, Test accuracy: 82.54
Round  58, Train loss: 0.465, Test loss: 0.442, Test accuracy: 83.28
Round  59, Train loss: 0.534, Test loss: 0.450, Test accuracy: 82.58
Round  60, Train loss: 0.329, Test loss: 0.439, Test accuracy: 83.01
Round  61, Train loss: 0.465, Test loss: 0.444, Test accuracy: 82.58
Round  62, Train loss: 0.302, Test loss: 0.438, Test accuracy: 83.07
Round  63, Train loss: 0.460, Test loss: 0.434, Test accuracy: 83.11
Round  64, Train loss: 0.352, Test loss: 0.440, Test accuracy: 82.96
Round  65, Train loss: 0.287, Test loss: 0.439, Test accuracy: 83.22
Round  66, Train loss: 0.434, Test loss: 0.437, Test accuracy: 82.96
Round  67, Train loss: 0.505, Test loss: 0.429, Test accuracy: 83.64
Round  68, Train loss: 0.334, Test loss: 0.424, Test accuracy: 83.91
Round  69, Train loss: 0.387, Test loss: 0.428, Test accuracy: 83.72
Round  70, Train loss: 0.450, Test loss: 0.426, Test accuracy: 83.33
Round  71, Train loss: 0.361, Test loss: 0.425, Test accuracy: 83.62
Round  72, Train loss: 0.313, Test loss: 0.427, Test accuracy: 83.53
Round  73, Train loss: 0.374, Test loss: 0.425, Test accuracy: 83.50
Round  74, Train loss: 0.379, Test loss: 0.425, Test accuracy: 83.40
Round  75, Train loss: 0.377, Test loss: 0.423, Test accuracy: 83.75
Round  76, Train loss: 0.476, Test loss: 0.431, Test accuracy: 83.10
Round  77, Train loss: 0.369, Test loss: 0.428, Test accuracy: 83.20
Round  78, Train loss: 0.278, Test loss: 0.425, Test accuracy: 83.25
Round  79, Train loss: 0.491, Test loss: 0.441, Test accuracy: 83.05
Round  80, Train loss: 0.311, Test loss: 0.427, Test accuracy: 83.28
Round  81, Train loss: 0.353, Test loss: 0.424, Test accuracy: 83.47
Round  82, Train loss: 0.401, Test loss: 0.419, Test accuracy: 83.67
Round  83, Train loss: 0.354, Test loss: 0.417, Test accuracy: 83.67
Round  84, Train loss: 0.370, Test loss: 0.414, Test accuracy: 84.00
Round  85, Train loss: 0.408, Test loss: 0.424, Test accuracy: 83.62
Round  86, Train loss: 0.332, Test loss: 0.427, Test accuracy: 83.58
Round  87, Train loss: 0.236, Test loss: 0.415, Test accuracy: 84.04
Round  88, Train loss: 0.314, Test loss: 0.416, Test accuracy: 84.03
Round  89, Train loss: 0.322, Test loss: 0.416, Test accuracy: 84.22
Round  90, Train loss: 0.296, Test loss: 0.415, Test accuracy: 84.22
Round  91, Train loss: 0.271, Test loss: 0.421, Test accuracy: 83.72
Round  92, Train loss: 0.291, Test loss: 0.415, Test accuracy: 83.94
Round  93, Train loss: 0.425, Test loss: 0.419, Test accuracy: 83.85
Round  94, Train loss: 0.360, Test loss: 0.420, Test accuracy: 83.81
Round  95, Train loss: 0.378, Test loss: 0.412, Test accuracy: 84.47
Round  96, Train loss: 0.304, Test loss: 0.414, Test accuracy: 84.03
Round  97, Train loss: 0.343, Test loss: 0.418, Test accuracy: 84.02
Round  98, Train loss: 0.273, Test loss: 0.413, Test accuracy: 84.26
Round  99, Train loss: 0.369, Test loss: 0.413, Test accuracy: 84.12
Final Round, Train loss: 0.279, Test loss: 0.412, Test accuracy: 84.39
Average accuracy final 10 rounds: 84.04416666666665
1888.6838157176971
[2.047327995300293, 3.763341188430786, 5.561362266540527, 7.293009519577026, 8.988960027694702, 10.685427188873291, 12.332839488983154, 14.013463497161865, 15.681836366653442, 17.351276636123657, 19.03286838531494, 20.762242794036865, 22.440059900283813, 24.218374967575073, 25.993008136749268, 27.679325580596924, 29.35486364364624, 31.047457456588745, 32.7421338558197, 34.391732692718506, 36.04571175575256, 38.9733202457428, 41.86752128601074, 44.728861570358276, 47.59379506111145, 50.46314191818237, 53.3334858417511, 56.15606737136841, 59.07525062561035, 61.99858045578003, 64.81976509094238, 67.69558644294739, 70.61695957183838, 73.69411087036133, 76.73468494415283, 79.63984107971191, 82.5082893371582, 85.44593405723572, 88.31655716896057, 91.17476201057434, 93.97852110862732, 96.77226758003235, 99.58734679222107, 102.52071523666382, 105.45507025718689, 108.25670099258423, 111.10418701171875, 114.02644395828247, 116.84034419059753, 119.69419860839844, 122.55884337425232, 125.46350646018982, 128.38969135284424, 131.28280878067017, 134.14799308776855, 137.07822823524475, 139.85800218582153, 142.69276642799377, 145.55564069747925, 148.40152955055237, 151.36009693145752, 154.35370087623596, 157.39246702194214, 160.44589924812317, 163.32829070091248, 166.12515234947205, 168.94616627693176, 171.7676920890808, 174.602707862854, 177.44456362724304, 180.34906673431396, 183.2427372932434, 186.11923789978027, 189.0468785762787, 191.92929005622864, 194.8552587032318, 197.80263781547546, 200.67423725128174, 203.5237066745758, 206.40842509269714, 209.30470037460327, 212.12921357154846, 215.03633856773376, 217.98812413215637, 220.94934844970703, 223.9413161277771, 226.73509764671326, 229.59073877334595, 232.4045009613037, 235.21417450904846, 238.01843094825745, 240.90895128250122, 243.83372378349304, 246.75924396514893, 249.69048023223877, 252.52216005325317, 255.33896231651306, 258.1908161640167, 261.1306257247925, 264.0002112388611, 266.26191782951355]
[26.45, 35.50833333333333, 40.11666666666667, 50.43333333333333, 53.141666666666666, 55.35, 56.141666666666666, 59.53333333333333, 64.49166666666666, 64.575, 69.3, 69.975, 71.05, 73.76666666666667, 74.24166666666666, 74.99166666666666, 74.93333333333334, 75.5, 75.81666666666666, 76.38333333333334, 76.58333333333333, 76.10833333333333, 76.88333333333334, 77.49166666666666, 77.59166666666667, 77.73333333333333, 77.89166666666667, 78.68333333333334, 78.95, 78.31666666666666, 78.84166666666667, 78.65833333333333, 79.70833333333333, 79.45, 79.45833333333333, 78.91666666666667, 79.375, 80.21666666666667, 79.85, 80.4, 80.60833333333333, 80.51666666666667, 80.75833333333334, 80.76666666666667, 81.425, 81.14166666666667, 81.65, 81.5, 81.41666666666667, 81.71666666666667, 81.81666666666666, 82.275, 82.2, 82.025, 81.84166666666667, 82.28333333333333, 81.66666666666667, 82.54166666666667, 83.28333333333333, 82.575, 83.00833333333334, 82.575, 83.06666666666666, 83.10833333333333, 82.95833333333333, 83.21666666666667, 82.95833333333333, 83.64166666666667, 83.90833333333333, 83.71666666666667, 83.33333333333333, 83.61666666666666, 83.525, 83.5, 83.4, 83.75, 83.1, 83.2, 83.25, 83.05, 83.275, 83.475, 83.66666666666667, 83.66666666666667, 84.0, 83.625, 83.575, 84.04166666666667, 84.025, 84.21666666666667, 84.21666666666667, 83.725, 83.94166666666666, 83.85, 83.80833333333334, 84.475, 84.025, 84.01666666666667, 84.25833333333334, 84.125, 84.39166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 201, in get_data_from_file
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_co_teaching2.py", line 92, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "RFL.py", line 62, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 93, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 95, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 16, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.171, Test loss: 1.931, Test accuracy: 27.23
Round   0, Global train loss: 1.171, Global test loss: 2.233, Global test accuracy: 18.33
Round   1, Train loss: 1.193, Test loss: 1.831, Test accuracy: 34.07
Round   1, Global train loss: 1.193, Global test loss: 2.489, Global test accuracy: 18.33
Round   2, Train loss: 1.039, Test loss: 1.455, Test accuracy: 42.12
Round   2, Global train loss: 1.039, Global test loss: 2.218, Global test accuracy: 20.98
Round   3, Train loss: 1.051, Test loss: 1.240, Test accuracy: 49.89
Round   3, Global train loss: 1.051, Global test loss: 2.038, Global test accuracy: 25.55
Round   4, Train loss: 1.011, Test loss: 1.140, Test accuracy: 56.08
Round   4, Global train loss: 1.011, Global test loss: 2.053, Global test accuracy: 31.02
Round   5, Train loss: 1.096, Test loss: 1.077, Test accuracy: 56.47
Round   5, Global train loss: 1.096, Global test loss: 2.062, Global test accuracy: 18.99
Round   6, Train loss: 0.919, Test loss: 1.006, Test accuracy: 59.94
Round   6, Global train loss: 0.919, Global test loss: 2.064, Global test accuracy: 27.81
Round   7, Train loss: 1.137, Test loss: 0.909, Test accuracy: 63.03
Round   7, Global train loss: 1.137, Global test loss: 2.205, Global test accuracy: 18.93
Round   8, Train loss: 0.984, Test loss: 0.890, Test accuracy: 63.86
Round   8, Global train loss: 0.984, Global test loss: 2.087, Global test accuracy: 27.39
Round   9, Train loss: 0.856, Test loss: 0.811, Test accuracy: 67.53
Round   9, Global train loss: 0.856, Global test loss: 2.118, Global test accuracy: 28.71
Round  10, Train loss: 0.977, Test loss: 0.793, Test accuracy: 69.79
Round  10, Global train loss: 0.977, Global test loss: 2.047, Global test accuracy: 27.44
Round  11, Train loss: 0.960, Test loss: 0.782, Test accuracy: 69.74
Round  11, Global train loss: 0.960, Global test loss: 2.162, Global test accuracy: 28.98
Round  12, Train loss: 0.902, Test loss: 0.778, Test accuracy: 70.72
Round  12, Global train loss: 0.902, Global test loss: 2.020, Global test accuracy: 26.03
Round  13, Train loss: 0.971, Test loss: 0.775, Test accuracy: 71.21
Round  13, Global train loss: 0.971, Global test loss: 2.153, Global test accuracy: 28.98
Round  14, Train loss: 0.952, Test loss: 0.772, Test accuracy: 71.08
Round  14, Global train loss: 0.952, Global test loss: 2.176, Global test accuracy: 26.78
Round  15, Train loss: 0.989, Test loss: 0.762, Test accuracy: 72.02
Round  15, Global train loss: 0.989, Global test loss: 2.011, Global test accuracy: 31.64
Round  16, Train loss: 0.798, Test loss: 0.760, Test accuracy: 71.47
Round  16, Global train loss: 0.798, Global test loss: 2.033, Global test accuracy: 29.60
Round  17, Train loss: 0.960, Test loss: 0.752, Test accuracy: 70.81
Round  17, Global train loss: 0.960, Global test loss: 2.018, Global test accuracy: 25.42
Round  18, Train loss: 0.931, Test loss: 0.756, Test accuracy: 70.86
Round  18, Global train loss: 0.931, Global test loss: 2.079, Global test accuracy: 31.60
Round  19, Train loss: 0.789, Test loss: 0.763, Test accuracy: 70.67
Round  19, Global train loss: 0.789, Global test loss: 2.060, Global test accuracy: 29.88
Round  20, Train loss: 0.763, Test loss: 0.748, Test accuracy: 71.28
Round  20, Global train loss: 0.763, Global test loss: 2.064, Global test accuracy: 33.11
Round  21, Train loss: 0.866, Test loss: 0.752, Test accuracy: 71.22
Round  21, Global train loss: 0.866, Global test loss: 2.104, Global test accuracy: 25.31
Round  22, Train loss: 0.664, Test loss: 0.742, Test accuracy: 71.14
Round  22, Global train loss: 0.664, Global test loss: 2.004, Global test accuracy: 36.56
Round  23, Train loss: 0.782, Test loss: 0.744, Test accuracy: 70.85
Round  23, Global train loss: 0.782, Global test loss: 2.107, Global test accuracy: 22.43
Round  24, Train loss: 0.582, Test loss: 0.734, Test accuracy: 71.92
Round  24, Global train loss: 0.582, Global test loss: 2.141, Global test accuracy: 23.50
Round  25, Train loss: 0.846, Test loss: 0.740, Test accuracy: 71.30
Round  25, Global train loss: 0.846, Global test loss: 1.991, Global test accuracy: 33.08
Round  26, Train loss: 0.669, Test loss: 0.734, Test accuracy: 72.22
Round  26, Global train loss: 0.669, Global test loss: 2.409, Global test accuracy: 17.93
Round  27, Train loss: 0.847, Test loss: 0.739, Test accuracy: 72.02
Round  27, Global train loss: 0.847, Global test loss: 2.120, Global test accuracy: 22.24
Round  28, Train loss: 0.637, Test loss: 0.747, Test accuracy: 70.76
Round  28, Global train loss: 0.637, Global test loss: 2.057, Global test accuracy: 33.79
Round  29, Train loss: 0.607, Test loss: 0.755, Test accuracy: 70.74
Round  29, Global train loss: 0.607, Global test loss: 2.076, Global test accuracy: 29.69
Round  30, Train loss: 0.772, Test loss: 0.749, Test accuracy: 71.46
Round  30, Global train loss: 0.772, Global test loss: 2.045, Global test accuracy: 28.70
Round  31, Train loss: 0.719, Test loss: 0.751, Test accuracy: 71.17
Round  31, Global train loss: 0.719, Global test loss: 2.122, Global test accuracy: 24.05
Round  32, Train loss: 0.657, Test loss: 0.748, Test accuracy: 71.39
Round  32, Global train loss: 0.657, Global test loss: 2.094, Global test accuracy: 24.69
Round  33, Train loss: 0.590, Test loss: 0.753, Test accuracy: 71.72
Round  33, Global train loss: 0.590, Global test loss: 1.958, Global test accuracy: 35.71
Round  34, Train loss: 0.588, Test loss: 0.765, Test accuracy: 71.07
Round  34, Global train loss: 0.588, Global test loss: 2.021, Global test accuracy: 33.09
Round  35, Train loss: 0.726, Test loss: 0.759, Test accuracy: 71.51
Round  35, Global train loss: 0.726, Global test loss: 2.065, Global test accuracy: 24.57
Round  36, Train loss: 0.632, Test loss: 0.757, Test accuracy: 71.13
Round  36, Global train loss: 0.632, Global test loss: 2.069, Global test accuracy: 23.18
Round  37, Train loss: 0.681, Test loss: 0.758, Test accuracy: 70.62
Round  37, Global train loss: 0.681, Global test loss: 2.029, Global test accuracy: 22.76
Round  38, Train loss: 0.585, Test loss: 0.764, Test accuracy: 70.38
Round  38, Global train loss: 0.585, Global test loss: 2.024, Global test accuracy: 18.86
Round  39, Train loss: 0.694, Test loss: 0.774, Test accuracy: 70.03
Round  39, Global train loss: 0.694, Global test loss: 2.133, Global test accuracy: 27.00
Round  40, Train loss: 0.780, Test loss: 0.781, Test accuracy: 69.93
Round  40, Global train loss: 0.780, Global test loss: 2.124, Global test accuracy: 21.50
Round  41, Train loss: 0.634, Test loss: 0.808, Test accuracy: 69.01
Round  41, Global train loss: 0.634, Global test loss: 2.069, Global test accuracy: 15.82
Round  42, Train loss: 0.405, Test loss: 0.814, Test accuracy: 68.90
Round  42, Global train loss: 0.405, Global test loss: 2.175, Global test accuracy: 22.89
Round  43, Train loss: 0.633, Test loss: 0.802, Test accuracy: 69.47
Round  43, Global train loss: 0.633, Global test loss: 2.016, Global test accuracy: 30.77
Round  44, Train loss: 0.670, Test loss: 0.825, Test accuracy: 68.33
Round  44, Global train loss: 0.670, Global test loss: 1.973, Global test accuracy: 30.12
Round  45, Train loss: 0.612, Test loss: 0.802, Test accuracy: 69.55
Round  45, Global train loss: 0.612, Global test loss: 2.104, Global test accuracy: 30.98
Round  46, Train loss: 0.547, Test loss: 0.808, Test accuracy: 69.38
Round  46, Global train loss: 0.547, Global test loss: 1.978, Global test accuracy: 29.43
Round  47, Train loss: 0.416, Test loss: 0.824, Test accuracy: 69.16
Round  47, Global train loss: 0.416, Global test loss: 1.982, Global test accuracy: 23.19
Round  48, Train loss: 0.733, Test loss: 0.849, Test accuracy: 68.38
Round  48, Global train loss: 0.733, Global test loss: 2.218, Global test accuracy: 20.08
Round  49, Train loss: 0.565, Test loss: 0.900, Test accuracy: 66.69
Round  49, Global train loss: 0.565, Global test loss: 2.044, Global test accuracy: 22.82
Round  50, Train loss: 0.546, Test loss: 0.890, Test accuracy: 67.30
Round  50, Global train loss: 0.546, Global test loss: 2.067, Global test accuracy: 28.59
Round  51, Train loss: 0.518, Test loss: 0.889, Test accuracy: 66.91
Round  51, Global train loss: 0.518, Global test loss: 1.998, Global test accuracy: 30.45
Round  52, Train loss: 0.500, Test loss: 0.902, Test accuracy: 67.12
Round  52, Global train loss: 0.500, Global test loss: 2.144, Global test accuracy: 27.28
Round  53, Train loss: 0.444, Test loss: 0.889, Test accuracy: 67.58
Round  53, Global train loss: 0.444, Global test loss: 2.059, Global test accuracy: 24.82
Round  54, Train loss: 0.577, Test loss: 0.903, Test accuracy: 67.22
Round  54, Global train loss: 0.577, Global test loss: 2.136, Global test accuracy: 25.95
Round  55, Train loss: 0.589, Test loss: 0.904, Test accuracy: 67.34
Round  55, Global train loss: 0.589, Global test loss: 2.108, Global test accuracy: 25.45
Round  56, Train loss: 0.522, Test loss: 0.910, Test accuracy: 67.70
Round  56, Global train loss: 0.522, Global test loss: 2.185, Global test accuracy: 19.08
Round  57, Train loss: 0.553, Test loss: 0.931, Test accuracy: 67.91
Round  57, Global train loss: 0.553, Global test loss: 2.020, Global test accuracy: 29.11
Round  58, Train loss: 0.575, Test loss: 0.951, Test accuracy: 66.80
Round  58, Global train loss: 0.575, Global test loss: 2.245, Global test accuracy: 18.93
Round  59, Train loss: 0.514, Test loss: 0.962, Test accuracy: 65.82
Round  59, Global train loss: 0.514, Global test loss: 2.071, Global test accuracy: 23.05
Round  60, Train loss: 0.434, Test loss: 0.995, Test accuracy: 65.30
Round  60, Global train loss: 0.434, Global test loss: 2.025, Global test accuracy: 28.14
Round  61, Train loss: 0.360, Test loss: 1.032, Test accuracy: 64.85
Round  61, Global train loss: 0.360, Global test loss: 1.922, Global test accuracy: 30.78
Round  62, Train loss: 0.278, Test loss: 1.040, Test accuracy: 65.03
Round  62, Global train loss: 0.278, Global test loss: 2.231, Global test accuracy: 21.90
Round  63, Train loss: 0.467, Test loss: 1.049, Test accuracy: 65.36
Round  63, Global train loss: 0.467, Global test loss: 2.122, Global test accuracy: 24.45
Round  64, Train loss: 0.429, Test loss: 1.048, Test accuracy: 66.06
Round  64, Global train loss: 0.429, Global test loss: 2.137, Global test accuracy: 25.48
Round  65, Train loss: 0.418, Test loss: 1.042, Test accuracy: 67.02
Round  65, Global train loss: 0.418, Global test loss: 2.089, Global test accuracy: 28.94
Round  66, Train loss: 0.481, Test loss: 1.025, Test accuracy: 66.79
Round  66, Global train loss: 0.481, Global test loss: 2.228, Global test accuracy: 21.84
Round  67, Train loss: 0.365, Test loss: 1.044, Test accuracy: 65.71
Round  67, Global train loss: 0.365, Global test loss: 2.016, Global test accuracy: 25.42
Round  68, Train loss: 0.319, Test loss: 1.064, Test accuracy: 65.68
Round  68, Global train loss: 0.319, Global test loss: 2.022, Global test accuracy: 26.03
Round  69, Train loss: 0.306, Test loss: 1.100, Test accuracy: 65.67
Round  69, Global train loss: 0.306, Global test loss: 2.206, Global test accuracy: 20.77
Round  70, Train loss: 0.324, Test loss: 1.102, Test accuracy: 66.16
Round  70, Global train loss: 0.324, Global test loss: 2.171, Global test accuracy: 18.15
Round  71, Train loss: 0.348, Test loss: 1.110, Test accuracy: 66.80
Round  71, Global train loss: 0.348, Global test loss: 2.207, Global test accuracy: 18.41
Round  72, Train loss: 0.334, Test loss: 1.135, Test accuracy: 66.37
Round  72, Global train loss: 0.334, Global test loss: 2.083, Global test accuracy: 24.72
Round  73, Train loss: 0.235, Test loss: 1.176, Test accuracy: 65.31
Round  73, Global train loss: 0.235, Global test loss: 2.056, Global test accuracy: 31.64
Round  74, Train loss: 0.251, Test loss: 1.188, Test accuracy: 64.90
Round  74, Global train loss: 0.251, Global test loss: 1.959, Global test accuracy: 24.60
Round  75, Train loss: 0.265, Test loss: 1.203, Test accuracy: 65.74
Round  75, Global train loss: 0.265, Global test loss: 2.099, Global test accuracy: 23.78
Round  76, Train loss: 0.223, Test loss: 1.208, Test accuracy: 65.50
Round  76, Global train loss: 0.223, Global test loss: 2.094, Global test accuracy: 20.35
Round  77, Train loss: 0.228, Test loss: 1.216, Test accuracy: 65.58
Round  77, Global train loss: 0.228, Global test loss: 2.003, Global test accuracy: 30.90
Round  78, Train loss: 0.346, Test loss: 1.216, Test accuracy: 65.69
Round  78, Global train loss: 0.346, Global test loss: 2.151, Global test accuracy: 26.77
Round  79, Train loss: 0.224, Test loss: 1.229, Test accuracy: 65.38
Round  79, Global train loss: 0.224, Global test loss: 2.038, Global test accuracy: 26.69
Round  80, Train loss: 0.243, Test loss: 1.253, Test accuracy: 65.14
Round  80, Global train loss: 0.243, Global test loss: 2.010, Global test accuracy: 28.57
Round  81, Train loss: 0.227, Test loss: 1.284, Test accuracy: 64.84
Round  81, Global train loss: 0.227, Global test loss: 2.143, Global test accuracy: 24.31
Round  82, Train loss: 0.376, Test loss: 1.293, Test accuracy: 65.21
Round  82, Global train loss: 0.376, Global test loss: 2.174, Global test accuracy: 21.98
Round  83, Train loss: 0.291, Test loss: 1.344, Test accuracy: 64.24
Round  83, Global train loss: 0.291, Global test loss: 2.170, Global test accuracy: 19.71
Round  84, Train loss: 0.344, Test loss: 1.339, Test accuracy: 64.58
Round  84, Global train loss: 0.344, Global test loss: 2.119, Global test accuracy: 22.50
Round  85, Train loss: 0.241, Test loss: 1.363, Test accuracy: 64.41
Round  85, Global train loss: 0.241, Global test loss: 2.116, Global test accuracy: 20.93
Round  86, Train loss: 0.247, Test loss: 1.398, Test accuracy: 64.53
Round  86, Global train loss: 0.247, Global test loss: 2.060, Global test accuracy: 28.07
Round  87, Train loss: 0.184, Test loss: 1.411, Test accuracy: 64.90
Round  87, Global train loss: 0.184, Global test loss: 2.020, Global test accuracy: 27.54
Round  88, Train loss: 0.251, Test loss: 1.421, Test accuracy: 65.03
Round  88, Global train loss: 0.251, Global test loss: 2.239, Global test accuracy: 22.88
Round  89, Train loss: 0.138, Test loss: 1.354, Test accuracy: 65.48
Round  89, Global train loss: 0.138, Global test loss: 2.103, Global test accuracy: 18.20
Round  90, Train loss: 0.283, Test loss: 1.348, Test accuracy: 65.72
Round  90, Global train loss: 0.283, Global test loss: 2.211, Global test accuracy: 20.89
Round  91, Train loss: 0.183, Test loss: 1.346, Test accuracy: 65.73
Round  91, Global train loss: 0.183, Global test loss: 2.117, Global test accuracy: 23.73
Round  92, Train loss: 0.245, Test loss: 1.364, Test accuracy: 65.82
Round  92, Global train loss: 0.245, Global test loss: 2.162, Global test accuracy: 24.49
Round  93, Train loss: 0.232, Test loss: 1.377, Test accuracy: 66.47
Round  93, Global train loss: 0.232, Global test loss: 2.150, Global test accuracy: 18.92
Round  94, Train loss: 0.168, Test loss: 1.410, Test accuracy: 65.90
Round  94, Global train loss: 0.168, Global test loss: 2.133, Global test accuracy: 28.82
Round  95, Train loss: 0.187, Test loss: 1.487, Test accuracy: 64.52
Round  95, Global train loss: 0.187, Global test loss: 2.127, Global test accuracy: 20.46
Round  96, Train loss: 0.171, Test loss: 1.460, Test accuracy: 64.93
Round  96, Global train loss: 0.171, Global test loss: 2.123, Global test accuracy: 16.87
Round  97, Train loss: 0.174, Test loss: 1.442, Test accuracy: 65.37
Round  97, Global train loss: 0.174, Global test loss: 2.001, Global test accuracy: 31.13
Round  98, Train loss: 0.117, Test loss: 1.476, Test accuracy: 65.28
Round  98, Global train loss: 0.117, Global test loss: 2.138, Global test accuracy: 23.10
Round  99, Train loss: 0.136, Test loss: 1.509, Test accuracy: 64.92
Round  99, Global train loss: 0.136, Global test loss: 2.078, Global test accuracy: 30.09
Final Round, Train loss: 0.151, Test loss: 1.618, Test accuracy: 64.96
Final Round, Global train loss: 0.151, Global test loss: 2.078, Global test accuracy: 30.09
Average accuracy final 10 rounds: 65.46666666666667 

Average global accuracy final 10 rounds: 23.849999999999998 

1847.686487197876
[1.7161269187927246, 3.432253837585449, 4.845155954360962, 6.258058071136475, 7.656708478927612, 9.05535888671875, 10.435872316360474, 11.816385746002197, 13.197694063186646, 14.579002380371094, 15.98169732093811, 17.384392261505127, 18.82713556289673, 20.26987886428833, 21.677186012268066, 23.084493160247803, 24.494216918945312, 25.903940677642822, 27.411413431167603, 28.918886184692383, 30.332383632659912, 31.74588108062744, 33.13233780860901, 34.518794536590576, 35.8993616104126, 37.27992868423462, 38.65670609474182, 40.03348350524902, 41.43036508560181, 42.82724666595459, 44.222222089767456, 45.61719751358032, 47.021446228027344, 48.425694942474365, 49.84961128234863, 51.2735276222229, 52.68960690498352, 54.10568618774414, 55.50388836860657, 56.902090549468994, 58.29778981208801, 59.69348907470703, 61.06734657287598, 62.44120407104492, 63.83143496513367, 65.22166585922241, 66.64639210700989, 68.07111835479736, 69.51163101196289, 70.95214366912842, 72.34619235992432, 73.74024105072021, 75.16131663322449, 76.58239221572876, 78.00832629203796, 79.43426036834717, 80.8571846485138, 82.28010892868042, 83.68183159828186, 85.0835542678833, 86.46278548240662, 87.84201669692993, 89.25972008705139, 90.67742347717285, 92.0773856639862, 93.47734785079956, 94.8624541759491, 96.24756050109863, 97.65628504753113, 99.06500959396362, 100.49657011032104, 101.92813062667847, 103.3137309551239, 104.69933128356934, 106.1089117527008, 107.51849222183228, 108.97135829925537, 110.42422437667847, 111.84747076034546, 113.27071714401245, 114.70607852935791, 116.14143991470337, 117.52923703193665, 118.91703414916992, 120.31303691864014, 121.70903968811035, 123.11045169830322, 124.5118637084961, 125.95177936553955, 127.39169502258301, 128.76548194885254, 130.13926887512207, 131.52072024345398, 132.9021716117859, 134.31542110443115, 135.72867059707642, 137.13561272621155, 138.54255485534668, 139.93655109405518, 141.33054733276367, 142.71170711517334, 144.092866897583, 145.4814953804016, 146.87012386322021, 148.25683903694153, 149.64355421066284, 151.05280208587646, 152.4620499610901, 153.87260746955872, 155.28316497802734, 156.71274375915527, 158.1423225402832, 159.58050107955933, 161.01867961883545, 162.42646050453186, 163.83424139022827, 165.22117972373962, 166.60811805725098, 167.9830298423767, 169.35794162750244, 170.75569128990173, 172.15344095230103, 173.5759449005127, 174.99844884872437, 176.39476799964905, 177.79108715057373, 179.17807531356812, 180.5650634765625, 181.9508979320526, 183.33673238754272, 184.7404797077179, 186.14422702789307, 187.547621011734, 188.95101499557495, 190.3724708557129, 191.79392671585083, 193.21732330322266, 194.64071989059448, 196.0343062877655, 197.42789268493652, 198.82483100891113, 200.22176933288574, 201.62508010864258, 203.0283908843994, 204.40551114082336, 205.78263139724731, 207.182799577713, 208.5829677581787, 209.95521521568298, 211.32746267318726, 212.70519638061523, 214.0829300880432, 215.4946050643921, 216.90628004074097, 218.32406902313232, 219.74185800552368, 221.20739936828613, 222.67294073104858, 224.0965166091919, 225.5200924873352, 226.92881155014038, 228.33753061294556, 229.718106508255, 231.09868240356445, 232.50823545455933, 233.9177885055542, 235.3220739364624, 236.7263593673706, 238.11814522743225, 239.5099310874939, 240.905011177063, 242.30009126663208, 243.68113708496094, 245.0621829032898, 246.4528775215149, 247.84357213974, 249.25824356079102, 250.67291498184204, 252.0893211364746, 253.50572729110718, 254.9376256465912, 256.3695240020752, 257.75626611709595, 259.1430082321167, 260.6117880344391, 262.0805678367615, 263.4725692272186, 264.8645706176758, 266.2618772983551, 267.6591839790344, 269.0510096549988, 270.44283533096313, 271.8218877315521, 273.2009401321411, 274.5949947834015, 275.98904943466187, 277.3575212955475, 278.7259931564331, 280.0989599227905, 281.47192668914795, 283.78360533714294, 286.09528398513794]
[27.233333333333334, 27.233333333333334, 34.06666666666667, 34.06666666666667, 42.125, 42.125, 49.891666666666666, 49.891666666666666, 56.075, 56.075, 56.46666666666667, 56.46666666666667, 59.94166666666667, 59.94166666666667, 63.03333333333333, 63.03333333333333, 63.858333333333334, 63.858333333333334, 67.525, 67.525, 69.79166666666667, 69.79166666666667, 69.74166666666666, 69.74166666666666, 70.725, 70.725, 71.20833333333333, 71.20833333333333, 71.08333333333333, 71.08333333333333, 72.01666666666667, 72.01666666666667, 71.46666666666667, 71.46666666666667, 70.80833333333334, 70.80833333333334, 70.85833333333333, 70.85833333333333, 70.675, 70.675, 71.275, 71.275, 71.225, 71.225, 71.14166666666667, 71.14166666666667, 70.85, 70.85, 71.925, 71.925, 71.3, 71.3, 72.225, 72.225, 72.01666666666667, 72.01666666666667, 70.75833333333334, 70.75833333333334, 70.74166666666666, 70.74166666666666, 71.45833333333333, 71.45833333333333, 71.175, 71.175, 71.39166666666667, 71.39166666666667, 71.725, 71.725, 71.06666666666666, 71.06666666666666, 71.50833333333334, 71.50833333333334, 71.13333333333334, 71.13333333333334, 70.625, 70.625, 70.38333333333334, 70.38333333333334, 70.03333333333333, 70.03333333333333, 69.93333333333334, 69.93333333333334, 69.00833333333334, 69.00833333333334, 68.9, 68.9, 69.46666666666667, 69.46666666666667, 68.33333333333333, 68.33333333333333, 69.55, 69.55, 69.375, 69.375, 69.15833333333333, 69.15833333333333, 68.375, 68.375, 66.69166666666666, 66.69166666666666, 67.3, 67.3, 66.90833333333333, 66.90833333333333, 67.11666666666666, 67.11666666666666, 67.575, 67.575, 67.225, 67.225, 67.34166666666667, 67.34166666666667, 67.7, 67.7, 67.90833333333333, 67.90833333333333, 66.8, 66.8, 65.81666666666666, 65.81666666666666, 65.3, 65.3, 64.85, 64.85, 65.025, 65.025, 65.35833333333333, 65.35833333333333, 66.05833333333334, 66.05833333333334, 67.01666666666667, 67.01666666666667, 66.79166666666667, 66.79166666666667, 65.70833333333333, 65.70833333333333, 65.68333333333334, 65.68333333333334, 65.66666666666667, 65.66666666666667, 66.15833333333333, 66.15833333333333, 66.8, 66.8, 66.36666666666666, 66.36666666666666, 65.30833333333334, 65.30833333333334, 64.9, 64.9, 65.74166666666666, 65.74166666666666, 65.5, 65.5, 65.575, 65.575, 65.69166666666666, 65.69166666666666, 65.375, 65.375, 65.14166666666667, 65.14166666666667, 64.84166666666667, 64.84166666666667, 65.20833333333333, 65.20833333333333, 64.24166666666666, 64.24166666666666, 64.58333333333333, 64.58333333333333, 64.40833333333333, 64.40833333333333, 64.525, 64.525, 64.9, 64.9, 65.03333333333333, 65.03333333333333, 65.48333333333333, 65.48333333333333, 65.725, 65.725, 65.73333333333333, 65.73333333333333, 65.81666666666666, 65.81666666666666, 66.475, 66.475, 65.9, 65.9, 64.51666666666667, 64.51666666666667, 64.93333333333334, 64.93333333333334, 65.36666666666666, 65.36666666666666, 65.28333333333333, 65.28333333333333, 64.91666666666667, 64.91666666666667, 64.95833333333333, 64.95833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 19, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.126, Test loss: 1.852, Test accuracy: 32.32
Round   0, Global train loss: 1.126, Global test loss: 2.155, Global test accuracy: 23.12
Round   1, Train loss: 0.970, Test loss: 1.763, Test accuracy: 38.99
Round   1, Global train loss: 0.970, Global test loss: 2.437, Global test accuracy: 19.82
Round   2, Train loss: 0.901, Test loss: 1.356, Test accuracy: 46.83
Round   2, Global train loss: 0.901, Global test loss: 2.016, Global test accuracy: 23.79
Round   3, Train loss: 0.783, Test loss: 1.122, Test accuracy: 56.00
Round   3, Global train loss: 0.783, Global test loss: 1.856, Global test accuracy: 30.62
Round   4, Train loss: 0.718, Test loss: 1.072, Test accuracy: 59.52
Round   4, Global train loss: 0.718, Global test loss: 2.168, Global test accuracy: 29.73
Round   5, Train loss: 0.815, Test loss: 0.931, Test accuracy: 63.34
Round   5, Global train loss: 0.815, Global test loss: 1.852, Global test accuracy: 33.72
Round   6, Train loss: 0.811, Test loss: 0.798, Test accuracy: 68.85
Round   6, Global train loss: 0.811, Global test loss: 1.629, Global test accuracy: 42.62
Round   7, Train loss: 0.690, Test loss: 0.731, Test accuracy: 70.33
Round   7, Global train loss: 0.690, Global test loss: 1.894, Global test accuracy: 32.33
Round   8, Train loss: 0.680, Test loss: 0.696, Test accuracy: 71.53
Round   8, Global train loss: 0.680, Global test loss: 1.830, Global test accuracy: 34.58
Round   9, Train loss: 0.655, Test loss: 0.618, Test accuracy: 75.01
Round   9, Global train loss: 0.655, Global test loss: 1.701, Global test accuracy: 40.99
Round  10, Train loss: 0.658, Test loss: 0.588, Test accuracy: 76.49
Round  10, Global train loss: 0.658, Global test loss: 1.529, Global test accuracy: 46.83
Round  11, Train loss: 0.717, Test loss: 0.576, Test accuracy: 76.99
Round  11, Global train loss: 0.717, Global test loss: 1.633, Global test accuracy: 41.14
Round  12, Train loss: 0.610, Test loss: 0.589, Test accuracy: 76.71
Round  12, Global train loss: 0.610, Global test loss: 1.487, Global test accuracy: 46.73
Round  13, Train loss: 0.546, Test loss: 0.586, Test accuracy: 76.62
Round  13, Global train loss: 0.546, Global test loss: 1.676, Global test accuracy: 43.67
Round  14, Train loss: 0.689, Test loss: 0.574, Test accuracy: 76.87
Round  14, Global train loss: 0.689, Global test loss: 1.749, Global test accuracy: 39.69
Round  15, Train loss: 0.604, Test loss: 0.565, Test accuracy: 77.28
Round  15, Global train loss: 0.604, Global test loss: 1.327, Global test accuracy: 54.34
Round  16, Train loss: 0.604, Test loss: 0.546, Test accuracy: 78.19
Round  16, Global train loss: 0.604, Global test loss: 1.338, Global test accuracy: 52.77
Round  17, Train loss: 0.481, Test loss: 0.520, Test accuracy: 79.72
Round  17, Global train loss: 0.481, Global test loss: 1.345, Global test accuracy: 55.48
Round  18, Train loss: 0.476, Test loss: 0.513, Test accuracy: 80.03
Round  18, Global train loss: 0.476, Global test loss: 1.400, Global test accuracy: 52.50
Round  19, Train loss: 0.466, Test loss: 0.510, Test accuracy: 80.55
Round  19, Global train loss: 0.466, Global test loss: 1.524, Global test accuracy: 53.92
Round  20, Train loss: 0.483, Test loss: 0.511, Test accuracy: 80.58
Round  20, Global train loss: 0.483, Global test loss: 1.369, Global test accuracy: 55.96
Round  21, Train loss: 0.489, Test loss: 0.499, Test accuracy: 81.04
Round  21, Global train loss: 0.489, Global test loss: 1.265, Global test accuracy: 57.71
Round  22, Train loss: 0.453, Test loss: 0.525, Test accuracy: 80.61
Round  22, Global train loss: 0.453, Global test loss: 1.481, Global test accuracy: 54.35
Round  23, Train loss: 0.546, Test loss: 0.517, Test accuracy: 80.78
Round  23, Global train loss: 0.546, Global test loss: 1.662, Global test accuracy: 45.03
Round  24, Train loss: 0.537, Test loss: 0.501, Test accuracy: 81.48
Round  24, Global train loss: 0.537, Global test loss: 1.351, Global test accuracy: 53.94
Round  25, Train loss: 0.557, Test loss: 0.483, Test accuracy: 82.10
Round  25, Global train loss: 0.557, Global test loss: 1.161, Global test accuracy: 60.89
Round  26, Train loss: 0.463, Test loss: 0.484, Test accuracy: 81.88
Round  26, Global train loss: 0.463, Global test loss: 1.339, Global test accuracy: 54.23
Round  27, Train loss: 0.471, Test loss: 0.475, Test accuracy: 82.34
Round  27, Global train loss: 0.471, Global test loss: 1.592, Global test accuracy: 49.81
Round  28, Train loss: 0.399, Test loss: 0.475, Test accuracy: 82.20
Round  28, Global train loss: 0.399, Global test loss: 1.514, Global test accuracy: 51.77
Round  29, Train loss: 0.430, Test loss: 0.468, Test accuracy: 82.44
Round  29, Global train loss: 0.430, Global test loss: 1.462, Global test accuracy: 52.93
Round  30, Train loss: 0.438, Test loss: 0.462, Test accuracy: 82.88
Round  30, Global train loss: 0.438, Global test loss: 1.144, Global test accuracy: 62.55
Round  31, Train loss: 0.445, Test loss: 0.470, Test accuracy: 82.78
Round  31, Global train loss: 0.445, Global test loss: 1.326, Global test accuracy: 54.77
Round  32, Train loss: 0.434, Test loss: 0.464, Test accuracy: 82.92
Round  32, Global train loss: 0.434, Global test loss: 1.368, Global test accuracy: 54.84
Round  33, Train loss: 0.511, Test loss: 0.478, Test accuracy: 82.48
Round  33, Global train loss: 0.511, Global test loss: 1.218, Global test accuracy: 59.11
Round  34, Train loss: 0.400, Test loss: 0.472, Test accuracy: 83.00
Round  34, Global train loss: 0.400, Global test loss: 1.410, Global test accuracy: 55.62
Round  35, Train loss: 0.391, Test loss: 0.477, Test accuracy: 82.76
Round  35, Global train loss: 0.391, Global test loss: 1.170, Global test accuracy: 60.86
Round  36, Train loss: 0.420, Test loss: 0.457, Test accuracy: 83.51
Round  36, Global train loss: 0.420, Global test loss: 1.259, Global test accuracy: 56.64
Round  37, Train loss: 0.374, Test loss: 0.459, Test accuracy: 83.22
Round  37, Global train loss: 0.374, Global test loss: 1.173, Global test accuracy: 60.98
Round  38, Train loss: 0.336, Test loss: 0.452, Test accuracy: 83.72
Round  38, Global train loss: 0.336, Global test loss: 1.142, Global test accuracy: 61.69
Round  39, Train loss: 0.440, Test loss: 0.439, Test accuracy: 84.20
Round  39, Global train loss: 0.440, Global test loss: 1.279, Global test accuracy: 57.49
Round  40, Train loss: 0.297, Test loss: 0.436, Test accuracy: 84.17
Round  40, Global train loss: 0.297, Global test loss: 1.586, Global test accuracy: 53.24
Round  41, Train loss: 0.348, Test loss: 0.445, Test accuracy: 83.97
Round  41, Global train loss: 0.348, Global test loss: 1.149, Global test accuracy: 61.56
Round  42, Train loss: 0.366, Test loss: 0.434, Test accuracy: 84.17
Round  42, Global train loss: 0.366, Global test loss: 1.275, Global test accuracy: 59.15
Round  43, Train loss: 0.381, Test loss: 0.438, Test accuracy: 84.03
Round  43, Global train loss: 0.381, Global test loss: 1.289, Global test accuracy: 58.54
Round  44, Train loss: 0.289, Test loss: 0.428, Test accuracy: 84.42
Round  44, Global train loss: 0.289, Global test loss: 1.401, Global test accuracy: 55.99
Round  45, Train loss: 0.400, Test loss: 0.424, Test accuracy: 84.57
Round  45, Global train loss: 0.400, Global test loss: 1.134, Global test accuracy: 62.39
Round  46, Train loss: 0.356, Test loss: 0.418, Test accuracy: 84.69
Round  46, Global train loss: 0.356, Global test loss: 1.156, Global test accuracy: 62.22
Round  47, Train loss: 0.363, Test loss: 0.422, Test accuracy: 84.62
Round  47, Global train loss: 0.363, Global test loss: 1.109, Global test accuracy: 63.15
Round  48, Train loss: 0.454, Test loss: 0.439, Test accuracy: 84.32
Round  48, Global train loss: 0.454, Global test loss: 1.356, Global test accuracy: 56.36
Round  49, Train loss: 0.269, Test loss: 0.453, Test accuracy: 83.95
Round  49, Global train loss: 0.269, Global test loss: 1.176, Global test accuracy: 61.07
Round  50, Train loss: 0.277, Test loss: 0.444, Test accuracy: 84.43
Round  50, Global train loss: 0.277, Global test loss: 1.124, Global test accuracy: 62.58
Round  51, Train loss: 0.334, Test loss: 0.455, Test accuracy: 84.04
Round  51, Global train loss: 0.334, Global test loss: 1.178, Global test accuracy: 61.77
Round  52, Train loss: 0.396, Test loss: 0.456, Test accuracy: 84.17
Round  52, Global train loss: 0.396, Global test loss: 1.166, Global test accuracy: 60.32
Round  53, Train loss: 0.348, Test loss: 0.454, Test accuracy: 84.47
Round  53, Global train loss: 0.348, Global test loss: 1.181, Global test accuracy: 60.89
Round  54, Train loss: 0.401, Test loss: 0.440, Test accuracy: 84.83
Round  54, Global train loss: 0.401, Global test loss: 1.201, Global test accuracy: 60.53
Round  55, Train loss: 0.274, Test loss: 0.441, Test accuracy: 84.72
Round  55, Global train loss: 0.274, Global test loss: 1.291, Global test accuracy: 60.11
Round  56, Train loss: 0.328, Test loss: 0.444, Test accuracy: 84.88
Round  56, Global train loss: 0.328, Global test loss: 1.731, Global test accuracy: 52.25
Round  57, Train loss: 0.325, Test loss: 0.449, Test accuracy: 84.69
Round  57, Global train loss: 0.325, Global test loss: 1.229, Global test accuracy: 62.06
Round  58, Train loss: 0.403, Test loss: 0.455, Test accuracy: 84.36
Round  58, Global train loss: 0.403, Global test loss: 1.278, Global test accuracy: 57.25
Round  59, Train loss: 0.229, Test loss: 0.455, Test accuracy: 84.50
Round  59, Global train loss: 0.229, Global test loss: 1.063, Global test accuracy: 65.80
Round  60, Train loss: 0.294, Test loss: 0.451, Test accuracy: 84.68
Round  60, Global train loss: 0.294, Global test loss: 1.227, Global test accuracy: 62.89
Round  61, Train loss: 0.192, Test loss: 0.453, Test accuracy: 84.59
Round  61, Global train loss: 0.192, Global test loss: 1.388, Global test accuracy: 61.16
Round  62, Train loss: 0.368, Test loss: 0.439, Test accuracy: 85.21
Round  62, Global train loss: 0.368, Global test loss: 1.184, Global test accuracy: 61.83
Round  63, Train loss: 0.341, Test loss: 0.442, Test accuracy: 85.33
Round  63, Global train loss: 0.341, Global test loss: 1.138, Global test accuracy: 62.54
Round  64, Train loss: 0.320, Test loss: 0.462, Test accuracy: 84.22
Round  64, Global train loss: 0.320, Global test loss: 1.214, Global test accuracy: 60.55
Round  65, Train loss: 0.286, Test loss: 0.454, Test accuracy: 84.71
Round  65, Global train loss: 0.286, Global test loss: 1.325, Global test accuracy: 60.77
Round  66, Train loss: 0.307, Test loss: 0.460, Test accuracy: 84.24
Round  66, Global train loss: 0.307, Global test loss: 1.609, Global test accuracy: 53.12
Round  67, Train loss: 0.271, Test loss: 0.452, Test accuracy: 84.37
Round  67, Global train loss: 0.271, Global test loss: 1.084, Global test accuracy: 65.88
Round  68, Train loss: 0.266, Test loss: 0.459, Test accuracy: 84.37
Round  68, Global train loss: 0.266, Global test loss: 1.139, Global test accuracy: 64.15
Round  69, Train loss: 0.346, Test loss: 0.462, Test accuracy: 84.55
Round  69, Global train loss: 0.346, Global test loss: 1.044, Global test accuracy: 65.72
Round  70, Train loss: 0.362, Test loss: 0.456, Test accuracy: 84.83
Round  70, Global train loss: 0.362, Global test loss: 1.119, Global test accuracy: 63.51
Round  71, Train loss: 0.307, Test loss: 0.450, Test accuracy: 85.03
Round  71, Global train loss: 0.307, Global test loss: 1.384, Global test accuracy: 56.69
Round  72, Train loss: 0.252, Test loss: 0.441, Test accuracy: 85.61
Round  72, Global train loss: 0.252, Global test loss: 1.140, Global test accuracy: 65.33
Round  73, Train loss: 0.290, Test loss: 0.450, Test accuracy: 85.21
Round  73, Global train loss: 0.290, Global test loss: 1.073, Global test accuracy: 65.83
Round  74, Train loss: 0.223, Test loss: 0.447, Test accuracy: 85.28
Round  74, Global train loss: 0.223, Global test loss: 1.077, Global test accuracy: 66.81
Round  75, Train loss: 0.238, Test loss: 0.444, Test accuracy: 85.42
Round  75, Global train loss: 0.238, Global test loss: 1.130, Global test accuracy: 65.26
Round  76, Train loss: 0.233, Test loss: 0.442, Test accuracy: 85.70
Round  76, Global train loss: 0.233, Global test loss: 1.208, Global test accuracy: 61.91
Round  77, Train loss: 0.250, Test loss: 0.442, Test accuracy: 85.69
Round  77, Global train loss: 0.250, Global test loss: 1.169, Global test accuracy: 64.09
Round  78, Train loss: 0.263, Test loss: 0.460, Test accuracy: 84.96
Round  78, Global train loss: 0.263, Global test loss: 1.501, Global test accuracy: 57.10
Round  79, Train loss: 0.198, Test loss: 0.468, Test accuracy: 84.83
Round  79, Global train loss: 0.198, Global test loss: 1.178, Global test accuracy: 63.16
Round  80, Train loss: 0.271, Test loss: 0.478, Test accuracy: 84.50
Round  80, Global train loss: 0.271, Global test loss: 1.188, Global test accuracy: 62.98
Round  81, Train loss: 0.265, Test loss: 0.486, Test accuracy: 84.37
Round  81, Global train loss: 0.265, Global test loss: 1.074, Global test accuracy: 65.53
Round  82, Train loss: 0.304, Test loss: 0.479, Test accuracy: 84.46
Round  82, Global train loss: 0.304, Global test loss: 1.083, Global test accuracy: 64.78
Round  83, Train loss: 0.199, Test loss: 0.464, Test accuracy: 84.79
Round  83, Global train loss: 0.199, Global test loss: 1.264, Global test accuracy: 62.62
Round  84, Train loss: 0.243, Test loss: 0.467, Test accuracy: 84.95
Round  84, Global train loss: 0.243, Global test loss: 1.298, Global test accuracy: 64.12
Round  85, Train loss: 0.217, Test loss: 0.461, Test accuracy: 84.84
Round  85, Global train loss: 0.217, Global test loss: 1.106, Global test accuracy: 66.37
Round  86, Train loss: 0.195, Test loss: 0.491, Test accuracy: 84.42
Round  86, Global train loss: 0.195, Global test loss: 1.455, Global test accuracy: 62.78
Round  87, Train loss: 0.239, Test loss: 0.470, Test accuracy: 84.95
Round  87, Global train loss: 0.239, Global test loss: 1.261, Global test accuracy: 64.32
Round  88, Train loss: 0.234, Test loss: 0.473, Test accuracy: 84.73
Round  88, Global train loss: 0.234, Global test loss: 1.785, Global test accuracy: 53.50
Round  89, Train loss: 0.246, Test loss: 0.475, Test accuracy: 84.93
Round  89, Global train loss: 0.246, Global test loss: 1.158, Global test accuracy: 65.87
Round  90, Train loss: 0.242, Test loss: 0.485, Test accuracy: 84.85
Round  90, Global train loss: 0.242, Global test loss: 1.213, Global test accuracy: 63.05
Round  91, Train loss: 0.253, Test loss: 0.482, Test accuracy: 84.93
Round  91, Global train loss: 0.253, Global test loss: 1.244, Global test accuracy: 63.88
Round  92, Train loss: 0.244, Test loss: 0.465, Test accuracy: 85.41
Round  92, Global train loss: 0.244, Global test loss: 1.162, Global test accuracy: 64.78
Round  93, Train loss: 0.204, Test loss: 0.469, Test accuracy: 85.53
Round  93, Global train loss: 0.204, Global test loss: 1.260, Global test accuracy: 63.17
Round  94, Train loss: 0.231, Test loss: 0.478, Test accuracy: 85.32
Round  94, Global train loss: 0.231, Global test loss: 1.185, Global test accuracy: 64.01
Round  95, Train loss: 0.173, Test loss: 0.454, Test accuracy: 85.47
Round  95, Global train loss: 0.173, Global test loss: 1.276, Global test accuracy: 61.85
Round  96, Train loss: 0.161, Test loss: 0.457, Test accuracy: 85.38
Round  96, Global train loss: 0.161, Global test loss: 1.245, Global test accuracy: 63.59
Round  97, Train loss: 0.182, Test loss: 0.457, Test accuracy: 85.56
Round  97, Global train loss: 0.182, Global test loss: 1.295, Global test accuracy: 64.21
Round  98, Train loss: 0.189, Test loss: 0.457, Test accuracy: 85.69
Round  98, Global train loss: 0.189, Global test loss: 1.071, Global test accuracy: 67.32
Round  99, Train loss: 0.195, Test loss: 0.450, Test accuracy: 85.90
Round  99, Global train loss: 0.195, Global test loss: 1.347, Global test accuracy: 64.13
Final Round, Train loss: 0.172, Test loss: 0.532, Test accuracy: 84.83
Final Round, Global train loss: 0.172, Global test loss: 1.347, Global test accuracy: 64.13
Average accuracy final 10 rounds: 85.40333333333334 

Average global accuracy final 10 rounds: 63.99750000000001 

1830.2160634994507
[1.6994154453277588, 3.3988308906555176, 4.819364070892334, 6.23989725112915, 7.660950183868408, 9.082003116607666, 10.550015211105347, 12.018027305603027, 13.39687204360962, 14.775716781616211, 16.20603060722351, 17.63634443283081, 19.056545972824097, 20.476747512817383, 21.875676155090332, 23.27460479736328, 24.672821521759033, 26.071038246154785, 27.463080883026123, 28.85512351989746, 30.254538774490356, 31.653954029083252, 33.01051068305969, 34.36706733703613, 35.7430579662323, 37.11904859542847, 38.53626084327698, 39.95347309112549, 41.377426624298096, 42.8013801574707, 44.16873264312744, 45.53608512878418, 46.895618200302124, 48.25515127182007, 49.6295850276947, 51.004018783569336, 52.40737771987915, 53.810736656188965, 55.23453712463379, 56.65833759307861, 58.04702353477478, 59.43570947647095, 60.79098916053772, 62.14626884460449, 63.528411626815796, 64.9105544090271, 66.30389928817749, 67.69724416732788, 69.07579183578491, 70.45433950424194, 71.89185237884521, 73.32936525344849, 74.75139236450195, 76.17341947555542, 77.60708045959473, 79.04074144363403, 80.4407126903534, 81.84068393707275, 83.21120834350586, 84.58173274993896, 85.95087099075317, 87.32000923156738, 88.69184422492981, 90.06367921829224, 91.46232151985168, 92.86096382141113, 94.27114415168762, 95.68132448196411, 97.08117413520813, 98.48102378845215, 99.85805344581604, 101.23508310317993, 102.60701990127563, 103.97895669937134, 105.346116065979, 106.71327543258667, 108.07663989067078, 109.44000434875488, 110.81493353843689, 112.1898627281189, 113.59747123718262, 115.00507974624634, 116.4095721244812, 117.81406450271606, 119.2159652709961, 120.61786603927612, 121.97908449172974, 123.34030294418335, 124.70889949798584, 126.07749605178833, 127.48494744300842, 128.89239883422852, 130.26722598075867, 131.64205312728882, 133.00406169891357, 134.36607027053833, 135.7467336654663, 137.1273970603943, 138.5357689857483, 139.9441409111023, 141.32675528526306, 142.70936965942383, 144.07053661346436, 145.43170356750488, 146.80517292022705, 148.17864227294922, 149.56021451950073, 150.94178676605225, 152.34404754638672, 153.7463083267212, 155.17156600952148, 156.59682369232178, 158.00597953796387, 159.41513538360596, 160.7886426448822, 162.16214990615845, 163.57535934448242, 164.9885687828064, 166.36655974388123, 167.74455070495605, 169.13502955436707, 170.52550840377808, 171.92124915122986, 173.31698989868164, 174.72944569587708, 176.1419014930725, 177.55736112594604, 178.97282075881958, 180.345929145813, 181.7190375328064, 183.09797167778015, 184.4769058227539, 185.83953762054443, 187.20216941833496, 188.59770822525024, 189.99324703216553, 191.39049243927002, 192.7877378463745, 194.1745047569275, 195.56127166748047, 196.95746898651123, 198.353666305542, 199.72392654418945, 201.0941867828369, 202.4976830482483, 203.90117931365967, 205.30176401138306, 206.70234870910645, 208.0775294303894, 209.45271015167236, 210.82892894744873, 212.2051477432251, 213.60286736488342, 215.00058698654175, 216.44212365150452, 217.88366031646729, 219.26984024047852, 220.65602016448975, 222.01526021957397, 223.3745002746582, 224.7368040084839, 226.09910774230957, 227.49564576148987, 228.89218378067017, 230.2747941017151, 231.65740442276, 233.07571816444397, 234.49403190612793, 235.8688681125641, 237.24370431900024, 238.62869262695312, 240.013680934906, 241.38503861427307, 242.75639629364014, 244.19155168533325, 245.62670707702637, 246.99271368980408, 248.3587203025818, 249.73353171348572, 251.10834312438965, 252.50760173797607, 253.9068603515625, 255.30496263504028, 256.70306491851807, 258.11968874931335, 259.53631258010864, 260.92523431777954, 262.31415605545044, 263.68968081474304, 265.06520557403564, 266.4252746105194, 267.7853436470032, 269.18765687942505, 270.5899701118469, 271.9869041442871, 273.3838381767273, 274.8080611228943, 276.2322840690613, 277.6269021034241, 279.02152013778687, 281.3362398147583, 283.65095949172974]
[32.31666666666667, 32.31666666666667, 38.99166666666667, 38.99166666666667, 46.833333333333336, 46.833333333333336, 56.0, 56.0, 59.516666666666666, 59.516666666666666, 63.34166666666667, 63.34166666666667, 68.85, 68.85, 70.33333333333333, 70.33333333333333, 71.525, 71.525, 75.00833333333334, 75.00833333333334, 76.49166666666666, 76.49166666666666, 76.99166666666666, 76.99166666666666, 76.70833333333333, 76.70833333333333, 76.61666666666666, 76.61666666666666, 76.86666666666666, 76.86666666666666, 77.275, 77.275, 78.19166666666666, 78.19166666666666, 79.725, 79.725, 80.025, 80.025, 80.55, 80.55, 80.58333333333333, 80.58333333333333, 81.04166666666667, 81.04166666666667, 80.60833333333333, 80.60833333333333, 80.78333333333333, 80.78333333333333, 81.48333333333333, 81.48333333333333, 82.1, 82.1, 81.875, 81.875, 82.34166666666667, 82.34166666666667, 82.2, 82.2, 82.44166666666666, 82.44166666666666, 82.875, 82.875, 82.775, 82.775, 82.91666666666667, 82.91666666666667, 82.48333333333333, 82.48333333333333, 83.0, 83.0, 82.75833333333334, 82.75833333333334, 83.50833333333334, 83.50833333333334, 83.225, 83.225, 83.725, 83.725, 84.2, 84.2, 84.175, 84.175, 83.975, 83.975, 84.16666666666667, 84.16666666666667, 84.03333333333333, 84.03333333333333, 84.425, 84.425, 84.56666666666666, 84.56666666666666, 84.69166666666666, 84.69166666666666, 84.625, 84.625, 84.31666666666666, 84.31666666666666, 83.95, 83.95, 84.43333333333334, 84.43333333333334, 84.04166666666667, 84.04166666666667, 84.175, 84.175, 84.475, 84.475, 84.825, 84.825, 84.725, 84.725, 84.88333333333334, 84.88333333333334, 84.69166666666666, 84.69166666666666, 84.35833333333333, 84.35833333333333, 84.5, 84.5, 84.68333333333334, 84.68333333333334, 84.59166666666667, 84.59166666666667, 85.20833333333333, 85.20833333333333, 85.325, 85.325, 84.21666666666667, 84.21666666666667, 84.70833333333333, 84.70833333333333, 84.24166666666666, 84.24166666666666, 84.36666666666666, 84.36666666666666, 84.36666666666666, 84.36666666666666, 84.55, 84.55, 84.83333333333333, 84.83333333333333, 85.03333333333333, 85.03333333333333, 85.60833333333333, 85.60833333333333, 85.20833333333333, 85.20833333333333, 85.275, 85.275, 85.41666666666667, 85.41666666666667, 85.7, 85.7, 85.69166666666666, 85.69166666666666, 84.95833333333333, 84.95833333333333, 84.83333333333333, 84.83333333333333, 84.5, 84.5, 84.36666666666666, 84.36666666666666, 84.45833333333333, 84.45833333333333, 84.79166666666667, 84.79166666666667, 84.95, 84.95, 84.84166666666667, 84.84166666666667, 84.41666666666667, 84.41666666666667, 84.95, 84.95, 84.73333333333333, 84.73333333333333, 84.93333333333334, 84.93333333333334, 84.85, 84.85, 84.93333333333334, 84.93333333333334, 85.40833333333333, 85.40833333333333, 85.525, 85.525, 85.31666666666666, 85.31666666666666, 85.46666666666667, 85.46666666666667, 85.38333333333334, 85.38333333333334, 85.55833333333334, 85.55833333333334, 85.69166666666666, 85.69166666666666, 85.9, 85.9, 84.825, 84.825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 9, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.262, Test loss: 1.895, Test accuracy: 32.01
Round   0, Global train loss: 1.262, Global test loss: 2.175, Global test accuracy: 23.55
Round   1, Train loss: 0.919, Test loss: 1.773, Test accuracy: 38.38
Round   1, Global train loss: 0.919, Global test loss: 2.441, Global test accuracy: 18.73
Round   2, Train loss: 0.934, Test loss: 1.387, Test accuracy: 45.70
Round   2, Global train loss: 0.934, Global test loss: 1.995, Global test accuracy: 23.41
Round   3, Train loss: 0.964, Test loss: 1.146, Test accuracy: 56.08
Round   3, Global train loss: 0.964, Global test loss: 1.883, Global test accuracy: 28.76
Round   4, Train loss: 0.844, Test loss: 1.077, Test accuracy: 59.83
Round   4, Global train loss: 0.844, Global test loss: 2.009, Global test accuracy: 30.20
Round   5, Train loss: 0.908, Test loss: 0.972, Test accuracy: 63.25
Round   5, Global train loss: 0.908, Global test loss: 1.904, Global test accuracy: 33.43
Round   6, Train loss: 0.789, Test loss: 0.912, Test accuracy: 66.08
Round   6, Global train loss: 0.789, Global test loss: 1.788, Global test accuracy: 36.42
Round   7, Train loss: 0.844, Test loss: 0.793, Test accuracy: 69.54
Round   7, Global train loss: 0.844, Global test loss: 1.803, Global test accuracy: 36.48
Round   8, Train loss: 0.798, Test loss: 0.778, Test accuracy: 70.40
Round   8, Global train loss: 0.798, Global test loss: 1.756, Global test accuracy: 38.30
Round   9, Train loss: 0.700, Test loss: 0.699, Test accuracy: 73.62
Round   9, Global train loss: 0.700, Global test loss: 1.695, Global test accuracy: 43.52
Round  10, Train loss: 0.695, Test loss: 0.684, Test accuracy: 74.30
Round  10, Global train loss: 0.695, Global test loss: 1.514, Global test accuracy: 48.85
Round  11, Train loss: 0.662, Test loss: 0.677, Test accuracy: 74.42
Round  11, Global train loss: 0.662, Global test loss: 1.521, Global test accuracy: 45.77
Round  12, Train loss: 0.777, Test loss: 0.652, Test accuracy: 75.48
Round  12, Global train loss: 0.777, Global test loss: 1.535, Global test accuracy: 44.27
Round  13, Train loss: 0.758, Test loss: 0.652, Test accuracy: 75.42
Round  13, Global train loss: 0.758, Global test loss: 1.669, Global test accuracy: 40.09
Round  14, Train loss: 0.669, Test loss: 0.632, Test accuracy: 76.67
Round  14, Global train loss: 0.669, Global test loss: 1.544, Global test accuracy: 45.48
Round  15, Train loss: 0.701, Test loss: 0.629, Test accuracy: 77.22
Round  15, Global train loss: 0.701, Global test loss: 1.417, Global test accuracy: 52.01
Round  16, Train loss: 0.691, Test loss: 0.617, Test accuracy: 77.34
Round  16, Global train loss: 0.691, Global test loss: 1.368, Global test accuracy: 51.93
Round  17, Train loss: 0.710, Test loss: 0.600, Test accuracy: 78.20
Round  17, Global train loss: 0.710, Global test loss: 1.384, Global test accuracy: 53.08
Round  18, Train loss: 0.727, Test loss: 0.591, Test accuracy: 78.70
Round  18, Global train loss: 0.727, Global test loss: 1.447, Global test accuracy: 54.38
Round  19, Train loss: 0.597, Test loss: 0.577, Test accuracy: 79.37
Round  19, Global train loss: 0.597, Global test loss: 1.541, Global test accuracy: 54.27
Round  20, Train loss: 0.664, Test loss: 0.580, Test accuracy: 79.14
Round  20, Global train loss: 0.664, Global test loss: 1.419, Global test accuracy: 55.70
Round  21, Train loss: 0.581, Test loss: 0.574, Test accuracy: 79.20
Round  21, Global train loss: 0.581, Global test loss: 1.337, Global test accuracy: 56.35
Round  22, Train loss: 0.622, Test loss: 0.569, Test accuracy: 79.70
Round  22, Global train loss: 0.622, Global test loss: 1.508, Global test accuracy: 52.84
Round  23, Train loss: 0.705, Test loss: 0.543, Test accuracy: 80.78
Round  23, Global train loss: 0.705, Global test loss: 1.612, Global test accuracy: 44.73
Round  24, Train loss: 0.819, Test loss: 0.536, Test accuracy: 80.42
Round  24, Global train loss: 0.819, Global test loss: 1.676, Global test accuracy: 47.14
Round  25, Train loss: 0.505, Test loss: 0.550, Test accuracy: 80.13
Round  25, Global train loss: 0.505, Global test loss: 1.319, Global test accuracy: 55.42
Round  26, Train loss: 0.495, Test loss: 0.556, Test accuracy: 80.00
Round  26, Global train loss: 0.495, Global test loss: 1.410, Global test accuracy: 54.32
Round  27, Train loss: 0.574, Test loss: 0.553, Test accuracy: 80.01
Round  27, Global train loss: 0.574, Global test loss: 1.433, Global test accuracy: 52.05
Round  28, Train loss: 0.487, Test loss: 0.545, Test accuracy: 80.42
Round  28, Global train loss: 0.487, Global test loss: 1.492, Global test accuracy: 50.91
Round  29, Train loss: 0.662, Test loss: 0.544, Test accuracy: 80.20
Round  29, Global train loss: 0.662, Global test loss: 1.579, Global test accuracy: 47.46
Round  30, Train loss: 0.499, Test loss: 0.540, Test accuracy: 80.44
Round  30, Global train loss: 0.499, Global test loss: 1.298, Global test accuracy: 59.03
Round  31, Train loss: 0.668, Test loss: 0.543, Test accuracy: 80.66
Round  31, Global train loss: 0.668, Global test loss: 1.444, Global test accuracy: 51.89
Round  32, Train loss: 0.680, Test loss: 0.533, Test accuracy: 80.98
Round  32, Global train loss: 0.680, Global test loss: 1.550, Global test accuracy: 50.41
Round  33, Train loss: 0.600, Test loss: 0.526, Test accuracy: 81.53
Round  33, Global train loss: 0.600, Global test loss: 1.372, Global test accuracy: 56.48
Round  34, Train loss: 0.645, Test loss: 0.516, Test accuracy: 82.08
Round  34, Global train loss: 0.645, Global test loss: 1.436, Global test accuracy: 55.55
Round  35, Train loss: 0.477, Test loss: 0.536, Test accuracy: 81.44
Round  35, Global train loss: 0.477, Global test loss: 1.216, Global test accuracy: 62.23
Round  36, Train loss: 0.616, Test loss: 0.533, Test accuracy: 80.99
Round  36, Global train loss: 0.616, Global test loss: 1.428, Global test accuracy: 50.81
Round  37, Train loss: 0.468, Test loss: 0.521, Test accuracy: 81.47
Round  37, Global train loss: 0.468, Global test loss: 1.208, Global test accuracy: 59.88
Round  38, Train loss: 0.448, Test loss: 0.518, Test accuracy: 81.38
Round  38, Global train loss: 0.448, Global test loss: 1.192, Global test accuracy: 60.88
Round  39, Train loss: 0.491, Test loss: 0.521, Test accuracy: 81.51
Round  39, Global train loss: 0.491, Global test loss: 1.280, Global test accuracy: 57.06
Round  40, Train loss: 0.432, Test loss: 0.514, Test accuracy: 81.78
Round  40, Global train loss: 0.432, Global test loss: 1.418, Global test accuracy: 54.33
Round  41, Train loss: 0.399, Test loss: 0.498, Test accuracy: 82.14
Round  41, Global train loss: 0.399, Global test loss: 1.213, Global test accuracy: 59.87
Round  42, Train loss: 0.478, Test loss: 0.499, Test accuracy: 82.11
Round  42, Global train loss: 0.478, Global test loss: 1.276, Global test accuracy: 60.37
Round  43, Train loss: 0.457, Test loss: 0.476, Test accuracy: 83.18
Round  43, Global train loss: 0.457, Global test loss: 1.289, Global test accuracy: 57.64
Round  44, Train loss: 0.419, Test loss: 0.486, Test accuracy: 83.11
Round  44, Global train loss: 0.419, Global test loss: 1.274, Global test accuracy: 58.04
Round  45, Train loss: 0.513, Test loss: 0.505, Test accuracy: 82.81
Round  45, Global train loss: 0.513, Global test loss: 1.234, Global test accuracy: 58.67
Round  46, Train loss: 0.590, Test loss: 0.532, Test accuracy: 81.48
Round  46, Global train loss: 0.590, Global test loss: 1.282, Global test accuracy: 58.39
Round  47, Train loss: 0.449, Test loss: 0.521, Test accuracy: 82.42
Round  47, Global train loss: 0.449, Global test loss: 1.188, Global test accuracy: 62.23
Round  48, Train loss: 0.562, Test loss: 0.516, Test accuracy: 82.53
Round  48, Global train loss: 0.562, Global test loss: 1.334, Global test accuracy: 54.79
Round  49, Train loss: 0.351, Test loss: 0.499, Test accuracy: 83.42
Round  49, Global train loss: 0.351, Global test loss: 1.210, Global test accuracy: 60.58
Round  50, Train loss: 0.452, Test loss: 0.491, Test accuracy: 83.52
Round  50, Global train loss: 0.452, Global test loss: 1.207, Global test accuracy: 60.87
Round  51, Train loss: 0.409, Test loss: 0.509, Test accuracy: 82.86
Round  51, Global train loss: 0.409, Global test loss: 1.316, Global test accuracy: 57.96
Round  52, Train loss: 0.462, Test loss: 0.490, Test accuracy: 83.51
Round  52, Global train loss: 0.462, Global test loss: 1.338, Global test accuracy: 57.16
Round  53, Train loss: 0.325, Test loss: 0.499, Test accuracy: 83.35
Round  53, Global train loss: 0.325, Global test loss: 1.189, Global test accuracy: 60.75
Round  54, Train loss: 0.505, Test loss: 0.498, Test accuracy: 83.24
Round  54, Global train loss: 0.505, Global test loss: 1.223, Global test accuracy: 59.48
Round  55, Train loss: 0.330, Test loss: 0.494, Test accuracy: 83.62
Round  55, Global train loss: 0.330, Global test loss: 1.252, Global test accuracy: 59.76
Round  56, Train loss: 0.464, Test loss: 0.499, Test accuracy: 83.41
Round  56, Global train loss: 0.464, Global test loss: 1.587, Global test accuracy: 51.13
Round  57, Train loss: 0.458, Test loss: 0.504, Test accuracy: 83.49
Round  57, Global train loss: 0.458, Global test loss: 1.356, Global test accuracy: 58.68
Round  58, Train loss: 0.499, Test loss: 0.519, Test accuracy: 82.96
Round  58, Global train loss: 0.499, Global test loss: 1.414, Global test accuracy: 56.50
Round  59, Train loss: 0.360, Test loss: 0.524, Test accuracy: 82.62
Round  59, Global train loss: 0.360, Global test loss: 1.158, Global test accuracy: 62.51
Round  60, Train loss: 0.387, Test loss: 0.538, Test accuracy: 81.69
Round  60, Global train loss: 0.387, Global test loss: 1.200, Global test accuracy: 62.88
Round  61, Train loss: 0.284, Test loss: 0.543, Test accuracy: 81.40
Round  61, Global train loss: 0.284, Global test loss: 1.280, Global test accuracy: 61.14
Round  62, Train loss: 0.418, Test loss: 0.528, Test accuracy: 81.78
Round  62, Global train loss: 0.418, Global test loss: 1.292, Global test accuracy: 59.30
Round  63, Train loss: 0.523, Test loss: 0.524, Test accuracy: 81.57
Round  63, Global train loss: 0.523, Global test loss: 1.214, Global test accuracy: 59.27
Round  64, Train loss: 0.407, Test loss: 0.507, Test accuracy: 82.71
Round  64, Global train loss: 0.407, Global test loss: 1.319, Global test accuracy: 57.63
Round  65, Train loss: 0.386, Test loss: 0.527, Test accuracy: 81.85
Round  65, Global train loss: 0.386, Global test loss: 1.240, Global test accuracy: 59.62
Round  66, Train loss: 0.489, Test loss: 0.523, Test accuracy: 81.94
Round  66, Global train loss: 0.489, Global test loss: 1.432, Global test accuracy: 53.98
Round  67, Train loss: 0.379, Test loss: 0.512, Test accuracy: 82.17
Round  67, Global train loss: 0.379, Global test loss: 1.095, Global test accuracy: 65.30
Round  68, Train loss: 0.358, Test loss: 0.513, Test accuracy: 82.06
Round  68, Global train loss: 0.358, Global test loss: 1.192, Global test accuracy: 62.45
Round  69, Train loss: 0.568, Test loss: 0.518, Test accuracy: 81.97
Round  69, Global train loss: 0.568, Global test loss: 1.153, Global test accuracy: 63.58
Round  70, Train loss: 0.482, Test loss: 0.518, Test accuracy: 82.03
Round  70, Global train loss: 0.482, Global test loss: 1.268, Global test accuracy: 58.85
Round  71, Train loss: 0.430, Test loss: 0.515, Test accuracy: 82.17
Round  71, Global train loss: 0.430, Global test loss: 1.324, Global test accuracy: 57.35
Round  72, Train loss: 0.304, Test loss: 0.499, Test accuracy: 82.73
Round  72, Global train loss: 0.304, Global test loss: 1.195, Global test accuracy: 63.21
Round  73, Train loss: 0.439, Test loss: 0.505, Test accuracy: 82.37
Round  73, Global train loss: 0.439, Global test loss: 1.091, Global test accuracy: 65.97
Round  74, Train loss: 0.390, Test loss: 0.503, Test accuracy: 82.52
Round  74, Global train loss: 0.390, Global test loss: 1.118, Global test accuracy: 65.99
Round  75, Train loss: 0.252, Test loss: 0.511, Test accuracy: 82.37
Round  75, Global train loss: 0.252, Global test loss: 1.140, Global test accuracy: 64.72
Round  76, Train loss: 0.423, Test loss: 0.525, Test accuracy: 82.06
Round  76, Global train loss: 0.423, Global test loss: 1.175, Global test accuracy: 62.08
Round  77, Train loss: 0.340, Test loss: 0.512, Test accuracy: 82.73
Round  77, Global train loss: 0.340, Global test loss: 1.200, Global test accuracy: 62.44
Round  78, Train loss: 0.407, Test loss: 0.507, Test accuracy: 82.83
Round  78, Global train loss: 0.407, Global test loss: 1.405, Global test accuracy: 57.18
Round  79, Train loss: 0.341, Test loss: 0.516, Test accuracy: 82.14
Round  79, Global train loss: 0.341, Global test loss: 1.224, Global test accuracy: 62.17
Round  80, Train loss: 0.324, Test loss: 0.520, Test accuracy: 81.97
Round  80, Global train loss: 0.324, Global test loss: 1.196, Global test accuracy: 62.18
Round  81, Train loss: 0.268, Test loss: 0.495, Test accuracy: 83.31
Round  81, Global train loss: 0.268, Global test loss: 1.094, Global test accuracy: 65.71
Round  82, Train loss: 0.442, Test loss: 0.504, Test accuracy: 83.45
Round  82, Global train loss: 0.442, Global test loss: 1.147, Global test accuracy: 64.02
Round  83, Train loss: 0.354, Test loss: 0.493, Test accuracy: 84.25
Round  83, Global train loss: 0.354, Global test loss: 1.266, Global test accuracy: 62.15
Round  84, Train loss: 0.310, Test loss: 0.492, Test accuracy: 84.08
Round  84, Global train loss: 0.310, Global test loss: 1.246, Global test accuracy: 64.06
Round  85, Train loss: 0.256, Test loss: 0.489, Test accuracy: 84.15
Round  85, Global train loss: 0.256, Global test loss: 1.137, Global test accuracy: 66.16
Round  86, Train loss: 0.354, Test loss: 0.501, Test accuracy: 83.68
Round  86, Global train loss: 0.354, Global test loss: 1.410, Global test accuracy: 61.04
Round  87, Train loss: 0.300, Test loss: 0.503, Test accuracy: 83.70
Round  87, Global train loss: 0.300, Global test loss: 1.358, Global test accuracy: 60.80
Round  88, Train loss: 0.325, Test loss: 0.498, Test accuracy: 83.94
Round  88, Global train loss: 0.325, Global test loss: 1.547, Global test accuracy: 57.24
Round  89, Train loss: 0.371, Test loss: 0.494, Test accuracy: 84.03
Round  89, Global train loss: 0.371, Global test loss: 1.152, Global test accuracy: 65.82
Round  90, Train loss: 0.265, Test loss: 0.522, Test accuracy: 83.22
Round  90, Global train loss: 0.265, Global test loss: 1.203, Global test accuracy: 65.22
Round  91, Train loss: 0.366, Test loss: 0.502, Test accuracy: 83.47
Round  91, Global train loss: 0.366, Global test loss: 1.262, Global test accuracy: 63.28
Round  92, Train loss: 0.425, Test loss: 0.529, Test accuracy: 82.60
Round  92, Global train loss: 0.425, Global test loss: 1.237, Global test accuracy: 63.00
Round  93, Train loss: 0.460, Test loss: 0.542, Test accuracy: 82.32
Round  93, Global train loss: 0.460, Global test loss: 1.315, Global test accuracy: 59.45
Round  94, Train loss: 0.245, Test loss: 0.546, Test accuracy: 82.45
Round  94, Global train loss: 0.245, Global test loss: 1.234, Global test accuracy: 62.67
Round  95, Train loss: 0.217, Test loss: 0.528, Test accuracy: 82.92
Round  95, Global train loss: 0.217, Global test loss: 1.336, Global test accuracy: 60.54
Round  96, Train loss: 0.271, Test loss: 0.550, Test accuracy: 82.08
Round  96, Global train loss: 0.271, Global test loss: 1.239, Global test accuracy: 61.92
Round  97, Train loss: 0.225, Test loss: 0.538, Test accuracy: 82.62
Round  97, Global train loss: 0.225, Global test loss: 1.320, Global test accuracy: 62.93
Round  98, Train loss: 0.300, Test loss: 0.538, Test accuracy: 82.69
Round  98, Global train loss: 0.300, Global test loss: 1.163, Global test accuracy: 66.19
Round  99, Train loss: 0.272, Test loss: 0.546, Test accuracy: 82.43
Round  99, Global train loss: 0.272, Global test loss: 1.399, Global test accuracy: 63.14
Final Round, Train loss: 0.275, Test loss: 0.562, Test accuracy: 83.02
Final Round, Global train loss: 0.275, Global test loss: 1.399, Global test accuracy: 63.14
Average accuracy final 10 rounds: 82.68 

Average global accuracy final 10 rounds: 62.83583333333333 

1908.2424855232239
[1.7846903800964355, 3.569380760192871, 5.0846123695373535, 6.599843978881836, 8.137794971466064, 9.675745964050293, 11.228072881698608, 12.780399799346924, 14.311734437942505, 15.843069076538086, 17.340691566467285, 18.838314056396484, 20.322547912597656, 21.806781768798828, 23.304537773132324, 24.80229377746582, 26.281301259994507, 27.760308742523193, 29.267656326293945, 30.775003910064697, 32.26803779602051, 33.76107168197632, 35.24888253211975, 36.736693382263184, 38.23690581321716, 39.73711824417114, 41.245413303375244, 42.753708362579346, 44.2572705745697, 45.76083278656006, 47.24883008003235, 48.73682737350464, 50.214126110076904, 51.69142484664917, 53.166722536087036, 54.6420202255249, 56.12827515602112, 57.614530086517334, 59.10663819313049, 60.59874629974365, 62.07019376754761, 63.54164123535156, 65.04376745223999, 66.54589366912842, 68.05188655853271, 69.55787944793701, 71.05613565444946, 72.55439186096191, 74.06667399406433, 75.57895612716675, 77.05056357383728, 78.52217102050781, 79.98008394241333, 81.43799686431885, 82.94890666007996, 84.45981645584106, 85.9503231048584, 87.44082975387573, 88.94762849807739, 90.45442724227905, 91.97121572494507, 93.48800420761108, 94.99959254264832, 96.51118087768555, 98.0439805984497, 99.57678031921387, 101.07549023628235, 102.57420015335083, 104.05759501457214, 105.54098987579346, 107.04443979263306, 108.54788970947266, 110.07492399215698, 111.60195827484131, 113.09457349777222, 114.58718872070312, 116.09226250648499, 117.59733629226685, 119.1053524017334, 120.61336851119995, 122.1102602481842, 123.60715198516846, 125.11241316795349, 126.61767435073853, 128.11124539375305, 129.60481643676758, 131.07666611671448, 132.54851579666138, 134.0334029197693, 135.5182900428772, 136.9784038066864, 138.4385175704956, 139.91354370117188, 141.38856983184814, 142.87181282043457, 144.355055809021, 145.84345054626465, 147.3318452835083, 148.81667375564575, 150.3015022277832, 151.77984428405762, 153.25818634033203, 154.73737168312073, 156.21655702590942, 157.69300508499146, 159.1694531440735, 160.64570665359497, 162.12196016311646, 163.58400988578796, 165.04605960845947, 166.52784776687622, 168.00963592529297, 169.50686287879944, 171.0040898323059, 172.49248027801514, 173.98087072372437, 175.45617938041687, 176.93148803710938, 178.42147994041443, 179.91147184371948, 181.38032054901123, 182.84916925430298, 184.3458788394928, 185.84258842468262, 187.3297472000122, 188.8169059753418, 190.31851935386658, 191.82013273239136, 193.29813861846924, 194.77614450454712, 196.25020337104797, 197.72426223754883, 199.23333835601807, 200.7424144744873, 202.23255729675293, 203.72270011901855, 205.23559188842773, 206.7484836578369, 208.24458980560303, 209.74069595336914, 211.21264123916626, 212.68458652496338, 214.1726336479187, 215.66068077087402, 217.19454526901245, 218.72840976715088, 220.20585346221924, 221.6832971572876, 223.15721940994263, 224.63114166259766, 226.0974440574646, 227.56374645233154, 229.05967020988464, 230.55559396743774, 232.1152102947235, 233.67482662200928, 235.15559148788452, 236.63635635375977, 238.1007912158966, 239.56522607803345, 241.04291915893555, 242.52061223983765, 244.0170123577118, 245.51341247558594, 247.05202293395996, 248.59063339233398, 250.14962029457092, 251.70860719680786, 253.23605251312256, 254.76349782943726, 256.2563383579254, 257.7491788864136, 259.23388481140137, 260.71859073638916, 262.1853835582733, 263.65217638015747, 265.1308903694153, 266.6096043586731, 268.083703994751, 269.55780363082886, 271.03021001815796, 272.50261640548706, 273.9809458255768, 275.4592752456665, 276.9256794452667, 278.39208364486694, 279.8579034805298, 281.3237233161926, 282.8146278858185, 284.30553245544434, 285.7814700603485, 287.2574076652527, 288.74058389663696, 290.22376012802124, 291.7284474372864, 293.2331347465515, 294.71168756484985, 296.1902403831482, 297.7010223865509, 299.2118043899536, 301.6940095424652, 304.1762146949768]
[32.00833333333333, 32.00833333333333, 38.375, 38.375, 45.7, 45.7, 56.075, 56.075, 59.825, 59.825, 63.25, 63.25, 66.075, 66.075, 69.54166666666667, 69.54166666666667, 70.4, 70.4, 73.61666666666666, 73.61666666666666, 74.3, 74.3, 74.41666666666667, 74.41666666666667, 75.48333333333333, 75.48333333333333, 75.425, 75.425, 76.675, 76.675, 77.225, 77.225, 77.34166666666667, 77.34166666666667, 78.2, 78.2, 78.7, 78.7, 79.36666666666666, 79.36666666666666, 79.14166666666667, 79.14166666666667, 79.2, 79.2, 79.7, 79.7, 80.78333333333333, 80.78333333333333, 80.425, 80.425, 80.13333333333334, 80.13333333333334, 80.0, 80.0, 80.00833333333334, 80.00833333333334, 80.41666666666667, 80.41666666666667, 80.2, 80.2, 80.44166666666666, 80.44166666666666, 80.65833333333333, 80.65833333333333, 80.98333333333333, 80.98333333333333, 81.53333333333333, 81.53333333333333, 82.08333333333333, 82.08333333333333, 81.44166666666666, 81.44166666666666, 80.99166666666666, 80.99166666666666, 81.475, 81.475, 81.375, 81.375, 81.50833333333334, 81.50833333333334, 81.78333333333333, 81.78333333333333, 82.14166666666667, 82.14166666666667, 82.10833333333333, 82.10833333333333, 83.18333333333334, 83.18333333333334, 83.10833333333333, 83.10833333333333, 82.80833333333334, 82.80833333333334, 81.48333333333333, 81.48333333333333, 82.41666666666667, 82.41666666666667, 82.525, 82.525, 83.41666666666667, 83.41666666666667, 83.51666666666667, 83.51666666666667, 82.85833333333333, 82.85833333333333, 83.50833333333334, 83.50833333333334, 83.35, 83.35, 83.24166666666666, 83.24166666666666, 83.625, 83.625, 83.40833333333333, 83.40833333333333, 83.49166666666666, 83.49166666666666, 82.95833333333333, 82.95833333333333, 82.61666666666666, 82.61666666666666, 81.69166666666666, 81.69166666666666, 81.4, 81.4, 81.78333333333333, 81.78333333333333, 81.56666666666666, 81.56666666666666, 82.70833333333333, 82.70833333333333, 81.85, 81.85, 81.94166666666666, 81.94166666666666, 82.16666666666667, 82.16666666666667, 82.05833333333334, 82.05833333333334, 81.975, 81.975, 82.03333333333333, 82.03333333333333, 82.175, 82.175, 82.73333333333333, 82.73333333333333, 82.36666666666666, 82.36666666666666, 82.51666666666667, 82.51666666666667, 82.36666666666666, 82.36666666666666, 82.05833333333334, 82.05833333333334, 82.73333333333333, 82.73333333333333, 82.83333333333333, 82.83333333333333, 82.14166666666667, 82.14166666666667, 81.96666666666667, 81.96666666666667, 83.30833333333334, 83.30833333333334, 83.45, 83.45, 84.25, 84.25, 84.08333333333333, 84.08333333333333, 84.15, 84.15, 83.68333333333334, 83.68333333333334, 83.7, 83.7, 83.94166666666666, 83.94166666666666, 84.025, 84.025, 83.21666666666667, 83.21666666666667, 83.46666666666667, 83.46666666666667, 82.6, 82.6, 82.31666666666666, 82.31666666666666, 82.45, 82.45, 82.91666666666667, 82.91666666666667, 82.08333333333333, 82.08333333333333, 82.625, 82.625, 82.69166666666666, 82.69166666666666, 82.43333333333334, 82.43333333333334, 83.01666666666667, 83.01666666666667]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 8, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.546, Test loss: 2.044, Test accuracy: 24.58
Round   1, Train loss: 1.012, Test loss: 2.255, Test accuracy: 31.62
Round   2, Train loss: 1.045, Test loss: 1.484, Test accuracy: 41.13
Round   3, Train loss: 1.031, Test loss: 1.213, Test accuracy: 52.75
Round   4, Train loss: 0.920, Test loss: 1.199, Test accuracy: 56.52
Round   5, Train loss: 0.926, Test loss: 1.081, Test accuracy: 58.65
Round   6, Train loss: 0.878, Test loss: 0.994, Test accuracy: 62.92
Round   7, Train loss: 0.911, Test loss: 0.895, Test accuracy: 65.38
Round   8, Train loss: 0.781, Test loss: 0.839, Test accuracy: 67.27
Round   9, Train loss: 0.752, Test loss: 0.750, Test accuracy: 71.07
Round  10, Train loss: 0.794, Test loss: 0.711, Test accuracy: 73.10
Round  11, Train loss: 0.816, Test loss: 0.694, Test accuracy: 74.58
Round  12, Train loss: 0.743, Test loss: 0.690, Test accuracy: 74.50
Round  13, Train loss: 0.625, Test loss: 0.684, Test accuracy: 75.28
Round  14, Train loss: 0.693, Test loss: 0.682, Test accuracy: 74.62
Round  15, Train loss: 0.830, Test loss: 0.664, Test accuracy: 76.56
Round  16, Train loss: 0.812, Test loss: 0.645, Test accuracy: 76.77
Round  17, Train loss: 0.634, Test loss: 0.623, Test accuracy: 78.09
Round  18, Train loss: 0.713, Test loss: 0.611, Test accuracy: 78.73
Round  19, Train loss: 0.694, Test loss: 0.606, Test accuracy: 79.12
Round  20, Train loss: 0.626, Test loss: 0.583, Test accuracy: 79.59
Round  21, Train loss: 0.612, Test loss: 0.592, Test accuracy: 80.08
Round  22, Train loss: 0.637, Test loss: 0.570, Test accuracy: 79.94
Round  23, Train loss: 0.638, Test loss: 0.556, Test accuracy: 80.74
Round  24, Train loss: 0.778, Test loss: 0.548, Test accuracy: 81.39
Round  25, Train loss: 0.738, Test loss: 0.555, Test accuracy: 81.65
Round  26, Train loss: 0.774, Test loss: 0.545, Test accuracy: 82.01
Round  27, Train loss: 0.561, Test loss: 0.548, Test accuracy: 82.42
Round  28, Train loss: 0.458, Test loss: 0.536, Test accuracy: 82.78
Round  29, Train loss: 0.642, Test loss: 0.534, Test accuracy: 82.59
Round  30, Train loss: 0.679, Test loss: 0.529, Test accuracy: 82.39
Round  31, Train loss: 0.500, Test loss: 0.537, Test accuracy: 82.17
Round  32, Train loss: 0.693, Test loss: 0.518, Test accuracy: 82.55
Round  33, Train loss: 0.787, Test loss: 0.517, Test accuracy: 82.58
Round  34, Train loss: 0.709, Test loss: 0.509, Test accuracy: 82.82
Round  35, Train loss: 0.754, Test loss: 0.506, Test accuracy: 83.29
Round  36, Train loss: 0.469, Test loss: 0.499, Test accuracy: 83.57
Round  37, Train loss: 0.548, Test loss: 0.490, Test accuracy: 83.50
Round  38, Train loss: 0.630, Test loss: 0.501, Test accuracy: 83.24
Round  39, Train loss: 0.545, Test loss: 0.494, Test accuracy: 83.25
Round  40, Train loss: 0.401, Test loss: 0.491, Test accuracy: 84.06
Round  41, Train loss: 0.687, Test loss: 0.501, Test accuracy: 83.58
Round  42, Train loss: 0.669, Test loss: 0.495, Test accuracy: 83.94
Round  43, Train loss: 0.523, Test loss: 0.481, Test accuracy: 83.84
Round  44, Train loss: 0.442, Test loss: 0.477, Test accuracy: 83.83
Round  45, Train loss: 0.575, Test loss: 0.484, Test accuracy: 83.94
Round  46, Train loss: 0.528, Test loss: 0.477, Test accuracy: 84.27
Round  47, Train loss: 0.541, Test loss: 0.465, Test accuracy: 84.51
Round  48, Train loss: 0.468, Test loss: 0.459, Test accuracy: 84.78
Round  49, Train loss: 0.590, Test loss: 0.465, Test accuracy: 84.61
Round  50, Train loss: 0.480, Test loss: 0.467, Test accuracy: 84.70
Round  51, Train loss: 0.514, Test loss: 0.465, Test accuracy: 84.72
Round  52, Train loss: 0.531, Test loss: 0.473, Test accuracy: 84.23
Round  53, Train loss: 0.423, Test loss: 0.461, Test accuracy: 84.56
Round  54, Train loss: 0.513, Test loss: 0.465, Test accuracy: 84.71
Round  55, Train loss: 0.329, Test loss: 0.453, Test accuracy: 85.17
Round  56, Train loss: 0.342, Test loss: 0.454, Test accuracy: 84.83
Round  57, Train loss: 0.518, Test loss: 0.450, Test accuracy: 85.01
Round  58, Train loss: 0.499, Test loss: 0.455, Test accuracy: 85.15
Round  59, Train loss: 0.443, Test loss: 0.451, Test accuracy: 85.01
Round  60, Train loss: 0.487, Test loss: 0.458, Test accuracy: 84.83
Round  61, Train loss: 0.373, Test loss: 0.451, Test accuracy: 84.88
Round  62, Train loss: 0.594, Test loss: 0.439, Test accuracy: 85.30
Round  63, Train loss: 0.452, Test loss: 0.438, Test accuracy: 85.48
Round  64, Train loss: 0.395, Test loss: 0.431, Test accuracy: 85.74
Round  65, Train loss: 0.374, Test loss: 0.429, Test accuracy: 85.80
Round  66, Train loss: 0.424, Test loss: 0.424, Test accuracy: 85.97
Round  67, Train loss: 0.512, Test loss: 0.432, Test accuracy: 85.58
Round  68, Train loss: 0.529, Test loss: 0.437, Test accuracy: 85.42
Round  69, Train loss: 0.406, Test loss: 0.445, Test accuracy: 85.09
Round  70, Train loss: 0.433, Test loss: 0.439, Test accuracy: 85.07
Round  71, Train loss: 0.426, Test loss: 0.440, Test accuracy: 85.35
Round  72, Train loss: 0.423, Test loss: 0.436, Test accuracy: 85.43
Round  73, Train loss: 0.601, Test loss: 0.440, Test accuracy: 85.28
Round  74, Train loss: 0.444, Test loss: 0.439, Test accuracy: 85.50
Round  75, Train loss: 0.450, Test loss: 0.428, Test accuracy: 85.67
Round  76, Train loss: 0.427, Test loss: 0.440, Test accuracy: 85.28
Round  77, Train loss: 0.472, Test loss: 0.436, Test accuracy: 85.41
Round  78, Train loss: 0.388, Test loss: 0.432, Test accuracy: 85.63
Round  79, Train loss: 0.573, Test loss: 0.432, Test accuracy: 85.57
Round  80, Train loss: 0.407, Test loss: 0.428, Test accuracy: 85.53
Round  81, Train loss: 0.372, Test loss: 0.433, Test accuracy: 85.41
Round  82, Train loss: 0.445, Test loss: 0.434, Test accuracy: 85.81
Round  83, Train loss: 0.353, Test loss: 0.435, Test accuracy: 85.67
Round  84, Train loss: 0.374, Test loss: 0.436, Test accuracy: 85.88
Round  85, Train loss: 0.325, Test loss: 0.433, Test accuracy: 85.98
Round  86, Train loss: 0.420, Test loss: 0.427, Test accuracy: 86.06
Round  87, Train loss: 0.416, Test loss: 0.425, Test accuracy: 86.22
Round  88, Train loss: 0.249, Test loss: 0.430, Test accuracy: 85.88
Round  89, Train loss: 0.413, Test loss: 0.423, Test accuracy: 86.24
Round  90, Train loss: 0.350, Test loss: 0.432, Test accuracy: 86.20
Round  91, Train loss: 0.386, Test loss: 0.425, Test accuracy: 86.21
Round  92, Train loss: 0.411, Test loss: 0.425, Test accuracy: 86.31
Round  93, Train loss: 0.362, Test loss: 0.427, Test accuracy: 86.28
Round  94, Train loss: 0.402, Test loss: 0.437, Test accuracy: 85.59
Round  95, Train loss: 0.415, Test loss: 0.440, Test accuracy: 85.47
Round  96, Train loss: 0.328, Test loss: 0.438, Test accuracy: 85.53
Round  97, Train loss: 0.317, Test loss: 0.437, Test accuracy: 85.38
Round  98, Train loss: 0.507, Test loss: 0.441, Test accuracy: 85.68
Round  99, Train loss: 0.402, Test loss: 0.439, Test accuracy: 85.52
Final Round, Train loss: 0.339, Test loss: 0.430, Test accuracy: 85.40
Average accuracy final 10 rounds: 85.8175
1349.2961571216583
[2.0865676403045654, 3.750890016555786, 5.4270501136779785, 7.1249799728393555, 8.850856304168701, 10.594590902328491, 12.318331718444824, 14.040998458862305, 15.730850458145142, 17.42257523536682, 19.091397285461426, 20.76919412612915, 22.449589729309082, 24.124755144119263, 25.838287115097046, 27.515256643295288, 29.167690753936768, 30.827441930770874, 32.522064208984375, 34.21181130409241, 35.96035027503967, 37.69576668739319, 39.39873552322388, 41.08014392852783, 42.77694034576416, 44.473512411117554, 46.13249206542969, 47.812862157821655, 49.49105381965637, 51.1398766040802, 52.78975248336792, 54.44273924827576, 56.10704159736633, 57.827232837677, 59.527053356170654, 61.20014786720276, 62.886619567871094, 64.55224823951721, 66.19484496116638, 67.83421063423157, 69.47318172454834, 71.15458488464355, 72.90479230880737, 74.60657477378845, 76.26978373527527, 77.96589660644531, 79.69368529319763, 81.39420819282532, 83.15854167938232, 84.90110182762146, 86.68740582466125, 88.39260005950928, 90.06940293312073, 91.75356411933899, 93.40289115905762, 95.06356072425842, 96.73068237304688, 98.41168785095215, 100.0805835723877, 101.74846982955933, 103.41350865364075, 105.06367707252502, 106.7154393196106, 108.37695908546448, 110.01387667655945, 111.65145802497864, 113.29320979118347, 114.95667576789856, 116.65235638618469, 118.34301209449768, 120.07591390609741, 121.82134962081909, 123.57525682449341, 125.31304812431335, 127.02441930770874, 128.72747230529785, 130.40442824363708, 132.0916543006897, 133.7540786266327, 135.44709491729736, 137.154465675354, 138.80989718437195, 140.4741814136505, 142.13625311851501, 143.79821133613586, 145.49621534347534, 147.15757989883423, 148.81311178207397, 150.4733591079712, 152.16621661186218, 153.88312196731567, 155.552330493927, 157.2192096710205, 158.88444566726685, 160.56430554389954, 162.26510453224182, 163.97084975242615, 165.69481706619263, 167.42514443397522, 169.1483976840973, 171.41897773742676]
[24.583333333333332, 31.625, 41.13333333333333, 52.75, 56.516666666666666, 58.65, 62.916666666666664, 65.38333333333334, 67.26666666666667, 71.06666666666666, 73.1, 74.58333333333333, 74.5, 75.28333333333333, 74.61666666666666, 76.55833333333334, 76.76666666666667, 78.09166666666667, 78.73333333333333, 79.11666666666666, 79.59166666666667, 80.075, 79.94166666666666, 80.74166666666666, 81.39166666666667, 81.65, 82.00833333333334, 82.425, 82.78333333333333, 82.59166666666667, 82.39166666666667, 82.16666666666667, 82.55, 82.575, 82.81666666666666, 83.29166666666667, 83.56666666666666, 83.5, 83.24166666666666, 83.25, 84.05833333333334, 83.575, 83.94166666666666, 83.84166666666667, 83.83333333333333, 83.94166666666666, 84.26666666666667, 84.50833333333334, 84.775, 84.60833333333333, 84.7, 84.71666666666667, 84.23333333333333, 84.55833333333334, 84.70833333333333, 85.175, 84.83333333333333, 85.00833333333334, 85.15, 85.00833333333334, 84.83333333333333, 84.875, 85.3, 85.48333333333333, 85.74166666666666, 85.8, 85.96666666666667, 85.58333333333333, 85.41666666666667, 85.09166666666667, 85.06666666666666, 85.35, 85.43333333333334, 85.28333333333333, 85.5, 85.66666666666667, 85.275, 85.40833333333333, 85.63333333333334, 85.56666666666666, 85.525, 85.40833333333333, 85.80833333333334, 85.66666666666667, 85.875, 85.98333333333333, 86.05833333333334, 86.21666666666667, 85.875, 86.24166666666666, 86.2, 86.20833333333333, 86.30833333333334, 86.28333333333333, 85.59166666666667, 85.475, 85.53333333333333, 85.375, 85.68333333333334, 85.51666666666667, 85.4]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 11, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  17.0800
Round 1 global test acc  16.2600
Round 2 global test acc  16.4600
Round 3 global test acc  27.4800
Round 4 global test acc  20.0100
Round 5 global test acc  21.1600
Round 6 global test acc  27.7200
Round 7 global test acc  20.2400
Round 8 global test acc  26.9000
Round 9 global test acc  28.1000
Round 10 global test acc  25.0600
Round 11 global test acc  25.2700
Round 12 global test acc  25.5600
Round 13 global test acc  22.9900
Round 14 global test acc  28.9800
Round 15 global test acc  25.0200
Round 16 global test acc  27.7300
Round 17 global test acc  34.4500
Round 18 global test acc  26.2300
Round 19 global test acc  35.2800
Round 20 global test acc  28.2500
Round 21 global test acc  25.6400
Round 22 global test acc  21.5400
Round 23 global test acc  32.2700
Round 24 global test acc  30.2400
Round 25 global test acc  30.9100
Round 26 global test acc  21.9500
Round 27 global test acc  27.2200
Round 28 global test acc  30.6500
Round 29 global test acc  30.9600
Round 30 global test acc  28.6000
Round 31 global test acc  25.3300
Round 32 global test acc  22.8200
Round 33 global test acc  23.3900
Round 34 global test acc  29.5600
Round 35 global test acc  33.2600
Round 36 global test acc  25.7800
Round 37 global test acc  30.1700
Round 38 global test acc  34.8800
Round 39 global test acc  38.9300
Round 40 global test acc  40.0400
Round 41 global test acc  39.5800
Round 42 global test acc  29.5300
Round 43 global test acc  37.9500
Round 44 global test acc  26.8700
Round 45 global test acc  29.1000
Round 46 global test acc  27.4100
Round 47 global test acc  28.3800
Round 48 global test acc  30.6900
Round 49 global test acc  24.0600
Round 50 global test acc  26.8600
Round 51 global test acc  31.2800
Round 52 global test acc  41.2800
Round 53 global test acc  29.6500
Round 54 global test acc  28.4600
Round 55 global test acc  31.2800
Round 56 global test acc  28.9300
Round 57 global test acc  34.7600
Round 58 global test acc  30.3300
Round 59 global test acc  27.2400
Round 60 global test acc  35.7600
Round 61 global test acc  34.3600
Round 62 global test acc  36.1900
Round 63 global test acc  31.3200
Round 64 global test acc  27.7600
Round 65 global test acc  30.8800
Round 66 global test acc  32.8200
Round 67 global test acc  30.1300
Round 68 global test acc  38.9700
Round 69 global test acc  31.2900
Round 70 global test acc  38.2300
Round 71 global test acc  41.1300
Round 72 global test acc  31.2100
Round 73 global test acc  30.2000
Round 74 global test acc  35.0600
Round 75 global test acc  32.8100
Round 76 global test acc  34.1800
Round 77 global test acc  34.6400
Round 78 global test acc  36.5100
Round 79 global test acc  27.7900
Round 80 global test acc  24.9000
Round 81 global test acc  23.9400
Round 82 global test acc  21.6600
Round 83 global test acc  20.0200
Round 84 global test acc  20.1400
Round 85 global test acc  19.5700
Round 86 global test acc  19.4200
Round 87 global test acc  20.3200
Round 88 global test acc  20.8100
Round 89 global test acc  19.9200
Round 90 global test acc  18.8700
Round 91 global test acc  18.2500
Round 92 global test acc  19.3100
Round 93 global test acc  18.7800
Round 94 global test acc  18.1100
Round 95 global test acc  17.8800
Round 96 global test acc  17.4400
Round 97 global test acc  17.1100
Round 98 global test acc  16.7400
Round 99 global test acc  15.9900
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 15, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.645, Test loss: 2.100, Test accuracy: 27.49
Round   1, Train loss: 1.066, Test loss: 2.249, Test accuracy: 32.11
Round   2, Train loss: 0.946, Test loss: 1.525, Test accuracy: 40.49
Round   3, Train loss: 0.901, Test loss: 1.285, Test accuracy: 50.18
Round   4, Train loss: 0.812, Test loss: 1.217, Test accuracy: 55.55
Round   5, Train loss: 0.922, Test loss: 1.028, Test accuracy: 60.27
Round   6, Train loss: 0.813, Test loss: 0.965, Test accuracy: 63.73
Round   7, Train loss: 0.779, Test loss: 0.888, Test accuracy: 65.48
Round   8, Train loss: 0.649, Test loss: 0.824, Test accuracy: 65.92
Round   9, Train loss: 0.704, Test loss: 0.736, Test accuracy: 69.78
Round  10, Train loss: 0.719, Test loss: 0.697, Test accuracy: 72.33
Round  11, Train loss: 0.710, Test loss: 0.672, Test accuracy: 73.44
Round  12, Train loss: 0.732, Test loss: 0.662, Test accuracy: 74.45
Round  13, Train loss: 0.638, Test loss: 0.673, Test accuracy: 74.91
Round  14, Train loss: 0.632, Test loss: 0.656, Test accuracy: 75.59
Round  15, Train loss: 0.709, Test loss: 0.627, Test accuracy: 76.26
Round  16, Train loss: 0.777, Test loss: 0.596, Test accuracy: 78.58
Round  17, Train loss: 0.660, Test loss: 0.558, Test accuracy: 79.40
Round  18, Train loss: 0.607, Test loss: 0.568, Test accuracy: 78.91
Round  19, Train loss: 0.642, Test loss: 0.564, Test accuracy: 78.32
Round  20, Train loss: 0.609, Test loss: 0.545, Test accuracy: 79.42
Round  21, Train loss: 0.541, Test loss: 0.598, Test accuracy: 77.55
Round  22, Train loss: 0.643, Test loss: 0.557, Test accuracy: 79.49
Round  23, Train loss: 0.686, Test loss: 0.532, Test accuracy: 81.04
Round  24, Train loss: 0.690, Test loss: 0.521, Test accuracy: 81.15
Round  25, Train loss: 0.544, Test loss: 0.515, Test accuracy: 81.58
Round  26, Train loss: 0.525, Test loss: 0.507, Test accuracy: 81.85
Round  27, Train loss: 0.600, Test loss: 0.509, Test accuracy: 82.33
Round  28, Train loss: 0.506, Test loss: 0.504, Test accuracy: 81.44
Round  29, Train loss: 0.630, Test loss: 0.492, Test accuracy: 82.30
Round  30, Train loss: 0.463, Test loss: 0.492, Test accuracy: 82.22
Round  31, Train loss: 0.512, Test loss: 0.484, Test accuracy: 82.67
Round  32, Train loss: 0.620, Test loss: 0.493, Test accuracy: 82.26
Round  33, Train loss: 0.608, Test loss: 0.484, Test accuracy: 82.53
Round  34, Train loss: 0.598, Test loss: 0.477, Test accuracy: 82.67
Round  35, Train loss: 0.466, Test loss: 0.473, Test accuracy: 83.05
Round  36, Train loss: 0.588, Test loss: 0.479, Test accuracy: 82.42
Round  37, Train loss: 0.509, Test loss: 0.469, Test accuracy: 82.56
Round  38, Train loss: 0.402, Test loss: 0.474, Test accuracy: 83.00
Round  39, Train loss: 0.600, Test loss: 0.463, Test accuracy: 83.03
Round  40, Train loss: 0.414, Test loss: 0.476, Test accuracy: 82.77
Round  41, Train loss: 0.432, Test loss: 0.464, Test accuracy: 83.12
Round  42, Train loss: 0.428, Test loss: 0.460, Test accuracy: 83.57
Round  43, Train loss: 0.478, Test loss: 0.458, Test accuracy: 83.42
Round  44, Train loss: 0.413, Test loss: 0.459, Test accuracy: 83.40
Round  45, Train loss: 0.507, Test loss: 0.459, Test accuracy: 83.51
Round  46, Train loss: 0.587, Test loss: 0.452, Test accuracy: 84.11
Round  47, Train loss: 0.467, Test loss: 0.452, Test accuracy: 84.00
Round  48, Train loss: 0.552, Test loss: 0.448, Test accuracy: 84.34
Round  49, Train loss: 0.398, Test loss: 0.449, Test accuracy: 84.50
Round  50, Train loss: 0.450, Test loss: 0.454, Test accuracy: 83.64
Round  51, Train loss: 0.414, Test loss: 0.452, Test accuracy: 84.13
Round  52, Train loss: 0.436, Test loss: 0.448, Test accuracy: 84.91
Round  53, Train loss: 0.477, Test loss: 0.440, Test accuracy: 84.56
Round  54, Train loss: 0.504, Test loss: 0.431, Test accuracy: 84.82
Round  55, Train loss: 0.354, Test loss: 0.431, Test accuracy: 84.82
Round  56, Train loss: 0.464, Test loss: 0.432, Test accuracy: 84.72
Round  57, Train loss: 0.484, Test loss: 0.430, Test accuracy: 85.10
Round  58, Train loss: 0.458, Test loss: 0.424, Test accuracy: 85.13
Round  59, Train loss: 0.341, Test loss: 0.433, Test accuracy: 84.34
Round  60, Train loss: 0.496, Test loss: 0.426, Test accuracy: 84.88
Round  61, Train loss: 0.311, Test loss: 0.424, Test accuracy: 84.88
Round  62, Train loss: 0.527, Test loss: 0.425, Test accuracy: 84.92
Round  63, Train loss: 0.479, Test loss: 0.426, Test accuracy: 85.30
Round  64, Train loss: 0.362, Test loss: 0.429, Test accuracy: 85.17
Round  65, Train loss: 0.436, Test loss: 0.427, Test accuracy: 85.06
Round  66, Train loss: 0.447, Test loss: 0.426, Test accuracy: 84.90
Round  67, Train loss: 0.418, Test loss: 0.427, Test accuracy: 84.88
Round  68, Train loss: 0.358, Test loss: 0.418, Test accuracy: 85.02
Round  69, Train loss: 0.492, Test loss: 0.414, Test accuracy: 85.62
Round  70, Train loss: 0.422, Test loss: 0.410, Test accuracy: 85.66
Round  71, Train loss: 0.410, Test loss: 0.408, Test accuracy: 86.11
Round  72, Train loss: 0.384, Test loss: 0.413, Test accuracy: 85.58
Round  73, Train loss: 0.465, Test loss: 0.414, Test accuracy: 85.70
Round  74, Train loss: 0.351, Test loss: 0.404, Test accuracy: 86.25
Round  75, Train loss: 0.330, Test loss: 0.404, Test accuracy: 86.22
Round  76, Train loss: 0.330, Test loss: 0.400, Test accuracy: 86.39
Round  77, Train loss: 0.490, Test loss: 0.402, Test accuracy: 85.80
Round  78, Train loss: 0.367, Test loss: 0.408, Test accuracy: 85.78
Round  79, Train loss: 0.303, Test loss: 0.404, Test accuracy: 85.92
Round  80, Train loss: 0.411, Test loss: 0.411, Test accuracy: 85.72
Round  81, Train loss: 0.348, Test loss: 0.413, Test accuracy: 85.68
Round  82, Train loss: 0.373, Test loss: 0.421, Test accuracy: 85.57
Round  83, Train loss: 0.369, Test loss: 0.418, Test accuracy: 85.63
Round  84, Train loss: 0.300, Test loss: 0.414, Test accuracy: 85.81
Round  85, Train loss: 0.208, Test loss: 0.420, Test accuracy: 85.58
Round  86, Train loss: 0.321, Test loss: 0.425, Test accuracy: 85.25
Round  87, Train loss: 0.337, Test loss: 0.423, Test accuracy: 85.19
Round  88, Train loss: 0.371, Test loss: 0.418, Test accuracy: 85.53
Round  89, Train loss: 0.404, Test loss: 0.407, Test accuracy: 86.12
Round  90, Train loss: 0.278, Test loss: 0.414, Test accuracy: 85.69
Round  91, Train loss: 0.430, Test loss: 0.419, Test accuracy: 85.59
Round  92, Train loss: 0.368, Test loss: 0.413, Test accuracy: 86.01
Round  93, Train loss: 0.406, Test loss: 0.412, Test accuracy: 86.15
Round  94, Train loss: 0.313, Test loss: 0.410, Test accuracy: 86.14
Round  95, Train loss: 0.209, Test loss: 0.403, Test accuracy: 86.18
Round  96, Train loss: 0.279, Test loss: 0.403, Test accuracy: 86.48
Round  97, Train loss: 0.327, Test loss: 0.404, Test accuracy: 86.22
Round  98, Train loss: 0.341, Test loss: 0.403, Test accuracy: 86.51
Round  99, Train loss: 0.276, Test loss: 0.406, Test accuracy: 86.28
Final Round, Train loss: 0.301, Test loss: 0.410, Test accuracy: 86.10
Average accuracy final 10 rounds: 86.12583333333333
1371.9408795833588
[2.157722234725952, 3.9337029457092285, 5.697027683258057, 7.406058073043823, 9.106378555297852, 10.857872009277344, 12.656728267669678, 14.453023672103882, 16.2260959148407, 17.959556102752686, 19.709830045700073, 21.36812686920166, 23.034373998641968, 24.724108934402466, 26.403655767440796, 28.091856718063354, 29.782668828964233, 31.451416492462158, 33.136255502700806, 34.84559988975525, 36.564993143081665, 38.27062273025513, 40.019165992736816, 41.726197242736816, 43.43229150772095, 45.14071989059448, 46.86389780044556, 48.56644058227539, 50.256279945373535, 51.91998648643494, 53.5780611038208, 55.245126247406006, 56.956135749816895, 58.64867949485779, 60.3107750415802, 62.000593185424805, 63.85666060447693, 65.6614179611206, 67.44734692573547, 69.26855111122131, 71.0710802078247, 72.83184313774109, 74.6208848953247, 76.37184143066406, 78.07406449317932, 79.84308433532715, 81.62959790229797, 83.39402770996094, 85.15633487701416, 86.8275191783905, 88.48835372924805, 90.17638516426086, 91.8727056980133, 93.52698707580566, 95.1857123374939, 96.8772554397583, 98.61327838897705, 100.36357975006104, 102.0791449546814, 103.77842092514038, 105.47827100753784, 107.24684953689575, 108.99279594421387, 110.68074107170105, 112.42259240150452, 114.13451170921326, 115.78747248649597, 117.46470284461975, 119.18757843971252, 120.96240520477295, 122.70793223381042, 124.45874643325806, 126.17088174819946, 128.00601887702942, 129.7748532295227, 131.49309825897217, 133.1949405670166, 134.93982100486755, 136.62289309501648, 138.39192843437195, 140.11748552322388, 141.9147868156433, 143.7092444896698, 145.5631067752838, 147.34814834594727, 149.09845995903015, 150.79410433769226, 152.4888858795166, 154.21906781196594, 155.91534948349, 157.60369229316711, 159.27025198936462, 161.00375962257385, 162.67339277267456, 164.36593508720398, 166.0710437297821, 167.76055717468262, 169.4116244316101, 171.11455988883972, 172.8464059829712, 175.15661001205444]
[27.491666666666667, 32.108333333333334, 40.49166666666667, 50.18333333333333, 55.55, 60.266666666666666, 63.725, 65.48333333333333, 65.925, 69.78333333333333, 72.33333333333333, 73.44166666666666, 74.45, 74.90833333333333, 75.59166666666667, 76.25833333333334, 78.58333333333333, 79.4, 78.90833333333333, 78.31666666666666, 79.425, 77.55, 79.49166666666666, 81.04166666666667, 81.15, 81.575, 81.85, 82.325, 81.44166666666666, 82.3, 82.21666666666667, 82.66666666666667, 82.25833333333334, 82.53333333333333, 82.66666666666667, 83.05, 82.41666666666667, 82.55833333333334, 83.0, 83.03333333333333, 82.76666666666667, 83.125, 83.56666666666666, 83.41666666666667, 83.4, 83.50833333333334, 84.10833333333333, 84.0, 84.34166666666667, 84.5, 83.64166666666667, 84.13333333333334, 84.90833333333333, 84.55833333333334, 84.81666666666666, 84.81666666666666, 84.725, 85.1, 85.13333333333334, 84.34166666666667, 84.875, 84.875, 84.91666666666667, 85.3, 85.175, 85.05833333333334, 84.9, 84.88333333333334, 85.01666666666667, 85.61666666666666, 85.65833333333333, 86.10833333333333, 85.58333333333333, 85.7, 86.25, 86.21666666666667, 86.39166666666667, 85.8, 85.78333333333333, 85.91666666666667, 85.725, 85.68333333333334, 85.56666666666666, 85.63333333333334, 85.80833333333334, 85.575, 85.25, 85.19166666666666, 85.525, 86.125, 85.69166666666666, 85.59166666666667, 86.00833333333334, 86.15, 86.14166666666667, 86.18333333333334, 86.48333333333333, 86.225, 86.50833333333334, 86.275, 86.1]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.6 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 8, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.614, Test loss: 2.005, Test accuracy: 26.99
Round   1, Train loss: 1.066, Test loss: 2.103, Test accuracy: 33.42
Round   2, Train loss: 1.144, Test loss: 1.514, Test accuracy: 44.42
Round   3, Train loss: 0.866, Test loss: 1.301, Test accuracy: 50.56
Round   4, Train loss: 0.848, Test loss: 1.210, Test accuracy: 53.78
Round   5, Train loss: 0.798, Test loss: 1.060, Test accuracy: 59.45
Round   6, Train loss: 0.941, Test loss: 0.975, Test accuracy: 63.24
Round   7, Train loss: 0.783, Test loss: 0.835, Test accuracy: 67.75
Round   8, Train loss: 0.703, Test loss: 0.804, Test accuracy: 67.92
Round   9, Train loss: 0.809, Test loss: 0.702, Test accuracy: 72.50
Round  10, Train loss: 0.838, Test loss: 0.696, Test accuracy: 74.00
Round  11, Train loss: 0.763, Test loss: 0.661, Test accuracy: 74.65
Round  12, Train loss: 0.708, Test loss: 0.644, Test accuracy: 75.90
Round  13, Train loss: 0.706, Test loss: 0.651, Test accuracy: 76.15
Round  14, Train loss: 0.706, Test loss: 0.633, Test accuracy: 76.36
Round  15, Train loss: 0.617, Test loss: 0.618, Test accuracy: 77.53
Round  16, Train loss: 0.686, Test loss: 0.609, Test accuracy: 78.12
Round  17, Train loss: 0.635, Test loss: 0.567, Test accuracy: 78.69
Round  18, Train loss: 0.639, Test loss: 0.569, Test accuracy: 78.90
Round  19, Train loss: 0.734, Test loss: 0.576, Test accuracy: 78.73
Round  20, Train loss: 0.614, Test loss: 0.558, Test accuracy: 79.66
Round  21, Train loss: 0.574, Test loss: 0.556, Test accuracy: 79.16
Round  22, Train loss: 0.677, Test loss: 0.539, Test accuracy: 80.08
Round  23, Train loss: 0.634, Test loss: 0.529, Test accuracy: 81.27
Round  24, Train loss: 0.621, Test loss: 0.532, Test accuracy: 80.22
Round  25, Train loss: 0.637, Test loss: 0.519, Test accuracy: 81.04
Round  26, Train loss: 0.602, Test loss: 0.521, Test accuracy: 81.14
Round  27, Train loss: 0.557, Test loss: 0.523, Test accuracy: 80.77
Round  28, Train loss: 0.585, Test loss: 0.519, Test accuracy: 80.88
Round  29, Train loss: 0.552, Test loss: 0.518, Test accuracy: 81.78
Round  30, Train loss: 0.516, Test loss: 0.508, Test accuracy: 82.26
Round  31, Train loss: 0.578, Test loss: 0.508, Test accuracy: 82.48
Round  32, Train loss: 0.559, Test loss: 0.504, Test accuracy: 82.33
Round  33, Train loss: 0.579, Test loss: 0.498, Test accuracy: 82.50
Round  34, Train loss: 0.520, Test loss: 0.496, Test accuracy: 82.17
Round  35, Train loss: 0.508, Test loss: 0.476, Test accuracy: 83.30
Round  36, Train loss: 0.593, Test loss: 0.474, Test accuracy: 83.22
Round  37, Train loss: 0.464, Test loss: 0.475, Test accuracy: 83.21
Round  38, Train loss: 0.542, Test loss: 0.469, Test accuracy: 83.33
Round  39, Train loss: 0.550, Test loss: 0.473, Test accuracy: 83.47
Round  40, Train loss: 0.490, Test loss: 0.462, Test accuracy: 83.49
Round  41, Train loss: 0.478, Test loss: 0.461, Test accuracy: 83.50
Round  42, Train loss: 0.551, Test loss: 0.448, Test accuracy: 84.16
Round  43, Train loss: 0.421, Test loss: 0.445, Test accuracy: 84.27
Round  44, Train loss: 0.511, Test loss: 0.450, Test accuracy: 83.92
Round  45, Train loss: 0.493, Test loss: 0.443, Test accuracy: 84.61
Round  46, Train loss: 0.458, Test loss: 0.449, Test accuracy: 84.47
Round  47, Train loss: 0.459, Test loss: 0.430, Test accuracy: 84.99
Round  48, Train loss: 0.501, Test loss: 0.432, Test accuracy: 85.40
Round  49, Train loss: 0.427, Test loss: 0.430, Test accuracy: 84.85
Round  50, Train loss: 0.485, Test loss: 0.429, Test accuracy: 85.07
Round  51, Train loss: 0.458, Test loss: 0.427, Test accuracy: 85.10
Round  52, Train loss: 0.477, Test loss: 0.427, Test accuracy: 85.38
Round  53, Train loss: 0.414, Test loss: 0.431, Test accuracy: 85.18
Round  54, Train loss: 0.499, Test loss: 0.439, Test accuracy: 85.13
Round  55, Train loss: 0.415, Test loss: 0.423, Test accuracy: 85.58
Round  56, Train loss: 0.504, Test loss: 0.434, Test accuracy: 85.12
Round  57, Train loss: 0.398, Test loss: 0.435, Test accuracy: 84.87
Round  58, Train loss: 0.496, Test loss: 0.426, Test accuracy: 85.09
Round  59, Train loss: 0.398, Test loss: 0.424, Test accuracy: 85.46
Round  60, Train loss: 0.386, Test loss: 0.433, Test accuracy: 84.71
Round  61, Train loss: 0.372, Test loss: 0.434, Test accuracy: 84.66
Round  62, Train loss: 0.491, Test loss: 0.415, Test accuracy: 85.47
Round  63, Train loss: 0.480, Test loss: 0.419, Test accuracy: 85.37
Round  64, Train loss: 0.335, Test loss: 0.422, Test accuracy: 85.51
Round  65, Train loss: 0.475, Test loss: 0.419, Test accuracy: 85.23
Round  66, Train loss: 0.406, Test loss: 0.412, Test accuracy: 85.29
Round  67, Train loss: 0.313, Test loss: 0.407, Test accuracy: 85.59
Round  68, Train loss: 0.301, Test loss: 0.408, Test accuracy: 85.48
Round  69, Train loss: 0.497, Test loss: 0.412, Test accuracy: 85.41
Round  70, Train loss: 0.454, Test loss: 0.409, Test accuracy: 85.66
Round  71, Train loss: 0.443, Test loss: 0.408, Test accuracy: 85.59
Round  72, Train loss: 0.405, Test loss: 0.405, Test accuracy: 85.83
Round  73, Train loss: 0.375, Test loss: 0.409, Test accuracy: 85.71
Round  74, Train loss: 0.272, Test loss: 0.404, Test accuracy: 85.85
Round  75, Train loss: 0.318, Test loss: 0.409, Test accuracy: 85.83
Round  76, Train loss: 0.409, Test loss: 0.404, Test accuracy: 85.81
Round  77, Train loss: 0.421, Test loss: 0.406, Test accuracy: 85.90
Round  78, Train loss: 0.376, Test loss: 0.403, Test accuracy: 86.31
Round  79, Train loss: 0.291, Test loss: 0.397, Test accuracy: 86.24
Round  80, Train loss: 0.462, Test loss: 0.406, Test accuracy: 86.21
Round  81, Train loss: 0.350, Test loss: 0.417, Test accuracy: 85.71
Round  82, Train loss: 0.404, Test loss: 0.418, Test accuracy: 85.49
Round  83, Train loss: 0.371, Test loss: 0.426, Test accuracy: 85.15
Round  84, Train loss: 0.343, Test loss: 0.431, Test accuracy: 85.08
Round  85, Train loss: 0.378, Test loss: 0.428, Test accuracy: 84.94
Round  86, Train loss: 0.355, Test loss: 0.417, Test accuracy: 85.57
Round  87, Train loss: 0.353, Test loss: 0.413, Test accuracy: 85.49
Round  88, Train loss: 0.401, Test loss: 0.411, Test accuracy: 85.66
Round  89, Train loss: 0.367, Test loss: 0.404, Test accuracy: 85.69
Round  90, Train loss: 0.296, Test loss: 0.414, Test accuracy: 85.32
Round  91, Train loss: 0.379, Test loss: 0.409, Test accuracy: 85.50
Round  92, Train loss: 0.415, Test loss: 0.398, Test accuracy: 86.05
Round  93, Train loss: 0.307, Test loss: 0.403, Test accuracy: 86.00
Round  94, Train loss: 0.308, Test loss: 0.402, Test accuracy: 86.24
Round  95, Train loss: 0.341, Test loss: 0.401, Test accuracy: 86.08
Round  96, Train loss: 0.279, Test loss: 0.402, Test accuracy: 85.92
Round  97, Train loss: 0.304, Test loss: 0.403, Test accuracy: 85.72
Round  98, Train loss: 0.378, Test loss: 0.401, Test accuracy: 85.89
Round  99, Train loss: 0.425, Test loss: 0.411, Test accuracy: 85.38
Final Round, Train loss: 0.298, Test loss: 0.413, Test accuracy: 85.35
Average accuracy final 10 rounds: 85.81
1905.0202839374542
[2.0049684047698975, 3.6997740268707275, 5.392364263534546, 7.070359945297241, 8.748735427856445, 10.44167184829712, 12.116880416870117, 13.807690620422363, 15.56803584098816, 17.248358488082886, 18.93518304824829, 20.617244720458984, 22.375754356384277, 24.1276113986969, 25.83406639099121, 27.603180646896362, 29.27284860610962, 31.004973888397217, 32.72655534744263, 34.42314624786377, 36.13390350341797, 39.174007415771484, 42.23636078834534, 45.133477449417114, 48.10102105140686, 51.08426880836487, 54.04922389984131, 56.9653959274292, 59.79527831077576, 62.67081117630005, 65.52650499343872, 68.38144207000732, 71.23248887062073, 74.15741491317749, 77.10298705101013, 80.02437663078308, 82.87642240524292, 85.68928146362305, 88.4841992855072, 91.36982440948486, 94.31351208686829, 97.20377922058105, 100.11769604682922, 103.07346558570862, 105.91935348510742, 108.7787733078003, 111.66765832901001, 114.49615788459778, 117.38256216049194, 120.25835824012756, 123.12457799911499, 126.008540391922, 128.8965620994568, 131.6840102672577, 134.50926733016968, 137.32856726646423, 140.1659128665924, 143.037691116333, 145.89720153808594, 148.78942775726318, 151.62766075134277, 154.49063372612, 157.305570602417, 160.1515655517578, 163.05706000328064, 165.96008276939392, 168.8221297264099, 171.70188355445862, 174.53940296173096, 177.38675022125244, 180.3282434940338, 183.32443070411682, 186.2128551006317, 189.13894748687744, 192.0223035812378, 194.98774695396423, 197.91586017608643, 200.71490907669067, 203.5118281841278, 206.3924708366394, 209.42992091178894, 212.4023506641388, 215.30626392364502, 218.12673664093018, 220.95001816749573, 223.7994396686554, 226.6961829662323, 229.55530858039856, 232.4935872554779, 235.3717565536499, 238.2947371006012, 241.24893164634705, 244.18922877311707, 247.04080367088318, 249.95305824279785, 252.90172672271729, 255.87858033180237, 258.7419788837433, 261.5982303619385, 264.4972541332245, 266.75922775268555]
[26.991666666666667, 33.425, 44.425, 50.55833333333333, 53.78333333333333, 59.45, 63.24166666666667, 67.75, 67.925, 72.5, 74.0, 74.65, 75.9, 76.15, 76.35833333333333, 77.525, 78.125, 78.69166666666666, 78.9, 78.73333333333333, 79.65833333333333, 79.15833333333333, 80.08333333333333, 81.26666666666667, 80.225, 81.04166666666667, 81.14166666666667, 80.76666666666667, 80.88333333333334, 81.775, 82.25833333333334, 82.48333333333333, 82.33333333333333, 82.5, 82.175, 83.3, 83.21666666666667, 83.20833333333333, 83.325, 83.475, 83.49166666666666, 83.5, 84.15833333333333, 84.26666666666667, 83.91666666666667, 84.60833333333333, 84.475, 84.99166666666666, 85.4, 84.85, 85.06666666666666, 85.1, 85.375, 85.18333333333334, 85.13333333333334, 85.575, 85.125, 84.86666666666666, 85.09166666666667, 85.45833333333333, 84.70833333333333, 84.65833333333333, 85.46666666666667, 85.36666666666666, 85.50833333333334, 85.23333333333333, 85.29166666666667, 85.59166666666667, 85.48333333333333, 85.40833333333333, 85.65833333333333, 85.59166666666667, 85.83333333333333, 85.70833333333333, 85.85, 85.825, 85.80833333333334, 85.9, 86.30833333333334, 86.24166666666666, 86.20833333333333, 85.70833333333333, 85.49166666666666, 85.15, 85.075, 84.94166666666666, 85.56666666666666, 85.49166666666666, 85.65833333333333, 85.69166666666666, 85.31666666666666, 85.5, 86.05, 86.0, 86.24166666666666, 86.075, 85.91666666666667, 85.725, 85.89166666666667, 85.38333333333334, 85.35]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 201, in get_data_from_file
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_v3(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedrep.py", line 94, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_co_teaching2.py", line 92, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "RFL.py", line 62, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac.py", line 93, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
Traceback (most recent call last):
  File "main_fedpac_psl.py", line 95, in <module>
    dataset_train, dataset_test, dict_users_train, dict_users_test, concept_matrix, rand_set_all = get_data_from_file(
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 216, in get_data_from_file
    dataset_train, dataset_test, _, _, _, _ = get_data_v3(args)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 190, in get_data_v3
    y_train_noisy, gamma_s, real_noise_level = add_noise(args, y_train, dict_users_train, rand_set_all)
  File "/home/ChenSM/code/FL_HLS/utils/train_utils.py", line 292, in add_noise
    y_train_noisy[rand_sample_idx] = nosiy_sample_labels
IndexError: arrays used as indices must be of integer (or boolean) type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 1, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.336, Test loss: 2.034, Test accuracy: 19.78
Round   0, Global train loss: 1.336, Global test loss: 2.297, Global test accuracy: 14.42
Round   1, Train loss: 1.346, Test loss: 1.745, Test accuracy: 30.94
Round   1, Global train loss: 1.346, Global test loss: 2.287, Global test accuracy: 16.53
Round   2, Train loss: 1.151, Test loss: 1.426, Test accuracy: 43.77
Round   2, Global train loss: 1.151, Global test loss: 2.244, Global test accuracy: 24.33
Round   3, Train loss: 1.237, Test loss: 1.348, Test accuracy: 47.62
Round   3, Global train loss: 1.237, Global test loss: 2.250, Global test accuracy: 25.77
Round   4, Train loss: 1.054, Test loss: 1.121, Test accuracy: 52.62
Round   4, Global train loss: 1.054, Global test loss: 2.117, Global test accuracy: 27.77
Round   5, Train loss: 1.087, Test loss: 1.100, Test accuracy: 53.88
Round   5, Global train loss: 1.087, Global test loss: 2.153, Global test accuracy: 27.01
Round   6, Train loss: 1.050, Test loss: 1.058, Test accuracy: 55.43
Round   6, Global train loss: 1.050, Global test loss: 2.131, Global test accuracy: 25.82
Round   7, Train loss: 1.071, Test loss: 1.047, Test accuracy: 56.81
Round   7, Global train loss: 1.071, Global test loss: 2.231, Global test accuracy: 26.18
Round   8, Train loss: 1.112, Test loss: 0.960, Test accuracy: 59.07
Round   8, Global train loss: 1.112, Global test loss: 2.243, Global test accuracy: 27.34
Round   9, Train loss: 1.244, Test loss: 0.945, Test accuracy: 60.02
Round   9, Global train loss: 1.244, Global test loss: 2.350, Global test accuracy: 13.94
Round  10, Train loss: 1.154, Test loss: 0.935, Test accuracy: 60.79
Round  10, Global train loss: 1.154, Global test loss: 2.386, Global test accuracy: 21.13
Round  11, Train loss: 1.076, Test loss: 0.925, Test accuracy: 61.37
Round  11, Global train loss: 1.076, Global test loss: 2.299, Global test accuracy: 13.31
Round  12, Train loss: 0.988, Test loss: 0.919, Test accuracy: 62.54
Round  12, Global train loss: 0.988, Global test loss: 2.254, Global test accuracy: 20.93
Round  13, Train loss: 1.189, Test loss: 0.907, Test accuracy: 63.13
Round  13, Global train loss: 1.189, Global test loss: 2.269, Global test accuracy: 27.11
Round  14, Train loss: 1.064, Test loss: 0.906, Test accuracy: 63.03
Round  14, Global train loss: 1.064, Global test loss: 2.110, Global test accuracy: 25.09
Round  15, Train loss: 0.976, Test loss: 0.897, Test accuracy: 62.70
Round  15, Global train loss: 0.976, Global test loss: 2.202, Global test accuracy: 25.49
Round  16, Train loss: 0.970, Test loss: 0.892, Test accuracy: 62.67
Round  16, Global train loss: 0.970, Global test loss: 2.251, Global test accuracy: 23.21
Round  17, Train loss: 1.095, Test loss: 0.893, Test accuracy: 63.36
Round  17, Global train loss: 1.095, Global test loss: 2.165, Global test accuracy: 30.60
Round  18, Train loss: 0.931, Test loss: 0.896, Test accuracy: 63.73
Round  18, Global train loss: 0.931, Global test loss: 2.103, Global test accuracy: 23.77
Round  19, Train loss: 1.156, Test loss: 0.885, Test accuracy: 63.91
Round  19, Global train loss: 1.156, Global test loss: 2.229, Global test accuracy: 25.90
Round  20, Train loss: 0.982, Test loss: 0.883, Test accuracy: 64.21
Round  20, Global train loss: 0.982, Global test loss: 2.099, Global test accuracy: 27.32
Round  21, Train loss: 1.031, Test loss: 0.882, Test accuracy: 64.21
Round  21, Global train loss: 1.031, Global test loss: 2.178, Global test accuracy: 19.62
Round  22, Train loss: 1.103, Test loss: 0.885, Test accuracy: 63.98
Round  22, Global train loss: 1.103, Global test loss: 2.195, Global test accuracy: 30.38
Round  23, Train loss: 1.006, Test loss: 0.880, Test accuracy: 63.84
Round  23, Global train loss: 1.006, Global test loss: 2.340, Global test accuracy: 23.26
Round  24, Train loss: 0.937, Test loss: 0.898, Test accuracy: 63.18
Round  24, Global train loss: 0.937, Global test loss: 2.125, Global test accuracy: 25.65
Round  25, Train loss: 1.049, Test loss: 0.896, Test accuracy: 63.46
Round  25, Global train loss: 1.049, Global test loss: 2.191, Global test accuracy: 18.57
Round  26, Train loss: 0.849, Test loss: 0.886, Test accuracy: 64.33
Round  26, Global train loss: 0.849, Global test loss: 2.126, Global test accuracy: 28.84
Round  27, Train loss: 0.927, Test loss: 0.883, Test accuracy: 63.83
Round  27, Global train loss: 0.927, Global test loss: 2.146, Global test accuracy: 20.35
Round  28, Train loss: 0.926, Test loss: 0.878, Test accuracy: 64.26
Round  28, Global train loss: 0.926, Global test loss: 2.199, Global test accuracy: 27.72
Round  29, Train loss: 0.873, Test loss: 0.867, Test accuracy: 65.04
Round  29, Global train loss: 0.873, Global test loss: 2.204, Global test accuracy: 20.23
Round  30, Train loss: 0.896, Test loss: 0.881, Test accuracy: 64.62
Round  30, Global train loss: 0.896, Global test loss: 2.127, Global test accuracy: 28.29
Round  31, Train loss: 1.035, Test loss: 0.886, Test accuracy: 63.84
Round  31, Global train loss: 1.035, Global test loss: 2.202, Global test accuracy: 26.84
Round  32, Train loss: 0.873, Test loss: 0.887, Test accuracy: 63.78
Round  32, Global train loss: 0.873, Global test loss: 2.092, Global test accuracy: 30.47
Round  33, Train loss: 0.737, Test loss: 0.879, Test accuracy: 64.32
Round  33, Global train loss: 0.737, Global test loss: 2.145, Global test accuracy: 28.51
Round  34, Train loss: 0.903, Test loss: 0.883, Test accuracy: 64.17
Round  34, Global train loss: 0.903, Global test loss: 2.130, Global test accuracy: 23.08
Round  35, Train loss: 1.025, Test loss: 0.884, Test accuracy: 63.81
Round  35, Global train loss: 1.025, Global test loss: 2.191, Global test accuracy: 23.85
Round  36, Train loss: 0.840, Test loss: 0.884, Test accuracy: 64.58
Round  36, Global train loss: 0.840, Global test loss: 2.100, Global test accuracy: 26.93
Round  37, Train loss: 0.792, Test loss: 0.914, Test accuracy: 63.65
Round  37, Global train loss: 0.792, Global test loss: 2.122, Global test accuracy: 27.96
Round  38, Train loss: 0.676, Test loss: 0.919, Test accuracy: 63.33
Round  38, Global train loss: 0.676, Global test loss: 2.085, Global test accuracy: 28.72
Round  39, Train loss: 0.749, Test loss: 0.923, Test accuracy: 63.31
Round  39, Global train loss: 0.749, Global test loss: 2.156, Global test accuracy: 22.44
Round  40, Train loss: 0.869, Test loss: 0.905, Test accuracy: 63.54
Round  40, Global train loss: 0.869, Global test loss: 2.242, Global test accuracy: 30.64
Round  41, Train loss: 0.665, Test loss: 0.916, Test accuracy: 62.76
Round  41, Global train loss: 0.665, Global test loss: 2.145, Global test accuracy: 32.73
Round  42, Train loss: 0.807, Test loss: 0.923, Test accuracy: 62.90
Round  42, Global train loss: 0.807, Global test loss: 2.135, Global test accuracy: 22.48
Round  43, Train loss: 0.796, Test loss: 0.953, Test accuracy: 62.69
Round  43, Global train loss: 0.796, Global test loss: 2.109, Global test accuracy: 29.38
Round  44, Train loss: 0.725, Test loss: 0.959, Test accuracy: 62.33
Round  44, Global train loss: 0.725, Global test loss: 2.136, Global test accuracy: 29.47
Round  45, Train loss: 0.652, Test loss: 0.951, Test accuracy: 62.49
Round  45, Global train loss: 0.652, Global test loss: 2.154, Global test accuracy: 21.07
Round  46, Train loss: 0.608, Test loss: 0.961, Test accuracy: 62.20
Round  46, Global train loss: 0.608, Global test loss: 2.073, Global test accuracy: 27.56
Round  47, Train loss: 0.601, Test loss: 0.969, Test accuracy: 62.80
Round  47, Global train loss: 0.601, Global test loss: 2.153, Global test accuracy: 30.37
Round  48, Train loss: 0.664, Test loss: 0.964, Test accuracy: 63.17
Round  48, Global train loss: 0.664, Global test loss: 2.122, Global test accuracy: 30.82
Round  49, Train loss: 0.721, Test loss: 0.968, Test accuracy: 63.41
Round  49, Global train loss: 0.721, Global test loss: 2.077, Global test accuracy: 28.86
Round  50, Train loss: 0.651, Test loss: 0.981, Test accuracy: 62.70
Round  50, Global train loss: 0.651, Global test loss: 2.135, Global test accuracy: 28.55
Round  51, Train loss: 0.700, Test loss: 1.024, Test accuracy: 61.29
Round  51, Global train loss: 0.700, Global test loss: 2.226, Global test accuracy: 23.23
Round  52, Train loss: 0.585, Test loss: 1.013, Test accuracy: 61.61
Round  52, Global train loss: 0.585, Global test loss: 2.139, Global test accuracy: 18.34
Round  53, Train loss: 0.645, Test loss: 1.010, Test accuracy: 61.35
Round  53, Global train loss: 0.645, Global test loss: 2.131, Global test accuracy: 29.02
Round  54, Train loss: 0.580, Test loss: 1.035, Test accuracy: 61.08
Round  54, Global train loss: 0.580, Global test loss: 2.223, Global test accuracy: 20.95
Round  55, Train loss: 0.674, Test loss: 1.039, Test accuracy: 61.18
Round  55, Global train loss: 0.674, Global test loss: 2.162, Global test accuracy: 27.81
Round  56, Train loss: 0.649, Test loss: 1.054, Test accuracy: 61.28
Round  56, Global train loss: 0.649, Global test loss: 2.169, Global test accuracy: 25.18
Round  57, Train loss: 0.733, Test loss: 1.060, Test accuracy: 61.23
Round  57, Global train loss: 0.733, Global test loss: 2.176, Global test accuracy: 13.18
Round  58, Train loss: 0.590, Test loss: 1.101, Test accuracy: 60.71
Round  58, Global train loss: 0.590, Global test loss: 2.189, Global test accuracy: 16.25
Round  59, Train loss: 0.547, Test loss: 1.106, Test accuracy: 60.59
Round  59, Global train loss: 0.547, Global test loss: 2.141, Global test accuracy: 20.03
Round  60, Train loss: 0.413, Test loss: 1.125, Test accuracy: 60.50
Round  60, Global train loss: 0.413, Global test loss: 2.209, Global test accuracy: 12.47
Round  61, Train loss: 0.464, Test loss: 1.161, Test accuracy: 59.61
Round  61, Global train loss: 0.464, Global test loss: 2.125, Global test accuracy: 21.83
Round  62, Train loss: 0.439, Test loss: 1.194, Test accuracy: 59.50
Round  62, Global train loss: 0.439, Global test loss: 2.083, Global test accuracy: 26.99
Round  63, Train loss: 0.658, Test loss: 1.255, Test accuracy: 58.48
Round  63, Global train loss: 0.658, Global test loss: 2.183, Global test accuracy: 26.07
Round  64, Train loss: 0.506, Test loss: 1.251, Test accuracy: 58.49
Round  64, Global train loss: 0.506, Global test loss: 2.240, Global test accuracy: 16.73
Round  65, Train loss: 0.566, Test loss: 1.261, Test accuracy: 58.89
Round  65, Global train loss: 0.566, Global test loss: 2.114, Global test accuracy: 24.35
Round  66, Train loss: 0.552, Test loss: 1.234, Test accuracy: 60.10
Round  66, Global train loss: 0.552, Global test loss: 2.189, Global test accuracy: 22.43
Round  67, Train loss: 0.466, Test loss: 1.291, Test accuracy: 59.32
Round  67, Global train loss: 0.466, Global test loss: 2.137, Global test accuracy: 21.76
Round  68, Train loss: 0.496, Test loss: 1.323, Test accuracy: 59.02
Round  68, Global train loss: 0.496, Global test loss: 2.181, Global test accuracy: 19.43
Round  69, Train loss: 0.473, Test loss: 1.340, Test accuracy: 58.44
Round  69, Global train loss: 0.473, Global test loss: 2.154, Global test accuracy: 24.38
Round  70, Train loss: 0.424, Test loss: 1.375, Test accuracy: 59.11
Round  70, Global train loss: 0.424, Global test loss: 2.192, Global test accuracy: 15.86
Round  71, Train loss: 0.575, Test loss: 1.384, Test accuracy: 59.36
Round  71, Global train loss: 0.575, Global test loss: 2.197, Global test accuracy: 25.67
Round  72, Train loss: 0.423, Test loss: 1.406, Test accuracy: 59.10
Round  72, Global train loss: 0.423, Global test loss: 2.151, Global test accuracy: 25.61
Round  73, Train loss: 0.468, Test loss: 1.439, Test accuracy: 57.50
Round  73, Global train loss: 0.468, Global test loss: 2.155, Global test accuracy: 24.37
Round  74, Train loss: 0.494, Test loss: 1.404, Test accuracy: 57.92
Round  74, Global train loss: 0.494, Global test loss: 2.255, Global test accuracy: 17.52
Round  75, Train loss: 0.256, Test loss: 1.408, Test accuracy: 58.27
Round  75, Global train loss: 0.256, Global test loss: 2.081, Global test accuracy: 29.89
Round  76, Train loss: 0.563, Test loss: 1.429, Test accuracy: 58.80
Round  76, Global train loss: 0.563, Global test loss: 2.175, Global test accuracy: 24.73
Round  77, Train loss: 0.480, Test loss: 1.449, Test accuracy: 58.65
Round  77, Global train loss: 0.480, Global test loss: 2.184, Global test accuracy: 23.63
Round  78, Train loss: 0.387, Test loss: 1.441, Test accuracy: 57.97
Round  78, Global train loss: 0.387, Global test loss: 2.189, Global test accuracy: 26.09
Round  79, Train loss: 0.294, Test loss: 1.481, Test accuracy: 58.10
Round  79, Global train loss: 0.294, Global test loss: 2.174, Global test accuracy: 25.82
Round  80, Train loss: 0.349, Test loss: 1.497, Test accuracy: 58.18
Round  80, Global train loss: 0.349, Global test loss: 2.208, Global test accuracy: 21.21
Round  81, Train loss: 0.269, Test loss: 1.467, Test accuracy: 58.44
Round  81, Global train loss: 0.269, Global test loss: 2.154, Global test accuracy: 22.25
Round  82, Train loss: 0.317, Test loss: 1.505, Test accuracy: 58.11
Round  82, Global train loss: 0.317, Global test loss: 2.255, Global test accuracy: 15.07
Round  83, Train loss: 0.251, Test loss: 1.527, Test accuracy: 57.91
Round  83, Global train loss: 0.251, Global test loss: 2.075, Global test accuracy: 26.67
Round  84, Train loss: 0.285, Test loss: 1.547, Test accuracy: 57.53
Round  84, Global train loss: 0.285, Global test loss: 2.184, Global test accuracy: 24.39
Round  85, Train loss: 0.309, Test loss: 1.538, Test accuracy: 57.42
Round  85, Global train loss: 0.309, Global test loss: 2.169, Global test accuracy: 25.04
Round  86, Train loss: 0.271, Test loss: 1.592, Test accuracy: 57.67
Round  86, Global train loss: 0.271, Global test loss: 2.265, Global test accuracy: 19.69
Round  87, Train loss: 0.350, Test loss: 1.603, Test accuracy: 58.04
Round  87, Global train loss: 0.350, Global test loss: 2.208, Global test accuracy: 18.74
Round  88, Train loss: 0.300, Test loss: 1.610, Test accuracy: 57.49
Round  88, Global train loss: 0.300, Global test loss: 2.082, Global test accuracy: 29.22
Round  89, Train loss: 0.320, Test loss: 1.640, Test accuracy: 56.98
Round  89, Global train loss: 0.320, Global test loss: 2.213, Global test accuracy: 13.58
Round  90, Train loss: 0.315, Test loss: 1.629, Test accuracy: 58.01
Round  90, Global train loss: 0.315, Global test loss: 2.239, Global test accuracy: 14.42
Round  91, Train loss: 0.295, Test loss: 1.650, Test accuracy: 57.77
Round  91, Global train loss: 0.295, Global test loss: 2.248, Global test accuracy: 18.27
Round  92, Train loss: 0.220, Test loss: 1.639, Test accuracy: 58.11
Round  92, Global train loss: 0.220, Global test loss: 2.155, Global test accuracy: 26.75
Round  93, Train loss: 0.196, Test loss: 1.641, Test accuracy: 58.53
Round  93, Global train loss: 0.196, Global test loss: 2.155, Global test accuracy: 25.91
Round  94, Train loss: 0.357, Test loss: 1.652, Test accuracy: 58.13
Round  94, Global train loss: 0.357, Global test loss: 2.217, Global test accuracy: 18.75
Round  95, Train loss: 0.293, Test loss: 1.678, Test accuracy: 57.66
Round  95, Global train loss: 0.293, Global test loss: 2.155, Global test accuracy: 20.20
Round  96, Train loss: 0.315, Test loss: 1.723, Test accuracy: 57.46
Round  96, Global train loss: 0.315, Global test loss: 2.196, Global test accuracy: 16.46
Round  97, Train loss: 0.267, Test loss: 1.759, Test accuracy: 57.81
Round  97, Global train loss: 0.267, Global test loss: 2.201, Global test accuracy: 20.52
Round  98, Train loss: 0.181, Test loss: 1.809, Test accuracy: 58.08
Round  98, Global train loss: 0.181, Global test loss: 2.093, Global test accuracy: 30.11
Round  99, Train loss: 0.297, Test loss: 1.803, Test accuracy: 58.57
Round  99, Global train loss: 0.297, Global test loss: 2.209, Global test accuracy: 26.79
Final Round, Train loss: 0.207, Test loss: 1.974, Test accuracy: 57.56
Final Round, Global train loss: 0.207, Global test loss: 2.209, Global test accuracy: 26.79
Average accuracy final 10 rounds: 58.01166666666666 

Average global accuracy final 10 rounds: 21.818333333333335 

1890.5764200687408
[1.6906988620758057, 3.3813977241516113, 4.768901348114014, 6.156404972076416, 7.549059152603149, 8.941713333129883, 10.350865364074707, 11.760017395019531, 13.14954924583435, 14.53908109664917, 15.922097444534302, 17.305113792419434, 18.694207906723022, 20.08330202102661, 21.545948028564453, 23.008594036102295, 24.42289423942566, 25.837194442749023, 27.239728689193726, 28.642262935638428, 30.03813338279724, 31.434003829956055, 32.86734199523926, 34.30068016052246, 35.68543720245361, 37.070194244384766, 38.46300172805786, 39.85580921173096, 41.26131343841553, 42.6668176651001, 44.07678747177124, 45.48675727844238, 46.90658974647522, 48.32642221450806, 49.71035289764404, 51.09428358078003, 52.517799377441406, 53.94131517410278, 55.334206104278564, 56.727097034454346, 58.13441205024719, 59.54172706604004, 60.9359130859375, 62.33009910583496, 63.71144962310791, 65.09280014038086, 66.49177956581116, 67.89075899124146, 69.30590581893921, 70.72105264663696, 72.12311720848083, 73.5251817703247, 74.93326711654663, 76.34135246276855, 77.72018527984619, 79.09901809692383, 80.4830961227417, 81.86717414855957, 83.24701142311096, 84.62684869766235, 85.98951983451843, 87.35219097137451, 88.73088526725769, 90.10957956314087, 91.48886966705322, 92.86815977096558, 94.24429202079773, 95.62042427062988, 96.99976897239685, 98.37911367416382, 99.76145315170288, 101.14379262924194, 102.56286025047302, 103.9819278717041, 105.38760328292847, 106.79327869415283, 108.25784206390381, 109.72240543365479, 111.21774125099182, 112.71307706832886, 114.29198122024536, 115.87088537216187, 117.3681435585022, 118.86540174484253, 120.32564878463745, 121.78589582443237, 123.25514698028564, 124.72439813613892, 126.23000049591064, 127.73560285568237, 129.1846284866333, 130.63365411758423, 132.07214641571045, 133.51063871383667, 135.01050281524658, 136.5103669166565, 138.03042125701904, 139.5504755973816, 140.985435962677, 142.4203963279724, 143.83444476127625, 145.24849319458008, 146.72021341323853, 148.19193363189697, 149.67095756530762, 151.14998149871826, 152.60086274147034, 154.0517439842224, 155.46711778640747, 156.88249158859253, 158.34044075012207, 159.7983899116516, 161.26011085510254, 162.72183179855347, 164.13249015808105, 165.54314851760864, 166.91629314422607, 168.2894377708435, 169.67932271957397, 171.06920766830444, 172.43700051307678, 173.80479335784912, 175.17926049232483, 176.55372762680054, 177.93080067634583, 179.3078737258911, 180.68497014045715, 182.0620665550232, 183.4534821510315, 184.8448977470398, 186.23577213287354, 187.62664651870728, 189.0186586380005, 190.4106707572937, 191.9097626209259, 193.4088544845581, 194.90914130210876, 196.40942811965942, 197.89886021614075, 199.38829231262207, 200.91210865974426, 202.43592500686646, 203.95686268806458, 205.4778003692627, 207.00780653953552, 208.53781270980835, 210.04521894454956, 211.55262517929077, 212.92808079719543, 214.3035364151001, 215.68979620933533, 217.07605600357056, 218.47108817100525, 219.86612033843994, 221.29109692573547, 222.716073513031, 224.09593868255615, 225.4758038520813, 226.84261465072632, 228.20942544937134, 229.58950877189636, 230.9695920944214, 232.3443169593811, 233.71904182434082, 235.09437227249146, 236.4697027206421, 237.86828589439392, 239.26686906814575, 240.62910151481628, 241.99133396148682, 243.37717127799988, 244.76300859451294, 246.28138995170593, 247.79977130889893, 249.3327386379242, 250.86570596694946, 252.37478828430176, 253.88387060165405, 255.46239686012268, 257.0409231185913, 258.5617377758026, 260.0825524330139, 261.65053820610046, 263.218523979187, 264.8336455821991, 266.4487671852112, 267.9788362979889, 269.5089054107666, 271.07546043395996, 272.6420154571533, 274.33294701576233, 276.02387857437134, 277.41226029396057, 278.8006420135498, 280.173704624176, 281.54676723480225, 282.91818165779114, 284.28959608078003, 285.6636517047882, 287.0377073287964, 289.3239600658417, 291.61021280288696]
[19.783333333333335, 19.783333333333335, 30.941666666666666, 30.941666666666666, 43.775, 43.775, 47.625, 47.625, 52.625, 52.625, 53.88333333333333, 53.88333333333333, 55.43333333333333, 55.43333333333333, 56.80833333333333, 56.80833333333333, 59.06666666666667, 59.06666666666667, 60.025, 60.025, 60.791666666666664, 60.791666666666664, 61.36666666666667, 61.36666666666667, 62.541666666666664, 62.541666666666664, 63.13333333333333, 63.13333333333333, 63.03333333333333, 63.03333333333333, 62.7, 62.7, 62.666666666666664, 62.666666666666664, 63.358333333333334, 63.358333333333334, 63.725, 63.725, 63.90833333333333, 63.90833333333333, 64.20833333333333, 64.20833333333333, 64.20833333333333, 64.20833333333333, 63.975, 63.975, 63.84166666666667, 63.84166666666667, 63.18333333333333, 63.18333333333333, 63.458333333333336, 63.458333333333336, 64.325, 64.325, 63.833333333333336, 63.833333333333336, 64.25833333333334, 64.25833333333334, 65.04166666666667, 65.04166666666667, 64.625, 64.625, 63.84166666666667, 63.84166666666667, 63.78333333333333, 63.78333333333333, 64.31666666666666, 64.31666666666666, 64.175, 64.175, 63.80833333333333, 63.80833333333333, 64.575, 64.575, 63.65, 63.65, 63.333333333333336, 63.333333333333336, 63.30833333333333, 63.30833333333333, 63.541666666666664, 63.541666666666664, 62.75833333333333, 62.75833333333333, 62.9, 62.9, 62.69166666666667, 62.69166666666667, 62.325, 62.325, 62.49166666666667, 62.49166666666667, 62.2, 62.2, 62.8, 62.8, 63.166666666666664, 63.166666666666664, 63.40833333333333, 63.40833333333333, 62.7, 62.7, 61.291666666666664, 61.291666666666664, 61.608333333333334, 61.608333333333334, 61.35, 61.35, 61.075, 61.075, 61.18333333333333, 61.18333333333333, 61.28333333333333, 61.28333333333333, 61.233333333333334, 61.233333333333334, 60.708333333333336, 60.708333333333336, 60.59166666666667, 60.59166666666667, 60.5, 60.5, 59.608333333333334, 59.608333333333334, 59.5, 59.5, 58.475, 58.475, 58.49166666666667, 58.49166666666667, 58.891666666666666, 58.891666666666666, 60.1, 60.1, 59.31666666666667, 59.31666666666667, 59.025, 59.025, 58.44166666666667, 58.44166666666667, 59.108333333333334, 59.108333333333334, 59.358333333333334, 59.358333333333334, 59.1, 59.1, 57.5, 57.5, 57.916666666666664, 57.916666666666664, 58.266666666666666, 58.266666666666666, 58.8, 58.8, 58.65, 58.65, 57.96666666666667, 57.96666666666667, 58.1, 58.1, 58.18333333333333, 58.18333333333333, 58.44166666666667, 58.44166666666667, 58.108333333333334, 58.108333333333334, 57.90833333333333, 57.90833333333333, 57.53333333333333, 57.53333333333333, 57.416666666666664, 57.416666666666664, 57.666666666666664, 57.666666666666664, 58.041666666666664, 58.041666666666664, 57.49166666666667, 57.49166666666667, 56.975, 56.975, 58.00833333333333, 58.00833333333333, 57.766666666666666, 57.766666666666666, 58.108333333333334, 58.108333333333334, 58.53333333333333, 58.53333333333333, 58.13333333333333, 58.13333333333333, 57.65833333333333, 57.65833333333333, 57.458333333333336, 57.458333333333336, 57.80833333333333, 57.80833333333333, 58.075, 58.075, 58.56666666666667, 58.56666666666667, 57.55833333333333, 57.55833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 0, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.128, Test loss: 2.028, Test accuracy: 32.06
Round   0, Global train loss: 1.128, Global test loss: 2.314, Global test accuracy: 24.72
Round   1, Train loss: 1.027, Test loss: 1.573, Test accuracy: 43.92
Round   1, Global train loss: 1.027, Global test loss: 2.096, Global test accuracy: 29.32
Round   2, Train loss: 0.992, Test loss: 1.271, Test accuracy: 52.60
Round   2, Global train loss: 0.992, Global test loss: 2.156, Global test accuracy: 30.62
Round   3, Train loss: 0.837, Test loss: 1.239, Test accuracy: 56.64
Round   3, Global train loss: 0.837, Global test loss: 2.336, Global test accuracy: 31.33
Round   4, Train loss: 0.917, Test loss: 0.918, Test accuracy: 61.63
Round   4, Global train loss: 0.917, Global test loss: 1.846, Global test accuracy: 36.83
Round   5, Train loss: 0.867, Test loss: 0.919, Test accuracy: 61.95
Round   5, Global train loss: 0.867, Global test loss: 1.847, Global test accuracy: 37.35
Round   6, Train loss: 0.886, Test loss: 0.849, Test accuracy: 64.37
Round   6, Global train loss: 0.886, Global test loss: 1.779, Global test accuracy: 36.70
Round   7, Train loss: 0.744, Test loss: 0.853, Test accuracy: 65.28
Round   7, Global train loss: 0.744, Global test loss: 2.058, Global test accuracy: 32.80
Round   8, Train loss: 0.827, Test loss: 0.743, Test accuracy: 68.18
Round   8, Global train loss: 0.827, Global test loss: 2.072, Global test accuracy: 32.13
Round   9, Train loss: 0.799, Test loss: 0.720, Test accuracy: 69.64
Round   9, Global train loss: 0.799, Global test loss: 2.169, Global test accuracy: 30.14
Round  10, Train loss: 0.696, Test loss: 0.741, Test accuracy: 68.79
Round  10, Global train loss: 0.696, Global test loss: 2.056, Global test accuracy: 39.68
Round  11, Train loss: 0.852, Test loss: 0.736, Test accuracy: 69.73
Round  11, Global train loss: 0.852, Global test loss: 1.752, Global test accuracy: 38.20
Round  12, Train loss: 0.677, Test loss: 0.701, Test accuracy: 71.57
Round  12, Global train loss: 0.677, Global test loss: 1.935, Global test accuracy: 37.25
Round  13, Train loss: 0.728, Test loss: 0.675, Test accuracy: 72.90
Round  13, Global train loss: 0.728, Global test loss: 1.608, Global test accuracy: 44.28
Round  14, Train loss: 0.688, Test loss: 0.678, Test accuracy: 72.91
Round  14, Global train loss: 0.688, Global test loss: 1.706, Global test accuracy: 42.21
Round  15, Train loss: 0.667, Test loss: 0.685, Test accuracy: 72.82
Round  15, Global train loss: 0.667, Global test loss: 1.893, Global test accuracy: 37.61
Round  16, Train loss: 0.683, Test loss: 0.657, Test accuracy: 74.10
Round  16, Global train loss: 0.683, Global test loss: 1.796, Global test accuracy: 39.58
Round  17, Train loss: 0.755, Test loss: 0.640, Test accuracy: 75.14
Round  17, Global train loss: 0.755, Global test loss: 1.514, Global test accuracy: 45.92
Round  18, Train loss: 0.606, Test loss: 0.632, Test accuracy: 75.65
Round  18, Global train loss: 0.606, Global test loss: 1.485, Global test accuracy: 48.81
Round  19, Train loss: 0.598, Test loss: 0.638, Test accuracy: 75.15
Round  19, Global train loss: 0.598, Global test loss: 1.611, Global test accuracy: 45.22
Round  20, Train loss: 0.689, Test loss: 0.606, Test accuracy: 76.54
Round  20, Global train loss: 0.689, Global test loss: 1.371, Global test accuracy: 50.18
Round  21, Train loss: 0.620, Test loss: 0.606, Test accuracy: 76.58
Round  21, Global train loss: 0.620, Global test loss: 1.649, Global test accuracy: 40.22
Round  22, Train loss: 0.596, Test loss: 0.603, Test accuracy: 76.74
Round  22, Global train loss: 0.596, Global test loss: 1.485, Global test accuracy: 48.45
Round  23, Train loss: 0.600, Test loss: 0.609, Test accuracy: 76.31
Round  23, Global train loss: 0.600, Global test loss: 1.696, Global test accuracy: 43.16
Round  24, Train loss: 0.596, Test loss: 0.585, Test accuracy: 77.21
Round  24, Global train loss: 0.596, Global test loss: 1.504, Global test accuracy: 46.96
Round  25, Train loss: 0.616, Test loss: 0.586, Test accuracy: 77.22
Round  25, Global train loss: 0.616, Global test loss: 1.409, Global test accuracy: 49.95
Round  26, Train loss: 0.469, Test loss: 0.585, Test accuracy: 77.47
Round  26, Global train loss: 0.469, Global test loss: 1.588, Global test accuracy: 46.21
Round  27, Train loss: 0.624, Test loss: 0.573, Test accuracy: 77.90
Round  27, Global train loss: 0.624, Global test loss: 1.462, Global test accuracy: 48.92
Round  28, Train loss: 0.480, Test loss: 0.575, Test accuracy: 77.98
Round  28, Global train loss: 0.480, Global test loss: 1.563, Global test accuracy: 47.98
Round  29, Train loss: 0.591, Test loss: 0.581, Test accuracy: 77.95
Round  29, Global train loss: 0.591, Global test loss: 1.475, Global test accuracy: 51.58
Round  30, Train loss: 0.609, Test loss: 0.571, Test accuracy: 78.47
Round  30, Global train loss: 0.609, Global test loss: 1.332, Global test accuracy: 52.74
Round  31, Train loss: 0.515, Test loss: 0.560, Test accuracy: 78.85
Round  31, Global train loss: 0.515, Global test loss: 1.489, Global test accuracy: 52.27
Round  32, Train loss: 0.545, Test loss: 0.563, Test accuracy: 79.05
Round  32, Global train loss: 0.545, Global test loss: 1.320, Global test accuracy: 54.14
Round  33, Train loss: 0.578, Test loss: 0.561, Test accuracy: 79.09
Round  33, Global train loss: 0.578, Global test loss: 1.780, Global test accuracy: 43.82
Round  34, Train loss: 0.610, Test loss: 0.558, Test accuracy: 78.62
Round  34, Global train loss: 0.610, Global test loss: 1.376, Global test accuracy: 53.38
Round  35, Train loss: 0.436, Test loss: 0.566, Test accuracy: 78.78
Round  35, Global train loss: 0.436, Global test loss: 1.410, Global test accuracy: 50.98
Round  36, Train loss: 0.531, Test loss: 0.573, Test accuracy: 78.52
Round  36, Global train loss: 0.531, Global test loss: 1.271, Global test accuracy: 55.81
Round  37, Train loss: 0.519, Test loss: 0.563, Test accuracy: 79.08
Round  37, Global train loss: 0.519, Global test loss: 1.397, Global test accuracy: 53.54
Round  38, Train loss: 0.506, Test loss: 0.571, Test accuracy: 78.64
Round  38, Global train loss: 0.506, Global test loss: 1.314, Global test accuracy: 55.40
Round  39, Train loss: 0.455, Test loss: 0.586, Test accuracy: 78.42
Round  39, Global train loss: 0.455, Global test loss: 1.478, Global test accuracy: 50.98
Round  40, Train loss: 0.367, Test loss: 0.587, Test accuracy: 78.33
Round  40, Global train loss: 0.367, Global test loss: 1.784, Global test accuracy: 46.51
Round  41, Train loss: 0.382, Test loss: 0.609, Test accuracy: 78.20
Round  41, Global train loss: 0.382, Global test loss: 1.742, Global test accuracy: 46.73
Round  42, Train loss: 0.474, Test loss: 0.601, Test accuracy: 78.49
Round  42, Global train loss: 0.474, Global test loss: 1.230, Global test accuracy: 58.39
Round  43, Train loss: 0.491, Test loss: 0.609, Test accuracy: 78.15
Round  43, Global train loss: 0.491, Global test loss: 1.376, Global test accuracy: 52.92
Round  44, Train loss: 0.404, Test loss: 0.594, Test accuracy: 78.35
Round  44, Global train loss: 0.404, Global test loss: 1.437, Global test accuracy: 54.19
Round  45, Train loss: 0.412, Test loss: 0.579, Test accuracy: 78.72
Round  45, Global train loss: 0.412, Global test loss: 1.464, Global test accuracy: 51.68
Round  46, Train loss: 0.442, Test loss: 0.606, Test accuracy: 77.88
Round  46, Global train loss: 0.442, Global test loss: 1.217, Global test accuracy: 58.04
Round  47, Train loss: 0.401, Test loss: 0.606, Test accuracy: 77.78
Round  47, Global train loss: 0.401, Global test loss: 1.451, Global test accuracy: 52.15
Round  48, Train loss: 0.468, Test loss: 0.599, Test accuracy: 77.85
Round  48, Global train loss: 0.468, Global test loss: 1.266, Global test accuracy: 57.78
Round  49, Train loss: 0.423, Test loss: 0.572, Test accuracy: 79.22
Round  49, Global train loss: 0.423, Global test loss: 1.317, Global test accuracy: 56.77
Round  50, Train loss: 0.481, Test loss: 0.593, Test accuracy: 78.40
Round  50, Global train loss: 0.481, Global test loss: 1.238, Global test accuracy: 58.92
Round  51, Train loss: 0.359, Test loss: 0.597, Test accuracy: 78.57
Round  51, Global train loss: 0.359, Global test loss: 1.633, Global test accuracy: 48.75
Round  52, Train loss: 0.444, Test loss: 0.596, Test accuracy: 78.58
Round  52, Global train loss: 0.444, Global test loss: 1.277, Global test accuracy: 54.88
Round  53, Train loss: 0.322, Test loss: 0.589, Test accuracy: 78.71
Round  53, Global train loss: 0.322, Global test loss: 1.409, Global test accuracy: 55.18
Round  54, Train loss: 0.340, Test loss: 0.581, Test accuracy: 79.22
Round  54, Global train loss: 0.340, Global test loss: 1.593, Global test accuracy: 52.58
Round  55, Train loss: 0.473, Test loss: 0.581, Test accuracy: 79.38
Round  55, Global train loss: 0.473, Global test loss: 1.678, Global test accuracy: 48.18
Round  56, Train loss: 0.334, Test loss: 0.576, Test accuracy: 79.78
Round  56, Global train loss: 0.334, Global test loss: 1.360, Global test accuracy: 57.09
Round  57, Train loss: 0.469, Test loss: 0.573, Test accuracy: 79.89
Round  57, Global train loss: 0.469, Global test loss: 1.268, Global test accuracy: 56.72
Round  58, Train loss: 0.461, Test loss: 0.587, Test accuracy: 79.24
Round  58, Global train loss: 0.461, Global test loss: 1.583, Global test accuracy: 49.76
Round  59, Train loss: 0.383, Test loss: 0.579, Test accuracy: 79.36
Round  59, Global train loss: 0.383, Global test loss: 1.214, Global test accuracy: 58.98
Round  60, Train loss: 0.422, Test loss: 0.572, Test accuracy: 79.52
Round  60, Global train loss: 0.422, Global test loss: 1.392, Global test accuracy: 51.81
Round  61, Train loss: 0.421, Test loss: 0.581, Test accuracy: 79.32
Round  61, Global train loss: 0.421, Global test loss: 1.407, Global test accuracy: 53.48
Round  62, Train loss: 0.400, Test loss: 0.589, Test accuracy: 78.97
Round  62, Global train loss: 0.400, Global test loss: 1.449, Global test accuracy: 52.14
Round  63, Train loss: 0.321, Test loss: 0.586, Test accuracy: 79.27
Round  63, Global train loss: 0.321, Global test loss: 1.457, Global test accuracy: 54.84
Round  64, Train loss: 0.408, Test loss: 0.596, Test accuracy: 79.08
Round  64, Global train loss: 0.408, Global test loss: 1.608, Global test accuracy: 47.83
Round  65, Train loss: 0.372, Test loss: 0.605, Test accuracy: 78.58
Round  65, Global train loss: 0.372, Global test loss: 1.270, Global test accuracy: 57.12
Round  66, Train loss: 0.355, Test loss: 0.624, Test accuracy: 78.05
Round  66, Global train loss: 0.355, Global test loss: 1.363, Global test accuracy: 55.67
Round  67, Train loss: 0.325, Test loss: 0.619, Test accuracy: 78.52
Round  67, Global train loss: 0.325, Global test loss: 1.297, Global test accuracy: 58.90
Round  68, Train loss: 0.365, Test loss: 0.608, Test accuracy: 78.71
Round  68, Global train loss: 0.365, Global test loss: 1.308, Global test accuracy: 57.87
Round  69, Train loss: 0.323, Test loss: 0.619, Test accuracy: 78.94
Round  69, Global train loss: 0.323, Global test loss: 1.559, Global test accuracy: 53.40
Round  70, Train loss: 0.328, Test loss: 0.620, Test accuracy: 78.76
Round  70, Global train loss: 0.328, Global test loss: 1.344, Global test accuracy: 56.83
Round  71, Train loss: 0.336, Test loss: 0.623, Test accuracy: 79.09
Round  71, Global train loss: 0.336, Global test loss: 1.307, Global test accuracy: 59.26
Round  72, Train loss: 0.323, Test loss: 0.629, Test accuracy: 79.02
Round  72, Global train loss: 0.323, Global test loss: 1.472, Global test accuracy: 55.27
Round  73, Train loss: 0.346, Test loss: 0.615, Test accuracy: 79.27
Round  73, Global train loss: 0.346, Global test loss: 1.246, Global test accuracy: 59.25
Round  74, Train loss: 0.338, Test loss: 0.614, Test accuracy: 79.25
Round  74, Global train loss: 0.338, Global test loss: 1.452, Global test accuracy: 54.18
Round  75, Train loss: 0.327, Test loss: 0.604, Test accuracy: 79.63
Round  75, Global train loss: 0.327, Global test loss: 1.358, Global test accuracy: 55.85
Round  76, Train loss: 0.321, Test loss: 0.605, Test accuracy: 80.02
Round  76, Global train loss: 0.321, Global test loss: 1.361, Global test accuracy: 57.83
Round  77, Train loss: 0.284, Test loss: 0.606, Test accuracy: 80.11
Round  77, Global train loss: 0.284, Global test loss: 1.421, Global test accuracy: 57.76
Round  78, Train loss: 0.379, Test loss: 0.594, Test accuracy: 80.47
Round  78, Global train loss: 0.379, Global test loss: 1.541, Global test accuracy: 54.62
Round  79, Train loss: 0.294, Test loss: 0.613, Test accuracy: 79.77
Round  79, Global train loss: 0.294, Global test loss: 1.803, Global test accuracy: 49.28
Round  80, Train loss: 0.241, Test loss: 0.616, Test accuracy: 79.76
Round  80, Global train loss: 0.241, Global test loss: 1.643, Global test accuracy: 55.67
Round  81, Train loss: 0.352, Test loss: 0.616, Test accuracy: 79.73
Round  81, Global train loss: 0.352, Global test loss: 1.721, Global test accuracy: 48.85
Round  82, Train loss: 0.343, Test loss: 0.637, Test accuracy: 79.03
Round  82, Global train loss: 0.343, Global test loss: 1.522, Global test accuracy: 52.06
Round  83, Train loss: 0.270, Test loss: 0.636, Test accuracy: 79.35
Round  83, Global train loss: 0.270, Global test loss: 1.413, Global test accuracy: 56.40
Round  84, Train loss: 0.343, Test loss: 0.629, Test accuracy: 80.23
Round  84, Global train loss: 0.343, Global test loss: 1.604, Global test accuracy: 54.54
Round  85, Train loss: 0.308, Test loss: 0.637, Test accuracy: 79.88
Round  85, Global train loss: 0.308, Global test loss: 1.401, Global test accuracy: 58.40
Round  86, Train loss: 0.346, Test loss: 0.632, Test accuracy: 79.73
Round  86, Global train loss: 0.346, Global test loss: 1.637, Global test accuracy: 51.08
Round  87, Train loss: 0.292, Test loss: 0.639, Test accuracy: 79.83
Round  87, Global train loss: 0.292, Global test loss: 1.361, Global test accuracy: 58.35
Round  88, Train loss: 0.262, Test loss: 0.646, Test accuracy: 79.58
Round  88, Global train loss: 0.262, Global test loss: 1.396, Global test accuracy: 59.04
Round  89, Train loss: 0.330, Test loss: 0.632, Test accuracy: 80.33
Round  89, Global train loss: 0.330, Global test loss: 1.534, Global test accuracy: 54.82
Round  90, Train loss: 0.352, Test loss: 0.607, Test accuracy: 80.99
Round  90, Global train loss: 0.352, Global test loss: 1.506, Global test accuracy: 53.27
Round  91, Train loss: 0.298, Test loss: 0.581, Test accuracy: 81.43
Round  91, Global train loss: 0.298, Global test loss: 1.438, Global test accuracy: 57.63
Round  92, Train loss: 0.281, Test loss: 0.590, Test accuracy: 81.25
Round  92, Global train loss: 0.281, Global test loss: 1.451, Global test accuracy: 56.94
Round  93, Train loss: 0.275, Test loss: 0.592, Test accuracy: 81.33
Round  93, Global train loss: 0.275, Global test loss: 1.392, Global test accuracy: 57.31
Round  94, Train loss: 0.362, Test loss: 0.627, Test accuracy: 80.68
Round  94, Global train loss: 0.362, Global test loss: 1.324, Global test accuracy: 57.58
Round  95, Train loss: 0.267, Test loss: 0.633, Test accuracy: 80.22
Round  95, Global train loss: 0.267, Global test loss: 1.430, Global test accuracy: 57.57
Round  96, Train loss: 0.283, Test loss: 0.608, Test accuracy: 80.89
Round  96, Global train loss: 0.283, Global test loss: 1.341, Global test accuracy: 58.36
Round  97, Train loss: 0.364, Test loss: 0.600, Test accuracy: 81.36
Round  97, Global train loss: 0.364, Global test loss: 1.492, Global test accuracy: 55.21
Round  98, Train loss: 0.220, Test loss: 0.595, Test accuracy: 81.72
Round  98, Global train loss: 0.220, Global test loss: 1.361, Global test accuracy: 59.84
Round  99, Train loss: 0.220, Test loss: 0.586, Test accuracy: 81.97
Round  99, Global train loss: 0.220, Global test loss: 1.715, Global test accuracy: 54.85
Final Round, Train loss: 0.216, Test loss: 0.664, Test accuracy: 81.02
Final Round, Global train loss: 0.216, Global test loss: 1.715, Global test accuracy: 54.85
Average accuracy final 10 rounds: 81.18583333333335 

Average global accuracy final 10 rounds: 56.85666666666667 

1888.7294163703918
[1.897845983505249, 3.795691967010498, 5.3092567920684814, 6.822821617126465, 8.33935832977295, 9.855895042419434, 11.378159761428833, 12.900424480438232, 14.416988134384155, 15.933551788330078, 17.439114332199097, 18.944676876068115, 20.443689346313477, 21.942701816558838, 23.416248321533203, 24.88979482650757, 26.397658824920654, 27.90552282333374, 29.443501949310303, 30.981481075286865, 32.372053384780884, 33.7626256942749, 35.2948522567749, 36.8270788192749, 38.23553705215454, 39.64399528503418, 41.069495677948, 42.494996070861816, 43.92644262313843, 45.35788917541504, 46.825419664382935, 48.29295015335083, 49.77613711357117, 51.259324073791504, 52.712735176086426, 54.16614627838135, 55.53871536254883, 56.91128444671631, 58.28544497489929, 59.659605503082275, 61.02858567237854, 62.397565841674805, 63.81944918632507, 65.24133253097534, 66.64132523536682, 68.0413179397583, 69.47378063201904, 70.90624332427979, 72.38293075561523, 73.85961818695068, 75.28624629974365, 76.71287441253662, 78.12664318084717, 79.54041194915771, 81.01195406913757, 82.48349618911743, 83.89425015449524, 85.30500411987305, 86.75259709358215, 88.20019006729126, 89.67419075965881, 91.14819145202637, 92.5983304977417, 94.04846954345703, 95.46512961387634, 96.88178968429565, 98.29265475273132, 99.70351982116699, 101.16456151008606, 102.62560319900513, 104.10708379745483, 105.58856439590454, 107.02785396575928, 108.46714353561401, 109.91691708564758, 111.36669063568115, 112.90469026565552, 114.44268989562988, 115.91001677513123, 117.37734365463257, 118.76735854148865, 120.15737342834473, 121.55336475372314, 122.94935607910156, 124.37299585342407, 125.79663562774658, 127.19461488723755, 128.59259414672852, 129.97309494018555, 131.35359573364258, 132.75312876701355, 134.15266180038452, 135.57549452781677, 136.99832725524902, 138.41615867614746, 139.8339900970459, 141.24137616157532, 142.64876222610474, 144.0329713821411, 145.4171805381775, 146.80000138282776, 148.18282222747803, 149.55514550209045, 150.92746877670288, 152.3022301197052, 153.67699146270752, 155.05371499061584, 156.43043851852417, 157.8460545539856, 159.26167058944702, 160.68536400794983, 162.10905742645264, 163.49812245368958, 164.8871874809265, 166.2706618309021, 167.65413618087769, 169.10034275054932, 170.54654932022095, 172.0719599723816, 173.59737062454224, 175.07379722595215, 176.55022382736206, 178.05504298210144, 179.55986213684082, 181.1536943912506, 182.7475266456604, 184.25401902198792, 185.76051139831543, 187.2613775730133, 188.76224374771118, 190.27253437042236, 191.78282499313354, 193.2766628265381, 194.77050065994263, 196.2680060863495, 197.76551151275635, 199.2724630832672, 200.77941465377808, 202.2784662246704, 203.77751779556274, 205.24193716049194, 206.70635652542114, 208.31993460655212, 209.9335126876831, 211.4597351551056, 212.98595762252808, 214.44450998306274, 215.9030623435974, 217.42379593849182, 218.94452953338623, 220.5184519290924, 222.09237432479858, 223.66377568244934, 225.2351770401001, 226.74705958366394, 228.25894212722778, 229.75646829605103, 231.25399446487427, 232.7467007637024, 234.23940706253052, 235.71500945091248, 237.19061183929443, 238.62060928344727, 240.0506067276001, 241.45667028427124, 242.86273384094238, 244.26653671264648, 245.6703395843506, 247.13127255439758, 248.59220552444458, 250.02624487876892, 251.46028423309326, 252.88211178779602, 254.30393934249878, 255.7029571533203, 257.10197496414185, 258.48412322998047, 259.8662714958191, 261.2593426704407, 262.65241384506226, 264.06133008003235, 265.47024631500244, 266.8607745170593, 268.2513027191162, 269.62802815437317, 271.0047535896301, 272.38621497154236, 273.7676763534546, 275.19539880752563, 276.6231212615967, 278.02601289749146, 279.42890453338623, 280.8403341770172, 282.2517638206482, 283.67249488830566, 285.09322595596313, 286.51790595054626, 287.9425859451294, 289.3680477142334, 290.7935094833374, 293.13416814804077, 295.47482681274414]
[32.05833333333333, 32.05833333333333, 43.925, 43.925, 52.6, 52.6, 56.641666666666666, 56.641666666666666, 61.63333333333333, 61.63333333333333, 61.95, 61.95, 64.36666666666666, 64.36666666666666, 65.28333333333333, 65.28333333333333, 68.18333333333334, 68.18333333333334, 69.64166666666667, 69.64166666666667, 68.79166666666667, 68.79166666666667, 69.73333333333333, 69.73333333333333, 71.56666666666666, 71.56666666666666, 72.9, 72.9, 72.90833333333333, 72.90833333333333, 72.81666666666666, 72.81666666666666, 74.1, 74.1, 75.14166666666667, 75.14166666666667, 75.65, 75.65, 75.15, 75.15, 76.54166666666667, 76.54166666666667, 76.58333333333333, 76.58333333333333, 76.74166666666666, 76.74166666666666, 76.30833333333334, 76.30833333333334, 77.20833333333333, 77.20833333333333, 77.225, 77.225, 77.46666666666667, 77.46666666666667, 77.9, 77.9, 77.98333333333333, 77.98333333333333, 77.95, 77.95, 78.46666666666667, 78.46666666666667, 78.85, 78.85, 79.05, 79.05, 79.09166666666667, 79.09166666666667, 78.61666666666666, 78.61666666666666, 78.78333333333333, 78.78333333333333, 78.51666666666667, 78.51666666666667, 79.08333333333333, 79.08333333333333, 78.64166666666667, 78.64166666666667, 78.41666666666667, 78.41666666666667, 78.325, 78.325, 78.2, 78.2, 78.49166666666666, 78.49166666666666, 78.15, 78.15, 78.35, 78.35, 78.725, 78.725, 77.875, 77.875, 77.78333333333333, 77.78333333333333, 77.85, 77.85, 79.21666666666667, 79.21666666666667, 78.4, 78.4, 78.56666666666666, 78.56666666666666, 78.58333333333333, 78.58333333333333, 78.70833333333333, 78.70833333333333, 79.225, 79.225, 79.38333333333334, 79.38333333333334, 79.775, 79.775, 79.89166666666667, 79.89166666666667, 79.24166666666666, 79.24166666666666, 79.35833333333333, 79.35833333333333, 79.51666666666667, 79.51666666666667, 79.31666666666666, 79.31666666666666, 78.96666666666667, 78.96666666666667, 79.26666666666667, 79.26666666666667, 79.08333333333333, 79.08333333333333, 78.575, 78.575, 78.05, 78.05, 78.51666666666667, 78.51666666666667, 78.70833333333333, 78.70833333333333, 78.94166666666666, 78.94166666666666, 78.75833333333334, 78.75833333333334, 79.09166666666667, 79.09166666666667, 79.01666666666667, 79.01666666666667, 79.26666666666667, 79.26666666666667, 79.25, 79.25, 79.63333333333334, 79.63333333333334, 80.01666666666667, 80.01666666666667, 80.10833333333333, 80.10833333333333, 80.46666666666667, 80.46666666666667, 79.76666666666667, 79.76666666666667, 79.75833333333334, 79.75833333333334, 79.73333333333333, 79.73333333333333, 79.03333333333333, 79.03333333333333, 79.35, 79.35, 80.23333333333333, 80.23333333333333, 79.875, 79.875, 79.73333333333333, 79.73333333333333, 79.83333333333333, 79.83333333333333, 79.58333333333333, 79.58333333333333, 80.33333333333333, 80.33333333333333, 80.99166666666666, 80.99166666666666, 81.43333333333334, 81.43333333333334, 81.25, 81.25, 81.325, 81.325, 80.68333333333334, 80.68333333333334, 80.225, 80.225, 80.89166666666667, 80.89166666666667, 81.35833333333333, 81.35833333333333, 81.725, 81.725, 81.975, 81.975, 81.01666666666667, 81.01666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  prox  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: prox , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 2, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
prox
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.294, Test loss: 2.014, Test accuracy: 21.85
Round   0, Global train loss: 1.294, Global test loss: 2.303, Global test accuracy: 13.53
Round   1, Train loss: 1.078, Test loss: 1.664, Test accuracy: 37.49
Round   1, Global train loss: 1.078, Global test loss: 2.198, Global test accuracy: 22.91
Round   2, Train loss: 0.990, Test loss: 1.355, Test accuracy: 49.56
Round   2, Global train loss: 0.990, Global test loss: 2.177, Global test accuracy: 29.67
Round   3, Train loss: 0.968, Test loss: 1.253, Test accuracy: 54.97
Round   3, Global train loss: 0.968, Global test loss: 2.269, Global test accuracy: 31.39
Round   4, Train loss: 0.990, Test loss: 0.988, Test accuracy: 60.41
Round   4, Global train loss: 0.990, Global test loss: 1.972, Global test accuracy: 35.27
Round   5, Train loss: 1.002, Test loss: 0.946, Test accuracy: 61.29
Round   5, Global train loss: 1.002, Global test loss: 1.829, Global test accuracy: 39.05
Round   6, Train loss: 0.995, Test loss: 0.912, Test accuracy: 62.86
Round   6, Global train loss: 0.995, Global test loss: 1.839, Global test accuracy: 37.47
Round   7, Train loss: 0.778, Test loss: 0.918, Test accuracy: 63.92
Round   7, Global train loss: 0.778, Global test loss: 2.156, Global test accuracy: 30.98
Round   8, Train loss: 0.837, Test loss: 0.811, Test accuracy: 66.83
Round   8, Global train loss: 0.837, Global test loss: 2.083, Global test accuracy: 32.66
Round   9, Train loss: 0.999, Test loss: 0.802, Test accuracy: 67.61
Round   9, Global train loss: 0.999, Global test loss: 2.082, Global test accuracy: 32.27
Round  10, Train loss: 0.915, Test loss: 0.802, Test accuracy: 68.13
Round  10, Global train loss: 0.915, Global test loss: 2.162, Global test accuracy: 36.56
Round  11, Train loss: 0.949, Test loss: 0.786, Test accuracy: 68.53
Round  11, Global train loss: 0.949, Global test loss: 1.918, Global test accuracy: 33.44
Round  12, Train loss: 0.773, Test loss: 0.755, Test accuracy: 70.32
Round  12, Global train loss: 0.773, Global test loss: 1.805, Global test accuracy: 37.92
Round  13, Train loss: 0.938, Test loss: 0.746, Test accuracy: 70.83
Round  13, Global train loss: 0.938, Global test loss: 1.665, Global test accuracy: 41.31
Round  14, Train loss: 0.822, Test loss: 0.748, Test accuracy: 71.08
Round  14, Global train loss: 0.822, Global test loss: 1.635, Global test accuracy: 42.85
Round  15, Train loss: 0.711, Test loss: 0.731, Test accuracy: 72.05
Round  15, Global train loss: 0.711, Global test loss: 1.947, Global test accuracy: 34.55
Round  16, Train loss: 0.802, Test loss: 0.735, Test accuracy: 72.00
Round  16, Global train loss: 0.802, Global test loss: 1.896, Global test accuracy: 37.20
Round  17, Train loss: 0.890, Test loss: 0.721, Test accuracy: 72.38
Round  17, Global train loss: 0.890, Global test loss: 1.559, Global test accuracy: 46.42
Round  18, Train loss: 0.806, Test loss: 0.706, Test accuracy: 73.00
Round  18, Global train loss: 0.806, Global test loss: 1.604, Global test accuracy: 45.09
Round  19, Train loss: 0.861, Test loss: 0.720, Test accuracy: 72.18
Round  19, Global train loss: 0.861, Global test loss: 1.614, Global test accuracy: 44.45
Round  20, Train loss: 0.797, Test loss: 0.690, Test accuracy: 74.14
Round  20, Global train loss: 0.797, Global test loss: 1.594, Global test accuracy: 46.42
Round  21, Train loss: 0.684, Test loss: 0.688, Test accuracy: 74.09
Round  21, Global train loss: 0.684, Global test loss: 1.680, Global test accuracy: 41.81
Round  22, Train loss: 0.706, Test loss: 0.691, Test accuracy: 73.68
Round  22, Global train loss: 0.706, Global test loss: 1.616, Global test accuracy: 46.94
Round  23, Train loss: 0.732, Test loss: 0.682, Test accuracy: 74.06
Round  23, Global train loss: 0.732, Global test loss: 1.616, Global test accuracy: 44.87
Round  24, Train loss: 0.784, Test loss: 0.665, Test accuracy: 75.16
Round  24, Global train loss: 0.784, Global test loss: 1.588, Global test accuracy: 44.88
Round  25, Train loss: 0.772, Test loss: 0.664, Test accuracy: 75.03
Round  25, Global train loss: 0.772, Global test loss: 1.480, Global test accuracy: 50.23
Round  26, Train loss: 0.650, Test loss: 0.690, Test accuracy: 73.96
Round  26, Global train loss: 0.650, Global test loss: 1.643, Global test accuracy: 43.83
Round  27, Train loss: 0.766, Test loss: 0.701, Test accuracy: 73.29
Round  27, Global train loss: 0.766, Global test loss: 1.371, Global test accuracy: 53.48
Round  28, Train loss: 0.673, Test loss: 0.686, Test accuracy: 74.22
Round  28, Global train loss: 0.673, Global test loss: 1.523, Global test accuracy: 47.76
Round  29, Train loss: 0.719, Test loss: 0.676, Test accuracy: 75.23
Round  29, Global train loss: 0.719, Global test loss: 1.443, Global test accuracy: 51.38
Round  30, Train loss: 0.720, Test loss: 0.689, Test accuracy: 74.45
Round  30, Global train loss: 0.720, Global test loss: 1.520, Global test accuracy: 49.32
Round  31, Train loss: 0.838, Test loss: 0.675, Test accuracy: 74.62
Round  31, Global train loss: 0.838, Global test loss: 1.423, Global test accuracy: 52.95
Round  32, Train loss: 0.578, Test loss: 0.683, Test accuracy: 74.49
Round  32, Global train loss: 0.578, Global test loss: 1.408, Global test accuracy: 51.05
Round  33, Train loss: 0.692, Test loss: 0.664, Test accuracy: 75.65
Round  33, Global train loss: 0.692, Global test loss: 1.663, Global test accuracy: 44.65
Round  34, Train loss: 0.709, Test loss: 0.672, Test accuracy: 74.54
Round  34, Global train loss: 0.709, Global test loss: 1.345, Global test accuracy: 53.41
Round  35, Train loss: 0.714, Test loss: 0.665, Test accuracy: 74.67
Round  35, Global train loss: 0.714, Global test loss: 1.506, Global test accuracy: 48.48
Round  36, Train loss: 0.663, Test loss: 0.657, Test accuracy: 74.92
Round  36, Global train loss: 0.663, Global test loss: 1.361, Global test accuracy: 55.13
Round  37, Train loss: 0.776, Test loss: 0.665, Test accuracy: 74.12
Round  37, Global train loss: 0.776, Global test loss: 1.460, Global test accuracy: 50.56
Round  38, Train loss: 0.635, Test loss: 0.661, Test accuracy: 74.24
Round  38, Global train loss: 0.635, Global test loss: 1.344, Global test accuracy: 54.27
Round  39, Train loss: 0.687, Test loss: 0.658, Test accuracy: 75.27
Round  39, Global train loss: 0.687, Global test loss: 1.568, Global test accuracy: 48.83
Round  40, Train loss: 0.666, Test loss: 0.668, Test accuracy: 75.12
Round  40, Global train loss: 0.666, Global test loss: 1.741, Global test accuracy: 45.22
Round  41, Train loss: 0.546, Test loss: 0.668, Test accuracy: 75.58
Round  41, Global train loss: 0.546, Global test loss: 1.748, Global test accuracy: 45.21
Round  42, Train loss: 0.588, Test loss: 0.658, Test accuracy: 75.62
Round  42, Global train loss: 0.588, Global test loss: 1.258, Global test accuracy: 57.55
Round  43, Train loss: 0.634, Test loss: 0.647, Test accuracy: 75.85
Round  43, Global train loss: 0.634, Global test loss: 1.357, Global test accuracy: 54.00
Round  44, Train loss: 0.540, Test loss: 0.644, Test accuracy: 76.11
Round  44, Global train loss: 0.540, Global test loss: 1.446, Global test accuracy: 52.85
Round  45, Train loss: 0.567, Test loss: 0.623, Test accuracy: 76.34
Round  45, Global train loss: 0.567, Global test loss: 1.589, Global test accuracy: 47.62
Round  46, Train loss: 0.539, Test loss: 0.614, Test accuracy: 77.25
Round  46, Global train loss: 0.539, Global test loss: 1.306, Global test accuracy: 55.71
Round  47, Train loss: 0.630, Test loss: 0.632, Test accuracy: 77.12
Round  47, Global train loss: 0.630, Global test loss: 1.580, Global test accuracy: 48.70
Round  48, Train loss: 0.480, Test loss: 0.614, Test accuracy: 77.78
Round  48, Global train loss: 0.480, Global test loss: 1.228, Global test accuracy: 58.02
Round  49, Train loss: 0.502, Test loss: 0.631, Test accuracy: 77.27
Round  49, Global train loss: 0.502, Global test loss: 1.315, Global test accuracy: 57.23
Round  50, Train loss: 0.565, Test loss: 0.631, Test accuracy: 77.57
Round  50, Global train loss: 0.565, Global test loss: 1.288, Global test accuracy: 57.79
Round  51, Train loss: 0.473, Test loss: 0.624, Test accuracy: 77.79
Round  51, Global train loss: 0.473, Global test loss: 1.352, Global test accuracy: 54.85
Round  52, Train loss: 0.473, Test loss: 0.623, Test accuracy: 77.78
Round  52, Global train loss: 0.473, Global test loss: 1.263, Global test accuracy: 55.81
Round  53, Train loss: 0.437, Test loss: 0.619, Test accuracy: 77.86
Round  53, Global train loss: 0.437, Global test loss: 1.295, Global test accuracy: 57.12
Round  54, Train loss: 0.460, Test loss: 0.605, Test accuracy: 78.30
Round  54, Global train loss: 0.460, Global test loss: 1.583, Global test accuracy: 51.89
Round  55, Train loss: 0.459, Test loss: 0.604, Test accuracy: 78.38
Round  55, Global train loss: 0.459, Global test loss: 1.665, Global test accuracy: 49.58
Round  56, Train loss: 0.529, Test loss: 0.583, Test accuracy: 78.95
Round  56, Global train loss: 0.529, Global test loss: 1.322, Global test accuracy: 56.73
Round  57, Train loss: 0.532, Test loss: 0.575, Test accuracy: 79.47
Round  57, Global train loss: 0.532, Global test loss: 1.288, Global test accuracy: 56.36
Round  58, Train loss: 0.535, Test loss: 0.577, Test accuracy: 79.06
Round  58, Global train loss: 0.535, Global test loss: 1.465, Global test accuracy: 52.09
Round  59, Train loss: 0.519, Test loss: 0.593, Test accuracy: 78.40
Round  59, Global train loss: 0.519, Global test loss: 1.318, Global test accuracy: 54.12
Round  60, Train loss: 0.462, Test loss: 0.591, Test accuracy: 78.68
Round  60, Global train loss: 0.462, Global test loss: 1.479, Global test accuracy: 47.84
Round  61, Train loss: 0.460, Test loss: 0.608, Test accuracy: 77.90
Round  61, Global train loss: 0.460, Global test loss: 1.281, Global test accuracy: 56.84
Round  62, Train loss: 0.463, Test loss: 0.616, Test accuracy: 77.72
Round  62, Global train loss: 0.463, Global test loss: 1.413, Global test accuracy: 52.92
Round  63, Train loss: 0.633, Test loss: 0.619, Test accuracy: 77.42
Round  63, Global train loss: 0.633, Global test loss: 1.381, Global test accuracy: 54.88
Round  64, Train loss: 0.512, Test loss: 0.621, Test accuracy: 77.35
Round  64, Global train loss: 0.512, Global test loss: 1.523, Global test accuracy: 50.66
Round  65, Train loss: 0.444, Test loss: 0.623, Test accuracy: 77.08
Round  65, Global train loss: 0.444, Global test loss: 1.376, Global test accuracy: 54.87
Round  66, Train loss: 0.406, Test loss: 0.624, Test accuracy: 77.14
Round  66, Global train loss: 0.406, Global test loss: 1.363, Global test accuracy: 54.50
Round  67, Train loss: 0.405, Test loss: 0.631, Test accuracy: 76.97
Round  67, Global train loss: 0.405, Global test loss: 1.305, Global test accuracy: 56.48
Round  68, Train loss: 0.468, Test loss: 0.652, Test accuracy: 76.10
Round  68, Global train loss: 0.468, Global test loss: 1.326, Global test accuracy: 56.20
Round  69, Train loss: 0.485, Test loss: 0.650, Test accuracy: 75.93
Round  69, Global train loss: 0.485, Global test loss: 1.392, Global test accuracy: 54.73
Round  70, Train loss: 0.418, Test loss: 0.654, Test accuracy: 76.13
Round  70, Global train loss: 0.418, Global test loss: 1.287, Global test accuracy: 56.62
Round  71, Train loss: 0.570, Test loss: 0.636, Test accuracy: 77.08
Round  71, Global train loss: 0.570, Global test loss: 1.301, Global test accuracy: 58.24
Round  72, Train loss: 0.424, Test loss: 0.650, Test accuracy: 76.74
Round  72, Global train loss: 0.424, Global test loss: 1.511, Global test accuracy: 52.47
Round  73, Train loss: 0.496, Test loss: 0.619, Test accuracy: 78.12
Round  73, Global train loss: 0.496, Global test loss: 1.274, Global test accuracy: 58.54
Round  74, Train loss: 0.434, Test loss: 0.620, Test accuracy: 78.74
Round  74, Global train loss: 0.434, Global test loss: 1.439, Global test accuracy: 54.12
Round  75, Train loss: 0.450, Test loss: 0.646, Test accuracy: 77.90
Round  75, Global train loss: 0.450, Global test loss: 1.327, Global test accuracy: 55.91
Round  76, Train loss: 0.410, Test loss: 0.657, Test accuracy: 77.12
Round  76, Global train loss: 0.410, Global test loss: 1.376, Global test accuracy: 57.22
Round  77, Train loss: 0.478, Test loss: 0.630, Test accuracy: 77.78
Round  77, Global train loss: 0.478, Global test loss: 1.320, Global test accuracy: 57.53
Round  78, Train loss: 0.410, Test loss: 0.645, Test accuracy: 77.48
Round  78, Global train loss: 0.410, Global test loss: 1.316, Global test accuracy: 59.26
Round  79, Train loss: 0.415, Test loss: 0.614, Test accuracy: 78.53
Round  79, Global train loss: 0.415, Global test loss: 1.565, Global test accuracy: 50.71
Round  80, Train loss: 0.394, Test loss: 0.621, Test accuracy: 77.99
Round  80, Global train loss: 0.394, Global test loss: 1.335, Global test accuracy: 58.24
Round  81, Train loss: 0.439, Test loss: 0.639, Test accuracy: 78.00
Round  81, Global train loss: 0.439, Global test loss: 1.503, Global test accuracy: 53.82
Round  82, Train loss: 0.438, Test loss: 0.639, Test accuracy: 77.88
Round  82, Global train loss: 0.438, Global test loss: 1.540, Global test accuracy: 52.77
Round  83, Train loss: 0.325, Test loss: 0.659, Test accuracy: 77.81
Round  83, Global train loss: 0.325, Global test loss: 1.356, Global test accuracy: 56.83
Round  84, Train loss: 0.467, Test loss: 0.671, Test accuracy: 77.59
Round  84, Global train loss: 0.467, Global test loss: 1.507, Global test accuracy: 54.44
Round  85, Train loss: 0.502, Test loss: 0.678, Test accuracy: 77.24
Round  85, Global train loss: 0.502, Global test loss: 1.323, Global test accuracy: 57.39
Round  86, Train loss: 0.446, Test loss: 0.666, Test accuracy: 77.82
Round  86, Global train loss: 0.446, Global test loss: 1.500, Global test accuracy: 54.05
Round  87, Train loss: 0.424, Test loss: 0.668, Test accuracy: 77.42
Round  87, Global train loss: 0.424, Global test loss: 1.393, Global test accuracy: 56.89
Round  88, Train loss: 0.377, Test loss: 0.651, Test accuracy: 77.79
Round  88, Global train loss: 0.377, Global test loss: 1.353, Global test accuracy: 58.94
Round  89, Train loss: 0.462, Test loss: 0.689, Test accuracy: 76.72
Round  89, Global train loss: 0.462, Global test loss: 1.663, Global test accuracy: 52.02
Round  90, Train loss: 0.475, Test loss: 0.681, Test accuracy: 77.32
Round  90, Global train loss: 0.475, Global test loss: 1.548, Global test accuracy: 51.01
Round  91, Train loss: 0.353, Test loss: 0.691, Test accuracy: 77.02
Round  91, Global train loss: 0.353, Global test loss: 1.548, Global test accuracy: 54.30
Round  92, Train loss: 0.436, Test loss: 0.675, Test accuracy: 76.58
Round  92, Global train loss: 0.436, Global test loss: 1.479, Global test accuracy: 54.51
Round  93, Train loss: 0.322, Test loss: 0.673, Test accuracy: 76.97
Round  93, Global train loss: 0.322, Global test loss: 1.593, Global test accuracy: 53.80
Round  94, Train loss: 0.424, Test loss: 0.663, Test accuracy: 77.86
Round  94, Global train loss: 0.424, Global test loss: 1.296, Global test accuracy: 58.23
Round  95, Train loss: 0.421, Test loss: 0.685, Test accuracy: 76.74
Round  95, Global train loss: 0.421, Global test loss: 1.441, Global test accuracy: 55.26
Round  96, Train loss: 0.416, Test loss: 0.673, Test accuracy: 77.42
Round  96, Global train loss: 0.416, Global test loss: 1.353, Global test accuracy: 56.72
Round  97, Train loss: 0.418, Test loss: 0.666, Test accuracy: 77.50
Round  97, Global train loss: 0.418, Global test loss: 1.421, Global test accuracy: 55.58
Round  98, Train loss: 0.300, Test loss: 0.677, Test accuracy: 77.60
Round  98, Global train loss: 0.300, Global test loss: 1.328, Global test accuracy: 59.31
Round  99, Train loss: 0.336, Test loss: 0.701, Test accuracy: 77.38
Round  99, Global train loss: 0.336, Global test loss: 1.674, Global test accuracy: 54.28
Final Round, Train loss: 0.298, Test loss: 0.686, Test accuracy: 78.79
Final Round, Global train loss: 0.298, Global test loss: 1.674, Global test accuracy: 54.28
Average accuracy final 10 rounds: 77.23916666666666 

Average global accuracy final 10 rounds: 55.3 

1905.8718123435974
[1.8228695392608643, 3.6457390785217285, 5.200925350189209, 6.7561116218566895, 8.245012283325195, 9.733912944793701, 11.230997323989868, 12.728081703186035, 14.234616041183472, 15.741150379180908, 17.28575897216797, 18.83036756515503, 20.355971097946167, 21.881574630737305, 23.383504152297974, 24.885433673858643, 26.414859771728516, 27.94428586959839, 29.413262128829956, 30.882238388061523, 32.3671178817749, 33.85199737548828, 35.322031021118164, 36.79206466674805, 38.280829668045044, 39.76959466934204, 41.23612713813782, 42.702659606933594, 44.193354845047, 45.6840500831604, 47.17203378677368, 48.66001749038696, 50.1424503326416, 51.62488317489624, 53.09972524642944, 54.57456731796265, 56.06369352340698, 57.55281972885132, 59.05771088600159, 60.562602043151855, 62.05331778526306, 63.54403352737427, 65.02941012382507, 66.51478672027588, 67.98533701896667, 69.45588731765747, 70.94468879699707, 72.43349027633667, 73.93311262130737, 75.43273496627808, 76.92421865463257, 78.41570234298706, 79.90081572532654, 81.38592910766602, 82.89087319374084, 84.39581727981567, 85.89560985565186, 87.39540243148804, 88.89213681221008, 90.38887119293213, 91.88230037689209, 93.37572956085205, 94.851154088974, 96.32657861709595, 97.8321807384491, 99.33778285980225, 100.8751163482666, 102.41244983673096, 103.91481804847717, 105.41718626022339, 106.90599203109741, 108.39479780197144, 109.88056945800781, 111.36634111404419, 112.85091781616211, 114.33549451828003, 115.83278036117554, 117.33006620407104, 118.80195617675781, 120.27384614944458, 121.74571299552917, 123.21757984161377, 124.69337630271912, 126.16917276382446, 127.64310789108276, 129.11704301834106, 130.58277201652527, 132.04850101470947, 133.62861514091492, 135.20872926712036, 136.69697189331055, 138.18521451950073, 139.68832397460938, 141.19143342971802, 142.6642870903015, 144.137140750885, 145.62677311897278, 147.11640548706055, 148.64457416534424, 150.17274284362793, 151.72876524925232, 153.2847876548767, 154.77256894111633, 156.26035022735596, 157.74287605285645, 159.22540187835693, 160.73636412620544, 162.24732637405396, 163.77149176597595, 165.29565715789795, 166.81231880187988, 168.32898044586182, 169.81149220466614, 171.29400396347046, 172.773211479187, 174.25241899490356, 175.74190711975098, 177.2313952445984, 178.7218418121338, 180.2122883796692, 181.70437741279602, 183.19646644592285, 184.67802929878235, 186.15959215164185, 187.67303729057312, 189.1864824295044, 190.65058207511902, 192.11468172073364, 193.60122632980347, 195.0877709388733, 196.5853714942932, 198.08297204971313, 199.59178948402405, 201.10060691833496, 202.597421169281, 204.09423542022705, 205.59296441078186, 207.09169340133667, 208.61928844451904, 210.14688348770142, 211.6187891960144, 213.0906949043274, 214.61013221740723, 216.12956953048706, 217.64503860473633, 219.1605076789856, 220.66811537742615, 222.1757230758667, 223.6769061088562, 225.1780891418457, 226.66773629188538, 228.15738344192505, 229.6356658935547, 231.11394834518433, 232.59831547737122, 234.0826826095581, 235.5829701423645, 237.0832576751709, 238.5922291278839, 240.10120058059692, 241.57934498786926, 243.0574893951416, 244.53466176986694, 246.01183414459229, 247.48341464996338, 248.95499515533447, 250.4393289089203, 251.9236626625061, 253.4200119972229, 254.9163613319397, 256.41485595703125, 257.9133505821228, 259.39989829063416, 260.8864459991455, 262.4028317928314, 263.91921758651733, 265.4040033817291, 266.8887891769409, 268.38748145103455, 269.8861737251282, 271.3907313346863, 272.8952889442444, 274.4136230945587, 275.93195724487305, 277.4658839702606, 278.9998106956482, 280.502480506897, 282.00515031814575, 283.48333859443665, 284.96152687072754, 286.4599220752716, 287.9583172798157, 289.4685266017914, 290.9787359237671, 292.46663331985474, 293.9545307159424, 295.43077206611633, 296.9070134162903, 298.3935196399689, 299.88002586364746, 302.39808177948, 304.9161376953125]
[21.85, 21.85, 37.49166666666667, 37.49166666666667, 49.55833333333333, 49.55833333333333, 54.96666666666667, 54.96666666666667, 60.40833333333333, 60.40833333333333, 61.291666666666664, 61.291666666666664, 62.858333333333334, 62.858333333333334, 63.916666666666664, 63.916666666666664, 66.83333333333333, 66.83333333333333, 67.60833333333333, 67.60833333333333, 68.13333333333334, 68.13333333333334, 68.53333333333333, 68.53333333333333, 70.31666666666666, 70.31666666666666, 70.825, 70.825, 71.075, 71.075, 72.05, 72.05, 72.0, 72.0, 72.375, 72.375, 73.0, 73.0, 72.18333333333334, 72.18333333333334, 74.14166666666667, 74.14166666666667, 74.09166666666667, 74.09166666666667, 73.68333333333334, 73.68333333333334, 74.05833333333334, 74.05833333333334, 75.15833333333333, 75.15833333333333, 75.03333333333333, 75.03333333333333, 73.95833333333333, 73.95833333333333, 73.29166666666667, 73.29166666666667, 74.225, 74.225, 75.23333333333333, 75.23333333333333, 74.45, 74.45, 74.625, 74.625, 74.49166666666666, 74.49166666666666, 75.65, 75.65, 74.54166666666667, 74.54166666666667, 74.675, 74.675, 74.91666666666667, 74.91666666666667, 74.11666666666666, 74.11666666666666, 74.24166666666666, 74.24166666666666, 75.26666666666667, 75.26666666666667, 75.125, 75.125, 75.58333333333333, 75.58333333333333, 75.625, 75.625, 75.85, 75.85, 76.10833333333333, 76.10833333333333, 76.34166666666667, 76.34166666666667, 77.25, 77.25, 77.11666666666666, 77.11666666666666, 77.78333333333333, 77.78333333333333, 77.26666666666667, 77.26666666666667, 77.56666666666666, 77.56666666666666, 77.79166666666667, 77.79166666666667, 77.78333333333333, 77.78333333333333, 77.85833333333333, 77.85833333333333, 78.3, 78.3, 78.375, 78.375, 78.95, 78.95, 79.475, 79.475, 79.05833333333334, 79.05833333333334, 78.4, 78.4, 78.68333333333334, 78.68333333333334, 77.9, 77.9, 77.71666666666667, 77.71666666666667, 77.41666666666667, 77.41666666666667, 77.35, 77.35, 77.08333333333333, 77.08333333333333, 77.14166666666667, 77.14166666666667, 76.96666666666667, 76.96666666666667, 76.1, 76.1, 75.93333333333334, 75.93333333333334, 76.13333333333334, 76.13333333333334, 77.075, 77.075, 76.74166666666666, 76.74166666666666, 78.11666666666666, 78.11666666666666, 78.74166666666666, 78.74166666666666, 77.9, 77.9, 77.11666666666666, 77.11666666666666, 77.775, 77.775, 77.48333333333333, 77.48333333333333, 78.53333333333333, 78.53333333333333, 77.99166666666666, 77.99166666666666, 78.0, 78.0, 77.875, 77.875, 77.80833333333334, 77.80833333333334, 77.59166666666667, 77.59166666666667, 77.24166666666666, 77.24166666666666, 77.81666666666666, 77.81666666666666, 77.41666666666667, 77.41666666666667, 77.79166666666667, 77.79166666666667, 76.71666666666667, 76.71666666666667, 77.31666666666666, 77.31666666666666, 77.01666666666667, 77.01666666666667, 76.58333333333333, 76.58333333333333, 76.96666666666667, 76.96666666666667, 77.85833333333333, 77.85833333333333, 76.74166666666666, 76.74166666666666, 77.425, 77.425, 77.5, 77.5, 77.6, 77.6, 77.38333333333334, 77.38333333333334, 78.79166666666667, 78.79166666666667]/home/ChenSM/code/FL_HLS/FedProx.py:100: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1630.)
  d_p.add_(weight_decay, p.data)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Co-teaching%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 0, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.605, Test loss: 2.093, Test accuracy: 18.18
Round   1, Train loss: 1.209, Test loss: 1.810, Test accuracy: 30.44
Round   2, Train loss: 1.222, Test loss: 1.494, Test accuracy: 40.28
Round   3, Train loss: 1.118, Test loss: 1.404, Test accuracy: 46.92
Round   4, Train loss: 0.949, Test loss: 1.063, Test accuracy: 57.72
Round   5, Train loss: 0.962, Test loss: 1.063, Test accuracy: 56.91
Round   6, Train loss: 0.985, Test loss: 1.011, Test accuracy: 59.75
Round   7, Train loss: 0.885, Test loss: 0.958, Test accuracy: 62.38
Round   8, Train loss: 1.011, Test loss: 0.851, Test accuracy: 65.22
Round   9, Train loss: 1.004, Test loss: 0.837, Test accuracy: 66.83
Round  10, Train loss: 0.972, Test loss: 0.821, Test accuracy: 66.84
Round  11, Train loss: 0.812, Test loss: 0.838, Test accuracy: 67.33
Round  12, Train loss: 0.920, Test loss: 0.745, Test accuracy: 71.00
Round  13, Train loss: 0.926, Test loss: 0.722, Test accuracy: 72.43
Round  14, Train loss: 0.881, Test loss: 0.749, Test accuracy: 71.66
Round  15, Train loss: 0.691, Test loss: 0.745, Test accuracy: 71.98
Round  16, Train loss: 0.853, Test loss: 0.717, Test accuracy: 73.24
Round  17, Train loss: 0.793, Test loss: 0.708, Test accuracy: 74.42
Round  18, Train loss: 0.722, Test loss: 0.685, Test accuracy: 74.97
Round  19, Train loss: 0.799, Test loss: 0.667, Test accuracy: 75.04
Round  20, Train loss: 0.707, Test loss: 0.671, Test accuracy: 74.91
Round  21, Train loss: 0.703, Test loss: 0.666, Test accuracy: 74.86
Round  22, Train loss: 0.689, Test loss: 0.677, Test accuracy: 74.08
Round  23, Train loss: 0.777, Test loss: 0.667, Test accuracy: 74.65
Round  24, Train loss: 0.809, Test loss: 0.634, Test accuracy: 77.28
Round  25, Train loss: 0.738, Test loss: 0.617, Test accuracy: 77.44
Round  26, Train loss: 0.681, Test loss: 0.603, Test accuracy: 78.03
Round  27, Train loss: 0.673, Test loss: 0.599, Test accuracy: 78.03
Round  28, Train loss: 0.620, Test loss: 0.619, Test accuracy: 76.84
Round  29, Train loss: 0.787, Test loss: 0.600, Test accuracy: 77.29
Round  30, Train loss: 0.692, Test loss: 0.598, Test accuracy: 77.78
Round  31, Train loss: 0.741, Test loss: 0.585, Test accuracy: 78.67
Round  32, Train loss: 0.755, Test loss: 0.583, Test accuracy: 78.93
Round  33, Train loss: 0.738, Test loss: 0.589, Test accuracy: 78.59
Round  34, Train loss: 0.718, Test loss: 0.579, Test accuracy: 79.84
Round  35, Train loss: 0.680, Test loss: 0.574, Test accuracy: 79.61
Round  36, Train loss: 0.595, Test loss: 0.565, Test accuracy: 79.78
Round  37, Train loss: 0.668, Test loss: 0.546, Test accuracy: 80.49
Round  38, Train loss: 0.596, Test loss: 0.555, Test accuracy: 79.76
Round  39, Train loss: 0.724, Test loss: 0.567, Test accuracy: 79.80
Round  40, Train loss: 0.688, Test loss: 0.565, Test accuracy: 79.87
Round  41, Train loss: 0.528, Test loss: 0.552, Test accuracy: 80.44
Round  42, Train loss: 0.638, Test loss: 0.542, Test accuracy: 80.74
Round  43, Train loss: 0.618, Test loss: 0.532, Test accuracy: 81.03
Round  44, Train loss: 0.585, Test loss: 0.544, Test accuracy: 80.37
Round  45, Train loss: 0.597, Test loss: 0.539, Test accuracy: 81.10
Round  46, Train loss: 0.563, Test loss: 0.542, Test accuracy: 81.00
Round  47, Train loss: 0.551, Test loss: 0.545, Test accuracy: 81.09
Round  48, Train loss: 0.616, Test loss: 0.523, Test accuracy: 81.42
Round  49, Train loss: 0.489, Test loss: 0.537, Test accuracy: 80.76
Round  50, Train loss: 0.549, Test loss: 0.529, Test accuracy: 81.57
Round  51, Train loss: 0.588, Test loss: 0.532, Test accuracy: 81.26
Round  52, Train loss: 0.595, Test loss: 0.518, Test accuracy: 81.93
Round  53, Train loss: 0.528, Test loss: 0.523, Test accuracy: 81.67
Round  54, Train loss: 0.531, Test loss: 0.521, Test accuracy: 81.95
Round  55, Train loss: 0.543, Test loss: 0.520, Test accuracy: 82.07
Round  56, Train loss: 0.544, Test loss: 0.524, Test accuracy: 81.76
Round  57, Train loss: 0.678, Test loss: 0.520, Test accuracy: 81.76
Round  58, Train loss: 0.437, Test loss: 0.508, Test accuracy: 82.33
Round  59, Train loss: 0.643, Test loss: 0.498, Test accuracy: 82.86
Round  60, Train loss: 0.518, Test loss: 0.516, Test accuracy: 82.11
Round  61, Train loss: 0.543, Test loss: 0.511, Test accuracy: 82.27
Round  62, Train loss: 0.600, Test loss: 0.509, Test accuracy: 82.36
Round  63, Train loss: 0.651, Test loss: 0.503, Test accuracy: 83.00
Round  64, Train loss: 0.635, Test loss: 0.502, Test accuracy: 82.67
Round  65, Train loss: 0.499, Test loss: 0.491, Test accuracy: 82.96
Round  66, Train loss: 0.585, Test loss: 0.500, Test accuracy: 82.86
Round  67, Train loss: 0.532, Test loss: 0.501, Test accuracy: 83.03
Round  68, Train loss: 0.640, Test loss: 0.499, Test accuracy: 83.01
Round  69, Train loss: 0.593, Test loss: 0.499, Test accuracy: 82.77
Round  70, Train loss: 0.591, Test loss: 0.516, Test accuracy: 82.57
Round  71, Train loss: 0.531, Test loss: 0.495, Test accuracy: 83.07
Round  72, Train loss: 0.651, Test loss: 0.503, Test accuracy: 82.92
Round  73, Train loss: 0.502, Test loss: 0.498, Test accuracy: 83.22
Round  74, Train loss: 0.499, Test loss: 0.496, Test accuracy: 82.93
Round  75, Train loss: 0.476, Test loss: 0.492, Test accuracy: 83.38
Round  76, Train loss: 0.483, Test loss: 0.496, Test accuracy: 83.17
Round  77, Train loss: 0.382, Test loss: 0.496, Test accuracy: 83.22
Round  78, Train loss: 0.569, Test loss: 0.503, Test accuracy: 83.12
Round  79, Train loss: 0.522, Test loss: 0.506, Test accuracy: 82.79
Round  80, Train loss: 0.480, Test loss: 0.502, Test accuracy: 83.18
Round  81, Train loss: 0.516, Test loss: 0.507, Test accuracy: 83.37
Round  82, Train loss: 0.527, Test loss: 0.509, Test accuracy: 83.11
Round  83, Train loss: 0.408, Test loss: 0.506, Test accuracy: 82.98
Round  84, Train loss: 0.492, Test loss: 0.492, Test accuracy: 83.52
Round  85, Train loss: 0.530, Test loss: 0.497, Test accuracy: 83.27
Round  86, Train loss: 0.486, Test loss: 0.491, Test accuracy: 83.92
Round  87, Train loss: 0.478, Test loss: 0.500, Test accuracy: 83.26
Round  88, Train loss: 0.481, Test loss: 0.498, Test accuracy: 83.50
Round  89, Train loss: 0.467, Test loss: 0.511, Test accuracy: 83.06
Round  90, Train loss: 0.473, Test loss: 0.497, Test accuracy: 83.15
Round  91, Train loss: 0.486, Test loss: 0.502, Test accuracy: 83.13
Round  92, Train loss: 0.514, Test loss: 0.504, Test accuracy: 83.07
Round  93, Train loss: 0.451, Test loss: 0.498, Test accuracy: 83.36
Round  94, Train loss: 0.444, Test loss: 0.485, Test accuracy: 83.44
Round  95, Train loss: 0.389, Test loss: 0.491, Test accuracy: 83.57
Round  96, Train loss: 0.390, Test loss: 0.491, Test accuracy: 83.66
Round  97, Train loss: 0.424, Test loss: 0.492, Test accuracy: 83.38
Round  98, Train loss: 0.365, Test loss: 0.495, Test accuracy: 83.24
Round  99, Train loss: 0.427, Test loss: 0.489, Test accuracy: 83.78
Final Round, Train loss: 0.385, Test loss: 0.481, Test accuracy: 83.89
Average accuracy final 10 rounds: 83.37833333333333
1342.0228908061981
[1.9457666873931885, 3.6115341186523438, 5.2399373054504395, 6.921063661575317, 8.646062850952148, 10.380399465560913, 12.057152032852173, 13.709193229675293, 15.402108907699585, 17.066354274749756, 18.73653221130371, 20.417049646377563, 22.091492414474487, 23.7454617023468, 25.42828869819641, 27.099218368530273, 28.79505491256714, 30.43425965309143, 32.07527208328247, 33.777257204055786, 35.45932698249817, 37.141489028930664, 38.83786153793335, 40.51121163368225, 42.16496229171753, 43.83561134338379, 45.541240215301514, 47.24955344200134, 48.89840507507324, 50.54898524284363, 52.208805322647095, 53.929763317108154, 55.69517755508423, 57.41242575645447, 59.12889242172241, 60.86559748649597, 62.53468179702759, 64.20213866233826, 65.86041307449341, 67.49145722389221, 69.13847398757935, 70.796560049057, 72.4509437084198, 74.11094665527344, 75.75652599334717, 77.48811197280884, 79.23365330696106, 80.88276481628418, 82.59482002258301, 84.31669068336487, 86.01368021965027, 87.68337368965149, 89.42000770568848, 91.06662464141846, 92.77623748779297, 94.51152682304382, 96.28621196746826, 97.98874378204346, 99.69744038581848, 101.33066439628601, 102.98178291320801, 104.6300880908966, 106.27531552314758, 107.91323757171631, 109.55967617034912, 111.2098217010498, 112.8572268486023, 114.50275945663452, 116.15364646911621, 117.84335947036743, 119.51209306716919, 121.20655941963196, 122.90717029571533, 124.56591701507568, 126.2035071849823, 127.87020516395569, 129.5296323299408, 131.2985715866089, 133.0172758102417, 134.6980185508728, 136.37089610099792, 138.00609159469604, 139.65359473228455, 141.295574426651, 142.96892786026, 144.62363839149475, 146.28457617759705, 147.94869470596313, 149.59746384620667, 151.24109292030334, 152.94102215766907, 154.63328099250793, 156.3493778705597, 158.2009847164154, 159.9671802520752, 161.7443995475769, 163.46624755859375, 165.12575435638428, 166.77175116539001, 168.4050509929657, 170.65386819839478]
[18.175, 30.441666666666666, 40.28333333333333, 46.916666666666664, 57.71666666666667, 56.90833333333333, 59.75, 62.375, 65.21666666666667, 66.825, 66.84166666666667, 67.33333333333333, 71.0, 72.43333333333334, 71.65833333333333, 71.98333333333333, 73.24166666666666, 74.41666666666667, 74.96666666666667, 75.04166666666667, 74.90833333333333, 74.85833333333333, 74.08333333333333, 74.65, 77.275, 77.44166666666666, 78.03333333333333, 78.03333333333333, 76.84166666666667, 77.29166666666667, 77.78333333333333, 78.66666666666667, 78.93333333333334, 78.59166666666667, 79.84166666666667, 79.60833333333333, 79.78333333333333, 80.49166666666666, 79.75833333333334, 79.8, 79.86666666666666, 80.44166666666666, 80.74166666666666, 81.03333333333333, 80.36666666666666, 81.1, 81.0, 81.09166666666667, 81.425, 80.75833333333334, 81.56666666666666, 81.25833333333334, 81.93333333333334, 81.675, 81.95, 82.06666666666666, 81.75833333333334, 81.75833333333334, 82.33333333333333, 82.85833333333333, 82.10833333333333, 82.26666666666667, 82.35833333333333, 83.0, 82.66666666666667, 82.95833333333333, 82.85833333333333, 83.03333333333333, 83.00833333333334, 82.76666666666667, 82.56666666666666, 83.06666666666666, 82.91666666666667, 83.21666666666667, 82.93333333333334, 83.38333333333334, 83.175, 83.21666666666667, 83.125, 82.79166666666667, 83.18333333333334, 83.36666666666666, 83.10833333333333, 82.98333333333333, 83.51666666666667, 83.26666666666667, 83.925, 83.25833333333334, 83.5, 83.05833333333334, 83.15, 83.13333333333334, 83.06666666666666, 83.35833333333333, 83.44166666666666, 83.56666666666666, 83.65833333333333, 83.38333333333334, 83.24166666666666, 83.78333333333333, 83.89166666666667]
RFL.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  from numpy import long
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%RFL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: RFL , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 16, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
LeNet(
  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (batch_norm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
Round 0 global test acc  17.1400
Round 1 global test acc  20.6000
Round 2 global test acc  22.6900
Round 3 global test acc  26.7900
Round 4 global test acc  20.2400
Round 5 global test acc  15.8900
Round 6 global test acc  20.5100
Round 7 global test acc  23.5500
Round 8 global test acc  17.6500
Round 9 global test acc  19.6000
Round 10 global test acc  22.3000
Round 11 global test acc  29.1300
Round 12 global test acc  17.2900
Round 13 global test acc  26.9100
Round 14 global test acc  27.8100
Round 15 global test acc  19.8100
Round 16 global test acc  24.4900
Round 17 global test acc  26.6400
Round 18 global test acc  24.6500
Round 19 global test acc  27.2600
Round 20 global test acc  25.4800
Round 21 global test acc  27.8200
Round 22 global test acc  21.9100
Round 23 global test acc  24.3200
Round 24 global test acc  24.4000
Round 25 global test acc  26.6700
Round 26 global test acc  26.2900
Round 27 global test acc  32.3200
Round 28 global test acc  27.7500
Round 29 global test acc  22.4200
Round 30 global test acc  22.0400
Round 31 global test acc  34.0400
Round 32 global test acc  35.4600
Round 33 global test acc  23.0500
Round 34 global test acc  26.6300
Round 35 global test acc  25.5600
Round 36 global test acc  31.7500
Round 37 global test acc  36.9600
Round 38 global test acc  24.5200
Round 39 global test acc  29.7700
Round 40 global test acc  31.8800
Round 41 global test acc  30.2800
Round 42 global test acc  31.1300
Round 43 global test acc  30.2000
Round 44 global test acc  37.2800
Round 45 global test acc  24.5500
Round 46 global test acc  32.0000
Round 47 global test acc  26.7800
Round 48 global test acc  30.4000
Round 49 global test acc  30.3600
Round 50 global test acc  30.4400
Round 51 global test acc  29.2000
Round 52 global test acc  31.9200
Round 53 global test acc  27.4300
Round 54 global test acc  37.0200
Round 55 global test acc  29.3900
Round 56 global test acc  29.2400
Round 57 global test acc  35.9800
Round 58 global test acc  25.1600
Round 59 global test acc  31.2400
Round 60 global test acc  25.1100
Round 61 global test acc  25.4100
Round 62 global test acc  38.3300
Round 63 global test acc  31.7000
Round 64 global test acc  30.1400
Round 65 global test acc  29.6400
Round 66 global test acc  32.8600
Round 67 global test acc  34.3900
Round 68 global test acc  36.1600
Round 69 global test acc  35.9900
Round 70 global test acc  29.8900
Round 71 global test acc  31.5300
Round 72 global test acc  42.7200
Round 73 global test acc  22.2300
Round 74 global test acc  37.3200
Round 75 global test acc  33.7900
Round 76 global test acc  31.9800
Round 77 global test acc  25.5000
Round 78 global test acc  19.6400
Round 79 global test acc  26.7700
Round 80 global test acc  24.3000
Round 81 global test acc  24.1100
Round 82 global test acc  22.9900
Round 83 global test acc  20.5200
Round 84 global test acc  18.1700
Round 85 global test acc  17.9400
Round 86 global test acc  16.8300
Round 87 global test acc  15.1800
Round 88 global test acc  14.3400
Round 89 global test acc  13.9600
Round 90 global test acc  14.0200
Round 91 global test acc  13.9000
Round 92 global test acc  13.7800
Round 93 global test acc  14.0400
Round 94 global test acc  13.3800
Round 95 global test acc  12.1600
Round 96 global test acc  11.8600
Round 97 global test acc  11.5800
Round 98 global test acc  11.4800
Round 99 global test acc  11.6800
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
   Client 12, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 19, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 8, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 3, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 4, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 5, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.580, Test loss: 2.147, Test accuracy: 21.07
Round   1, Train loss: 1.297, Test loss: 1.829, Test accuracy: 29.12
Round   2, Train loss: 1.041, Test loss: 1.509, Test accuracy: 46.65
Round   3, Train loss: 0.944, Test loss: 1.363, Test accuracy: 51.73
Round   4, Train loss: 1.097, Test loss: 1.122, Test accuracy: 55.79
Round   5, Train loss: 1.038, Test loss: 1.068, Test accuracy: 56.68
Round   6, Train loss: 0.950, Test loss: 1.022, Test accuracy: 60.24
Round   7, Train loss: 0.942, Test loss: 0.992, Test accuracy: 62.31
Round   8, Train loss: 0.886, Test loss: 0.855, Test accuracy: 64.88
Round   9, Train loss: 0.979, Test loss: 0.864, Test accuracy: 65.46
Round  10, Train loss: 0.751, Test loss: 0.852, Test accuracy: 66.70
Round  11, Train loss: 0.872, Test loss: 0.827, Test accuracy: 67.62
Round  12, Train loss: 0.794, Test loss: 0.769, Test accuracy: 69.29
Round  13, Train loss: 0.820, Test loss: 0.744, Test accuracy: 70.87
Round  14, Train loss: 0.697, Test loss: 0.741, Test accuracy: 70.99
Round  15, Train loss: 0.773, Test loss: 0.742, Test accuracy: 71.11
Round  16, Train loss: 0.856, Test loss: 0.739, Test accuracy: 72.05
Round  17, Train loss: 0.879, Test loss: 0.719, Test accuracy: 72.73
Round  18, Train loss: 0.690, Test loss: 0.718, Test accuracy: 72.37
Round  19, Train loss: 0.807, Test loss: 0.718, Test accuracy: 72.73
Round  20, Train loss: 0.909, Test loss: 0.694, Test accuracy: 73.82
Round  21, Train loss: 0.755, Test loss: 0.702, Test accuracy: 73.68
Round  22, Train loss: 0.837, Test loss: 0.688, Test accuracy: 74.19
Round  23, Train loss: 0.721, Test loss: 0.681, Test accuracy: 74.74
Round  24, Train loss: 0.867, Test loss: 0.656, Test accuracy: 75.75
Round  25, Train loss: 0.753, Test loss: 0.661, Test accuracy: 76.05
Round  26, Train loss: 0.883, Test loss: 0.661, Test accuracy: 75.33
Round  27, Train loss: 0.843, Test loss: 0.655, Test accuracy: 75.77
Round  28, Train loss: 0.776, Test loss: 0.650, Test accuracy: 76.02
Round  29, Train loss: 0.612, Test loss: 0.630, Test accuracy: 76.27
Round  30, Train loss: 0.670, Test loss: 0.621, Test accuracy: 77.14
Round  31, Train loss: 0.622, Test loss: 0.610, Test accuracy: 77.28
Round  32, Train loss: 0.712, Test loss: 0.620, Test accuracy: 77.24
Round  33, Train loss: 0.662, Test loss: 0.634, Test accuracy: 76.58
Round  34, Train loss: 0.786, Test loss: 0.609, Test accuracy: 77.74
Round  35, Train loss: 0.649, Test loss: 0.590, Test accuracy: 78.26
Round  36, Train loss: 0.747, Test loss: 0.597, Test accuracy: 77.96
Round  37, Train loss: 0.710, Test loss: 0.589, Test accuracy: 78.60
Round  38, Train loss: 0.723, Test loss: 0.595, Test accuracy: 78.55
Round  39, Train loss: 0.580, Test loss: 0.606, Test accuracy: 77.95
Round  40, Train loss: 0.651, Test loss: 0.594, Test accuracy: 78.43
Round  41, Train loss: 0.579, Test loss: 0.594, Test accuracy: 78.94
Round  42, Train loss: 0.680, Test loss: 0.585, Test accuracy: 79.13
Round  43, Train loss: 0.628, Test loss: 0.593, Test accuracy: 79.00
Round  44, Train loss: 0.502, Test loss: 0.581, Test accuracy: 79.09
Round  45, Train loss: 0.597, Test loss: 0.582, Test accuracy: 78.68
Round  46, Train loss: 0.803, Test loss: 0.572, Test accuracy: 79.65
Round  47, Train loss: 0.743, Test loss: 0.574, Test accuracy: 79.40
Round  48, Train loss: 0.650, Test loss: 0.562, Test accuracy: 80.03
Round  49, Train loss: 0.526, Test loss: 0.574, Test accuracy: 79.62
Round  50, Train loss: 0.602, Test loss: 0.564, Test accuracy: 79.53
Round  51, Train loss: 0.612, Test loss: 0.579, Test accuracy: 79.42
Round  52, Train loss: 0.658, Test loss: 0.578, Test accuracy: 79.21
Round  53, Train loss: 0.536, Test loss: 0.573, Test accuracy: 78.78
Round  54, Train loss: 0.570, Test loss: 0.570, Test accuracy: 79.14
Round  55, Train loss: 0.528, Test loss: 0.552, Test accuracy: 79.73
Round  56, Train loss: 0.500, Test loss: 0.544, Test accuracy: 79.97
Round  57, Train loss: 0.528, Test loss: 0.547, Test accuracy: 80.31
Round  58, Train loss: 0.550, Test loss: 0.537, Test accuracy: 80.88
Round  59, Train loss: 0.584, Test loss: 0.551, Test accuracy: 80.38
Round  60, Train loss: 0.743, Test loss: 0.554, Test accuracy: 80.22
Round  61, Train loss: 0.583, Test loss: 0.551, Test accuracy: 80.08
Round  62, Train loss: 0.555, Test loss: 0.547, Test accuracy: 80.33
Round  63, Train loss: 0.539, Test loss: 0.541, Test accuracy: 80.26
Round  64, Train loss: 0.607, Test loss: 0.551, Test accuracy: 80.03
Round  65, Train loss: 0.526, Test loss: 0.539, Test accuracy: 80.62
Round  66, Train loss: 0.593, Test loss: 0.532, Test accuracy: 80.78
Round  67, Train loss: 0.548, Test loss: 0.525, Test accuracy: 80.74
Round  68, Train loss: 0.485, Test loss: 0.534, Test accuracy: 80.85
Round  69, Train loss: 0.416, Test loss: 0.518, Test accuracy: 81.42
Round  70, Train loss: 0.586, Test loss: 0.527, Test accuracy: 80.89
Round  71, Train loss: 0.513, Test loss: 0.534, Test accuracy: 80.83
Round  72, Train loss: 0.462, Test loss: 0.537, Test accuracy: 80.70
Round  73, Train loss: 0.460, Test loss: 0.537, Test accuracy: 80.88
Round  74, Train loss: 0.566, Test loss: 0.530, Test accuracy: 80.97
Round  75, Train loss: 0.616, Test loss: 0.543, Test accuracy: 80.84
Round  76, Train loss: 0.366, Test loss: 0.525, Test accuracy: 81.21
Round  77, Train loss: 0.526, Test loss: 0.531, Test accuracy: 80.72
Round  78, Train loss: 0.428, Test loss: 0.537, Test accuracy: 80.83
Round  79, Train loss: 0.477, Test loss: 0.531, Test accuracy: 80.97
Round  80, Train loss: 0.452, Test loss: 0.526, Test accuracy: 81.13
Round  81, Train loss: 0.553, Test loss: 0.534, Test accuracy: 80.61
Round  82, Train loss: 0.505, Test loss: 0.542, Test accuracy: 80.35
Round  83, Train loss: 0.496, Test loss: 0.540, Test accuracy: 80.94
Round  84, Train loss: 0.466, Test loss: 0.533, Test accuracy: 81.16
Round  85, Train loss: 0.424, Test loss: 0.528, Test accuracy: 81.18
Round  86, Train loss: 0.508, Test loss: 0.533, Test accuracy: 80.80
Round  87, Train loss: 0.476, Test loss: 0.541, Test accuracy: 80.12
Round  88, Train loss: 0.344, Test loss: 0.527, Test accuracy: 81.09
Round  89, Train loss: 0.376, Test loss: 0.538, Test accuracy: 80.75
Round  90, Train loss: 0.470, Test loss: 0.524, Test accuracy: 81.34
Round  91, Train loss: 0.483, Test loss: 0.529, Test accuracy: 81.09
Round  92, Train loss: 0.523, Test loss: 0.525, Test accuracy: 80.82
Round  93, Train loss: 0.493, Test loss: 0.528, Test accuracy: 80.67
Round  94, Train loss: 0.444, Test loss: 0.534, Test accuracy: 81.03
Round  95, Train loss: 0.437, Test loss: 0.537, Test accuracy: 80.81
Round  96, Train loss: 0.477, Test loss: 0.533, Test accuracy: 81.22
Round  97, Train loss: 0.463, Test loss: 0.532, Test accuracy: 81.02
Round  98, Train loss: 0.342, Test loss: 0.524, Test accuracy: 81.47
Round  99, Train loss: 0.424, Test loss: 0.535, Test accuracy: 81.03
Final Round, Train loss: 0.387, Test loss: 0.540, Test accuracy: 80.75
Average accuracy final 10 rounds: 81.04916666666668
1353.205287694931
[2.1492040157318115, 3.838290214538574, 5.542135000228882, 7.249282360076904, 8.921624898910522, 10.618484497070312, 12.300724506378174, 13.987399578094482, 15.681084632873535, 17.409460067749023, 19.141709566116333, 20.903847217559814, 22.63180446624756, 24.343690156936646, 26.04113531112671, 27.726035118103027, 29.42576575279236, 31.16300630569458, 32.92321014404297, 34.6540801525116, 36.34562635421753, 38.00443696975708, 39.658833503723145, 41.34340238571167, 43.04210352897644, 44.75378394126892, 46.421021461486816, 48.124765396118164, 49.791972637176514, 51.50435447692871, 53.190598011016846, 54.8836305141449, 56.55872869491577, 58.25198936462402, 59.93661141395569, 61.604100942611694, 63.27004909515381, 64.9293532371521, 66.59545516967773, 68.29284238815308, 70.07994937896729, 71.83927178382874, 73.58780431747437, 75.27057027816772, 77.05442523956299, 78.72104001045227, 80.41405081748962, 82.15688037872314, 83.91475367546082, 85.63733458518982, 87.3159749507904, 88.9686930179596, 90.63434338569641, 92.34384632110596, 94.01449465751648, 95.69080352783203, 97.36178469657898, 99.01459121704102, 100.65072679519653, 102.32184958457947, 103.98885989189148, 105.75169372558594, 107.45920610427856, 109.17363238334656, 110.91315698623657, 112.64880871772766, 114.32260036468506, 115.99932312965393, 117.66862845420837, 119.32632327079773, 121.03155255317688, 122.70709443092346, 124.39342927932739, 126.09000658988953, 127.77347540855408, 129.4764370918274, 131.19835209846497, 132.89020800590515, 134.5560541152954, 136.22117805480957, 137.87925028800964, 139.5415759086609, 141.24261164665222, 142.9896092414856, 144.7610855102539, 146.49621200561523, 148.19672870635986, 149.86878871917725, 151.5238037109375, 153.18263006210327, 154.82710719108582, 156.49266242980957, 158.13155794143677, 159.77673435211182, 161.4326057434082, 163.10657811164856, 164.76223945617676, 166.46096229553223, 168.1470501422882, 169.86599898338318, 172.18436121940613]
[21.075, 29.125, 46.65, 51.725, 55.791666666666664, 56.68333333333333, 60.24166666666667, 62.30833333333333, 64.875, 65.45833333333333, 66.7, 67.625, 69.29166666666667, 70.86666666666666, 70.99166666666666, 71.10833333333333, 72.05, 72.73333333333333, 72.36666666666666, 72.73333333333333, 73.81666666666666, 73.68333333333334, 74.19166666666666, 74.74166666666666, 75.75, 76.05, 75.325, 75.76666666666667, 76.01666666666667, 76.26666666666667, 77.14166666666667, 77.28333333333333, 77.24166666666666, 76.575, 77.74166666666666, 78.25833333333334, 77.95833333333333, 78.6, 78.55, 77.95, 78.43333333333334, 78.94166666666666, 79.13333333333334, 79.0, 79.09166666666667, 78.68333333333334, 79.65, 79.4, 80.03333333333333, 79.61666666666666, 79.525, 79.41666666666667, 79.20833333333333, 78.78333333333333, 79.14166666666667, 79.73333333333333, 79.96666666666667, 80.30833333333334, 80.88333333333334, 80.375, 80.21666666666667, 80.08333333333333, 80.325, 80.25833333333334, 80.03333333333333, 80.61666666666666, 80.78333333333333, 80.74166666666666, 80.85, 81.425, 80.89166666666667, 80.83333333333333, 80.7, 80.875, 80.975, 80.84166666666667, 81.20833333333333, 80.725, 80.83333333333333, 80.975, 81.13333333333334, 80.60833333333333, 80.35, 80.94166666666666, 81.15833333333333, 81.18333333333334, 80.8, 80.125, 81.09166666666667, 80.75, 81.34166666666667, 81.09166666666667, 80.81666666666666, 80.66666666666667, 81.025, 80.80833333333334, 81.21666666666667, 81.01666666666667, 81.475, 81.03333333333333, 80.75]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC_PSL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10,  filter_alg: loss_psl, level_n_system: 0.8 , level_n_lowerb:0.5  

Files already downloaded and verified
Files already downloaded and verified
   Client 3, noise    level: 0.5000 
   Client 11, noise    level: 0.5000 
   Client 9, noise    level: 0.5000 
   Client 0, noise    level: 0.5000 
   Client 18, noise    level: 0.5000 
   Client 17, noise    level: 0.5000 
   Client 10, noise    level: 0.5000 
   Client 12, noise    level: 0.5000 
   Client 14, noise    level: 0.5000 
   Client 7, noise    level: 0.5000 
   Client 15, noise    level: 0.5000 
   Client 2, noise    level: 0.5000 
   Client 6, noise    level: 0.5000 
   Client 1, noise    level: 0.5000 
   Client 16, noise    level: 0.5000 
   Client 13, noise    level: 0.5000 
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.673, Test loss: 2.050, Test accuracy: 27.33
Round   1, Train loss: 1.348, Test loss: 1.744, Test accuracy: 34.16
Round   2, Train loss: 1.115, Test loss: 1.579, Test accuracy: 44.47
Round   3, Train loss: 0.894, Test loss: 1.359, Test accuracy: 51.12
Round   4, Train loss: 1.108, Test loss: 1.096, Test accuracy: 55.58
Round   5, Train loss: 0.925, Test loss: 1.022, Test accuracy: 58.70
Round   6, Train loss: 0.909, Test loss: 0.998, Test accuracy: 60.41
Round   7, Train loss: 0.884, Test loss: 0.956, Test accuracy: 63.15
Round   8, Train loss: 0.881, Test loss: 0.849, Test accuracy: 65.53
Round   9, Train loss: 0.985, Test loss: 0.834, Test accuracy: 67.86
Round  10, Train loss: 0.635, Test loss: 0.829, Test accuracy: 68.15
Round  11, Train loss: 0.815, Test loss: 0.809, Test accuracy: 68.77
Round  12, Train loss: 0.845, Test loss: 0.764, Test accuracy: 70.44
Round  13, Train loss: 0.994, Test loss: 0.755, Test accuracy: 71.75
Round  14, Train loss: 0.639, Test loss: 0.758, Test accuracy: 71.28
Round  15, Train loss: 0.801, Test loss: 0.754, Test accuracy: 71.99
Round  16, Train loss: 0.654, Test loss: 0.759, Test accuracy: 71.99
Round  17, Train loss: 1.006, Test loss: 0.722, Test accuracy: 73.21
Round  18, Train loss: 0.823, Test loss: 0.731, Test accuracy: 73.16
Round  19, Train loss: 0.813, Test loss: 0.714, Test accuracy: 73.72
Round  20, Train loss: 0.865, Test loss: 0.723, Test accuracy: 73.14
Round  21, Train loss: 0.987, Test loss: 0.693, Test accuracy: 74.41
Round  22, Train loss: 0.814, Test loss: 0.698, Test accuracy: 74.72
Round  23, Train loss: 0.917, Test loss: 0.692, Test accuracy: 74.60
Round  24, Train loss: 0.679, Test loss: 0.668, Test accuracy: 76.69
Round  25, Train loss: 0.743, Test loss: 0.648, Test accuracy: 77.39
Round  26, Train loss: 0.872, Test loss: 0.659, Test accuracy: 77.06
Round  27, Train loss: 0.789, Test loss: 0.657, Test accuracy: 77.26
Round  28, Train loss: 0.951, Test loss: 0.633, Test accuracy: 77.47
Round  29, Train loss: 0.602, Test loss: 0.633, Test accuracy: 77.62
Round  30, Train loss: 0.654, Test loss: 0.644, Test accuracy: 77.83
Round  31, Train loss: 0.604, Test loss: 0.628, Test accuracy: 78.12
Round  32, Train loss: 0.676, Test loss: 0.619, Test accuracy: 78.28
Round  33, Train loss: 0.569, Test loss: 0.626, Test accuracy: 78.38
Round  34, Train loss: 0.706, Test loss: 0.621, Test accuracy: 78.59
Round  35, Train loss: 0.749, Test loss: 0.595, Test accuracy: 79.04
Round  36, Train loss: 0.770, Test loss: 0.598, Test accuracy: 79.39
Round  37, Train loss: 0.752, Test loss: 0.593, Test accuracy: 79.40
Round  38, Train loss: 0.650, Test loss: 0.586, Test accuracy: 79.69
Round  39, Train loss: 0.505, Test loss: 0.583, Test accuracy: 79.76
Round  40, Train loss: 0.731, Test loss: 0.583, Test accuracy: 79.50
Round  41, Train loss: 0.685, Test loss: 0.573, Test accuracy: 80.03
Round  42, Train loss: 0.723, Test loss: 0.568, Test accuracy: 80.33
Round  43, Train loss: 0.608, Test loss: 0.568, Test accuracy: 80.37
Round  44, Train loss: 0.590, Test loss: 0.572, Test accuracy: 80.27
Round  45, Train loss: 0.509, Test loss: 0.571, Test accuracy: 80.07
Round  46, Train loss: 0.730, Test loss: 0.564, Test accuracy: 80.51
Round  47, Train loss: 0.724, Test loss: 0.583, Test accuracy: 80.20
Round  48, Train loss: 0.662, Test loss: 0.556, Test accuracy: 80.72
Round  49, Train loss: 0.577, Test loss: 0.552, Test accuracy: 81.28
Round  50, Train loss: 0.582, Test loss: 0.546, Test accuracy: 81.39
Round  51, Train loss: 0.679, Test loss: 0.548, Test accuracy: 81.55
Round  52, Train loss: 0.655, Test loss: 0.541, Test accuracy: 81.64
Round  53, Train loss: 0.529, Test loss: 0.544, Test accuracy: 81.52
Round  54, Train loss: 0.480, Test loss: 0.554, Test accuracy: 80.85
Round  55, Train loss: 0.504, Test loss: 0.538, Test accuracy: 81.77
Round  56, Train loss: 0.665, Test loss: 0.533, Test accuracy: 81.97
Round  57, Train loss: 0.594, Test loss: 0.530, Test accuracy: 81.86
Round  58, Train loss: 0.521, Test loss: 0.529, Test accuracy: 81.78
Round  59, Train loss: 0.584, Test loss: 0.528, Test accuracy: 81.80
Round  60, Train loss: 0.754, Test loss: 0.527, Test accuracy: 81.84
Round  61, Train loss: 0.477, Test loss: 0.538, Test accuracy: 81.83
Round  62, Train loss: 0.495, Test loss: 0.524, Test accuracy: 82.86
Round  63, Train loss: 0.597, Test loss: 0.527, Test accuracy: 81.93
Round  64, Train loss: 0.696, Test loss: 0.515, Test accuracy: 82.70
Round  65, Train loss: 0.491, Test loss: 0.515, Test accuracy: 82.91
Round  66, Train loss: 0.436, Test loss: 0.515, Test accuracy: 82.58
Round  67, Train loss: 0.397, Test loss: 0.504, Test accuracy: 82.69
Round  68, Train loss: 0.543, Test loss: 0.503, Test accuracy: 83.08
Round  69, Train loss: 0.420, Test loss: 0.507, Test accuracy: 83.12
Round  70, Train loss: 0.547, Test loss: 0.512, Test accuracy: 82.86
Round  71, Train loss: 0.554, Test loss: 0.509, Test accuracy: 82.78
Round  72, Train loss: 0.347, Test loss: 0.520, Test accuracy: 82.69
Round  73, Train loss: 0.445, Test loss: 0.511, Test accuracy: 82.76
Round  74, Train loss: 0.615, Test loss: 0.500, Test accuracy: 83.29
Round  75, Train loss: 0.588, Test loss: 0.510, Test accuracy: 82.85
Round  76, Train loss: 0.492, Test loss: 0.506, Test accuracy: 83.00
Round  77, Train loss: 0.622, Test loss: 0.515, Test accuracy: 82.57
Round  78, Train loss: 0.380, Test loss: 0.506, Test accuracy: 83.07
Round  79, Train loss: 0.434, Test loss: 0.509, Test accuracy: 82.93
Round  80, Train loss: 0.437, Test loss: 0.511, Test accuracy: 82.77
Round  81, Train loss: 0.528, Test loss: 0.517, Test accuracy: 82.32
Round  82, Train loss: 0.463, Test loss: 0.513, Test accuracy: 82.48
Round  83, Train loss: 0.452, Test loss: 0.513, Test accuracy: 82.72
Round  84, Train loss: 0.420, Test loss: 0.508, Test accuracy: 82.65
Round  85, Train loss: 0.445, Test loss: 0.515, Test accuracy: 82.53
Round  86, Train loss: 0.537, Test loss: 0.507, Test accuracy: 83.03
Round  87, Train loss: 0.551, Test loss: 0.504, Test accuracy: 83.09
Round  88, Train loss: 0.408, Test loss: 0.503, Test accuracy: 83.33
Round  89, Train loss: 0.397, Test loss: 0.512, Test accuracy: 83.12
Round  90, Train loss: 0.506, Test loss: 0.504, Test accuracy: 82.88
Round  91, Train loss: 0.622, Test loss: 0.500, Test accuracy: 83.20
Round  92, Train loss: 0.459, Test loss: 0.508, Test accuracy: 83.24
Round  93, Train loss: 0.502, Test loss: 0.510, Test accuracy: 82.97
Round  94, Train loss: 0.527, Test loss: 0.511, Test accuracy: 82.85
Round  95, Train loss: 0.431, Test loss: 0.520, Test accuracy: 82.41
Round  96, Train loss: 0.497, Test loss: 0.520, Test accuracy: 82.65
Round  97, Train loss: 0.448, Test loss: 0.514, Test accuracy: 82.72
Round  98, Train loss: 0.448, Test loss: 0.503, Test accuracy: 83.42
Round  99, Train loss: 0.400, Test loss: 0.504, Test accuracy: 82.91
Final Round, Train loss: 0.399, Test loss: 0.504, Test accuracy: 83.13
Average accuracy final 10 rounds: 82.92500000000001
1903.4075050354004
[2.280196189880371, 4.021147012710571, 5.8127968311309814, 7.607708692550659, 9.358102083206177, 11.021032333374023, 12.706364870071411, 14.377441644668579, 16.062605619430542, 17.749229192733765, 19.418724536895752, 21.108980417251587, 22.782135486602783, 24.455039501190186, 26.160448789596558, 27.832068920135498, 29.498739004135132, 31.158016204833984, 32.868335485458374, 34.595746755599976, 36.34052491188049, 39.26602363586426, 42.099200963974, 45.016539573669434, 47.897884130477905, 50.902384519577026, 53.82162952423096, 56.755045652389526, 59.720203161239624, 62.76720833778381, 65.62612009048462, 68.57091307640076, 71.46622967720032, 74.35395526885986, 77.25780487060547, 80.18921232223511, 83.08706521987915, 85.95176219940186, 88.86642575263977, 91.71951460838318, 94.56959700584412, 97.50501108169556, 100.4470624923706, 103.38403677940369, 106.24351358413696, 109.12433195114136, 112.03374457359314, 114.94968485832214, 117.817063331604, 120.91970705986023, 123.85856199264526, 126.70041346549988, 129.61644577980042, 132.52408003807068, 135.48005747795105, 138.40800428390503, 141.24752473831177, 144.15884375572205, 147.1304178237915, 150.0396285057068, 152.93201684951782, 155.82020902633667, 158.71732449531555, 161.54095768928528, 164.4242434501648, 167.30231547355652, 170.19261002540588, 173.1533477306366, 176.16630673408508, 179.09343457221985, 181.98241329193115, 184.8127624988556, 187.81061959266663, 190.7740888595581, 193.65759372711182, 196.5037181377411, 199.4554307460785, 202.53644442558289, 205.4585120677948, 208.32867002487183, 211.2126886844635, 214.1202700138092, 216.99424147605896, 219.8500792980194, 222.77187180519104, 225.67106223106384, 228.56166172027588, 231.48981738090515, 234.45134902000427, 237.45708417892456, 240.41210198402405, 243.25381016731262, 246.10164666175842, 249.02433800697327, 251.92928886413574, 254.78196501731873, 257.6836245059967, 260.47559666633606, 263.2892162799835, 266.14187359809875, 268.3736765384674]
[27.333333333333332, 34.15833333333333, 44.46666666666667, 51.125, 55.583333333333336, 58.7, 60.40833333333333, 63.15, 65.53333333333333, 67.85833333333333, 68.15, 68.76666666666667, 70.44166666666666, 71.75, 71.275, 71.99166666666666, 71.99166666666666, 73.20833333333333, 73.15833333333333, 73.725, 73.14166666666667, 74.40833333333333, 74.71666666666667, 74.6, 76.69166666666666, 77.39166666666667, 77.05833333333334, 77.25833333333334, 77.46666666666667, 77.61666666666666, 77.83333333333333, 78.125, 78.275, 78.38333333333334, 78.59166666666667, 79.04166666666667, 79.39166666666667, 79.4, 79.69166666666666, 79.75833333333334, 79.5, 80.025, 80.325, 80.36666666666666, 80.26666666666667, 80.06666666666666, 80.50833333333334, 80.2, 80.725, 81.275, 81.39166666666667, 81.55, 81.64166666666667, 81.51666666666667, 80.85, 81.76666666666667, 81.975, 81.85833333333333, 81.78333333333333, 81.8, 81.84166666666667, 81.825, 82.85833333333333, 81.93333333333334, 82.7, 82.90833333333333, 82.58333333333333, 82.69166666666666, 83.08333333333333, 83.11666666666666, 82.85833333333333, 82.78333333333333, 82.69166666666666, 82.75833333333334, 83.29166666666667, 82.85, 83.0, 82.56666666666666, 83.06666666666666, 82.93333333333334, 82.76666666666667, 82.31666666666666, 82.48333333333333, 82.71666666666667, 82.65, 82.525, 83.025, 83.09166666666667, 83.33333333333333, 83.11666666666666, 82.88333333333334, 83.2, 83.24166666666666, 82.975, 82.85, 82.40833333333333, 82.65, 82.71666666666667, 83.41666666666667, 82.90833333333333, 83.13333333333334]
