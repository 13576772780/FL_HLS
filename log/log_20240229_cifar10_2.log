nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.996, Test loss: 1.689, Test accuracy: 38.90
Round   0, Global train loss: 1.996, Global test loss: 1.676, Global test accuracy: 40.06
Round   1, Train loss: 1.670, Test loss: 1.543, Test accuracy: 44.23
Round   1, Global train loss: 1.670, Global test loss: 1.476, Global test accuracy: 47.59
Round   2, Train loss: 1.523, Test loss: 1.492, Test accuracy: 46.13
Round   2, Global train loss: 1.523, Global test loss: 1.352, Global test accuracy: 51.88
Round   3, Train loss: 1.485, Test loss: 1.472, Test accuracy: 46.87
Round   3, Global train loss: 1.485, Global test loss: 1.324, Global test accuracy: 54.08
Round   4, Train loss: 1.340, Test loss: 1.448, Test accuracy: 47.67
Round   4, Global train loss: 1.340, Global test loss: 1.231, Global test accuracy: 56.96
Round   5, Train loss: 1.461, Test loss: 1.447, Test accuracy: 48.20
Round   5, Global train loss: 1.461, Global test loss: 1.367, Global test accuracy: 52.86
Round   6, Train loss: 1.296, Test loss: 1.426, Test accuracy: 49.09
Round   6, Global train loss: 1.296, Global test loss: 1.244, Global test accuracy: 56.38
Round   7, Train loss: 1.267, Test loss: 1.416, Test accuracy: 49.55
Round   7, Global train loss: 1.267, Global test loss: 1.189, Global test accuracy: 58.87
Round   8, Train loss: 1.321, Test loss: 1.393, Test accuracy: 50.63
Round   8, Global train loss: 1.321, Global test loss: 1.229, Global test accuracy: 57.76
Round   9, Train loss: 1.345, Test loss: 1.388, Test accuracy: 50.81
Round   9, Global train loss: 1.345, Global test loss: 1.299, Global test accuracy: 56.10
Round  10, Train loss: 1.137, Test loss: 1.384, Test accuracy: 51.45
Round  10, Global train loss: 1.137, Global test loss: 1.181, Global test accuracy: 59.62
Round  11, Train loss: 1.138, Test loss: 1.379, Test accuracy: 51.99
Round  11, Global train loss: 1.138, Global test loss: 1.183, Global test accuracy: 58.48
Round  12, Train loss: 1.101, Test loss: 1.376, Test accuracy: 52.42
Round  12, Global train loss: 1.101, Global test loss: 1.193, Global test accuracy: 59.21
Round  13, Train loss: 1.059, Test loss: 1.380, Test accuracy: 52.84
Round  13, Global train loss: 1.059, Global test loss: 1.216, Global test accuracy: 58.11
Round  14, Train loss: 0.977, Test loss: 1.382, Test accuracy: 53.24
Round  14, Global train loss: 0.977, Global test loss: 1.148, Global test accuracy: 60.24
Round  15, Train loss: 0.967, Test loss: 1.396, Test accuracy: 53.42
Round  15, Global train loss: 0.967, Global test loss: 1.154, Global test accuracy: 59.63
Round  16, Train loss: 1.096, Test loss: 1.390, Test accuracy: 53.81
Round  16, Global train loss: 1.096, Global test loss: 1.224, Global test accuracy: 58.02
Round  17, Train loss: 0.986, Test loss: 1.410, Test accuracy: 53.59
Round  17, Global train loss: 0.986, Global test loss: 1.164, Global test accuracy: 59.40
Round  18, Train loss: 0.992, Test loss: 1.418, Test accuracy: 53.81
Round  18, Global train loss: 0.992, Global test loss: 1.233, Global test accuracy: 56.99
Round  19, Train loss: 0.937, Test loss: 1.420, Test accuracy: 53.85
Round  19, Global train loss: 0.937, Global test loss: 1.133, Global test accuracy: 60.58
Round  20, Train loss: 0.909, Test loss: 1.435, Test accuracy: 53.85
Round  20, Global train loss: 0.909, Global test loss: 1.183, Global test accuracy: 58.80
Round  21, Train loss: 0.859, Test loss: 1.447, Test accuracy: 54.12
Round  21, Global train loss: 0.859, Global test loss: 1.135, Global test accuracy: 60.33
Round  22, Train loss: 0.931, Test loss: 1.454, Test accuracy: 54.34
Round  22, Global train loss: 0.931, Global test loss: 1.153, Global test accuracy: 59.94
Round  23, Train loss: 0.939, Test loss: 1.449, Test accuracy: 54.83
Round  23, Global train loss: 0.939, Global test loss: 1.134, Global test accuracy: 60.24
Round  24, Train loss: 0.800, Test loss: 1.460, Test accuracy: 55.11
Round  24, Global train loss: 0.800, Global test loss: 1.080, Global test accuracy: 62.26
Round  25, Train loss: 0.838, Test loss: 1.484, Test accuracy: 54.77
Round  25, Global train loss: 0.838, Global test loss: 1.147, Global test accuracy: 59.76
Round  26, Train loss: 0.824, Test loss: 1.492, Test accuracy: 54.91
Round  26, Global train loss: 0.824, Global test loss: 1.185, Global test accuracy: 59.64
Round  27, Train loss: 0.845, Test loss: 1.529, Test accuracy: 54.62
Round  27, Global train loss: 0.845, Global test loss: 1.113, Global test accuracy: 60.58
Round  28, Train loss: 0.696, Test loss: 1.519, Test accuracy: 55.28
Round  28, Global train loss: 0.696, Global test loss: 1.182, Global test accuracy: 58.70
Round  29, Train loss: 0.730, Test loss: 1.541, Test accuracy: 54.98
Round  29, Global train loss: 0.730, Global test loss: 1.159, Global test accuracy: 59.60
Round  30, Train loss: 0.684, Test loss: 1.562, Test accuracy: 55.00
Round  30, Global train loss: 0.684, Global test loss: 1.138, Global test accuracy: 59.69
Round  31, Train loss: 0.655, Test loss: 1.569, Test accuracy: 54.88
Round  31, Global train loss: 0.655, Global test loss: 1.122, Global test accuracy: 60.99
Round  32, Train loss: 0.671, Test loss: 1.584, Test accuracy: 54.88
Round  32, Global train loss: 0.671, Global test loss: 1.121, Global test accuracy: 61.06
Round  33, Train loss: 0.693, Test loss: 1.604, Test accuracy: 54.95
Round  33, Global train loss: 0.693, Global test loss: 1.203, Global test accuracy: 58.09
Round  34, Train loss: 0.616, Test loss: 1.632, Test accuracy: 55.02
Round  34, Global train loss: 0.616, Global test loss: 1.192, Global test accuracy: 58.15
Round  35, Train loss: 0.639, Test loss: 1.654, Test accuracy: 55.28
Round  35, Global train loss: 0.639, Global test loss: 1.188, Global test accuracy: 58.14
Round  36, Train loss: 0.606, Test loss: 1.675, Test accuracy: 55.16
Round  36, Global train loss: 0.606, Global test loss: 1.211, Global test accuracy: 57.65
Round  37, Train loss: 0.568, Test loss: 1.694, Test accuracy: 55.16
Round  37, Global train loss: 0.568, Global test loss: 1.148, Global test accuracy: 60.54
Round  38, Train loss: 0.566, Test loss: 1.699, Test accuracy: 55.51
Round  38, Global train loss: 0.566, Global test loss: 1.181, Global test accuracy: 59.33
Round  39, Train loss: 0.556, Test loss: 1.699, Test accuracy: 55.81
Round  39, Global train loss: 0.556, Global test loss: 1.127, Global test accuracy: 61.07
Round  40, Train loss: 0.523, Test loss: 1.731, Test accuracy: 55.55
Round  40, Global train loss: 0.523, Global test loss: 1.139, Global test accuracy: 60.94
Round  41, Train loss: 0.490, Test loss: 1.761, Test accuracy: 55.19
Round  41, Global train loss: 0.490, Global test loss: 1.124, Global test accuracy: 62.15
Round  42, Train loss: 0.493, Test loss: 1.776, Test accuracy: 55.30
Round  42, Global train loss: 0.493, Global test loss: 1.172, Global test accuracy: 59.83
Round  43, Train loss: 0.518, Test loss: 1.778, Test accuracy: 55.62
Round  43, Global train loss: 0.518, Global test loss: 1.085, Global test accuracy: 63.43
Round  44, Train loss: 0.467, Test loss: 1.803, Test accuracy: 55.52
Round  44, Global train loss: 0.467, Global test loss: 1.140, Global test accuracy: 61.61
Round  45, Train loss: 0.524, Test loss: 1.844, Test accuracy: 55.21
Round  45, Global train loss: 0.524, Global test loss: 1.252, Global test accuracy: 55.76
Round  46, Train loss: 0.472, Test loss: 1.870, Test accuracy: 55.28
Round  46, Global train loss: 0.472, Global test loss: 1.156, Global test accuracy: 60.33
Round  47, Train loss: 0.462, Test loss: 1.909, Test accuracy: 55.04
Round  47, Global train loss: 0.462, Global test loss: 1.176, Global test accuracy: 60.52
Round  48, Train loss: 0.448, Test loss: 1.966, Test accuracy: 54.52
Round  48, Global train loss: 0.448, Global test loss: 1.165, Global test accuracy: 60.67
Round  49, Train loss: 0.465, Test loss: 1.948, Test accuracy: 55.00
Round  49, Global train loss: 0.465, Global test loss: 1.129, Global test accuracy: 60.91
Round  50, Train loss: 0.460, Test loss: 1.931, Test accuracy: 55.34
Round  50, Global train loss: 0.460, Global test loss: 1.216, Global test accuracy: 57.69
Round  51, Train loss: 0.441, Test loss: 1.920, Test accuracy: 55.67
Round  51, Global train loss: 0.441, Global test loss: 1.197, Global test accuracy: 59.21
Round  52, Train loss: 0.389, Test loss: 1.957, Test accuracy: 55.90
Round  52, Global train loss: 0.389, Global test loss: 1.241, Global test accuracy: 57.57
Round  53, Train loss: 0.399, Test loss: 1.958, Test accuracy: 56.05
Round  53, Global train loss: 0.399, Global test loss: 1.131, Global test accuracy: 61.84
Round  54, Train loss: 0.375, Test loss: 2.013, Test accuracy: 55.64
Round  54, Global train loss: 0.375, Global test loss: 1.209, Global test accuracy: 59.34
Round  55, Train loss: 0.395, Test loss: 2.026, Test accuracy: 55.55
Round  55, Global train loss: 0.395, Global test loss: 1.272, Global test accuracy: 55.81
Round  56, Train loss: 0.378, Test loss: 2.036, Test accuracy: 55.30
Round  56, Global train loss: 0.378, Global test loss: 1.167, Global test accuracy: 61.04
Round  57, Train loss: 0.347, Test loss: 2.053, Test accuracy: 55.51
Round  57, Global train loss: 0.347, Global test loss: 1.227, Global test accuracy: 59.15
Round  58, Train loss: 0.387, Test loss: 2.071, Test accuracy: 55.52
Round  58, Global train loss: 0.387, Global test loss: 1.193, Global test accuracy: 59.98
Round  59, Train loss: 0.381, Test loss: 2.090, Test accuracy: 55.59
Round  59, Global train loss: 0.381, Global test loss: 1.249, Global test accuracy: 57.43
Round  60, Train loss: 0.342, Test loss: 2.091, Test accuracy: 55.82
Round  60, Global train loss: 0.342, Global test loss: 1.168, Global test accuracy: 61.52
Round  61, Train loss: 0.358, Test loss: 2.128, Test accuracy: 56.04
Round  61, Global train loss: 0.358, Global test loss: 1.159, Global test accuracy: 63.17
Round  62, Train loss: 0.341, Test loss: 2.131, Test accuracy: 55.94
Round  62, Global train loss: 0.341, Global test loss: 1.178, Global test accuracy: 62.12
Round  63, Train loss: 0.315, Test loss: 2.153, Test accuracy: 55.64
Round  63, Global train loss: 0.315, Global test loss: 1.181, Global test accuracy: 60.92
Round  64, Train loss: 0.358, Test loss: 2.195, Test accuracy: 55.44
Round  64, Global train loss: 0.358, Global test loss: 1.194, Global test accuracy: 61.12
Round  65, Train loss: 0.339, Test loss: 2.213, Test accuracy: 55.40
Round  65, Global train loss: 0.339, Global test loss: 1.236, Global test accuracy: 58.04
Round  66, Train loss: 0.323, Test loss: 2.240, Test accuracy: 55.23
Round  66, Global train loss: 0.323, Global test loss: 1.305, Global test accuracy: 56.84
Round  67, Train loss: 0.305, Test loss: 2.208, Test accuracy: 55.53
Round  67, Global train loss: 0.305, Global test loss: 1.211, Global test accuracy: 60.79
Round  68, Train loss: 0.303, Test loss: 2.215, Test accuracy: 55.58
Round  68, Global train loss: 0.303, Global test loss: 1.206, Global test accuracy: 60.48
Round  69, Train loss: 0.297, Test loss: 2.219, Test accuracy: 55.79
Round  69, Global train loss: 0.297, Global test loss: 1.195, Global test accuracy: 59.88
Round  70, Train loss: 0.354, Test loss: 2.246, Test accuracy: 55.69
Round  70, Global train loss: 0.354, Global test loss: 1.325, Global test accuracy: 53.75
Round  71, Train loss: 0.313, Test loss: 2.274, Test accuracy: 55.41
Round  71, Global train loss: 0.313, Global test loss: 1.284, Global test accuracy: 56.70
Round  72, Train loss: 0.329, Test loss: 2.309, Test accuracy: 55.02
Round  72, Global train loss: 0.329, Global test loss: 1.222, Global test accuracy: 58.12
Round  73, Train loss: 0.304, Test loss: 2.352, Test accuracy: 55.03
Round  73, Global train loss: 0.304, Global test loss: 1.191, Global test accuracy: 60.48
Round  74, Train loss: 0.278, Test loss: 2.360, Test accuracy: 55.09
Round  74, Global train loss: 0.278, Global test loss: 1.226, Global test accuracy: 60.49
Round  75, Train loss: 0.305, Test loss: 2.336, Test accuracy: 55.34
Round  75, Global train loss: 0.305, Global test loss: 1.226, Global test accuracy: 59.85
Round  76, Train loss: 0.280, Test loss: 2.342, Test accuracy: 55.56
Round  76, Global train loss: 0.280, Global test loss: 1.213, Global test accuracy: 59.02
Round  77, Train loss: 0.302, Test loss: 2.337, Test accuracy: 55.61
Round  77, Global train loss: 0.302, Global test loss: 1.251, Global test accuracy: 57.35
Round  78, Train loss: 0.284, Test loss: 2.330, Test accuracy: 55.72
Round  78, Global train loss: 0.284, Global test loss: 1.225, Global test accuracy: 58.20
Round  79, Train loss: 0.255, Test loss: 2.371, Test accuracy: 55.45
Round  79, Global train loss: 0.255, Global test loss: 1.244, Global test accuracy: 58.87
Round  80, Train loss: 0.289, Test loss: 2.360, Test accuracy: 55.48
Round  80, Global train loss: 0.289, Global test loss: 1.223, Global test accuracy: 60.55
Round  81, Train loss: 0.272, Test loss: 2.372, Test accuracy: 55.70
Round  81, Global train loss: 0.272, Global test loss: 1.209, Global test accuracy: 60.69
Round  82, Train loss: 0.254, Test loss: 2.391, Test accuracy: 55.81
Round  82, Global train loss: 0.254, Global test loss: 1.205, Global test accuracy: 60.87
Round  83, Train loss: 0.258, Test loss: 2.428, Test accuracy: 56.01
Round  83, Global train loss: 0.258, Global test loss: 1.231, Global test accuracy: 61.20
Round  84, Train loss: 0.262, Test loss: 2.432, Test accuracy: 56.05
Round  84, Global train loss: 0.262, Global test loss: 1.200, Global test accuracy: 60.22
Round  85, Train loss: 0.264, Test loss: 2.453, Test accuracy: 55.96
Round  85, Global train loss: 0.264, Global test loss: 1.280, Global test accuracy: 57.76
Round  86, Train loss: 0.257, Test loss: 2.441, Test accuracy: 56.22
Round  86, Global train loss: 0.257, Global test loss: 1.227, Global test accuracy: 61.07
Round  87, Train loss: 0.235, Test loss: 2.437, Test accuracy: 56.36
Round  87, Global train loss: 0.235, Global test loss: 1.222, Global test accuracy: 60.31
Round  88, Train loss: 0.256, Test loss: 2.460, Test accuracy: 56.34
Round  88, Global train loss: 0.256, Global test loss: 1.223, Global test accuracy: 59.52
Round  89, Train loss: 0.248, Test loss: 2.459, Test accuracy: 56.03
Round  89, Global train loss: 0.248, Global test loss: 1.261, Global test accuracy: 56.80
Round  90, Train loss: 0.268, Test loss: 2.464, Test accuracy: 56.30
Round  90, Global train loss: 0.268, Global test loss: 1.225, Global test accuracy: 58.55
Round  91, Train loss: 0.216, Test loss: 2.494, Test accuracy: 56.16
Round  91, Global train loss: 0.216, Global test loss: 1.196, Global test accuracy: 61.83
Round  92, Train loss: 0.229, Test loss: 2.471, Test accuracy: 56.09
Round  92, Global train loss: 0.229, Global test loss: 1.247, Global test accuracy: 58.27
Round  93, Train loss: 0.236, Test loss: 2.476, Test accuracy: 56.20
Round  93, Global train loss: 0.236, Global test loss: 1.214, Global test accuracy: 60.06
Round  94, Train loss: 0.225, Test loss: 2.493, Test accuracy: 56.22
Round  94, Global train loss: 0.225, Global test loss: 1.236, Global test accuracy: 60.84
Round  95, Train loss: 0.221, Test loss: 2.549, Test accuracy: 55.91
Round  95, Global train loss: 0.221, Global test loss: 1.226, Global test accuracy: 61.14
Round  96, Train loss: 0.215, Test loss: 2.587, Test accuracy: 56.06
Round  96, Global train loss: 0.215, Global test loss: 1.304, Global test accuracy: 56.27
Round  97, Train loss: 0.210, Test loss: 2.601, Test accuracy: 55.88
Round  97, Global train loss: 0.210, Global test loss: 1.273, Global test accuracy: 59.79
Round  98, Train loss: 0.231, Test loss: 2.611, Test accuracy: 55.89
Round  98, Global train loss: 0.231, Global test loss: 1.228, Global test accuracy: 61.68
Round  99, Train loss: 0.231, Test loss: 2.633, Test accuracy: 55.84
Round  99, Global train loss: 0.231, Global test loss: 1.214, Global test accuracy: 60.72
Final Round, Train loss: 0.156, Test loss: 2.821, Test accuracy: 56.49
Final Round, Global train loss: 0.156, Global test loss: 1.214, Global test accuracy: 60.72
Average accuracy final 10 rounds: 56.054750000000006 

Average global accuracy final 10 rounds: 59.915 

5973.737426042557
[4.155946493148804, 8.311892986297607, 12.133862018585205, 15.955831050872803, 19.853676319122314, 23.751521587371826, 27.688217639923096, 31.624913692474365, 35.505765438079834, 39.3866171836853, 43.278465032577515, 47.17031288146973, 51.04309296607971, 54.9158730506897, 58.88852334022522, 62.86117362976074, 67.38384532928467, 71.9065170288086, 76.05108952522278, 80.19566202163696, 84.16819190979004, 88.14072179794312, 92.34387111663818, 96.54702043533325, 100.79278898239136, 105.03855752944946, 109.2471170425415, 113.45567655563354, 117.55800223350525, 121.66032791137695, 125.75005984306335, 129.83979177474976, 134.24467492103577, 138.64955806732178, 142.89967370033264, 147.1497893333435, 151.41567087173462, 155.68155241012573, 160.45303058624268, 165.22450876235962, 169.85139775276184, 174.47828674316406, 179.4991364479065, 184.51998615264893, 189.39862513542175, 194.27726411819458, 199.22019290924072, 204.16312170028687, 209.16519284248352, 214.16726398468018, 219.11131310462952, 224.05536222457886, 228.54681825637817, 233.0382742881775, 237.6484055519104, 242.2585368156433, 246.76864767074585, 251.2787585258484, 255.8041000366211, 260.3294415473938, 264.8007810115814, 269.27212047576904, 273.83259177207947, 278.3930630683899, 282.90786576271057, 287.42266845703125, 291.8755350112915, 296.32840156555176, 301.2750885486603, 306.2217755317688, 310.78711795806885, 315.3524603843689, 319.9211745262146, 324.4898886680603, 329.4549026489258, 334.41991662979126, 339.1078085899353, 343.79570055007935, 348.32732129096985, 352.85894203186035, 357.75071477890015, 362.64248752593994, 367.5398414134979, 372.4371953010559, 377.27773094177246, 382.118266582489, 386.63575077056885, 391.1532349586487, 395.9215006828308, 400.68976640701294, 405.1301078796387, 409.5704493522644, 413.97251296043396, 418.3745765686035, 423.1714336872101, 427.96829080581665, 432.86533212661743, 437.7623734474182, 442.20540618896484, 446.6484389305115, 451.17657709121704, 455.7047152519226, 460.2841341495514, 464.8635530471802, 469.34708619117737, 473.83061933517456, 478.7552773952484, 483.67993545532227, 488.3942701816559, 493.1086049079895, 497.6189913749695, 502.12937784194946, 506.65883779525757, 511.1882977485657, 515.5864157676697, 519.9845337867737, 524.838330745697, 529.6921277046204, 534.0681850910187, 538.444242477417, 542.9207501411438, 547.3972578048706, 552.1794636249542, 556.9616694450378, 561.359352350235, 565.7570352554321, 570.2731010913849, 574.7891669273376, 579.2903804779053, 583.7915940284729, 588.2522201538086, 592.7128462791443, 597.1925082206726, 601.6721701622009, 606.0717887878418, 610.4714074134827, 615.3654274940491, 620.2594475746155, 625.1899185180664, 630.1203894615173, 634.8825302124023, 639.6446709632874, 644.1494646072388, 648.6542582511902, 652.9923377037048, 657.3304171562195, 661.8446650505066, 666.3589129447937, 670.6107468605042, 674.8625807762146, 679.4669141769409, 684.0712475776672, 689.014940738678, 693.9586338996887, 698.8473217487335, 703.7360095977783, 708.1332802772522, 712.5305509567261, 717.0557525157928, 721.5809540748596, 726.3171331882477, 731.0533123016357, 735.9976708889008, 740.9420294761658, 745.6345748901367, 750.3271203041077, 754.8813948631287, 759.4356694221497, 763.8953726291656, 768.3550758361816, 772.7241857051849, 777.0932955741882, 781.4297873973846, 785.766279220581, 790.1513693332672, 794.5364594459534, 798.9906685352325, 803.4448776245117, 807.8306963443756, 812.2165150642395, 816.5570538043976, 820.8975925445557, 825.2772390842438, 829.6568856239319, 833.9949643611908, 838.3330430984497, 842.6767816543579, 847.0205202102661, 851.5341997146606, 856.0478792190552, 860.5624582767487, 865.0770373344421, 869.5927858352661, 874.1085343360901, 878.803617477417, 883.4987006187439, 888.0447511672974, 892.5908017158508, 897.1138005256653, 901.6367993354797, 903.8949112892151, 906.1530232429504]
[38.9025, 38.9025, 44.23, 44.23, 46.135, 46.135, 46.865, 46.865, 47.6675, 47.6675, 48.195, 48.195, 49.095, 49.095, 49.5475, 49.5475, 50.6325, 50.6325, 50.815, 50.815, 51.4525, 51.4525, 51.9875, 51.9875, 52.4175, 52.4175, 52.84, 52.84, 53.24, 53.24, 53.42, 53.42, 53.8075, 53.8075, 53.59, 53.59, 53.81, 53.81, 53.855, 53.855, 53.8475, 53.8475, 54.12, 54.12, 54.3375, 54.3375, 54.8325, 54.8325, 55.1125, 55.1125, 54.775, 54.775, 54.9075, 54.9075, 54.615, 54.615, 55.2825, 55.2825, 54.985, 54.985, 55.0, 55.0, 54.8825, 54.8825, 54.8825, 54.8825, 54.9475, 54.9475, 55.0175, 55.0175, 55.28, 55.28, 55.1625, 55.1625, 55.1575, 55.1575, 55.5125, 55.5125, 55.8125, 55.8125, 55.55, 55.55, 55.1875, 55.1875, 55.3, 55.3, 55.615, 55.615, 55.5175, 55.5175, 55.2125, 55.2125, 55.285, 55.285, 55.0375, 55.0375, 54.525, 54.525, 55.0025, 55.0025, 55.335, 55.335, 55.6725, 55.6725, 55.9, 55.9, 56.0525, 56.0525, 55.64, 55.64, 55.545, 55.545, 55.3025, 55.3025, 55.5075, 55.5075, 55.5175, 55.5175, 55.59, 55.59, 55.8225, 55.8225, 56.0375, 56.0375, 55.94, 55.94, 55.64, 55.64, 55.4425, 55.4425, 55.395, 55.395, 55.2325, 55.2325, 55.5325, 55.5325, 55.575, 55.575, 55.7875, 55.7875, 55.69, 55.69, 55.405, 55.405, 55.025, 55.025, 55.03, 55.03, 55.0925, 55.0925, 55.345, 55.345, 55.5625, 55.5625, 55.6075, 55.6075, 55.715, 55.715, 55.45, 55.45, 55.48, 55.48, 55.7, 55.7, 55.815, 55.815, 56.0075, 56.0075, 56.055, 56.055, 55.96, 55.96, 56.2225, 56.2225, 56.3625, 56.3625, 56.3375, 56.3375, 56.03, 56.03, 56.3025, 56.3025, 56.1625, 56.1625, 56.085, 56.085, 56.2025, 56.2025, 56.22, 56.22, 55.9125, 55.9125, 56.06, 56.06, 55.8775, 55.8775, 55.8875, 55.8875, 55.8375, 55.8375, 56.49, 56.49]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.997, Test loss: 1.734, Test accuracy: 37.41
Round   0, Global train loss: 1.997, Global test loss: 1.732, Global test accuracy: 38.37
Round   1, Train loss: 1.698, Test loss: 1.551, Test accuracy: 43.78
Round   1, Global train loss: 1.698, Global test loss: 1.479, Global test accuracy: 46.92
Round   2, Train loss: 1.535, Test loss: 1.509, Test accuracy: 45.42
Round   2, Global train loss: 1.535, Global test loss: 1.342, Global test accuracy: 51.90
Round   3, Train loss: 1.450, Test loss: 1.401, Test accuracy: 49.82
Round   3, Global train loss: 1.450, Global test loss: 1.246, Global test accuracy: 56.28
Round   4, Train loss: 1.332, Test loss: 1.386, Test accuracy: 50.14
Round   4, Global train loss: 1.332, Global test loss: 1.166, Global test accuracy: 58.39
Round   5, Train loss: 1.270, Test loss: 1.400, Test accuracy: 49.80
Round   5, Global train loss: 1.270, Global test loss: 1.111, Global test accuracy: 60.45
Round   6, Train loss: 1.206, Test loss: 1.333, Test accuracy: 52.67
Round   6, Global train loss: 1.206, Global test loss: 1.064, Global test accuracy: 62.00
Round   7, Train loss: 1.157, Test loss: 1.299, Test accuracy: 54.28
Round   7, Global train loss: 1.157, Global test loss: 1.008, Global test accuracy: 64.34
Round   8, Train loss: 1.092, Test loss: 1.234, Test accuracy: 56.78
Round   8, Global train loss: 1.092, Global test loss: 0.991, Global test accuracy: 65.03
Round   9, Train loss: 1.051, Test loss: 1.196, Test accuracy: 58.23
Round   9, Global train loss: 1.051, Global test loss: 0.961, Global test accuracy: 66.42
Round  10, Train loss: 1.029, Test loss: 1.158, Test accuracy: 59.78
Round  10, Global train loss: 1.029, Global test loss: 0.920, Global test accuracy: 68.10
Round  11, Train loss: 0.966, Test loss: 1.133, Test accuracy: 60.64
Round  11, Global train loss: 0.966, Global test loss: 0.909, Global test accuracy: 68.69
Round  12, Train loss: 0.944, Test loss: 1.113, Test accuracy: 61.33
Round  12, Global train loss: 0.944, Global test loss: 0.875, Global test accuracy: 69.65
Round  13, Train loss: 0.912, Test loss: 1.089, Test accuracy: 62.24
Round  13, Global train loss: 0.912, Global test loss: 0.865, Global test accuracy: 70.11
Round  14, Train loss: 0.874, Test loss: 1.081, Test accuracy: 62.78
Round  14, Global train loss: 0.874, Global test loss: 0.851, Global test accuracy: 71.23
Round  15, Train loss: 0.865, Test loss: 1.072, Test accuracy: 63.29
Round  15, Global train loss: 0.865, Global test loss: 0.838, Global test accuracy: 71.67
Round  16, Train loss: 0.855, Test loss: 1.058, Test accuracy: 64.01
Round  16, Global train loss: 0.855, Global test loss: 0.821, Global test accuracy: 71.59
Round  17, Train loss: 0.844, Test loss: 1.043, Test accuracy: 64.75
Round  17, Global train loss: 0.844, Global test loss: 0.812, Global test accuracy: 72.59
Round  18, Train loss: 0.830, Test loss: 1.028, Test accuracy: 65.19
Round  18, Global train loss: 0.830, Global test loss: 0.788, Global test accuracy: 73.17
Round  19, Train loss: 0.793, Test loss: 1.027, Test accuracy: 65.51
Round  19, Global train loss: 0.793, Global test loss: 0.793, Global test accuracy: 73.00
Round  20, Train loss: 0.782, Test loss: 1.018, Test accuracy: 65.95
Round  20, Global train loss: 0.782, Global test loss: 0.773, Global test accuracy: 73.97
Round  21, Train loss: 0.741, Test loss: 1.009, Test accuracy: 66.32
Round  21, Global train loss: 0.741, Global test loss: 0.783, Global test accuracy: 73.73
Round  22, Train loss: 0.755, Test loss: 0.989, Test accuracy: 67.25
Round  22, Global train loss: 0.755, Global test loss: 0.776, Global test accuracy: 73.84
Round  23, Train loss: 0.732, Test loss: 0.976, Test accuracy: 67.73
Round  23, Global train loss: 0.732, Global test loss: 0.775, Global test accuracy: 74.09
Round  24, Train loss: 0.718, Test loss: 0.981, Test accuracy: 67.78
Round  24, Global train loss: 0.718, Global test loss: 0.766, Global test accuracy: 74.54
Round  25, Train loss: 0.698, Test loss: 0.979, Test accuracy: 67.92
Round  25, Global train loss: 0.698, Global test loss: 0.765, Global test accuracy: 74.14
Round  26, Train loss: 0.693, Test loss: 0.990, Test accuracy: 67.91
Round  26, Global train loss: 0.693, Global test loss: 0.774, Global test accuracy: 74.24
Round  27, Train loss: 0.711, Test loss: 0.995, Test accuracy: 68.25
Round  27, Global train loss: 0.711, Global test loss: 0.760, Global test accuracy: 74.84
Round  28, Train loss: 0.668, Test loss: 0.984, Test accuracy: 68.50
Round  28, Global train loss: 0.668, Global test loss: 0.759, Global test accuracy: 74.61
Round  29, Train loss: 0.673, Test loss: 0.986, Test accuracy: 68.66
Round  29, Global train loss: 0.673, Global test loss: 0.757, Global test accuracy: 74.66
Round  30, Train loss: 0.642, Test loss: 0.979, Test accuracy: 69.20
Round  30, Global train loss: 0.642, Global test loss: 0.751, Global test accuracy: 75.21
Round  31, Train loss: 0.656, Test loss: 0.975, Test accuracy: 69.33
Round  31, Global train loss: 0.656, Global test loss: 0.741, Global test accuracy: 75.29
Round  32, Train loss: 0.635, Test loss: 0.974, Test accuracy: 69.41
Round  32, Global train loss: 0.635, Global test loss: 0.751, Global test accuracy: 75.29
Round  33, Train loss: 0.646, Test loss: 0.967, Test accuracy: 69.62
Round  33, Global train loss: 0.646, Global test loss: 0.740, Global test accuracy: 75.78
Round  34, Train loss: 0.599, Test loss: 0.965, Test accuracy: 69.73
Round  34, Global train loss: 0.599, Global test loss: 0.744, Global test accuracy: 75.98
Round  35, Train loss: 0.589, Test loss: 0.966, Test accuracy: 69.83
Round  35, Global train loss: 0.589, Global test loss: 0.752, Global test accuracy: 75.71
Round  36, Train loss: 0.591, Test loss: 0.968, Test accuracy: 69.81
Round  36, Global train loss: 0.591, Global test loss: 0.755, Global test accuracy: 75.69
Round  37, Train loss: 0.589, Test loss: 0.962, Test accuracy: 70.29
Round  37, Global train loss: 0.589, Global test loss: 0.759, Global test accuracy: 75.82
Round  38, Train loss: 0.611, Test loss: 0.955, Test accuracy: 70.36
Round  38, Global train loss: 0.611, Global test loss: 0.732, Global test accuracy: 76.21
Round  39, Train loss: 0.613, Test loss: 0.956, Test accuracy: 70.23
Round  39, Global train loss: 0.613, Global test loss: 0.737, Global test accuracy: 75.76
Round  40, Train loss: 0.577, Test loss: 0.949, Test accuracy: 70.41
Round  40, Global train loss: 0.577, Global test loss: 0.733, Global test accuracy: 76.13
Round  41, Train loss: 0.580, Test loss: 0.951, Test accuracy: 70.44
Round  41, Global train loss: 0.580, Global test loss: 0.735, Global test accuracy: 76.28
Round  42, Train loss: 0.549, Test loss: 0.967, Test accuracy: 70.11
Round  42, Global train loss: 0.549, Global test loss: 0.749, Global test accuracy: 76.42
Round  43, Train loss: 0.576, Test loss: 0.962, Test accuracy: 70.39
Round  43, Global train loss: 0.576, Global test loss: 0.737, Global test accuracy: 76.63
Round  44, Train loss: 0.540, Test loss: 0.959, Test accuracy: 70.51
Round  44, Global train loss: 0.540, Global test loss: 0.744, Global test accuracy: 76.49
Round  45, Train loss: 0.559, Test loss: 0.949, Test accuracy: 70.81
Round  45, Global train loss: 0.559, Global test loss: 0.734, Global test accuracy: 76.46
Round  46, Train loss: 0.542, Test loss: 0.943, Test accuracy: 71.11
Round  46, Global train loss: 0.542, Global test loss: 0.740, Global test accuracy: 76.84
Round  47, Train loss: 0.528, Test loss: 0.954, Test accuracy: 70.84
Round  47, Global train loss: 0.528, Global test loss: 0.746, Global test accuracy: 76.25
Round  48, Train loss: 0.555, Test loss: 0.942, Test accuracy: 71.18
Round  48, Global train loss: 0.555, Global test loss: 0.722, Global test accuracy: 76.99
Round  49, Train loss: 0.529, Test loss: 0.936, Test accuracy: 71.50
Round  49, Global train loss: 0.529, Global test loss: 0.731, Global test accuracy: 77.25
Round  50, Train loss: 0.540, Test loss: 0.934, Test accuracy: 71.77
Round  50, Global train loss: 0.540, Global test loss: 0.729, Global test accuracy: 77.12
Round  51, Train loss: 0.526, Test loss: 0.935, Test accuracy: 71.72
Round  51, Global train loss: 0.526, Global test loss: 0.727, Global test accuracy: 77.12
Round  52, Train loss: 0.532, Test loss: 0.942, Test accuracy: 71.93
Round  52, Global train loss: 0.532, Global test loss: 0.722, Global test accuracy: 77.35
Round  53, Train loss: 0.528, Test loss: 0.949, Test accuracy: 71.88
Round  53, Global train loss: 0.528, Global test loss: 0.731, Global test accuracy: 77.52
Round  54, Train loss: 0.482, Test loss: 0.956, Test accuracy: 71.91
Round  54, Global train loss: 0.482, Global test loss: 0.754, Global test accuracy: 77.20
Round  55, Train loss: 0.508, Test loss: 0.974, Test accuracy: 71.58
Round  55, Global train loss: 0.508, Global test loss: 0.748, Global test accuracy: 76.56
Round  56, Train loss: 0.500, Test loss: 0.964, Test accuracy: 71.80
Round  56, Global train loss: 0.500, Global test loss: 0.728, Global test accuracy: 77.22
Round  57, Train loss: 0.488, Test loss: 0.959, Test accuracy: 71.79
Round  57, Global train loss: 0.488, Global test loss: 0.744, Global test accuracy: 77.47
Round  58, Train loss: 0.502, Test loss: 0.959, Test accuracy: 71.97
Round  58, Global train loss: 0.502, Global test loss: 0.745, Global test accuracy: 77.28
Round  59, Train loss: 0.489, Test loss: 0.959, Test accuracy: 72.10
Round  59, Global train loss: 0.489, Global test loss: 0.733, Global test accuracy: 77.41
Round  60, Train loss: 0.508, Test loss: 0.959, Test accuracy: 72.31
Round  60, Global train loss: 0.508, Global test loss: 0.740, Global test accuracy: 77.04
Round  61, Train loss: 0.499, Test loss: 0.957, Test accuracy: 72.36
Round  61, Global train loss: 0.499, Global test loss: 0.728, Global test accuracy: 77.55
Round  62, Train loss: 0.498, Test loss: 0.951, Test accuracy: 72.49
Round  62, Global train loss: 0.498, Global test loss: 0.730, Global test accuracy: 77.66
Round  63, Train loss: 0.478, Test loss: 0.945, Test accuracy: 72.62
Round  63, Global train loss: 0.478, Global test loss: 0.729, Global test accuracy: 77.92
Round  64, Train loss: 0.497, Test loss: 0.946, Test accuracy: 72.67
Round  64, Global train loss: 0.497, Global test loss: 0.734, Global test accuracy: 77.63
Round  65, Train loss: 0.485, Test loss: 0.936, Test accuracy: 72.97
Round  65, Global train loss: 0.485, Global test loss: 0.722, Global test accuracy: 77.47
Round  66, Train loss: 0.476, Test loss: 0.932, Test accuracy: 73.13
Round  66, Global train loss: 0.476, Global test loss: 0.718, Global test accuracy: 77.88
Round  67, Train loss: 0.443, Test loss: 0.933, Test accuracy: 73.22
Round  67, Global train loss: 0.443, Global test loss: 0.762, Global test accuracy: 77.60
Round  68, Train loss: 0.493, Test loss: 0.939, Test accuracy: 73.04
Round  68, Global train loss: 0.493, Global test loss: 0.738, Global test accuracy: 77.49
Round  69, Train loss: 0.452, Test loss: 0.938, Test accuracy: 73.09
Round  69, Global train loss: 0.452, Global test loss: 0.742, Global test accuracy: 77.64
Round  70, Train loss: 0.501, Test loss: 0.943, Test accuracy: 73.07
Round  70, Global train loss: 0.501, Global test loss: 0.722, Global test accuracy: 77.78
Round  71, Train loss: 0.461, Test loss: 0.952, Test accuracy: 73.01
Round  71, Global train loss: 0.461, Global test loss: 0.743, Global test accuracy: 77.51
Round  72, Train loss: 0.468, Test loss: 0.959, Test accuracy: 72.76
Round  72, Global train loss: 0.468, Global test loss: 0.737, Global test accuracy: 77.62
Round  73, Train loss: 0.437, Test loss: 0.952, Test accuracy: 73.02
Round  73, Global train loss: 0.437, Global test loss: 0.744, Global test accuracy: 77.69
Round  74, Train loss: 0.481, Test loss: 0.961, Test accuracy: 72.78
Round  74, Global train loss: 0.481, Global test loss: 0.724, Global test accuracy: 77.41
Round  75, Train loss: 0.449, Test loss: 0.956, Test accuracy: 72.92
Round  75, Global train loss: 0.449, Global test loss: 0.739, Global test accuracy: 77.95
Round  76, Train loss: 0.446, Test loss: 0.951, Test accuracy: 73.33
Round  76, Global train loss: 0.446, Global test loss: 0.749, Global test accuracy: 77.68
Round  77, Train loss: 0.456, Test loss: 0.963, Test accuracy: 72.96
Round  77, Global train loss: 0.456, Global test loss: 0.747, Global test accuracy: 77.67
Round  78, Train loss: 0.450, Test loss: 0.965, Test accuracy: 73.07
Round  78, Global train loss: 0.450, Global test loss: 0.754, Global test accuracy: 77.86
Round  79, Train loss: 0.438, Test loss: 0.969, Test accuracy: 72.94
Round  79, Global train loss: 0.438, Global test loss: 0.749, Global test accuracy: 77.57
Round  80, Train loss: 0.446, Test loss: 0.974, Test accuracy: 72.91
Round  80, Global train loss: 0.446, Global test loss: 0.749, Global test accuracy: 77.77
Round  81, Train loss: 0.433, Test loss: 0.972, Test accuracy: 72.93
Round  81, Global train loss: 0.433, Global test loss: 0.742, Global test accuracy: 77.81
Round  82, Train loss: 0.428, Test loss: 0.969, Test accuracy: 73.00
Round  82, Global train loss: 0.428, Global test loss: 0.752, Global test accuracy: 78.00
Round  83, Train loss: 0.446, Test loss: 0.976, Test accuracy: 72.67
Round  83, Global train loss: 0.446, Global test loss: 0.733, Global test accuracy: 77.65
Round  84, Train loss: 0.439, Test loss: 0.970, Test accuracy: 72.83
Round  84, Global train loss: 0.439, Global test loss: 0.737, Global test accuracy: 77.88
Round  85, Train loss: 0.429, Test loss: 0.959, Test accuracy: 73.12
Round  85, Global train loss: 0.429, Global test loss: 0.730, Global test accuracy: 78.31
Round  86, Train loss: 0.418, Test loss: 0.963, Test accuracy: 73.03
Round  86, Global train loss: 0.418, Global test loss: 0.744, Global test accuracy: 78.14
Round  87, Train loss: 0.424, Test loss: 0.963, Test accuracy: 73.20
Round  87, Global train loss: 0.424, Global test loss: 0.745, Global test accuracy: 77.86
Round  88, Train loss: 0.419, Test loss: 0.967, Test accuracy: 72.98
Round  88, Global train loss: 0.419, Global test loss: 0.749, Global test accuracy: 78.12
Round  89, Train loss: 0.425, Test loss: 0.953, Test accuracy: 73.33
Round  89, Global train loss: 0.425, Global test loss: 0.740, Global test accuracy: 78.12
Round  90, Train loss: 0.416, Test loss: 0.961, Test accuracy: 73.28
Round  90, Global train loss: 0.416, Global test loss: 0.744, Global test accuracy: 78.25
Round  91, Train loss: 0.423, Test loss: 0.960, Test accuracy: 73.47
Round  91, Global train loss: 0.423, Global test loss: 0.733, Global test accuracy: 78.34
Round  92, Train loss: 0.430, Test loss: 0.963, Test accuracy: 73.44
Round  92, Global train loss: 0.430, Global test loss: 0.732, Global test accuracy: 78.46
Round  93, Train loss: 0.402, Test loss: 0.954, Test accuracy: 73.62
Round  93, Global train loss: 0.402, Global test loss: 0.758, Global test accuracy: 78.00
Round  94, Train loss: 0.379, Test loss: 0.960, Test accuracy: 73.56
Round  94, Global train loss: 0.379, Global test loss: 0.765, Global test accuracy: 77.85
Round  95, Train loss: 0.391, Test loss: 0.970, Test accuracy: 73.25
Round  95, Global train loss: 0.391, Global test loss: 0.759, Global test accuracy: 77.99
Round  96, Train loss: 0.377, Test loss: 0.969, Test accuracy: 73.38
Round  96, Global train loss: 0.377, Global test loss: 0.768, Global test accuracy: 77.98
Round  97, Train loss: 0.391, Test loss: 0.982, Test accuracy: 73.16
Round  97, Global train loss: 0.391, Global test loss: 0.767, Global test accuracy: 77.97
Round  98, Train loss: 0.405, Test loss: 0.980, Test accuracy: 73.25
Round  98, Global train loss: 0.405, Global test loss: 0.774, Global test accuracy: 77.90
Round  99, Train loss: 0.401, Test loss: 0.993, Test accuracy: 73.12
Round  99, Global train loss: 0.401, Global test loss: 0.753, Global test accuracy: 78.09
Final Round, Train loss: 0.241, Test loss: 1.071, Test accuracy: 74.21
Final Round, Global train loss: 0.241, Global test loss: 0.753, Global test accuracy: 78.09
Average accuracy final 10 rounds: 73.35425 

Average global accuracy final 10 rounds: 78.08375 

5962.4187796115875
[5.13657546043396, 10.27315092086792, 14.880733966827393, 19.488317012786865, 23.691633224487305, 27.894949436187744, 32.12674164772034, 36.35853385925293, 40.535797119140625, 44.71306037902832, 48.99341368675232, 53.27376699447632, 57.5109429359436, 61.74811887741089, 66.10927629470825, 70.47043371200562, 74.7695620059967, 79.0686902999878, 83.4077365398407, 87.7467827796936, 92.00243163108826, 96.25808048248291, 100.85112142562866, 105.44416236877441, 109.93855404853821, 114.432945728302, 118.84601354598999, 123.25908136367798, 127.91394138336182, 132.56880140304565, 137.18787813186646, 141.80695486068726, 146.65319418907166, 151.49943351745605, 156.34327816963196, 161.18712282180786, 166.0920226573944, 170.99692249298096, 176.0488474369049, 181.10077238082886, 185.975088596344, 190.84940481185913, 195.91770029067993, 200.98599576950073, 205.97038173675537, 210.95476770401, 215.62713623046875, 220.2995047569275, 225.40542435646057, 230.51134395599365, 235.60660195350647, 240.7018599510193, 245.3238832950592, 249.94590663909912, 254.5084846019745, 259.07106256484985, 263.6229717731476, 268.1748809814453, 272.7850263118744, 277.39517164230347, 281.92642283439636, 286.45767402648926, 290.999493598938, 295.5413131713867, 300.53594398498535, 305.530574798584, 310.1076691150665, 314.6847634315491, 319.2488420009613, 323.81292057037354, 328.3067035675049, 332.80048656463623, 337.2976896762848, 341.79489278793335, 346.3912823200226, 350.9876718521118, 355.91691517829895, 360.8461585044861, 365.7247381210327, 370.60331773757935, 375.32607984542847, 380.0488419532776, 384.9891414642334, 389.9294409751892, 394.7647535800934, 399.60006618499756, 404.5623481273651, 409.52463006973267, 414.0615770816803, 418.59852409362793, 423.0986578464508, 427.5987915992737, 431.89897871017456, 436.19916582107544, 440.5945029258728, 444.98984003067017, 449.35877776145935, 453.72771549224854, 458.05038571357727, 462.373055934906, 466.80419158935547, 471.23532724380493, 475.6528730392456, 480.0704188346863, 484.54893040657043, 489.0274419784546, 493.5734796524048, 498.119517326355, 502.60551619529724, 507.0915150642395, 511.65705394744873, 516.222592830658, 520.7368903160095, 525.2511878013611, 529.7179715633392, 534.1847553253174, 538.8313882350922, 543.478021144867, 548.399085521698, 553.320149898529, 557.9857187271118, 562.6512875556946, 567.191103219986, 571.7309188842773, 576.3011717796326, 580.8714246749878, 585.4275496006012, 589.9836745262146, 594.5114023685455, 599.0391302108765, 603.5501987934113, 608.061267375946, 612.6534233093262, 617.2455792427063, 621.7220139503479, 626.1984486579895, 630.8622968196869, 635.5261449813843, 640.1260182857513, 644.7258915901184, 649.2357723712921, 653.7456531524658, 658.1047523021698, 662.4638514518738, 666.726624250412, 670.9893970489502, 675.2602124214172, 679.5310277938843, 683.7173855304718, 687.9037432670593, 692.1254861354828, 696.3472290039062, 700.6089150905609, 704.8706011772156, 709.0840511322021, 713.2975010871887, 717.5124924182892, 721.7274837493896, 725.8979160785675, 730.0683484077454, 734.3198285102844, 738.5713086128235, 742.711484670639, 746.8516607284546, 751.128427028656, 755.4051933288574, 759.6615650653839, 763.9179368019104, 768.1745595932007, 772.431182384491, 776.5608019828796, 780.6904215812683, 784.945511341095, 789.2006011009216, 793.4117171764374, 797.6228332519531, 801.7356109619141, 805.848388671875, 809.8798277378082, 813.9112668037415, 818.1219687461853, 822.3326706886292, 826.5332000255585, 830.7337293624878, 834.9752233028412, 839.2167172431946, 843.4302477836609, 847.6437783241272, 851.8606584072113, 856.0775384902954, 860.2820429801941, 864.4865474700928, 868.6948692798615, 872.9031910896301, 877.001654624939, 881.1001181602478, 885.3449060916901, 889.5896940231323, 894.1438047885895, 898.6979155540466, 901.1016635894775, 903.5054116249084]
[37.4125, 37.4125, 43.785, 43.785, 45.425, 45.425, 49.82, 49.82, 50.1425, 50.1425, 49.805, 49.805, 52.6725, 52.6725, 54.28, 54.28, 56.78, 56.78, 58.225, 58.225, 59.7825, 59.7825, 60.6425, 60.6425, 61.3275, 61.3275, 62.2375, 62.2375, 62.7775, 62.7775, 63.29, 63.29, 64.0075, 64.0075, 64.75, 64.75, 65.19, 65.19, 65.5075, 65.5075, 65.95, 65.95, 66.3175, 66.3175, 67.2475, 67.2475, 67.73, 67.73, 67.775, 67.775, 67.925, 67.925, 67.9125, 67.9125, 68.245, 68.245, 68.4975, 68.4975, 68.6575, 68.6575, 69.205, 69.205, 69.325, 69.325, 69.41, 69.41, 69.625, 69.625, 69.73, 69.73, 69.8275, 69.8275, 69.8075, 69.8075, 70.2925, 70.2925, 70.355, 70.355, 70.235, 70.235, 70.41, 70.41, 70.4375, 70.4375, 70.105, 70.105, 70.385, 70.385, 70.51, 70.51, 70.8125, 70.8125, 71.105, 71.105, 70.845, 70.845, 71.1825, 71.1825, 71.4975, 71.4975, 71.7725, 71.7725, 71.7175, 71.7175, 71.9325, 71.9325, 71.8825, 71.8825, 71.91, 71.91, 71.5775, 71.5775, 71.795, 71.795, 71.7925, 71.7925, 71.975, 71.975, 72.1, 72.1, 72.3075, 72.3075, 72.3575, 72.3575, 72.4875, 72.4875, 72.625, 72.625, 72.6675, 72.6675, 72.9675, 72.9675, 73.1275, 73.1275, 73.215, 73.215, 73.04, 73.04, 73.0925, 73.0925, 73.0725, 73.0725, 73.0125, 73.0125, 72.7625, 72.7625, 73.0225, 73.0225, 72.775, 72.775, 72.9175, 72.9175, 73.33, 73.33, 72.9625, 72.9625, 73.0675, 73.0675, 72.9375, 72.9375, 72.905, 72.905, 72.93, 72.93, 72.9975, 72.9975, 72.665, 72.665, 72.8275, 72.8275, 73.1175, 73.1175, 73.0275, 73.0275, 73.205, 73.205, 72.9825, 72.9825, 73.3275, 73.3275, 73.275, 73.275, 73.47, 73.47, 73.4425, 73.4425, 73.6225, 73.6225, 73.565, 73.565, 73.2525, 73.2525, 73.3775, 73.3775, 73.1575, 73.1575, 73.255, 73.255, 73.125, 73.125, 74.2125, 74.2125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.194, Test loss: 1.909, Test accuracy: 32.70
Round   1, Train loss: 1.872, Test loss: 1.679, Test accuracy: 38.30
Round   2, Train loss: 1.693, Test loss: 1.559, Test accuracy: 43.12
Round   3, Train loss: 1.598, Test loss: 1.480, Test accuracy: 46.24
Round   4, Train loss: 1.522, Test loss: 1.410, Test accuracy: 49.51
Round   5, Train loss: 1.435, Test loss: 1.355, Test accuracy: 52.47
Round   6, Train loss: 1.401, Test loss: 1.276, Test accuracy: 55.07
Round   7, Train loss: 1.327, Test loss: 1.243, Test accuracy: 56.77
Round   8, Train loss: 1.293, Test loss: 1.199, Test accuracy: 59.18
Round   9, Train loss: 1.244, Test loss: 1.153, Test accuracy: 59.50
Round  10, Train loss: 1.211, Test loss: 1.121, Test accuracy: 60.99
Round  11, Train loss: 1.165, Test loss: 1.085, Test accuracy: 62.47
Round  12, Train loss: 1.142, Test loss: 1.059, Test accuracy: 63.66
Round  13, Train loss: 1.109, Test loss: 1.033, Test accuracy: 64.37
Round  14, Train loss: 1.078, Test loss: 1.020, Test accuracy: 64.93
Round  15, Train loss: 1.052, Test loss: 0.989, Test accuracy: 66.23
Round  16, Train loss: 1.035, Test loss: 0.992, Test accuracy: 66.04
Round  17, Train loss: 1.011, Test loss: 0.956, Test accuracy: 67.38
Round  18, Train loss: 0.976, Test loss: 0.946, Test accuracy: 67.74
Round  19, Train loss: 0.957, Test loss: 0.932, Test accuracy: 68.35
Round  20, Train loss: 0.944, Test loss: 0.914, Test accuracy: 68.59
Round  21, Train loss: 0.926, Test loss: 0.894, Test accuracy: 69.35
Round  22, Train loss: 0.909, Test loss: 0.887, Test accuracy: 69.55
Round  23, Train loss: 0.867, Test loss: 0.884, Test accuracy: 69.71
Round  24, Train loss: 0.870, Test loss: 0.877, Test accuracy: 70.11
Round  25, Train loss: 0.846, Test loss: 0.866, Test accuracy: 70.38
Round  26, Train loss: 0.825, Test loss: 0.868, Test accuracy: 70.57
Round  27, Train loss: 0.851, Test loss: 0.852, Test accuracy: 71.63
Round  28, Train loss: 0.804, Test loss: 0.855, Test accuracy: 71.05
Round  29, Train loss: 0.846, Test loss: 0.840, Test accuracy: 71.93
Round  30, Train loss: 0.784, Test loss: 0.831, Test accuracy: 71.64
Round  31, Train loss: 0.781, Test loss: 0.832, Test accuracy: 72.14
Round  32, Train loss: 0.771, Test loss: 0.811, Test accuracy: 72.76
Round  33, Train loss: 0.770, Test loss: 0.799, Test accuracy: 73.10
Round  34, Train loss: 0.753, Test loss: 0.804, Test accuracy: 73.20
Round  35, Train loss: 0.775, Test loss: 0.818, Test accuracy: 72.30
Round  36, Train loss: 0.743, Test loss: 0.811, Test accuracy: 72.69
Round  37, Train loss: 0.737, Test loss: 0.809, Test accuracy: 72.91
Round  38, Train loss: 0.728, Test loss: 0.808, Test accuracy: 72.97
Round  39, Train loss: 0.710, Test loss: 0.820, Test accuracy: 73.00
Round  40, Train loss: 0.699, Test loss: 0.796, Test accuracy: 73.43
Round  41, Train loss: 0.704, Test loss: 0.793, Test accuracy: 73.56
Round  42, Train loss: 0.683, Test loss: 0.791, Test accuracy: 73.80
Round  43, Train loss: 0.677, Test loss: 0.802, Test accuracy: 73.08
Round  44, Train loss: 0.683, Test loss: 0.786, Test accuracy: 73.79
Round  45, Train loss: 0.673, Test loss: 0.788, Test accuracy: 73.90
Round  46, Train loss: 0.674, Test loss: 0.790, Test accuracy: 73.81
Round  47, Train loss: 0.658, Test loss: 0.780, Test accuracy: 74.51
Round  48, Train loss: 0.649, Test loss: 0.778, Test accuracy: 74.49
Round  49, Train loss: 0.650, Test loss: 0.765, Test accuracy: 74.63
Round  50, Train loss: 0.622, Test loss: 0.767, Test accuracy: 75.01
Round  51, Train loss: 0.642, Test loss: 0.763, Test accuracy: 74.97
Round  52, Train loss: 0.638, Test loss: 0.771, Test accuracy: 74.75
Round  53, Train loss: 0.634, Test loss: 0.769, Test accuracy: 75.10
Round  54, Train loss: 0.617, Test loss: 0.768, Test accuracy: 74.83
Round  55, Train loss: 0.634, Test loss: 0.766, Test accuracy: 75.54
Round  56, Train loss: 0.631, Test loss: 0.760, Test accuracy: 75.22
Round  57, Train loss: 0.613, Test loss: 0.758, Test accuracy: 75.23
Round  58, Train loss: 0.614, Test loss: 0.763, Test accuracy: 75.25
Round  59, Train loss: 0.616, Test loss: 0.761, Test accuracy: 75.25
Round  60, Train loss: 0.591, Test loss: 0.763, Test accuracy: 75.07
Round  61, Train loss: 0.607, Test loss: 0.743, Test accuracy: 75.88
Round  62, Train loss: 0.590, Test loss: 0.759, Test accuracy: 75.58
Round  63, Train loss: 0.582, Test loss: 0.751, Test accuracy: 75.80
Round  64, Train loss: 0.585, Test loss: 0.769, Test accuracy: 75.29
Round  65, Train loss: 0.587, Test loss: 0.746, Test accuracy: 75.79
Round  66, Train loss: 0.561, Test loss: 0.754, Test accuracy: 75.42
Round  67, Train loss: 0.566, Test loss: 0.754, Test accuracy: 75.70
Round  68, Train loss: 0.571, Test loss: 0.759, Test accuracy: 75.79
Round  69, Train loss: 0.566, Test loss: 0.767, Test accuracy: 75.41
Round  70, Train loss: 0.581, Test loss: 0.754, Test accuracy: 75.70
Round  71, Train loss: 0.560, Test loss: 0.748, Test accuracy: 75.79
Round  72, Train loss: 0.571, Test loss: 0.741, Test accuracy: 75.98
Round  73, Train loss: 0.531, Test loss: 0.740, Test accuracy: 76.24
Round  74, Train loss: 0.555, Test loss: 0.748, Test accuracy: 76.31
Round  75, Train loss: 0.540, Test loss: 0.746, Test accuracy: 76.26
Round  76, Train loss: 0.538, Test loss: 0.750, Test accuracy: 76.00
Round  77, Train loss: 0.552, Test loss: 0.740, Test accuracy: 76.39
Round  78, Train loss: 0.554, Test loss: 0.746, Test accuracy: 76.28
Round  79, Train loss: 0.520, Test loss: 0.753, Test accuracy: 76.08
Round  80, Train loss: 0.504, Test loss: 0.765, Test accuracy: 75.94
Round  81, Train loss: 0.533, Test loss: 0.754, Test accuracy: 76.12
Round  82, Train loss: 0.531, Test loss: 0.743, Test accuracy: 76.52
Round  83, Train loss: 0.510, Test loss: 0.756, Test accuracy: 76.38
Round  84, Train loss: 0.514, Test loss: 0.752, Test accuracy: 76.61
Round  85, Train loss: 0.527, Test loss: 0.765, Test accuracy: 76.19
Round  86, Train loss: 0.496, Test loss: 0.754, Test accuracy: 76.52
Round  87, Train loss: 0.510, Test loss: 0.757, Test accuracy: 76.50
Round  88, Train loss: 0.535, Test loss: 0.756, Test accuracy: 76.16
Round  89, Train loss: 0.505, Test loss: 0.755, Test accuracy: 76.70
Round  90, Train loss: 0.509, Test loss: 0.745, Test accuracy: 76.89
Round  91, Train loss: 0.507, Test loss: 0.758, Test accuracy: 76.70
Round  92, Train loss: 0.491, Test loss: 0.755, Test accuracy: 76.46
Round  93, Train loss: 0.530, Test loss: 0.741, Test accuracy: 77.16
Round  94, Train loss: 0.498, Test loss: 0.761, Test accuracy: 76.56
Round  95, Train loss: 0.502, Test loss: 0.754, Test accuracy: 76.77
Round  96, Train loss: 0.491, Test loss: 0.756, Test accuracy: 76.59
Round  97, Train loss: 0.487, Test loss: 0.762, Test accuracy: 76.55
Round  98, Train loss: 0.504, Test loss: 0.763, Test accuracy: 76.78
Round  99, Train loss: 0.485, Test loss: 0.752, Test accuracy: 76.94
Final Round, Train loss: 0.429, Test loss: 0.754, Test accuracy: 77.05
Average accuracy final 10 rounds: 76.739 

5348.449532747269
[5.082571268081665, 10.16514253616333, 15.091081857681274, 20.01702117919922, 24.977850675582886, 29.938680171966553, 34.91903519630432, 39.89939022064209, 44.55125570297241, 49.203121185302734, 54.02049374580383, 58.83786630630493, 63.650442123413086, 68.46301794052124, 73.29684138298035, 78.13066482543945, 82.82393908500671, 87.51721334457397, 92.35849666595459, 97.1997799873352, 101.91212487220764, 106.62446975708008, 111.3459746837616, 116.06747961044312, 120.70110821723938, 125.33473682403564, 129.92448377609253, 134.5142307281494, 139.0160608291626, 143.51789093017578, 148.2081470489502, 152.8984031677246, 157.8480212688446, 162.7976393699646, 167.4434802532196, 172.0893211364746, 176.82906770706177, 181.56881427764893, 186.17015385627747, 190.771493434906, 195.61969351768494, 200.46789360046387, 205.2347116470337, 210.00152969360352, 214.70543265342712, 219.40933561325073, 224.24494314193726, 229.08055067062378, 233.6323926448822, 238.18423461914062, 242.82858800888062, 247.4729413986206, 252.19709420204163, 256.92124700546265, 261.7738628387451, 266.6264786720276, 271.4259328842163, 276.22538709640503, 281.02359795570374, 285.82180881500244, 290.6374206542969, 295.4530324935913, 300.2606449127197, 305.06825733184814, 309.8032286167145, 314.5381999015808, 319.2208774089813, 323.90355491638184, 328.59333419799805, 333.28311347961426, 338.0665650367737, 342.8500165939331, 347.717289686203, 352.5845627784729, 357.5031497478485, 362.4217367172241, 367.3009865283966, 372.1802363395691, 376.8892824649811, 381.59832859039307, 386.2450051307678, 390.8916816711426, 395.6049015522003, 400.31812143325806, 405.20510029792786, 410.09207916259766, 415.0860068798065, 420.0799345970154, 424.9303181171417, 429.78070163726807, 434.4862895011902, 439.1918773651123, 444.035108089447, 448.87833881378174, 453.72499203681946, 458.5716452598572, 463.31389260292053, 468.0561399459839, 472.8133592605591, 477.5705785751343, 482.1976680755615, 486.82475757598877, 491.531938791275, 496.2391200065613, 500.9405484199524, 505.6419768333435, 510.50017404556274, 515.358371257782, 520.3800766468048, 525.4017820358276, 530.2517621517181, 535.1017422676086, 540.015839099884, 544.9299359321594, 549.5307161808014, 554.1314964294434, 559.0649528503418, 563.9984092712402, 568.8849737644196, 573.7715382575989, 578.5679247379303, 583.3643112182617, 588.1232507228851, 592.8821902275085, 597.8341012001038, 602.786012172699, 607.7208003997803, 612.6555886268616, 617.6531691551208, 622.6507496833801, 627.2460582256317, 631.8413667678833, 636.465717792511, 641.0900688171387, 645.5773847103119, 650.0647006034851, 654.9244046211243, 659.7841086387634, 664.586633682251, 669.3891587257385, 673.9212436676025, 678.4533286094666, 683.0656838417053, 687.6780390739441, 692.3636705875397, 697.0493021011353, 701.5665214061737, 706.0837407112122, 710.5997548103333, 715.1157689094543, 719.9699101448059, 724.8240513801575, 729.67937541008, 734.5346994400024, 739.0016958713531, 743.4686923027039, 747.8904585838318, 752.3122248649597, 757.0683176517487, 761.8244104385376, 766.5827217102051, 771.3410329818726, 775.8863639831543, 780.431694984436, 785.0882894992828, 789.7448840141296, 794.5841820240021, 799.4234800338745, 804.1865801811218, 808.9496803283691, 813.569329738617, 818.1889791488647, 822.8260312080383, 827.4630832672119, 832.2285933494568, 836.9941034317017, 841.6505205631256, 846.3069376945496, 851.1172139644623, 855.927490234375, 860.4805812835693, 865.0336723327637, 869.8773281574249, 874.7209839820862, 879.4970102310181, 884.27303647995, 888.8335921764374, 893.3941478729248, 897.927907705307, 902.4616675376892, 906.933265209198, 911.4048628807068, 916.1670327186584, 920.9292025566101, 925.7104935646057, 930.4917845726013, 935.3464143276215, 940.2010440826416, 945.1118168830872, 950.0225896835327, 952.2585372924805, 954.4944849014282]
[32.7025, 32.7025, 38.3025, 38.3025, 43.12, 43.12, 46.245, 46.245, 49.5075, 49.5075, 52.465, 52.465, 55.07, 55.07, 56.7725, 56.7725, 59.1825, 59.1825, 59.5025, 59.5025, 60.995, 60.995, 62.47, 62.47, 63.655, 63.655, 64.3725, 64.3725, 64.9275, 64.9275, 66.2325, 66.2325, 66.04, 66.04, 67.38, 67.38, 67.7425, 67.7425, 68.3475, 68.3475, 68.595, 68.595, 69.35, 69.35, 69.545, 69.545, 69.71, 69.71, 70.115, 70.115, 70.3825, 70.3825, 70.5725, 70.5725, 71.6325, 71.6325, 71.045, 71.045, 71.9275, 71.9275, 71.6375, 71.6375, 72.135, 72.135, 72.7575, 72.7575, 73.0975, 73.0975, 73.2, 73.2, 72.3, 72.3, 72.685, 72.685, 72.905, 72.905, 72.9675, 72.9675, 73.0, 73.0, 73.4275, 73.4275, 73.5625, 73.5625, 73.7975, 73.7975, 73.0775, 73.0775, 73.7925, 73.7925, 73.9025, 73.9025, 73.805, 73.805, 74.51, 74.51, 74.4875, 74.4875, 74.6275, 74.6275, 75.0075, 75.0075, 74.9725, 74.9725, 74.75, 74.75, 75.0975, 75.0975, 74.83, 74.83, 75.54, 75.54, 75.22, 75.22, 75.235, 75.235, 75.245, 75.245, 75.245, 75.245, 75.07, 75.07, 75.8825, 75.8825, 75.575, 75.575, 75.7975, 75.7975, 75.29, 75.29, 75.7925, 75.7925, 75.4225, 75.4225, 75.7025, 75.7025, 75.7875, 75.7875, 75.405, 75.405, 75.6975, 75.6975, 75.7925, 75.7925, 75.985, 75.985, 76.24, 76.24, 76.305, 76.305, 76.26, 76.26, 76.0025, 76.0025, 76.39, 76.39, 76.2825, 76.2825, 76.08, 76.08, 75.945, 75.945, 76.1175, 76.1175, 76.515, 76.515, 76.375, 76.375, 76.615, 76.615, 76.195, 76.195, 76.5225, 76.5225, 76.4975, 76.4975, 76.1625, 76.1625, 76.705, 76.705, 76.885, 76.885, 76.6975, 76.6975, 76.46, 76.46, 77.155, 77.155, 76.555, 76.555, 76.7725, 76.7725, 76.5875, 76.5875, 76.5525, 76.5525, 76.78, 76.78, 76.945, 76.945, 77.0475, 77.0475]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 0.820, Test loss: 2.108, Test accuracy: 23.32
Round   1, Train loss: 0.724, Test loss: 1.598, Test accuracy: 42.23
Round   2, Train loss: 0.648, Test loss: 1.742, Test accuracy: 48.01
Round   3, Train loss: 0.576, Test loss: 1.116, Test accuracy: 57.13
Round   4, Train loss: 0.571, Test loss: 1.010, Test accuracy: 62.01
Round   5, Train loss: 0.548, Test loss: 0.965, Test accuracy: 62.74
Round   6, Train loss: 0.459, Test loss: 0.750, Test accuracy: 71.64
Round   7, Train loss: 0.448, Test loss: 0.789, Test accuracy: 71.82
Round   8, Train loss: 0.472, Test loss: 0.676, Test accuracy: 75.46
Round   9, Train loss: 0.395, Test loss: 0.513, Test accuracy: 80.21
Round  10, Train loss: 0.413, Test loss: 0.471, Test accuracy: 82.21
Round  11, Train loss: 0.360, Test loss: 0.451, Test accuracy: 82.47
Round  12, Train loss: 0.408, Test loss: 0.437, Test accuracy: 84.06
Round  13, Train loss: 0.354, Test loss: 0.406, Test accuracy: 84.61
Round  14, Train loss: 0.314, Test loss: 0.406, Test accuracy: 84.24
Round  15, Train loss: 0.335, Test loss: 0.389, Test accuracy: 84.65
Round  16, Train loss: 0.321, Test loss: 0.389, Test accuracy: 84.65
Round  17, Train loss: 0.353, Test loss: 0.380, Test accuracy: 85.06
Round  18, Train loss: 0.310, Test loss: 0.359, Test accuracy: 86.39
Round  19, Train loss: 0.318, Test loss: 0.351, Test accuracy: 86.44
Round  20, Train loss: 0.337, Test loss: 0.345, Test accuracy: 86.76
Round  21, Train loss: 0.330, Test loss: 0.350, Test accuracy: 86.44
Round  22, Train loss: 0.373, Test loss: 0.342, Test accuracy: 86.95
Round  23, Train loss: 0.305, Test loss: 0.335, Test accuracy: 87.06
Round  24, Train loss: 0.276, Test loss: 0.332, Test accuracy: 87.19
Round  25, Train loss: 0.231, Test loss: 0.322, Test accuracy: 87.68
Round  26, Train loss: 0.240, Test loss: 0.326, Test accuracy: 87.18
Round  27, Train loss: 0.267, Test loss: 0.326, Test accuracy: 87.30
Round  28, Train loss: 0.266, Test loss: 0.319, Test accuracy: 87.92
Round  29, Train loss: 0.285, Test loss: 0.308, Test accuracy: 88.17
Round  30, Train loss: 0.274, Test loss: 0.307, Test accuracy: 88.31
Round  31, Train loss: 0.224, Test loss: 0.312, Test accuracy: 88.09
Round  32, Train loss: 0.287, Test loss: 0.312, Test accuracy: 88.15
Round  33, Train loss: 0.287, Test loss: 0.313, Test accuracy: 88.13
Round  34, Train loss: 0.256, Test loss: 0.311, Test accuracy: 88.17
Round  35, Train loss: 0.236, Test loss: 0.311, Test accuracy: 88.16
Round  36, Train loss: 0.249, Test loss: 0.306, Test accuracy: 88.42
Round  37, Train loss: 0.189, Test loss: 0.312, Test accuracy: 88.27
Round  38, Train loss: 0.232, Test loss: 0.314, Test accuracy: 88.34
Round  39, Train loss: 0.224, Test loss: 0.308, Test accuracy: 88.54
Round  40, Train loss: 0.186, Test loss: 0.307, Test accuracy: 88.70
Round  41, Train loss: 0.179, Test loss: 0.299, Test accuracy: 88.87
Round  42, Train loss: 0.185, Test loss: 0.303, Test accuracy: 88.70
Round  43, Train loss: 0.210, Test loss: 0.306, Test accuracy: 88.73
Round  44, Train loss: 0.166, Test loss: 0.298, Test accuracy: 88.87
Round  45, Train loss: 0.239, Test loss: 0.300, Test accuracy: 88.89
Round  46, Train loss: 0.215, Test loss: 0.307, Test accuracy: 88.89
Round  47, Train loss: 0.180, Test loss: 0.300, Test accuracy: 88.98
Round  48, Train loss: 0.184, Test loss: 0.299, Test accuracy: 89.17
Round  49, Train loss: 0.172, Test loss: 0.309, Test accuracy: 88.83
Round  50, Train loss: 0.176, Test loss: 0.301, Test accuracy: 89.14
Round  51, Train loss: 0.202, Test loss: 0.300, Test accuracy: 89.15
Round  52, Train loss: 0.164, Test loss: 0.303, Test accuracy: 89.11
Round  53, Train loss: 0.186, Test loss: 0.301, Test accuracy: 89.09
Round  54, Train loss: 0.152, Test loss: 0.300, Test accuracy: 89.25
Round  55, Train loss: 0.211, Test loss: 0.300, Test accuracy: 89.42
Round  56, Train loss: 0.199, Test loss: 0.293, Test accuracy: 89.48
Round  57, Train loss: 0.127, Test loss: 0.300, Test accuracy: 89.25
Round  58, Train loss: 0.208, Test loss: 0.307, Test accuracy: 89.00
Round  59, Train loss: 0.149, Test loss: 0.297, Test accuracy: 89.44
Round  60, Train loss: 0.160, Test loss: 0.298, Test accuracy: 89.44
Round  61, Train loss: 0.183, Test loss: 0.298, Test accuracy: 89.55
Round  62, Train loss: 0.159, Test loss: 0.303, Test accuracy: 89.38
Round  63, Train loss: 0.202, Test loss: 0.303, Test accuracy: 89.47
Round  64, Train loss: 0.187, Test loss: 0.302, Test accuracy: 89.61
Round  65, Train loss: 0.176, Test loss: 0.302, Test accuracy: 89.33
Round  66, Train loss: 0.154, Test loss: 0.303, Test accuracy: 89.51
Round  67, Train loss: 0.161, Test loss: 0.312, Test accuracy: 89.22
Round  68, Train loss: 0.155, Test loss: 0.304, Test accuracy: 89.29
Round  69, Train loss: 0.163, Test loss: 0.306, Test accuracy: 89.35
Round  70, Train loss: 0.160, Test loss: 0.303, Test accuracy: 89.64
Round  71, Train loss: 0.177, Test loss: 0.300, Test accuracy: 89.60
Round  72, Train loss: 0.145, Test loss: 0.310, Test accuracy: 89.37
Round  73, Train loss: 0.135, Test loss: 0.306, Test accuracy: 89.53
Round  74, Train loss: 0.175, Test loss: 0.304, Test accuracy: 89.65
Round  75, Train loss: 0.143, Test loss: 0.303, Test accuracy: 89.67
Round  76, Train loss: 0.159, Test loss: 0.308, Test accuracy: 89.75
Round  77, Train loss: 0.159, Test loss: 0.318, Test accuracy: 89.14
Round  78, Train loss: 0.118, Test loss: 0.319, Test accuracy: 89.49
Round  79, Train loss: 0.173, Test loss: 0.317, Test accuracy: 89.58
Round  80, Train loss: 0.118, Test loss: 0.318, Test accuracy: 89.48
Round  81, Train loss: 0.183, Test loss: 0.313, Test accuracy: 89.52
Round  82, Train loss: 0.144, Test loss: 0.307, Test accuracy: 89.70
Round  83, Train loss: 0.114, Test loss: 0.313, Test accuracy: 89.62
Round  84, Train loss: 0.152, Test loss: 0.309, Test accuracy: 89.84
Round  85, Train loss: 0.164, Test loss: 0.313, Test accuracy: 89.70
Round  86, Train loss: 0.131, Test loss: 0.324, Test accuracy: 89.38
Round  87, Train loss: 0.093, Test loss: 0.328, Test accuracy: 89.53
Round  88, Train loss: 0.130, Test loss: 0.315, Test accuracy: 89.64
Round  89, Train loss: 0.132, Test loss: 0.318, Test accuracy: 89.61
Round  90, Train loss: 0.155, Test loss: 0.314, Test accuracy: 89.80
Round  91, Train loss: 0.121, Test loss: 0.316, Test accuracy: 89.81
Round  92, Train loss: 0.176, Test loss: 0.319, Test accuracy: 89.78
Round  93, Train loss: 0.109, Test loss: 0.316, Test accuracy: 89.87
Round  94, Train loss: 0.162, Test loss: 0.318, Test accuracy: 89.82
Round  95, Train loss: 0.117, Test loss: 0.320, Test accuracy: 89.71
Round  96, Train loss: 0.105, Test loss: 0.332, Test accuracy: 89.71
Round  97, Train loss: 0.138, Test loss: 0.325, Test accuracy: 89.49
Round  98, Train loss: 0.105, Test loss: 0.324, Test accuracy: 89.52
Round  99, Train loss: 0.123, Test loss: 0.322, Test accuracy: 89.65
Final Round, Train loss: 0.100, Test loss: 0.323, Test accuracy: 89.76
Average accuracy final 10 rounds: 89.71472222222222 

5050.147883653641
[4.750868797302246, 9.501737594604492, 13.98945426940918, 18.477170944213867, 23.026941537857056, 27.576712131500244, 32.0575053691864, 36.53829860687256, 40.97066831588745, 45.403038024902344, 49.82847023010254, 54.253902435302734, 58.63152551651001, 63.009148597717285, 67.71845984458923, 72.42777109146118, 77.07732486724854, 81.72687864303589, 86.1796772480011, 90.63247585296631, 95.02069926261902, 99.40892267227173, 103.80953025817871, 108.2101378440857, 112.63503813743591, 117.05993843078613, 121.45396542549133, 125.84799242019653, 130.26906895637512, 134.6901454925537, 139.00602507591248, 143.32190465927124, 147.72983503341675, 152.13776540756226, 156.5055170059204, 160.87326860427856, 165.23034143447876, 169.58741426467896, 173.91846561431885, 178.24951696395874, 182.53879714012146, 186.82807731628418, 191.29582524299622, 195.76357316970825, 200.15731120109558, 204.5510492324829, 208.89333581924438, 213.23562240600586, 217.6035692691803, 221.97151613235474, 226.33060479164124, 230.68969345092773, 235.45191049575806, 240.21412754058838, 244.8524398803711, 249.4907522201538, 254.273592710495, 259.0564332008362, 263.75790214538574, 268.4593710899353, 272.8883283138275, 277.3172855377197, 281.57619047164917, 285.8350954055786, 290.26372480392456, 294.6923542022705, 299.0867965221405, 303.4812388420105, 307.80885887145996, 312.1364789009094, 316.4847524166107, 320.833025932312, 325.38839840888977, 329.94377088546753, 334.70095014572144, 339.45812940597534, 344.0514979362488, 348.6448664665222, 353.01634454727173, 357.38782262802124, 361.76671600341797, 366.1456093788147, 370.6517004966736, 375.15779161453247, 379.5379054546356, 383.91801929473877, 388.39254546165466, 392.86707162857056, 397.283935546875, 401.70079946517944, 406.11967635154724, 410.53855323791504, 414.8028817176819, 419.06721019744873, 423.8921091556549, 428.7170081138611, 433.5769510269165, 438.4368939399719, 443.2152032852173, 447.99351263046265, 452.6171946525574, 457.2408766746521, 461.76038217544556, 466.279887676239, 471.12402391433716, 475.9681601524353, 480.3976106643677, 484.82706117630005, 489.2424576282501, 493.6578540802002, 498.57464957237244, 503.4914450645447, 508.3779067993164, 513.2643685340881, 517.7108986377716, 522.1574287414551, 526.4791042804718, 530.8007798194885, 535.1932229995728, 539.585666179657, 544.0689902305603, 548.5523142814636, 552.9653260707855, 557.3783378601074, 561.7060341835022, 566.033730506897, 570.3952145576477, 574.7566986083984, 579.1218996047974, 583.4871006011963, 587.9474151134491, 592.4077296257019, 596.8429288864136, 601.2781281471252, 605.8348407745361, 610.391553401947, 614.7930541038513, 619.1945548057556, 623.5926916599274, 627.9908285140991, 632.6022500991821, 637.2136716842651, 641.598470211029, 645.983268737793, 650.4621548652649, 654.9410409927368, 659.1498951911926, 663.3587493896484, 667.5299739837646, 671.7011985778809, 676.1243598461151, 680.5475211143494, 685.2401366233826, 689.9327521324158, 694.522346496582, 699.1119408607483, 703.6686506271362, 708.2253603935242, 712.9246025085449, 717.6238446235657, 721.8585052490234, 726.0931658744812, 730.2887709140778, 734.4843759536743, 738.6529428958893, 742.8215098381042, 747.0056099891663, 751.1897101402283, 755.4985899925232, 759.8074698448181, 764.0001895427704, 768.1929092407227, 772.3524458408356, 776.5119824409485, 780.6795263290405, 784.8470702171326, 788.9891347885132, 793.1311993598938, 797.2360889911652, 801.3409786224365, 805.4856142997742, 809.6302499771118, 813.7642865180969, 817.898323059082, 822.0058815479279, 826.1134400367737, 830.2607865333557, 834.4081330299377, 838.5960960388184, 842.784059047699, 846.9090299606323, 851.0340008735657, 855.3071229457855, 859.5802450180054, 863.8495366573334, 868.1188282966614, 872.2713348865509, 876.4238414764404, 880.5949590206146, 884.7660765647888, 886.8731021881104, 888.9801278114319]
[23.319444444444443, 23.319444444444443, 42.230555555555554, 42.230555555555554, 48.00833333333333, 48.00833333333333, 57.13055555555555, 57.13055555555555, 62.013888888888886, 62.013888888888886, 62.736111111111114, 62.736111111111114, 71.63888888888889, 71.63888888888889, 71.81666666666666, 71.81666666666666, 75.46388888888889, 75.46388888888889, 80.21111111111111, 80.21111111111111, 82.20555555555555, 82.20555555555555, 82.47222222222223, 82.47222222222223, 84.05555555555556, 84.05555555555556, 84.6138888888889, 84.6138888888889, 84.2388888888889, 84.2388888888889, 84.64722222222223, 84.64722222222223, 84.65277777777777, 84.65277777777777, 85.05833333333334, 85.05833333333334, 86.39166666666667, 86.39166666666667, 86.43888888888888, 86.43888888888888, 86.75555555555556, 86.75555555555556, 86.44444444444444, 86.44444444444444, 86.95, 86.95, 87.06111111111112, 87.06111111111112, 87.18611111111112, 87.18611111111112, 87.68055555555556, 87.68055555555556, 87.17777777777778, 87.17777777777778, 87.30277777777778, 87.30277777777778, 87.925, 87.925, 88.175, 88.175, 88.30833333333334, 88.30833333333334, 88.08611111111111, 88.08611111111111, 88.14722222222223, 88.14722222222223, 88.12777777777778, 88.12777777777778, 88.175, 88.175, 88.15555555555555, 88.15555555555555, 88.42222222222222, 88.42222222222222, 88.26666666666667, 88.26666666666667, 88.34166666666667, 88.34166666666667, 88.54444444444445, 88.54444444444445, 88.70277777777778, 88.70277777777778, 88.86944444444444, 88.86944444444444, 88.69722222222222, 88.69722222222222, 88.73333333333333, 88.73333333333333, 88.86666666666666, 88.86666666666666, 88.89444444444445, 88.89444444444445, 88.89444444444445, 88.89444444444445, 88.97777777777777, 88.97777777777777, 89.17222222222222, 89.17222222222222, 88.83333333333333, 88.83333333333333, 89.14444444444445, 89.14444444444445, 89.15277777777777, 89.15277777777777, 89.11111111111111, 89.11111111111111, 89.08611111111111, 89.08611111111111, 89.25277777777778, 89.25277777777778, 89.41944444444445, 89.41944444444445, 89.48333333333333, 89.48333333333333, 89.25, 89.25, 89.0, 89.0, 89.44166666666666, 89.44166666666666, 89.43888888888888, 89.43888888888888, 89.55277777777778, 89.55277777777778, 89.375, 89.375, 89.46944444444445, 89.46944444444445, 89.60555555555555, 89.60555555555555, 89.32777777777778, 89.32777777777778, 89.51388888888889, 89.51388888888889, 89.21944444444445, 89.21944444444445, 89.28611111111111, 89.28611111111111, 89.34722222222223, 89.34722222222223, 89.6361111111111, 89.6361111111111, 89.59722222222223, 89.59722222222223, 89.36666666666666, 89.36666666666666, 89.53055555555555, 89.53055555555555, 89.64722222222223, 89.64722222222223, 89.675, 89.675, 89.75277777777778, 89.75277777777778, 89.14166666666667, 89.14166666666667, 89.49444444444444, 89.49444444444444, 89.58055555555555, 89.58055555555555, 89.47777777777777, 89.47777777777777, 89.52222222222223, 89.52222222222223, 89.69722222222222, 89.69722222222222, 89.61944444444444, 89.61944444444444, 89.84444444444445, 89.84444444444445, 89.70277777777778, 89.70277777777778, 89.38055555555556, 89.38055555555556, 89.525, 89.525, 89.63888888888889, 89.63888888888889, 89.6138888888889, 89.6138888888889, 89.8, 89.8, 89.81111111111112, 89.81111111111112, 89.775, 89.775, 89.86666666666666, 89.86666666666666, 89.81944444444444, 89.81944444444444, 89.70555555555555, 89.70555555555555, 89.70833333333333, 89.70833333333333, 89.49444444444444, 89.49444444444444, 89.51944444444445, 89.51944444444445, 89.64722222222223, 89.64722222222223, 89.7611111111111, 89.7611111111111]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.006, Test loss: 1.743, Test accuracy: 38.56
Round   1, Train loss: 1.688, Test loss: 1.540, Test accuracy: 44.75
Round   2, Train loss: 1.546, Test loss: 1.458, Test accuracy: 47.20
Round   3, Train loss: 1.440, Test loss: 1.438, Test accuracy: 48.43
Round   4, Train loss: 1.418, Test loss: 1.414, Test accuracy: 49.03
Round   5, Train loss: 1.371, Test loss: 1.412, Test accuracy: 49.65
Round   6, Train loss: 1.219, Test loss: 1.402, Test accuracy: 50.75
Round   7, Train loss: 1.232, Test loss: 1.396, Test accuracy: 51.19
Round   8, Train loss: 1.242, Test loss: 1.400, Test accuracy: 51.23
Round   9, Train loss: 1.227, Test loss: 1.384, Test accuracy: 52.00
Round  10, Train loss: 1.170, Test loss: 1.378, Test accuracy: 52.30
Round  11, Train loss: 1.218, Test loss: 1.355, Test accuracy: 53.37
Round  12, Train loss: 1.110, Test loss: 1.343, Test accuracy: 53.77
Round  13, Train loss: 1.004, Test loss: 1.352, Test accuracy: 54.31
Round  14, Train loss: 1.048, Test loss: 1.362, Test accuracy: 54.18
Round  15, Train loss: 0.935, Test loss: 1.354, Test accuracy: 54.72
Round  16, Train loss: 0.848, Test loss: 1.374, Test accuracy: 54.72
Round  17, Train loss: 0.878, Test loss: 1.387, Test accuracy: 54.88
Round  18, Train loss: 0.890, Test loss: 1.385, Test accuracy: 55.01
Round  19, Train loss: 0.913, Test loss: 1.372, Test accuracy: 55.35
Round  20, Train loss: 0.812, Test loss: 1.405, Test accuracy: 55.26
Round  21, Train loss: 0.940, Test loss: 1.417, Test accuracy: 55.44
Round  22, Train loss: 0.784, Test loss: 1.438, Test accuracy: 55.60
Round  23, Train loss: 0.744, Test loss: 1.461, Test accuracy: 55.54
Round  24, Train loss: 0.813, Test loss: 1.465, Test accuracy: 55.54
Round  25, Train loss: 0.710, Test loss: 1.508, Test accuracy: 55.33
Round  26, Train loss: 0.796, Test loss: 1.501, Test accuracy: 55.11
Round  27, Train loss: 0.686, Test loss: 1.513, Test accuracy: 55.40
Round  28, Train loss: 0.768, Test loss: 1.519, Test accuracy: 55.63
Round  29, Train loss: 0.716, Test loss: 1.517, Test accuracy: 56.15
Round  30, Train loss: 0.614, Test loss: 1.528, Test accuracy: 56.27
Round  31, Train loss: 0.712, Test loss: 1.552, Test accuracy: 55.94
Round  32, Train loss: 0.628, Test loss: 1.583, Test accuracy: 55.97
Round  33, Train loss: 0.526, Test loss: 1.585, Test accuracy: 56.26
Round  34, Train loss: 0.619, Test loss: 1.588, Test accuracy: 56.16
Round  35, Train loss: 0.636, Test loss: 1.621, Test accuracy: 56.34
Round  36, Train loss: 0.509, Test loss: 1.624, Test accuracy: 56.13
Round  37, Train loss: 0.561, Test loss: 1.642, Test accuracy: 56.25
Round  38, Train loss: 0.508, Test loss: 1.657, Test accuracy: 56.20
Round  39, Train loss: 0.487, Test loss: 1.669, Test accuracy: 56.09
Round  40, Train loss: 0.448, Test loss: 1.667, Test accuracy: 55.94
Round  41, Train loss: 0.572, Test loss: 1.676, Test accuracy: 56.13
Round  42, Train loss: 0.467, Test loss: 1.702, Test accuracy: 56.15
Round  43, Train loss: 0.450, Test loss: 1.729, Test accuracy: 55.79
Round  44, Train loss: 0.492, Test loss: 1.764, Test accuracy: 56.24
Round  45, Train loss: 0.463, Test loss: 1.785, Test accuracy: 56.06
Round  46, Train loss: 0.442, Test loss: 1.787, Test accuracy: 55.99
Round  47, Train loss: 0.468, Test loss: 1.777, Test accuracy: 56.45
Round  48, Train loss: 0.451, Test loss: 1.796, Test accuracy: 56.60
Round  49, Train loss: 0.397, Test loss: 1.785, Test accuracy: 56.72
Round  50, Train loss: 0.434, Test loss: 1.819, Test accuracy: 56.43
Round  51, Train loss: 0.403, Test loss: 1.825, Test accuracy: 56.54
Round  52, Train loss: 0.421, Test loss: 1.849, Test accuracy: 56.78
Round  53, Train loss: 0.390, Test loss: 1.863, Test accuracy: 56.95
Round  54, Train loss: 0.382, Test loss: 1.896, Test accuracy: 57.04
Round  55, Train loss: 0.345, Test loss: 1.904, Test accuracy: 57.12
Round  56, Train loss: 0.368, Test loss: 1.891, Test accuracy: 56.86
Round  57, Train loss: 0.347, Test loss: 1.918, Test accuracy: 56.68
Round  58, Train loss: 0.367, Test loss: 1.926, Test accuracy: 56.63
Round  59, Train loss: 0.320, Test loss: 1.985, Test accuracy: 56.51
Round  60, Train loss: 0.360, Test loss: 1.978, Test accuracy: 56.48
Round  61, Train loss: 0.314, Test loss: 1.994, Test accuracy: 56.52
Round  62, Train loss: 0.323, Test loss: 1.976, Test accuracy: 56.69
Round  63, Train loss: 0.329, Test loss: 1.996, Test accuracy: 56.73
Round  64, Train loss: 0.281, Test loss: 2.025, Test accuracy: 56.77
Round  65, Train loss: 0.334, Test loss: 2.046, Test accuracy: 56.76
Round  66, Train loss: 0.322, Test loss: 2.067, Test accuracy: 56.72
Round  67, Train loss: 0.326, Test loss: 2.116, Test accuracy: 56.59
Round  68, Train loss: 0.309, Test loss: 2.128, Test accuracy: 56.59
Round  69, Train loss: 0.287, Test loss: 2.121, Test accuracy: 56.69
Round  70, Train loss: 0.257, Test loss: 2.129, Test accuracy: 56.60
Round  71, Train loss: 0.263, Test loss: 2.109, Test accuracy: 56.38
Round  72, Train loss: 0.254, Test loss: 2.125, Test accuracy: 56.50
Round  73, Train loss: 0.297, Test loss: 2.103, Test accuracy: 56.91
Round  74, Train loss: 0.272, Test loss: 2.106, Test accuracy: 57.27
Round  75, Train loss: 0.259, Test loss: 2.130, Test accuracy: 57.44
Round  76, Train loss: 0.228, Test loss: 2.163, Test accuracy: 57.15
Round  77, Train loss: 0.258, Test loss: 2.167, Test accuracy: 57.06
Round  78, Train loss: 0.249, Test loss: 2.158, Test accuracy: 57.06
Round  79, Train loss: 0.258, Test loss: 2.209, Test accuracy: 56.76
Round  80, Train loss: 0.271, Test loss: 2.171, Test accuracy: 57.02
Round  81, Train loss: 0.238, Test loss: 2.224, Test accuracy: 56.92
Round  82, Train loss: 0.290, Test loss: 2.240, Test accuracy: 56.56
Round  83, Train loss: 0.260, Test loss: 2.263, Test accuracy: 56.62
Round  84, Train loss: 0.244, Test loss: 2.283, Test accuracy: 56.71
Round  85, Train loss: 0.240, Test loss: 2.320, Test accuracy: 56.99
Round  86, Train loss: 0.240, Test loss: 2.300, Test accuracy: 57.16
Round  87, Train loss: 0.202, Test loss: 2.249, Test accuracy: 56.82
Round  88, Train loss: 0.239, Test loss: 2.263, Test accuracy: 56.45
Round  89, Train loss: 0.213, Test loss: 2.274, Test accuracy: 56.81
Round  90, Train loss: 0.201, Test loss: 2.292, Test accuracy: 56.52
Round  91, Train loss: 0.197, Test loss: 2.338, Test accuracy: 56.84
Round  92, Train loss: 0.220, Test loss: 2.288, Test accuracy: 56.53
Round  93, Train loss: 0.205, Test loss: 2.342, Test accuracy: 56.51
Round  94, Train loss: 0.210, Test loss: 2.315, Test accuracy: 56.70
Round  95, Train loss: 0.200, Test loss: 2.354, Test accuracy: 56.97
Round  96, Train loss: 0.207, Test loss: 2.362, Test accuracy: 56.91
Round  97, Train loss: 0.198, Test loss: 2.323, Test accuracy: 57.26
Round  98, Train loss: 0.185, Test loss: 2.327, Test accuracy: 57.31
Round  99, Train loss: 0.178, Test loss: 2.320, Test accuracy: 57.30
Final Round, Train loss: 0.103, Test loss: 2.423, Test accuracy: 57.74
Average accuracy final 10 rounds: 56.88524999999999 

5362.567185163498
[4.860565423965454, 9.721130847930908, 14.203001022338867, 18.684871196746826, 23.184399127960205, 27.683927059173584, 32.475051403045654, 37.266175746917725, 41.813605546951294, 46.36103534698486, 50.98814034461975, 55.61524534225464, 60.17938733100891, 64.74352931976318, 69.24172806739807, 73.73992681503296, 78.3895194530487, 83.03911209106445, 87.70139241218567, 92.36367273330688, 97.07519483566284, 101.7867169380188, 106.23322200775146, 110.67972707748413, 115.2208456993103, 119.76196432113647, 124.45808815956116, 129.15421199798584, 133.75061130523682, 138.3470106124878, 142.8267331123352, 147.30645561218262, 151.83092522621155, 156.35539484024048, 161.12024521827698, 165.88509559631348, 170.48805785179138, 175.0910201072693, 179.64810347557068, 184.20518684387207, 188.73358130455017, 193.26197576522827, 198.0544879436493, 202.8470001220703, 207.65619564056396, 212.46539115905762, 216.96330451965332, 221.46121788024902, 225.8999059200287, 230.33859395980835, 234.79908561706543, 239.2595772743225, 243.906840801239, 248.55410432815552, 253.50591659545898, 258.45772886276245, 263.66803550720215, 268.87834215164185, 273.8368856906891, 278.7954292297363, 283.94070267677307, 289.0859761238098, 294.2752056121826, 299.4644351005554, 304.59518480300903, 309.72593450546265, 314.75605058670044, 319.78616666793823, 324.7211685180664, 329.6561703681946, 334.6242530345917, 339.59233570098877, 344.7084882259369, 349.824640750885, 354.8568687438965, 359.88909673690796, 364.9058835506439, 369.9226703643799, 374.88856387138367, 379.85445737838745, 385.0723605155945, 390.2902636528015, 395.20090317726135, 400.1115427017212, 404.99774622917175, 409.8839497566223, 414.8182604312897, 419.75257110595703, 424.3502161502838, 428.9478611946106, 433.78007078170776, 438.61228036880493, 443.57130694389343, 448.53033351898193, 453.4542016983032, 458.3780698776245, 463.2361283302307, 468.0941867828369, 473.08626341819763, 478.07834005355835, 483.0340983867645, 487.9898567199707, 492.89923095703125, 497.8086051940918, 502.65506982803345, 507.5015344619751, 512.2328906059265, 516.9642467498779, 521.914945602417, 526.865644454956, 531.6109821796417, 536.3563199043274, 540.9468834400177, 545.537446975708, 550.0381572246552, 554.5388674736023, 558.9192636013031, 563.2996597290039, 567.7557857036591, 572.2119116783142, 576.8984274864197, 581.5849432945251, 586.344619512558, 591.1042957305908, 595.7046995162964, 600.305103302002, 604.6727447509766, 609.0403861999512, 613.6935107707977, 618.3466353416443, 623.0527546405792, 627.7588739395142, 632.4354999065399, 637.1121258735657, 641.5431237220764, 645.9741215705872, 650.3880531787872, 654.8019847869873, 659.3516983985901, 663.9014120101929, 668.5517008304596, 673.2019896507263, 677.867968082428, 682.5339465141296, 686.912095785141, 691.2902450561523, 695.749504327774, 700.2087635993958, 704.9024522304535, 709.5961408615112, 714.3444488048553, 719.0927567481995, 723.840024471283, 728.5872921943665, 733.0407946109772, 737.4942970275879, 741.9631788730621, 746.4320607185364, 751.1321449279785, 755.8322291374207, 760.5556974411011, 765.2791657447815, 769.7543041706085, 774.2294425964355, 778.65056681633, 783.0716910362244, 787.6604781150818, 792.2492651939392, 796.9162111282349, 801.5831570625305, 806.2825272083282, 810.981897354126, 815.4897828102112, 819.9976682662964, 824.715548992157, 829.4334297180176, 834.1236734390259, 838.8139171600342, 843.579402923584, 848.3448886871338, 853.5056755542755, 858.6664624214172, 863.2883677482605, 867.9102730751038, 872.5267241001129, 877.1431751251221, 881.7642598152161, 886.3853445053101, 891.0727453231812, 895.7601461410522, 900.4707293510437, 905.1813125610352, 909.9207816123962, 914.6602506637573, 919.433197259903, 924.2061438560486, 928.7824506759644, 933.3587574958801, 938.0622057914734, 942.7656540870667, 945.071959733963, 947.3782653808594]
[38.5575, 38.5575, 44.75, 44.75, 47.1975, 47.1975, 48.4275, 48.4275, 49.03, 49.03, 49.65, 49.65, 50.7475, 50.7475, 51.1875, 51.1875, 51.225, 51.225, 52.0, 52.0, 52.2975, 52.2975, 53.3725, 53.3725, 53.775, 53.775, 54.3075, 54.3075, 54.1775, 54.1775, 54.7175, 54.7175, 54.715, 54.715, 54.88, 54.88, 55.0075, 55.0075, 55.355, 55.355, 55.2625, 55.2625, 55.4375, 55.4375, 55.6, 55.6, 55.5375, 55.5375, 55.54, 55.54, 55.33, 55.33, 55.11, 55.11, 55.4, 55.4, 55.6325, 55.6325, 56.1475, 56.1475, 56.2675, 56.2675, 55.9375, 55.9375, 55.97, 55.97, 56.255, 56.255, 56.155, 56.155, 56.3425, 56.3425, 56.135, 56.135, 56.25, 56.25, 56.2025, 56.2025, 56.085, 56.085, 55.94, 55.94, 56.1275, 56.1275, 56.145, 56.145, 55.79, 55.79, 56.2375, 56.2375, 56.0625, 56.0625, 55.9925, 55.9925, 56.4475, 56.4475, 56.6025, 56.6025, 56.715, 56.715, 56.4275, 56.4275, 56.54, 56.54, 56.785, 56.785, 56.945, 56.945, 57.0425, 57.0425, 57.1225, 57.1225, 56.8625, 56.8625, 56.6825, 56.6825, 56.6325, 56.6325, 56.505, 56.505, 56.475, 56.475, 56.5175, 56.5175, 56.6875, 56.6875, 56.735, 56.735, 56.7675, 56.7675, 56.7575, 56.7575, 56.715, 56.715, 56.585, 56.585, 56.5875, 56.5875, 56.69, 56.69, 56.6025, 56.6025, 56.3825, 56.3825, 56.5, 56.5, 56.9125, 56.9125, 57.2725, 57.2725, 57.44, 57.44, 57.1475, 57.1475, 57.0625, 57.0625, 57.06, 57.06, 56.7625, 56.7625, 57.025, 57.025, 56.92, 56.92, 56.56, 56.56, 56.62, 56.62, 56.7125, 56.7125, 56.995, 56.995, 57.1575, 57.1575, 56.8175, 56.8175, 56.4525, 56.4525, 56.815, 56.815, 56.52, 56.52, 56.835, 56.835, 56.5325, 56.5325, 56.5125, 56.5125, 56.7025, 56.7025, 56.965, 56.965, 56.9075, 56.9075, 57.2575, 57.2575, 57.315, 57.315, 57.305, 57.305, 57.74, 57.74]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 1.440, Test loss: 1.865, Test accuracy: 33.30
Round   1, Train loss: 1.265, Test loss: 1.833, Test accuracy: 31.13
Round   2, Train loss: 1.160, Test loss: 1.848, Test accuracy: 30.96
Round   3, Train loss: 1.132, Test loss: 1.949, Test accuracy: 26.98
Round   4, Train loss: 1.061, Test loss: 2.018, Test accuracy: 24.35
Round   5, Train loss: 1.002, Test loss: 2.146, Test accuracy: 20.09
Round   6, Train loss: 0.924, Test loss: 2.146, Test accuracy: 19.41
Round   7, Train loss: 0.908, Test loss: 2.138, Test accuracy: 18.80
Round   8, Train loss: 0.854, Test loss: 2.140, Test accuracy: 18.99
Round   9, Train loss: 0.799, Test loss: 2.127, Test accuracy: 20.16
Round  10, Train loss: 0.814, Test loss: 2.125, Test accuracy: 19.61
Round  11, Train loss: 0.730, Test loss: 2.113, Test accuracy: 20.09
Round  12, Train loss: 0.734, Test loss: 2.106, Test accuracy: 20.56
Round  13, Train loss: 0.694, Test loss: 2.097, Test accuracy: 21.13
Round  14, Train loss: 0.670, Test loss: 2.100, Test accuracy: 20.29
Round  15, Train loss: 0.592, Test loss: 2.098, Test accuracy: 20.55
Round  16, Train loss: 0.618, Test loss: 2.083, Test accuracy: 21.65
Round  17, Train loss: 0.552, Test loss: 2.086, Test accuracy: 22.02
Round  18, Train loss: 0.583, Test loss: 2.077, Test accuracy: 22.26
Round  19, Train loss: 0.597, Test loss: 2.068, Test accuracy: 22.90
Round  20, Train loss: 0.527, Test loss: 2.057, Test accuracy: 23.18
Round  21, Train loss: 0.533, Test loss: 2.052, Test accuracy: 23.51
Round  22, Train loss: 0.510, Test loss: 2.044, Test accuracy: 23.31
Round  23, Train loss: 0.458, Test loss: 2.037, Test accuracy: 24.26
Round  24, Train loss: 0.479, Test loss: 2.031, Test accuracy: 25.12
Round  25, Train loss: 0.435, Test loss: 2.019, Test accuracy: 26.26
Round  26, Train loss: 0.427, Test loss: 2.017, Test accuracy: 25.48
Round  27, Train loss: 0.417, Test loss: 2.011, Test accuracy: 26.46
Round  28, Train loss: 0.401, Test loss: 2.008, Test accuracy: 26.18
Round  29, Train loss: 0.424, Test loss: 1.999, Test accuracy: 27.10
Round  30, Train loss: 0.391, Test loss: 1.997, Test accuracy: 27.34
Round  31, Train loss: 0.399, Test loss: 1.987, Test accuracy: 28.25
Round  32, Train loss: 0.331, Test loss: 1.968, Test accuracy: 29.71
Round  33, Train loss: 0.370, Test loss: 1.964, Test accuracy: 29.87
Round  34, Train loss: 0.336, Test loss: 1.958, Test accuracy: 30.07
Round  35, Train loss: 0.367, Test loss: 1.951, Test accuracy: 30.66
Round  36, Train loss: 0.303, Test loss: 1.934, Test accuracy: 32.20
Round  37, Train loss: 0.308, Test loss: 1.930, Test accuracy: 31.84
Round  38, Train loss: 0.346, Test loss: 1.922, Test accuracy: 32.99
Round  39, Train loss: 0.290, Test loss: 1.916, Test accuracy: 33.42
Round  40, Train loss: 0.316, Test loss: 1.911, Test accuracy: 33.70
Round  41, Train loss: 0.288, Test loss: 1.912, Test accuracy: 33.73
Round  42, Train loss: 0.308, Test loss: 1.895, Test accuracy: 34.90
Round  43, Train loss: 0.271, Test loss: 1.887, Test accuracy: 35.11
Round  44, Train loss: 0.266, Test loss: 1.887, Test accuracy: 34.83
Round  45, Train loss: 0.265, Test loss: 1.884, Test accuracy: 35.08
Round  46, Train loss: 0.242, Test loss: 1.870, Test accuracy: 35.52
Round  47, Train loss: 0.270, Test loss: 1.870, Test accuracy: 35.89
Round  48, Train loss: 0.227, Test loss: 1.869, Test accuracy: 35.25
Round  49, Train loss: 0.250, Test loss: 1.861, Test accuracy: 36.51
Round  50, Train loss: 0.245, Test loss: 1.856, Test accuracy: 36.59
Round  51, Train loss: 0.239, Test loss: 1.855, Test accuracy: 36.96
Round  52, Train loss: 0.210, Test loss: 1.852, Test accuracy: 36.82
Round  53, Train loss: 0.230, Test loss: 1.835, Test accuracy: 37.92
Round  54, Train loss: 0.211, Test loss: 1.833, Test accuracy: 38.01
Round  55, Train loss: 0.196, Test loss: 1.835, Test accuracy: 37.58
Round  56, Train loss: 0.199, Test loss: 1.834, Test accuracy: 37.64
Round  57, Train loss: 0.213, Test loss: 1.825, Test accuracy: 37.78
Round  58, Train loss: 0.206, Test loss: 1.827, Test accuracy: 38.05
Round  59, Train loss: 0.232, Test loss: 1.824, Test accuracy: 38.22
Round  60, Train loss: 0.198, Test loss: 1.810, Test accuracy: 38.26
Round  61, Train loss: 0.181, Test loss: 1.805, Test accuracy: 38.36
Round  62, Train loss: 0.209, Test loss: 1.806, Test accuracy: 38.53
Round  63, Train loss: 0.209, Test loss: 1.804, Test accuracy: 38.58
Round  64, Train loss: 0.171, Test loss: 1.790, Test accuracy: 39.56
Round  65, Train loss: 0.188, Test loss: 1.791, Test accuracy: 39.50
Round  66, Train loss: 0.181, Test loss: 1.781, Test accuracy: 40.86
Round  67, Train loss: 0.200, Test loss: 1.802, Test accuracy: 39.28
Round  68, Train loss: 0.190, Test loss: 1.791, Test accuracy: 39.65
Round  69, Train loss: 0.179, Test loss: 1.776, Test accuracy: 40.55
Round  70, Train loss: 0.186, Test loss: 1.784, Test accuracy: 39.50
Round  71, Train loss: 0.174, Test loss: 1.772, Test accuracy: 40.04
Round  72, Train loss: 0.176, Test loss: 1.770, Test accuracy: 39.98
Round  73, Train loss: 0.165, Test loss: 1.763, Test accuracy: 40.26
Round  74, Train loss: 0.162, Test loss: 1.756, Test accuracy: 41.00
Round  75, Train loss: 0.165, Test loss: 1.757, Test accuracy: 41.17
Round  76, Train loss: 0.166, Test loss: 1.760, Test accuracy: 40.48
Round  77, Train loss: 0.157, Test loss: 1.745, Test accuracy: 41.33
Round  78, Train loss: 0.155, Test loss: 1.727, Test accuracy: 42.20
Round  79, Train loss: 0.164, Test loss: 1.735, Test accuracy: 41.95
Round  80, Train loss: 0.153, Test loss: 1.724, Test accuracy: 42.42
Round  81, Train loss: 0.150, Test loss: 1.726, Test accuracy: 42.63
Round  82, Train loss: 0.159, Test loss: 1.731, Test accuracy: 42.20
Round  83, Train loss: 0.154, Test loss: 1.725, Test accuracy: 41.76
Round  84, Train loss: 0.164, Test loss: 1.734, Test accuracy: 41.72
Round  85, Train loss: 0.155, Test loss: 1.725, Test accuracy: 41.94
Round  86, Train loss: 0.153, Test loss: 1.729, Test accuracy: 41.77
Round  87, Train loss: 0.138, Test loss: 1.718, Test accuracy: 42.26
Round  88, Train loss: 0.140, Test loss: 1.709, Test accuracy: 42.61
Round  89, Train loss: 0.141, Test loss: 1.706, Test accuracy: 42.54
Round  90, Train loss: 0.152, Test loss: 1.713, Test accuracy: 42.38
Round  91, Train loss: 0.143, Test loss: 1.715, Test accuracy: 42.48
Round  92, Train loss: 0.143, Test loss: 1.703, Test accuracy: 42.94
Round  93, Train loss: 0.137, Test loss: 1.702, Test accuracy: 42.59
Round  94, Train loss: 0.142, Test loss: 1.703, Test accuracy: 42.42
Round  95, Train loss: 0.138, Test loss: 1.706, Test accuracy: 42.48
Round  96, Train loss: 0.145, Test loss: 1.714, Test accuracy: 41.96
Round  97, Train loss: 0.141, Test loss: 1.702, Test accuracy: 42.17
Round  98, Train loss: 0.143, Test loss: 1.688, Test accuracy: 42.83
Round  99, Train loss: 0.137, Test loss: 1.691, Test accuracy: 42.70
Final Round, Train loss: 0.135, Test loss: 1.680, Test accuracy: 43.62
Average accuracy final 10 rounds: 42.49375
6873.4615433216095
[]
[33.3, 31.13, 30.9575, 26.9775, 24.35, 20.0875, 19.405, 18.7975, 18.9875, 20.165, 19.6075, 20.095, 20.5625, 21.1325, 20.2925, 20.5475, 21.65, 22.025, 22.2575, 22.8975, 23.1825, 23.5125, 23.3125, 24.26, 25.1175, 26.2625, 25.4775, 26.465, 26.18, 27.0975, 27.345, 28.25, 29.7075, 29.8725, 30.0675, 30.66, 32.1975, 31.8425, 32.99, 33.42, 33.6975, 33.735, 34.9025, 35.1075, 34.8275, 35.075, 35.5175, 35.8925, 35.25, 36.5075, 36.595, 36.9575, 36.8225, 37.9175, 38.0075, 37.58, 37.6375, 37.7775, 38.055, 38.215, 38.255, 38.36, 38.53, 38.5825, 39.565, 39.5, 40.8575, 39.285, 39.6475, 40.555, 39.5, 40.0425, 39.9825, 40.255, 41.0025, 41.17, 40.4825, 41.325, 42.1975, 41.95, 42.425, 42.6275, 42.195, 41.7575, 41.715, 41.9425, 41.77, 42.255, 42.6125, 42.54, 42.375, 42.485, 42.935, 42.585, 42.42, 42.48, 41.96, 42.17, 42.83, 42.6975, 43.62]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.176, Test loss: 1.907, Test accuracy: 17.48
Round   1, Train loss: 1.116, Test loss: 1.597, Test accuracy: 28.90
Round   2, Train loss: 0.869, Test loss: 1.437, Test accuracy: 35.47
Round   3, Train loss: 1.088, Test loss: 1.308, Test accuracy: 41.72
Round   4, Train loss: 0.679, Test loss: 1.240, Test accuracy: 43.42
Round   5, Train loss: 0.812, Test loss: 1.074, Test accuracy: 50.97
Round   6, Train loss: 0.392, Test loss: 0.996, Test accuracy: 54.22
Round   7, Train loss: 0.510, Test loss: 0.944, Test accuracy: 54.40
Round   8, Train loss: 0.442, Test loss: 0.949, Test accuracy: 55.12
Round   9, Train loss: 0.060, Test loss: 0.931, Test accuracy: 56.15
Round  10, Train loss: 0.020, Test loss: 0.913, Test accuracy: 56.53
Round  11, Train loss: 0.117, Test loss: 0.908, Test accuracy: 57.48
Round  12, Train loss: -0.117, Test loss: 0.912, Test accuracy: 58.75
Round  13, Train loss: -0.428, Test loss: 0.891, Test accuracy: 58.90
Round  14, Train loss: -0.393, Test loss: 0.862, Test accuracy: 60.58
Round  15, Train loss: -0.165, Test loss: 0.859, Test accuracy: 60.23
Round  16, Train loss: -0.448, Test loss: 0.845, Test accuracy: 60.67
Round  17, Train loss: -0.844, Test loss: 0.836, Test accuracy: 61.58
Round  18, Train loss: -0.972, Test loss: 0.862, Test accuracy: 60.12
Round  19, Train loss: -0.307, Test loss: 0.857, Test accuracy: 60.40
Round  20, Train loss: -0.809, Test loss: 0.822, Test accuracy: 62.48
Round  21, Train loss: -0.642, Test loss: 0.811, Test accuracy: 62.95
Round  22, Train loss: -1.049, Test loss: 0.831, Test accuracy: 61.65
Round  23, Train loss: -1.190, Test loss: 0.814, Test accuracy: 62.90
Round  24, Train loss: -1.207, Test loss: 0.803, Test accuracy: 63.10
Round  25, Train loss: -1.727, Test loss: 0.797, Test accuracy: 63.78
Round  26, Train loss: -1.335, Test loss: 0.809, Test accuracy: 63.13
Round  27, Train loss: -1.173, Test loss: 0.782, Test accuracy: 64.65
Round  28, Train loss: -1.192, Test loss: 0.777, Test accuracy: 65.25
Round  29, Train loss: -1.577, Test loss: 0.790, Test accuracy: 65.48
Round  30, Train loss: -1.325, Test loss: 0.762, Test accuracy: 66.82
Round  31, Train loss: -1.751, Test loss: 0.779, Test accuracy: 65.80
Round  32, Train loss: -1.219, Test loss: 0.761, Test accuracy: 66.85
Round  33, Train loss: -1.935, Test loss: 0.759, Test accuracy: 67.37
Round  34, Train loss: -1.943, Test loss: 0.747, Test accuracy: 67.43
Round  35, Train loss: -2.190, Test loss: 0.752, Test accuracy: 67.40
Round  36, Train loss: -1.735, Test loss: 0.769, Test accuracy: 66.63
Round  37, Train loss: -1.951, Test loss: 0.760, Test accuracy: 67.07
Round  38, Train loss: -1.851, Test loss: 0.742, Test accuracy: 67.48
Round  39, Train loss: -1.764, Test loss: 0.745, Test accuracy: 66.72
Round  40, Train loss: -1.916, Test loss: 0.756, Test accuracy: 66.67
Round  41, Train loss: -2.394, Test loss: 0.754, Test accuracy: 66.93
Round  42, Train loss: -2.496, Test loss: 0.746, Test accuracy: 67.77
Round  43, Train loss: -2.426, Test loss: 0.723, Test accuracy: 69.43
Round  44, Train loss: -2.661, Test loss: 0.715, Test accuracy: 69.32
Round  45, Train loss: -2.962, Test loss: 0.722, Test accuracy: 69.48
Round  46, Train loss: -2.836, Test loss: 0.733, Test accuracy: 69.45
Round  47, Train loss: -2.991, Test loss: 0.733, Test accuracy: 68.92
Round  48, Train loss: -2.269, Test loss: 0.732, Test accuracy: 69.25
Round  49, Train loss: -2.568, Test loss: 0.740, Test accuracy: 69.02
Round  50, Train loss: -3.110, Test loss: 0.746, Test accuracy: 69.20
Round  51, Train loss: -2.569, Test loss: 0.752, Test accuracy: 69.25
Round  52, Train loss: -2.577, Test loss: 0.749, Test accuracy: 68.92
Round  53, Train loss: -2.414, Test loss: 0.745, Test accuracy: 68.92
Round  54, Train loss: -2.935, Test loss: 0.729, Test accuracy: 69.65
Round  55, Train loss: -2.923, Test loss: 0.760, Test accuracy: 69.38
Round  56, Train loss: -3.255, Test loss: 0.731, Test accuracy: 70.98
Round  57, Train loss: -2.529, Test loss: 0.724, Test accuracy: 70.63
Round  58, Train loss: -3.294, Test loss: 0.726, Test accuracy: 70.03
Round  59, Train loss: -3.136, Test loss: 0.729, Test accuracy: 69.72
Round  60, Train loss: -2.764, Test loss: 0.736, Test accuracy: 69.03
Round  61, Train loss: -3.637, Test loss: 0.722, Test accuracy: 70.07
Round  62, Train loss: -3.112, Test loss: 0.731, Test accuracy: 69.97
Round  63, Train loss: -2.222, Test loss: 0.743, Test accuracy: 69.02
Round  64, Train loss: -3.067, Test loss: 0.732, Test accuracy: 69.10
Round  65, Train loss: -3.716, Test loss: 0.726, Test accuracy: 69.57
Round  66, Train loss: -3.190, Test loss: 0.714, Test accuracy: 70.33
Round  67, Train loss: -3.374, Test loss: 0.718, Test accuracy: 69.90
Round  68, Train loss: -3.475, Test loss: 0.711, Test accuracy: 70.47
Round  69, Train loss: -3.828, Test loss: 0.700, Test accuracy: 71.52
Round  70, Train loss: -3.648, Test loss: 0.698, Test accuracy: 71.88
Round  71, Train loss: -3.570, Test loss: 0.697, Test accuracy: 71.75
Round  72, Train loss: -3.832, Test loss: 0.705, Test accuracy: 70.87
Round  73, Train loss: -3.684, Test loss: 0.732, Test accuracy: 69.85
Round  74, Train loss: -3.947, Test loss: 0.735, Test accuracy: 70.43
Round  75, Train loss: -3.536, Test loss: 0.712, Test accuracy: 70.90
Round  76, Train loss: -4.272, Test loss: 0.702, Test accuracy: 71.08
Round  77, Train loss: -3.366, Test loss: 0.715, Test accuracy: 71.07
Round  78, Train loss: -4.057, Test loss: 0.718, Test accuracy: 70.83
Round  79, Train loss: -4.097, Test loss: 0.733, Test accuracy: 70.08
Round  80, Train loss: -3.895, Test loss: 0.728, Test accuracy: 70.03
Round  81, Train loss: -3.655, Test loss: 0.722, Test accuracy: 70.70
Round  82, Train loss: -3.865, Test loss: 0.723, Test accuracy: 70.98
Round  83, Train loss: -3.972, Test loss: 0.729, Test accuracy: 71.03
Round  84, Train loss: -3.971, Test loss: 0.719, Test accuracy: 71.13
Round  85, Train loss: -4.259, Test loss: 0.719, Test accuracy: 71.68
Round  86, Train loss: -4.779, Test loss: 0.713, Test accuracy: 71.67
Round  87, Train loss: -3.521, Test loss: 0.720, Test accuracy: 71.62
Round  88, Train loss: -4.145, Test loss: 0.707, Test accuracy: 72.12
Round  89, Train loss: -3.936, Test loss: 0.718, Test accuracy: 72.22
Round  90, Train loss: -3.804, Test loss: 0.721, Test accuracy: 71.95
Round  91, Train loss: -4.396, Test loss: 0.711, Test accuracy: 72.00
Round  92, Train loss: -3.698, Test loss: 0.710, Test accuracy: 71.90
Round  93, Train loss: -3.730, Test loss: 0.721, Test accuracy: 71.85
Round  94, Train loss: -4.183, Test loss: 0.724, Test accuracy: 71.75
Round  95, Train loss: -4.331, Test loss: 0.724, Test accuracy: 72.32
Round  96, Train loss: -3.838, Test loss: 0.711, Test accuracy: 72.70
Round  97, Train loss: -3.513, Test loss: 0.710, Test accuracy: 72.70
Round  98, Train loss: -4.279, Test loss: 0.730, Test accuracy: 72.28
Round  99, Train loss: -4.344, Test loss: 0.710, Test accuracy: 73.03
Final Round, Train loss: 0.681, Test loss: 0.671, Test accuracy: 71.08
Average accuracy final 10 rounds: 72.24833333333333
Average global accuracy final 10 rounds: 72.24833333333333
746.1851563453674
[]
[17.483333333333334, 28.9, 35.46666666666667, 41.71666666666667, 43.416666666666664, 50.96666666666667, 54.21666666666667, 54.4, 55.11666666666667, 56.15, 56.53333333333333, 57.483333333333334, 58.75, 58.9, 60.583333333333336, 60.233333333333334, 60.666666666666664, 61.583333333333336, 60.11666666666667, 60.4, 62.483333333333334, 62.95, 61.65, 62.9, 63.1, 63.78333333333333, 63.13333333333333, 64.65, 65.25, 65.48333333333333, 66.81666666666666, 65.8, 66.85, 67.36666666666666, 67.43333333333334, 67.4, 66.63333333333334, 67.06666666666666, 67.48333333333333, 66.71666666666667, 66.66666666666667, 66.93333333333334, 67.76666666666667, 69.43333333333334, 69.31666666666666, 69.48333333333333, 69.45, 68.91666666666667, 69.25, 69.01666666666667, 69.2, 69.25, 68.91666666666667, 68.91666666666667, 69.65, 69.38333333333334, 70.98333333333333, 70.63333333333334, 70.03333333333333, 69.71666666666667, 69.03333333333333, 70.06666666666666, 69.96666666666667, 69.01666666666667, 69.1, 69.56666666666666, 70.33333333333333, 69.9, 70.46666666666667, 71.51666666666667, 71.88333333333334, 71.75, 70.86666666666666, 69.85, 70.43333333333334, 70.9, 71.08333333333333, 71.06666666666666, 70.83333333333333, 70.08333333333333, 70.03333333333333, 70.7, 70.98333333333333, 71.03333333333333, 71.13333333333334, 71.68333333333334, 71.66666666666667, 71.61666666666666, 72.11666666666666, 72.21666666666667, 71.95, 72.0, 71.9, 71.85, 71.75, 72.31666666666666, 72.7, 72.7, 72.28333333333333, 73.03333333333333, 71.08333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.213, Test loss: 2.390, Test accuracy: 23.75
Round   1, Train loss: 0.958, Test loss: 2.089, Test accuracy: 25.33
Round   2, Train loss: 0.944, Test loss: 2.189, Test accuracy: 22.87
Round   3, Train loss: 0.826, Test loss: 1.966, Test accuracy: 28.43
Round   4, Train loss: 0.795, Test loss: 2.248, Test accuracy: 26.87
Round   5, Train loss: 0.899, Test loss: 1.939, Test accuracy: 26.40
Round   6, Train loss: 0.769, Test loss: 2.152, Test accuracy: 28.88
Round   7, Train loss: 0.753, Test loss: 1.803, Test accuracy: 35.83
Round   8, Train loss: 0.825, Test loss: 2.286, Test accuracy: 28.65
Round   9, Train loss: 0.783, Test loss: 1.922, Test accuracy: 32.92
Round  10, Train loss: 0.732, Test loss: 1.970, Test accuracy: 35.38
Round  11, Train loss: 0.681, Test loss: 1.813, Test accuracy: 33.07
Round  12, Train loss: 0.732, Test loss: 1.759, Test accuracy: 34.83
Round  13, Train loss: 0.739, Test loss: 1.634, Test accuracy: 40.03
Round  14, Train loss: 0.672, Test loss: 1.760, Test accuracy: 39.73
Round  15, Train loss: 0.606, Test loss: 1.760, Test accuracy: 36.52
Round  16, Train loss: 0.732, Test loss: 2.328, Test accuracy: 23.28
Round  17, Train loss: 0.724, Test loss: 1.975, Test accuracy: 33.33
Round  18, Train loss: 0.680, Test loss: 1.928, Test accuracy: 38.37
Round  19, Train loss: 0.667, Test loss: 1.874, Test accuracy: 35.78
Round  20, Train loss: 0.673, Test loss: 2.282, Test accuracy: 31.68
Round  21, Train loss: 0.662, Test loss: 1.978, Test accuracy: 38.13
Round  22, Train loss: 0.623, Test loss: 1.693, Test accuracy: 40.60
Round  23, Train loss: 0.565, Test loss: 1.566, Test accuracy: 42.40
Round  24, Train loss: 0.649, Test loss: 1.635, Test accuracy: 43.22
Round  25, Train loss: 0.539, Test loss: 1.583, Test accuracy: 44.08
Round  26, Train loss: 0.579, Test loss: 1.668, Test accuracy: 42.77
Round  27, Train loss: 0.576, Test loss: 1.614, Test accuracy: 42.18
Round  28, Train loss: 0.530, Test loss: 1.571, Test accuracy: 42.20
Round  29, Train loss: 0.556, Test loss: 1.827, Test accuracy: 42.10
Round  30, Train loss: 0.500, Test loss: 1.516, Test accuracy: 45.95
Round  31, Train loss: 0.519, Test loss: 1.493, Test accuracy: 46.80
Round  32, Train loss: 0.495, Test loss: 1.594, Test accuracy: 44.82
Round  33, Train loss: 0.484, Test loss: 1.693, Test accuracy: 45.53
Round  34, Train loss: 0.468, Test loss: 1.768, Test accuracy: 45.45
Round  35, Train loss: 0.508, Test loss: 1.645, Test accuracy: 45.37
Round  36, Train loss: 0.396, Test loss: 1.732, Test accuracy: 45.73
Round  37, Train loss: 0.512, Test loss: 1.521, Test accuracy: 48.08
Round  38, Train loss: 0.462, Test loss: 1.783, Test accuracy: 42.37
Round  39, Train loss: 0.442, Test loss: 1.374, Test accuracy: 50.28
Round  40, Train loss: 0.524, Test loss: 1.512, Test accuracy: 47.80
Round  41, Train loss: 0.427, Test loss: 1.407, Test accuracy: 50.43
Round  42, Train loss: 0.425, Test loss: 1.540, Test accuracy: 48.57
Round  43, Train loss: 0.362, Test loss: 1.578, Test accuracy: 47.98
Round  44, Train loss: 0.376, Test loss: 1.823, Test accuracy: 43.93
Round  45, Train loss: 0.560, Test loss: 1.561, Test accuracy: 44.48
Round  46, Train loss: 0.402, Test loss: 1.539, Test accuracy: 49.07
Round  47, Train loss: 0.442, Test loss: 1.454, Test accuracy: 49.78
Round  48, Train loss: 0.410, Test loss: 1.412, Test accuracy: 52.57
Round  49, Train loss: 0.427, Test loss: 1.412, Test accuracy: 52.15
Round  50, Train loss: 0.409, Test loss: 1.437, Test accuracy: 50.82
Round  51, Train loss: 0.381, Test loss: 1.555, Test accuracy: 49.93
Round  52, Train loss: 0.416, Test loss: 1.477, Test accuracy: 49.63
Round  53, Train loss: 0.366, Test loss: 1.394, Test accuracy: 51.83
Round  54, Train loss: 0.310, Test loss: 2.406, Test accuracy: 35.90
Round  55, Train loss: 0.298, Test loss: 1.585, Test accuracy: 47.22
Round  56, Train loss: 0.343, Test loss: 1.387, Test accuracy: 52.57
Round  57, Train loss: 0.347, Test loss: 1.420, Test accuracy: 51.42
Round  58, Train loss: 0.293, Test loss: 1.507, Test accuracy: 50.90
Round  59, Train loss: 0.317, Test loss: 1.362, Test accuracy: 54.48
Round  60, Train loss: 0.402, Test loss: 1.778, Test accuracy: 43.52
Round  61, Train loss: 0.289, Test loss: 1.537, Test accuracy: 50.37
Round  62, Train loss: 0.394, Test loss: 1.333, Test accuracy: 54.85
Round  63, Train loss: 0.369, Test loss: 1.310, Test accuracy: 53.80
Round  64, Train loss: 0.325, Test loss: 1.526, Test accuracy: 50.85
Round  65, Train loss: 0.322, Test loss: 1.680, Test accuracy: 47.80
Round  66, Train loss: 0.306, Test loss: 1.513, Test accuracy: 51.00
Round  67, Train loss: 0.293, Test loss: 1.399, Test accuracy: 55.12
Round  68, Train loss: 0.280, Test loss: 1.892, Test accuracy: 46.53
Round  69, Train loss: 0.326, Test loss: 1.255, Test accuracy: 56.67
Round  70, Train loss: 0.269, Test loss: 1.520, Test accuracy: 52.47
Round  71, Train loss: 0.343, Test loss: 1.390, Test accuracy: 52.47
Round  72, Train loss: 0.309, Test loss: 1.673, Test accuracy: 47.07
Round  73, Train loss: 0.349, Test loss: 1.384, Test accuracy: 53.12
Round  74, Train loss: 0.290, Test loss: 1.337, Test accuracy: 55.87
Round  75, Train loss: 0.300, Test loss: 1.426, Test accuracy: 53.72
Round  76, Train loss: 0.238, Test loss: 1.265, Test accuracy: 58.53
Round  77, Train loss: 0.360, Test loss: 1.514, Test accuracy: 50.67
Round  78, Train loss: 0.289, Test loss: 1.852, Test accuracy: 46.78
Round  79, Train loss: 0.249, Test loss: 1.269, Test accuracy: 57.70
Round  80, Train loss: 0.314, Test loss: 1.549, Test accuracy: 50.42
Round  81, Train loss: 0.336, Test loss: 1.521, Test accuracy: 50.92
Round  82, Train loss: 0.303, Test loss: 1.275, Test accuracy: 56.23
Round  83, Train loss: 0.238, Test loss: 1.426, Test accuracy: 53.00
Round  84, Train loss: 0.209, Test loss: 1.321, Test accuracy: 56.58
Round  85, Train loss: 0.291, Test loss: 1.409, Test accuracy: 52.95
Round  86, Train loss: 0.273, Test loss: 1.388, Test accuracy: 53.47
Round  87, Train loss: 0.317, Test loss: 1.399, Test accuracy: 54.48
Round  88, Train loss: 0.277, Test loss: 1.811, Test accuracy: 45.48
Round  89, Train loss: 0.225, Test loss: 1.510, Test accuracy: 54.02
Round  90, Train loss: 0.192, Test loss: 1.664, Test accuracy: 50.33
Round  91, Train loss: 0.243, Test loss: 1.506, Test accuracy: 52.23
Round  92, Train loss: 0.254, Test loss: 1.644, Test accuracy: 50.47
Round  93, Train loss: 0.253, Test loss: 1.427, Test accuracy: 57.22
Round  94, Train loss: 0.184, Test loss: 1.522, Test accuracy: 54.02
Round  95, Train loss: 0.250, Test loss: 1.454, Test accuracy: 54.32
Round  96, Train loss: 0.206, Test loss: 1.955, Test accuracy: 47.22
Round  97, Train loss: 0.188, Test loss: 1.507, Test accuracy: 54.85
Round  98, Train loss: 0.247, Test loss: 1.639, Test accuracy: 51.43
Round  99, Train loss: 0.203, Test loss: 1.256, Test accuracy: 59.03
Final Round, Train loss: 0.204, Test loss: 1.208, Test accuracy: 60.80
Average accuracy final 10 rounds: 53.111666666666665
1392.0237593650818
[2.4365084171295166, 4.52963399887085, 6.550595760345459, 8.635818004608154, 10.735658884048462, 12.808356761932373, 14.858612775802612, 16.759012699127197, 18.675625562667847, 20.58709478378296, 22.52396321296692, 24.413260221481323, 26.27879023551941, 28.169434547424316, 30.12745451927185, 32.11804986000061, 33.99179744720459, 35.88356256484985, 37.78583312034607, 39.857239961624146, 41.93583536148071, 43.806196212768555, 45.705750465393066, 47.584904193878174, 49.52273607254028, 51.431424140930176, 53.29389500617981, 55.17015528678894, 57.05430293083191, 59.01727557182312, 60.980024576187134, 62.929012298583984, 64.77207899093628, 66.67862153053284, 68.61911082267761, 70.56543946266174, 72.45945143699646, 74.32846355438232, 76.2424635887146, 78.19607329368591, 80.1458044052124, 82.081387758255, 83.97770428657532, 85.85935425758362, 87.81948661804199, 89.76434588432312, 91.71049332618713, 93.59440088272095, 95.50434970855713, 97.45633292198181, 99.44284510612488, 101.38773226737976, 103.24740886688232, 105.15116238594055, 107.11475658416748, 109.06502318382263, 110.97800993919373, 112.88690781593323, 114.75013613700867, 116.67288422584534, 118.63243532180786, 120.59346747398376, 122.48032426834106, 124.34926986694336, 126.27001118659973, 128.21193957328796, 130.16257786750793, 132.02735376358032, 133.91163086891174, 135.96223855018616, 138.08163785934448, 140.17394018173218, 142.24701523780823, 144.2971544265747, 146.40204095840454, 148.53019547462463, 150.65750169754028, 152.75012016296387, 154.77646398544312, 156.87730026245117, 158.95643830299377, 160.87385606765747, 162.76014637947083, 164.67686700820923, 166.58252620697021, 168.4884054660797, 170.38826608657837, 172.28011775016785, 174.184672832489, 176.07193851470947, 177.9992163181305, 179.8969624042511, 181.75356125831604, 183.6419563293457, 185.54375624656677, 187.46745562553406, 189.3834900856018, 191.2668480873108, 193.14726614952087, 195.05961179733276, 196.98083233833313]
[23.75, 25.333333333333332, 22.866666666666667, 28.433333333333334, 26.866666666666667, 26.4, 28.883333333333333, 35.833333333333336, 28.65, 32.916666666666664, 35.38333333333333, 33.06666666666667, 34.833333333333336, 40.03333333333333, 39.733333333333334, 36.516666666666666, 23.283333333333335, 33.333333333333336, 38.36666666666667, 35.78333333333333, 31.683333333333334, 38.13333333333333, 40.6, 42.4, 43.21666666666667, 44.083333333333336, 42.766666666666666, 42.18333333333333, 42.2, 42.1, 45.95, 46.8, 44.81666666666667, 45.53333333333333, 45.45, 45.36666666666667, 45.733333333333334, 48.083333333333336, 42.36666666666667, 50.28333333333333, 47.8, 50.43333333333333, 48.56666666666667, 47.983333333333334, 43.93333333333333, 44.483333333333334, 49.06666666666667, 49.78333333333333, 52.56666666666667, 52.15, 50.81666666666667, 49.93333333333333, 49.63333333333333, 51.833333333333336, 35.9, 47.21666666666667, 52.56666666666667, 51.416666666666664, 50.9, 54.483333333333334, 43.516666666666666, 50.36666666666667, 54.85, 53.8, 50.85, 47.8, 51.0, 55.11666666666667, 46.53333333333333, 56.666666666666664, 52.46666666666667, 52.46666666666667, 47.06666666666667, 53.11666666666667, 55.86666666666667, 53.71666666666667, 58.53333333333333, 50.666666666666664, 46.78333333333333, 57.7, 50.416666666666664, 50.916666666666664, 56.233333333333334, 53.0, 56.583333333333336, 52.95, 53.46666666666667, 54.483333333333334, 45.483333333333334, 54.016666666666666, 50.333333333333336, 52.233333333333334, 50.46666666666667, 57.21666666666667, 54.016666666666666, 54.31666666666667, 47.21666666666667, 54.85, 51.43333333333333, 59.03333333333333, 60.8]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.299, Test loss: 2.313, Test accuracy: 7.25
Round   0, Global train loss: 2.299, Global test loss: 2.315, Global test accuracy: 7.02
Round   1, Train loss: 2.281, Test loss: 2.310, Test accuracy: 7.93
Round   1, Global train loss: 2.281, Global test loss: 2.315, Global test accuracy: 7.00
Round   2, Train loss: 2.254, Test loss: 2.310, Test accuracy: 8.82
Round   2, Global train loss: 2.254, Global test loss: 2.317, Global test accuracy: 6.95
Round   3, Train loss: 2.328, Test loss: 2.319, Test accuracy: 7.80
Round   3, Global train loss: 2.328, Global test loss: 2.320, Global test accuracy: 7.08
Round   4, Train loss: nan, Test loss: nan, Test accuracy: 14.38
Round   4, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   5, Train loss: nan, Test loss: nan, Test accuracy: 14.42
Round   5, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   6, Train loss: nan, Test loss: nan, Test accuracy: 16.07
Round   6, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   7, Train loss: nan, Test loss: nan, Test accuracy: 15.05
Round   7, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   8, Train loss: nan, Test loss: nan, Test accuracy: 15.05
Round   8, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round   9, Train loss: nan, Test loss: nan, Test accuracy: 13.38
Round   9, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  10, Train loss: nan, Test loss: nan, Test accuracy: 11.72
Round  10, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  11, Train loss: nan, Test loss: nan, Test accuracy: 11.72
Round  11, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  12, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  12, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  13, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  13, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  14, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  14, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  15, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  15, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  16, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  16, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  17, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  17, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  18, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  18, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  19, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  19, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  20, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  20, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  21, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  21, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  22, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  22, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  23, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  23, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  24, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  24, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  25, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  25, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  26, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  26, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  27, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  27, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  28, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  28, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  29, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  29, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  30, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  30, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  31, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  31, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  32, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  32, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  33, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  33, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  34, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  34, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  35, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  35, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  36, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  36, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  37, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  37, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  38, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  38, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  39, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  39, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  40, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  40, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  41, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  41, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  42, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  42, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  43, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  43, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  44, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  44, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  45, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  45, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  46, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  46, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  47, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  47, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  48, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  48, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  49, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  49, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  50, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  50, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  51, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  51, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  52, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  52, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  53, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  53, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  54, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  54, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  55, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  55, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  56, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  56, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  57, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  57, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  58, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  58, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  59, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  59, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  60, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  60, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  61, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  61, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  62, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  62, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  63, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  63, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  64, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  64, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  65, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  65, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  66, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  66, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  67, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  67, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  68, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  68, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  69, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  69, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  70, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  70, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  71, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  71, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  72, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  72, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  73, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  73, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  74, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  74, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  75, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  75, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  76, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  76, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  77, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  77, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  78, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  78, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  79, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  79, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  80, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  80, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  81, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  81, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  82, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  82, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  83, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  83, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  84, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  84, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  85, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  85, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  86, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  86, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  87, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  87, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  88, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  88, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  89, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  89, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  90, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  90, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  91, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  91, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  92, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  92, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  93, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  93, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  94, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  94, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  95, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  95, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  96, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  96, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  97, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  97, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  98, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  98, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Round  99, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Round  99, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Final Round, Train loss: nan, Test loss: nan, Test accuracy: 13.33
Final Round, Global train loss: nan, Global test loss: nan, Global test accuracy: 13.33
Average accuracy final 10 rounds: 13.333333333333337 

Average global accuracy final 10 rounds: 13.333333333333337 

877.6529738903046
[1.019477367401123, 1.756075143814087, 2.487915277481079, 3.247049331665039, 3.9982171058654785, 4.7595391273498535, 5.490556240081787, 6.223198652267456, 6.955308198928833, 7.702017068862915, 8.434632301330566, 9.200014114379883, 9.944538593292236, 10.673142194747925, 11.40089201927185, 12.131983995437622, 12.863325357437134, 13.59545087814331, 14.320331335067749, 15.05222487449646, 15.81484842300415, 16.551435708999634, 17.28562617301941, 18.012789487838745, 18.74368643760681, 19.475382566452026, 20.201471090316772, 20.949880838394165, 21.68901252746582, 22.43662929534912, 23.165595531463623, 23.881988048553467, 24.5859477519989, 25.300159215927124, 26.014282703399658, 26.74409580230713, 27.464589595794678, 28.177027702331543, 28.8902645111084, 29.595690965652466, 30.304197549819946, 31.011927366256714, 31.724608659744263, 32.521973848342896, 33.308974266052246, 34.07996487617493, 34.890341997146606, 35.669758796691895, 36.45173406600952, 37.234156370162964, 38.008766412734985, 38.80095887184143, 39.604312896728516, 40.4277777671814, 41.23709464073181, 42.041998624801636, 42.85437607765198, 43.661154985427856, 44.46518301963806, 45.27605175971985, 46.1039080619812, 46.9186806678772, 47.7364227771759, 48.55916976928711, 49.36936855316162, 50.172783851623535, 51.00110054016113, 51.828683853149414, 52.655295610427856, 53.46830439567566, 54.27930045127869, 55.08888792991638, 55.89494967460632, 56.71255707740784, 57.5243353843689, 58.33606219291687, 59.15275287628174, 59.962870836257935, 60.771005392074585, 61.58260703086853, 62.394420862197876, 63.204559087753296, 64.02404689788818, 64.84458446502686, 65.66049456596375, 66.46171236038208, 67.27235054969788, 68.07800364494324, 68.89341616630554, 69.70603609085083, 70.52429747581482, 71.34091329574585, 72.14581418037415, 72.94716358184814, 73.74298644065857, 74.54847502708435, 75.3658058643341, 76.18665075302124, 77.01095628738403, 77.81796097755432, 79.43275737762451]
[7.25, 7.933333333333334, 8.816666666666666, 7.8, 14.383333333333333, 14.416666666666666, 16.066666666666666, 15.05, 15.05, 13.383333333333333, 11.716666666666667, 11.716666666666667, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334, 13.333333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.653, Test loss: 2.093, Test accuracy: 30.05
Round   1, Train loss: 1.121, Test loss: 1.668, Test accuracy: 39.42
Round   2, Train loss: 0.982, Test loss: 1.338, Test accuracy: 47.42
Round   3, Train loss: 0.927, Test loss: 1.071, Test accuracy: 53.28
Round   4, Train loss: 0.868, Test loss: 1.068, Test accuracy: 52.78
Round   5, Train loss: 0.816, Test loss: 0.998, Test accuracy: 55.60
Round   6, Train loss: 0.824, Test loss: 1.061, Test accuracy: 56.88
Round   7, Train loss: 0.771, Test loss: 0.939, Test accuracy: 57.07
Round   8, Train loss: 0.781, Test loss: 0.934, Test accuracy: 59.25
Round   9, Train loss: 0.725, Test loss: 0.969, Test accuracy: 58.53
Round  10, Train loss: 0.712, Test loss: 0.878, Test accuracy: 60.92
Round  11, Train loss: 0.833, Test loss: 0.771, Test accuracy: 65.45
Round  12, Train loss: 0.739, Test loss: 0.734, Test accuracy: 67.18
Round  13, Train loss: 0.714, Test loss: 0.727, Test accuracy: 67.85
Round  14, Train loss: 0.694, Test loss: 0.708, Test accuracy: 69.78
Round  15, Train loss: 0.732, Test loss: 0.708, Test accuracy: 70.18
Round  16, Train loss: 0.689, Test loss: 0.686, Test accuracy: 70.75
Round  17, Train loss: 0.667, Test loss: 0.673, Test accuracy: 70.82
Round  18, Train loss: 0.665, Test loss: 0.669, Test accuracy: 71.67
Round  19, Train loss: 0.594, Test loss: 0.649, Test accuracy: 72.28
Round  20, Train loss: 0.631, Test loss: 0.648, Test accuracy: 72.08
Round  21, Train loss: 0.576, Test loss: 0.645, Test accuracy: 72.50
Round  22, Train loss: 0.665, Test loss: 0.660, Test accuracy: 71.65
Round  23, Train loss: 0.655, Test loss: 0.647, Test accuracy: 72.63
Round  24, Train loss: 0.712, Test loss: 0.632, Test accuracy: 73.42
Round  25, Train loss: 0.630, Test loss: 0.622, Test accuracy: 73.13
Round  26, Train loss: 0.589, Test loss: 0.607, Test accuracy: 73.92
Round  27, Train loss: 0.633, Test loss: 0.607, Test accuracy: 73.95
Round  28, Train loss: 0.589, Test loss: 0.609, Test accuracy: 74.08
Round  29, Train loss: 0.524, Test loss: 0.611, Test accuracy: 74.15
Round  30, Train loss: 0.614, Test loss: 0.603, Test accuracy: 74.77
Round  31, Train loss: 0.542, Test loss: 0.593, Test accuracy: 75.18
Round  32, Train loss: 0.595, Test loss: 0.594, Test accuracy: 75.35
Round  33, Train loss: 0.481, Test loss: 0.592, Test accuracy: 75.17
Round  34, Train loss: 0.476, Test loss: 0.588, Test accuracy: 75.00
Round  35, Train loss: 0.539, Test loss: 0.587, Test accuracy: 74.75
Round  36, Train loss: 0.580, Test loss: 0.571, Test accuracy: 75.87
Round  37, Train loss: 0.593, Test loss: 0.574, Test accuracy: 75.32
Round  38, Train loss: 0.590, Test loss: 0.572, Test accuracy: 76.03
Round  39, Train loss: 0.505, Test loss: 0.567, Test accuracy: 75.83
Round  40, Train loss: 0.538, Test loss: 0.562, Test accuracy: 76.13
Round  41, Train loss: 0.618, Test loss: 0.569, Test accuracy: 75.92
Round  42, Train loss: 0.590, Test loss: 0.554, Test accuracy: 76.28
Round  43, Train loss: 0.571, Test loss: 0.561, Test accuracy: 76.38
Round  44, Train loss: 0.569, Test loss: 0.571, Test accuracy: 76.15
Round  45, Train loss: 0.558, Test loss: 0.556, Test accuracy: 76.92
Round  46, Train loss: 0.457, Test loss: 0.556, Test accuracy: 76.93
Round  47, Train loss: 0.454, Test loss: 0.548, Test accuracy: 77.13
Round  48, Train loss: 0.421, Test loss: 0.544, Test accuracy: 77.23
Round  49, Train loss: 0.521, Test loss: 0.541, Test accuracy: 77.53
Round  50, Train loss: 0.504, Test loss: 0.541, Test accuracy: 77.13
Round  51, Train loss: 0.496, Test loss: 0.523, Test accuracy: 77.65
Round  52, Train loss: 0.549, Test loss: 0.534, Test accuracy: 77.43
Round  53, Train loss: 0.403, Test loss: 0.528, Test accuracy: 77.83
Round  54, Train loss: 0.491, Test loss: 0.521, Test accuracy: 78.02
Round  55, Train loss: 0.486, Test loss: 0.524, Test accuracy: 78.27
Round  56, Train loss: 0.385, Test loss: 0.514, Test accuracy: 78.43
Round  57, Train loss: 0.522, Test loss: 0.511, Test accuracy: 78.90
Round  58, Train loss: 0.456, Test loss: 0.509, Test accuracy: 79.20
Round  59, Train loss: 0.463, Test loss: 0.507, Test accuracy: 78.90
Round  60, Train loss: 0.457, Test loss: 0.509, Test accuracy: 78.68
Round  61, Train loss: 0.494, Test loss: 0.506, Test accuracy: 78.48
Round  62, Train loss: 0.448, Test loss: 0.505, Test accuracy: 79.15
Round  63, Train loss: 0.409, Test loss: 0.509, Test accuracy: 79.07
Round  64, Train loss: 0.486, Test loss: 0.501, Test accuracy: 79.30
Round  65, Train loss: 0.376, Test loss: 0.499, Test accuracy: 79.07
Round  66, Train loss: 0.414, Test loss: 0.501, Test accuracy: 78.85
Round  67, Train loss: 0.417, Test loss: 0.500, Test accuracy: 79.25
Round  68, Train loss: 0.384, Test loss: 0.504, Test accuracy: 79.22
Round  69, Train loss: 0.393, Test loss: 0.502, Test accuracy: 79.20
Round  70, Train loss: 0.332, Test loss: 0.488, Test accuracy: 79.63
Round  71, Train loss: 0.430, Test loss: 0.486, Test accuracy: 79.83
Round  72, Train loss: 0.450, Test loss: 0.479, Test accuracy: 79.98
Round  73, Train loss: 0.393, Test loss: 0.479, Test accuracy: 80.17
Round  74, Train loss: 0.382, Test loss: 0.480, Test accuracy: 80.23
Round  75, Train loss: 0.415, Test loss: 0.477, Test accuracy: 80.38
Round  76, Train loss: 0.350, Test loss: 0.482, Test accuracy: 79.65
Round  77, Train loss: 0.346, Test loss: 0.478, Test accuracy: 79.75
Round  78, Train loss: 0.408, Test loss: 0.479, Test accuracy: 79.87
Round  79, Train loss: 0.355, Test loss: 0.472, Test accuracy: 80.45
Round  80, Train loss: 0.379, Test loss: 0.472, Test accuracy: 80.37
Round  81, Train loss: 0.343, Test loss: 0.482, Test accuracy: 80.25
Round  82, Train loss: 0.383, Test loss: 0.471, Test accuracy: 80.40
Round  83, Train loss: 0.343, Test loss: 0.480, Test accuracy: 80.48
Round  84, Train loss: 0.327, Test loss: 0.478, Test accuracy: 80.40
Round  85, Train loss: 0.307, Test loss: 0.485, Test accuracy: 80.10
Round  86, Train loss: 0.376, Test loss: 0.471, Test accuracy: 80.78
Round  87, Train loss: 0.353, Test loss: 0.465, Test accuracy: 81.07
Round  88, Train loss: 0.346, Test loss: 0.469, Test accuracy: 81.02
Round  89, Train loss: 0.350, Test loss: 0.466, Test accuracy: 81.00
Round  90, Train loss: 0.304, Test loss: 0.460, Test accuracy: 81.32
Round  91, Train loss: 0.361, Test loss: 0.467, Test accuracy: 80.65
Round  92, Train loss: 0.488, Test loss: 0.467, Test accuracy: 80.77
Round  93, Train loss: 0.375, Test loss: 0.462, Test accuracy: 80.78
Round  94, Train loss: 0.298, Test loss: 0.467, Test accuracy: 80.72
Round  95, Train loss: 0.314, Test loss: 0.471, Test accuracy: 80.62
Round  96, Train loss: 0.339, Test loss: 0.462, Test accuracy: 81.13
Round  97, Train loss: 0.377, Test loss: 0.464, Test accuracy: 81.37
Round  98, Train loss: 0.358, Test loss: 0.457, Test accuracy: 80.95
Round  99, Train loss: 0.354, Test loss: 0.466, Test accuracy: 80.70
Final Round, Train loss: 0.294, Test loss: 0.458, Test accuracy: 81.65
Average accuracy final 10 rounds: 80.9
718.1087346076965
[1.1595029830932617, 1.999720811843872, 2.8411574363708496, 3.6888771057128906, 4.525793790817261, 5.358326435089111, 6.197158575057983, 7.0374815464019775, 7.886994361877441, 8.735552072525024, 9.594986200332642, 10.43474531173706, 11.26560664176941, 12.098721265792847, 12.938874959945679, 13.782938241958618, 14.625801801681519, 15.48439073562622, 16.33508038520813, 17.18277359008789, 18.02176070213318, 18.853097200393677, 19.69381046295166, 20.528571367263794, 21.357808828353882, 22.19785213470459, 23.053282022476196, 23.89204502105713, 24.735063552856445, 25.588687419891357, 26.430294275283813, 27.27553105354309, 28.11550259590149, 28.95090365409851, 29.78209614753723, 30.61520504951477, 31.462767601013184, 32.3150417804718, 33.17210912704468, 34.015307903289795, 34.85029101371765, 35.68966722488403, 36.5776162147522, 37.47305178642273, 38.374425172805786, 39.29258108139038, 40.20888543128967, 41.11068844795227, 41.9411039352417, 42.772178173065186, 43.60665035247803, 44.454397678375244, 45.285438537597656, 46.121636629104614, 46.95843720436096, 47.79974603652954, 48.65337872505188, 49.496556997299194, 50.348334074020386, 51.18468904495239, 52.01444458961487, 52.83507561683655, 53.66680884361267, 54.55173468589783, 55.434651136398315, 56.29601263999939, 57.17317724227905, 58.01192331314087, 58.84321999549866, 59.69507813453674, 60.534011125564575, 61.37666964530945, 62.2416307926178, 63.11972498893738, 63.95103645324707, 64.77554750442505, 65.63369107246399, 66.51301741600037, 67.37990617752075, 68.2197859287262, 69.04780673980713, 69.8854603767395, 70.72026705741882, 71.56051778793335, 72.38626623153687, 73.22833132743835, 74.0669059753418, 74.89826941490173, 75.72856068611145, 76.55216431617737, 77.40480065345764, 78.25303506851196, 79.106205701828, 79.94911503791809, 80.77554726600647, 81.60636711120605, 82.44783425331116, 83.30761528015137, 84.17510890960693, 85.04321575164795, 86.38885974884033]
[30.05, 39.416666666666664, 47.416666666666664, 53.28333333333333, 52.78333333333333, 55.6, 56.88333333333333, 57.06666666666667, 59.25, 58.53333333333333, 60.916666666666664, 65.45, 67.18333333333334, 67.85, 69.78333333333333, 70.18333333333334, 70.75, 70.81666666666666, 71.66666666666667, 72.28333333333333, 72.08333333333333, 72.5, 71.65, 72.63333333333334, 73.41666666666667, 73.13333333333334, 73.91666666666667, 73.95, 74.08333333333333, 74.15, 74.76666666666667, 75.18333333333334, 75.35, 75.16666666666667, 75.0, 74.75, 75.86666666666666, 75.31666666666666, 76.03333333333333, 75.83333333333333, 76.13333333333334, 75.91666666666667, 76.28333333333333, 76.38333333333334, 76.15, 76.91666666666667, 76.93333333333334, 77.13333333333334, 77.23333333333333, 77.53333333333333, 77.13333333333334, 77.65, 77.43333333333334, 77.83333333333333, 78.01666666666667, 78.26666666666667, 78.43333333333334, 78.9, 79.2, 78.9, 78.68333333333334, 78.48333333333333, 79.15, 79.06666666666666, 79.3, 79.06666666666666, 78.85, 79.25, 79.21666666666667, 79.2, 79.63333333333334, 79.83333333333333, 79.98333333333333, 80.16666666666667, 80.23333333333333, 80.38333333333334, 79.65, 79.75, 79.86666666666666, 80.45, 80.36666666666666, 80.25, 80.4, 80.48333333333333, 80.4, 80.1, 80.78333333333333, 81.06666666666666, 81.01666666666667, 81.0, 81.31666666666666, 80.65, 80.76666666666667, 80.78333333333333, 80.71666666666667, 80.61666666666666, 81.13333333333334, 81.36666666666666, 80.95, 80.7, 81.65]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.740, Test loss: 2.129, Test accuracy: 23.42
Round   1, Train loss: 1.116, Test loss: 1.937, Test accuracy: 31.22
Round   2, Train loss: 1.055, Test loss: 1.377, Test accuracy: 43.88
Round   3, Train loss: 0.896, Test loss: 1.183, Test accuracy: 53.82
Round   4, Train loss: 0.865, Test loss: 1.121, Test accuracy: 55.75
Round   5, Train loss: 0.802, Test loss: 0.925, Test accuracy: 63.28
Round   6, Train loss: 0.763, Test loss: 0.905, Test accuracy: 63.20
Round   7, Train loss: 0.748, Test loss: 0.890, Test accuracy: 65.85
Round   8, Train loss: 0.740, Test loss: 0.864, Test accuracy: 65.43
Round   9, Train loss: 0.724, Test loss: 0.733, Test accuracy: 70.25
Round  10, Train loss: 0.709, Test loss: 0.660, Test accuracy: 72.87
Round  11, Train loss: 0.663, Test loss: 0.642, Test accuracy: 73.67
Round  12, Train loss: 0.652, Test loss: 0.623, Test accuracy: 74.97
Round  13, Train loss: 0.642, Test loss: 0.610, Test accuracy: 75.07
Round  14, Train loss: 0.641, Test loss: 0.605, Test accuracy: 76.23
Round  15, Train loss: 0.617, Test loss: 0.597, Test accuracy: 76.43
Round  16, Train loss: 0.616, Test loss: 0.578, Test accuracy: 76.62
Round  17, Train loss: 0.608, Test loss: 0.576, Test accuracy: 77.17
Round  18, Train loss: 0.626, Test loss: 0.582, Test accuracy: 76.88
Round  19, Train loss: 0.570, Test loss: 0.566, Test accuracy: 77.73
Round  20, Train loss: 0.623, Test loss: 0.564, Test accuracy: 78.30
Round  21, Train loss: 0.654, Test loss: 0.532, Test accuracy: 78.73
Round  22, Train loss: 0.633, Test loss: 0.533, Test accuracy: 79.23
Round  23, Train loss: 0.540, Test loss: 0.543, Test accuracy: 78.48
Round  24, Train loss: 0.627, Test loss: 0.522, Test accuracy: 79.45
Round  25, Train loss: 0.545, Test loss: 0.523, Test accuracy: 79.20
Round  26, Train loss: 0.532, Test loss: 0.508, Test accuracy: 79.68
Round  27, Train loss: 0.534, Test loss: 0.504, Test accuracy: 80.20
Round  28, Train loss: 0.496, Test loss: 0.503, Test accuracy: 80.33
Round  29, Train loss: 0.510, Test loss: 0.498, Test accuracy: 80.48
Round  30, Train loss: 0.596, Test loss: 0.485, Test accuracy: 80.78
Round  31, Train loss: 0.556, Test loss: 0.493, Test accuracy: 80.72
Round  32, Train loss: 0.486, Test loss: 0.497, Test accuracy: 80.07
Round  33, Train loss: 0.498, Test loss: 0.477, Test accuracy: 81.23
Round  34, Train loss: 0.447, Test loss: 0.468, Test accuracy: 81.42
Round  35, Train loss: 0.440, Test loss: 0.472, Test accuracy: 81.87
Round  36, Train loss: 0.488, Test loss: 0.469, Test accuracy: 81.38
Round  37, Train loss: 0.497, Test loss: 0.464, Test accuracy: 81.95
Round  38, Train loss: 0.450, Test loss: 0.466, Test accuracy: 81.92
Round  39, Train loss: 0.448, Test loss: 0.457, Test accuracy: 81.90
Round  40, Train loss: 0.462, Test loss: 0.456, Test accuracy: 82.43
Round  41, Train loss: 0.534, Test loss: 0.444, Test accuracy: 82.15
Round  42, Train loss: 0.515, Test loss: 0.442, Test accuracy: 82.83
Round  43, Train loss: 0.419, Test loss: 0.446, Test accuracy: 82.52
Round  44, Train loss: 0.485, Test loss: 0.441, Test accuracy: 82.07
Round  45, Train loss: 0.416, Test loss: 0.436, Test accuracy: 82.93
Round  46, Train loss: 0.425, Test loss: 0.433, Test accuracy: 82.95
Round  47, Train loss: 0.413, Test loss: 0.430, Test accuracy: 83.15
Round  48, Train loss: 0.470, Test loss: 0.422, Test accuracy: 83.67
Round  49, Train loss: 0.453, Test loss: 0.422, Test accuracy: 83.30
Round  50, Train loss: 0.415, Test loss: 0.431, Test accuracy: 82.77
Round  51, Train loss: 0.475, Test loss: 0.425, Test accuracy: 83.35
Round  52, Train loss: 0.400, Test loss: 0.411, Test accuracy: 83.52
Round  53, Train loss: 0.413, Test loss: 0.414, Test accuracy: 83.63
Round  54, Train loss: 0.370, Test loss: 0.415, Test accuracy: 83.37
Round  55, Train loss: 0.410, Test loss: 0.412, Test accuracy: 83.83
Round  56, Train loss: 0.379, Test loss: 0.411, Test accuracy: 83.72
Round  57, Train loss: 0.352, Test loss: 0.404, Test accuracy: 84.15
Round  58, Train loss: 0.388, Test loss: 0.410, Test accuracy: 83.65
Round  59, Train loss: 0.435, Test loss: 0.399, Test accuracy: 83.92
Round  60, Train loss: 0.362, Test loss: 0.405, Test accuracy: 84.12
Round  61, Train loss: 0.362, Test loss: 0.404, Test accuracy: 84.43
Round  62, Train loss: 0.370, Test loss: 0.403, Test accuracy: 83.87
Round  63, Train loss: 0.346, Test loss: 0.406, Test accuracy: 84.07
Round  64, Train loss: 0.347, Test loss: 0.400, Test accuracy: 83.88
Round  65, Train loss: 0.302, Test loss: 0.396, Test accuracy: 84.60
Round  66, Train loss: 0.349, Test loss: 0.400, Test accuracy: 84.23
Round  67, Train loss: 0.370, Test loss: 0.394, Test accuracy: 84.93
Round  68, Train loss: 0.333, Test loss: 0.387, Test accuracy: 84.90
Round  69, Train loss: 0.329, Test loss: 0.388, Test accuracy: 84.95
Round  70, Train loss: 0.340, Test loss: 0.384, Test accuracy: 85.32
Round  71, Train loss: 0.325, Test loss: 0.379, Test accuracy: 85.08
Round  72, Train loss: 0.278, Test loss: 0.381, Test accuracy: 85.18
Round  73, Train loss: 0.304, Test loss: 0.379, Test accuracy: 85.23
Round  74, Train loss: 0.327, Test loss: 0.381, Test accuracy: 84.95
Round  75, Train loss: 0.292, Test loss: 0.382, Test accuracy: 84.93
Round  76, Train loss: 0.287, Test loss: 0.370, Test accuracy: 85.40
Round  77, Train loss: 0.312, Test loss: 0.370, Test accuracy: 85.25
Round  78, Train loss: 0.326, Test loss: 0.381, Test accuracy: 85.03
Round  79, Train loss: 0.342, Test loss: 0.377, Test accuracy: 85.05
Round  80, Train loss: 0.296, Test loss: 0.374, Test accuracy: 85.50
Round  81, Train loss: 0.304, Test loss: 0.375, Test accuracy: 85.45
Round  82, Train loss: 0.349, Test loss: 0.379, Test accuracy: 84.95
Round  83, Train loss: 0.275, Test loss: 0.370, Test accuracy: 85.43
Round  84, Train loss: 0.334, Test loss: 0.375, Test accuracy: 85.17
Round  85, Train loss: 0.301, Test loss: 0.373, Test accuracy: 85.72
Round  86, Train loss: 0.312, Test loss: 0.376, Test accuracy: 84.82
Round  87, Train loss: 0.329, Test loss: 0.372, Test accuracy: 85.60
Round  88, Train loss: 0.277, Test loss: 0.364, Test accuracy: 85.85
Round  89, Train loss: 0.317, Test loss: 0.378, Test accuracy: 85.13
Round  90, Train loss: 0.352, Test loss: 0.371, Test accuracy: 85.37
Round  91, Train loss: 0.325, Test loss: 0.364, Test accuracy: 85.65
Round  92, Train loss: 0.289, Test loss: 0.365, Test accuracy: 85.75
Round  93, Train loss: 0.312, Test loss: 0.360, Test accuracy: 85.95
Round  94, Train loss: 0.241, Test loss: 0.356, Test accuracy: 86.32
Round  95, Train loss: 0.324, Test loss: 0.367, Test accuracy: 85.68
Round  96, Train loss: 0.273, Test loss: 0.370, Test accuracy: 85.42
Round  97, Train loss: 0.267, Test loss: 0.362, Test accuracy: 85.63
Round  98, Train loss: 0.259, Test loss: 0.360, Test accuracy: 85.73
Round  99, Train loss: 0.256, Test loss: 0.353, Test accuracy: 86.35
Final Round, Train loss: 0.226, Test loss: 0.352, Test accuracy: 86.20
Average accuracy final 10 rounds: 85.78500000000001
949.6588296890259
[1.2420601844787598, 2.4841203689575195, 3.4101831912994385, 4.336246013641357, 5.218605279922485, 6.100964546203613, 7.004390716552734, 7.9078168869018555, 8.828437328338623, 9.74905776977539, 10.67057204246521, 11.59208631515503, 12.485260009765625, 13.37843370437622, 14.26690125465393, 15.15536880493164, 16.0713152885437, 16.98726177215576, 17.91560673713684, 18.84395170211792, 19.79743480682373, 20.75091791152954, 21.682957410812378, 22.614996910095215, 23.546741485595703, 24.47848606109619, 25.408642292022705, 26.33879852294922, 27.276549100875854, 28.21429967880249, 29.1456515789032, 30.077003479003906, 30.994099140167236, 31.911194801330566, 32.80376386642456, 33.696332931518555, 34.61602187156677, 35.53571081161499, 36.44401526451111, 37.35231971740723, 38.29340434074402, 39.23448896408081, 40.166344165802, 41.09819936752319, 42.03489565849304, 42.97159194946289, 43.86854696273804, 44.765501976013184, 45.70161414146423, 46.63772630691528, 47.54708123207092, 48.45643615722656, 49.377524614334106, 50.29861307144165, 51.21674942970276, 52.13488578796387, 53.091941356658936, 54.048996925354004, 55.01467156410217, 55.98034620285034, 56.91655492782593, 57.852763652801514, 58.76949334144592, 59.68622303009033, 60.60650968551636, 61.52679634094238, 62.44040369987488, 63.35401105880737, 64.27306795120239, 65.19212484359741, 66.14815020561218, 67.10417556762695, 68.01991701126099, 68.93565845489502, 69.83309674263, 70.73053503036499, 71.65688395500183, 72.58323287963867, 73.46725058555603, 74.35126829147339, 75.28002834320068, 76.20878839492798, 77.12200951576233, 78.03523063659668, 78.94508242607117, 79.85493421554565, 80.79490876197815, 81.73488330841064, 82.64410758018494, 83.55333185195923, 84.4792788028717, 85.40522575378418, 86.31532287597656, 87.22541999816895, 88.16725635528564, 89.10909271240234, 90.06152844429016, 91.01396417617798, 91.94412684440613, 92.87428951263428, 93.786865234375, 94.69944095611572, 95.60936880111694, 96.51929664611816, 97.42111825942993, 98.3229398727417, 99.2562243938446, 100.18950891494751, 101.02547192573547, 101.86143493652344, 102.69290614128113, 103.52437734603882, 104.43276977539062, 105.34116220474243, 106.2557258605957, 107.17028951644897, 108.09448432922363, 109.01867914199829, 109.94304394721985, 110.8674087524414, 111.78278279304504, 112.69815683364868, 113.636727809906, 114.57529878616333, 115.50605320930481, 116.43680763244629, 117.36738395690918, 118.29796028137207, 119.19922065734863, 120.1004810333252, 121.02076554298401, 121.94105005264282, 122.8596465587616, 123.77824306488037, 124.69344902038574, 125.60865497589111, 126.55043244361877, 127.49220991134644, 128.41682934761047, 129.3414487838745, 130.2585506439209, 131.17565250396729, 132.0962278842926, 133.01680326461792, 133.9522054195404, 134.8876075744629, 135.79998803138733, 136.71236848831177, 137.63616561889648, 138.5599627494812, 139.48344230651855, 140.4069218635559, 141.332914352417, 142.25890684127808, 143.1698706150055, 144.0808343887329, 145.00585508346558, 145.93087577819824, 146.8528778553009, 147.77487993240356, 148.69225001335144, 149.60962009429932, 150.51503252983093, 151.42044496536255, 152.33104515075684, 153.24164533615112, 154.14042615890503, 155.03920698165894, 155.96940517425537, 156.8996033668518, 157.81570410728455, 158.73180484771729, 159.6591625213623, 160.58652019500732, 161.51001405715942, 162.43350791931152, 163.36942267417908, 164.30533742904663, 165.23097324371338, 166.15660905838013, 167.09083724021912, 168.0250654220581, 168.95233345031738, 169.87960147857666, 170.7860312461853, 171.69246101379395, 172.60451793670654, 173.51657485961914, 174.42914128303528, 175.34170770645142, 176.25454711914062, 177.16738653182983, 178.1097183227539, 179.05205011367798, 179.97232484817505, 180.89259958267212, 181.8128912448883, 182.7331829071045, 183.64277338981628, 184.55236387252808, 185.91438341140747, 187.27640295028687]
[23.416666666666668, 23.416666666666668, 31.216666666666665, 31.216666666666665, 43.88333333333333, 43.88333333333333, 53.81666666666667, 53.81666666666667, 55.75, 55.75, 63.28333333333333, 63.28333333333333, 63.2, 63.2, 65.85, 65.85, 65.43333333333334, 65.43333333333334, 70.25, 70.25, 72.86666666666666, 72.86666666666666, 73.66666666666667, 73.66666666666667, 74.96666666666667, 74.96666666666667, 75.06666666666666, 75.06666666666666, 76.23333333333333, 76.23333333333333, 76.43333333333334, 76.43333333333334, 76.61666666666666, 76.61666666666666, 77.16666666666667, 77.16666666666667, 76.88333333333334, 76.88333333333334, 77.73333333333333, 77.73333333333333, 78.3, 78.3, 78.73333333333333, 78.73333333333333, 79.23333333333333, 79.23333333333333, 78.48333333333333, 78.48333333333333, 79.45, 79.45, 79.2, 79.2, 79.68333333333334, 79.68333333333334, 80.2, 80.2, 80.33333333333333, 80.33333333333333, 80.48333333333333, 80.48333333333333, 80.78333333333333, 80.78333333333333, 80.71666666666667, 80.71666666666667, 80.06666666666666, 80.06666666666666, 81.23333333333333, 81.23333333333333, 81.41666666666667, 81.41666666666667, 81.86666666666666, 81.86666666666666, 81.38333333333334, 81.38333333333334, 81.95, 81.95, 81.91666666666667, 81.91666666666667, 81.9, 81.9, 82.43333333333334, 82.43333333333334, 82.15, 82.15, 82.83333333333333, 82.83333333333333, 82.51666666666667, 82.51666666666667, 82.06666666666666, 82.06666666666666, 82.93333333333334, 82.93333333333334, 82.95, 82.95, 83.15, 83.15, 83.66666666666667, 83.66666666666667, 83.3, 83.3, 82.76666666666667, 82.76666666666667, 83.35, 83.35, 83.51666666666667, 83.51666666666667, 83.63333333333334, 83.63333333333334, 83.36666666666666, 83.36666666666666, 83.83333333333333, 83.83333333333333, 83.71666666666667, 83.71666666666667, 84.15, 84.15, 83.65, 83.65, 83.91666666666667, 83.91666666666667, 84.11666666666666, 84.11666666666666, 84.43333333333334, 84.43333333333334, 83.86666666666666, 83.86666666666666, 84.06666666666666, 84.06666666666666, 83.88333333333334, 83.88333333333334, 84.6, 84.6, 84.23333333333333, 84.23333333333333, 84.93333333333334, 84.93333333333334, 84.9, 84.9, 84.95, 84.95, 85.31666666666666, 85.31666666666666, 85.08333333333333, 85.08333333333333, 85.18333333333334, 85.18333333333334, 85.23333333333333, 85.23333333333333, 84.95, 84.95, 84.93333333333334, 84.93333333333334, 85.4, 85.4, 85.25, 85.25, 85.03333333333333, 85.03333333333333, 85.05, 85.05, 85.5, 85.5, 85.45, 85.45, 84.95, 84.95, 85.43333333333334, 85.43333333333334, 85.16666666666667, 85.16666666666667, 85.71666666666667, 85.71666666666667, 84.81666666666666, 84.81666666666666, 85.6, 85.6, 85.85, 85.85, 85.13333333333334, 85.13333333333334, 85.36666666666666, 85.36666666666666, 85.65, 85.65, 85.75, 85.75, 85.95, 85.95, 86.31666666666666, 86.31666666666666, 85.68333333333334, 85.68333333333334, 85.41666666666667, 85.41666666666667, 85.63333333333334, 85.63333333333334, 85.73333333333333, 85.73333333333333, 86.35, 86.35, 86.2, 86.2]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.118, Test loss: 1.917, Test accuracy: 25.58
Round   0, Global train loss: 1.118, Global test loss: 2.250, Global test accuracy: 18.12
Round   1, Train loss: 0.901, Test loss: 1.828, Test accuracy: 32.37
Round   1, Global train loss: 0.901, Global test loss: 2.368, Global test accuracy: 20.00
Round   2, Train loss: 0.758, Test loss: 1.170, Test accuracy: 54.80
Round   2, Global train loss: 0.758, Global test loss: 2.062, Global test accuracy: 31.65
Round   3, Train loss: 0.775, Test loss: 1.060, Test accuracy: 55.16
Round   3, Global train loss: 0.775, Global test loss: 2.158, Global test accuracy: 27.45
Round   4, Train loss: 0.773, Test loss: 0.977, Test accuracy: 56.34
Round   4, Global train loss: 0.773, Global test loss: 2.162, Global test accuracy: 11.96
Round   5, Train loss: 0.741, Test loss: 0.926, Test accuracy: 62.24
Round   5, Global train loss: 0.741, Global test loss: 2.236, Global test accuracy: 29.15
Round   6, Train loss: 0.670, Test loss: 0.801, Test accuracy: 65.78
Round   6, Global train loss: 0.670, Global test loss: 1.881, Global test accuracy: 34.78
Round   7, Train loss: 0.713, Test loss: 0.697, Test accuracy: 68.75
Round   7, Global train loss: 0.713, Global test loss: 1.986, Global test accuracy: 30.41
Round   8, Train loss: 0.672, Test loss: 0.685, Test accuracy: 69.95
Round   8, Global train loss: 0.672, Global test loss: 2.155, Global test accuracy: 21.17
Round   9, Train loss: 0.531, Test loss: 0.679, Test accuracy: 70.26
Round   9, Global train loss: 0.531, Global test loss: 1.962, Global test accuracy: 26.76
Round  10, Train loss: 0.650, Test loss: 0.675, Test accuracy: 70.85
Round  10, Global train loss: 0.650, Global test loss: 2.206, Global test accuracy: 20.27
Round  11, Train loss: 0.557, Test loss: 0.652, Test accuracy: 71.83
Round  11, Global train loss: 0.557, Global test loss: 1.959, Global test accuracy: 24.12
Round  12, Train loss: 0.500, Test loss: 0.650, Test accuracy: 71.81
Round  12, Global train loss: 0.500, Global test loss: 2.067, Global test accuracy: 25.89
Round  13, Train loss: 0.588, Test loss: 0.637, Test accuracy: 72.62
Round  13, Global train loss: 0.588, Global test loss: 2.087, Global test accuracy: 27.37
Round  14, Train loss: 0.564, Test loss: 0.626, Test accuracy: 73.22
Round  14, Global train loss: 0.564, Global test loss: 1.954, Global test accuracy: 30.58
Round  15, Train loss: 0.455, Test loss: 0.636, Test accuracy: 73.17
Round  15, Global train loss: 0.455, Global test loss: 2.004, Global test accuracy: 26.40
Round  16, Train loss: 0.525, Test loss: 0.636, Test accuracy: 73.04
Round  16, Global train loss: 0.525, Global test loss: 1.980, Global test accuracy: 32.45
Round  17, Train loss: 0.458, Test loss: 0.616, Test accuracy: 73.88
Round  17, Global train loss: 0.458, Global test loss: 2.111, Global test accuracy: 30.68
Round  18, Train loss: 0.611, Test loss: 0.624, Test accuracy: 73.77
Round  18, Global train loss: 0.611, Global test loss: 2.106, Global test accuracy: 21.26
Round  19, Train loss: 0.597, Test loss: 0.620, Test accuracy: 74.12
Round  19, Global train loss: 0.597, Global test loss: 2.543, Global test accuracy: 16.67
Round  20, Train loss: 0.420, Test loss: 0.627, Test accuracy: 74.27
Round  20, Global train loss: 0.420, Global test loss: 1.924, Global test accuracy: 38.88
Round  21, Train loss: 0.508, Test loss: 0.629, Test accuracy: 74.51
Round  21, Global train loss: 0.508, Global test loss: 2.034, Global test accuracy: 27.82
Round  22, Train loss: 0.472, Test loss: 0.651, Test accuracy: 74.16
Round  22, Global train loss: 0.472, Global test loss: 2.002, Global test accuracy: 31.57
Round  23, Train loss: 0.440, Test loss: 0.638, Test accuracy: 74.29
Round  23, Global train loss: 0.440, Global test loss: 1.981, Global test accuracy: 31.71
Round  24, Train loss: 0.399, Test loss: 0.636, Test accuracy: 74.49
Round  24, Global train loss: 0.399, Global test loss: 2.048, Global test accuracy: 26.32
Round  25, Train loss: 0.460, Test loss: 0.639, Test accuracy: 75.00
Round  25, Global train loss: 0.460, Global test loss: 1.926, Global test accuracy: 34.23
Round  26, Train loss: 0.376, Test loss: 0.627, Test accuracy: 75.57
Round  26, Global train loss: 0.376, Global test loss: 1.924, Global test accuracy: 30.82
Round  27, Train loss: 0.403, Test loss: 0.626, Test accuracy: 76.29
Round  27, Global train loss: 0.403, Global test loss: 1.996, Global test accuracy: 31.29
Round  28, Train loss: 0.431, Test loss: 0.621, Test accuracy: 76.37
Round  28, Global train loss: 0.431, Global test loss: 2.024, Global test accuracy: 18.50
Round  29, Train loss: 0.347, Test loss: 0.620, Test accuracy: 76.04
Round  29, Global train loss: 0.347, Global test loss: 1.955, Global test accuracy: 34.98
Round  30, Train loss: 0.433, Test loss: 0.633, Test accuracy: 75.81
Round  30, Global train loss: 0.433, Global test loss: 2.179, Global test accuracy: 22.01
Round  31, Train loss: 0.420, Test loss: 0.624, Test accuracy: 76.22
Round  31, Global train loss: 0.420, Global test loss: 2.172, Global test accuracy: 21.41
Round  32, Train loss: 0.311, Test loss: 0.658, Test accuracy: 75.51
Round  32, Global train loss: 0.311, Global test loss: 2.075, Global test accuracy: 30.44
Round  33, Train loss: 0.443, Test loss: 0.647, Test accuracy: 75.76
Round  33, Global train loss: 0.443, Global test loss: 2.145, Global test accuracy: 23.42
Round  34, Train loss: 0.352, Test loss: 0.664, Test accuracy: 75.72
Round  34, Global train loss: 0.352, Global test loss: 2.017, Global test accuracy: 27.17
Round  35, Train loss: 0.300, Test loss: 0.664, Test accuracy: 76.28
Round  35, Global train loss: 0.300, Global test loss: 1.998, Global test accuracy: 31.02
Round  36, Train loss: 0.300, Test loss: 0.685, Test accuracy: 75.96
Round  36, Global train loss: 0.300, Global test loss: 2.074, Global test accuracy: 30.43
Round  37, Train loss: 0.255, Test loss: 0.699, Test accuracy: 75.97
Round  37, Global train loss: 0.255, Global test loss: 2.125, Global test accuracy: 22.44
Round  38, Train loss: 0.326, Test loss: 0.692, Test accuracy: 76.31
Round  38, Global train loss: 0.326, Global test loss: 2.271, Global test accuracy: 25.98
Round  39, Train loss: 0.351, Test loss: 0.678, Test accuracy: 76.98
Round  39, Global train loss: 0.351, Global test loss: 2.091, Global test accuracy: 23.33
Round  40, Train loss: 0.304, Test loss: 0.687, Test accuracy: 77.06
Round  40, Global train loss: 0.304, Global test loss: 2.194, Global test accuracy: 19.38
Round  41, Train loss: 0.228, Test loss: 0.714, Test accuracy: 76.58
Round  41, Global train loss: 0.228, Global test loss: 2.032, Global test accuracy: 30.19
Round  42, Train loss: 0.283, Test loss: 0.692, Test accuracy: 76.93
Round  42, Global train loss: 0.283, Global test loss: 2.080, Global test accuracy: 25.87
Round  43, Train loss: 0.295, Test loss: 0.699, Test accuracy: 77.04
Round  43, Global train loss: 0.295, Global test loss: 2.059, Global test accuracy: 20.83
Round  44, Train loss: 0.211, Test loss: 0.682, Test accuracy: 77.04
Round  44, Global train loss: 0.211, Global test loss: 2.072, Global test accuracy: 25.07
Round  45, Train loss: 0.303, Test loss: 0.706, Test accuracy: 76.67
Round  45, Global train loss: 0.303, Global test loss: 2.192, Global test accuracy: 19.98
Round  46, Train loss: 0.241, Test loss: 0.719, Test accuracy: 76.55
Round  46, Global train loss: 0.241, Global test loss: 1.987, Global test accuracy: 30.34
Round  47, Train loss: 0.223, Test loss: 0.720, Test accuracy: 76.71
Round  47, Global train loss: 0.223, Global test loss: 2.024, Global test accuracy: 29.66
Round  48, Train loss: 0.148, Test loss: 0.744, Test accuracy: 76.50
Round  48, Global train loss: 0.148, Global test loss: 1.984, Global test accuracy: 29.62
Round  49, Train loss: 0.232, Test loss: 0.759, Test accuracy: 76.51
Round  49, Global train loss: 0.232, Global test loss: 2.196, Global test accuracy: 19.27
Round  50, Train loss: 0.179, Test loss: 0.772, Test accuracy: 76.38
Round  50, Global train loss: 0.179, Global test loss: 1.952, Global test accuracy: 28.96
Round  51, Train loss: 0.165, Test loss: 0.806, Test accuracy: 76.17
Round  51, Global train loss: 0.165, Global test loss: 2.085, Global test accuracy: 25.11
Round  52, Train loss: 0.188, Test loss: 0.781, Test accuracy: 76.49
Round  52, Global train loss: 0.188, Global test loss: 1.948, Global test accuracy: 33.24
Round  53, Train loss: 0.199, Test loss: 0.782, Test accuracy: 76.79
Round  53, Global train loss: 0.199, Global test loss: 1.978, Global test accuracy: 30.66
Round  54, Train loss: 0.200, Test loss: 0.790, Test accuracy: 76.47
Round  54, Global train loss: 0.200, Global test loss: 2.081, Global test accuracy: 22.10
Round  55, Train loss: 0.165, Test loss: 0.792, Test accuracy: 76.75
Round  55, Global train loss: 0.165, Global test loss: 1.913, Global test accuracy: 33.38
Round  56, Train loss: 0.244, Test loss: 0.818, Test accuracy: 76.81
Round  56, Global train loss: 0.244, Global test loss: 2.164, Global test accuracy: 20.20
Round  57, Train loss: 0.209, Test loss: 0.832, Test accuracy: 76.89
Round  57, Global train loss: 0.209, Global test loss: 2.186, Global test accuracy: 23.33
Round  58, Train loss: 0.166, Test loss: 0.839, Test accuracy: 76.81
Round  58, Global train loss: 0.166, Global test loss: 2.288, Global test accuracy: 26.33
Round  59, Train loss: 0.170, Test loss: 0.842, Test accuracy: 76.42
Round  59, Global train loss: 0.170, Global test loss: 2.071, Global test accuracy: 17.80
Round  60, Train loss: 0.185, Test loss: 0.836, Test accuracy: 76.88
Round  60, Global train loss: 0.185, Global test loss: 2.056, Global test accuracy: 25.42
Round  61, Train loss: 0.186, Test loss: 0.831, Test accuracy: 77.39
Round  61, Global train loss: 0.186, Global test loss: 2.054, Global test accuracy: 24.17
Round  62, Train loss: 0.151, Test loss: 0.835, Test accuracy: 77.40
Round  62, Global train loss: 0.151, Global test loss: 2.178, Global test accuracy: 19.61
Round  63, Train loss: 0.135, Test loss: 0.862, Test accuracy: 76.83
Round  63, Global train loss: 0.135, Global test loss: 1.948, Global test accuracy: 33.99
Round  64, Train loss: 0.133, Test loss: 0.902, Test accuracy: 76.64
Round  64, Global train loss: 0.133, Global test loss: 1.971, Global test accuracy: 30.59
Round  65, Train loss: 0.172, Test loss: 0.896, Test accuracy: 76.72
Round  65, Global train loss: 0.172, Global test loss: 2.225, Global test accuracy: 16.71
Round  66, Train loss: 0.141, Test loss: 0.916, Test accuracy: 76.66
Round  66, Global train loss: 0.141, Global test loss: 2.181, Global test accuracy: 19.88
Round  67, Train loss: 0.145, Test loss: 0.923, Test accuracy: 77.15
Round  67, Global train loss: 0.145, Global test loss: 2.123, Global test accuracy: 19.68
Round  68, Train loss: 0.137, Test loss: 0.913, Test accuracy: 77.36
Round  68, Global train loss: 0.137, Global test loss: 2.236, Global test accuracy: 17.57
Round  69, Train loss: 0.105, Test loss: 0.918, Test accuracy: 77.35
Round  69, Global train loss: 0.105, Global test loss: 2.101, Global test accuracy: 26.02
Round  70, Train loss: 0.111, Test loss: 0.916, Test accuracy: 77.67
Round  70, Global train loss: 0.111, Global test loss: 1.901, Global test accuracy: 33.06
Round  71, Train loss: 0.117, Test loss: 0.937, Test accuracy: 77.55
Round  71, Global train loss: 0.117, Global test loss: 2.641, Global test accuracy: 20.32
Round  72, Train loss: 0.112, Test loss: 0.957, Test accuracy: 77.60
Round  72, Global train loss: 0.112, Global test loss: 2.005, Global test accuracy: 35.42
Round  73, Train loss: 0.110, Test loss: 0.944, Test accuracy: 77.61
Round  73, Global train loss: 0.110, Global test loss: 2.058, Global test accuracy: 25.83
Round  74, Train loss: 0.122, Test loss: 0.932, Test accuracy: 77.69
Round  74, Global train loss: 0.122, Global test loss: 2.058, Global test accuracy: 26.67
Round  75, Train loss: 0.090, Test loss: 0.942, Test accuracy: 77.58
Round  75, Global train loss: 0.090, Global test loss: 1.994, Global test accuracy: 25.77
Round  76, Train loss: 0.108, Test loss: 0.951, Test accuracy: 77.46
Round  76, Global train loss: 0.108, Global test loss: 2.587, Global test accuracy: 16.73
Round  77, Train loss: 0.112, Test loss: 0.967, Test accuracy: 77.20
Round  77, Global train loss: 0.112, Global test loss: 2.044, Global test accuracy: 30.70
Round  78, Train loss: 0.069, Test loss: 0.984, Test accuracy: 77.62
Round  78, Global train loss: 0.069, Global test loss: 2.532, Global test accuracy: 22.90
Round  79, Train loss: 0.111, Test loss: 1.009, Test accuracy: 77.62
Round  79, Global train loss: 0.111, Global test loss: 2.054, Global test accuracy: 26.82
Round  80, Train loss: 0.087, Test loss: 1.004, Test accuracy: 77.47
Round  80, Global train loss: 0.087, Global test loss: 2.037, Global test accuracy: 29.24
Round  81, Train loss: 0.116, Test loss: 1.002, Test accuracy: 77.39
Round  81, Global train loss: 0.116, Global test loss: 2.158, Global test accuracy: 18.68
Round  82, Train loss: 0.111, Test loss: 0.963, Test accuracy: 77.61
Round  82, Global train loss: 0.111, Global test loss: 2.207, Global test accuracy: 18.27
Round  83, Train loss: 0.076, Test loss: 0.955, Test accuracy: 77.92
Round  83, Global train loss: 0.076, Global test loss: 1.910, Global test accuracy: 33.43
Round  84, Train loss: 0.078, Test loss: 0.949, Test accuracy: 78.17
Round  84, Global train loss: 0.078, Global test loss: 2.018, Global test accuracy: 21.43
Round  85, Train loss: 0.067, Test loss: 0.977, Test accuracy: 77.89
Round  85, Global train loss: 0.067, Global test loss: 1.932, Global test accuracy: 27.38
Round  86, Train loss: 0.054, Test loss: 1.025, Test accuracy: 77.78
Round  86, Global train loss: 0.054, Global test loss: 1.962, Global test accuracy: 30.09
Round  87, Train loss: 0.061, Test loss: 1.018, Test accuracy: 77.70
Round  87, Global train loss: 0.061, Global test loss: 1.877, Global test accuracy: 32.73
Round  88, Train loss: 0.074, Test loss: 1.033, Test accuracy: 77.50
Round  88, Global train loss: 0.074, Global test loss: 2.148, Global test accuracy: 28.00
Round  89, Train loss: 0.088, Test loss: 1.015, Test accuracy: 77.69
Round  89, Global train loss: 0.088, Global test loss: 2.049, Global test accuracy: 28.05
Round  90, Train loss: 0.049, Test loss: 1.075, Test accuracy: 77.20
Round  90, Global train loss: 0.049, Global test loss: 2.058, Global test accuracy: 20.32
Round  91, Train loss: 0.056, Test loss: 1.064, Test accuracy: 77.23
Round  91, Global train loss: 0.056, Global test loss: 2.194, Global test accuracy: 21.02
Round  92, Train loss: 0.085, Test loss: 1.071, Test accuracy: 77.35
Round  92, Global train loss: 0.085, Global test loss: 2.325, Global test accuracy: 26.16
Round  93, Train loss: 0.049, Test loss: 1.080, Test accuracy: 76.79
Round  93, Global train loss: 0.049, Global test loss: 2.686, Global test accuracy: 29.10
Round  94, Train loss: 0.084, Test loss: 1.085, Test accuracy: 77.08
Round  94, Global train loss: 0.084, Global test loss: 1.948, Global test accuracy: 32.75
Round  95, Train loss: 0.069, Test loss: 1.080, Test accuracy: 77.46
Round  95, Global train loss: 0.069, Global test loss: 1.958, Global test accuracy: 33.20
Round  96, Train loss: 0.073, Test loss: 1.083, Test accuracy: 77.49
Round  96, Global train loss: 0.073, Global test loss: 2.067, Global test accuracy: 28.01
Round  97, Train loss: 0.081, Test loss: 1.113, Test accuracy: 77.44
Round  97, Global train loss: 0.081, Global test loss: 2.112, Global test accuracy: 20.40
Round  98, Train loss: 0.077, Test loss: 1.110, Test accuracy: 77.61
Round  98, Global train loss: 0.077, Global test loss: 2.043, Global test accuracy: 25.34
Round  99, Train loss: 0.055, Test loss: 1.101, Test accuracy: 77.82
Round  99, Global train loss: 0.055, Global test loss: 2.065, Global test accuracy: 30.18
Final Round, Train loss: 0.061, Test loss: 1.080, Test accuracy: 78.43
Final Round, Global train loss: 0.061, Global test loss: 2.065, Global test accuracy: 30.18
Average accuracy final 10 rounds: 77.3475 

Average global accuracy final 10 rounds: 26.64666666666667 

2029.4132735729218
[1.6399972438812256, 3.279994487762451, 4.642056226730347, 6.004117965698242, 7.35974645614624, 8.715374946594238, 10.064854383468628, 11.414333820343018, 12.764193773269653, 14.114053726196289, 15.46241044998169, 16.81076717376709, 18.15388059616089, 19.496994018554688, 20.850387811660767, 22.203781604766846, 23.56047224998474, 24.917162895202637, 26.269314765930176, 27.621466636657715, 29.148260593414307, 30.6750545501709, 32.20251774787903, 33.72998094558716, 35.25298500061035, 36.775989055633545, 38.31906056404114, 39.86213207244873, 41.39621353149414, 42.93029499053955, 44.468339681625366, 46.00638437271118, 47.527302980422974, 49.048221588134766, 50.5857629776001, 52.12330436706543, 53.66402316093445, 55.20474195480347, 56.72820329666138, 58.25166463851929, 59.77179956436157, 61.29193449020386, 62.82890200614929, 64.36586952209473, 65.90960741043091, 67.45334529876709, 68.98922228813171, 70.52509927749634, 72.04716277122498, 73.56922626495361, 75.09779071807861, 76.62635517120361, 77.96203303337097, 79.29771089553833, 80.63066983222961, 81.9636287689209, 83.30771684646606, 84.65180492401123, 85.98667812347412, 87.32155132293701, 88.67211389541626, 90.02267646789551, 91.37122130393982, 92.71976613998413, 94.05274033546448, 95.38571453094482, 96.72114419937134, 98.05657386779785, 99.39814329147339, 100.73971271514893, 102.07687091827393, 103.41402912139893, 104.75682473182678, 106.09962034225464, 107.41672039031982, 108.73382043838501, 110.07383823394775, 111.4138560295105, 112.78789162635803, 114.16192722320557, 115.50854444503784, 116.85516166687012, 118.20973062515259, 119.56429958343506, 120.89535093307495, 122.22640228271484, 123.6082034111023, 124.99000453948975, 126.35571455955505, 127.72142457962036, 129.05490159988403, 130.3883786201477, 131.71007776260376, 133.03177690505981, 134.37289261817932, 135.71400833129883, 137.05667853355408, 138.39934873580933, 139.7377529144287, 141.0761570930481, 142.4129993915558, 143.74984169006348, 145.10241889953613, 146.4549961090088, 147.7908627986908, 149.1267294883728, 150.4648096561432, 151.80288982391357, 153.13581705093384, 154.4687442779541, 155.81092858314514, 157.15311288833618, 158.4813928604126, 159.809672832489, 161.1355710029602, 162.4614691734314, 163.87120246887207, 165.28093576431274, 166.63048124313354, 167.98002672195435, 169.31635403633118, 170.652681350708, 171.99555897712708, 173.33843660354614, 174.67485404014587, 176.0112714767456, 177.35599613189697, 178.70072078704834, 180.0336480140686, 181.36657524108887, 182.69879174232483, 184.0310082435608, 185.38258004188538, 186.73415184020996, 188.0699644088745, 189.40577697753906, 190.73401474952698, 192.0622525215149, 193.402667760849, 194.7430830001831, 196.08152389526367, 197.41996479034424, 198.75499176979065, 200.09001874923706, 201.4302430152893, 202.77046728134155, 204.1291801929474, 205.48789310455322, 206.83573484420776, 208.1835765838623, 209.52623105049133, 210.86888551712036, 212.2099711894989, 213.55105686187744, 214.89799165725708, 216.24492645263672, 217.56884956359863, 218.89277267456055, 220.24185943603516, 221.59094619750977, 222.93429017066956, 224.27763414382935, 225.62307739257812, 226.9685206413269, 228.38360786437988, 229.79869508743286, 231.1828019618988, 232.56690883636475, 233.94569206237793, 235.3244752883911, 236.7844820022583, 238.2444887161255, 239.7612283229828, 241.2779679298401, 242.7845208644867, 244.2910737991333, 245.80183100700378, 247.31258821487427, 248.85172820091248, 250.39086818695068, 251.9158263206482, 253.4407844543457, 254.95428204536438, 256.46777963638306, 257.81840419769287, 259.1690287590027, 260.5424199104309, 261.91581106185913, 263.2692403793335, 264.62266969680786, 265.9739730358124, 267.3252763748169, 268.6762511730194, 270.0272259712219, 271.52898597717285, 273.0307459831238, 274.544068813324, 276.05739164352417, 277.5526719093323, 279.0479521751404, 281.58753848075867, 284.12712478637695]
[25.583333333333332, 25.583333333333332, 32.36666666666667, 32.36666666666667, 54.8, 54.8, 55.15833333333333, 55.15833333333333, 56.34166666666667, 56.34166666666667, 62.24166666666667, 62.24166666666667, 65.775, 65.775, 68.75, 68.75, 69.95, 69.95, 70.25833333333334, 70.25833333333334, 70.85, 70.85, 71.825, 71.825, 71.80833333333334, 71.80833333333334, 72.61666666666666, 72.61666666666666, 73.21666666666667, 73.21666666666667, 73.175, 73.175, 73.04166666666667, 73.04166666666667, 73.88333333333334, 73.88333333333334, 73.76666666666667, 73.76666666666667, 74.11666666666666, 74.11666666666666, 74.26666666666667, 74.26666666666667, 74.50833333333334, 74.50833333333334, 74.15833333333333, 74.15833333333333, 74.29166666666667, 74.29166666666667, 74.49166666666666, 74.49166666666666, 75.0, 75.0, 75.56666666666666, 75.56666666666666, 76.29166666666667, 76.29166666666667, 76.36666666666666, 76.36666666666666, 76.04166666666667, 76.04166666666667, 75.80833333333334, 75.80833333333334, 76.225, 76.225, 75.50833333333334, 75.50833333333334, 75.75833333333334, 75.75833333333334, 75.71666666666667, 75.71666666666667, 76.275, 76.275, 75.95833333333333, 75.95833333333333, 75.975, 75.975, 76.30833333333334, 76.30833333333334, 76.98333333333333, 76.98333333333333, 77.05833333333334, 77.05833333333334, 76.575, 76.575, 76.93333333333334, 76.93333333333334, 77.04166666666667, 77.04166666666667, 77.04166666666667, 77.04166666666667, 76.675, 76.675, 76.55, 76.55, 76.70833333333333, 76.70833333333333, 76.5, 76.5, 76.50833333333334, 76.50833333333334, 76.375, 76.375, 76.16666666666667, 76.16666666666667, 76.49166666666666, 76.49166666666666, 76.79166666666667, 76.79166666666667, 76.46666666666667, 76.46666666666667, 76.75, 76.75, 76.80833333333334, 76.80833333333334, 76.89166666666667, 76.89166666666667, 76.80833333333334, 76.80833333333334, 76.425, 76.425, 76.88333333333334, 76.88333333333334, 77.39166666666667, 77.39166666666667, 77.4, 77.4, 76.83333333333333, 76.83333333333333, 76.64166666666667, 76.64166666666667, 76.71666666666667, 76.71666666666667, 76.65833333333333, 76.65833333333333, 77.15, 77.15, 77.35833333333333, 77.35833333333333, 77.35, 77.35, 77.66666666666667, 77.66666666666667, 77.55, 77.55, 77.6, 77.6, 77.60833333333333, 77.60833333333333, 77.69166666666666, 77.69166666666666, 77.58333333333333, 77.58333333333333, 77.45833333333333, 77.45833333333333, 77.2, 77.2, 77.625, 77.625, 77.61666666666666, 77.61666666666666, 77.46666666666667, 77.46666666666667, 77.39166666666667, 77.39166666666667, 77.60833333333333, 77.60833333333333, 77.925, 77.925, 78.175, 78.175, 77.89166666666667, 77.89166666666667, 77.775, 77.775, 77.7, 77.7, 77.5, 77.5, 77.69166666666666, 77.69166666666666, 77.2, 77.2, 77.23333333333333, 77.23333333333333, 77.35, 77.35, 76.79166666666667, 76.79166666666667, 77.08333333333333, 77.08333333333333, 77.45833333333333, 77.45833333333333, 77.49166666666666, 77.49166666666666, 77.44166666666666, 77.44166666666666, 77.60833333333333, 77.60833333333333, 77.81666666666666, 77.81666666666666, 78.43333333333334, 78.43333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.066, Test loss: 1.851, Test accuracy: 31.89
Round   0, Global train loss: 1.066, Global test loss: 2.222, Global test accuracy: 24.35
Round   1, Train loss: 0.923, Test loss: 1.607, Test accuracy: 42.27
Round   1, Global train loss: 0.923, Global test loss: 2.224, Global test accuracy: 26.92
Round   2, Train loss: 0.815, Test loss: 1.055, Test accuracy: 56.05
Round   2, Global train loss: 0.815, Global test loss: 1.915, Global test accuracy: 26.14
Round   3, Train loss: 0.767, Test loss: 0.966, Test accuracy: 61.65
Round   3, Global train loss: 0.767, Global test loss: 2.359, Global test accuracy: 22.49
Round   4, Train loss: 0.733, Test loss: 0.773, Test accuracy: 66.78
Round   4, Global train loss: 0.733, Global test loss: 1.824, Global test accuracy: 36.17
Round   5, Train loss: 0.686, Test loss: 0.668, Test accuracy: 70.83
Round   5, Global train loss: 0.686, Global test loss: 1.871, Global test accuracy: 36.07
Round   6, Train loss: 0.680, Test loss: 0.641, Test accuracy: 72.47
Round   6, Global train loss: 0.680, Global test loss: 1.856, Global test accuracy: 36.52
Round   7, Train loss: 0.622, Test loss: 0.626, Test accuracy: 73.12
Round   7, Global train loss: 0.622, Global test loss: 1.656, Global test accuracy: 38.13
Round   8, Train loss: 0.678, Test loss: 0.616, Test accuracy: 73.95
Round   8, Global train loss: 0.678, Global test loss: 1.689, Global test accuracy: 39.79
Round   9, Train loss: 0.622, Test loss: 0.612, Test accuracy: 74.12
Round   9, Global train loss: 0.622, Global test loss: 1.743, Global test accuracy: 34.70
Round  10, Train loss: 0.577, Test loss: 0.579, Test accuracy: 76.02
Round  10, Global train loss: 0.577, Global test loss: 1.556, Global test accuracy: 43.70
Round  11, Train loss: 0.546, Test loss: 0.579, Test accuracy: 75.93
Round  11, Global train loss: 0.546, Global test loss: 1.706, Global test accuracy: 37.63
Round  12, Train loss: 0.571, Test loss: 0.578, Test accuracy: 76.08
Round  12, Global train loss: 0.571, Global test loss: 1.722, Global test accuracy: 41.00
Round  13, Train loss: 0.624, Test loss: 0.557, Test accuracy: 77.14
Round  13, Global train loss: 0.624, Global test loss: 1.721, Global test accuracy: 40.26
Round  14, Train loss: 0.528, Test loss: 0.547, Test accuracy: 77.70
Round  14, Global train loss: 0.528, Global test loss: 1.554, Global test accuracy: 44.23
Round  15, Train loss: 0.543, Test loss: 0.548, Test accuracy: 77.63
Round  15, Global train loss: 0.543, Global test loss: 1.573, Global test accuracy: 44.06
Round  16, Train loss: 0.528, Test loss: 0.544, Test accuracy: 77.60
Round  16, Global train loss: 0.528, Global test loss: 1.328, Global test accuracy: 51.58
Round  17, Train loss: 0.531, Test loss: 0.541, Test accuracy: 77.65
Round  17, Global train loss: 0.531, Global test loss: 1.568, Global test accuracy: 43.89
Round  18, Train loss: 0.506, Test loss: 0.509, Test accuracy: 79.17
Round  18, Global train loss: 0.506, Global test loss: 1.307, Global test accuracy: 53.45
Round  19, Train loss: 0.517, Test loss: 0.498, Test accuracy: 80.13
Round  19, Global train loss: 0.517, Global test loss: 1.331, Global test accuracy: 52.48
Round  20, Train loss: 0.478, Test loss: 0.497, Test accuracy: 80.33
Round  20, Global train loss: 0.478, Global test loss: 1.393, Global test accuracy: 52.08
Round  21, Train loss: 0.487, Test loss: 0.489, Test accuracy: 80.72
Round  21, Global train loss: 0.487, Global test loss: 1.486, Global test accuracy: 49.65
Round  22, Train loss: 0.479, Test loss: 0.490, Test accuracy: 80.44
Round  22, Global train loss: 0.479, Global test loss: 1.272, Global test accuracy: 54.61
Round  23, Train loss: 0.465, Test loss: 0.483, Test accuracy: 80.69
Round  23, Global train loss: 0.465, Global test loss: 1.536, Global test accuracy: 44.52
Round  24, Train loss: 0.441, Test loss: 0.478, Test accuracy: 81.28
Round  24, Global train loss: 0.441, Global test loss: 1.434, Global test accuracy: 48.85
Round  25, Train loss: 0.428, Test loss: 0.487, Test accuracy: 80.76
Round  25, Global train loss: 0.428, Global test loss: 1.241, Global test accuracy: 55.92
Round  26, Train loss: 0.427, Test loss: 0.479, Test accuracy: 81.21
Round  26, Global train loss: 0.427, Global test loss: 1.246, Global test accuracy: 56.79
Round  27, Train loss: 0.429, Test loss: 0.477, Test accuracy: 81.51
Round  27, Global train loss: 0.429, Global test loss: 1.404, Global test accuracy: 51.54
Round  28, Train loss: 0.451, Test loss: 0.485, Test accuracy: 81.56
Round  28, Global train loss: 0.451, Global test loss: 1.454, Global test accuracy: 49.23
Round  29, Train loss: 0.407, Test loss: 0.482, Test accuracy: 81.73
Round  29, Global train loss: 0.407, Global test loss: 1.366, Global test accuracy: 54.05
Round  30, Train loss: 0.420, Test loss: 0.480, Test accuracy: 81.78
Round  30, Global train loss: 0.420, Global test loss: 1.528, Global test accuracy: 49.60
Round  31, Train loss: 0.413, Test loss: 0.468, Test accuracy: 82.11
Round  31, Global train loss: 0.413, Global test loss: 1.279, Global test accuracy: 56.77
Round  32, Train loss: 0.391, Test loss: 0.473, Test accuracy: 81.89
Round  32, Global train loss: 0.391, Global test loss: 1.272, Global test accuracy: 58.15
Round  33, Train loss: 0.400, Test loss: 0.473, Test accuracy: 82.02
Round  33, Global train loss: 0.400, Global test loss: 1.274, Global test accuracy: 55.38
Round  34, Train loss: 0.397, Test loss: 0.468, Test accuracy: 82.31
Round  34, Global train loss: 0.397, Global test loss: 1.532, Global test accuracy: 49.14
Round  35, Train loss: 0.430, Test loss: 0.462, Test accuracy: 82.62
Round  35, Global train loss: 0.430, Global test loss: 1.147, Global test accuracy: 60.12
Round  36, Train loss: 0.400, Test loss: 0.455, Test accuracy: 83.02
Round  36, Global train loss: 0.400, Global test loss: 1.322, Global test accuracy: 52.83
Round  37, Train loss: 0.408, Test loss: 0.444, Test accuracy: 83.37
Round  37, Global train loss: 0.408, Global test loss: 1.197, Global test accuracy: 58.22
Round  38, Train loss: 0.351, Test loss: 0.447, Test accuracy: 83.21
Round  38, Global train loss: 0.351, Global test loss: 1.328, Global test accuracy: 55.11
Round  39, Train loss: 0.332, Test loss: 0.467, Test accuracy: 82.85
Round  39, Global train loss: 0.332, Global test loss: 1.351, Global test accuracy: 54.61
Round  40, Train loss: 0.361, Test loss: 0.451, Test accuracy: 83.26
Round  40, Global train loss: 0.361, Global test loss: 1.159, Global test accuracy: 61.03
Round  41, Train loss: 0.340, Test loss: 0.465, Test accuracy: 82.58
Round  41, Global train loss: 0.340, Global test loss: 1.374, Global test accuracy: 55.67
Round  42, Train loss: 0.350, Test loss: 0.456, Test accuracy: 82.92
Round  42, Global train loss: 0.350, Global test loss: 1.522, Global test accuracy: 52.37
Round  43, Train loss: 0.332, Test loss: 0.442, Test accuracy: 83.43
Round  43, Global train loss: 0.332, Global test loss: 1.404, Global test accuracy: 57.44
Round  44, Train loss: 0.337, Test loss: 0.435, Test accuracy: 83.93
Round  44, Global train loss: 0.337, Global test loss: 1.279, Global test accuracy: 58.74
Round  45, Train loss: 0.346, Test loss: 0.434, Test accuracy: 83.72
Round  45, Global train loss: 0.346, Global test loss: 1.213, Global test accuracy: 58.29
Round  46, Train loss: 0.335, Test loss: 0.434, Test accuracy: 83.76
Round  46, Global train loss: 0.335, Global test loss: 1.389, Global test accuracy: 55.77
Round  47, Train loss: 0.304, Test loss: 0.431, Test accuracy: 84.08
Round  47, Global train loss: 0.304, Global test loss: 1.438, Global test accuracy: 56.56
Round  48, Train loss: 0.355, Test loss: 0.418, Test accuracy: 84.44
Round  48, Global train loss: 0.355, Global test loss: 1.406, Global test accuracy: 54.89
Round  49, Train loss: 0.308, Test loss: 0.434, Test accuracy: 84.22
Round  49, Global train loss: 0.308, Global test loss: 1.601, Global test accuracy: 52.29
Round  50, Train loss: 0.321, Test loss: 0.420, Test accuracy: 84.64
Round  50, Global train loss: 0.321, Global test loss: 1.330, Global test accuracy: 56.92
Round  51, Train loss: 0.300, Test loss: 0.425, Test accuracy: 84.53
Round  51, Global train loss: 0.300, Global test loss: 1.423, Global test accuracy: 54.26
Round  52, Train loss: 0.294, Test loss: 0.433, Test accuracy: 84.13
Round  52, Global train loss: 0.294, Global test loss: 1.226, Global test accuracy: 59.63
Round  53, Train loss: 0.257, Test loss: 0.436, Test accuracy: 84.19
Round  53, Global train loss: 0.257, Global test loss: 1.265, Global test accuracy: 58.91
Round  54, Train loss: 0.278, Test loss: 0.431, Test accuracy: 84.47
Round  54, Global train loss: 0.278, Global test loss: 1.402, Global test accuracy: 58.73
Round  55, Train loss: 0.297, Test loss: 0.434, Test accuracy: 84.83
Round  55, Global train loss: 0.297, Global test loss: 1.234, Global test accuracy: 59.03
Round  56, Train loss: 0.257, Test loss: 0.436, Test accuracy: 84.77
Round  56, Global train loss: 0.257, Global test loss: 1.248, Global test accuracy: 60.84
Round  57, Train loss: 0.279, Test loss: 0.433, Test accuracy: 84.85
Round  57, Global train loss: 0.279, Global test loss: 1.325, Global test accuracy: 57.80
Round  58, Train loss: 0.278, Test loss: 0.432, Test accuracy: 84.82
Round  58, Global train loss: 0.278, Global test loss: 1.262, Global test accuracy: 57.41
Round  59, Train loss: 0.281, Test loss: 0.445, Test accuracy: 84.53
Round  59, Global train loss: 0.281, Global test loss: 1.478, Global test accuracy: 55.98
Round  60, Train loss: 0.293, Test loss: 0.432, Test accuracy: 84.94
Round  60, Global train loss: 0.293, Global test loss: 1.238, Global test accuracy: 59.29
Round  61, Train loss: 0.286, Test loss: 0.428, Test accuracy: 84.95
Round  61, Global train loss: 0.286, Global test loss: 1.660, Global test accuracy: 54.39
Round  62, Train loss: 0.258, Test loss: 0.430, Test accuracy: 84.84
Round  62, Global train loss: 0.258, Global test loss: 1.123, Global test accuracy: 62.83
Round  63, Train loss: 0.284, Test loss: 0.444, Test accuracy: 84.60
Round  63, Global train loss: 0.284, Global test loss: 1.160, Global test accuracy: 61.83
Round  64, Train loss: 0.261, Test loss: 0.444, Test accuracy: 84.58
Round  64, Global train loss: 0.261, Global test loss: 1.244, Global test accuracy: 60.68
Round  65, Train loss: 0.255, Test loss: 0.450, Test accuracy: 84.54
Round  65, Global train loss: 0.255, Global test loss: 1.259, Global test accuracy: 59.92
Round  66, Train loss: 0.308, Test loss: 0.443, Test accuracy: 84.92
Round  66, Global train loss: 0.308, Global test loss: 1.485, Global test accuracy: 53.97
Round  67, Train loss: 0.274, Test loss: 0.446, Test accuracy: 84.86
Round  67, Global train loss: 0.274, Global test loss: 1.233, Global test accuracy: 59.94
Round  68, Train loss: 0.255, Test loss: 0.447, Test accuracy: 84.87
Round  68, Global train loss: 0.255, Global test loss: 1.222, Global test accuracy: 61.43
Round  69, Train loss: 0.282, Test loss: 0.451, Test accuracy: 84.89
Round  69, Global train loss: 0.282, Global test loss: 1.820, Global test accuracy: 48.30
Round  70, Train loss: 0.266, Test loss: 0.447, Test accuracy: 84.93
Round  70, Global train loss: 0.266, Global test loss: 1.224, Global test accuracy: 60.52
Round  71, Train loss: 0.223, Test loss: 0.442, Test accuracy: 85.08
Round  71, Global train loss: 0.223, Global test loss: 1.301, Global test accuracy: 58.99
Round  72, Train loss: 0.236, Test loss: 0.438, Test accuracy: 85.10
Round  72, Global train loss: 0.236, Global test loss: 1.078, Global test accuracy: 63.42
Round  73, Train loss: 0.269, Test loss: 0.437, Test accuracy: 85.19
Round  73, Global train loss: 0.269, Global test loss: 1.297, Global test accuracy: 58.40
Round  74, Train loss: 0.246, Test loss: 0.435, Test accuracy: 85.32
Round  74, Global train loss: 0.246, Global test loss: 1.121, Global test accuracy: 64.26
Round  75, Train loss: 0.272, Test loss: 0.424, Test accuracy: 85.57
Round  75, Global train loss: 0.272, Global test loss: 1.358, Global test accuracy: 56.59
Round  76, Train loss: 0.228, Test loss: 0.429, Test accuracy: 85.42
Round  76, Global train loss: 0.228, Global test loss: 1.179, Global test accuracy: 63.03
Round  77, Train loss: 0.224, Test loss: 0.437, Test accuracy: 85.59
Round  77, Global train loss: 0.224, Global test loss: 1.039, Global test accuracy: 65.28
Round  78, Train loss: 0.232, Test loss: 0.440, Test accuracy: 85.54
Round  78, Global train loss: 0.232, Global test loss: 1.426, Global test accuracy: 57.27
Round  79, Train loss: 0.219, Test loss: 0.439, Test accuracy: 85.52
Round  79, Global train loss: 0.219, Global test loss: 1.279, Global test accuracy: 60.55
Round  80, Train loss: 0.209, Test loss: 0.461, Test accuracy: 85.03
Round  80, Global train loss: 0.209, Global test loss: 1.345, Global test accuracy: 59.83
Round  81, Train loss: 0.224, Test loss: 0.473, Test accuracy: 85.10
Round  81, Global train loss: 0.224, Global test loss: 1.370, Global test accuracy: 59.93
Round  82, Train loss: 0.233, Test loss: 0.460, Test accuracy: 85.17
Round  82, Global train loss: 0.233, Global test loss: 1.156, Global test accuracy: 63.58
Round  83, Train loss: 0.213, Test loss: 0.459, Test accuracy: 85.32
Round  83, Global train loss: 0.213, Global test loss: 1.184, Global test accuracy: 63.60
Round  84, Train loss: 0.220, Test loss: 0.447, Test accuracy: 85.74
Round  84, Global train loss: 0.220, Global test loss: 1.134, Global test accuracy: 65.40
Round  85, Train loss: 0.205, Test loss: 0.476, Test accuracy: 85.38
Round  85, Global train loss: 0.205, Global test loss: 1.253, Global test accuracy: 60.92
Round  86, Train loss: 0.217, Test loss: 0.454, Test accuracy: 85.63
Round  86, Global train loss: 0.217, Global test loss: 1.221, Global test accuracy: 60.24
Round  87, Train loss: 0.203, Test loss: 0.441, Test accuracy: 86.17
Round  87, Global train loss: 0.203, Global test loss: 1.171, Global test accuracy: 64.62
Round  88, Train loss: 0.200, Test loss: 0.449, Test accuracy: 85.95
Round  88, Global train loss: 0.200, Global test loss: 1.125, Global test accuracy: 64.22
Round  89, Train loss: 0.202, Test loss: 0.447, Test accuracy: 86.22
Round  89, Global train loss: 0.202, Global test loss: 1.067, Global test accuracy: 65.80
Round  90, Train loss: 0.206, Test loss: 0.439, Test accuracy: 86.45
Round  90, Global train loss: 0.206, Global test loss: 1.231, Global test accuracy: 61.87
Round  91, Train loss: 0.202, Test loss: 0.446, Test accuracy: 86.06
Round  91, Global train loss: 0.202, Global test loss: 1.174, Global test accuracy: 61.28
Round  92, Train loss: 0.171, Test loss: 0.464, Test accuracy: 85.54
Round  92, Global train loss: 0.171, Global test loss: 1.283, Global test accuracy: 62.28
Round  93, Train loss: 0.182, Test loss: 0.445, Test accuracy: 85.97
Round  93, Global train loss: 0.182, Global test loss: 1.193, Global test accuracy: 64.04
Round  94, Train loss: 0.185, Test loss: 0.434, Test accuracy: 86.13
Round  94, Global train loss: 0.185, Global test loss: 1.270, Global test accuracy: 61.27
Round  95, Train loss: 0.198, Test loss: 0.445, Test accuracy: 86.05
Round  95, Global train loss: 0.198, Global test loss: 1.113, Global test accuracy: 63.82
Round  96, Train loss: 0.214, Test loss: 0.441, Test accuracy: 86.12
Round  96, Global train loss: 0.214, Global test loss: 1.179, Global test accuracy: 63.22
Round  97, Train loss: 0.171, Test loss: 0.452, Test accuracy: 85.88
Round  97, Global train loss: 0.171, Global test loss: 1.242, Global test accuracy: 63.71
Round  98, Train loss: 0.199, Test loss: 0.468, Test accuracy: 85.53
Round  98, Global train loss: 0.199, Global test loss: 1.486, Global test accuracy: 59.48
Round  99, Train loss: 0.194, Test loss: 0.466, Test accuracy: 85.80
Round  99, Global train loss: 0.194, Global test loss: 1.300, Global test accuracy: 61.96
Final Round, Train loss: 0.149, Test loss: 0.476, Test accuracy: 86.12
Final Round, Global train loss: 0.149, Global test loss: 1.300, Global test accuracy: 61.96
Average accuracy final 10 rounds: 85.95333333333333 

Average global accuracy final 10 rounds: 62.29333333333332 

2008.6706910133362
[1.7086975574493408, 3.4173951148986816, 4.907046556472778, 6.396697998046875, 7.847329378128052, 9.297960758209229, 10.731663942337036, 12.165367126464844, 13.656594276428223, 15.147821426391602, 16.626991748809814, 18.106162071228027, 19.590239763259888, 21.074317455291748, 22.582452535629272, 24.090587615966797, 25.60954189300537, 27.128496170043945, 28.603896617889404, 30.079297065734863, 31.579630613327026, 33.07996416091919, 34.601664543151855, 36.12336492538452, 37.63848328590393, 39.15360164642334, 40.64815974235535, 42.14271783828735, 43.64574098587036, 45.14876413345337, 46.65494441986084, 48.16112470626831, 49.65967917442322, 51.158233642578125, 52.656116247177124, 54.15399885177612, 55.65830969810486, 57.162620544433594, 58.701359033584595, 60.240097522735596, 61.76260209083557, 63.28510665893555, 64.7868230342865, 66.28853940963745, 67.80443954467773, 69.32033967971802, 70.84341144561768, 72.36648321151733, 73.85189604759216, 75.33730888366699, 76.86591362953186, 78.39451837539673, 79.90127062797546, 81.4080228805542, 82.91451048851013, 84.42099809646606, 85.92659330368042, 87.43218851089478, 88.9382438659668, 90.44429922103882, 91.9360122680664, 93.427725315094, 94.95011878013611, 96.47251224517822, 97.98363137245178, 99.49475049972534, 100.99342942237854, 102.49210834503174, 103.99484038352966, 105.49757242202759, 107.006680727005, 108.51578903198242, 110.02867388725281, 111.5415587425232, 113.0395233631134, 114.53748798370361, 116.07393217086792, 117.61037635803223, 119.1176106929779, 120.62484502792358, 122.15087223052979, 123.67689943313599, 125.23480725288391, 126.79271507263184, 128.338684797287, 129.88465452194214, 131.44930362701416, 133.01395273208618, 134.5807225704193, 136.14749240875244, 137.7155351638794, 139.28357791900635, 140.70208287239075, 142.12058782577515, 143.51360154151917, 144.90661525726318, 146.25239539146423, 147.59817552566528, 148.9122223854065, 150.2262692451477, 151.54609513282776, 152.8659210205078, 154.1724991798401, 155.47907733917236, 156.78347873687744, 158.08788013458252, 159.40350079536438, 160.71912145614624, 162.0188376903534, 163.31855392456055, 164.70264649391174, 166.08673906326294, 167.416832447052, 168.74692583084106, 170.05394554138184, 171.3609652519226, 172.75634670257568, 174.15172815322876, 175.49405145645142, 176.83637475967407, 178.1552073955536, 179.4740400314331, 180.82662630081177, 182.17921257019043, 183.4968078136444, 184.8144030570984, 186.1857669353485, 187.55713081359863, 188.8799331188202, 190.20273542404175, 191.49279856681824, 192.78286170959473, 194.08120679855347, 195.3795518875122, 196.68379473686218, 197.98803758621216, 199.31184029579163, 200.6356430053711, 202.02459931373596, 203.41355562210083, 204.7982895374298, 206.1830234527588, 207.53483200073242, 208.88664054870605, 210.23707628250122, 211.5875120162964, 212.98008799552917, 214.37266397476196, 215.75104403495789, 217.1294240951538, 218.43696308135986, 219.74450206756592, 221.09765243530273, 222.45080280303955, 223.80755710601807, 225.16431140899658, 226.51750302314758, 227.87069463729858, 229.17158675193787, 230.47247886657715, 231.77844667434692, 233.0844144821167, 234.4159116744995, 235.74740886688232, 237.06367087364197, 238.3799328804016, 239.70566606521606, 241.03139925003052, 242.3345205783844, 243.63764190673828, 244.97503852844238, 246.31243515014648, 247.67077231407166, 249.02910947799683, 250.3794298171997, 251.7297501564026, 253.1313977241516, 254.53304529190063, 255.9354305267334, 257.33781576156616, 258.6857371330261, 260.0336585044861, 261.3781991004944, 262.7227396965027, 264.04863452911377, 265.37452936172485, 266.68539810180664, 267.9962668418884, 269.32754039764404, 270.65881395339966, 271.9969265460968, 273.33503913879395, 274.7095398902893, 276.08404064178467, 277.466037273407, 278.8480339050293, 280.22804975509644, 281.6080656051636, 282.99704670906067, 284.38602781295776, 286.65673661231995, 288.92744541168213]
[31.891666666666666, 31.891666666666666, 42.266666666666666, 42.266666666666666, 56.05, 56.05, 61.65, 61.65, 66.78333333333333, 66.78333333333333, 70.83333333333333, 70.83333333333333, 72.46666666666667, 72.46666666666667, 73.11666666666666, 73.11666666666666, 73.95, 73.95, 74.11666666666666, 74.11666666666666, 76.01666666666667, 76.01666666666667, 75.93333333333334, 75.93333333333334, 76.075, 76.075, 77.14166666666667, 77.14166666666667, 77.7, 77.7, 77.63333333333334, 77.63333333333334, 77.6, 77.6, 77.65, 77.65, 79.16666666666667, 79.16666666666667, 80.13333333333334, 80.13333333333334, 80.325, 80.325, 80.725, 80.725, 80.44166666666666, 80.44166666666666, 80.69166666666666, 80.69166666666666, 81.275, 81.275, 80.75833333333334, 80.75833333333334, 81.20833333333333, 81.20833333333333, 81.50833333333334, 81.50833333333334, 81.55833333333334, 81.55833333333334, 81.73333333333333, 81.73333333333333, 81.78333333333333, 81.78333333333333, 82.10833333333333, 82.10833333333333, 81.89166666666667, 81.89166666666667, 82.01666666666667, 82.01666666666667, 82.30833333333334, 82.30833333333334, 82.61666666666666, 82.61666666666666, 83.01666666666667, 83.01666666666667, 83.36666666666666, 83.36666666666666, 83.20833333333333, 83.20833333333333, 82.85, 82.85, 83.25833333333334, 83.25833333333334, 82.575, 82.575, 82.91666666666667, 82.91666666666667, 83.43333333333334, 83.43333333333334, 83.93333333333334, 83.93333333333334, 83.71666666666667, 83.71666666666667, 83.75833333333334, 83.75833333333334, 84.08333333333333, 84.08333333333333, 84.44166666666666, 84.44166666666666, 84.21666666666667, 84.21666666666667, 84.64166666666667, 84.64166666666667, 84.525, 84.525, 84.13333333333334, 84.13333333333334, 84.19166666666666, 84.19166666666666, 84.46666666666667, 84.46666666666667, 84.83333333333333, 84.83333333333333, 84.76666666666667, 84.76666666666667, 84.85, 84.85, 84.81666666666666, 84.81666666666666, 84.525, 84.525, 84.94166666666666, 84.94166666666666, 84.95, 84.95, 84.84166666666667, 84.84166666666667, 84.6, 84.6, 84.58333333333333, 84.58333333333333, 84.54166666666667, 84.54166666666667, 84.91666666666667, 84.91666666666667, 84.85833333333333, 84.85833333333333, 84.86666666666666, 84.86666666666666, 84.89166666666667, 84.89166666666667, 84.93333333333334, 84.93333333333334, 85.08333333333333, 85.08333333333333, 85.1, 85.1, 85.19166666666666, 85.19166666666666, 85.31666666666666, 85.31666666666666, 85.56666666666666, 85.56666666666666, 85.41666666666667, 85.41666666666667, 85.59166666666667, 85.59166666666667, 85.54166666666667, 85.54166666666667, 85.51666666666667, 85.51666666666667, 85.03333333333333, 85.03333333333333, 85.1, 85.1, 85.16666666666667, 85.16666666666667, 85.31666666666666, 85.31666666666666, 85.74166666666666, 85.74166666666666, 85.375, 85.375, 85.63333333333334, 85.63333333333334, 86.175, 86.175, 85.95, 85.95, 86.21666666666667, 86.21666666666667, 86.45, 86.45, 86.05833333333334, 86.05833333333334, 85.54166666666667, 85.54166666666667, 85.975, 85.975, 86.13333333333334, 86.13333333333334, 86.05, 86.05, 86.11666666666666, 86.11666666666666, 85.875, 85.875, 85.53333333333333, 85.53333333333333, 85.8, 85.8, 86.125, 86.125]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.388, Test loss: 3.143, Test accuracy: 19.22
Round   1, Train loss: 0.969, Test loss: 2.072, Test accuracy: 31.23
Round   2, Train loss: 0.850, Test loss: 1.394, Test accuracy: 44.66
Round   3, Train loss: 0.798, Test loss: 1.000, Test accuracy: 54.77
Round   4, Train loss: 0.762, Test loss: 1.303, Test accuracy: 56.81
Round   5, Train loss: 0.716, Test loss: 1.342, Test accuracy: 60.33
Round   6, Train loss: 0.681, Test loss: 1.019, Test accuracy: 62.45
Round   7, Train loss: 0.724, Test loss: 0.767, Test accuracy: 68.63
Round   8, Train loss: 0.706, Test loss: 0.666, Test accuracy: 71.78
Round   9, Train loss: 0.652, Test loss: 0.647, Test accuracy: 72.15
Round  10, Train loss: 0.588, Test loss: 0.632, Test accuracy: 73.37
Round  11, Train loss: 0.611, Test loss: 0.618, Test accuracy: 73.98
Round  12, Train loss: 0.643, Test loss: 0.583, Test accuracy: 74.52
Round  13, Train loss: 0.604, Test loss: 0.583, Test accuracy: 74.39
Round  14, Train loss: 0.591, Test loss: 0.578, Test accuracy: 74.89
Round  15, Train loss: 0.565, Test loss: 0.576, Test accuracy: 75.12
Round  16, Train loss: 0.551, Test loss: 0.558, Test accuracy: 76.69
Round  17, Train loss: 0.578, Test loss: 0.562, Test accuracy: 76.56
Round  18, Train loss: 0.542, Test loss: 0.546, Test accuracy: 77.21
Round  19, Train loss: 0.546, Test loss: 0.542, Test accuracy: 77.53
Round  20, Train loss: 0.525, Test loss: 0.514, Test accuracy: 78.48
Round  21, Train loss: 0.555, Test loss: 0.497, Test accuracy: 79.35
Round  22, Train loss: 0.469, Test loss: 0.487, Test accuracy: 79.69
Round  23, Train loss: 0.517, Test loss: 0.489, Test accuracy: 80.22
Round  24, Train loss: 0.520, Test loss: 0.477, Test accuracy: 80.46
Round  25, Train loss: 0.474, Test loss: 0.458, Test accuracy: 81.31
Round  26, Train loss: 0.536, Test loss: 0.445, Test accuracy: 81.58
Round  27, Train loss: 0.432, Test loss: 0.446, Test accuracy: 81.62
Round  28, Train loss: 0.462, Test loss: 0.447, Test accuracy: 81.53
Round  29, Train loss: 0.423, Test loss: 0.442, Test accuracy: 81.77
Round  30, Train loss: 0.495, Test loss: 0.435, Test accuracy: 82.04
Round  31, Train loss: 0.445, Test loss: 0.439, Test accuracy: 81.87
Round  32, Train loss: 0.413, Test loss: 0.436, Test accuracy: 82.15
Round  33, Train loss: 0.439, Test loss: 0.438, Test accuracy: 82.00
Round  34, Train loss: 0.403, Test loss: 0.432, Test accuracy: 82.33
Round  35, Train loss: 0.395, Test loss: 0.415, Test accuracy: 82.91
Round  36, Train loss: 0.405, Test loss: 0.408, Test accuracy: 83.31
Round  37, Train loss: 0.413, Test loss: 0.401, Test accuracy: 83.48
Round  38, Train loss: 0.423, Test loss: 0.400, Test accuracy: 84.08
Round  39, Train loss: 0.395, Test loss: 0.396, Test accuracy: 83.86
Round  40, Train loss: 0.418, Test loss: 0.393, Test accuracy: 84.14
Round  41, Train loss: 0.391, Test loss: 0.396, Test accuracy: 84.09
Round  42, Train loss: 0.400, Test loss: 0.398, Test accuracy: 84.25
Round  43, Train loss: 0.359, Test loss: 0.389, Test accuracy: 84.28
Round  44, Train loss: 0.400, Test loss: 0.387, Test accuracy: 84.55
Round  45, Train loss: 0.353, Test loss: 0.391, Test accuracy: 84.33
Round  46, Train loss: 0.376, Test loss: 0.389, Test accuracy: 84.63
Round  47, Train loss: 0.402, Test loss: 0.391, Test accuracy: 84.28
Round  48, Train loss: 0.389, Test loss: 0.391, Test accuracy: 84.77
Round  49, Train loss: 0.371, Test loss: 0.386, Test accuracy: 84.47
Round  50, Train loss: 0.367, Test loss: 0.385, Test accuracy: 84.67
Round  51, Train loss: 0.323, Test loss: 0.389, Test accuracy: 84.47
Round  52, Train loss: 0.305, Test loss: 0.381, Test accuracy: 84.60
Round  53, Train loss: 0.289, Test loss: 0.388, Test accuracy: 84.29
Round  54, Train loss: 0.325, Test loss: 0.380, Test accuracy: 84.49
Round  55, Train loss: 0.338, Test loss: 0.379, Test accuracy: 85.15
Round  56, Train loss: 0.323, Test loss: 0.381, Test accuracy: 84.79
Round  57, Train loss: 0.344, Test loss: 0.376, Test accuracy: 85.14
Round  58, Train loss: 0.359, Test loss: 0.366, Test accuracy: 85.33
Round  59, Train loss: 0.315, Test loss: 0.365, Test accuracy: 85.61
Round  60, Train loss: 0.301, Test loss: 0.362, Test accuracy: 85.82
Round  61, Train loss: 0.277, Test loss: 0.372, Test accuracy: 85.59
Round  62, Train loss: 0.328, Test loss: 0.371, Test accuracy: 85.26
Round  63, Train loss: 0.331, Test loss: 0.364, Test accuracy: 85.73
Round  64, Train loss: 0.315, Test loss: 0.363, Test accuracy: 85.50
Round  65, Train loss: 0.318, Test loss: 0.370, Test accuracy: 85.68
Round  66, Train loss: 0.303, Test loss: 0.357, Test accuracy: 86.08
Round  67, Train loss: 0.292, Test loss: 0.369, Test accuracy: 85.61
Round  68, Train loss: 0.269, Test loss: 0.365, Test accuracy: 85.91
Round  69, Train loss: 0.280, Test loss: 0.370, Test accuracy: 85.57
Round  70, Train loss: 0.310, Test loss: 0.360, Test accuracy: 86.21
Round  71, Train loss: 0.281, Test loss: 0.363, Test accuracy: 85.88
Round  72, Train loss: 0.284, Test loss: 0.360, Test accuracy: 86.11
Round  73, Train loss: 0.271, Test loss: 0.365, Test accuracy: 85.78
Round  74, Train loss: 0.304, Test loss: 0.363, Test accuracy: 86.01
Round  75, Train loss: 0.258, Test loss: 0.356, Test accuracy: 86.32
Round  76, Train loss: 0.266, Test loss: 0.375, Test accuracy: 85.68
Round  77, Train loss: 0.297, Test loss: 0.355, Test accuracy: 86.28
Round  78, Train loss: 0.276, Test loss: 0.346, Test accuracy: 86.93
Round  79, Train loss: 0.242, Test loss: 0.353, Test accuracy: 86.65
Round  80, Train loss: 0.314, Test loss: 0.358, Test accuracy: 86.40
Round  81, Train loss: 0.266, Test loss: 0.363, Test accuracy: 86.24
Round  82, Train loss: 0.283, Test loss: 0.361, Test accuracy: 86.42
Round  83, Train loss: 0.242, Test loss: 0.361, Test accuracy: 86.47
Round  84, Train loss: 0.274, Test loss: 0.352, Test accuracy: 86.37
Round  85, Train loss: 0.297, Test loss: 0.344, Test accuracy: 86.79
Round  86, Train loss: 0.254, Test loss: 0.351, Test accuracy: 86.72
Round  87, Train loss: 0.252, Test loss: 0.351, Test accuracy: 86.50
Round  88, Train loss: 0.264, Test loss: 0.349, Test accuracy: 86.68
Round  89, Train loss: 0.254, Test loss: 0.343, Test accuracy: 87.08
Round  90, Train loss: 0.235, Test loss: 0.356, Test accuracy: 86.72
Round  91, Train loss: 0.215, Test loss: 0.345, Test accuracy: 87.06
Round  92, Train loss: 0.288, Test loss: 0.339, Test accuracy: 87.22
Round  93, Train loss: 0.246, Test loss: 0.340, Test accuracy: 87.00
Round  94, Train loss: 0.243, Test loss: 0.344, Test accuracy: 86.99
Round  95, Train loss: 0.236, Test loss: 0.341, Test accuracy: 87.06
Round  96, Train loss: 0.236, Test loss: 0.353, Test accuracy: 86.72
Round  97, Train loss: 0.227, Test loss: 0.354, Test accuracy: 87.02
Round  98, Train loss: 0.248, Test loss: 0.351, Test accuracy: 87.13
Round  99, Train loss: 0.242, Test loss: 0.344, Test accuracy: 87.29
Final Round, Train loss: 0.195, Test loss: 0.349, Test accuracy: 87.41
Average accuracy final 10 rounds: 87.02166666666668 

1640.9574570655823
[1.66652512550354, 3.33305025100708, 4.718442678451538, 6.103835105895996, 7.502389907836914, 8.900944709777832, 10.287222862243652, 11.673501014709473, 13.052359819412231, 14.43121862411499, 15.82322382926941, 17.215229034423828, 18.639518976211548, 20.063808917999268, 21.46874165534973, 22.873674392700195, 24.263269901275635, 25.652865409851074, 27.050727367401123, 28.448589324951172, 29.826393365859985, 31.2041974067688, 32.62135934829712, 34.03852128982544, 35.45658993721008, 36.87465858459473, 38.26702332496643, 39.659388065338135, 41.0392849445343, 42.41918182373047, 43.79767322540283, 45.176164627075195, 46.56594800949097, 47.95573139190674, 49.347431659698486, 50.739131927490234, 52.11398410797119, 53.48883628845215, 54.866559743881226, 56.2442831993103, 57.64557433128357, 59.046865463256836, 60.42514967918396, 61.803433895111084, 63.19302821159363, 64.58262252807617, 65.97957611083984, 67.37652969360352, 68.77682542800903, 70.17712116241455, 71.54760813713074, 72.91809511184692, 74.31689167022705, 75.71568822860718, 77.1205472946167, 78.52540636062622, 79.93718767166138, 81.34896898269653, 82.75002765655518, 84.15108633041382, 85.52698636054993, 86.90288639068604, 88.31992983818054, 89.73697328567505, 91.15456962585449, 92.57216596603394, 94.00080585479736, 95.42944574356079, 96.83288431167603, 98.23632287979126, 99.64053392410278, 101.0447449684143, 102.44137740135193, 103.83800983428955, 105.23760437965393, 106.63719892501831, 108.01738286018372, 109.39756679534912, 110.77924108505249, 112.16091537475586, 113.56964921951294, 114.97838306427002, 116.38735723495483, 117.79633140563965, 119.19689750671387, 120.59746360778809, 122.0036051273346, 123.4097466468811, 124.8216142654419, 126.23348188400269, 127.62864375114441, 129.02380561828613, 130.41444897651672, 131.80509233474731, 133.1882209777832, 134.5713496208191, 135.96907711029053, 137.36680459976196, 138.7508044242859, 140.13480424880981, 141.53487873077393, 142.93495321273804, 144.32007336616516, 145.70519351959229, 147.12056684494019, 148.5359401702881, 149.93492460250854, 151.333909034729, 152.72484302520752, 154.11577701568604, 155.53371214866638, 156.95164728164673, 158.35958290100098, 159.76751852035522, 161.1694107055664, 162.5713028907776, 163.9733452796936, 165.37538766860962, 166.76693272590637, 168.15847778320312, 169.53668570518494, 170.91489362716675, 172.332994222641, 173.75109481811523, 175.1579360961914, 176.56477737426758, 177.9617714881897, 179.35876560211182, 180.76330256462097, 182.16783952713013, 183.57696866989136, 184.9860978126526, 186.3904242515564, 187.7947506904602, 189.20402717590332, 190.61330366134644, 192.0269901752472, 193.44067668914795, 194.8541979789734, 196.26771926879883, 197.6548113822937, 199.04190349578857, 200.43567085266113, 201.8294382095337, 203.2317304611206, 204.63402271270752, 206.031263589859, 207.4285044670105, 208.82893538475037, 210.22936630249023, 211.67764115333557, 213.1259160041809, 214.5380539894104, 215.9501919746399, 217.3504283428192, 218.75066471099854, 220.16112685203552, 221.5715889930725, 222.96149897575378, 224.35140895843506, 225.7418975830078, 227.13238620758057, 228.54161620140076, 229.95084619522095, 231.3500030040741, 232.74915981292725, 234.13466215133667, 235.5201644897461, 236.8124189376831, 238.10467338562012, 239.6147584915161, 241.1248435974121, 242.60374808311462, 244.08265256881714, 245.502290725708, 246.92192888259888, 248.35836267471313, 249.7947964668274, 251.20346903800964, 252.6121416091919, 254.07673716545105, 255.5413327217102, 257.032173871994, 258.52301502227783, 259.96449637413025, 261.40597772598267, 262.89492630958557, 264.3838748931885, 265.8800618648529, 267.37624883651733, 268.8311047554016, 270.2859606742859, 271.742981672287, 273.2000026702881, 274.6848318576813, 276.16966104507446, 277.6364862918854, 279.1033115386963, 280.5585095882416, 282.01370763778687, 284.2301948070526, 286.44668197631836]
[19.216666666666665, 19.216666666666665, 31.225, 31.225, 44.65833333333333, 44.65833333333333, 54.775, 54.775, 56.80833333333333, 56.80833333333333, 60.325, 60.325, 62.45, 62.45, 68.63333333333334, 68.63333333333334, 71.78333333333333, 71.78333333333333, 72.15, 72.15, 73.36666666666666, 73.36666666666666, 73.98333333333333, 73.98333333333333, 74.51666666666667, 74.51666666666667, 74.39166666666667, 74.39166666666667, 74.89166666666667, 74.89166666666667, 75.125, 75.125, 76.69166666666666, 76.69166666666666, 76.55833333333334, 76.55833333333334, 77.20833333333333, 77.20833333333333, 77.53333333333333, 77.53333333333333, 78.48333333333333, 78.48333333333333, 79.35, 79.35, 79.69166666666666, 79.69166666666666, 80.21666666666667, 80.21666666666667, 80.45833333333333, 80.45833333333333, 81.30833333333334, 81.30833333333334, 81.58333333333333, 81.58333333333333, 81.625, 81.625, 81.53333333333333, 81.53333333333333, 81.76666666666667, 81.76666666666667, 82.04166666666667, 82.04166666666667, 81.86666666666666, 81.86666666666666, 82.15, 82.15, 82.0, 82.0, 82.33333333333333, 82.33333333333333, 82.90833333333333, 82.90833333333333, 83.30833333333334, 83.30833333333334, 83.48333333333333, 83.48333333333333, 84.08333333333333, 84.08333333333333, 83.85833333333333, 83.85833333333333, 84.14166666666667, 84.14166666666667, 84.09166666666667, 84.09166666666667, 84.25, 84.25, 84.28333333333333, 84.28333333333333, 84.55, 84.55, 84.33333333333333, 84.33333333333333, 84.63333333333334, 84.63333333333334, 84.28333333333333, 84.28333333333333, 84.76666666666667, 84.76666666666667, 84.46666666666667, 84.46666666666667, 84.66666666666667, 84.66666666666667, 84.475, 84.475, 84.6, 84.6, 84.29166666666667, 84.29166666666667, 84.49166666666666, 84.49166666666666, 85.15, 85.15, 84.79166666666667, 84.79166666666667, 85.14166666666667, 85.14166666666667, 85.33333333333333, 85.33333333333333, 85.60833333333333, 85.60833333333333, 85.81666666666666, 85.81666666666666, 85.59166666666667, 85.59166666666667, 85.25833333333334, 85.25833333333334, 85.73333333333333, 85.73333333333333, 85.5, 85.5, 85.68333333333334, 85.68333333333334, 86.075, 86.075, 85.60833333333333, 85.60833333333333, 85.90833333333333, 85.90833333333333, 85.56666666666666, 85.56666666666666, 86.20833333333333, 86.20833333333333, 85.875, 85.875, 86.10833333333333, 86.10833333333333, 85.775, 85.775, 86.00833333333334, 86.00833333333334, 86.31666666666666, 86.31666666666666, 85.68333333333334, 85.68333333333334, 86.28333333333333, 86.28333333333333, 86.93333333333334, 86.93333333333334, 86.65, 86.65, 86.4, 86.4, 86.24166666666666, 86.24166666666666, 86.425, 86.425, 86.475, 86.475, 86.36666666666666, 86.36666666666666, 86.79166666666667, 86.79166666666667, 86.725, 86.725, 86.5, 86.5, 86.68333333333334, 86.68333333333334, 87.08333333333333, 87.08333333333333, 86.725, 86.725, 87.05833333333334, 87.05833333333334, 87.225, 87.225, 87.0, 87.0, 86.99166666666666, 86.99166666666666, 87.05833333333334, 87.05833333333334, 86.71666666666667, 86.71666666666667, 87.01666666666667, 87.01666666666667, 87.13333333333334, 87.13333333333334, 87.29166666666667, 87.29166666666667, 87.40833333333333, 87.40833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.045, Test loss: 2.023, Test accuracy: 25.83
Round   1, Train loss: 0.855, Test loss: 1.635, Test accuracy: 41.48
Round   2, Train loss: 0.821, Test loss: 1.303, Test accuracy: 50.75
Round   3, Train loss: 0.784, Test loss: 1.264, Test accuracy: 51.97
Round   4, Train loss: 0.703, Test loss: 1.020, Test accuracy: 60.05
Round   5, Train loss: 0.684, Test loss: 0.895, Test accuracy: 62.36
Round   6, Train loss: 0.602, Test loss: 0.853, Test accuracy: 65.87
Round   7, Train loss: 0.590, Test loss: 0.763, Test accuracy: 68.08
Round   8, Train loss: 0.610, Test loss: 0.670, Test accuracy: 71.43
Round   9, Train loss: 0.613, Test loss: 0.635, Test accuracy: 74.69
Round  10, Train loss: 0.532, Test loss: 0.603, Test accuracy: 75.32
Round  11, Train loss: 0.583, Test loss: 0.615, Test accuracy: 75.36
Round  12, Train loss: 0.536, Test loss: 0.586, Test accuracy: 76.29
Round  13, Train loss: 0.538, Test loss: 0.579, Test accuracy: 76.26
Round  14, Train loss: 0.569, Test loss: 0.572, Test accuracy: 76.39
Round  15, Train loss: 0.509, Test loss: 0.562, Test accuracy: 77.41
Round  16, Train loss: 0.516, Test loss: 0.540, Test accuracy: 78.11
Round  17, Train loss: 0.537, Test loss: 0.489, Test accuracy: 80.79
Round  18, Train loss: 0.516, Test loss: 0.496, Test accuracy: 80.25
Round  19, Train loss: 0.486, Test loss: 0.499, Test accuracy: 80.10
Round  20, Train loss: 0.468, Test loss: 0.507, Test accuracy: 79.52
Round  21, Train loss: 0.440, Test loss: 0.517, Test accuracy: 78.92
Round  22, Train loss: 0.424, Test loss: 0.487, Test accuracy: 80.47
Round  23, Train loss: 0.463, Test loss: 0.475, Test accuracy: 81.45
Round  24, Train loss: 0.417, Test loss: 0.487, Test accuracy: 80.21
Round  25, Train loss: 0.418, Test loss: 0.465, Test accuracy: 81.68
Round  26, Train loss: 0.451, Test loss: 0.454, Test accuracy: 81.78
Round  27, Train loss: 0.453, Test loss: 0.457, Test accuracy: 81.90
Round  28, Train loss: 0.402, Test loss: 0.447, Test accuracy: 82.05
Round  29, Train loss: 0.363, Test loss: 0.427, Test accuracy: 83.08
Round  30, Train loss: 0.396, Test loss: 0.436, Test accuracy: 82.55
Round  31, Train loss: 0.391, Test loss: 0.434, Test accuracy: 82.67
Round  32, Train loss: 0.377, Test loss: 0.430, Test accuracy: 83.03
Round  33, Train loss: 0.376, Test loss: 0.423, Test accuracy: 83.32
Round  34, Train loss: 0.348, Test loss: 0.430, Test accuracy: 83.19
Round  35, Train loss: 0.339, Test loss: 0.425, Test accuracy: 83.53
Round  36, Train loss: 0.361, Test loss: 0.430, Test accuracy: 83.23
Round  37, Train loss: 0.358, Test loss: 0.419, Test accuracy: 83.89
Round  38, Train loss: 0.354, Test loss: 0.410, Test accuracy: 84.08
Round  39, Train loss: 0.359, Test loss: 0.415, Test accuracy: 83.97
Round  40, Train loss: 0.315, Test loss: 0.420, Test accuracy: 83.94
Round  41, Train loss: 0.305, Test loss: 0.420, Test accuracy: 83.79
Round  42, Train loss: 0.306, Test loss: 0.412, Test accuracy: 83.92
Round  43, Train loss: 0.314, Test loss: 0.421, Test accuracy: 83.78
Round  44, Train loss: 0.301, Test loss: 0.411, Test accuracy: 84.36
Round  45, Train loss: 0.298, Test loss: 0.408, Test accuracy: 84.38
Round  46, Train loss: 0.330, Test loss: 0.408, Test accuracy: 84.28
Round  47, Train loss: 0.294, Test loss: 0.407, Test accuracy: 84.65
Round  48, Train loss: 0.300, Test loss: 0.398, Test accuracy: 84.76
Round  49, Train loss: 0.290, Test loss: 0.403, Test accuracy: 84.38
Round  50, Train loss: 0.274, Test loss: 0.413, Test accuracy: 84.29
Round  51, Train loss: 0.291, Test loss: 0.396, Test accuracy: 84.76
Round  52, Train loss: 0.272, Test loss: 0.399, Test accuracy: 84.48
Round  53, Train loss: 0.234, Test loss: 0.402, Test accuracy: 84.89
Round  54, Train loss: 0.268, Test loss: 0.396, Test accuracy: 84.93
Round  55, Train loss: 0.269, Test loss: 0.396, Test accuracy: 84.79
Round  56, Train loss: 0.257, Test loss: 0.394, Test accuracy: 85.04
Round  57, Train loss: 0.240, Test loss: 0.414, Test accuracy: 84.58
Round  58, Train loss: 0.236, Test loss: 0.410, Test accuracy: 84.83
Round  59, Train loss: 0.261, Test loss: 0.411, Test accuracy: 84.37
Round  60, Train loss: 0.220, Test loss: 0.401, Test accuracy: 85.11
Round  61, Train loss: 0.228, Test loss: 0.400, Test accuracy: 85.03
Round  62, Train loss: 0.229, Test loss: 0.413, Test accuracy: 84.96
Round  63, Train loss: 0.218, Test loss: 0.415, Test accuracy: 84.77
Round  64, Train loss: 0.223, Test loss: 0.407, Test accuracy: 85.39
Round  65, Train loss: 0.238, Test loss: 0.400, Test accuracy: 85.22
Round  66, Train loss: 0.216, Test loss: 0.416, Test accuracy: 85.05
Round  67, Train loss: 0.241, Test loss: 0.406, Test accuracy: 84.99
Round  68, Train loss: 0.215, Test loss: 0.415, Test accuracy: 84.97
Round  69, Train loss: 0.205, Test loss: 0.419, Test accuracy: 85.29
Round  70, Train loss: 0.228, Test loss: 0.417, Test accuracy: 85.08
Round  71, Train loss: 0.188, Test loss: 0.419, Test accuracy: 85.28
Round  72, Train loss: 0.235, Test loss: 0.410, Test accuracy: 85.22
Round  73, Train loss: 0.202, Test loss: 0.406, Test accuracy: 85.67
Round  74, Train loss: 0.189, Test loss: 0.408, Test accuracy: 85.69
Round  75, Train loss: 0.190, Test loss: 0.423, Test accuracy: 85.32
Round  76, Train loss: 0.173, Test loss: 0.417, Test accuracy: 85.59
Round  77, Train loss: 0.183, Test loss: 0.404, Test accuracy: 85.58
Round  78, Train loss: 0.164, Test loss: 0.416, Test accuracy: 85.91
Round  79, Train loss: 0.171, Test loss: 0.413, Test accuracy: 85.71
Round  80, Train loss: 0.186, Test loss: 0.409, Test accuracy: 85.59
Round  81, Train loss: 0.189, Test loss: 0.415, Test accuracy: 85.58
Round  82, Train loss: 0.161, Test loss: 0.423, Test accuracy: 85.57
Round  83, Train loss: 0.188, Test loss: 0.421, Test accuracy: 85.71
Round  84, Train loss: 0.199, Test loss: 0.431, Test accuracy: 85.17
Round  85, Train loss: 0.175, Test loss: 0.447, Test accuracy: 85.12
Round  86, Train loss: 0.177, Test loss: 0.419, Test accuracy: 85.66
Round  87, Train loss: 0.200, Test loss: 0.422, Test accuracy: 85.30
Round  88, Train loss: 0.183, Test loss: 0.425, Test accuracy: 85.62
Round  89, Train loss: 0.162, Test loss: 0.422, Test accuracy: 85.58
Round  90, Train loss: 0.155, Test loss: 0.444, Test accuracy: 85.56
Round  91, Train loss: 0.170, Test loss: 0.435, Test accuracy: 85.27
Round  92, Train loss: 0.159, Test loss: 0.444, Test accuracy: 85.30
Round  93, Train loss: 0.151, Test loss: 0.443, Test accuracy: 85.60
Round  94, Train loss: 0.182, Test loss: 0.445, Test accuracy: 85.18
Round  95, Train loss: 0.157, Test loss: 0.426, Test accuracy: 85.64
Round  96, Train loss: 0.172, Test loss: 0.439, Test accuracy: 85.39
Round  97, Train loss: 0.161, Test loss: 0.442, Test accuracy: 85.52
Round  98, Train loss: 0.128, Test loss: 0.439, Test accuracy: 85.70
Round  99, Train loss: 0.149, Test loss: 0.455, Test accuracy: 85.83
Final Round, Train loss: 0.131, Test loss: 0.426, Test accuracy: 86.47
Average accuracy final 10 rounds: 85.49916666666665 

1596.2515895366669
[1.8182990550994873, 3.6365981101989746, 5.1313157081604, 6.626033306121826, 8.112227439880371, 9.598421573638916, 10.93974232673645, 12.281063079833984, 13.624683141708374, 14.968303203582764, 16.316726446151733, 17.665149688720703, 18.999946355819702, 20.3347430229187, 21.674285888671875, 23.01382875442505, 24.32631278038025, 25.63879680633545, 26.980874061584473, 28.322951316833496, 29.659719705581665, 30.996488094329834, 32.36190438270569, 33.72732067108154, 35.06371068954468, 36.40010070800781, 37.736780405044556, 39.0734601020813, 40.41806197166443, 41.76266384124756, 43.10063552856445, 44.43860721588135, 45.74970984458923, 47.06081247329712, 48.390660524368286, 49.72050857543945, 51.05984687805176, 52.39918518066406, 53.744592905044556, 55.09000062942505, 56.454877614974976, 57.8197546005249, 59.15774726867676, 60.49573993682861, 61.82194542884827, 63.14815092086792, 64.49000549316406, 65.8318600654602, 67.16164898872375, 68.4914379119873, 69.82636332511902, 71.16128873825073, 72.48781967163086, 73.81435060501099, 75.15477347373962, 76.49519634246826, 77.8426775932312, 79.19015884399414, 80.542555809021, 81.89495277404785, 83.2237195968628, 84.55248641967773, 85.89137554168701, 87.23026466369629, 88.55821251869202, 89.88616037368774, 91.22008419036865, 92.55400800704956, 93.88095688819885, 95.20790576934814, 96.56464457511902, 97.92138338088989, 99.27015018463135, 100.6189169883728, 101.98279881477356, 103.34668064117432, 104.69870972633362, 106.05073881149292, 107.38917303085327, 108.72760725021362, 110.06484961509705, 111.40209197998047, 112.73194074630737, 114.06178951263428, 115.38381767272949, 116.7058458328247, 118.02639174461365, 119.34693765640259, 120.68366241455078, 122.02038717269897, 123.3566951751709, 124.69300317764282, 126.02944660186768, 127.36589002609253, 128.69699120521545, 130.02809238433838, 131.364759683609, 132.70142698287964, 134.04224514961243, 135.38306331634521, 136.70252323150635, 138.02198314666748, 139.35009670257568, 140.6782102584839, 142.02608132362366, 143.37395238876343, 144.71445751190186, 146.05496263504028, 147.4038360118866, 148.7527093887329, 150.09504103660583, 151.43737268447876, 152.78452682495117, 154.13168096542358, 155.46617889404297, 156.80067682266235, 158.13713264465332, 159.4735884666443, 160.79372906684875, 162.11386966705322, 163.44884133338928, 164.78381299972534, 166.117573261261, 167.45133352279663, 168.77946591377258, 170.10759830474854, 171.44903540611267, 172.7904725074768, 174.14471793174744, 175.49896335601807, 176.84780287742615, 178.19664239883423, 179.54235911369324, 180.88807582855225, 182.2273097038269, 183.56654357910156, 184.91293168067932, 186.25931978225708, 187.62043690681458, 188.98155403137207, 190.32419109344482, 191.66682815551758, 193.00712943077087, 194.34743070602417, 195.68326330184937, 197.01909589767456, 198.3707616329193, 199.72242736816406, 201.0608937740326, 202.39936017990112, 203.7158386707306, 205.03231716156006, 206.3543074131012, 207.67629766464233, 209.00572228431702, 210.3351469039917, 211.69592809677124, 213.05670928955078, 214.42483806610107, 215.79296684265137, 217.15724277496338, 218.5215187072754, 219.87818717956543, 221.23485565185547, 222.5988004207611, 223.96274518966675, 225.2924826145172, 226.62222003936768, 227.9693238735199, 229.31642770767212, 230.6697051525116, 232.02298259735107, 233.38589358329773, 234.74880456924438, 236.09129881858826, 237.43379306793213, 238.778826713562, 240.1238603591919, 241.48061966896057, 242.83737897872925, 244.1762228012085, 245.51506662368774, 246.86288213729858, 248.21069765090942, 249.5544364452362, 250.898175239563, 252.20173954963684, 253.5053038597107, 254.8219039440155, 256.1385040283203, 257.48839831352234, 258.83829259872437, 260.2149646282196, 261.59163665771484, 262.96752738952637, 264.3434181213379, 265.7100098133087, 267.07660150527954, 268.49720287323, 269.9178042411804, 272.06560802459717, 274.2134118080139]
[25.833333333333332, 25.833333333333332, 41.483333333333334, 41.483333333333334, 50.75, 50.75, 51.96666666666667, 51.96666666666667, 60.05, 60.05, 62.358333333333334, 62.358333333333334, 65.86666666666666, 65.86666666666666, 68.075, 68.075, 71.43333333333334, 71.43333333333334, 74.69166666666666, 74.69166666666666, 75.31666666666666, 75.31666666666666, 75.35833333333333, 75.35833333333333, 76.29166666666667, 76.29166666666667, 76.25833333333334, 76.25833333333334, 76.39166666666667, 76.39166666666667, 77.40833333333333, 77.40833333333333, 78.10833333333333, 78.10833333333333, 80.79166666666667, 80.79166666666667, 80.25, 80.25, 80.1, 80.1, 79.51666666666667, 79.51666666666667, 78.91666666666667, 78.91666666666667, 80.475, 80.475, 81.45, 81.45, 80.20833333333333, 80.20833333333333, 81.68333333333334, 81.68333333333334, 81.775, 81.775, 81.9, 81.9, 82.05, 82.05, 83.08333333333333, 83.08333333333333, 82.55, 82.55, 82.66666666666667, 82.66666666666667, 83.025, 83.025, 83.31666666666666, 83.31666666666666, 83.19166666666666, 83.19166666666666, 83.525, 83.525, 83.23333333333333, 83.23333333333333, 83.89166666666667, 83.89166666666667, 84.075, 84.075, 83.96666666666667, 83.96666666666667, 83.94166666666666, 83.94166666666666, 83.79166666666667, 83.79166666666667, 83.925, 83.925, 83.78333333333333, 83.78333333333333, 84.35833333333333, 84.35833333333333, 84.375, 84.375, 84.28333333333333, 84.28333333333333, 84.65, 84.65, 84.75833333333334, 84.75833333333334, 84.375, 84.375, 84.29166666666667, 84.29166666666667, 84.75833333333334, 84.75833333333334, 84.48333333333333, 84.48333333333333, 84.89166666666667, 84.89166666666667, 84.93333333333334, 84.93333333333334, 84.79166666666667, 84.79166666666667, 85.04166666666667, 85.04166666666667, 84.575, 84.575, 84.825, 84.825, 84.36666666666666, 84.36666666666666, 85.10833333333333, 85.10833333333333, 85.025, 85.025, 84.95833333333333, 84.95833333333333, 84.76666666666667, 84.76666666666667, 85.39166666666667, 85.39166666666667, 85.225, 85.225, 85.05, 85.05, 84.99166666666666, 84.99166666666666, 84.975, 84.975, 85.29166666666667, 85.29166666666667, 85.075, 85.075, 85.275, 85.275, 85.225, 85.225, 85.66666666666667, 85.66666666666667, 85.69166666666666, 85.69166666666666, 85.31666666666666, 85.31666666666666, 85.59166666666667, 85.59166666666667, 85.58333333333333, 85.58333333333333, 85.90833333333333, 85.90833333333333, 85.70833333333333, 85.70833333333333, 85.59166666666667, 85.59166666666667, 85.58333333333333, 85.58333333333333, 85.56666666666666, 85.56666666666666, 85.70833333333333, 85.70833333333333, 85.16666666666667, 85.16666666666667, 85.125, 85.125, 85.65833333333333, 85.65833333333333, 85.3, 85.3, 85.61666666666666, 85.61666666666666, 85.575, 85.575, 85.55833333333334, 85.55833333333334, 85.26666666666667, 85.26666666666667, 85.3, 85.3, 85.6, 85.6, 85.18333333333334, 85.18333333333334, 85.64166666666667, 85.64166666666667, 85.39166666666667, 85.39166666666667, 85.51666666666667, 85.51666666666667, 85.7, 85.7, 85.83333333333333, 85.83333333333333, 86.46666666666667, 86.46666666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.987, Test loss: 1.693, Test accuracy: 39.30
Round   1, Train loss: 1.665, Test loss: 1.523, Test accuracy: 44.41
Round   2, Train loss: 1.532, Test loss: 1.444, Test accuracy: 48.25
Round   3, Train loss: 1.444, Test loss: 1.413, Test accuracy: 49.63
Round   4, Train loss: 1.363, Test loss: 1.375, Test accuracy: 51.47
Round   5, Train loss: 1.377, Test loss: 1.373, Test accuracy: 51.48
Round   6, Train loss: 1.271, Test loss: 1.373, Test accuracy: 51.65
Round   7, Train loss: 1.174, Test loss: 1.367, Test accuracy: 52.46
Round   8, Train loss: 1.161, Test loss: 1.372, Test accuracy: 52.49
Round   9, Train loss: 1.108, Test loss: 1.359, Test accuracy: 53.47
Round  10, Train loss: 1.029, Test loss: 1.361, Test accuracy: 53.56
Round  11, Train loss: 1.048, Test loss: 1.347, Test accuracy: 54.15
Round  12, Train loss: 0.982, Test loss: 1.355, Test accuracy: 54.45
Round  13, Train loss: 1.258, Test loss: 1.316, Test accuracy: 54.70
Round  14, Train loss: 0.958, Test loss: 1.323, Test accuracy: 55.52
Round  15, Train loss: 1.066, Test loss: 1.337, Test accuracy: 55.46
Round  16, Train loss: 0.915, Test loss: 1.342, Test accuracy: 55.71
Round  17, Train loss: 0.910, Test loss: 1.361, Test accuracy: 55.85
Round  18, Train loss: 0.894, Test loss: 1.372, Test accuracy: 55.69
Round  19, Train loss: 0.772, Test loss: 1.391, Test accuracy: 55.57
Round  20, Train loss: 0.730, Test loss: 1.389, Test accuracy: 56.16
Round  21, Train loss: 0.782, Test loss: 1.396, Test accuracy: 55.90
Round  22, Train loss: 0.737, Test loss: 1.390, Test accuracy: 56.49
Round  23, Train loss: 0.947, Test loss: 1.383, Test accuracy: 56.55
Round  24, Train loss: 0.816, Test loss: 1.424, Test accuracy: 56.51
Round  25, Train loss: 0.649, Test loss: 1.430, Test accuracy: 56.88
Round  26, Train loss: 0.724, Test loss: 1.448, Test accuracy: 56.62
Round  27, Train loss: 0.658, Test loss: 1.445, Test accuracy: 56.72
Round  28, Train loss: 0.600, Test loss: 1.476, Test accuracy: 56.69
Round  29, Train loss: 0.738, Test loss: 1.473, Test accuracy: 57.17
Round  30, Train loss: 0.624, Test loss: 1.518, Test accuracy: 56.38
Round  31, Train loss: 0.611, Test loss: 1.505, Test accuracy: 56.79
Round  32, Train loss: 0.643, Test loss: 1.522, Test accuracy: 56.80
Round  33, Train loss: 0.593, Test loss: 1.561, Test accuracy: 56.75
Round  34, Train loss: 0.584, Test loss: 1.549, Test accuracy: 57.04
Round  35, Train loss: 0.607, Test loss: 1.577, Test accuracy: 57.02
Round  36, Train loss: 0.446, Test loss: 1.594, Test accuracy: 57.12
Round  37, Train loss: 0.548, Test loss: 1.612, Test accuracy: 56.77
Round  38, Train loss: 0.601, Test loss: 1.609, Test accuracy: 56.98
Round  39, Train loss: 0.492, Test loss: 1.632, Test accuracy: 57.23
Round  40, Train loss: 0.514, Test loss: 1.643, Test accuracy: 57.05
Round  41, Train loss: 0.481, Test loss: 1.657, Test accuracy: 57.07
Round  42, Train loss: 0.495, Test loss: 1.680, Test accuracy: 56.99
Round  43, Train loss: 0.459, Test loss: 1.724, Test accuracy: 57.12
Round  44, Train loss: 0.453, Test loss: 1.714, Test accuracy: 57.22
Round  45, Train loss: 0.470, Test loss: 1.738, Test accuracy: 57.05
Round  46, Train loss: 0.423, Test loss: 1.748, Test accuracy: 57.15
Round  47, Train loss: 0.415, Test loss: 1.742, Test accuracy: 56.98
Round  48, Train loss: 0.383, Test loss: 1.737, Test accuracy: 57.49
Round  49, Train loss: 0.452, Test loss: 1.761, Test accuracy: 57.55
Round  50, Train loss: 0.363, Test loss: 1.748, Test accuracy: 57.89
Round  51, Train loss: 0.368, Test loss: 1.801, Test accuracy: 57.71
Round  52, Train loss: 0.369, Test loss: 1.801, Test accuracy: 57.56
Round  53, Train loss: 0.359, Test loss: 1.833, Test accuracy: 57.57
Round  54, Train loss: 0.324, Test loss: 1.853, Test accuracy: 57.59
Round  55, Train loss: 0.333, Test loss: 1.838, Test accuracy: 57.76
Round  56, Train loss: 0.384, Test loss: 1.857, Test accuracy: 58.13
Round  57, Train loss: 0.347, Test loss: 1.863, Test accuracy: 58.03
Round  58, Train loss: 0.308, Test loss: 1.869, Test accuracy: 58.04
Round  59, Train loss: 0.339, Test loss: 1.905, Test accuracy: 57.79
Round  60, Train loss: 0.334, Test loss: 1.911, Test accuracy: 57.69
Round  61, Train loss: 0.313, Test loss: 1.927, Test accuracy: 57.78
Round  62, Train loss: 0.303, Test loss: 1.944, Test accuracy: 57.79
Round  63, Train loss: 0.310, Test loss: 1.941, Test accuracy: 58.13
Round  64, Train loss: 0.276, Test loss: 1.932, Test accuracy: 57.90
Round  65, Train loss: 0.288, Test loss: 1.953, Test accuracy: 57.82
Round  66, Train loss: 0.293, Test loss: 2.000, Test accuracy: 58.02
Round  67, Train loss: 0.260, Test loss: 1.987, Test accuracy: 57.73
Round  68, Train loss: 0.308, Test loss: 2.008, Test accuracy: 57.73
Round  69, Train loss: 0.290, Test loss: 2.043, Test accuracy: 57.77
Round  70, Train loss: 0.286, Test loss: 2.038, Test accuracy: 57.87
Round  71, Train loss: 0.254, Test loss: 2.086, Test accuracy: 57.46
Round  72, Train loss: 0.274, Test loss: 2.070, Test accuracy: 57.51
Round  73, Train loss: 0.257, Test loss: 2.073, Test accuracy: 57.72
Round  74, Train loss: 0.238, Test loss: 2.082, Test accuracy: 57.69
Round  75, Train loss: 0.245, Test loss: 2.098, Test accuracy: 57.70
Round  76, Train loss: 0.230, Test loss: 2.112, Test accuracy: 57.91
Round  77, Train loss: 0.228, Test loss: 2.137, Test accuracy: 57.87
Round  78, Train loss: 0.254, Test loss: 2.118, Test accuracy: 57.69
Round  79, Train loss: 0.227, Test loss: 2.133, Test accuracy: 58.16
Round  80, Train loss: 0.250, Test loss: 2.155, Test accuracy: 57.91
Round  81, Train loss: 0.229, Test loss: 2.127, Test accuracy: 57.98
Round  82, Train loss: 0.212, Test loss: 2.122, Test accuracy: 57.87
Round  83, Train loss: 0.214, Test loss: 2.141, Test accuracy: 57.98
Round  84, Train loss: 0.211, Test loss: 2.202, Test accuracy: 58.02
Round  85, Train loss: 0.203, Test loss: 2.210, Test accuracy: 58.00
Round  86, Train loss: 0.186, Test loss: 2.205, Test accuracy: 57.99
Round  87, Train loss: 0.216, Test loss: 2.201, Test accuracy: 58.00
Round  88, Train loss: 0.209, Test loss: 2.176, Test accuracy: 57.85
Round  89, Train loss: 0.191, Test loss: 2.196, Test accuracy: 57.93
Round  90, Train loss: 0.204, Test loss: 2.216, Test accuracy: 58.18
Round  91, Train loss: 0.193, Test loss: 2.268, Test accuracy: 58.14
Round  92, Train loss: 0.204, Test loss: 2.307, Test accuracy: 58.34
Round  93, Train loss: 0.179, Test loss: 2.226, Test accuracy: 58.10
Round  94, Train loss: 0.185, Test loss: 2.263, Test accuracy: 58.19
Round  95, Train loss: 0.180, Test loss: 2.275, Test accuracy: 57.87
Round  96, Train loss: 0.200, Test loss: 2.280, Test accuracy: 58.15
Round  97, Train loss: 0.216, Test loss: 2.298, Test accuracy: 58.30
Round  98, Train loss: 0.200, Test loss: 2.329, Test accuracy: 58.41
Round  99, Train loss: 0.188, Test loss: 2.292, Test accuracy: 58.63
Final Round, Train loss: 0.104, Test loss: 2.389, Test accuracy: 58.66
Average accuracy final 10 rounds: 58.23074999999999 

5375.416367530823
[4.840437412261963, 9.680874824523926, 14.140401363372803, 18.59992790222168, 23.069839000701904, 27.53975009918213, 32.000702142715454, 36.46165418624878, 40.93343472480774, 45.4052152633667, 49.887210845947266, 54.36920642852783, 58.819514751434326, 63.26982307434082, 68.53251957893372, 73.79521608352661, 79.10672783851624, 84.41823959350586, 89.72727823257446, 95.03631687164307, 100.3352198600769, 105.63412284851074, 110.93846845626831, 116.24281406402588, 121.59724831581116, 126.95168256759644, 132.2394106388092, 137.52713871002197, 142.865709066391, 148.20427942276, 153.35112404823303, 158.49796867370605, 163.28077340126038, 168.0635781288147, 172.812340259552, 177.5611023902893, 182.22439217567444, 186.88768196105957, 191.54336166381836, 196.19904136657715, 200.93054866790771, 205.66205596923828, 210.42417812347412, 215.18630027770996, 219.6527135372162, 224.1191267967224, 228.60797715187073, 233.09682750701904, 237.60402488708496, 242.11122226715088, 246.69823932647705, 251.28525638580322, 255.78408002853394, 260.28290367126465, 264.8305697441101, 269.37823581695557, 273.9068353176117, 278.4354348182678, 282.9530305862427, 287.47062635421753, 292.0006728172302, 296.5307192802429, 301.01290488243103, 305.49509048461914, 310.04456996917725, 314.59404945373535, 319.1164357662201, 323.63882207870483, 328.18183946609497, 332.7248568534851, 337.2246096134186, 341.72436237335205, 346.31329321861267, 350.9022240638733, 355.44491386413574, 359.9876036643982, 364.45075583457947, 368.91390800476074, 373.4483940601349, 377.98288011550903, 382.5029385089874, 387.0229969024658, 391.5358603000641, 396.04872369766235, 400.51293778419495, 404.97715187072754, 409.49995160102844, 414.02275133132935, 418.5084648132324, 422.9941782951355, 427.5151174068451, 432.0360565185547, 436.5035467147827, 440.97103691101074, 445.5354537963867, 450.0998706817627, 454.6710855960846, 459.2423005104065, 463.73203229904175, 468.221764087677, 472.7323091030121, 477.24285411834717, 481.83498191833496, 486.42710971832275, 491.0217339992523, 495.6163582801819, 500.3806643486023, 505.1449704170227, 509.63769936561584, 514.130428314209, 518.6729204654694, 523.2154126167297, 527.7353477478027, 532.2552828788757, 536.8212101459503, 541.3871374130249, 546.377072095871, 551.367006778717, 556.1615834236145, 560.956160068512, 565.422945022583, 569.889729976654, 574.3846900463104, 578.8796501159668, 583.3590214252472, 587.8383927345276, 592.2912042140961, 596.7440156936646, 601.2446224689484, 605.7452292442322, 610.2286202907562, 614.7120113372803, 619.2150793075562, 623.718147277832, 628.229504108429, 632.7408609390259, 637.2368905544281, 641.7329201698303, 646.8061401844025, 651.8793601989746, 656.9659390449524, 662.0525178909302, 667.1621706485748, 672.2718234062195, 677.3591597080231, 682.4464960098267, 687.5476267337799, 692.6487574577332, 697.7697937488556, 702.890830039978, 708.0354647636414, 713.1800994873047, 718.2882413864136, 723.3963832855225, 728.5240244865417, 733.651665687561, 738.7899367809296, 743.9282078742981, 749.0255517959595, 754.1228957176208, 759.2621457576752, 764.4013957977295, 769.507443189621, 774.6134905815125, 779.7396266460419, 784.8657627105713, 789.9432220458984, 795.0206813812256, 800.1840875148773, 805.347493648529, 810.4062447547913, 815.4649958610535, 821.0035610198975, 826.5421261787415, 831.6243376731873, 836.706549167633, 841.8001608848572, 846.8937726020813, 852.025365114212, 857.1569576263428, 862.2360126972198, 867.3150677680969, 872.381502866745, 877.4479379653931, 882.527027130127, 887.6061162948608, 892.7475640773773, 897.8890118598938, 902.9758594036102, 908.0627069473267, 913.2281131744385, 918.3935194015503, 923.4708428382874, 928.5481662750244, 933.6514127254486, 938.7546591758728, 943.8955519199371, 949.0364446640015, 954.1129651069641, 959.1894855499268, 961.6186347007751, 964.0477838516235]
[39.3025, 39.3025, 44.4125, 44.4125, 48.2475, 48.2475, 49.635, 49.635, 51.465, 51.465, 51.4775, 51.4775, 51.6475, 51.6475, 52.4575, 52.4575, 52.4925, 52.4925, 53.4725, 53.4725, 53.5625, 53.5625, 54.145, 54.145, 54.4525, 54.4525, 54.7025, 54.7025, 55.5225, 55.5225, 55.4625, 55.4625, 55.7125, 55.7125, 55.8475, 55.8475, 55.69, 55.69, 55.57, 55.57, 56.1575, 56.1575, 55.9025, 55.9025, 56.4875, 56.4875, 56.5525, 56.5525, 56.5075, 56.5075, 56.875, 56.875, 56.6225, 56.6225, 56.7175, 56.7175, 56.69, 56.69, 57.17, 57.17, 56.375, 56.375, 56.7925, 56.7925, 56.7975, 56.7975, 56.7475, 56.7475, 57.0425, 57.0425, 57.015, 57.015, 57.1225, 57.1225, 56.775, 56.775, 56.98, 56.98, 57.2325, 57.2325, 57.055, 57.055, 57.07, 57.07, 56.99, 56.99, 57.115, 57.115, 57.215, 57.215, 57.0475, 57.0475, 57.1475, 57.1475, 56.98, 56.98, 57.4925, 57.4925, 57.5475, 57.5475, 57.89, 57.89, 57.71, 57.71, 57.5625, 57.5625, 57.5675, 57.5675, 57.595, 57.595, 57.7575, 57.7575, 58.1325, 58.1325, 58.0325, 58.0325, 58.04, 58.04, 57.7875, 57.7875, 57.69, 57.69, 57.785, 57.785, 57.7925, 57.7925, 58.1325, 58.1325, 57.895, 57.895, 57.82, 57.82, 58.025, 58.025, 57.7325, 57.7325, 57.73, 57.73, 57.77, 57.77, 57.87, 57.87, 57.4625, 57.4625, 57.51, 57.51, 57.715, 57.715, 57.685, 57.685, 57.7, 57.7, 57.9075, 57.9075, 57.8675, 57.8675, 57.685, 57.685, 58.16, 58.16, 57.9075, 57.9075, 57.985, 57.985, 57.8675, 57.8675, 57.9775, 57.9775, 58.02, 58.02, 58.0025, 58.0025, 57.9875, 57.9875, 58.0, 58.0, 57.85, 57.85, 57.93, 57.93, 58.1825, 58.1825, 58.1375, 58.1375, 58.34, 58.34, 58.1025, 58.1025, 58.19, 58.19, 57.865, 57.865, 58.15, 58.15, 58.3, 58.3, 58.4125, 58.4125, 58.6275, 58.6275, 58.6625, 58.6625]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Round   0, Train loss: 0.766, Test loss: 2.300, Test accuracy: 22.27
Round   1, Train loss: 0.687, Test loss: 2.215, Test accuracy: 32.06
Round   2, Train loss: 0.658, Test loss: 2.110, Test accuracy: 36.90
Round   3, Train loss: 0.593, Test loss: 2.016, Test accuracy: 42.74
Round   4, Train loss: 0.493, Test loss: 1.976, Test accuracy: 48.18
Round   5, Train loss: 0.516, Test loss: 1.936, Test accuracy: 48.69
Round   6, Train loss: 0.411, Test loss: 1.894, Test accuracy: 49.24
Round   7, Train loss: 0.427, Test loss: 1.860, Test accuracy: 49.96
Round   8, Train loss: 0.390, Test loss: 1.826, Test accuracy: 52.12
Round   9, Train loss: 0.362, Test loss: 1.800, Test accuracy: 50.47
Round  10, Train loss: 0.445, Test loss: 1.778, Test accuracy: 50.51
Round  11, Train loss: 0.374, Test loss: 1.745, Test accuracy: 51.22
Round  12, Train loss: 0.378, Test loss: 1.721, Test accuracy: 52.20
Round  13, Train loss: 0.342, Test loss: 1.693, Test accuracy: 53.17
Round  14, Train loss: 0.347, Test loss: 1.660, Test accuracy: 54.17
Round  15, Train loss: 0.336, Test loss: 1.637, Test accuracy: 54.70
Round  16, Train loss: 0.363, Test loss: 1.607, Test accuracy: 54.64
Round  17, Train loss: 0.298, Test loss: 1.566, Test accuracy: 55.94
Round  18, Train loss: 0.329, Test loss: 1.538, Test accuracy: 58.05
Round  19, Train loss: 0.301, Test loss: 1.513, Test accuracy: 59.11
Round  20, Train loss: 0.366, Test loss: 1.488, Test accuracy: 59.87
Round  21, Train loss: 0.300, Test loss: 1.468, Test accuracy: 60.63
Round  22, Train loss: 0.260, Test loss: 1.432, Test accuracy: 61.24
Round  23, Train loss: 0.311, Test loss: 1.419, Test accuracy: 61.02
Round  24, Train loss: 0.243, Test loss: 1.380, Test accuracy: 61.76
Round  25, Train loss: 0.269, Test loss: 1.380, Test accuracy: 62.73
Round  26, Train loss: 0.247, Test loss: 1.346, Test accuracy: 62.60
Round  27, Train loss: 0.290, Test loss: 1.337, Test accuracy: 62.34
Round  28, Train loss: 0.184, Test loss: 1.331, Test accuracy: 61.73
Round  29, Train loss: 0.173, Test loss: 1.296, Test accuracy: 62.36
Round  30, Train loss: 0.183, Test loss: 1.286, Test accuracy: 62.33
Round  31, Train loss: 0.178, Test loss: 1.273, Test accuracy: 60.13
Round  32, Train loss: 0.205, Test loss: 1.249, Test accuracy: 61.29
Round  33, Train loss: 0.151, Test loss: 1.218, Test accuracy: 62.52
Round  34, Train loss: 0.183, Test loss: 1.214, Test accuracy: 62.46
Round  35, Train loss: 0.157, Test loss: 1.193, Test accuracy: 63.00
Round  36, Train loss: 0.174, Test loss: 1.165, Test accuracy: 64.33
Round  37, Train loss: 0.146, Test loss: 1.146, Test accuracy: 64.81
Round  38, Train loss: 0.175, Test loss: 1.145, Test accuracy: 64.35
Round  39, Train loss: 0.212, Test loss: 1.127, Test accuracy: 65.62
Round  40, Train loss: 0.143, Test loss: 1.106, Test accuracy: 65.59
Round  41, Train loss: 0.109, Test loss: 1.095, Test accuracy: 65.96
Round  42, Train loss: 0.203, Test loss: 1.076, Test accuracy: 66.96
Round  43, Train loss: 0.186, Test loss: 1.073, Test accuracy: 66.19
Round  44, Train loss: 0.128, Test loss: 1.041, Test accuracy: 66.98
Round  45, Train loss: 0.106, Test loss: 1.019, Test accuracy: 67.86
Round  46, Train loss: 0.165, Test loss: 1.034, Test accuracy: 66.26
Round  47, Train loss: 0.148, Test loss: 1.007, Test accuracy: 66.58
Round  48, Train loss: 0.143, Test loss: 1.000, Test accuracy: 67.05
Round  49, Train loss: 0.161, Test loss: 0.984, Test accuracy: 67.88
Round  50, Train loss: 0.106, Test loss: 0.980, Test accuracy: 67.61
Round  51, Train loss: 0.117, Test loss: 0.971, Test accuracy: 68.00
Round  52, Train loss: 0.095, Test loss: 0.963, Test accuracy: 68.29
Round  53, Train loss: 0.149, Test loss: 0.950, Test accuracy: 68.60
Round  54, Train loss: 0.095, Test loss: 0.943, Test accuracy: 68.57
Round  55, Train loss: 0.112, Test loss: 0.936, Test accuracy: 69.14
Round  56, Train loss: 0.163, Test loss: 0.926, Test accuracy: 68.87
Round  57, Train loss: 0.075, Test loss: 0.899, Test accuracy: 70.44
Round  58, Train loss: 0.098, Test loss: 0.899, Test accuracy: 69.88
Round  59, Train loss: 0.095, Test loss: 0.886, Test accuracy: 70.26
Round  60, Train loss: 0.104, Test loss: 0.875, Test accuracy: 69.88
Round  61, Train loss: 0.104, Test loss: 0.869, Test accuracy: 69.37
Round  62, Train loss: 0.122, Test loss: 0.866, Test accuracy: 69.28
Round  63, Train loss: 0.106, Test loss: 0.852, Test accuracy: 69.51
Round  64, Train loss: 0.135, Test loss: 0.851, Test accuracy: 69.72
Round  65, Train loss: 0.083, Test loss: 0.836, Test accuracy: 70.10
Round  66, Train loss: 0.093, Test loss: 0.826, Test accuracy: 69.83
Round  67, Train loss: 0.092, Test loss: 0.821, Test accuracy: 69.90
Round  68, Train loss: 0.070, Test loss: 0.807, Test accuracy: 70.68
Round  69, Train loss: 0.072, Test loss: 0.796, Test accuracy: 71.61
Round  70, Train loss: 0.122, Test loss: 0.796, Test accuracy: 71.03
Round  71, Train loss: 0.090, Test loss: 0.806, Test accuracy: 69.93
Round  72, Train loss: 0.116, Test loss: 0.798, Test accuracy: 70.30
Round  73, Train loss: 0.069, Test loss: 0.792, Test accuracy: 70.76
Round  74, Train loss: 0.076, Test loss: 0.778, Test accuracy: 71.53
Round  75, Train loss: 0.078, Test loss: 0.789, Test accuracy: 70.99
Round  76, Train loss: 0.057, Test loss: 0.776, Test accuracy: 71.72
Round  77, Train loss: 0.064, Test loss: 0.776, Test accuracy: 71.21
Round  78, Train loss: 0.066, Test loss: 0.785, Test accuracy: 70.37
Round  79, Train loss: 0.066, Test loss: 0.782, Test accuracy: 70.00
Round  80, Train loss: 0.083, Test loss: 0.782, Test accuracy: 70.13
Round  81, Train loss: 0.061, Test loss: 0.780, Test accuracy: 69.64
Round  82, Train loss: 0.056, Test loss: 0.775, Test accuracy: 70.39
Round  83, Train loss: 0.059, Test loss: 0.775, Test accuracy: 70.22
Round  84, Train loss: 0.109, Test loss: 0.763, Test accuracy: 70.38
Round  85, Train loss: 0.064, Test loss: 0.756, Test accuracy: 70.41
Round  86, Train loss: 0.100, Test loss: 0.761, Test accuracy: 70.77
Round  87, Train loss: 0.074, Test loss: 0.752, Test accuracy: 71.26
Round  88, Train loss: 0.078, Test loss: 0.750, Test accuracy: 71.47
Round  89, Train loss: 0.053, Test loss: 0.737, Test accuracy: 71.93
Round  90, Train loss: 0.057, Test loss: 0.739, Test accuracy: 71.22
Round  91, Train loss: 0.093, Test loss: 0.725, Test accuracy: 72.55
Round  92, Train loss: 0.069, Test loss: 0.724, Test accuracy: 72.01
Round  93, Train loss: 0.080, Test loss: 0.732, Test accuracy: 71.26
Round  94, Train loss: 0.059, Test loss: 0.727, Test accuracy: 71.43
Round  95, Train loss: 0.060, Test loss: 0.732, Test accuracy: 70.73
Round  96, Train loss: 0.071, Test loss: 0.728, Test accuracy: 71.58
Round  97, Train loss: 0.079, Test loss: 0.733, Test accuracy: 70.48
Round  98, Train loss: 0.062, Test loss: 0.725, Test accuracy: 70.83
Round  99, Train loss: 0.053, Test loss: 0.724, Test accuracy: 70.72
Final Round, Train loss: 0.058, Test loss: 0.708, Test accuracy: 71.82
Average accuracy final 10 rounds: 71.28166666666667
2224.876120328903
[]
[22.275, 32.05833333333333, 36.9, 42.74166666666667, 48.18333333333333, 48.69166666666667, 49.24166666666667, 49.958333333333336, 52.125, 50.46666666666667, 50.50833333333333, 51.21666666666667, 52.2, 53.175, 54.175, 54.7, 54.641666666666666, 55.94166666666667, 58.05, 59.108333333333334, 59.86666666666667, 60.63333333333333, 61.24166666666667, 61.025, 61.75833333333333, 62.725, 62.6, 62.34166666666667, 61.725, 62.358333333333334, 62.325, 60.13333333333333, 61.291666666666664, 62.525, 62.458333333333336, 63.0, 64.325, 64.80833333333334, 64.35, 65.61666666666666, 65.59166666666667, 65.95833333333333, 66.95833333333333, 66.19166666666666, 66.98333333333333, 67.85833333333333, 66.25833333333334, 66.575, 67.05, 67.875, 67.60833333333333, 68.0, 68.29166666666667, 68.6, 68.56666666666666, 69.14166666666667, 68.86666666666666, 70.44166666666666, 69.875, 70.25833333333334, 69.875, 69.36666666666666, 69.275, 69.50833333333334, 69.71666666666667, 70.1, 69.825, 69.9, 70.68333333333334, 71.60833333333333, 71.025, 69.93333333333334, 70.3, 70.75833333333334, 71.525, 70.99166666666666, 71.725, 71.20833333333333, 70.36666666666666, 70.0, 70.13333333333334, 69.64166666666667, 70.39166666666667, 70.225, 70.375, 70.40833333333333, 70.76666666666667, 71.25833333333334, 71.46666666666667, 71.93333333333334, 71.21666666666667, 72.55, 72.00833333333334, 71.25833333333334, 71.43333333333334, 70.73333333333333, 71.58333333333333, 70.48333333333333, 70.83333333333333, 70.71666666666667, 71.81666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.085, Test loss: 1.857, Test accuracy: 26.64
Round   1, Train loss: 0.969, Test loss: 1.511, Test accuracy: 38.11
Round   2, Train loss: 0.965, Test loss: 1.297, Test accuracy: 44.83
Round   3, Train loss: 0.865, Test loss: 1.194, Test accuracy: 46.85
Round   4, Train loss: 0.480, Test loss: 1.066, Test accuracy: 53.25
Round   5, Train loss: 0.509, Test loss: 0.960, Test accuracy: 58.12
Round   6, Train loss: 0.477, Test loss: 0.981, Test accuracy: 56.67
Round   7, Train loss: 0.219, Test loss: 0.944, Test accuracy: 59.36
Round   8, Train loss: 0.028, Test loss: 0.937, Test accuracy: 60.40
Round   9, Train loss: 0.032, Test loss: 0.903, Test accuracy: 62.23
Round  10, Train loss: -0.450, Test loss: 0.728, Test accuracy: 68.49
Round  11, Train loss: -0.239, Test loss: 0.757, Test accuracy: 66.91
Round  12, Train loss: -0.552, Test loss: 0.751, Test accuracy: 67.03
Round  13, Train loss: -0.452, Test loss: 0.739, Test accuracy: 66.97
Round  14, Train loss: -0.368, Test loss: 0.715, Test accuracy: 68.35
Round  15, Train loss: -0.830, Test loss: 0.697, Test accuracy: 69.47
Round  16, Train loss: -0.870, Test loss: 0.709, Test accuracy: 69.59
Round  17, Train loss: -1.507, Test loss: 0.703, Test accuracy: 69.86
Round  18, Train loss: -0.869, Test loss: 0.701, Test accuracy: 70.43
Round  19, Train loss: -0.847, Test loss: 0.707, Test accuracy: 69.94
Round  20, Train loss: -0.957, Test loss: 0.715, Test accuracy: 69.99
Round  21, Train loss: -1.444, Test loss: 0.706, Test accuracy: 69.95
Round  22, Train loss: -1.286, Test loss: 0.691, Test accuracy: 70.62
Round  23, Train loss: -1.451, Test loss: 0.683, Test accuracy: 71.03
Round  24, Train loss: -1.636, Test loss: 0.674, Test accuracy: 71.67
Round  25, Train loss: -1.657, Test loss: 0.654, Test accuracy: 72.62
Round  26, Train loss: -1.770, Test loss: 0.646, Test accuracy: 72.45
Round  27, Train loss: -1.589, Test loss: 0.640, Test accuracy: 72.59
Round  28, Train loss: -2.369, Test loss: 0.632, Test accuracy: 73.32
Round  29, Train loss: -2.020, Test loss: 0.628, Test accuracy: 73.69
Round  30, Train loss: -2.544, Test loss: 0.638, Test accuracy: 73.90
Round  31, Train loss: -2.836, Test loss: 0.636, Test accuracy: 73.67
Round  32, Train loss: -2.433, Test loss: 0.625, Test accuracy: 73.68
Round  33, Train loss: -2.282, Test loss: 0.640, Test accuracy: 73.40
Round  34, Train loss: -1.802, Test loss: 0.631, Test accuracy: 73.76
Round  35, Train loss: -2.835, Test loss: 0.632, Test accuracy: 74.18
Round  36, Train loss: -3.004, Test loss: 0.646, Test accuracy: 73.54
Round  37, Train loss: -2.299, Test loss: 0.654, Test accuracy: 73.61
Round  38, Train loss: -2.782, Test loss: 0.644, Test accuracy: 73.88
Round  39, Train loss: -2.855, Test loss: 0.635, Test accuracy: 74.13
Round  40, Train loss: -2.449, Test loss: 0.637, Test accuracy: 74.08
Round  41, Train loss: -2.734, Test loss: 0.641, Test accuracy: 74.09
Round  42, Train loss: -3.036, Test loss: 0.638, Test accuracy: 74.33
Round  43, Train loss: -3.002, Test loss: 0.631, Test accuracy: 74.64
Round  44, Train loss: -2.863, Test loss: 0.615, Test accuracy: 74.97
Round  45, Train loss: -2.639, Test loss: 0.627, Test accuracy: 74.55
Round  46, Train loss: -3.195, Test loss: 0.634, Test accuracy: 74.90
Round  47, Train loss: -3.468, Test loss: 0.632, Test accuracy: 74.74
Round  48, Train loss: -3.140, Test loss: 0.628, Test accuracy: 74.97
Round  49, Train loss: -3.495, Test loss: 0.621, Test accuracy: 75.47
Round  50, Train loss: -3.435, Test loss: 0.622, Test accuracy: 75.58
Round  51, Train loss: -3.295, Test loss: 0.613, Test accuracy: 75.88
Round  52, Train loss: -3.361, Test loss: 0.613, Test accuracy: 76.12
Round  53, Train loss: -3.493, Test loss: 0.611, Test accuracy: 76.10
Round  54, Train loss: -3.549, Test loss: 0.611, Test accuracy: 75.95
Round  55, Train loss: -3.758, Test loss: 0.612, Test accuracy: 75.75
Round  56, Train loss: -3.087, Test loss: 0.618, Test accuracy: 75.55
Round  57, Train loss: -3.892, Test loss: 0.623, Test accuracy: 75.91
Round  58, Train loss: -3.849, Test loss: 0.621, Test accuracy: 76.02
Round  59, Train loss: -3.830, Test loss: 0.597, Test accuracy: 76.49
Round  60, Train loss: -3.247, Test loss: 0.623, Test accuracy: 76.12
Round  61, Train loss: -3.234, Test loss: 0.622, Test accuracy: 75.92
Round  62, Train loss: -3.861, Test loss: 0.641, Test accuracy: 76.10
Round  63, Train loss: -3.558, Test loss: 0.627, Test accuracy: 76.55
Round  64, Train loss: -3.939, Test loss: 0.632, Test accuracy: 76.65
Round  65, Train loss: -3.701, Test loss: 0.650, Test accuracy: 76.17
Round  66, Train loss: -4.391, Test loss: 0.655, Test accuracy: 76.08
Round  67, Train loss: -3.679, Test loss: 0.626, Test accuracy: 76.41
Round  68, Train loss: -4.358, Test loss: 0.659, Test accuracy: 75.67
Round  69, Train loss: -4.240, Test loss: 0.634, Test accuracy: 76.31
Round  70, Train loss: -3.680, Test loss: 0.659, Test accuracy: 76.07
Round  71, Train loss: -4.497, Test loss: 0.649, Test accuracy: 76.22
Round  72, Train loss: -4.633, Test loss: 0.635, Test accuracy: 76.63
Round  73, Train loss: -3.899, Test loss: 0.625, Test accuracy: 76.50
Round  74, Train loss: -4.270, Test loss: 0.620, Test accuracy: 77.05
Round  75, Train loss: -4.708, Test loss: 0.630, Test accuracy: 76.84
Round  76, Train loss: -4.324, Test loss: 0.620, Test accuracy: 76.85
Round  77, Train loss: -4.445, Test loss: 0.659, Test accuracy: 76.42
Round  78, Train loss: -4.541, Test loss: 0.670, Test accuracy: 76.19
Round  79, Train loss: -4.556, Test loss: 0.650, Test accuracy: 76.62
Round  80, Train loss: -4.763, Test loss: 0.650, Test accuracy: 76.43
Round  81, Train loss: -4.249, Test loss: 0.660, Test accuracy: 75.68
Round  82, Train loss: -4.437, Test loss: 0.671, Test accuracy: 75.66
Round  83, Train loss: -3.865, Test loss: 0.661, Test accuracy: 76.38
Round  84, Train loss: -4.393, Test loss: 0.665, Test accuracy: 76.09
Round  85, Train loss: -4.858, Test loss: 0.654, Test accuracy: 76.45
Round  86, Train loss: -5.332, Test loss: 0.642, Test accuracy: 76.88
Round  87, Train loss: -4.564, Test loss: 0.639, Test accuracy: 77.14
Round  88, Train loss: -4.669, Test loss: 0.641, Test accuracy: 76.58
Round  89, Train loss: -4.380, Test loss: 0.664, Test accuracy: 76.53
Round  90, Train loss: -4.952, Test loss: 0.660, Test accuracy: 76.78
Round  91, Train loss: -4.952, Test loss: 0.635, Test accuracy: 77.00
Round  92, Train loss: -4.364, Test loss: 0.641, Test accuracy: 77.51
Round  93, Train loss: -4.199, Test loss: 0.650, Test accuracy: 77.39
Round  94, Train loss: -5.510, Test loss: 0.656, Test accuracy: 77.18
Round  95, Train loss: -4.659, Test loss: 0.628, Test accuracy: 77.48
Round  96, Train loss: -4.204, Test loss: 0.622, Test accuracy: 77.13
Round  97, Train loss: -4.719, Test loss: 0.641, Test accuracy: 76.78
Round  98, Train loss: -5.420, Test loss: 0.629, Test accuracy: 77.08
Round  99, Train loss: -4.768, Test loss: 0.649, Test accuracy: 77.28
Final Round, Train loss: 0.572, Test loss: 0.560, Test accuracy: 76.92
Average accuracy final 10 rounds: 77.16083333333334
Average global accuracy final 10 rounds: 77.16083333333334
1505.5925769805908
[]
[26.641666666666666, 38.108333333333334, 44.825, 46.85, 53.25, 58.11666666666667, 56.666666666666664, 59.358333333333334, 60.4, 62.233333333333334, 68.49166666666666, 66.90833333333333, 67.03333333333333, 66.975, 68.35, 69.46666666666667, 69.59166666666667, 69.85833333333333, 70.43333333333334, 69.94166666666666, 69.99166666666666, 69.95, 70.625, 71.025, 71.66666666666667, 72.625, 72.45, 72.59166666666667, 73.31666666666666, 73.69166666666666, 73.9, 73.66666666666667, 73.68333333333334, 73.4, 73.75833333333334, 74.18333333333334, 73.54166666666667, 73.60833333333333, 73.875, 74.13333333333334, 74.08333333333333, 74.09166666666667, 74.325, 74.64166666666667, 74.96666666666667, 74.55, 74.9, 74.74166666666666, 74.96666666666667, 75.46666666666667, 75.58333333333333, 75.875, 76.125, 76.1, 75.95, 75.75, 75.55, 75.90833333333333, 76.01666666666667, 76.49166666666666, 76.11666666666666, 75.925, 76.1, 76.55, 76.65, 76.16666666666667, 76.075, 76.40833333333333, 75.675, 76.30833333333334, 76.06666666666666, 76.225, 76.63333333333334, 76.5, 77.05, 76.84166666666667, 76.85, 76.41666666666667, 76.19166666666666, 76.61666666666666, 76.43333333333334, 75.68333333333334, 75.65833333333333, 76.38333333333334, 76.09166666666667, 76.45, 76.875, 77.14166666666667, 76.58333333333333, 76.53333333333333, 76.775, 77.0, 77.50833333333334, 77.39166666666667, 77.18333333333334, 77.48333333333333, 77.13333333333334, 76.78333333333333, 77.075, 77.275, 76.925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Round   0, Train loss: 1.083, Test loss: 2.255, Test accuracy: 24.91
Round   1, Train loss: 0.996, Test loss: 2.020, Test accuracy: 30.22
Round   2, Train loss: 0.788, Test loss: 2.137, Test accuracy: 26.47
Round   3, Train loss: 0.748, Test loss: 1.805, Test accuracy: 32.34
Round   4, Train loss: 0.804, Test loss: 1.843, Test accuracy: 30.88
Round   5, Train loss: 0.777, Test loss: 1.872, Test accuracy: 31.96
Round   6, Train loss: 0.696, Test loss: 1.784, Test accuracy: 34.60
Round   7, Train loss: 0.646, Test loss: 2.656, Test accuracy: 22.37
Round   8, Train loss: 0.635, Test loss: 1.878, Test accuracy: 34.17
Round   9, Train loss: 0.608, Test loss: 1.587, Test accuracy: 44.76
Round  10, Train loss: 0.510, Test loss: 1.539, Test accuracy: 47.92
Round  11, Train loss: 0.572, Test loss: 1.579, Test accuracy: 47.78
Round  12, Train loss: 0.634, Test loss: 1.676, Test accuracy: 41.04
Round  13, Train loss: 0.522, Test loss: 1.492, Test accuracy: 47.58
Round  14, Train loss: 0.543, Test loss: 1.914, Test accuracy: 40.14
Round  15, Train loss: 0.550, Test loss: 1.816, Test accuracy: 43.33
Round  16, Train loss: 0.527, Test loss: 1.470, Test accuracy: 49.86
Round  17, Train loss: 0.444, Test loss: 1.455, Test accuracy: 51.52
Round  18, Train loss: 0.535, Test loss: 1.600, Test accuracy: 46.34
Round  19, Train loss: 0.518, Test loss: 1.492, Test accuracy: 49.02
Round  20, Train loss: 0.435, Test loss: 1.384, Test accuracy: 53.52
Round  21, Train loss: 0.432, Test loss: 1.570, Test accuracy: 48.52
Round  22, Train loss: 0.472, Test loss: 1.341, Test accuracy: 54.15
Round  23, Train loss: 0.464, Test loss: 1.300, Test accuracy: 54.58
Round  24, Train loss: 0.493, Test loss: 1.394, Test accuracy: 51.80
Round  25, Train loss: 0.380, Test loss: 1.384, Test accuracy: 54.12
Round  26, Train loss: 0.458, Test loss: 1.465, Test accuracy: 51.12
Round  27, Train loss: 0.571, Test loss: 1.300, Test accuracy: 55.80
Round  28, Train loss: 0.608, Test loss: 1.340, Test accuracy: 53.35
Round  29, Train loss: 0.380, Test loss: 1.395, Test accuracy: 52.53
Round  30, Train loss: 0.458, Test loss: 1.524, Test accuracy: 49.43
Round  31, Train loss: 0.497, Test loss: 1.485, Test accuracy: 49.42
Round  32, Train loss: 0.379, Test loss: 1.300, Test accuracy: 56.58
Round  33, Train loss: 0.388, Test loss: 1.345, Test accuracy: 55.36
Round  34, Train loss: 0.417, Test loss: 1.356, Test accuracy: 54.45
Round  35, Train loss: 0.471, Test loss: 1.330, Test accuracy: 54.31
Round  36, Train loss: 0.446, Test loss: 1.191, Test accuracy: 58.00
Round  37, Train loss: 0.469, Test loss: 1.364, Test accuracy: 53.65
Round  38, Train loss: 0.337, Test loss: 1.660, Test accuracy: 50.18
Round  39, Train loss: 0.379, Test loss: 1.351, Test accuracy: 55.84
Round  40, Train loss: 0.494, Test loss: 1.304, Test accuracy: 55.25
Round  41, Train loss: 0.319, Test loss: 1.830, Test accuracy: 49.57
Round  42, Train loss: 0.381, Test loss: 1.320, Test accuracy: 55.98
Round  43, Train loss: 0.456, Test loss: 1.364, Test accuracy: 53.83
Round  44, Train loss: 0.348, Test loss: 1.223, Test accuracy: 59.63
Round  45, Train loss: 0.334, Test loss: 1.383, Test accuracy: 57.03
Round  46, Train loss: 0.302, Test loss: 1.217, Test accuracy: 60.39
Round  47, Train loss: 0.332, Test loss: 1.577, Test accuracy: 52.70
Round  48, Train loss: 0.313, Test loss: 1.450, Test accuracy: 53.35
Round  49, Train loss: 0.277, Test loss: 1.577, Test accuracy: 53.72
Round  50, Train loss: 0.356, Test loss: 1.320, Test accuracy: 55.79
Round  51, Train loss: 0.240, Test loss: 1.436, Test accuracy: 55.59
Round  52, Train loss: 0.315, Test loss: 1.266, Test accuracy: 58.14
Round  53, Train loss: 0.302, Test loss: 1.330, Test accuracy: 57.30
Round  54, Train loss: 0.295, Test loss: 1.277, Test accuracy: 59.31
Round  55, Train loss: 0.271, Test loss: 1.350, Test accuracy: 58.30
Round  56, Train loss: 0.241, Test loss: 1.138, Test accuracy: 63.91
Round  57, Train loss: 0.313, Test loss: 1.411, Test accuracy: 56.53
Round  58, Train loss: 0.257, Test loss: 1.432, Test accuracy: 57.73
Round  59, Train loss: 0.256, Test loss: 1.293, Test accuracy: 60.56
Round  60, Train loss: 0.298, Test loss: 1.258, Test accuracy: 60.09
Round  61, Train loss: 0.313, Test loss: 1.389, Test accuracy: 56.17
Round  62, Train loss: 0.259, Test loss: 1.348, Test accuracy: 57.79
Round  63, Train loss: 0.207, Test loss: 1.306, Test accuracy: 60.08
Round  64, Train loss: 0.326, Test loss: 1.154, Test accuracy: 62.76
Round  65, Train loss: 0.262, Test loss: 1.177, Test accuracy: 63.14
Round  66, Train loss: 0.209, Test loss: 1.295, Test accuracy: 61.56
Round  67, Train loss: 0.191, Test loss: 1.510, Test accuracy: 57.66
Round  68, Train loss: 0.274, Test loss: 1.388, Test accuracy: 56.86
Round  69, Train loss: 0.256, Test loss: 1.172, Test accuracy: 62.33
Round  70, Train loss: 0.231, Test loss: 1.358, Test accuracy: 58.32
Round  71, Train loss: 0.229, Test loss: 1.282, Test accuracy: 59.51
Round  72, Train loss: 0.189, Test loss: 1.523, Test accuracy: 54.82
Round  73, Train loss: 0.199, Test loss: 1.188, Test accuracy: 61.81
Round  74, Train loss: 0.277, Test loss: 1.170, Test accuracy: 62.92
Round  75, Train loss: 0.200, Test loss: 1.628, Test accuracy: 55.49
Round  76, Train loss: 0.164, Test loss: 1.399, Test accuracy: 58.90
Round  77, Train loss: 0.207, Test loss: 1.304, Test accuracy: 59.33
Round  78, Train loss: 0.184, Test loss: 1.223, Test accuracy: 63.21
Round  79, Train loss: 0.187, Test loss: 1.374, Test accuracy: 61.21
Round  80, Train loss: 0.238, Test loss: 1.311, Test accuracy: 59.92
Round  81, Train loss: 0.232, Test loss: 1.338, Test accuracy: 60.44
Round  82, Train loss: 0.232, Test loss: 1.357, Test accuracy: 58.01
Round  83, Train loss: 0.178, Test loss: 1.461, Test accuracy: 57.92
Round  84, Train loss: 0.187, Test loss: 1.246, Test accuracy: 62.99
Round  85, Train loss: 0.218, Test loss: 1.526, Test accuracy: 55.67
Round  86, Train loss: 0.170, Test loss: 1.169, Test accuracy: 64.08
Round  87, Train loss: 0.176, Test loss: 1.366, Test accuracy: 59.55
Round  88, Train loss: 0.159, Test loss: 1.274, Test accuracy: 62.18
Round  89, Train loss: 0.134, Test loss: 1.320, Test accuracy: 61.74
Round  90, Train loss: 0.201, Test loss: 1.208, Test accuracy: 61.83
Round  91, Train loss: 0.201, Test loss: 1.384, Test accuracy: 60.36
Round  92, Train loss: 0.174, Test loss: 1.169, Test accuracy: 62.95
Round  93, Train loss: 0.164, Test loss: 1.351, Test accuracy: 61.12
Round  94, Train loss: 0.168, Test loss: 1.247, Test accuracy: 63.85
Round  95, Train loss: 0.238, Test loss: 1.292, Test accuracy: 61.34
Round  96, Train loss: 0.193, Test loss: 1.957, Test accuracy: 52.87
Round  97, Train loss: 0.175, Test loss: 1.483, Test accuracy: 58.88
Round  98, Train loss: 0.200, Test loss: 1.488, Test accuracy: 56.47
Round  99, Train loss: 0.181, Test loss: 1.425, Test accuracy: 59.35
Final Round, Train loss: 0.155, Test loss: 1.133, Test accuracy: 65.22
Average accuracy final 10 rounds: 59.900833333333345
2552.1052684783936
[4.087427616119385, 7.797427415847778, 11.393731117248535, 15.036638975143433, 18.780386686325073, 22.361915111541748, 26.111573457717896, 29.81650948524475, 33.51432919502258, 37.23607397079468, 40.96996831893921, 44.78446435928345, 48.59142279624939, 52.38986134529114, 56.22711515426636, 60.012303590774536, 63.728323221206665, 67.62480187416077, 71.48491787910461, 75.35422611236572, 79.09466218948364, 82.81391954421997, 86.66544938087463, 90.35456323623657, 94.23729634284973, 98.05910086631775, 101.84196996688843, 105.648268699646, 109.39102911949158, 113.17837333679199, 116.82573223114014, 120.55921721458435, 124.20545864105225, 127.85070085525513, 131.55411458015442, 135.33225870132446, 139.03086876869202, 142.86369466781616, 146.56306409835815, 150.35799551010132, 153.995934009552, 157.81427645683289, 161.350604057312, 164.94064617156982, 168.50750041007996, 172.11362767219543, 175.70594811439514, 179.30218625068665, 182.8723065853119, 186.47878122329712, 190.09578609466553, 193.6920931339264, 197.2607114315033, 200.86545658111572, 204.45075964927673, 208.04662132263184, 211.6668562889099, 215.29515933990479, 218.88158011436462, 222.4764049053192, 226.1245527267456, 229.71229457855225, 233.29783010482788, 236.93481850624084, 240.57351756095886, 244.17474913597107, 247.7643759250641, 251.3728151321411, 254.9953055381775, 258.6229908466339, 262.19009375572205, 265.82645416259766, 269.47010564804077, 273.0282578468323, 276.64920592308044, 280.4915256500244, 284.3174614906311, 287.9940929412842, 291.67608404159546, 295.4595012664795, 299.25031447410583, 302.9720981121063, 306.5220744609833, 310.129763841629, 313.73037099838257, 317.24913120269775, 320.78620409965515, 324.36097288131714, 328.09141969680786, 331.63013768196106, 335.1967031955719, 338.7554078102112, 342.30816555023193, 345.83392572402954, 349.3928208351135, 352.92587208747864, 356.48158502578735, 360.03199338912964, 363.5506956577301, 367.0724413394928, 370.0648829936981]
[24.908333333333335, 30.216666666666665, 26.466666666666665, 32.34166666666667, 30.883333333333333, 31.958333333333332, 34.6, 22.366666666666667, 34.166666666666664, 44.75833333333333, 47.916666666666664, 47.78333333333333, 41.041666666666664, 47.575, 40.141666666666666, 43.325, 49.858333333333334, 51.516666666666666, 46.34166666666667, 49.025, 53.516666666666666, 48.516666666666666, 54.15, 54.575, 51.8, 54.125, 51.125, 55.8, 53.35, 52.53333333333333, 49.43333333333333, 49.416666666666664, 56.575, 55.358333333333334, 54.45, 54.30833333333333, 58.0, 53.65, 50.18333333333333, 55.84166666666667, 55.25, 49.56666666666667, 55.983333333333334, 53.825, 59.63333333333333, 57.03333333333333, 60.391666666666666, 52.7, 53.35, 53.71666666666667, 55.791666666666664, 55.59166666666667, 58.141666666666666, 57.3, 59.30833333333333, 58.3, 63.90833333333333, 56.53333333333333, 57.725, 60.55833333333333, 60.09166666666667, 56.175, 57.791666666666664, 60.075, 62.75833333333333, 63.141666666666666, 61.55833333333333, 57.65833333333333, 56.858333333333334, 62.333333333333336, 58.31666666666667, 59.50833333333333, 54.81666666666667, 61.80833333333333, 62.925, 55.49166666666667, 58.9, 59.333333333333336, 63.208333333333336, 61.208333333333336, 59.916666666666664, 60.44166666666667, 58.00833333333333, 57.916666666666664, 62.99166666666667, 55.675, 64.075, 59.55, 62.18333333333333, 61.74166666666667, 61.825, 60.358333333333334, 62.95, 61.125, 63.85, 61.34166666666667, 52.86666666666667, 58.875, 56.46666666666667, 59.35, 65.225]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 2.201, Test loss: 1.893, Test accuracy: 34.00
Round   1, Train loss: 1.842, Test loss: 1.602, Test accuracy: 42.43
Round   2, Train loss: 1.655, Test loss: 1.485, Test accuracy: 46.94
Round   3, Train loss: 1.558, Test loss: 1.432, Test accuracy: 49.16
Round   4, Train loss: 1.497, Test loss: 1.374, Test accuracy: 51.26
Round   5, Train loss: 1.411, Test loss: 1.311, Test accuracy: 54.19
Round   6, Train loss: 1.365, Test loss: 1.221, Test accuracy: 56.80
Round   7, Train loss: 1.292, Test loss: 1.194, Test accuracy: 58.35
Round   8, Train loss: 1.237, Test loss: 1.181, Test accuracy: 59.08
Round   9, Train loss: 1.220, Test loss: 1.110, Test accuracy: 61.76
Round  10, Train loss: 1.150, Test loss: 1.092, Test accuracy: 62.83
Round  11, Train loss: 1.132, Test loss: 1.054, Test accuracy: 63.62
Round  12, Train loss: 1.085, Test loss: 1.020, Test accuracy: 65.14
Round  13, Train loss: 1.052, Test loss: 1.018, Test accuracy: 65.45
Round  14, Train loss: 1.024, Test loss: 0.995, Test accuracy: 66.68
Round  15, Train loss: 1.030, Test loss: 0.951, Test accuracy: 67.67
Round  16, Train loss: 0.979, Test loss: 0.976, Test accuracy: 66.97
Round  17, Train loss: 0.944, Test loss: 0.940, Test accuracy: 68.06
Round  18, Train loss: 0.948, Test loss: 0.913, Test accuracy: 68.95
Round  19, Train loss: 0.928, Test loss: 0.882, Test accuracy: 70.25
Round  20, Train loss: 0.933, Test loss: 0.868, Test accuracy: 71.28
Round  21, Train loss: 0.882, Test loss: 0.850, Test accuracy: 71.82
Round  22, Train loss: 0.886, Test loss: 0.827, Test accuracy: 72.39
Round  23, Train loss: 0.843, Test loss: 0.828, Test accuracy: 72.14
Round  24, Train loss: 0.856, Test loss: 0.819, Test accuracy: 72.34
Round  25, Train loss: 0.831, Test loss: 0.817, Test accuracy: 72.44
Round  26, Train loss: 0.832, Test loss: 0.809, Test accuracy: 72.72
Round  27, Train loss: 0.813, Test loss: 0.805, Test accuracy: 72.48
Round  28, Train loss: 0.803, Test loss: 0.793, Test accuracy: 73.17
Round  29, Train loss: 0.776, Test loss: 0.791, Test accuracy: 73.47
Round  30, Train loss: 0.796, Test loss: 0.780, Test accuracy: 73.63
Round  31, Train loss: 0.764, Test loss: 0.770, Test accuracy: 74.11
Round  32, Train loss: 0.750, Test loss: 0.771, Test accuracy: 74.05
Round  33, Train loss: 0.734, Test loss: 0.770, Test accuracy: 74.20
Round  34, Train loss: 0.749, Test loss: 0.768, Test accuracy: 74.07
Round  35, Train loss: 0.743, Test loss: 0.759, Test accuracy: 74.41
Round  36, Train loss: 0.708, Test loss: 0.757, Test accuracy: 74.74
Round  37, Train loss: 0.708, Test loss: 0.755, Test accuracy: 74.67
Round  38, Train loss: 0.729, Test loss: 0.751, Test accuracy: 74.53
Round  39, Train loss: 0.702, Test loss: 0.758, Test accuracy: 74.30
Round  40, Train loss: 0.698, Test loss: 0.750, Test accuracy: 74.78
Round  41, Train loss: 0.695, Test loss: 0.745, Test accuracy: 75.12
Round  42, Train loss: 0.669, Test loss: 0.737, Test accuracy: 75.36
Round  43, Train loss: 0.663, Test loss: 0.737, Test accuracy: 75.26
Round  44, Train loss: 0.702, Test loss: 0.722, Test accuracy: 75.66
Round  45, Train loss: 0.666, Test loss: 0.728, Test accuracy: 75.78
Round  46, Train loss: 0.673, Test loss: 0.730, Test accuracy: 75.21
Round  47, Train loss: 0.665, Test loss: 0.722, Test accuracy: 75.78
Round  48, Train loss: 0.639, Test loss: 0.721, Test accuracy: 75.64
Round  49, Train loss: 0.617, Test loss: 0.717, Test accuracy: 76.04
Round  50, Train loss: 0.661, Test loss: 0.710, Test accuracy: 76.19
Round  51, Train loss: 0.624, Test loss: 0.723, Test accuracy: 75.89
Round  52, Train loss: 0.603, Test loss: 0.716, Test accuracy: 76.11
Round  53, Train loss: 0.630, Test loss: 0.719, Test accuracy: 75.94
Round  54, Train loss: 0.630, Test loss: 0.709, Test accuracy: 76.23
Round  55, Train loss: 0.592, Test loss: 0.717, Test accuracy: 75.92
Round  56, Train loss: 0.606, Test loss: 0.720, Test accuracy: 75.87
Round  57, Train loss: 0.633, Test loss: 0.716, Test accuracy: 76.00
Round  58, Train loss: 0.619, Test loss: 0.710, Test accuracy: 75.72
Round  59, Train loss: 0.615, Test loss: 0.727, Test accuracy: 75.82
Round  60, Train loss: 0.632, Test loss: 0.715, Test accuracy: 75.86
Round  61, Train loss: 0.589, Test loss: 0.716, Test accuracy: 75.80
Round  62, Train loss: 0.602, Test loss: 0.711, Test accuracy: 75.83
Round  63, Train loss: 0.573, Test loss: 0.713, Test accuracy: 76.31
Round  64, Train loss: 0.570, Test loss: 0.712, Test accuracy: 76.01
Round  65, Train loss: 0.573, Test loss: 0.709, Test accuracy: 76.22
Round  66, Train loss: 0.591, Test loss: 0.704, Test accuracy: 76.64
Round  67, Train loss: 0.571, Test loss: 0.699, Test accuracy: 76.74
Round  68, Train loss: 0.574, Test loss: 0.708, Test accuracy: 76.31
Round  69, Train loss: 0.552, Test loss: 0.709, Test accuracy: 76.19
Round  70, Train loss: 0.552, Test loss: 0.702, Test accuracy: 76.39
Round  71, Train loss: 0.551, Test loss: 0.711, Test accuracy: 76.15
Round  72, Train loss: 0.551, Test loss: 0.695, Test accuracy: 76.46
Round  73, Train loss: 0.569, Test loss: 0.692, Test accuracy: 76.59
Round  74, Train loss: 0.553, Test loss: 0.694, Test accuracy: 76.84
Round  75, Train loss: 0.541, Test loss: 0.698, Test accuracy: 76.77
Round  76, Train loss: 0.522, Test loss: 0.706, Test accuracy: 76.76
Round  77, Train loss: 0.534, Test loss: 0.703, Test accuracy: 76.64
Round  78, Train loss: 0.533, Test loss: 0.712, Test accuracy: 76.74
Round  79, Train loss: 0.564, Test loss: 0.696, Test accuracy: 76.96
Round  80, Train loss: 0.563, Test loss: 0.685, Test accuracy: 77.46
Round  81, Train loss: 0.518, Test loss: 0.692, Test accuracy: 77.48
Round  82, Train loss: 0.516, Test loss: 0.696, Test accuracy: 77.32
Round  83, Train loss: 0.529, Test loss: 0.696, Test accuracy: 77.01
Round  84, Train loss: 0.507, Test loss: 0.692, Test accuracy: 77.24
Round  85, Train loss: 0.532, Test loss: 0.693, Test accuracy: 77.35
Round  86, Train loss: 0.542, Test loss: 0.698, Test accuracy: 77.03
Round  87, Train loss: 0.494, Test loss: 0.702, Test accuracy: 76.95
Round  88, Train loss: 0.497, Test loss: 0.704, Test accuracy: 76.99
Round  89, Train loss: 0.491, Test loss: 0.710, Test accuracy: 76.61
Round  90, Train loss: 0.508, Test loss: 0.696, Test accuracy: 77.20
Round  91, Train loss: 0.518, Test loss: 0.699, Test accuracy: 77.31
Round  92, Train loss: 0.528, Test loss: 0.696, Test accuracy: 77.34
Round  93, Train loss: 0.516, Test loss: 0.695, Test accuracy: 77.25
Round  94, Train loss: 0.507, Test loss: 0.694, Test accuracy: 77.62
Round  95, Train loss: 0.493, Test loss: 0.701, Test accuracy: 77.29
Round  96, Train loss: 0.491, Test loss: 0.690, Test accuracy: 77.27
Round  97, Train loss: 0.523, Test loss: 0.678, Test accuracy: 77.69
Round  98, Train loss: 0.504, Test loss: 0.696, Test accuracy: 77.19
Round  99, Train loss: 0.495, Test loss: 0.688, Test accuracy: 77.65
Final Round, Train loss: 0.420, Test loss: 0.687, Test accuracy: 77.69
Average accuracy final 10 rounds: 77.383
4413.43687081337
[6.143571376800537, 12.25717043876648, 18.272233724594116, 24.34034514427185, 29.85217046737671, 35.304545402526855, 40.774672985076904, 46.22764253616333, 51.698845624923706, 57.15265655517578, 62.68251323699951, 68.13772487640381, 73.64723253250122, 79.19194555282593, 84.64933276176453, 90.22302961349487, 95.64736938476562, 101.0461699962616, 106.55595850944519, 111.9588348865509, 117.34807991981506, 122.71854734420776, 128.15473246574402, 133.49076986312866, 138.94117283821106, 144.434472322464, 149.85631704330444, 155.25942063331604, 160.724778175354, 166.1754174232483, 171.6068935394287, 177.1172957420349, 182.6002140045166, 187.96688199043274, 193.58751916885376, 199.04970026016235, 204.46341848373413, 209.9456000328064, 215.3788242340088, 220.71662139892578, 226.09680342674255, 231.42768359184265, 236.93878507614136, 242.38686561584473, 247.6988000869751, 253.02801632881165, 258.39651107788086, 263.71837854385376, 269.0352244377136, 274.35266160964966, 279.7718937397003, 285.1520802974701, 290.4540979862213, 295.72818303108215, 301.0887908935547, 306.3665871620178, 311.6718215942383, 317.05382442474365, 322.30595684051514, 327.61133074760437, 332.9539279937744, 338.2364594936371, 343.4964735507965, 348.8611261844635, 354.20782589912415, 359.4969711303711, 364.8261020183563, 370.21124958992004, 375.4657597541809, 380.83969283103943, 386.1227264404297, 391.41189336776733, 396.75424098968506, 401.9982228279114, 407.3175196647644, 412.62516140937805, 417.90094232559204, 423.2294452190399, 428.5399513244629, 433.90806770324707, 439.21620631217957, 444.54864287376404, 449.9349031448364, 455.363885641098, 460.66653656959534, 466.026150226593, 471.3304982185364, 476.6443350315094, 481.91180300712585, 487.17755103111267, 492.49661087989807, 497.86027359962463, 503.18498969078064, 508.5249607563019, 513.9149749279022, 519.1810915470123, 524.5288000106812, 529.9636964797974, 535.3266212940216, 540.621107339859, 542.966502904892]
[33.9975, 42.4325, 46.9375, 49.155, 51.2575, 54.19, 56.805, 58.3525, 59.08, 61.755, 62.83, 63.6175, 65.14, 65.45, 66.6775, 67.6725, 66.965, 68.06, 68.955, 70.2525, 71.2775, 71.8175, 72.385, 72.1375, 72.3425, 72.44, 72.7225, 72.485, 73.1725, 73.475, 73.63, 74.11, 74.0475, 74.1975, 74.0725, 74.41, 74.7375, 74.675, 74.5325, 74.3, 74.775, 75.1175, 75.365, 75.2625, 75.66, 75.7825, 75.21, 75.7825, 75.6425, 76.0425, 76.19, 75.8925, 76.1075, 75.945, 76.23, 75.92, 75.87, 76.0, 75.7225, 75.82, 75.8625, 75.8, 75.835, 76.305, 76.0125, 76.215, 76.645, 76.74, 76.31, 76.195, 76.385, 76.1525, 76.4575, 76.595, 76.8375, 76.7725, 76.7575, 76.645, 76.74, 76.96, 77.4625, 77.485, 77.3225, 77.01, 77.24, 77.35, 77.035, 76.9475, 76.9875, 76.61, 77.2025, 77.3075, 77.3425, 77.255, 77.6175, 77.2925, 77.2725, 77.695, 77.1925, 77.6525, 77.6925]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Round   0, Train loss: 1.421, Test loss: 2.085, Test accuracy: 19.89
Round   1, Train loss: 0.970, Test loss: 1.626, Test accuracy: 35.81
Round   2, Train loss: 0.861, Test loss: 1.537, Test accuracy: 45.09
Round   3, Train loss: 0.748, Test loss: 1.316, Test accuracy: 50.61
Round   4, Train loss: 0.730, Test loss: 1.038, Test accuracy: 58.16
Round   5, Train loss: 0.820, Test loss: 0.923, Test accuracy: 62.82
Round   6, Train loss: 0.765, Test loss: 0.838, Test accuracy: 66.11
Round   7, Train loss: 0.738, Test loss: 0.791, Test accuracy: 67.84
Round   8, Train loss: 0.695, Test loss: 0.794, Test accuracy: 68.69
Round   9, Train loss: 0.571, Test loss: 0.754, Test accuracy: 69.82
Round  10, Train loss: 0.585, Test loss: 0.762, Test accuracy: 70.33
Round  11, Train loss: 0.533, Test loss: 0.721, Test accuracy: 71.17
Round  12, Train loss: 0.691, Test loss: 0.614, Test accuracy: 75.34
Round  13, Train loss: 0.602, Test loss: 0.598, Test accuracy: 76.36
Round  14, Train loss: 0.638, Test loss: 0.575, Test accuracy: 77.18
Round  15, Train loss: 0.639, Test loss: 0.570, Test accuracy: 77.82
Round  16, Train loss: 0.572, Test loss: 0.556, Test accuracy: 78.17
Round  17, Train loss: 0.491, Test loss: 0.539, Test accuracy: 78.56
Round  18, Train loss: 0.623, Test loss: 0.535, Test accuracy: 79.11
Round  19, Train loss: 0.540, Test loss: 0.515, Test accuracy: 79.26
Round  20, Train loss: 0.458, Test loss: 0.519, Test accuracy: 79.54
Round  21, Train loss: 0.598, Test loss: 0.516, Test accuracy: 79.58
Round  22, Train loss: 0.540, Test loss: 0.525, Test accuracy: 79.60
Round  23, Train loss: 0.477, Test loss: 0.511, Test accuracy: 80.27
Round  24, Train loss: 0.462, Test loss: 0.489, Test accuracy: 80.55
Round  25, Train loss: 0.503, Test loss: 0.495, Test accuracy: 80.59
Round  26, Train loss: 0.530, Test loss: 0.493, Test accuracy: 80.48
Round  27, Train loss: 0.487, Test loss: 0.484, Test accuracy: 80.84
Round  28, Train loss: 0.461, Test loss: 0.475, Test accuracy: 81.31
Round  29, Train loss: 0.399, Test loss: 0.474, Test accuracy: 81.09
Round  30, Train loss: 0.564, Test loss: 0.477, Test accuracy: 81.69
Round  31, Train loss: 0.434, Test loss: 0.457, Test accuracy: 81.59
Round  32, Train loss: 0.483, Test loss: 0.458, Test accuracy: 82.02
Round  33, Train loss: 0.484, Test loss: 0.462, Test accuracy: 81.93
Round  34, Train loss: 0.410, Test loss: 0.447, Test accuracy: 82.26
Round  35, Train loss: 0.379, Test loss: 0.441, Test accuracy: 82.59
Round  36, Train loss: 0.476, Test loss: 0.438, Test accuracy: 82.74
Round  37, Train loss: 0.514, Test loss: 0.439, Test accuracy: 82.54
Round  38, Train loss: 0.410, Test loss: 0.439, Test accuracy: 82.54
Round  39, Train loss: 0.331, Test loss: 0.438, Test accuracy: 82.56
Round  40, Train loss: 0.476, Test loss: 0.439, Test accuracy: 82.59
Round  41, Train loss: 0.456, Test loss: 0.441, Test accuracy: 82.36
Round  42, Train loss: 0.420, Test loss: 0.437, Test accuracy: 82.26
Round  43, Train loss: 0.401, Test loss: 0.427, Test accuracy: 83.11
Round  44, Train loss: 0.322, Test loss: 0.419, Test accuracy: 83.26
Round  45, Train loss: 0.330, Test loss: 0.414, Test accuracy: 83.49
Round  46, Train loss: 0.408, Test loss: 0.413, Test accuracy: 83.52
Round  47, Train loss: 0.406, Test loss: 0.415, Test accuracy: 83.54
Round  48, Train loss: 0.419, Test loss: 0.409, Test accuracy: 83.80
Round  49, Train loss: 0.445, Test loss: 0.411, Test accuracy: 83.76
Round  50, Train loss: 0.326, Test loss: 0.404, Test accuracy: 83.88
Round  51, Train loss: 0.291, Test loss: 0.411, Test accuracy: 83.44
Round  52, Train loss: 0.350, Test loss: 0.406, Test accuracy: 83.75
Round  53, Train loss: 0.374, Test loss: 0.407, Test accuracy: 83.86
Round  54, Train loss: 0.388, Test loss: 0.405, Test accuracy: 83.79
Round  55, Train loss: 0.299, Test loss: 0.399, Test accuracy: 84.01
Round  56, Train loss: 0.303, Test loss: 0.395, Test accuracy: 84.03
Round  57, Train loss: 0.377, Test loss: 0.400, Test accuracy: 84.06
Round  58, Train loss: 0.302, Test loss: 0.400, Test accuracy: 84.09
Round  59, Train loss: 0.374, Test loss: 0.397, Test accuracy: 84.19
Round  60, Train loss: 0.419, Test loss: 0.400, Test accuracy: 84.16
Round  61, Train loss: 0.365, Test loss: 0.395, Test accuracy: 84.33
Round  62, Train loss: 0.296, Test loss: 0.388, Test accuracy: 84.61
Round  63, Train loss: 0.297, Test loss: 0.394, Test accuracy: 84.41
Round  64, Train loss: 0.385, Test loss: 0.389, Test accuracy: 84.62
Round  65, Train loss: 0.371, Test loss: 0.400, Test accuracy: 84.19
Round  66, Train loss: 0.281, Test loss: 0.390, Test accuracy: 84.43
Round  67, Train loss: 0.288, Test loss: 0.391, Test accuracy: 84.37
Round  68, Train loss: 0.296, Test loss: 0.389, Test accuracy: 84.73
Round  69, Train loss: 0.273, Test loss: 0.388, Test accuracy: 84.67
Round  70, Train loss: 0.378, Test loss: 0.389, Test accuracy: 84.62
Round  71, Train loss: 0.322, Test loss: 0.383, Test accuracy: 84.78
Round  72, Train loss: 0.240, Test loss: 0.380, Test accuracy: 85.16
Round  73, Train loss: 0.279, Test loss: 0.384, Test accuracy: 84.96
Round  74, Train loss: 0.279, Test loss: 0.386, Test accuracy: 85.10
Round  75, Train loss: 0.239, Test loss: 0.386, Test accuracy: 85.02
Round  76, Train loss: 0.346, Test loss: 0.388, Test accuracy: 84.57
Round  77, Train loss: 0.291, Test loss: 0.387, Test accuracy: 84.81
Round  78, Train loss: 0.257, Test loss: 0.391, Test accuracy: 84.92
Round  79, Train loss: 0.250, Test loss: 0.382, Test accuracy: 84.90
Round  80, Train loss: 0.336, Test loss: 0.395, Test accuracy: 84.47
Round  81, Train loss: 0.266, Test loss: 0.392, Test accuracy: 84.68
Round  82, Train loss: 0.284, Test loss: 0.386, Test accuracy: 84.83
Round  83, Train loss: 0.281, Test loss: 0.382, Test accuracy: 85.16
Round  84, Train loss: 0.267, Test loss: 0.377, Test accuracy: 85.18
Round  85, Train loss: 0.248, Test loss: 0.379, Test accuracy: 85.11
Round  86, Train loss: 0.306, Test loss: 0.385, Test accuracy: 84.99
Round  87, Train loss: 0.268, Test loss: 0.381, Test accuracy: 85.16
Round  88, Train loss: 0.308, Test loss: 0.382, Test accuracy: 85.24
Round  89, Train loss: 0.287, Test loss: 0.381, Test accuracy: 85.53
Round  90, Train loss: 0.225, Test loss: 0.385, Test accuracy: 85.04
Round  91, Train loss: 0.223, Test loss: 0.379, Test accuracy: 85.47
Round  92, Train loss: 0.293, Test loss: 0.382, Test accuracy: 85.39
Round  93, Train loss: 0.223, Test loss: 0.378, Test accuracy: 85.26
Round  94, Train loss: 0.278, Test loss: 0.383, Test accuracy: 85.15
Round  95, Train loss: 0.243, Test loss: 0.378, Test accuracy: 85.17
Round  96, Train loss: 0.233, Test loss: 0.380, Test accuracy: 85.22
Round  97, Train loss: 0.253, Test loss: 0.384, Test accuracy: 85.45
Round  98, Train loss: 0.187, Test loss: 0.389, Test accuracy: 85.35
Round  99, Train loss: 0.245, Test loss: 0.388, Test accuracy: 85.38
Final Round, Train loss: 0.210, Test loss: 0.383, Test accuracy: 85.62
Average accuracy final 10 rounds: 85.28888888888889
2646.4116270542145
[3.147061824798584, 6.294123649597168, 8.734822750091553, 11.175521850585938, 13.596836566925049, 16.01815128326416, 18.481355905532837, 20.944560527801514, 23.371783018112183, 25.79900550842285, 28.287038803100586, 30.77507209777832, 33.241984605789185, 35.70889711380005, 38.173760652542114, 40.63862419128418, 43.09180498123169, 45.5449857711792, 47.99162697792053, 50.438268184661865, 52.93807649612427, 55.43788480758667, 57.85881018638611, 60.27973556518555, 62.81303811073303, 65.34634065628052, 67.79397177696228, 70.24160289764404, 72.72982501983643, 75.21804714202881, 77.67294049263, 80.1278338432312, 82.53482151031494, 84.94180917739868, 87.42815279960632, 89.91449642181396, 92.36724638938904, 94.81999635696411, 97.27745032310486, 99.7349042892456, 102.16025280952454, 104.58560132980347, 107.07042145729065, 109.55524158477783, 112.00761389732361, 114.45998620986938, 116.89049935340881, 119.32101249694824, 121.78953886032104, 124.25806522369385, 126.68149971961975, 129.10493421554565, 131.57064843177795, 134.03636264801025, 136.45315742492676, 138.86995220184326, 141.34211945533752, 143.8142867088318, 146.22627329826355, 148.6382598876953, 151.08453512191772, 153.53081035614014, 155.9750051498413, 158.41919994354248, 160.85213470458984, 163.2850694656372, 165.7653739452362, 168.2456784248352, 170.7153868675232, 173.18509531021118, 175.67144346237183, 178.15779161453247, 180.63096261024475, 183.10413360595703, 185.59190893173218, 188.07968425750732, 190.52449464797974, 192.96930503845215, 195.45205450057983, 197.93480396270752, 200.3909935951233, 202.84718322753906, 205.30428743362427, 207.76139163970947, 210.2331280708313, 212.70486450195312, 215.13279747962952, 217.5607304573059, 220.03521704673767, 222.50970363616943, 224.95761585235596, 227.40552806854248, 229.84756922721863, 232.28961038589478, 234.7911057472229, 237.29260110855103, 239.70094180107117, 242.1092824935913, 244.55392742156982, 246.99857234954834, 249.4362277984619, 251.8738832473755, 254.3104817867279, 256.7470803260803, 259.1973178386688, 261.6475553512573, 264.11090207099915, 266.57424879074097, 269.2337338924408, 271.8932189941406, 274.5732419490814, 277.2532649040222, 279.9459984302521, 282.63873195648193, 285.3465905189514, 288.0544490814209, 290.7591667175293, 293.4638843536377, 296.1815891265869, 298.89929389953613, 301.58862566947937, 304.2779574394226, 306.9856264591217, 309.6932954788208, 312.38774824142456, 315.0822010040283, 317.78071641921997, 320.4792318344116, 323.1748387813568, 325.870445728302, 328.5603926181793, 331.25033950805664, 334.00308060646057, 336.7558217048645, 339.4226257801056, 342.0894298553467, 344.815541267395, 347.54165267944336, 350.2367901802063, 352.93192768096924, 355.6815664768219, 358.43120527267456, 361.12372040748596, 363.81623554229736, 366.5038380622864, 369.1914405822754, 371.91924500465393, 374.64704942703247, 377.33271408081055, 380.0183787345886, 382.7133343219757, 385.4082899093628, 388.08424067497253, 390.7601914405823, 393.50065875053406, 396.24112606048584, 398.91721081733704, 401.59329557418823, 404.3251860141754, 407.0570764541626, 409.7315378189087, 412.4059991836548, 415.12669014930725, 417.8473811149597, 420.5364713668823, 423.22556161880493, 425.9519054889679, 428.67824935913086, 431.37710762023926, 434.07596588134766, 436.796658039093, 439.5173501968384, 442.21041560173035, 444.9034810066223, 447.611976146698, 450.3204712867737, 453.0298902988434, 455.7393093109131, 458.4054353237152, 461.07156133651733, 463.77540349960327, 466.4792456626892, 469.16404247283936, 471.8488392829895, 474.58335041999817, 477.31786155700684, 479.97932028770447, 482.6407790184021, 485.3173623085022, 487.9939455986023, 490.64974093437195, 493.3055362701416, 496.0121269226074, 498.71871757507324, 501.4067327976227, 504.0947480201721, 506.81479930877686, 509.5348505973816, 512.1961951255798, 514.8575396537781, 517.0943846702576, 519.3312296867371]
[19.894444444444446, 19.894444444444446, 35.80555555555556, 35.80555555555556, 45.08888888888889, 45.08888888888889, 50.605555555555554, 50.605555555555554, 58.15555555555556, 58.15555555555556, 62.82222222222222, 62.82222222222222, 66.11111111111111, 66.11111111111111, 67.84444444444445, 67.84444444444445, 68.68888888888888, 68.68888888888888, 69.81666666666666, 69.81666666666666, 70.33333333333333, 70.33333333333333, 71.16666666666667, 71.16666666666667, 75.33888888888889, 75.33888888888889, 76.35555555555555, 76.35555555555555, 77.18333333333334, 77.18333333333334, 77.82222222222222, 77.82222222222222, 78.16666666666667, 78.16666666666667, 78.55555555555556, 78.55555555555556, 79.11111111111111, 79.11111111111111, 79.25555555555556, 79.25555555555556, 79.54444444444445, 79.54444444444445, 79.58333333333333, 79.58333333333333, 79.6, 79.6, 80.26666666666667, 80.26666666666667, 80.55, 80.55, 80.59444444444445, 80.59444444444445, 80.48333333333333, 80.48333333333333, 80.83888888888889, 80.83888888888889, 81.30555555555556, 81.30555555555556, 81.09444444444445, 81.09444444444445, 81.68888888888888, 81.68888888888888, 81.58888888888889, 81.58888888888889, 82.02222222222223, 82.02222222222223, 81.93333333333334, 81.93333333333334, 82.2611111111111, 82.2611111111111, 82.58888888888889, 82.58888888888889, 82.74444444444444, 82.74444444444444, 82.53888888888889, 82.53888888888889, 82.54444444444445, 82.54444444444445, 82.56111111111112, 82.56111111111112, 82.58888888888889, 82.58888888888889, 82.35555555555555, 82.35555555555555, 82.25555555555556, 82.25555555555556, 83.10555555555555, 83.10555555555555, 83.2611111111111, 83.2611111111111, 83.4888888888889, 83.4888888888889, 83.52222222222223, 83.52222222222223, 83.54444444444445, 83.54444444444445, 83.8, 83.8, 83.7611111111111, 83.7611111111111, 83.88333333333334, 83.88333333333334, 83.43888888888888, 83.43888888888888, 83.75, 83.75, 83.86111111111111, 83.86111111111111, 83.79444444444445, 83.79444444444445, 84.00555555555556, 84.00555555555556, 84.03333333333333, 84.03333333333333, 84.05555555555556, 84.05555555555556, 84.08888888888889, 84.08888888888889, 84.18888888888888, 84.18888888888888, 84.16111111111111, 84.16111111111111, 84.32777777777778, 84.32777777777778, 84.61111111111111, 84.61111111111111, 84.41111111111111, 84.41111111111111, 84.61666666666666, 84.61666666666666, 84.18888888888888, 84.18888888888888, 84.43333333333334, 84.43333333333334, 84.37222222222222, 84.37222222222222, 84.73333333333333, 84.73333333333333, 84.66666666666667, 84.66666666666667, 84.61666666666666, 84.61666666666666, 84.78333333333333, 84.78333333333333, 85.15555555555555, 85.15555555555555, 84.96111111111111, 84.96111111111111, 85.1, 85.1, 85.01666666666667, 85.01666666666667, 84.56666666666666, 84.56666666666666, 84.81111111111112, 84.81111111111112, 84.91666666666667, 84.91666666666667, 84.9, 84.9, 84.46666666666667, 84.46666666666667, 84.67777777777778, 84.67777777777778, 84.82777777777778, 84.82777777777778, 85.16111111111111, 85.16111111111111, 85.18333333333334, 85.18333333333334, 85.11111111111111, 85.11111111111111, 84.99444444444444, 84.99444444444444, 85.16111111111111, 85.16111111111111, 85.2388888888889, 85.2388888888889, 85.52777777777777, 85.52777777777777, 85.04444444444445, 85.04444444444445, 85.46666666666667, 85.46666666666667, 85.39444444444445, 85.39444444444445, 85.2611111111111, 85.2611111111111, 85.15, 85.15, 85.16666666666667, 85.16666666666667, 85.22222222222223, 85.22222222222223, 85.45, 85.45, 85.35, 85.35, 85.38333333333334, 85.38333333333334, 85.61666666666666, 85.61666666666666]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.045, Test loss: 2.107, Test accuracy: 23.21
Round   0, Global train loss: 1.045, Global test loss: 2.479, Global test accuracy: 12.64
Round   1, Train loss: 0.934, Test loss: 1.624, Test accuracy: 41.48
Round   1, Global train loss: 0.934, Global test loss: 2.449, Global test accuracy: 16.71
Round   2, Train loss: 0.750, Test loss: 1.448, Test accuracy: 49.26
Round   2, Global train loss: 0.750, Global test loss: 2.615, Global test accuracy: 19.81
Round   3, Train loss: 0.866, Test loss: 1.018, Test accuracy: 58.43
Round   3, Global train loss: 0.866, Global test loss: 2.336, Global test accuracy: 16.68
Round   4, Train loss: 0.641, Test loss: 1.006, Test accuracy: 59.15
Round   4, Global train loss: 0.641, Global test loss: 2.498, Global test accuracy: 17.52
Round   5, Train loss: 0.679, Test loss: 0.823, Test accuracy: 65.46
Round   5, Global train loss: 0.679, Global test loss: 2.456, Global test accuracy: 15.35
Round   6, Train loss: 0.674, Test loss: 0.673, Test accuracy: 70.55
Round   6, Global train loss: 0.674, Global test loss: 2.294, Global test accuracy: 15.21
Round   7, Train loss: 0.604, Test loss: 0.664, Test accuracy: 71.13
Round   7, Global train loss: 0.604, Global test loss: 2.265, Global test accuracy: 18.47
Round   8, Train loss: 0.722, Test loss: 0.654, Test accuracy: 72.11
Round   8, Global train loss: 0.722, Global test loss: 2.444, Global test accuracy: 16.60
Round   9, Train loss: 0.608, Test loss: 0.626, Test accuracy: 73.23
Round   9, Global train loss: 0.608, Global test loss: 2.668, Global test accuracy: 16.67
Round  10, Train loss: 0.620, Test loss: 0.620, Test accuracy: 73.59
Round  10, Global train loss: 0.620, Global test loss: 2.272, Global test accuracy: 20.31
Round  11, Train loss: 0.474, Test loss: 0.618, Test accuracy: 73.46
Round  11, Global train loss: 0.474, Global test loss: 2.373, Global test accuracy: 17.98
Round  12, Train loss: 0.535, Test loss: 0.620, Test accuracy: 73.52
Round  12, Global train loss: 0.535, Global test loss: 2.285, Global test accuracy: 15.97
Round  13, Train loss: 0.460, Test loss: 0.612, Test accuracy: 74.25
Round  13, Global train loss: 0.460, Global test loss: 2.224, Global test accuracy: 18.14
Round  14, Train loss: 0.538, Test loss: 0.616, Test accuracy: 74.26
Round  14, Global train loss: 0.538, Global test loss: 2.474, Global test accuracy: 20.23
Round  15, Train loss: 0.571, Test loss: 0.607, Test accuracy: 74.70
Round  15, Global train loss: 0.571, Global test loss: 2.480, Global test accuracy: 16.77
Round  16, Train loss: 0.533, Test loss: 0.596, Test accuracy: 75.18
Round  16, Global train loss: 0.533, Global test loss: 2.306, Global test accuracy: 17.48
Round  17, Train loss: 0.369, Test loss: 0.595, Test accuracy: 75.45
Round  17, Global train loss: 0.369, Global test loss: 2.353, Global test accuracy: 20.30
Round  18, Train loss: 0.460, Test loss: 0.594, Test accuracy: 75.89
Round  18, Global train loss: 0.460, Global test loss: 2.374, Global test accuracy: 19.58
Round  19, Train loss: 0.433, Test loss: 0.590, Test accuracy: 76.06
Round  19, Global train loss: 0.433, Global test loss: 2.568, Global test accuracy: 16.45
Round  20, Train loss: 0.381, Test loss: 0.585, Test accuracy: 76.38
Round  20, Global train loss: 0.381, Global test loss: 2.331, Global test accuracy: 16.89
Round  21, Train loss: 0.421, Test loss: 0.589, Test accuracy: 76.64
Round  21, Global train loss: 0.421, Global test loss: 2.335, Global test accuracy: 16.92
Round  22, Train loss: 0.462, Test loss: 0.582, Test accuracy: 77.15
Round  22, Global train loss: 0.462, Global test loss: 2.354, Global test accuracy: 16.05
Round  23, Train loss: 0.336, Test loss: 0.586, Test accuracy: 77.38
Round  23, Global train loss: 0.336, Global test loss: 2.297, Global test accuracy: 16.74
Round  24, Train loss: 0.388, Test loss: 0.588, Test accuracy: 77.35
Round  24, Global train loss: 0.388, Global test loss: 2.394, Global test accuracy: 15.12
Round  25, Train loss: 0.293, Test loss: 0.597, Test accuracy: 77.42
Round  25, Global train loss: 0.293, Global test loss: 2.235, Global test accuracy: 19.26
Round  26, Train loss: 0.370, Test loss: 0.602, Test accuracy: 77.35
Round  26, Global train loss: 0.370, Global test loss: 2.385, Global test accuracy: 16.67
Round  27, Train loss: 0.239, Test loss: 0.598, Test accuracy: 77.40
Round  27, Global train loss: 0.239, Global test loss: 2.488, Global test accuracy: 17.45
Round  28, Train loss: 0.286, Test loss: 0.584, Test accuracy: 78.14
Round  28, Global train loss: 0.286, Global test loss: 2.327, Global test accuracy: 19.18
Round  29, Train loss: 0.387, Test loss: 0.572, Test accuracy: 78.76
Round  29, Global train loss: 0.387, Global test loss: 2.327, Global test accuracy: 16.15
Round  30, Train loss: 0.244, Test loss: 0.583, Test accuracy: 78.42
Round  30, Global train loss: 0.244, Global test loss: 2.243, Global test accuracy: 18.88
Round  31, Train loss: 0.382, Test loss: 0.583, Test accuracy: 78.80
Round  31, Global train loss: 0.382, Global test loss: 2.287, Global test accuracy: 19.50
Round  32, Train loss: 0.346, Test loss: 0.591, Test accuracy: 78.79
Round  32, Global train loss: 0.346, Global test loss: 2.302, Global test accuracy: 18.98
Round  33, Train loss: 0.328, Test loss: 0.611, Test accuracy: 78.41
Round  33, Global train loss: 0.328, Global test loss: 2.248, Global test accuracy: 15.39
Round  34, Train loss: 0.296, Test loss: 0.612, Test accuracy: 78.40
Round  34, Global train loss: 0.296, Global test loss: 2.323, Global test accuracy: 14.82
Round  35, Train loss: 0.273, Test loss: 0.623, Test accuracy: 78.26
Round  35, Global train loss: 0.273, Global test loss: 2.243, Global test accuracy: 19.11
Round  36, Train loss: 0.325, Test loss: 0.631, Test accuracy: 78.11
Round  36, Global train loss: 0.325, Global test loss: 2.284, Global test accuracy: 18.73
Round  37, Train loss: 0.345, Test loss: 0.624, Test accuracy: 78.28
Round  37, Global train loss: 0.345, Global test loss: 2.268, Global test accuracy: 18.37
Round  38, Train loss: 0.215, Test loss: 0.613, Test accuracy: 78.71
Round  38, Global train loss: 0.215, Global test loss: 2.343, Global test accuracy: 17.25
Round  39, Train loss: 0.165, Test loss: 0.636, Test accuracy: 78.45
Round  39, Global train loss: 0.165, Global test loss: 2.343, Global test accuracy: 15.22
Round  40, Train loss: 0.327, Test loss: 0.639, Test accuracy: 78.58
Round  40, Global train loss: 0.327, Global test loss: 2.365, Global test accuracy: 16.98
Round  41, Train loss: 0.183, Test loss: 0.644, Test accuracy: 78.69
Round  41, Global train loss: 0.183, Global test loss: 2.397, Global test accuracy: 16.52
Round  42, Train loss: 0.303, Test loss: 0.637, Test accuracy: 79.38
Round  42, Global train loss: 0.303, Global test loss: 2.322, Global test accuracy: 18.48
Round  43, Train loss: 0.272, Test loss: 0.627, Test accuracy: 79.88
Round  43, Global train loss: 0.272, Global test loss: 2.319, Global test accuracy: 12.59
Round  44, Train loss: 0.220, Test loss: 0.646, Test accuracy: 79.90
Round  44, Global train loss: 0.220, Global test loss: 2.315, Global test accuracy: 17.02
Round  45, Train loss: 0.136, Test loss: 0.650, Test accuracy: 79.83
Round  45, Global train loss: 0.136, Global test loss: 2.325, Global test accuracy: 18.38
Round  46, Train loss: 0.169, Test loss: 0.695, Test accuracy: 79.09
Round  46, Global train loss: 0.169, Global test loss: 2.303, Global test accuracy: 18.38
Round  47, Train loss: 0.227, Test loss: 0.692, Test accuracy: 79.30
Round  47, Global train loss: 0.227, Global test loss: 2.298, Global test accuracy: 16.84
Round  48, Train loss: 0.171, Test loss: 0.687, Test accuracy: 79.39
Round  48, Global train loss: 0.171, Global test loss: 2.675, Global test accuracy: 19.28
Round  49, Train loss: 0.194, Test loss: 0.700, Test accuracy: 79.42
Round  49, Global train loss: 0.194, Global test loss: 2.284, Global test accuracy: 18.01
Round  50, Train loss: 0.105, Test loss: 0.733, Test accuracy: 79.25
Round  50, Global train loss: 0.105, Global test loss: 2.310, Global test accuracy: 16.43
Round  51, Train loss: 0.180, Test loss: 0.748, Test accuracy: 78.77
Round  51, Global train loss: 0.180, Global test loss: 2.296, Global test accuracy: 16.98
Round  52, Train loss: 0.121, Test loss: 0.728, Test accuracy: 78.87
Round  52, Global train loss: 0.121, Global test loss: 2.502, Global test accuracy: 14.55
Round  53, Train loss: 0.128, Test loss: 0.738, Test accuracy: 78.94
Round  53, Global train loss: 0.128, Global test loss: 2.268, Global test accuracy: 15.84
Round  54, Train loss: 0.211, Test loss: 0.737, Test accuracy: 79.32
Round  54, Global train loss: 0.211, Global test loss: 2.356, Global test accuracy: 19.73
Round  55, Train loss: 0.147, Test loss: 0.729, Test accuracy: 79.92
Round  55, Global train loss: 0.147, Global test loss: 2.437, Global test accuracy: 16.42
Round  56, Train loss: 0.167, Test loss: 0.746, Test accuracy: 80.15
Round  56, Global train loss: 0.167, Global test loss: 2.330, Global test accuracy: 18.34
Round  57, Train loss: 0.187, Test loss: 0.740, Test accuracy: 80.42
Round  57, Global train loss: 0.187, Global test loss: 2.482, Global test accuracy: 20.25
Round  58, Train loss: 0.096, Test loss: 0.744, Test accuracy: 80.41
Round  58, Global train loss: 0.096, Global test loss: 2.357, Global test accuracy: 12.98
Round  59, Train loss: 0.164, Test loss: 0.723, Test accuracy: 80.25
Round  59, Global train loss: 0.164, Global test loss: 2.472, Global test accuracy: 17.96
Round  60, Train loss: 0.142, Test loss: 0.745, Test accuracy: 79.93
Round  60, Global train loss: 0.142, Global test loss: 2.318, Global test accuracy: 18.84
Round  61, Train loss: 0.205, Test loss: 0.774, Test accuracy: 79.98
Round  61, Global train loss: 0.205, Global test loss: 2.277, Global test accuracy: 18.37
Round  62, Train loss: 0.186, Test loss: 0.791, Test accuracy: 79.76
Round  62, Global train loss: 0.186, Global test loss: 2.268, Global test accuracy: 12.78
Round  63, Train loss: 0.095, Test loss: 0.808, Test accuracy: 79.89
Round  63, Global train loss: 0.095, Global test loss: 2.308, Global test accuracy: 18.95
Round  64, Train loss: 0.140, Test loss: 0.813, Test accuracy: 80.08
Round  64, Global train loss: 0.140, Global test loss: 2.640, Global test accuracy: 16.41
Round  65, Train loss: 0.159, Test loss: 0.800, Test accuracy: 80.09
Round  65, Global train loss: 0.159, Global test loss: 2.339, Global test accuracy: 15.80
Round  66, Train loss: 0.133, Test loss: 0.806, Test accuracy: 80.28
Round  66, Global train loss: 0.133, Global test loss: 2.294, Global test accuracy: 18.50
Round  67, Train loss: 0.120, Test loss: 0.798, Test accuracy: 80.36
Round  67, Global train loss: 0.120, Global test loss: 2.284, Global test accuracy: 18.51
Round  68, Train loss: 0.112, Test loss: 0.795, Test accuracy: 80.63
Round  68, Global train loss: 0.112, Global test loss: 2.428, Global test accuracy: 20.00
Round  69, Train loss: 0.131, Test loss: 0.817, Test accuracy: 80.50
Round  69, Global train loss: 0.131, Global test loss: 2.346, Global test accuracy: 13.01
Round  70, Train loss: 0.095, Test loss: 0.833, Test accuracy: 80.53
Round  70, Global train loss: 0.095, Global test loss: 2.306, Global test accuracy: 18.29
Round  71, Train loss: 0.117, Test loss: 0.839, Test accuracy: 80.57
Round  71, Global train loss: 0.117, Global test loss: 2.394, Global test accuracy: 16.32
Round  72, Train loss: 0.116, Test loss: 0.841, Test accuracy: 80.56
Round  72, Global train loss: 0.116, Global test loss: 2.334, Global test accuracy: 16.71
Round  73, Train loss: 0.110, Test loss: 0.842, Test accuracy: 80.57
Round  73, Global train loss: 0.110, Global test loss: 2.285, Global test accuracy: 17.07
Round  74, Train loss: 0.090, Test loss: 0.876, Test accuracy: 80.20
Round  74, Global train loss: 0.090, Global test loss: 2.606, Global test accuracy: 20.73
Round  75, Train loss: 0.064, Test loss: 0.895, Test accuracy: 79.91
Round  75, Global train loss: 0.064, Global test loss: 2.437, Global test accuracy: 18.82
Round  76, Train loss: 0.083, Test loss: 0.897, Test accuracy: 79.78
Round  76, Global train loss: 0.083, Global test loss: 2.302, Global test accuracy: 19.89
Round  77, Train loss: 0.074, Test loss: 0.880, Test accuracy: 79.78
Round  77, Global train loss: 0.074, Global test loss: 2.276, Global test accuracy: 19.29
Round  78, Train loss: 0.106, Test loss: 0.886, Test accuracy: 80.17
Round  78, Global train loss: 0.106, Global test loss: 2.688, Global test accuracy: 14.26
Round  79, Train loss: 0.087, Test loss: 0.867, Test accuracy: 80.33
Round  79, Global train loss: 0.087, Global test loss: 2.271, Global test accuracy: 17.56
Round  80, Train loss: 0.089, Test loss: 0.857, Test accuracy: 80.47
Round  80, Global train loss: 0.089, Global test loss: 2.262, Global test accuracy: 17.51
Round  81, Train loss: 0.075, Test loss: 0.872, Test accuracy: 80.20
Round  81, Global train loss: 0.075, Global test loss: 2.419, Global test accuracy: 20.62
Round  82, Train loss: 0.084, Test loss: 0.924, Test accuracy: 79.74
Round  82, Global train loss: 0.084, Global test loss: 2.395, Global test accuracy: 12.59
Round  83, Train loss: 0.099, Test loss: 0.929, Test accuracy: 80.27
Round  83, Global train loss: 0.099, Global test loss: 2.313, Global test accuracy: 15.44
Round  84, Train loss: 0.090, Test loss: 0.929, Test accuracy: 79.97
Round  84, Global train loss: 0.090, Global test loss: 2.361, Global test accuracy: 16.52
Round  85, Train loss: 0.083, Test loss: 0.891, Test accuracy: 80.58
Round  85, Global train loss: 0.083, Global test loss: 2.410, Global test accuracy: 19.79
Round  86, Train loss: 0.083, Test loss: 0.933, Test accuracy: 80.56
Round  86, Global train loss: 0.083, Global test loss: 2.312, Global test accuracy: 13.29
Round  87, Train loss: 0.086, Test loss: 0.932, Test accuracy: 80.67
Round  87, Global train loss: 0.086, Global test loss: 2.256, Global test accuracy: 17.62
Round  88, Train loss: 0.070, Test loss: 0.939, Test accuracy: 80.63
Round  88, Global train loss: 0.070, Global test loss: 2.282, Global test accuracy: 18.93
Round  89, Train loss: 0.059, Test loss: 0.924, Test accuracy: 80.87
Round  89, Global train loss: 0.059, Global test loss: 2.282, Global test accuracy: 17.85
Round  90, Train loss: 0.062, Test loss: 0.946, Test accuracy: 80.87
Round  90, Global train loss: 0.062, Global test loss: 2.511, Global test accuracy: 19.43
Round  91, Train loss: 0.047, Test loss: 0.924, Test accuracy: 80.80
Round  91, Global train loss: 0.047, Global test loss: 2.327, Global test accuracy: 15.83
Round  92, Train loss: 0.067, Test loss: 0.935, Test accuracy: 80.70
Round  92, Global train loss: 0.067, Global test loss: 2.259, Global test accuracy: 19.44
Round  93, Train loss: 0.070, Test loss: 0.934, Test accuracy: 80.62
Round  93, Global train loss: 0.070, Global test loss: 2.262, Global test accuracy: 15.92
Round  94, Train loss: 0.049, Test loss: 0.925, Test accuracy: 80.46
Round  94, Global train loss: 0.049, Global test loss: 2.371, Global test accuracy: 20.33
Round  95, Train loss: 0.084, Test loss: 0.952, Test accuracy: 80.53
Round  95, Global train loss: 0.084, Global test loss: 2.348, Global test accuracy: 17.79
Round  96, Train loss: 0.073, Test loss: 0.927, Test accuracy: 80.64
Round  96, Global train loss: 0.073, Global test loss: 2.512, Global test accuracy: 20.48
Round  97, Train loss: 0.049, Test loss: 0.925, Test accuracy: 80.76
Round  97, Global train loss: 0.049, Global test loss: 2.437, Global test accuracy: 16.83
Round  98, Train loss: 0.064, Test loss: 0.933, Test accuracy: 80.85
Round  98, Global train loss: 0.064, Global test loss: 2.406, Global test accuracy: 19.49
Round  99, Train loss: 0.056, Test loss: 0.943, Test accuracy: 80.95
Round  99, Global train loss: 0.056, Global test loss: 2.742, Global test accuracy: 16.99
Final Round, Train loss: 0.052, Test loss: 0.999, Test accuracy: 81.09
Final Round, Global train loss: 0.052, Global test loss: 2.742, Global test accuracy: 16.99
Average accuracy final 10 rounds: 80.71749999999999 

Average global accuracy final 10 rounds: 18.254166666666666 

2019.0335063934326
[1.8248636722564697, 3.6497273445129395, 5.206083059310913, 6.762438774108887, 8.31432056427002, 9.866202354431152, 11.423296928405762, 12.980391502380371, 14.533530950546265, 16.086670398712158, 17.6325900554657, 19.17850971221924, 20.666964769363403, 22.15541982650757, 23.720662355422974, 25.28590488433838, 26.83066463470459, 28.3754243850708, 29.92683172225952, 31.478239059448242, 33.0583758354187, 34.63851261138916, 36.210344552993774, 37.78217649459839, 39.33302640914917, 40.88387632369995, 42.43696618080139, 43.99005603790283, 45.55947542190552, 47.1288948059082, 48.69138979911804, 50.25388479232788, 51.81950855255127, 53.38513231277466, 54.94476819038391, 56.504404067993164, 58.06437683105469, 59.62434959411621, 61.180119037628174, 62.73588848114014, 64.28838205337524, 65.84087562561035, 67.40849566459656, 68.97611570358276, 70.55666422843933, 72.1372127532959, 73.70253562927246, 75.26785850524902, 76.82988715171814, 78.39191579818726, 79.96036505699158, 81.5288143157959, 83.10307335853577, 84.67733240127563, 86.23744750022888, 87.79756259918213, 89.36505508422852, 90.9325475692749, 92.50083613395691, 94.06912469863892, 95.63926577568054, 97.20940685272217, 98.7645800113678, 100.31975317001343, 101.88632225990295, 103.45289134979248, 105.02137565612793, 106.58985996246338, 108.15261030197144, 109.71536064147949, 111.27888822555542, 112.84241580963135, 114.41723942756653, 115.99206304550171, 117.55053377151489, 119.10900449752808, 120.67965388298035, 122.25030326843262, 123.81926465034485, 125.38822603225708, 126.95286130905151, 128.51749658584595, 130.08412671089172, 131.6507568359375, 133.23528718948364, 134.81981754302979, 136.39617657661438, 137.97253561019897, 139.55183339118958, 141.13113117218018, 142.7002227306366, 144.26931428909302, 145.82822513580322, 147.38713598251343, 148.93990111351013, 150.49266624450684, 152.04633951187134, 153.60001277923584, 155.14793610572815, 156.69585943222046, 158.2479932308197, 159.80012702941895, 161.3535840511322, 162.90704107284546, 164.38379001617432, 165.86053895950317, 167.41394662857056, 168.96735429763794, 170.50471091270447, 172.042067527771, 173.56289839744568, 175.08372926712036, 176.63502478599548, 178.1863203048706, 179.74155712127686, 181.2967939376831, 182.84822344779968, 184.39965295791626, 185.98677563667297, 187.5738983154297, 189.15363574028015, 190.73337316513062, 192.2980227470398, 193.86267232894897, 195.44943475723267, 197.03619718551636, 198.61292457580566, 200.18965196609497, 201.76356863975525, 203.33748531341553, 204.90056443214417, 206.4636435508728, 208.0423288345337, 209.62101411819458, 211.19253873825073, 212.76406335830688, 214.34204769134521, 215.92003202438354, 217.49131083488464, 219.06258964538574, 220.64312481880188, 222.22365999221802, 223.79906511306763, 225.37447023391724, 226.9536211490631, 228.53277206420898, 230.09894347190857, 231.66511487960815, 233.23488688468933, 234.8046588897705, 236.36860537528992, 237.93255186080933, 239.225919008255, 240.51928615570068, 241.82124710083008, 243.12320804595947, 244.45904636383057, 245.79488468170166, 247.19288516044617, 248.59088563919067, 250.14692282676697, 251.70296001434326, 253.28794169425964, 254.87292337417603, 256.4288139343262, 257.9847044944763, 259.5555741786957, 261.12644386291504, 262.68140149116516, 264.2363591194153, 265.78660821914673, 267.3368573188782, 268.87424993515015, 270.4116425514221, 271.96627831459045, 273.5209140777588, 275.10015892982483, 276.67940378189087, 278.24273133277893, 279.806058883667, 281.3645601272583, 282.9230613708496, 284.49408984184265, 286.0651183128357, 287.629625082016, 289.1941318511963, 290.7248628139496, 292.2555937767029, 293.551296710968, 294.84699964523315, 296.1773235797882, 297.50764751434326, 298.81942105293274, 300.1311945915222, 301.43484568595886, 302.7384967803955, 304.0548117160797, 305.3711266517639, 306.67445397377014, 307.97778129577637, 310.1757667064667, 312.373752117157]
[23.208333333333332, 23.208333333333332, 41.483333333333334, 41.483333333333334, 49.25833333333333, 49.25833333333333, 58.43333333333333, 58.43333333333333, 59.15, 59.15, 65.45833333333333, 65.45833333333333, 70.55, 70.55, 71.13333333333334, 71.13333333333334, 72.10833333333333, 72.10833333333333, 73.23333333333333, 73.23333333333333, 73.59166666666667, 73.59166666666667, 73.45833333333333, 73.45833333333333, 73.51666666666667, 73.51666666666667, 74.25, 74.25, 74.25833333333334, 74.25833333333334, 74.7, 74.7, 75.18333333333334, 75.18333333333334, 75.45, 75.45, 75.89166666666667, 75.89166666666667, 76.05833333333334, 76.05833333333334, 76.38333333333334, 76.38333333333334, 76.64166666666667, 76.64166666666667, 77.15, 77.15, 77.375, 77.375, 77.35, 77.35, 77.41666666666667, 77.41666666666667, 77.35, 77.35, 77.4, 77.4, 78.14166666666667, 78.14166666666667, 78.75833333333334, 78.75833333333334, 78.425, 78.425, 78.8, 78.8, 78.79166666666667, 78.79166666666667, 78.40833333333333, 78.40833333333333, 78.4, 78.4, 78.25833333333334, 78.25833333333334, 78.10833333333333, 78.10833333333333, 78.28333333333333, 78.28333333333333, 78.70833333333333, 78.70833333333333, 78.45, 78.45, 78.575, 78.575, 78.69166666666666, 78.69166666666666, 79.375, 79.375, 79.875, 79.875, 79.9, 79.9, 79.825, 79.825, 79.09166666666667, 79.09166666666667, 79.3, 79.3, 79.39166666666667, 79.39166666666667, 79.425, 79.425, 79.25, 79.25, 78.76666666666667, 78.76666666666667, 78.86666666666666, 78.86666666666666, 78.94166666666666, 78.94166666666666, 79.31666666666666, 79.31666666666666, 79.91666666666667, 79.91666666666667, 80.15, 80.15, 80.41666666666667, 80.41666666666667, 80.40833333333333, 80.40833333333333, 80.25, 80.25, 79.93333333333334, 79.93333333333334, 79.98333333333333, 79.98333333333333, 79.75833333333334, 79.75833333333334, 79.89166666666667, 79.89166666666667, 80.08333333333333, 80.08333333333333, 80.09166666666667, 80.09166666666667, 80.275, 80.275, 80.35833333333333, 80.35833333333333, 80.63333333333334, 80.63333333333334, 80.5, 80.5, 80.53333333333333, 80.53333333333333, 80.56666666666666, 80.56666666666666, 80.55833333333334, 80.55833333333334, 80.56666666666666, 80.56666666666666, 80.2, 80.2, 79.90833333333333, 79.90833333333333, 79.775, 79.775, 79.775, 79.775, 80.16666666666667, 80.16666666666667, 80.325, 80.325, 80.475, 80.475, 80.2, 80.2, 79.74166666666666, 79.74166666666666, 80.26666666666667, 80.26666666666667, 79.96666666666667, 79.96666666666667, 80.575, 80.575, 80.55833333333334, 80.55833333333334, 80.675, 80.675, 80.63333333333334, 80.63333333333334, 80.86666666666666, 80.86666666666666, 80.86666666666666, 80.86666666666666, 80.8, 80.8, 80.7, 80.7, 80.625, 80.625, 80.45833333333333, 80.45833333333333, 80.525, 80.525, 80.64166666666667, 80.64166666666667, 80.75833333333334, 80.75833333333334, 80.85, 80.85, 80.95, 80.95, 81.09166666666667, 81.09166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 1.031, Test loss: 1.937, Test accuracy: 27.55
Round   0, Global train loss: 1.031, Global test loss: 2.285, Global test accuracy: 16.48
Round   1, Train loss: 0.897, Test loss: 1.824, Test accuracy: 40.69
Round   1, Global train loss: 0.897, Global test loss: 2.613, Global test accuracy: 18.07
Round   2, Train loss: 0.772, Test loss: 1.486, Test accuracy: 47.99
Round   2, Global train loss: 0.772, Global test loss: 2.649, Global test accuracy: 16.27
Round   3, Train loss: 0.819, Test loss: 1.193, Test accuracy: 56.16
Round   3, Global train loss: 0.819, Global test loss: 2.609, Global test accuracy: 15.10
Round   4, Train loss: 0.728, Test loss: 1.090, Test accuracy: 59.92
Round   4, Global train loss: 0.728, Global test loss: 2.701, Global test accuracy: 10.97
Round   5, Train loss: 0.826, Test loss: 0.834, Test accuracy: 66.75
Round   5, Global train loss: 0.826, Global test loss: 2.663, Global test accuracy: 14.79
Round   6, Train loss: 0.761, Test loss: 0.677, Test accuracy: 71.22
Round   6, Global train loss: 0.761, Global test loss: 2.377, Global test accuracy: 18.37
Round   7, Train loss: 0.737, Test loss: 0.651, Test accuracy: 72.22
Round   7, Global train loss: 0.737, Global test loss: 2.687, Global test accuracy: 15.60
Round   8, Train loss: 0.658, Test loss: 0.626, Test accuracy: 73.78
Round   8, Global train loss: 0.658, Global test loss: 2.437, Global test accuracy: 15.51
Round   9, Train loss: 0.648, Test loss: 0.602, Test accuracy: 75.03
Round   9, Global train loss: 0.648, Global test loss: 2.681, Global test accuracy: 19.99
Round  10, Train loss: 0.559, Test loss: 0.591, Test accuracy: 75.44
Round  10, Global train loss: 0.559, Global test loss: 2.585, Global test accuracy: 13.22
Round  11, Train loss: 0.679, Test loss: 0.582, Test accuracy: 75.48
Round  11, Global train loss: 0.679, Global test loss: 2.636, Global test accuracy: 13.62
Round  12, Train loss: 0.571, Test loss: 0.573, Test accuracy: 75.97
Round  12, Global train loss: 0.571, Global test loss: 2.560, Global test accuracy: 16.02
Round  13, Train loss: 0.559, Test loss: 0.570, Test accuracy: 76.12
Round  13, Global train loss: 0.559, Global test loss: 2.671, Global test accuracy: 20.07
Round  14, Train loss: 0.526, Test loss: 0.570, Test accuracy: 76.12
Round  14, Global train loss: 0.526, Global test loss: 2.551, Global test accuracy: 20.73
Round  15, Train loss: 0.625, Test loss: 0.559, Test accuracy: 76.62
Round  15, Global train loss: 0.625, Global test loss: 2.594, Global test accuracy: 18.01
Round  16, Train loss: 0.643, Test loss: 0.555, Test accuracy: 77.48
Round  16, Global train loss: 0.643, Global test loss: 2.530, Global test accuracy: 20.62
Round  17, Train loss: 0.578, Test loss: 0.547, Test accuracy: 77.96
Round  17, Global train loss: 0.578, Global test loss: 2.886, Global test accuracy: 18.93
Round  18, Train loss: 0.595, Test loss: 0.537, Test accuracy: 78.33
Round  18, Global train loss: 0.595, Global test loss: 2.454, Global test accuracy: 17.57
Round  19, Train loss: 0.582, Test loss: 0.529, Test accuracy: 78.58
Round  19, Global train loss: 0.582, Global test loss: 2.719, Global test accuracy: 13.97
Round  20, Train loss: 0.533, Test loss: 0.527, Test accuracy: 78.58
Round  20, Global train loss: 0.533, Global test loss: 2.612, Global test accuracy: 17.81
Round  21, Train loss: 0.569, Test loss: 0.508, Test accuracy: 79.59
Round  21, Global train loss: 0.569, Global test loss: 2.634, Global test accuracy: 18.31
Round  22, Train loss: 0.487, Test loss: 0.508, Test accuracy: 79.41
Round  22, Global train loss: 0.487, Global test loss: 2.541, Global test accuracy: 16.62
Round  23, Train loss: 0.440, Test loss: 0.499, Test accuracy: 79.84
Round  23, Global train loss: 0.440, Global test loss: 3.118, Global test accuracy: 14.93
Round  24, Train loss: 0.471, Test loss: 0.503, Test accuracy: 79.84
Round  24, Global train loss: 0.471, Global test loss: 2.920, Global test accuracy: 13.88
Round  25, Train loss: 0.417, Test loss: 0.504, Test accuracy: 79.74
Round  25, Global train loss: 0.417, Global test loss: 2.558, Global test accuracy: 20.43
Round  26, Train loss: 0.490, Test loss: 0.508, Test accuracy: 79.72
Round  26, Global train loss: 0.490, Global test loss: 2.833, Global test accuracy: 19.38
Round  27, Train loss: 0.485, Test loss: 0.500, Test accuracy: 80.06
Round  27, Global train loss: 0.485, Global test loss: 2.508, Global test accuracy: 18.65
Round  28, Train loss: 0.374, Test loss: 0.491, Test accuracy: 80.56
Round  28, Global train loss: 0.374, Global test loss: 2.779, Global test accuracy: 20.27
Round  29, Train loss: 0.469, Test loss: 0.499, Test accuracy: 80.68
Round  29, Global train loss: 0.469, Global test loss: 2.573, Global test accuracy: 19.86
Round  30, Train loss: 0.405, Test loss: 0.504, Test accuracy: 80.58
Round  30, Global train loss: 0.405, Global test loss: 2.952, Global test accuracy: 19.46
Round  31, Train loss: 0.399, Test loss: 0.504, Test accuracy: 80.79
Round  31, Global train loss: 0.399, Global test loss: 2.473, Global test accuracy: 20.24
Round  32, Train loss: 0.421, Test loss: 0.501, Test accuracy: 80.59
Round  32, Global train loss: 0.421, Global test loss: 2.749, Global test accuracy: 18.85
Round  33, Train loss: 0.555, Test loss: 0.516, Test accuracy: 79.88
Round  33, Global train loss: 0.555, Global test loss: 2.475, Global test accuracy: 20.96
Round  34, Train loss: 0.474, Test loss: 0.490, Test accuracy: 81.28
Round  34, Global train loss: 0.474, Global test loss: 2.862, Global test accuracy: 19.23
Round  35, Train loss: 0.355, Test loss: 0.479, Test accuracy: 81.62
Round  35, Global train loss: 0.355, Global test loss: 2.833, Global test accuracy: 19.38
Round  36, Train loss: 0.386, Test loss: 0.466, Test accuracy: 82.12
Round  36, Global train loss: 0.386, Global test loss: 3.221, Global test accuracy: 18.86
Round  37, Train loss: 0.404, Test loss: 0.463, Test accuracy: 82.25
Round  37, Global train loss: 0.404, Global test loss: 2.578, Global test accuracy: 19.32
Round  38, Train loss: 0.403, Test loss: 0.473, Test accuracy: 81.92
Round  38, Global train loss: 0.403, Global test loss: 2.841, Global test accuracy: 18.57
Round  39, Train loss: 0.363, Test loss: 0.462, Test accuracy: 82.45
Round  39, Global train loss: 0.363, Global test loss: 2.991, Global test accuracy: 18.19
Round  40, Train loss: 0.353, Test loss: 0.462, Test accuracy: 82.43
Round  40, Global train loss: 0.353, Global test loss: 2.948, Global test accuracy: 18.80
Round  41, Train loss: 0.391, Test loss: 0.455, Test accuracy: 82.62
Round  41, Global train loss: 0.391, Global test loss: 3.122, Global test accuracy: 20.81
Round  42, Train loss: 0.343, Test loss: 0.462, Test accuracy: 82.62
Round  42, Global train loss: 0.343, Global test loss: 2.773, Global test accuracy: 21.55
Round  43, Train loss: 0.364, Test loss: 0.482, Test accuracy: 82.33
Round  43, Global train loss: 0.364, Global test loss: 2.870, Global test accuracy: 19.38
Round  44, Train loss: 0.388, Test loss: 0.485, Test accuracy: 82.04
Round  44, Global train loss: 0.388, Global test loss: 3.019, Global test accuracy: 18.24
Round  45, Train loss: 0.325, Test loss: 0.498, Test accuracy: 81.74
Round  45, Global train loss: 0.325, Global test loss: 2.545, Global test accuracy: 19.18
Round  46, Train loss: 0.428, Test loss: 0.494, Test accuracy: 81.78
Round  46, Global train loss: 0.428, Global test loss: 2.438, Global test accuracy: 19.18
Round  47, Train loss: 0.355, Test loss: 0.471, Test accuracy: 82.72
Round  47, Global train loss: 0.355, Global test loss: 3.026, Global test accuracy: 18.19
Round  48, Train loss: 0.381, Test loss: 0.483, Test accuracy: 82.22
Round  48, Global train loss: 0.381, Global test loss: 3.140, Global test accuracy: 17.46
Round  49, Train loss: 0.339, Test loss: 0.486, Test accuracy: 82.32
Round  49, Global train loss: 0.339, Global test loss: 2.968, Global test accuracy: 19.98
Round  50, Train loss: 0.443, Test loss: 0.486, Test accuracy: 82.53
Round  50, Global train loss: 0.443, Global test loss: 2.961, Global test accuracy: 16.89
Round  51, Train loss: 0.328, Test loss: 0.472, Test accuracy: 82.99
Round  51, Global train loss: 0.328, Global test loss: 2.821, Global test accuracy: 20.51
Round  52, Train loss: 0.340, Test loss: 0.462, Test accuracy: 83.33
Round  52, Global train loss: 0.340, Global test loss: 2.713, Global test accuracy: 17.90
Round  53, Train loss: 0.416, Test loss: 0.463, Test accuracy: 83.47
Round  53, Global train loss: 0.416, Global test loss: 2.607, Global test accuracy: 18.12
Round  54, Train loss: 0.268, Test loss: 0.461, Test accuracy: 83.74
Round  54, Global train loss: 0.268, Global test loss: 2.867, Global test accuracy: 18.84
Round  55, Train loss: 0.421, Test loss: 0.462, Test accuracy: 83.72
Round  55, Global train loss: 0.421, Global test loss: 2.444, Global test accuracy: 19.93
Round  56, Train loss: 0.314, Test loss: 0.461, Test accuracy: 83.72
Round  56, Global train loss: 0.314, Global test loss: 2.967, Global test accuracy: 21.34
Round  57, Train loss: 0.363, Test loss: 0.466, Test accuracy: 83.60
Round  57, Global train loss: 0.363, Global test loss: 3.026, Global test accuracy: 17.02
Round  58, Train loss: 0.294, Test loss: 0.456, Test accuracy: 83.92
Round  58, Global train loss: 0.294, Global test loss: 3.204, Global test accuracy: 19.18
Round  59, Train loss: 0.331, Test loss: 0.471, Test accuracy: 83.83
Round  59, Global train loss: 0.331, Global test loss: 2.977, Global test accuracy: 19.82
Round  60, Train loss: 0.348, Test loss: 0.467, Test accuracy: 83.63
Round  60, Global train loss: 0.348, Global test loss: 2.880, Global test accuracy: 20.57
Round  61, Train loss: 0.307, Test loss: 0.463, Test accuracy: 83.62
Round  61, Global train loss: 0.307, Global test loss: 2.722, Global test accuracy: 20.53
Round  62, Train loss: 0.354, Test loss: 0.471, Test accuracy: 83.78
Round  62, Global train loss: 0.354, Global test loss: 2.709, Global test accuracy: 20.88
Round  63, Train loss: 0.302, Test loss: 0.473, Test accuracy: 83.66
Round  63, Global train loss: 0.302, Global test loss: 3.242, Global test accuracy: 20.38
Round  64, Train loss: 0.279, Test loss: 0.483, Test accuracy: 83.31
Round  64, Global train loss: 0.279, Global test loss: 3.134, Global test accuracy: 19.21
Round  65, Train loss: 0.313, Test loss: 0.479, Test accuracy: 83.57
Round  65, Global train loss: 0.313, Global test loss: 2.633, Global test accuracy: 19.63
Round  66, Train loss: 0.313, Test loss: 0.487, Test accuracy: 83.45
Round  66, Global train loss: 0.313, Global test loss: 2.759, Global test accuracy: 19.93
Round  67, Train loss: 0.279, Test loss: 0.485, Test accuracy: 83.51
Round  67, Global train loss: 0.279, Global test loss: 3.012, Global test accuracy: 19.36
Round  68, Train loss: 0.297, Test loss: 0.482, Test accuracy: 83.51
Round  68, Global train loss: 0.297, Global test loss: 3.412, Global test accuracy: 19.18
Round  69, Train loss: 0.281, Test loss: 0.491, Test accuracy: 83.64
Round  69, Global train loss: 0.281, Global test loss: 3.108, Global test accuracy: 18.19
Round  70, Train loss: 0.261, Test loss: 0.500, Test accuracy: 83.43
Round  70, Global train loss: 0.261, Global test loss: 3.380, Global test accuracy: 18.23
Round  71, Train loss: 0.247, Test loss: 0.494, Test accuracy: 83.49
Round  71, Global train loss: 0.247, Global test loss: 3.189, Global test accuracy: 21.68
Round  72, Train loss: 0.225, Test loss: 0.487, Test accuracy: 83.69
Round  72, Global train loss: 0.225, Global test loss: 2.892, Global test accuracy: 20.43
Round  73, Train loss: 0.279, Test loss: 0.479, Test accuracy: 83.97
Round  73, Global train loss: 0.279, Global test loss: 2.915, Global test accuracy: 18.22
Round  74, Train loss: 0.261, Test loss: 0.474, Test accuracy: 84.26
Round  74, Global train loss: 0.261, Global test loss: 2.966, Global test accuracy: 20.07
Round  75, Train loss: 0.265, Test loss: 0.473, Test accuracy: 84.01
Round  75, Global train loss: 0.265, Global test loss: 3.457, Global test accuracy: 19.10
Round  76, Train loss: 0.254, Test loss: 0.477, Test accuracy: 83.85
Round  76, Global train loss: 0.254, Global test loss: 3.094, Global test accuracy: 19.71
Round  77, Train loss: 0.261, Test loss: 0.482, Test accuracy: 83.83
Round  77, Global train loss: 0.261, Global test loss: 3.457, Global test accuracy: 17.71
Round  78, Train loss: 0.284, Test loss: 0.475, Test accuracy: 84.05
Round  78, Global train loss: 0.284, Global test loss: 3.107, Global test accuracy: 20.52
Round  79, Train loss: 0.259, Test loss: 0.468, Test accuracy: 84.29
Round  79, Global train loss: 0.259, Global test loss: 3.009, Global test accuracy: 17.17
Round  80, Train loss: 0.235, Test loss: 0.479, Test accuracy: 84.10
Round  80, Global train loss: 0.235, Global test loss: 3.125, Global test accuracy: 17.32
Round  81, Train loss: 0.248, Test loss: 0.493, Test accuracy: 83.92
Round  81, Global train loss: 0.248, Global test loss: 3.320, Global test accuracy: 19.03
Round  82, Train loss: 0.198, Test loss: 0.492, Test accuracy: 83.99
Round  82, Global train loss: 0.198, Global test loss: 3.568, Global test accuracy: 18.93
Round  83, Train loss: 0.259, Test loss: 0.484, Test accuracy: 83.85
Round  83, Global train loss: 0.259, Global test loss: 3.032, Global test accuracy: 18.31
Round  84, Train loss: 0.249, Test loss: 0.479, Test accuracy: 84.28
Round  84, Global train loss: 0.249, Global test loss: 3.228, Global test accuracy: 16.63
Round  85, Train loss: 0.230, Test loss: 0.478, Test accuracy: 84.37
Round  85, Global train loss: 0.230, Global test loss: 3.440, Global test accuracy: 20.69
Round  86, Train loss: 0.267, Test loss: 0.486, Test accuracy: 84.22
Round  86, Global train loss: 0.267, Global test loss: 2.954, Global test accuracy: 19.02
Round  87, Train loss: 0.235, Test loss: 0.479, Test accuracy: 84.12
Round  87, Global train loss: 0.235, Global test loss: 2.870, Global test accuracy: 18.52
Round  88, Train loss: 0.245, Test loss: 0.478, Test accuracy: 84.66
Round  88, Global train loss: 0.245, Global test loss: 3.545, Global test accuracy: 18.06
Round  89, Train loss: 0.257, Test loss: 0.463, Test accuracy: 85.08
Round  89, Global train loss: 0.257, Global test loss: 2.852, Global test accuracy: 20.34
Round  90, Train loss: 0.214, Test loss: 0.503, Test accuracy: 84.13
Round  90, Global train loss: 0.214, Global test loss: 3.562, Global test accuracy: 19.51
Round  91, Train loss: 0.235, Test loss: 0.512, Test accuracy: 84.05
Round  91, Global train loss: 0.235, Global test loss: 3.081, Global test accuracy: 18.66
Round  92, Train loss: 0.224, Test loss: 0.524, Test accuracy: 83.64
Round  92, Global train loss: 0.224, Global test loss: 3.537, Global test accuracy: 18.43
Round  93, Train loss: 0.202, Test loss: 0.549, Test accuracy: 83.22
Round  93, Global train loss: 0.202, Global test loss: 3.417, Global test accuracy: 17.63
Round  94, Train loss: 0.217, Test loss: 0.528, Test accuracy: 83.59
Round  94, Global train loss: 0.217, Global test loss: 3.314, Global test accuracy: 16.77
Round  95, Train loss: 0.243, Test loss: 0.515, Test accuracy: 83.77
Round  95, Global train loss: 0.243, Global test loss: 3.119, Global test accuracy: 19.16
Round  96, Train loss: 0.267, Test loss: 0.499, Test accuracy: 84.28
Round  96, Global train loss: 0.267, Global test loss: 3.092, Global test accuracy: 17.29
Round  97, Train loss: 0.234, Test loss: 0.492, Test accuracy: 84.67
Round  97, Global train loss: 0.234, Global test loss: 3.215, Global test accuracy: 19.62
Round  98, Train loss: 0.206, Test loss: 0.493, Test accuracy: 84.62
Round  98, Global train loss: 0.206, Global test loss: 3.329, Global test accuracy: 19.32
Round  99, Train loss: 0.231, Test loss: 0.494, Test accuracy: 84.71
Round  99, Global train loss: 0.231, Global test loss: 4.030, Global test accuracy: 18.62
Final Round, Train loss: 0.171, Test loss: 0.571, Test accuracy: 83.83
Final Round, Global train loss: 0.171, Global test loss: 4.030, Global test accuracy: 18.62
Average accuracy final 10 rounds: 84.06916666666667 

Average global accuracy final 10 rounds: 18.5025 

1968.029245376587
[1.6374099254608154, 3.274819850921631, 4.719018936157227, 6.163218021392822, 7.646232843399048, 9.129247665405273, 10.613030195236206, 12.096812725067139, 13.561444520950317, 15.026076316833496, 16.501564502716064, 17.977052688598633, 19.50841236114502, 21.039772033691406, 22.50470280647278, 23.96963357925415, 25.450195789337158, 26.930757999420166, 28.402263879776, 29.873769760131836, 31.337268114089966, 32.800766468048096, 34.2854528427124, 35.77013921737671, 37.24087429046631, 38.71160936355591, 40.17408895492554, 41.636568546295166, 43.11013054847717, 44.58369255065918, 46.05248403549194, 47.52127552032471, 48.92390561103821, 50.32653570175171, 51.738765478134155, 53.1509952545166, 54.59177207946777, 56.032548904418945, 57.494776248931885, 58.957003593444824, 60.42226505279541, 61.887526512145996, 63.29606294631958, 64.70459938049316, 66.1151967048645, 67.52579402923584, 68.98245310783386, 70.43911218643188, 71.8873610496521, 73.33560991287231, 74.8021719455719, 76.26873397827148, 77.74466323852539, 79.2205924987793, 80.71079421043396, 82.20099592208862, 83.68100547790527, 85.16101503372192, 86.62522625923157, 88.08943748474121, 89.56509494781494, 91.04075241088867, 92.52573013305664, 94.01070785522461, 95.48993277549744, 96.96915769577026, 98.44326424598694, 99.91737079620361, 101.40129065513611, 102.8852105140686, 104.38474702835083, 105.88428354263306, 107.37657189369202, 108.86886024475098, 110.36705017089844, 111.8652400970459, 113.34689664840698, 114.82855319976807, 116.32640290260315, 117.82425260543823, 119.28438925743103, 120.74452590942383, 122.21234202384949, 123.68015813827515, 125.14477181434631, 126.60938549041748, 128.06453657150269, 129.5196876525879, 130.931795835495, 132.3439040184021, 133.75069665908813, 135.15748929977417, 136.6038691997528, 138.05024909973145, 139.51205825805664, 140.97386741638184, 142.4522249698639, 143.93058252334595, 145.4160623550415, 146.90154218673706, 148.38417053222656, 149.86679887771606, 151.35243678092957, 152.83807468414307, 154.3109860420227, 155.78389739990234, 157.21077418327332, 158.6376509666443, 160.13415265083313, 161.63065433502197, 163.1211178302765, 164.611581325531, 166.11580681800842, 167.62003231048584, 169.10235357284546, 170.58467483520508, 172.06872415542603, 173.55277347564697, 175.03269052505493, 176.5126075744629, 178.00461626052856, 179.49662494659424, 180.98690533638, 182.47718572616577, 183.95188927650452, 185.42659282684326, 186.91154026985168, 188.3964877128601, 189.88750290870667, 191.37851810455322, 192.8627004623413, 194.3468828201294, 195.8471519947052, 197.347421169281, 198.8423833847046, 200.33734560012817, 201.82697558403015, 203.31660556793213, 204.79741024971008, 206.27821493148804, 207.7661509513855, 209.25408697128296, 210.73064255714417, 212.20719814300537, 213.6999671459198, 215.19273614883423, 216.6767635345459, 218.16079092025757, 219.6514654159546, 221.1421399116516, 222.62264966964722, 224.10315942764282, 225.59040212631226, 227.0776448249817, 228.55896878242493, 230.04029273986816, 231.53289890289307, 233.02550506591797, 234.49803686141968, 235.9705686569214, 237.4576714038849, 238.9447741508484, 240.43175840377808, 241.91874265670776, 243.39336514472961, 244.86798763275146, 246.35352492332458, 247.8390622138977, 249.16385960578918, 250.48865699768066, 251.81435561180115, 253.14005422592163, 254.4543595314026, 255.76866483688354, 257.0967495441437, 258.4248342514038, 259.76597595214844, 261.10711765289307, 262.44114923477173, 263.7751808166504, 265.1014173030853, 266.42765378952026, 267.74887800216675, 269.07010221481323, 270.3916153907776, 271.71312856674194, 273.03633642196655, 274.35954427719116, 275.6752624511719, 276.9909806251526, 278.3054449558258, 279.619909286499, 280.9433252811432, 282.26674127578735, 283.5957748889923, 284.92480850219727, 286.25609374046326, 287.58737897872925, 288.9286117553711, 290.26984453201294, 292.4969902038574, 294.7241358757019]
[27.55, 27.55, 40.69166666666667, 40.69166666666667, 47.99166666666667, 47.99166666666667, 56.15833333333333, 56.15833333333333, 59.925, 59.925, 66.75, 66.75, 71.225, 71.225, 72.225, 72.225, 73.78333333333333, 73.78333333333333, 75.03333333333333, 75.03333333333333, 75.44166666666666, 75.44166666666666, 75.48333333333333, 75.48333333333333, 75.96666666666667, 75.96666666666667, 76.11666666666666, 76.11666666666666, 76.125, 76.125, 76.625, 76.625, 77.48333333333333, 77.48333333333333, 77.95833333333333, 77.95833333333333, 78.33333333333333, 78.33333333333333, 78.58333333333333, 78.58333333333333, 78.575, 78.575, 79.59166666666667, 79.59166666666667, 79.40833333333333, 79.40833333333333, 79.84166666666667, 79.84166666666667, 79.84166666666667, 79.84166666666667, 79.74166666666666, 79.74166666666666, 79.725, 79.725, 80.05833333333334, 80.05833333333334, 80.55833333333334, 80.55833333333334, 80.68333333333334, 80.68333333333334, 80.575, 80.575, 80.79166666666667, 80.79166666666667, 80.59166666666667, 80.59166666666667, 79.88333333333334, 79.88333333333334, 81.275, 81.275, 81.625, 81.625, 82.125, 82.125, 82.25, 82.25, 81.91666666666667, 81.91666666666667, 82.45, 82.45, 82.43333333333334, 82.43333333333334, 82.61666666666666, 82.61666666666666, 82.61666666666666, 82.61666666666666, 82.33333333333333, 82.33333333333333, 82.04166666666667, 82.04166666666667, 81.74166666666666, 81.74166666666666, 81.775, 81.775, 82.71666666666667, 82.71666666666667, 82.21666666666667, 82.21666666666667, 82.31666666666666, 82.31666666666666, 82.525, 82.525, 82.99166666666666, 82.99166666666666, 83.325, 83.325, 83.46666666666667, 83.46666666666667, 83.74166666666666, 83.74166666666666, 83.725, 83.725, 83.725, 83.725, 83.6, 83.6, 83.925, 83.925, 83.83333333333333, 83.83333333333333, 83.63333333333334, 83.63333333333334, 83.625, 83.625, 83.775, 83.775, 83.65833333333333, 83.65833333333333, 83.30833333333334, 83.30833333333334, 83.56666666666666, 83.56666666666666, 83.45, 83.45, 83.50833333333334, 83.50833333333334, 83.50833333333334, 83.50833333333334, 83.64166666666667, 83.64166666666667, 83.43333333333334, 83.43333333333334, 83.49166666666666, 83.49166666666666, 83.69166666666666, 83.69166666666666, 83.975, 83.975, 84.25833333333334, 84.25833333333334, 84.00833333333334, 84.00833333333334, 83.85, 83.85, 83.825, 83.825, 84.05, 84.05, 84.29166666666667, 84.29166666666667, 84.1, 84.1, 83.925, 83.925, 83.99166666666666, 83.99166666666666, 83.85, 83.85, 84.275, 84.275, 84.36666666666666, 84.36666666666666, 84.225, 84.225, 84.125, 84.125, 84.65833333333333, 84.65833333333333, 85.08333333333333, 85.08333333333333, 84.13333333333334, 84.13333333333334, 84.05, 84.05, 83.64166666666667, 83.64166666666667, 83.225, 83.225, 83.59166666666667, 83.59166666666667, 83.76666666666667, 83.76666666666667, 84.275, 84.275, 84.675, 84.675, 84.625, 84.625, 84.70833333333333, 84.70833333333333, 83.825, 83.825]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 58364 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
fedper
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 59047 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10, level_n_system: 0.0 , level_n_lowerb:0.0  

Files already downloaded and verified
Files already downloaded and verified
lg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 8394 (global); Percentage 2.73 (8394/307842 
)
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_fedrep.py", line 242, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 656, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54864 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
Traceback (most recent call last):
  File "main_apfl.py", line 147, in <module>
    w_global, w_local, loss, indd = local.train(net=net_local.to(args.device),w_local=w_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 354, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54246 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_scaffold.py", line 150, in <module>
    w_local, loss, indd, count = local.train(net=net_local.to(args.device), idx=idx, lr=lr, c_list=c_list, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 237, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 55657 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedavg
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
learning rate, batch size: 0.01, 10 

Traceback (most recent call last):
  File "main_pfedme.py", line 236, in <module>
    w_local, loss, indd = local.train(net=net_local.to(args.device), idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx],w_locals = w_locals)
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 825, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 58906 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

Files already downloaded and verified
Files already downloaded and verified
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
[]
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
Traceback (most recent call last):
  File "main_ditto.py", line 182, in <module>
    w_k, loss, indd = local.train(net=net_global.to(args.device), idx=idx, lr=args.lr, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 504, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 54880 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac.py", line 232, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_glob, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 1272, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 59069 is out of bounds for axis 0 with size 50000
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC-K-Means%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 1, dataset: cifar10  

Files already downloaded and verified
Files already downloaded and verified
fedrep
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])
10
['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'conv2.weight', 'conv2.bias', 'conv1.weight', 'conv1.bias']
['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']
4800
4864
107264
107328
299328
299448
307128
307192
307832
307842
# Params: 307842 (local), 307192 (global); Percentage 99.79 (307192/307842)
learning rate, batch size: 0.01, 10
CNNCifar(
  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=1600, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=10, bias=True)
)
Traceback (most recent call last):
  File "main_fedpac_k_means.py", line 293, in <module>
    w_local, loss, indd, class_center_local, class_num = local.train(net=net_local.to(args.device), class_center_glob=class_center_grob_local, idx=idx, w_glob_keys=w_glob_keys, lr=args.lr, last=last, concept_matrix_local=concept_matrix[idx])
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 2181, in train
    for batch_idx, (images, labels) in enumerate(self.ldr_train):
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/ChenSM/code/FL_HLS/models/Update.py", line 35, in __getitem__
    image, label = self.dataset[self.idxs[item]]
  File "/home/ChenSM/anaconda3/envs/python38/lib/python3.8/site-packages/torchvision/datasets/cifar.py", line 111, in __getitem__
    img, target = self.data[index], self.targets[index]
IndexError: index 50247 is out of bounds for axis 0 with size 50000
