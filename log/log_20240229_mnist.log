nohup: ignoring input
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.273, Test loss: 2.177, Test accuracy: 30.91
Round   0, Global train loss: 2.273, Global test loss: 2.178, Global test accuracy: 31.34
Round   1, Train loss: 1.915, Test loss: 1.883, Test accuracy: 60.17
Round   1, Global train loss: 1.915, Global test loss: 1.756, Global test accuracy: 72.78
Round   2, Train loss: 1.770, Test loss: 1.820, Test accuracy: 65.71
Round   2, Global train loss: 1.770, Global test loss: 1.727, Global test accuracy: 74.11
Round   3, Train loss: 1.752, Test loss: 1.762, Test accuracy: 71.12
Round   3, Global train loss: 1.752, Global test loss: 1.709, Global test accuracy: 76.19
Round   4, Train loss: 1.706, Test loss: 1.724, Test accuracy: 75.22
Round   4, Global train loss: 1.706, Global test loss: 1.681, Global test accuracy: 78.96
Round   5, Train loss: 1.626, Test loss: 1.711, Test accuracy: 76.44
Round   5, Global train loss: 1.626, Global test loss: 1.639, Global test accuracy: 82.68
Round   6, Train loss: 1.641, Test loss: 1.699, Test accuracy: 77.41
Round   6, Global train loss: 1.641, Global test loss: 1.655, Global test accuracy: 81.47
Round   7, Train loss: 1.647, Test loss: 1.673, Test accuracy: 79.90
Round   7, Global train loss: 1.647, Global test loss: 1.653, Global test accuracy: 81.42
Round   8, Train loss: 1.586, Test loss: 1.665, Test accuracy: 80.71
Round   8, Global train loss: 1.586, Global test loss: 1.634, Global test accuracy: 82.58
Round   9, Train loss: 1.595, Test loss: 1.657, Test accuracy: 81.42
Round   9, Global train loss: 1.595, Global test loss: 1.634, Global test accuracy: 82.80
Round  10, Train loss: 1.564, Test loss: 1.649, Test accuracy: 82.15
Round  10, Global train loss: 1.564, Global test loss: 1.613, Global test accuracy: 85.39
Round  11, Train loss: 1.584, Test loss: 1.647, Test accuracy: 82.25
Round  11, Global train loss: 1.584, Global test loss: 1.634, Global test accuracy: 83.03
Round  12, Train loss: 1.554, Test loss: 1.643, Test accuracy: 82.62
Round  12, Global train loss: 1.554, Global test loss: 1.619, Global test accuracy: 84.17
Round  13, Train loss: 1.584, Test loss: 1.632, Test accuracy: 83.71
Round  13, Global train loss: 1.584, Global test loss: 1.625, Global test accuracy: 84.37
Round  14, Train loss: 1.566, Test loss: 1.625, Test accuracy: 84.37
Round  14, Global train loss: 1.566, Global test loss: 1.619, Global test accuracy: 84.21
Round  15, Train loss: 1.557, Test loss: 1.618, Test accuracy: 85.02
Round  15, Global train loss: 1.557, Global test loss: 1.614, Global test accuracy: 84.61
Round  16, Train loss: 1.542, Test loss: 1.613, Test accuracy: 85.59
Round  16, Global train loss: 1.542, Global test loss: 1.612, Global test accuracy: 85.03
Round  17, Train loss: 1.565, Test loss: 1.611, Test accuracy: 85.65
Round  17, Global train loss: 1.565, Global test loss: 1.631, Global test accuracy: 83.14
Round  18, Train loss: 1.544, Test loss: 1.607, Test accuracy: 86.10
Round  18, Global train loss: 1.544, Global test loss: 1.612, Global test accuracy: 84.96
Round  19, Train loss: 1.552, Test loss: 1.605, Test accuracy: 86.25
Round  19, Global train loss: 1.552, Global test loss: 1.613, Global test accuracy: 84.83
Round  20, Train loss: 1.563, Test loss: 1.601, Test accuracy: 86.65
Round  20, Global train loss: 1.563, Global test loss: 1.643, Global test accuracy: 81.67
Round  21, Train loss: 1.520, Test loss: 1.601, Test accuracy: 86.67
Round  21, Global train loss: 1.520, Global test loss: 1.607, Global test accuracy: 85.47
Round  22, Train loss: 1.524, Test loss: 1.600, Test accuracy: 86.76
Round  22, Global train loss: 1.524, Global test loss: 1.609, Global test accuracy: 85.06
Round  23, Train loss: 1.510, Test loss: 1.598, Test accuracy: 86.86
Round  23, Global train loss: 1.510, Global test loss: 1.578, Global test accuracy: 88.69
Round  24, Train loss: 1.488, Test loss: 1.597, Test accuracy: 86.94
Round  24, Global train loss: 1.488, Global test loss: 1.551, Global test accuracy: 91.52
Round  25, Train loss: 1.534, Test loss: 1.596, Test accuracy: 86.99
Round  25, Global train loss: 1.534, Global test loss: 1.618, Global test accuracy: 84.44
Round  26, Train loss: 1.520, Test loss: 1.596, Test accuracy: 87.01
Round  26, Global train loss: 1.520, Global test loss: 1.601, Global test accuracy: 86.20
Round  27, Train loss: 1.538, Test loss: 1.595, Test accuracy: 87.00
Round  27, Global train loss: 1.538, Global test loss: 1.622, Global test accuracy: 83.72
Round  28, Train loss: 1.517, Test loss: 1.595, Test accuracy: 87.07
Round  28, Global train loss: 1.517, Global test loss: 1.587, Global test accuracy: 87.89
Round  29, Train loss: 1.514, Test loss: 1.595, Test accuracy: 87.05
Round  29, Global train loss: 1.514, Global test loss: 1.584, Global test accuracy: 88.14
Round  30, Train loss: 1.516, Test loss: 1.594, Test accuracy: 87.08
Round  30, Global train loss: 1.516, Global test loss: 1.571, Global test accuracy: 89.53
Round  31, Train loss: 1.520, Test loss: 1.593, Test accuracy: 87.25
Round  31, Global train loss: 1.520, Global test loss: 1.609, Global test accuracy: 85.19
Round  32, Train loss: 1.512, Test loss: 1.590, Test accuracy: 87.53
Round  32, Global train loss: 1.512, Global test loss: 1.575, Global test accuracy: 89.08
Round  33, Train loss: 1.514, Test loss: 1.590, Test accuracy: 87.51
Round  33, Global train loss: 1.514, Global test loss: 1.600, Global test accuracy: 86.17
Round  34, Train loss: 1.486, Test loss: 1.588, Test accuracy: 87.68
Round  34, Global train loss: 1.486, Global test loss: 1.552, Global test accuracy: 91.31
Round  35, Train loss: 1.501, Test loss: 1.587, Test accuracy: 87.75
Round  35, Global train loss: 1.501, Global test loss: 1.559, Global test accuracy: 90.83
Round  36, Train loss: 1.527, Test loss: 1.584, Test accuracy: 88.06
Round  36, Global train loss: 1.527, Global test loss: 1.596, Global test accuracy: 86.87
Round  37, Train loss: 1.503, Test loss: 1.583, Test accuracy: 88.10
Round  37, Global train loss: 1.503, Global test loss: 1.567, Global test accuracy: 89.84
Round  38, Train loss: 1.515, Test loss: 1.583, Test accuracy: 88.11
Round  38, Global train loss: 1.515, Global test loss: 1.584, Global test accuracy: 88.25
Round  39, Train loss: 1.498, Test loss: 1.583, Test accuracy: 88.14
Round  39, Global train loss: 1.498, Global test loss: 1.563, Global test accuracy: 90.06
Round  40, Train loss: 1.505, Test loss: 1.580, Test accuracy: 88.50
Round  40, Global train loss: 1.505, Global test loss: 1.573, Global test accuracy: 89.08
Round  41, Train loss: 1.503, Test loss: 1.577, Test accuracy: 88.77
Round  41, Global train loss: 1.503, Global test loss: 1.572, Global test accuracy: 89.25
Round  42, Train loss: 1.498, Test loss: 1.577, Test accuracy: 88.72
Round  42, Global train loss: 1.498, Global test loss: 1.566, Global test accuracy: 89.82
Round  43, Train loss: 1.499, Test loss: 1.577, Test accuracy: 88.72
Round  43, Global train loss: 1.499, Global test loss: 1.572, Global test accuracy: 89.30
Round  44, Train loss: 1.496, Test loss: 1.577, Test accuracy: 88.72
Round  44, Global train loss: 1.496, Global test loss: 1.566, Global test accuracy: 90.01
Round  45, Train loss: 1.497, Test loss: 1.577, Test accuracy: 88.75
Round  45, Global train loss: 1.497, Global test loss: 1.565, Global test accuracy: 89.83
Round  46, Train loss: 1.513, Test loss: 1.577, Test accuracy: 88.77
Round  46, Global train loss: 1.513, Global test loss: 1.591, Global test accuracy: 87.34
Round  47, Train loss: 1.497, Test loss: 1.576, Test accuracy: 88.81
Round  47, Global train loss: 1.497, Global test loss: 1.566, Global test accuracy: 89.82
Round  48, Train loss: 1.482, Test loss: 1.576, Test accuracy: 88.79
Round  48, Global train loss: 1.482, Global test loss: 1.549, Global test accuracy: 91.58
Round  49, Train loss: 1.480, Test loss: 1.576, Test accuracy: 88.80
Round  49, Global train loss: 1.480, Global test loss: 1.549, Global test accuracy: 91.44
Round  50, Train loss: 1.495, Test loss: 1.576, Test accuracy: 88.78
Round  50, Global train loss: 1.495, Global test loss: 1.568, Global test accuracy: 89.56
Round  51, Train loss: 1.495, Test loss: 1.576, Test accuracy: 88.79
Round  51, Global train loss: 1.495, Global test loss: 1.566, Global test accuracy: 89.81
Round  52, Train loss: 1.497, Test loss: 1.575, Test accuracy: 88.82
Round  52, Global train loss: 1.497, Global test loss: 1.564, Global test accuracy: 90.06
Round  53, Train loss: 1.498, Test loss: 1.574, Test accuracy: 88.93
Round  53, Global train loss: 1.498, Global test loss: 1.553, Global test accuracy: 91.14
Round  54, Train loss: 1.480, Test loss: 1.574, Test accuracy: 88.95
Round  54, Global train loss: 1.480, Global test loss: 1.552, Global test accuracy: 91.27
Round  55, Train loss: 1.482, Test loss: 1.574, Test accuracy: 88.98
Round  55, Global train loss: 1.482, Global test loss: 1.549, Global test accuracy: 91.59
Round  56, Train loss: 1.485, Test loss: 1.571, Test accuracy: 89.22
Round  56, Global train loss: 1.485, Global test loss: 1.552, Global test accuracy: 91.34
Round  57, Train loss: 1.495, Test loss: 1.571, Test accuracy: 89.25
Round  57, Global train loss: 1.495, Global test loss: 1.567, Global test accuracy: 89.87
Round  58, Train loss: 1.480, Test loss: 1.571, Test accuracy: 89.27
Round  58, Global train loss: 1.480, Global test loss: 1.546, Global test accuracy: 91.79
Round  59, Train loss: 1.479, Test loss: 1.571, Test accuracy: 89.27
Round  59, Global train loss: 1.479, Global test loss: 1.549, Global test accuracy: 91.45
Round  60, Train loss: 1.480, Test loss: 1.571, Test accuracy: 89.29
Round  60, Global train loss: 1.480, Global test loss: 1.548, Global test accuracy: 91.49
Round  61, Train loss: 1.495, Test loss: 1.571, Test accuracy: 89.30
Round  61, Global train loss: 1.495, Global test loss: 1.561, Global test accuracy: 90.24
Round  62, Train loss: 1.496, Test loss: 1.571, Test accuracy: 89.28
Round  62, Global train loss: 1.496, Global test loss: 1.565, Global test accuracy: 89.85
Round  63, Train loss: 1.481, Test loss: 1.570, Test accuracy: 89.29
Round  63, Global train loss: 1.481, Global test loss: 1.546, Global test accuracy: 91.87
Round  64, Train loss: 1.495, Test loss: 1.570, Test accuracy: 89.29
Round  64, Global train loss: 1.495, Global test loss: 1.565, Global test accuracy: 89.72
Round  65, Train loss: 1.495, Test loss: 1.570, Test accuracy: 89.29
Round  65, Global train loss: 1.495, Global test loss: 1.567, Global test accuracy: 89.84
Round  66, Train loss: 1.479, Test loss: 1.570, Test accuracy: 89.30
Round  66, Global train loss: 1.479, Global test loss: 1.544, Global test accuracy: 91.88
Round  67, Train loss: 1.496, Test loss: 1.570, Test accuracy: 89.31
Round  67, Global train loss: 1.496, Global test loss: 1.561, Global test accuracy: 90.17
Round  68, Train loss: 1.478, Test loss: 1.570, Test accuracy: 89.31
Round  68, Global train loss: 1.478, Global test loss: 1.551, Global test accuracy: 91.18
Round  69, Train loss: 1.494, Test loss: 1.570, Test accuracy: 89.34
Round  69, Global train loss: 1.494, Global test loss: 1.562, Global test accuracy: 90.12
Round  70, Train loss: 1.477, Test loss: 1.570, Test accuracy: 89.32
Round  70, Global train loss: 1.477, Global test loss: 1.548, Global test accuracy: 91.56
Round  71, Train loss: 1.494, Test loss: 1.570, Test accuracy: 89.34
Round  71, Global train loss: 1.494, Global test loss: 1.561, Global test accuracy: 90.36
Round  72, Train loss: 1.495, Test loss: 1.570, Test accuracy: 89.36
Round  72, Global train loss: 1.495, Global test loss: 1.564, Global test accuracy: 89.92
Round  73, Train loss: 1.495, Test loss: 1.570, Test accuracy: 89.36
Round  73, Global train loss: 1.495, Global test loss: 1.559, Global test accuracy: 90.62
Round  74, Train loss: 1.478, Test loss: 1.570, Test accuracy: 89.34
Round  74, Global train loss: 1.478, Global test loss: 1.545, Global test accuracy: 91.89
Round  75, Train loss: 1.480, Test loss: 1.570, Test accuracy: 89.35
Round  75, Global train loss: 1.480, Global test loss: 1.547, Global test accuracy: 91.67
Round  76, Train loss: 1.509, Test loss: 1.570, Test accuracy: 89.34
Round  76, Global train loss: 1.509, Global test loss: 1.588, Global test accuracy: 87.50
Round  77, Train loss: 1.477, Test loss: 1.570, Test accuracy: 89.34
Round  77, Global train loss: 1.477, Global test loss: 1.545, Global test accuracy: 92.03
Round  78, Train loss: 1.490, Test loss: 1.568, Test accuracy: 89.58
Round  78, Global train loss: 1.490, Global test loss: 1.556, Global test accuracy: 90.79
Round  79, Train loss: 1.477, Test loss: 1.568, Test accuracy: 89.57
Round  79, Global train loss: 1.477, Global test loss: 1.548, Global test accuracy: 91.28
Round  80, Train loss: 1.483, Test loss: 1.566, Test accuracy: 89.67
Round  80, Global train loss: 1.483, Global test loss: 1.550, Global test accuracy: 91.48
Round  81, Train loss: 1.477, Test loss: 1.566, Test accuracy: 89.67
Round  81, Global train loss: 1.477, Global test loss: 1.546, Global test accuracy: 91.62
Round  82, Train loss: 1.496, Test loss: 1.566, Test accuracy: 89.70
Round  82, Global train loss: 1.496, Global test loss: 1.565, Global test accuracy: 90.06
Round  83, Train loss: 1.479, Test loss: 1.566, Test accuracy: 89.72
Round  83, Global train loss: 1.479, Global test loss: 1.547, Global test accuracy: 91.70
Round  84, Train loss: 1.481, Test loss: 1.566, Test accuracy: 89.75
Round  84, Global train loss: 1.481, Global test loss: 1.548, Global test accuracy: 91.71
Round  85, Train loss: 1.478, Test loss: 1.566, Test accuracy: 89.77
Round  85, Global train loss: 1.478, Global test loss: 1.549, Global test accuracy: 91.32
Round  86, Train loss: 1.479, Test loss: 1.565, Test accuracy: 89.76
Round  86, Global train loss: 1.479, Global test loss: 1.546, Global test accuracy: 91.69
Round  87, Train loss: 1.478, Test loss: 1.565, Test accuracy: 89.76
Round  87, Global train loss: 1.478, Global test loss: 1.545, Global test accuracy: 91.88
Round  88, Train loss: 1.496, Test loss: 1.565, Test accuracy: 89.77
Round  88, Global train loss: 1.496, Global test loss: 1.557, Global test accuracy: 90.65
Round  89, Train loss: 1.491, Test loss: 1.563, Test accuracy: 90.06
Round  89, Global train loss: 1.491, Global test loss: 1.551, Global test accuracy: 91.25
Round  90, Train loss: 1.479, Test loss: 1.563, Test accuracy: 90.05
Round  90, Global train loss: 1.479, Global test loss: 1.550, Global test accuracy: 91.32
Round  91, Train loss: 1.480, Test loss: 1.562, Test accuracy: 90.08
Round  91, Global train loss: 1.480, Global test loss: 1.549, Global test accuracy: 91.50
Round  92, Train loss: 1.478, Test loss: 1.563, Test accuracy: 90.06
Round  92, Global train loss: 1.478, Global test loss: 1.548, Global test accuracy: 91.44
Round  93, Train loss: 1.478, Test loss: 1.562, Test accuracy: 90.07
Round  93, Global train loss: 1.478, Global test loss: 1.549, Global test accuracy: 91.51
Round  94, Train loss: 1.478, Test loss: 1.562, Test accuracy: 90.07
Round  94, Global train loss: 1.478, Global test loss: 1.548, Global test accuracy: 91.61
Round  95, Train loss: 1.479, Test loss: 1.562, Test accuracy: 90.07
Round  95, Global train loss: 1.479, Global test loss: 1.547, Global test accuracy: 91.80/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.480, Test loss: 1.562, Test accuracy: 90.08
Round  96, Global train loss: 1.480, Global test loss: 1.548, Global test accuracy: 91.69
Round  97, Train loss: 1.480, Test loss: 1.562, Test accuracy: 90.10
Round  97, Global train loss: 1.480, Global test loss: 1.544, Global test accuracy: 92.04
Round  98, Train loss: 1.477, Test loss: 1.562, Test accuracy: 90.11
Round  98, Global train loss: 1.477, Global test loss: 1.547, Global test accuracy: 91.68
Round  99, Train loss: 1.481, Test loss: 1.562, Test accuracy: 90.09
Round  99, Global train loss: 1.481, Global test loss: 1.549, Global test accuracy: 91.47
Final Round, Train loss: 1.478, Test loss: 1.562, Test accuracy: 90.11
Final Round, Global train loss: 1.478, Global test loss: 1.549, Global test accuracy: 91.47
Average accuracy final 10 rounds: 90.07625 

Average global accuracy final 10 rounds: 91.6055 

6917.591922044754
[4.459770917892456, 8.919541835784912, 13.413642168045044, 17.907742500305176, 22.493781328201294, 27.079820156097412, 31.597607374191284, 36.115394592285156, 40.421486377716064, 44.72757816314697, 49.26205897331238, 53.79653978347778, 58.15240526199341, 62.50827074050903, 66.74406599998474, 70.97986125946045, 75.2633068561554, 79.54675245285034, 83.98903846740723, 88.43132448196411, 92.71031832695007, 96.98931217193604, 101.42594909667969, 105.86258602142334, 110.26012468338013, 114.65766334533691, 119.10497713088989, 123.55229091644287, 128.10199284553528, 132.65169477462769, 137.2017788887024, 141.7518630027771, 146.4427227973938, 151.1335825920105, 155.71876668930054, 160.30395078659058, 164.728422164917, 169.1528935432434, 173.68839693069458, 178.22390031814575, 182.69059491157532, 187.15728950500488, 191.55081224441528, 195.94433498382568, 200.39934849739075, 204.8543620109558, 209.25860381126404, 213.66284561157227, 218.07147812843323, 222.4801106452942, 227.04702496528625, 231.61393928527832, 236.14044666290283, 240.66695404052734, 245.15704035758972, 249.6471266746521, 254.20037508010864, 258.7536234855652, 263.3688805103302, 267.9841375350952, 272.5507369041443, 277.11733627319336, 281.7429909706116, 286.3686456680298, 290.86934781074524, 295.3700499534607, 299.8426094055176, 304.31516885757446, 308.71761894226074, 313.120069026947, 317.4240036010742, 321.7279381752014, 326.0562801361084, 330.3846220970154, 334.6578176021576, 338.9310131072998, 343.32436776161194, 347.7177224159241, 352.2812132835388, 356.84470415115356, 361.46989727020264, 366.0950903892517, 370.5738174915314, 375.05254459381104, 379.6377604007721, 384.22297620773315, 388.7784812450409, 393.33398628234863, 397.92604184150696, 402.5180974006653, 407.0791642665863, 411.6402311325073, 416.0511224269867, 420.46201372146606, 424.77369451522827, 429.0853753089905, 433.5958843231201, 438.10639333724976, 442.3249328136444, 446.54347229003906, 450.8288722038269, 455.11427211761475, 459.2915027141571, 463.46873331069946, 467.4275891780853, 471.3864450454712, 475.1604251861572, 478.93440532684326, 483.00157356262207, 487.0687417984009, 491.1442337036133, 495.2197256088257, 499.02229738235474, 502.8248691558838, 506.6860992908478, 510.54732942581177, 514.5723145008087, 518.5972995758057, 522.3808450698853, 526.1643905639648, 530.0553319454193, 533.9462733268738, 537.766143321991, 541.5860133171082, 545.3789811134338, 549.1719489097595, 553.0107340812683, 556.8495192527771, 560.6907720565796, 564.5320248603821, 568.3433697223663, 572.1547145843506, 576.0470118522644, 579.9393091201782, 583.7861886024475, 587.6330680847168, 591.4121644496918, 595.1912608146667, 598.9215993881226, 602.6519379615784, 606.4465861320496, 610.2412343025208, 614.1394171714783, 618.0376000404358, 621.8419678211212, 625.6463356018066, 629.5275685787201, 633.4088015556335, 637.3787059783936, 641.3486104011536, 645.1262335777283, 648.903856754303, 652.6064269542694, 656.3089971542358, 660.1769723892212, 664.0449476242065, 667.9616384506226, 671.8783292770386, 675.7696759700775, 679.6610226631165, 683.5214450359344, 687.3818674087524, 691.3166482448578, 695.2514290809631, 699.1335599422455, 703.0156908035278, 706.8228032588959, 710.6299157142639, 714.5417935848236, 718.4536714553833, 722.3210411071777, 726.1884107589722, 730.0032131671906, 733.8180155754089, 737.7373197078705, 741.656623840332, 745.5419297218323, 749.4272356033325, 753.1803045272827, 756.9333734512329, 760.6936845779419, 764.4539957046509, 768.2348873615265, 772.0157790184021, 775.8485205173492, 779.6812620162964, 783.4434902667999, 787.2057185173035, 790.989869594574, 794.7740206718445, 798.5769748687744, 802.3799290657043, 806.1487865447998, 809.9176440238953, 813.6525430679321, 817.387442111969, 821.1667258739471, 824.9460096359253, 828.7181763648987, 832.4903430938721, 834.2648136615753, 836.0392842292786]
[30.915, 30.915, 60.1725, 60.1725, 65.7125, 65.7125, 71.12, 71.12, 75.2225, 75.2225, 76.44, 76.44, 77.41, 77.41, 79.8975, 79.8975, 80.71, 80.71, 81.42, 81.42, 82.1475, 82.1475, 82.255, 82.255, 82.6225, 82.6225, 83.7125, 83.7125, 84.37, 84.37, 85.0225, 85.0225, 85.595, 85.595, 85.65, 85.65, 86.1, 86.1, 86.255, 86.255, 86.6525, 86.6525, 86.675, 86.675, 86.7575, 86.7575, 86.86, 86.86, 86.94, 86.94, 86.99, 86.99, 87.01, 87.01, 87.0, 87.0, 87.0675, 87.0675, 87.045, 87.045, 87.0825, 87.0825, 87.245, 87.245, 87.5325, 87.5325, 87.5075, 87.5075, 87.68, 87.68, 87.7475, 87.7475, 88.0575, 88.0575, 88.1, 88.1, 88.11, 88.11, 88.1425, 88.1425, 88.4975, 88.4975, 88.7675, 88.7675, 88.7225, 88.7225, 88.7175, 88.7175, 88.725, 88.725, 88.7475, 88.7475, 88.7725, 88.7725, 88.81, 88.81, 88.7875, 88.7875, 88.795, 88.795, 88.785, 88.785, 88.7875, 88.7875, 88.82, 88.82, 88.9325, 88.9325, 88.95, 88.95, 88.9775, 88.9775, 89.2225, 89.2225, 89.255, 89.255, 89.265, 89.265, 89.2675, 89.2675, 89.2925, 89.2925, 89.2975, 89.2975, 89.2775, 89.2775, 89.29, 89.29, 89.2925, 89.2925, 89.29, 89.29, 89.2975, 89.2975, 89.3075, 89.3075, 89.315, 89.315, 89.3375, 89.3375, 89.3225, 89.3225, 89.34, 89.34, 89.355, 89.355, 89.36, 89.36, 89.3425, 89.3425, 89.35, 89.35, 89.3375, 89.3375, 89.3425, 89.3425, 89.5775, 89.5775, 89.5725, 89.5725, 89.6725, 89.6725, 89.675, 89.675, 89.705, 89.705, 89.725, 89.725, 89.7475, 89.7475, 89.77, 89.77, 89.7575, 89.7575, 89.76, 89.76, 89.7675, 89.7675, 90.065, 90.065, 90.05, 90.05, 90.08, 90.08, 90.0575, 90.0575, 90.0675, 90.0675, 90.07, 90.07, 90.0675, 90.0675, 90.075, 90.075, 90.1025, 90.1025, 90.105, 90.105, 90.0875, 90.0875, 90.105, 90.105]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 11.42
Round   0, Global train loss: 2.302, Global test loss: 2.301, Global test accuracy: 11.55
Round   1, Train loss: 2.300, Test loss: 2.299, Test accuracy: 12.78
Round   1, Global train loss: 2.300, Global test loss: 2.299, Global test accuracy: 12.97
Round   2, Train loss: 2.298, Test loss: 2.297, Test accuracy: 14.18
Round   2, Global train loss: 2.298, Global test loss: 2.296, Global test accuracy: 14.17
Round   3, Train loss: 2.293, Test loss: 2.294, Test accuracy: 15.60
Round   3, Global train loss: 2.293, Global test loss: 2.291, Global test accuracy: 15.48
Round   4, Train loss: 2.288, Test loss: 2.289, Test accuracy: 18.43
Round   4, Global train loss: 2.288, Global test loss: 2.284, Global test accuracy: 20.13
Round   5, Train loss: 2.274, Test loss: 2.281, Test accuracy: 18.75
Round   5, Global train loss: 2.274, Global test loss: 2.265, Global test accuracy: 17.00
Round   6, Train loss: 2.241, Test loss: 2.266, Test accuracy: 22.02
Round   6, Global train loss: 2.241, Global test loss: 2.221, Global test accuracy: 30.80
Round   7, Train loss: 2.172, Test loss: 2.221, Test accuracy: 29.55
Round   7, Global train loss: 2.172, Global test loss: 2.112, Global test accuracy: 49.90
Round   8, Train loss: 2.013, Test loss: 2.136, Test accuracy: 38.15
Round   8, Global train loss: 2.013, Global test loss: 1.955, Global test accuracy: 54.45
Round   9, Train loss: 1.892, Test loss: 2.061, Test accuracy: 45.50
Round   9, Global train loss: 1.892, Global test loss: 1.853, Global test accuracy: 67.72
Round  10, Train loss: 1.815, Test loss: 1.956, Test accuracy: 54.67
Round  10, Global train loss: 1.815, Global test loss: 1.783, Global test accuracy: 74.02
Round  11, Train loss: 1.733, Test loss: 1.874, Test accuracy: 63.17
Round  11, Global train loss: 1.733, Global test loss: 1.729, Global test accuracy: 77.73
Round  12, Train loss: 1.684, Test loss: 1.790, Test accuracy: 72.45
Round  12, Global train loss: 1.684, Global test loss: 1.681, Global test accuracy: 83.28
Round  13, Train loss: 1.639, Test loss: 1.758, Test accuracy: 75.45
Round  13, Global train loss: 1.639, Global test loss: 1.650, Global test accuracy: 84.87
Round  14, Train loss: 1.611, Test loss: 1.726, Test accuracy: 78.10
Round  14, Global train loss: 1.611, Global test loss: 1.634, Global test accuracy: 85.77
Round  15, Train loss: 1.596, Test loss: 1.693, Test accuracy: 80.68
Round  15, Global train loss: 1.596, Global test loss: 1.618, Global test accuracy: 86.73
Round  16, Train loss: 1.577, Test loss: 1.675, Test accuracy: 81.92
Round  16, Global train loss: 1.577, Global test loss: 1.607, Global test accuracy: 87.47
Round  17, Train loss: 1.573, Test loss: 1.668, Test accuracy: 82.37
Round  17, Global train loss: 1.573, Global test loss: 1.605, Global test accuracy: 87.48
Round  18, Train loss: 1.563, Test loss: 1.644, Test accuracy: 83.92
Round  18, Global train loss: 1.563, Global test loss: 1.595, Global test accuracy: 88.33
Round  19, Train loss: 1.552, Test loss: 1.625, Test accuracy: 85.50
Round  19, Global train loss: 1.552, Global test loss: 1.591, Global test accuracy: 88.60
Round  20, Train loss: 1.548, Test loss: 1.621, Test accuracy: 85.85
Round  20, Global train loss: 1.548, Global test loss: 1.587, Global test accuracy: 88.68
Round  21, Train loss: 1.547, Test loss: 1.617, Test accuracy: 86.23
Round  21, Global train loss: 1.547, Global test loss: 1.583, Global test accuracy: 89.27
Round  22, Train loss: 1.549, Test loss: 1.612, Test accuracy: 86.42
Round  22, Global train loss: 1.549, Global test loss: 1.580, Global test accuracy: 89.33
Round  23, Train loss: 1.539, Test loss: 1.610, Test accuracy: 86.57
Round  23, Global train loss: 1.539, Global test loss: 1.576, Global test accuracy: 89.78
Round  24, Train loss: 1.537, Test loss: 1.594, Test accuracy: 87.87
Round  24, Global train loss: 1.537, Global test loss: 1.576, Global test accuracy: 89.57
Round  25, Train loss: 1.535, Test loss: 1.589, Test accuracy: 88.32
Round  25, Global train loss: 1.535, Global test loss: 1.573, Global test accuracy: 89.45
Round  26, Train loss: 1.529, Test loss: 1.586, Test accuracy: 88.70
Round  26, Global train loss: 1.529, Global test loss: 1.572, Global test accuracy: 89.98
Round  27, Train loss: 1.526, Test loss: 1.585, Test accuracy: 88.73
Round  27, Global train loss: 1.526, Global test loss: 1.571, Global test accuracy: 89.90
Round  28, Train loss: 1.525, Test loss: 1.580, Test accuracy: 89.03
Round  28, Global train loss: 1.525, Global test loss: 1.570, Global test accuracy: 90.28
Round  29, Train loss: 1.526, Test loss: 1.579, Test accuracy: 89.10
Round  29, Global train loss: 1.526, Global test loss: 1.569, Global test accuracy: 90.12
Round  30, Train loss: 1.523, Test loss: 1.577, Test accuracy: 89.35
Round  30, Global train loss: 1.523, Global test loss: 1.569, Global test accuracy: 90.15
Round  31, Train loss: 1.516, Test loss: 1.576, Test accuracy: 89.38
Round  31, Global train loss: 1.516, Global test loss: 1.568, Global test accuracy: 90.15
Round  32, Train loss: 1.519, Test loss: 1.575, Test accuracy: 89.48
Round  32, Global train loss: 1.519, Global test loss: 1.567, Global test accuracy: 90.32
Round  33, Train loss: 1.519, Test loss: 1.574, Test accuracy: 89.65
Round  33, Global train loss: 1.519, Global test loss: 1.566, Global test accuracy: 90.20
Round  34, Train loss: 1.516, Test loss: 1.572, Test accuracy: 89.80
Round  34, Global train loss: 1.516, Global test loss: 1.565, Global test accuracy: 90.28
Round  35, Train loss: 1.507, Test loss: 1.572, Test accuracy: 89.92
Round  35, Global train loss: 1.507, Global test loss: 1.566, Global test accuracy: 90.23
Round  36, Train loss: 1.509, Test loss: 1.572, Test accuracy: 89.80
Round  36, Global train loss: 1.509, Global test loss: 1.564, Global test accuracy: 90.47
Round  37, Train loss: 1.511, Test loss: 1.571, Test accuracy: 89.85
Round  37, Global train loss: 1.511, Global test loss: 1.564, Global test accuracy: 90.43
Round  38, Train loss: 1.516, Test loss: 1.570, Test accuracy: 89.80
Round  38, Global train loss: 1.516, Global test loss: 1.563, Global test accuracy: 90.40
Round  39, Train loss: 1.510, Test loss: 1.569, Test accuracy: 89.77
Round  39, Global train loss: 1.510, Global test loss: 1.561, Global test accuracy: 90.53
Round  40, Train loss: 1.514, Test loss: 1.568, Test accuracy: 89.83
Round  40, Global train loss: 1.514, Global test loss: 1.560, Global test accuracy: 90.73
Round  41, Train loss: 1.509, Test loss: 1.567, Test accuracy: 89.88
Round  41, Global train loss: 1.509, Global test loss: 1.560, Global test accuracy: 90.68
Round  42, Train loss: 1.505, Test loss: 1.565, Test accuracy: 90.20
Round  42, Global train loss: 1.505, Global test loss: 1.559, Global test accuracy: 90.85
Round  43, Train loss: 1.502, Test loss: 1.565, Test accuracy: 90.15
Round  43, Global train loss: 1.502, Global test loss: 1.560, Global test accuracy: 90.85
Round  44, Train loss: 1.502, Test loss: 1.565, Test accuracy: 90.20
Round  44, Global train loss: 1.502, Global test loss: 1.558, Global test accuracy: 90.95
Round  45, Train loss: 1.506, Test loss: 1.565, Test accuracy: 90.20
Round  45, Global train loss: 1.506, Global test loss: 1.558, Global test accuracy: 90.60
Round  46, Train loss: 1.508, Test loss: 1.564, Test accuracy: 90.22
Round  46, Global train loss: 1.508, Global test loss: 1.557, Global test accuracy: 90.90
Round  47, Train loss: 1.502, Test loss: 1.563, Test accuracy: 90.23
Round  47, Global train loss: 1.502, Global test loss: 1.557, Global test accuracy: 90.90
Round  48, Train loss: 1.508, Test loss: 1.563, Test accuracy: 90.33
Round  48, Global train loss: 1.508, Global test loss: 1.557, Global test accuracy: 91.05
Round  49, Train loss: 1.509, Test loss: 1.563, Test accuracy: 90.40
Round  49, Global train loss: 1.509, Global test loss: 1.555, Global test accuracy: 91.02
Round  50, Train loss: 1.502, Test loss: 1.562, Test accuracy: 90.43
Round  50, Global train loss: 1.502, Global test loss: 1.555, Global test accuracy: 91.00
Round  51, Train loss: 1.502, Test loss: 1.561, Test accuracy: 90.55
Round  51, Global train loss: 1.502, Global test loss: 1.554, Global test accuracy: 91.10
Round  52, Train loss: 1.496, Test loss: 1.560, Test accuracy: 90.77
Round  52, Global train loss: 1.496, Global test loss: 1.555, Global test accuracy: 90.83
Round  53, Train loss: 1.498, Test loss: 1.560, Test accuracy: 90.65
Round  53, Global train loss: 1.498, Global test loss: 1.554, Global test accuracy: 90.88
Round  54, Train loss: 1.498, Test loss: 1.560, Test accuracy: 90.67
Round  54, Global train loss: 1.498, Global test loss: 1.553, Global test accuracy: 91.15
Round  55, Train loss: 1.502, Test loss: 1.559, Test accuracy: 90.78
Round  55, Global train loss: 1.502, Global test loss: 1.553, Global test accuracy: 91.10
Round  56, Train loss: 1.497, Test loss: 1.558, Test accuracy: 90.82
Round  56, Global train loss: 1.497, Global test loss: 1.553, Global test accuracy: 91.03
Round  57, Train loss: 1.502, Test loss: 1.558, Test accuracy: 90.77
Round  57, Global train loss: 1.502, Global test loss: 1.553, Global test accuracy: 91.13
Round  58, Train loss: 1.496, Test loss: 1.558, Test accuracy: 90.82
Round  58, Global train loss: 1.496, Global test loss: 1.554, Global test accuracy: 90.88
Round  59, Train loss: 1.503, Test loss: 1.558, Test accuracy: 90.75
Round  59, Global train loss: 1.503, Global test loss: 1.553, Global test accuracy: 90.98
Round  60, Train loss: 1.498, Test loss: 1.557, Test accuracy: 90.78
Round  60, Global train loss: 1.498, Global test loss: 1.551, Global test accuracy: 91.30
Round  61, Train loss: 1.493, Test loss: 1.557, Test accuracy: 90.85
Round  61, Global train loss: 1.493, Global test loss: 1.551, Global test accuracy: 91.67
Round  62, Train loss: 1.492, Test loss: 1.556, Test accuracy: 90.88
Round  62, Global train loss: 1.492, Global test loss: 1.551, Global test accuracy: 91.43
Round  63, Train loss: 1.491, Test loss: 1.556, Test accuracy: 90.88
Round  63, Global train loss: 1.491, Global test loss: 1.551, Global test accuracy: 91.40
Round  64, Train loss: 1.495, Test loss: 1.556, Test accuracy: 90.93
Round  64, Global train loss: 1.495, Global test loss: 1.551, Global test accuracy: 91.43
Round  65, Train loss: 1.495, Test loss: 1.556, Test accuracy: 90.87
Round  65, Global train loss: 1.495, Global test loss: 1.550, Global test accuracy: 91.40
Round  66, Train loss: 1.493, Test loss: 1.556, Test accuracy: 90.85
Round  66, Global train loss: 1.493, Global test loss: 1.551, Global test accuracy: 91.30
Round  67, Train loss: 1.487, Test loss: 1.556, Test accuracy: 90.92
Round  67, Global train loss: 1.487, Global test loss: 1.550, Global test accuracy: 91.38
Round  68, Train loss: 1.504, Test loss: 1.556, Test accuracy: 91.00
Round  68, Global train loss: 1.504, Global test loss: 1.549, Global test accuracy: 91.57
Round  69, Train loss: 1.495, Test loss: 1.556, Test accuracy: 91.00
Round  69, Global train loss: 1.495, Global test loss: 1.550, Global test accuracy: 91.32
Round  70, Train loss: 1.494, Test loss: 1.555, Test accuracy: 91.03
Round  70, Global train loss: 1.494, Global test loss: 1.549, Global test accuracy: 91.52
Round  71, Train loss: 1.493, Test loss: 1.555, Test accuracy: 91.00
Round  71, Global train loss: 1.493, Global test loss: 1.549, Global test accuracy: 91.60
Round  72, Train loss: 1.493, Test loss: 1.554, Test accuracy: 91.07
Round  72, Global train loss: 1.493, Global test loss: 1.549, Global test accuracy: 91.58
Round  73, Train loss: 1.499, Test loss: 1.554, Test accuracy: 91.02
Round  73, Global train loss: 1.499, Global test loss: 1.549, Global test accuracy: 91.37
Round  74, Train loss: 1.492, Test loss: 1.554, Test accuracy: 91.08
Round  74, Global train loss: 1.492, Global test loss: 1.549, Global test accuracy: 91.43
Round  75, Train loss: 1.494, Test loss: 1.554, Test accuracy: 91.08
Round  75, Global train loss: 1.494, Global test loss: 1.548, Global test accuracy: 91.65
Round  76, Train loss: 1.497, Test loss: 1.553, Test accuracy: 91.08
Round  76, Global train loss: 1.497, Global test loss: 1.549, Global test accuracy: 91.42
Round  77, Train loss: 1.498, Test loss: 1.553, Test accuracy: 91.15
Round  77, Global train loss: 1.498, Global test loss: 1.548, Global test accuracy: 91.65
Round  78, Train loss: 1.496, Test loss: 1.552, Test accuracy: 91.13
Round  78, Global train loss: 1.496, Global test loss: 1.547, Global test accuracy: 91.85
Round  79, Train loss: 1.494, Test loss: 1.552, Test accuracy: 91.17
Round  79, Global train loss: 1.494, Global test loss: 1.547, Global test accuracy: 91.80
Round  80, Train loss: 1.497, Test loss: 1.551, Test accuracy: 91.20
Round  80, Global train loss: 1.497, Global test loss: 1.547, Global test accuracy: 91.67
Round  81, Train loss: 1.490, Test loss: 1.551, Test accuracy: 91.18
Round  81, Global train loss: 1.490, Global test loss: 1.547, Global test accuracy: 91.88
Round  82, Train loss: 1.486, Test loss: 1.550, Test accuracy: 91.13
Round  82, Global train loss: 1.486, Global test loss: 1.547, Global test accuracy: 91.85
Round  83, Train loss: 1.493, Test loss: 1.551, Test accuracy: 91.20
Round  83, Global train loss: 1.493, Global test loss: 1.546, Global test accuracy: 91.95
Round  84, Train loss: 1.492, Test loss: 1.550, Test accuracy: 91.20
Round  84, Global train loss: 1.492, Global test loss: 1.547, Global test accuracy: 91.62
Round  85, Train loss: 1.486, Test loss: 1.550, Test accuracy: 91.22
Round  85, Global train loss: 1.486, Global test loss: 1.547, Global test accuracy: 91.73
Round  86, Train loss: 1.494, Test loss: 1.550, Test accuracy: 91.37
Round  86, Global train loss: 1.494, Global test loss: 1.546, Global test accuracy: 91.87
Round  87, Train loss: 1.491, Test loss: 1.550, Test accuracy: 91.33
Round  87, Global train loss: 1.491, Global test loss: 1.546, Global test accuracy: 91.70
Round  88, Train loss: 1.495, Test loss: 1.550, Test accuracy: 91.38
Round  88, Global train loss: 1.495, Global test loss: 1.546, Global test accuracy: 91.75
Round  89, Train loss: 1.489, Test loss: 1.549, Test accuracy: 91.48
Round  89, Global train loss: 1.489, Global test loss: 1.545, Global test accuracy: 92.03
Round  90, Train loss: 1.489, Test loss: 1.549, Test accuracy: 91.62
Round  90, Global train loss: 1.489, Global test loss: 1.546, Global test accuracy: 91.67
Round  91, Train loss: 1.488, Test loss: 1.549, Test accuracy: 91.52
Round  91, Global train loss: 1.488, Global test loss: 1.545, Global test accuracy: 91.70
Round  92, Train loss: 1.490, Test loss: 1.548, Test accuracy: 91.50
Round  92, Global train loss: 1.490, Global test loss: 1.546, Global test accuracy: 91.72
Round  93, Train loss: 1.492, Test loss: 1.549, Test accuracy: 91.50
Round  93, Global train loss: 1.492, Global test loss: 1.545, Global test accuracy: 92.02
Round  94, Train loss: 1.494, Test loss: 1.548, Test accuracy: 91.55
Round  94, Global train loss: 1.494, Global test loss: 1.545, Global test accuracy: 91.95
Round  95, Train loss: 1.489, Test loss: 1.548, Test accuracy: 91.58
Round  95, Global train loss: 1.489, Global test loss: 1.545, Global test accuracy: 91.93/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.489, Test loss: 1.547, Test accuracy: 91.62
Round  96, Global train loss: 1.489, Global test loss: 1.544, Global test accuracy: 92.07
Round  97, Train loss: 1.486, Test loss: 1.547, Test accuracy: 91.53
Round  97, Global train loss: 1.486, Global test loss: 1.544, Global test accuracy: 92.25
Round  98, Train loss: 1.486, Test loss: 1.547, Test accuracy: 91.53
Round  98, Global train loss: 1.486, Global test loss: 1.543, Global test accuracy: 92.27
Round  99, Train loss: 1.490, Test loss: 1.546, Test accuracy: 91.67
Round  99, Global train loss: 1.490, Global test loss: 1.544, Global test accuracy: 91.87
Final Round, Train loss: 1.487, Test loss: 1.547, Test accuracy: 91.68
Final Round, Global train loss: 1.487, Global test loss: 1.544, Global test accuracy: 91.87
Average accuracy final 10 rounds: 91.56166666666667 

Average global accuracy final 10 rounds: 91.94333333333333 

977.4032974243164
[0.6697468757629395, 1.339493751525879, 1.9438354969024658, 2.5481772422790527, 3.1088802814483643, 3.669583320617676, 4.258597135543823, 4.847610950469971, 5.436311483383179, 6.025012016296387, 6.631711006164551, 7.238409996032715, 7.837512731552124, 8.436615467071533, 9.027275800704956, 9.617936134338379, 10.210380554199219, 10.802824974060059, 11.388527870178223, 11.974230766296387, 12.568295240402222, 13.162359714508057, 13.7737877368927, 14.385215759277344, 14.975435495376587, 15.56565523147583, 16.13998055458069, 16.714305877685547, 17.302565097808838, 17.89082431793213, 18.483375549316406, 19.075926780700684, 19.66649580001831, 20.257064819335938, 20.842816829681396, 21.428568840026855, 22.032199382781982, 22.63582992553711, 23.21036696434021, 23.78490400314331, 24.375717639923096, 24.96653127670288, 25.577393054962158, 26.188254833221436, 26.772189378738403, 27.35612392425537, 27.970235109329224, 28.584346294403076, 29.198636054992676, 29.812925815582275, 30.441287994384766, 31.069650173187256, 31.676222324371338, 32.28279447555542, 32.875444650650024, 33.46809482574463, 34.05328035354614, 34.638465881347656, 35.22044324874878, 35.8024206161499, 36.418960094451904, 37.035499572753906, 37.65621209144592, 38.27692461013794, 38.89348006248474, 39.51003551483154, 40.11040925979614, 40.71078300476074, 41.31148386001587, 41.912184715270996, 42.50017046928406, 43.08815622329712, 43.69985771179199, 44.311559200286865, 44.908979654312134, 45.5064001083374, 46.11319088935852, 46.71998167037964, 47.310832500457764, 47.90168333053589, 48.48891854286194, 49.07615375518799, 49.669724225997925, 50.26329469680786, 50.8579797744751, 51.452664852142334, 52.05905485153198, 52.66544485092163, 53.28329420089722, 53.9011435508728, 54.525362730026245, 55.14958190917969, 55.743807315826416, 56.338032722473145, 56.91798758506775, 57.49794244766235, 58.07806658744812, 58.65819072723389, 59.2576208114624, 59.85705089569092, 60.487892866134644, 61.11873483657837, 61.74492335319519, 62.37111186981201, 62.9808075428009, 63.590503215789795, 64.17308568954468, 64.75566816329956, 65.35937070846558, 65.96307325363159, 66.56614804267883, 67.16922283172607, 67.77599215507507, 68.38276147842407, 68.97611951828003, 69.56947755813599, 70.15826082229614, 70.7470440864563, 71.36254954338074, 71.97805500030518, 72.57171440124512, 73.16537380218506, 73.75563907623291, 74.34590435028076, 74.93758940696716, 75.52927446365356, 76.12226700782776, 76.71525955200195, 77.33013033866882, 77.9450011253357, 78.56381011009216, 79.18261909484863, 79.76240706443787, 80.3421950340271, 80.9276351928711, 81.51307535171509, 82.10349488258362, 82.69391441345215, 83.29095005989075, 83.88798570632935, 84.50995421409607, 85.1319227218628, 85.74923825263977, 86.36655378341675, 86.96524047851562, 87.5639271736145, 88.15161466598511, 88.73930215835571, 89.32691836357117, 89.91453456878662, 90.49616980552673, 91.07780504226685, 91.66222286224365, 92.24664068222046, 92.85873746871948, 93.4708342552185, 94.08301377296448, 94.69519329071045, 95.31157422065735, 95.92795515060425, 96.52596974372864, 97.12398433685303, 97.71223497390747, 98.30048561096191, 98.88914966583252, 99.47781372070312, 100.06923007965088, 100.66064643859863, 101.26320600509644, 101.86576557159424, 102.47278046607971, 103.07979536056519, 103.68067979812622, 104.28156423568726, 104.8732259273529, 105.46488761901855, 106.01910614967346, 106.57332468032837, 107.1517322063446, 107.73013973236084, 108.30126857757568, 108.87239742279053, 109.44607305526733, 110.01974868774414, 110.59016823768616, 111.16058778762817, 111.73967957496643, 112.31877136230469, 112.89339113235474, 113.46801090240479, 114.0498902797699, 114.63176965713501, 115.19919085502625, 115.76661205291748, 116.34255266189575, 116.91849327087402, 117.49784088134766, 118.07718849182129, 118.67559957504272, 119.27401065826416, 120.43511438369751, 121.59621810913086]
[11.416666666666666, 11.416666666666666, 12.783333333333333, 12.783333333333333, 14.183333333333334, 14.183333333333334, 15.6, 15.6, 18.433333333333334, 18.433333333333334, 18.75, 18.75, 22.016666666666666, 22.016666666666666, 29.55, 29.55, 38.15, 38.15, 45.5, 45.5, 54.666666666666664, 54.666666666666664, 63.166666666666664, 63.166666666666664, 72.45, 72.45, 75.45, 75.45, 78.1, 78.1, 80.68333333333334, 80.68333333333334, 81.91666666666667, 81.91666666666667, 82.36666666666666, 82.36666666666666, 83.91666666666667, 83.91666666666667, 85.5, 85.5, 85.85, 85.85, 86.23333333333333, 86.23333333333333, 86.41666666666667, 86.41666666666667, 86.56666666666666, 86.56666666666666, 87.86666666666666, 87.86666666666666, 88.31666666666666, 88.31666666666666, 88.7, 88.7, 88.73333333333333, 88.73333333333333, 89.03333333333333, 89.03333333333333, 89.1, 89.1, 89.35, 89.35, 89.38333333333334, 89.38333333333334, 89.48333333333333, 89.48333333333333, 89.65, 89.65, 89.8, 89.8, 89.91666666666667, 89.91666666666667, 89.8, 89.8, 89.85, 89.85, 89.8, 89.8, 89.76666666666667, 89.76666666666667, 89.83333333333333, 89.83333333333333, 89.88333333333334, 89.88333333333334, 90.2, 90.2, 90.15, 90.15, 90.2, 90.2, 90.2, 90.2, 90.21666666666667, 90.21666666666667, 90.23333333333333, 90.23333333333333, 90.33333333333333, 90.33333333333333, 90.4, 90.4, 90.43333333333334, 90.43333333333334, 90.55, 90.55, 90.76666666666667, 90.76666666666667, 90.65, 90.65, 90.66666666666667, 90.66666666666667, 90.78333333333333, 90.78333333333333, 90.81666666666666, 90.81666666666666, 90.76666666666667, 90.76666666666667, 90.81666666666666, 90.81666666666666, 90.75, 90.75, 90.78333333333333, 90.78333333333333, 90.85, 90.85, 90.88333333333334, 90.88333333333334, 90.88333333333334, 90.88333333333334, 90.93333333333334, 90.93333333333334, 90.86666666666666, 90.86666666666666, 90.85, 90.85, 90.91666666666667, 90.91666666666667, 91.0, 91.0, 91.0, 91.0, 91.03333333333333, 91.03333333333333, 91.0, 91.0, 91.06666666666666, 91.06666666666666, 91.01666666666667, 91.01666666666667, 91.08333333333333, 91.08333333333333, 91.08333333333333, 91.08333333333333, 91.08333333333333, 91.08333333333333, 91.15, 91.15, 91.13333333333334, 91.13333333333334, 91.16666666666667, 91.16666666666667, 91.2, 91.2, 91.18333333333334, 91.18333333333334, 91.13333333333334, 91.13333333333334, 91.2, 91.2, 91.2, 91.2, 91.21666666666667, 91.21666666666667, 91.36666666666666, 91.36666666666666, 91.33333333333333, 91.33333333333333, 91.38333333333334, 91.38333333333334, 91.48333333333333, 91.48333333333333, 91.61666666666666, 91.61666666666666, 91.51666666666667, 91.51666666666667, 91.5, 91.5, 91.5, 91.5, 91.55, 91.55, 91.58333333333333, 91.58333333333333, 91.61666666666666, 91.61666666666666, 91.53333333333333, 91.53333333333333, 91.53333333333333, 91.53333333333333, 91.66666666666667, 91.66666666666667, 91.68333333333334, 91.68333333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.288, Test loss: 2.295, Test accuracy: 21.39
Round   1, Train loss: 2.211, Test loss: 2.245, Test accuracy: 14.91
Round   2, Train loss: 1.996, Test loss: 2.163, Test accuracy: 27.02
Round   3, Train loss: 1.907, Test loss: 2.036, Test accuracy: 44.39
Round   4, Train loss: 1.820, Test loss: 1.954, Test accuracy: 50.22
Round   5, Train loss: 1.743, Test loss: 1.843, Test accuracy: 64.05
Round   6, Train loss: 1.698, Test loss: 1.791, Test accuracy: 69.41
Round   7, Train loss: 1.680, Test loss: 1.742, Test accuracy: 74.12
Round   8, Train loss: 1.602, Test loss: 1.713, Test accuracy: 76.49
Round   9, Train loss: 1.683, Test loss: 1.687, Test accuracy: 79.06
Round  10, Train loss: 1.568, Test loss: 1.649, Test accuracy: 82.95
Round  11, Train loss: 1.710, Test loss: 1.621, Test accuracy: 85.52
Round  12, Train loss: 1.509, Test loss: 1.617, Test accuracy: 85.61
Round  13, Train loss: 1.601, Test loss: 1.616, Test accuracy: 85.63
Round  14, Train loss: 1.609, Test loss: 1.610, Test accuracy: 86.15
Round  15, Train loss: 1.624, Test loss: 1.600, Test accuracy: 87.44
Round  16, Train loss: 1.573, Test loss: 1.595, Test accuracy: 87.47
Round  17, Train loss: 1.561, Test loss: 1.593, Test accuracy: 87.60
Round  18, Train loss: 1.527, Test loss: 1.592, Test accuracy: 87.42
Round  19, Train loss: 1.566, Test loss: 1.590, Test accuracy: 87.45
Round  20, Train loss: 1.546, Test loss: 1.588, Test accuracy: 87.70
Round  21, Train loss: 1.498, Test loss: 1.577, Test accuracy: 88.88
Round  22, Train loss: 1.559, Test loss: 1.566, Test accuracy: 90.08
Round  23, Train loss: 1.512, Test loss: 1.551, Test accuracy: 91.77
Round  24, Train loss: 1.532, Test loss: 1.526, Test accuracy: 94.20
Round  25, Train loss: 1.492, Test loss: 1.524, Test accuracy: 94.44
Round  26, Train loss: 1.496, Test loss: 1.522, Test accuracy: 94.62
Round  27, Train loss: 1.509, Test loss: 1.518, Test accuracy: 95.03
Round  28, Train loss: 1.500, Test loss: 1.517, Test accuracy: 95.19
Round  29, Train loss: 1.489, Test loss: 1.516, Test accuracy: 95.17
Round  30, Train loss: 1.484, Test loss: 1.516, Test accuracy: 95.24
Round  31, Train loss: 1.507, Test loss: 1.514, Test accuracy: 95.28
Round  32, Train loss: 1.504, Test loss: 1.510, Test accuracy: 95.74
Round  33, Train loss: 1.490, Test loss: 1.512, Test accuracy: 95.42
Round  34, Train loss: 1.494, Test loss: 1.511, Test accuracy: 95.53
Round  35, Train loss: 1.497, Test loss: 1.510, Test accuracy: 95.61
Round  36, Train loss: 1.484, Test loss: 1.509, Test accuracy: 95.70
Round  37, Train loss: 1.487, Test loss: 1.508, Test accuracy: 95.75
Round  38, Train loss: 1.487, Test loss: 1.508, Test accuracy: 95.81
Round  39, Train loss: 1.485, Test loss: 1.508, Test accuracy: 95.88
Round  40, Train loss: 1.490, Test loss: 1.506, Test accuracy: 96.02
Round  41, Train loss: 1.485, Test loss: 1.506, Test accuracy: 95.99
Round  42, Train loss: 1.481, Test loss: 1.505, Test accuracy: 96.01
Round  43, Train loss: 1.491, Test loss: 1.505, Test accuracy: 95.96
Round  44, Train loss: 1.484, Test loss: 1.505, Test accuracy: 95.94
Round  45, Train loss: 1.482, Test loss: 1.505, Test accuracy: 95.98
Round  46, Train loss: 1.481, Test loss: 1.505, Test accuracy: 95.97
Round  47, Train loss: 1.484, Test loss: 1.505, Test accuracy: 95.98
Round  48, Train loss: 1.486, Test loss: 1.503, Test accuracy: 96.27
Round  49, Train loss: 1.480, Test loss: 1.503, Test accuracy: 96.20
Round  50, Train loss: 1.482, Test loss: 1.502, Test accuracy: 96.25
Round  51, Train loss: 1.479, Test loss: 1.502, Test accuracy: 96.24
Round  52, Train loss: 1.477, Test loss: 1.501, Test accuracy: 96.29
Round  53, Train loss: 1.482, Test loss: 1.501, Test accuracy: 96.36
Round  54, Train loss: 1.478, Test loss: 1.500, Test accuracy: 96.51
Round  55, Train loss: 1.476, Test loss: 1.500, Test accuracy: 96.53
Round  56, Train loss: 1.476, Test loss: 1.499, Test accuracy: 96.46
Round  57, Train loss: 1.475, Test loss: 1.500, Test accuracy: 96.44
Round  58, Train loss: 1.476, Test loss: 1.500, Test accuracy: 96.47
Round  59, Train loss: 1.477, Test loss: 1.500, Test accuracy: 96.48
Round  60, Train loss: 1.473, Test loss: 1.500, Test accuracy: 96.53
Round  61, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.46
Round  62, Train loss: 1.474, Test loss: 1.500, Test accuracy: 96.46
Round  63, Train loss: 1.478, Test loss: 1.500, Test accuracy: 96.47
Round  64, Train loss: 1.477, Test loss: 1.499, Test accuracy: 96.48
Round  65, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.43
Round  66, Train loss: 1.478, Test loss: 1.499, Test accuracy: 96.41
Round  67, Train loss: 1.473, Test loss: 1.499, Test accuracy: 96.47
Round  68, Train loss: 1.474, Test loss: 1.498, Test accuracy: 96.53
Round  69, Train loss: 1.474, Test loss: 1.499, Test accuracy: 96.46
Round  70, Train loss: 1.482, Test loss: 1.498, Test accuracy: 96.61
Round  71, Train loss: 1.474, Test loss: 1.497, Test accuracy: 96.60
Round  72, Train loss: 1.478, Test loss: 1.497, Test accuracy: 96.67
Round  73, Train loss: 1.475, Test loss: 1.497, Test accuracy: 96.73
Round  74, Train loss: 1.475, Test loss: 1.496, Test accuracy: 96.70
Round  75, Train loss: 1.472, Test loss: 1.496, Test accuracy: 96.78
Round  76, Train loss: 1.472, Test loss: 1.496, Test accuracy: 96.78
Round  77, Train loss: 1.471, Test loss: 1.497, Test accuracy: 96.72
Round  78, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.67
Round  79, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.77
Round  80, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.79
Round  81, Train loss: 1.470, Test loss: 1.496, Test accuracy: 96.75
Round  82, Train loss: 1.471, Test loss: 1.496, Test accuracy: 96.78
Round  83, Train loss: 1.472, Test loss: 1.496, Test accuracy: 96.73
Round  84, Train loss: 1.479, Test loss: 1.495, Test accuracy: 96.77
Round  85, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.80
Round  86, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.76
Round  87, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.81
Round  88, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.83
Round  89, Train loss: 1.467, Test loss: 1.495, Test accuracy: 96.85
Round  90, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.85
Round  91, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.72
Round  92, Train loss: 1.475, Test loss: 1.495, Test accuracy: 96.78
Round  93, Train loss: 1.474, Test loss: 1.495, Test accuracy: 96.79
Round  94, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.78
Round  95, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.78
Round  96, Train loss: 1.467, Test loss: 1.495, Test accuracy: 96.87
Round  97, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.83
Round  98, Train loss: 1.470, Test loss: 1.495, Test accuracy: 96.79/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.468, Test loss: 1.495, Test accuracy: 96.77
Final Round, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.83
Average accuracy final 10 rounds: 96.79583333333333 

1442.7254874706268
[1.120415449142456, 2.240830898284912, 3.37375545501709, 4.506680011749268, 5.615207672119141, 6.723735332489014, 7.783222675323486, 8.842710018157959, 9.930948257446289, 11.01918649673462, 12.101230382919312, 13.183274269104004, 14.71011471748352, 16.236955165863037, 17.3759605884552, 18.514966011047363, 19.590999603271484, 20.667033195495605, 21.737858772277832, 22.80868434906006, 23.9047269821167, 25.00076961517334, 26.09469246864319, 27.188615322113037, 28.25943922996521, 29.330263137817383, 30.436585426330566, 31.54290771484375, 32.64718842506409, 33.751469135284424, 34.84639763832092, 35.94132614135742, 37.035460233688354, 38.12959432601929, 39.27353477478027, 40.41747522354126, 41.51572608947754, 42.61397695541382, 43.66757893562317, 44.72118091583252, 45.79107117652893, 46.86096143722534, 47.882728576660156, 48.90449571609497, 49.91975975036621, 50.93502378463745, 51.96355366706848, 52.99208354949951, 54.02015042304993, 55.04821729660034, 56.03298759460449, 57.01775789260864, 58.03080940246582, 59.043860912323, 60.08753037452698, 61.13119983673096, 62.15463852882385, 63.17807722091675, 64.16678810119629, 65.15549898147583, 66.18070197105408, 67.20590496063232, 68.22057175636292, 69.2352385520935, 70.20836615562439, 71.18149375915527, 72.18331027030945, 73.18512678146362, 74.20607686042786, 75.22702693939209, 76.24458527565002, 77.26214361190796, 78.25022101402283, 79.2382984161377, 80.23302245140076, 81.22774648666382, 82.2128221988678, 83.19789791107178, 84.22044134140015, 85.24298477172852, 86.27754831314087, 87.31211185455322, 88.30880951881409, 89.30550718307495, 90.31274509429932, 91.31998300552368, 92.33113765716553, 93.34229230880737, 94.37191581726074, 95.40153932571411, 96.39945769309998, 97.39737606048584, 98.38636636734009, 99.37535667419434, 100.40557527542114, 101.43579387664795, 102.47252345085144, 103.50925302505493, 104.51438879966736, 105.51952457427979, 106.52874755859375, 107.53797054290771, 108.56968855857849, 109.60140657424927, 110.65281534194946, 111.70422410964966, 112.72773599624634, 113.75124788284302, 114.7561411857605, 115.76103448867798, 116.77276706695557, 117.78449964523315, 118.81661772727966, 119.84873580932617, 120.86328506469727, 121.87783432006836, 122.87513136863708, 123.87242841720581, 124.89701223373413, 125.92159605026245, 126.97239756584167, 128.0231990814209, 129.08090543746948, 130.13861179351807, 131.19581270217896, 132.25301361083984, 133.26847457885742, 134.283935546875, 135.30946230888367, 136.33498907089233, 137.3899483680725, 138.44490766525269, 139.51567912101746, 140.58645057678223, 141.59118843078613, 142.59592628479004, 143.64248895645142, 144.6890516281128, 145.701922416687, 146.71479320526123, 147.77875471115112, 148.84271621704102, 149.871479511261, 150.90024280548096, 151.96305632591248, 153.025869846344, 154.06910729408264, 155.1123447418213, 156.1825659275055, 157.2527871131897, 158.28587293624878, 159.31895875930786, 160.34680891036987, 161.37465906143188, 162.4111452102661, 163.44763135910034, 164.47450590133667, 165.501380443573, 166.55416250228882, 167.60694456100464, 168.66412806510925, 169.72131156921387, 170.74513936042786, 171.76896715164185, 172.81460881233215, 173.86025047302246, 174.93058848381042, 176.0009264945984, 177.0643448829651, 178.1277632713318, 179.14309096336365, 180.1584186553955, 181.1981201171875, 182.2378215789795, 183.26115775108337, 184.28449392318726, 185.3607816696167, 186.43706941604614, 187.53686690330505, 188.63666439056396, 189.68487787246704, 190.73309135437012, 191.83255672454834, 192.93202209472656, 194.02446937561035, 195.11691665649414, 196.15255069732666, 197.18818473815918, 198.21233439445496, 199.23648405075073, 200.2811517715454, 201.3258194923401, 202.39459943771362, 203.46337938308716, 204.52595686912537, 205.58853435516357, 206.6189160346985, 207.6492977142334, 208.712895154953, 209.7764925956726, 211.48982000350952, 213.20314741134644]
[21.391666666666666, 21.391666666666666, 14.908333333333333, 14.908333333333333, 27.025, 27.025, 44.391666666666666, 44.391666666666666, 50.21666666666667, 50.21666666666667, 64.05, 64.05, 69.40833333333333, 69.40833333333333, 74.11666666666666, 74.11666666666666, 76.49166666666666, 76.49166666666666, 79.05833333333334, 79.05833333333334, 82.95, 82.95, 85.51666666666667, 85.51666666666667, 85.60833333333333, 85.60833333333333, 85.63333333333334, 85.63333333333334, 86.15, 86.15, 87.44166666666666, 87.44166666666666, 87.475, 87.475, 87.6, 87.6, 87.41666666666667, 87.41666666666667, 87.45, 87.45, 87.7, 87.7, 88.875, 88.875, 90.075, 90.075, 91.76666666666667, 91.76666666666667, 94.2, 94.2, 94.44166666666666, 94.44166666666666, 94.61666666666666, 94.61666666666666, 95.025, 95.025, 95.19166666666666, 95.19166666666666, 95.175, 95.175, 95.24166666666666, 95.24166666666666, 95.275, 95.275, 95.74166666666666, 95.74166666666666, 95.425, 95.425, 95.525, 95.525, 95.60833333333333, 95.60833333333333, 95.7, 95.7, 95.75, 95.75, 95.80833333333334, 95.80833333333334, 95.88333333333334, 95.88333333333334, 96.01666666666667, 96.01666666666667, 95.99166666666666, 95.99166666666666, 96.00833333333334, 96.00833333333334, 95.95833333333333, 95.95833333333333, 95.94166666666666, 95.94166666666666, 95.98333333333333, 95.98333333333333, 95.975, 95.975, 95.98333333333333, 95.98333333333333, 96.26666666666667, 96.26666666666667, 96.2, 96.2, 96.25, 96.25, 96.24166666666666, 96.24166666666666, 96.29166666666667, 96.29166666666667, 96.35833333333333, 96.35833333333333, 96.50833333333334, 96.50833333333334, 96.525, 96.525, 96.45833333333333, 96.45833333333333, 96.44166666666666, 96.44166666666666, 96.46666666666667, 96.46666666666667, 96.48333333333333, 96.48333333333333, 96.53333333333333, 96.53333333333333, 96.45833333333333, 96.45833333333333, 96.45833333333333, 96.45833333333333, 96.46666666666667, 96.46666666666667, 96.48333333333333, 96.48333333333333, 96.43333333333334, 96.43333333333334, 96.40833333333333, 96.40833333333333, 96.46666666666667, 96.46666666666667, 96.53333333333333, 96.53333333333333, 96.45833333333333, 96.45833333333333, 96.60833333333333, 96.60833333333333, 96.6, 96.6, 96.675, 96.675, 96.73333333333333, 96.73333333333333, 96.7, 96.7, 96.78333333333333, 96.78333333333333, 96.78333333333333, 96.78333333333333, 96.725, 96.725, 96.675, 96.675, 96.76666666666667, 96.76666666666667, 96.79166666666667, 96.79166666666667, 96.75, 96.75, 96.775, 96.775, 96.73333333333333, 96.73333333333333, 96.76666666666667, 96.76666666666667, 96.8, 96.8, 96.75833333333334, 96.75833333333334, 96.80833333333334, 96.80833333333334, 96.83333333333333, 96.83333333333333, 96.85, 96.85, 96.85, 96.85, 96.71666666666667, 96.71666666666667, 96.78333333333333, 96.78333333333333, 96.79166666666667, 96.79166666666667, 96.78333333333333, 96.78333333333333, 96.78333333333333, 96.78333333333333, 96.86666666666666, 96.86666666666666, 96.825, 96.825, 96.79166666666667, 96.79166666666667, 96.76666666666667, 96.76666666666667, 96.83333333333333, 96.83333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.300, Test loss: 2.298, Test accuracy: 20.43
Round   1, Train loss: 2.296, Test loss: 2.292, Test accuracy: 40.80
Round   2, Train loss: 2.285, Test loss: 2.274, Test accuracy: 54.83
Round   3, Train loss: 2.231, Test loss: 2.166, Test accuracy: 54.03
Round   4, Train loss: 2.010, Test loss: 1.930, Test accuracy: 62.60
Round   5, Train loss: 1.845, Test loss: 1.829, Test accuracy: 67.68
Round   6, Train loss: 1.767, Test loss: 1.771, Test accuracy: 72.20
Round   7, Train loss: 1.702, Test loss: 1.726, Test accuracy: 75.94
Round   8, Train loss: 1.666, Test loss: 1.697, Test accuracy: 78.49
Round   9, Train loss: 1.665, Test loss: 1.674, Test accuracy: 80.61
Round  10, Train loss: 1.643, Test loss: 1.652, Test accuracy: 82.39
Round  11, Train loss: 1.628, Test loss: 1.646, Test accuracy: 82.69
Round  12, Train loss: 1.632, Test loss: 1.633, Test accuracy: 83.92
Round  13, Train loss: 1.616, Test loss: 1.631, Test accuracy: 84.05
Round  14, Train loss: 1.622, Test loss: 1.624, Test accuracy: 84.57
Round  15, Train loss: 1.610, Test loss: 1.624, Test accuracy: 84.53
Round  16, Train loss: 1.613, Test loss: 1.622, Test accuracy: 84.47
Round  17, Train loss: 1.608, Test loss: 1.618, Test accuracy: 84.94
Round  18, Train loss: 1.599, Test loss: 1.618, Test accuracy: 84.92
Round  19, Train loss: 1.604, Test loss: 1.616, Test accuracy: 85.08
Round  20, Train loss: 1.599, Test loss: 1.615, Test accuracy: 85.08
Round  21, Train loss: 1.609, Test loss: 1.613, Test accuracy: 85.27
Round  22, Train loss: 1.594, Test loss: 1.612, Test accuracy: 85.33
Round  23, Train loss: 1.594, Test loss: 1.612, Test accuracy: 85.39
Round  24, Train loss: 1.582, Test loss: 1.608, Test accuracy: 85.74
Round  25, Train loss: 1.583, Test loss: 1.606, Test accuracy: 86.07
Round  26, Train loss: 1.579, Test loss: 1.604, Test accuracy: 86.19
Round  27, Train loss: 1.586, Test loss: 1.603, Test accuracy: 86.38
Round  28, Train loss: 1.574, Test loss: 1.598, Test accuracy: 86.85
Round  29, Train loss: 1.564, Test loss: 1.596, Test accuracy: 86.97
Round  30, Train loss: 1.576, Test loss: 1.595, Test accuracy: 86.94
Round  31, Train loss: 1.552, Test loss: 1.593, Test accuracy: 87.21
Round  32, Train loss: 1.563, Test loss: 1.592, Test accuracy: 87.26
Round  33, Train loss: 1.551, Test loss: 1.590, Test accuracy: 87.59
Round  34, Train loss: 1.560, Test loss: 1.587, Test accuracy: 87.89
Round  35, Train loss: 1.556, Test loss: 1.584, Test accuracy: 88.17
Round  36, Train loss: 1.539, Test loss: 1.579, Test accuracy: 88.75
Round  37, Train loss: 1.533, Test loss: 1.576, Test accuracy: 89.03
Round  38, Train loss: 1.540, Test loss: 1.575, Test accuracy: 88.95
Round  39, Train loss: 1.548, Test loss: 1.574, Test accuracy: 89.04
Round  40, Train loss: 1.512, Test loss: 1.571, Test accuracy: 89.39
Round  41, Train loss: 1.524, Test loss: 1.569, Test accuracy: 89.59
Round  42, Train loss: 1.513, Test loss: 1.566, Test accuracy: 89.96
Round  43, Train loss: 1.508, Test loss: 1.561, Test accuracy: 90.35
Round  44, Train loss: 1.509, Test loss: 1.559, Test accuracy: 90.60
Round  45, Train loss: 1.528, Test loss: 1.560, Test accuracy: 90.55
Round  46, Train loss: 1.511, Test loss: 1.556, Test accuracy: 90.73
Round  47, Train loss: 1.514, Test loss: 1.553, Test accuracy: 91.14
Round  48, Train loss: 1.505, Test loss: 1.549, Test accuracy: 91.54
Round  49, Train loss: 1.508, Test loss: 1.546, Test accuracy: 91.81
Round  50, Train loss: 1.500, Test loss: 1.545, Test accuracy: 91.95
Round  51, Train loss: 1.500, Test loss: 1.545, Test accuracy: 91.98
Round  52, Train loss: 1.497, Test loss: 1.544, Test accuracy: 91.95
Round  53, Train loss: 1.503, Test loss: 1.543, Test accuracy: 92.07
Round  54, Train loss: 1.501, Test loss: 1.542, Test accuracy: 92.17
Round  55, Train loss: 1.493, Test loss: 1.542, Test accuracy: 92.17
Round  56, Train loss: 1.493, Test loss: 1.542, Test accuracy: 92.11
Round  57, Train loss: 1.498, Test loss: 1.542, Test accuracy: 92.19
Round  58, Train loss: 1.497, Test loss: 1.541, Test accuracy: 92.23
Round  59, Train loss: 1.499, Test loss: 1.541, Test accuracy: 92.36
Round  60, Train loss: 1.495, Test loss: 1.540, Test accuracy: 92.35
Round  61, Train loss: 1.494, Test loss: 1.540, Test accuracy: 92.28
Round  62, Train loss: 1.495, Test loss: 1.540, Test accuracy: 92.43
Round  63, Train loss: 1.493, Test loss: 1.540, Test accuracy: 92.38
Round  64, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.36
Round  65, Train loss: 1.494, Test loss: 1.539, Test accuracy: 92.43
Round  66, Train loss: 1.491, Test loss: 1.539, Test accuracy: 92.46
Round  67, Train loss: 1.491, Test loss: 1.538, Test accuracy: 92.47
Round  68, Train loss: 1.492, Test loss: 1.538, Test accuracy: 92.48
Round  69, Train loss: 1.494, Test loss: 1.538, Test accuracy: 92.55
Round  70, Train loss: 1.491, Test loss: 1.538, Test accuracy: 92.61
Round  71, Train loss: 1.495, Test loss: 1.537, Test accuracy: 92.65
Round  72, Train loss: 1.491, Test loss: 1.537, Test accuracy: 92.63
Round  73, Train loss: 1.486, Test loss: 1.537, Test accuracy: 92.61
Round  74, Train loss: 1.492, Test loss: 1.537, Test accuracy: 92.53
Round  75, Train loss: 1.492, Test loss: 1.537, Test accuracy: 92.59
Round  76, Train loss: 1.491, Test loss: 1.536, Test accuracy: 92.61
Round  77, Train loss: 1.490, Test loss: 1.536, Test accuracy: 92.66
Round  78, Train loss: 1.489, Test loss: 1.536, Test accuracy: 92.62
Round  79, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.58
Round  80, Train loss: 1.490, Test loss: 1.536, Test accuracy: 92.62
Round  81, Train loss: 1.486, Test loss: 1.536, Test accuracy: 92.69
Round  82, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.73
Round  83, Train loss: 1.486, Test loss: 1.536, Test accuracy: 92.59
Round  84, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.68
Round  85, Train loss: 1.491, Test loss: 1.535, Test accuracy: 92.83
Round  86, Train loss: 1.486, Test loss: 1.535, Test accuracy: 92.78
Round  87, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.76
Round  88, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.72
Round  89, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.78
Round  90, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.77
Round  91, Train loss: 1.487, Test loss: 1.535, Test accuracy: 92.74
Round  92, Train loss: 1.486, Test loss: 1.534, Test accuracy: 92.79
Round  93, Train loss: 1.489, Test loss: 1.534, Test accuracy: 92.88
Round  94, Train loss: 1.486, Test loss: 1.534, Test accuracy: 92.83
Round  95, Train loss: 1.485, Test loss: 1.534, Test accuracy: 92.76
Round  96, Train loss: 1.491, Test loss: 1.534, Test accuracy: 92.88
Round  97, Train loss: 1.485, Test loss: 1.534, Test accuracy: 92.84
Round  98, Train loss: 1.486, Test loss: 1.533, Test accuracy: 92.86/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.484, Test loss: 1.534, Test accuracy: 92.84
Final Round, Train loss: 1.488, Test loss: 1.534, Test accuracy: 92.89
Average accuracy final 10 rounds: 92.82 

1481.825293302536
[1.2956876754760742, 2.5913753509521484, 3.6844804286956787, 4.777585506439209, 5.846227169036865, 6.9148688316345215, 8.041343212127686, 9.16781759262085, 10.290750741958618, 11.413683891296387, 12.509897232055664, 13.606110572814941, 14.711363792419434, 15.816617012023926, 16.94323706626892, 18.069857120513916, 19.168753385543823, 20.26764965057373, 21.362749814987183, 22.457849979400635, 23.571162223815918, 24.6844744682312, 25.80049157142639, 26.916508674621582, 28.01081895828247, 29.10512924194336, 30.186632871627808, 31.268136501312256, 32.341503381729126, 33.414870262145996, 34.52336239814758, 35.63185453414917, 36.76066470146179, 37.889474868774414, 39.00642442703247, 40.12337398529053, 41.20002770423889, 42.276681423187256, 43.38075804710388, 44.48483467102051, 45.57628870010376, 46.66774272918701, 47.774776220321655, 48.8818097114563, 49.9916729927063, 51.1015362739563, 52.176379442214966, 53.25122261047363, 54.352360010147095, 55.45349740982056, 56.5675323009491, 57.68156719207764, 58.79674744606018, 59.911927700042725, 60.987605810165405, 62.063283920288086, 63.15788459777832, 64.25248527526855, 65.37565994262695, 66.49883460998535, 67.57703018188477, 68.65522575378418, 69.72152996063232, 70.78783416748047, 71.89613795280457, 73.00444173812866, 74.12093186378479, 75.23742198944092, 76.30217480659485, 77.36692762374878, 78.47710943222046, 79.58729124069214, 80.69390392303467, 81.8005166053772, 82.8646879196167, 83.9288592338562, 85.01861453056335, 86.10836982727051, 87.18369483947754, 88.25901985168457, 89.40153932571411, 90.54405879974365, 91.66333389282227, 92.78260898590088, 93.86582708358765, 94.94904518127441, 95.99155473709106, 97.03406429290771, 98.10132837295532, 99.16859245300293, 100.20060396194458, 101.23261547088623, 102.30576467514038, 103.37891387939453, 104.45451164245605, 105.53010940551758, 106.59945464134216, 107.66879987716675, 108.73723578453064, 109.80567169189453, 110.836660861969, 111.86765003204346, 112.94342255592346, 114.01919507980347, 115.09298896789551, 116.16678285598755, 117.26770448684692, 118.3686261177063, 119.41151213645935, 120.4543981552124, 121.54825806617737, 122.64211797714233, 123.72952675819397, 124.8169355392456, 125.87107992172241, 126.92522430419922, 128.0085322856903, 129.0918402671814, 130.17123126983643, 131.25062227249146, 132.3093740940094, 133.36812591552734, 134.47296476364136, 135.57780361175537, 136.71161079406738, 137.8454179763794, 138.9069561958313, 139.9684944152832, 141.015527009964, 142.06255960464478, 143.12389063835144, 144.1852216720581, 145.32638692855835, 146.4675521850586, 147.60521507263184, 148.74287796020508, 149.8131868839264, 150.8834958076477, 151.93532419204712, 152.98715257644653, 154.07862639427185, 155.17010021209717, 156.25802969932556, 157.34595918655396, 158.39921689033508, 159.4524745941162, 160.50379967689514, 161.55512475967407, 162.63248896598816, 163.70985317230225, 164.85475969314575, 165.99966621398926, 167.1042230129242, 168.20877981185913, 169.23231601715088, 170.25585222244263, 171.3341748714447, 172.41249752044678, 173.4610652923584, 174.50963306427002, 175.59642362594604, 176.68321418762207, 177.74515867233276, 178.80710315704346, 179.8863353729248, 180.96556758880615, 182.0795066356659, 183.19344568252563, 184.22712182998657, 185.2607979774475, 186.35772347450256, 187.45464897155762, 188.58970189094543, 189.72475481033325, 190.80292630195618, 191.8810977935791, 192.8983438014984, 193.91558980941772, 194.98826551437378, 196.06094121932983, 197.13368558883667, 198.2064299583435, 199.28521180152893, 200.36399364471436, 201.417498588562, 202.47100353240967, 203.5449378490448, 204.61887216567993, 205.7398145198822, 206.86075687408447, 207.95552825927734, 209.05029964447021, 210.1256730556488, 211.2010464668274, 212.24617719650269, 213.29130792617798, 214.3745572566986, 215.45780658721924, 216.54944801330566, 217.6410894393921, 219.35544800758362, 221.06980657577515]
[20.433333333333334, 20.433333333333334, 40.8, 40.8, 54.833333333333336, 54.833333333333336, 54.03333333333333, 54.03333333333333, 62.6, 62.6, 67.68333333333334, 67.68333333333334, 72.2, 72.2, 75.94166666666666, 75.94166666666666, 78.49166666666666, 78.49166666666666, 80.60833333333333, 80.60833333333333, 82.39166666666667, 82.39166666666667, 82.69166666666666, 82.69166666666666, 83.925, 83.925, 84.05, 84.05, 84.56666666666666, 84.56666666666666, 84.525, 84.525, 84.475, 84.475, 84.94166666666666, 84.94166666666666, 84.91666666666667, 84.91666666666667, 85.075, 85.075, 85.08333333333333, 85.08333333333333, 85.26666666666667, 85.26666666666667, 85.33333333333333, 85.33333333333333, 85.39166666666667, 85.39166666666667, 85.74166666666666, 85.74166666666666, 86.06666666666666, 86.06666666666666, 86.19166666666666, 86.19166666666666, 86.375, 86.375, 86.85, 86.85, 86.96666666666667, 86.96666666666667, 86.94166666666666, 86.94166666666666, 87.20833333333333, 87.20833333333333, 87.25833333333334, 87.25833333333334, 87.59166666666667, 87.59166666666667, 87.89166666666667, 87.89166666666667, 88.175, 88.175, 88.75, 88.75, 89.025, 89.025, 88.95, 88.95, 89.04166666666667, 89.04166666666667, 89.39166666666667, 89.39166666666667, 89.59166666666667, 89.59166666666667, 89.95833333333333, 89.95833333333333, 90.35, 90.35, 90.6, 90.6, 90.55, 90.55, 90.73333333333333, 90.73333333333333, 91.14166666666667, 91.14166666666667, 91.54166666666667, 91.54166666666667, 91.80833333333334, 91.80833333333334, 91.95, 91.95, 91.98333333333333, 91.98333333333333, 91.95, 91.95, 92.06666666666666, 92.06666666666666, 92.16666666666667, 92.16666666666667, 92.175, 92.175, 92.10833333333333, 92.10833333333333, 92.19166666666666, 92.19166666666666, 92.23333333333333, 92.23333333333333, 92.35833333333333, 92.35833333333333, 92.35, 92.35, 92.275, 92.275, 92.43333333333334, 92.43333333333334, 92.38333333333334, 92.38333333333334, 92.35833333333333, 92.35833333333333, 92.43333333333334, 92.43333333333334, 92.45833333333333, 92.45833333333333, 92.475, 92.475, 92.48333333333333, 92.48333333333333, 92.55, 92.55, 92.60833333333333, 92.60833333333333, 92.65, 92.65, 92.63333333333334, 92.63333333333334, 92.60833333333333, 92.60833333333333, 92.53333333333333, 92.53333333333333, 92.59166666666667, 92.59166666666667, 92.60833333333333, 92.60833333333333, 92.65833333333333, 92.65833333333333, 92.625, 92.625, 92.575, 92.575, 92.61666666666666, 92.61666666666666, 92.69166666666666, 92.69166666666666, 92.73333333333333, 92.73333333333333, 92.59166666666667, 92.59166666666667, 92.68333333333334, 92.68333333333334, 92.83333333333333, 92.83333333333333, 92.775, 92.775, 92.75833333333334, 92.75833333333334, 92.725, 92.725, 92.78333333333333, 92.78333333333333, 92.76666666666667, 92.76666666666667, 92.74166666666666, 92.74166666666666, 92.79166666666667, 92.79166666666667, 92.88333333333334, 92.88333333333334, 92.83333333333333, 92.83333333333333, 92.75833333333334, 92.75833333333334, 92.88333333333334, 92.88333333333334, 92.84166666666667, 92.84166666666667, 92.85833333333333, 92.85833333333333, 92.84166666666667, 92.84166666666667, 92.89166666666667, 92.89166666666667]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.294, Test loss: 2.267, Test accuracy: 38.34
Round   1, Train loss: 1.962, Test loss: 1.851, Test accuracy: 71.65
Round   2, Train loss: 1.738, Test loss: 1.790, Test accuracy: 73.42
Round   3, Train loss: 1.718, Test loss: 1.744, Test accuracy: 74.76
Round   4, Train loss: 1.697, Test loss: 1.723, Test accuracy: 75.44
Round   5, Train loss: 1.684, Test loss: 1.699, Test accuracy: 76.31
Round   6, Train loss: 1.628, Test loss: 1.661, Test accuracy: 81.08
Round   7, Train loss: 1.606, Test loss: 1.654, Test accuracy: 81.49
Round   8, Train loss: 1.599, Test loss: 1.649, Test accuracy: 81.81
Round   9, Train loss: 1.595, Test loss: 1.646, Test accuracy: 82.02
Round  10, Train loss: 1.598, Test loss: 1.641, Test accuracy: 82.36
Round  11, Train loss: 1.591, Test loss: 1.639, Test accuracy: 82.51
Round  12, Train loss: 1.587, Test loss: 1.637, Test accuracy: 82.69
Round  13, Train loss: 1.585, Test loss: 1.636, Test accuracy: 82.79
Round  14, Train loss: 1.575, Test loss: 1.635, Test accuracy: 82.84
Round  15, Train loss: 1.574, Test loss: 1.635, Test accuracy: 82.90
Round  16, Train loss: 1.574, Test loss: 1.634, Test accuracy: 82.90
Round  17, Train loss: 1.579, Test loss: 1.634, Test accuracy: 82.98
Round  18, Train loss: 1.577, Test loss: 1.633, Test accuracy: 83.00
Round  19, Train loss: 1.581, Test loss: 1.633, Test accuracy: 82.97
Round  20, Train loss: 1.575, Test loss: 1.633, Test accuracy: 83.02
Round  21, Train loss: 1.575, Test loss: 1.633, Test accuracy: 83.03
Round  22, Train loss: 1.586, Test loss: 1.631, Test accuracy: 83.19
Round  23, Train loss: 1.578, Test loss: 1.631, Test accuracy: 83.24
Round  24, Train loss: 1.574, Test loss: 1.630, Test accuracy: 83.22
Round  25, Train loss: 1.571, Test loss: 1.631, Test accuracy: 83.18
Round  26, Train loss: 1.579, Test loss: 1.630, Test accuracy: 83.28
Round  27, Train loss: 1.577, Test loss: 1.630, Test accuracy: 83.33
Round  28, Train loss: 1.574, Test loss: 1.629, Test accuracy: 83.38
Round  29, Train loss: 1.570, Test loss: 1.629, Test accuracy: 83.36
Round  30, Train loss: 1.576, Test loss: 1.629, Test accuracy: 83.37
Round  31, Train loss: 1.575, Test loss: 1.629, Test accuracy: 83.39
Round  32, Train loss: 1.575, Test loss: 1.629, Test accuracy: 83.39
Round  33, Train loss: 1.573, Test loss: 1.629, Test accuracy: 83.36
Round  34, Train loss: 1.574, Test loss: 1.629, Test accuracy: 83.38
Round  35, Train loss: 1.576, Test loss: 1.629, Test accuracy: 83.39
Round  36, Train loss: 1.573, Test loss: 1.629, Test accuracy: 83.40
Round  37, Train loss: 1.575, Test loss: 1.629, Test accuracy: 83.38
Round  38, Train loss: 1.568, Test loss: 1.629, Test accuracy: 83.39
Round  39, Train loss: 1.573, Test loss: 1.629, Test accuracy: 83.40
Round  40, Train loss: 1.569, Test loss: 1.628, Test accuracy: 83.41
Round  41, Train loss: 1.573, Test loss: 1.628, Test accuracy: 83.39
Round  42, Train loss: 1.569, Test loss: 1.628, Test accuracy: 83.41
Round  43, Train loss: 1.571, Test loss: 1.628, Test accuracy: 83.42
Round  44, Train loss: 1.566, Test loss: 1.628, Test accuracy: 83.41
Round  45, Train loss: 1.573, Test loss: 1.628, Test accuracy: 83.43
Round  46, Train loss: 1.575, Test loss: 1.628, Test accuracy: 83.42
Round  47, Train loss: 1.572, Test loss: 1.628, Test accuracy: 83.46
Round  48, Train loss: 1.571, Test loss: 1.628, Test accuracy: 83.45
Round  49, Train loss: 1.571, Test loss: 1.628, Test accuracy: 83.42
Round  50, Train loss: 1.574, Test loss: 1.628, Test accuracy: 83.45
Round  51, Train loss: 1.570, Test loss: 1.628, Test accuracy: 83.47
Round  52, Train loss: 1.574, Test loss: 1.628, Test accuracy: 83.43
Round  53, Train loss: 1.572, Test loss: 1.628, Test accuracy: 83.41
Round  54, Train loss: 1.567, Test loss: 1.628, Test accuracy: 83.43
Round  55, Train loss: 1.569, Test loss: 1.628, Test accuracy: 83.43
Round  56, Train loss: 1.568, Test loss: 1.628, Test accuracy: 83.42
Round  57, Train loss: 1.571, Test loss: 1.628, Test accuracy: 83.42
Round  58, Train loss: 1.568, Test loss: 1.628, Test accuracy: 83.42
Round  59, Train loss: 1.567, Test loss: 1.628, Test accuracy: 83.44
Round  60, Train loss: 1.569, Test loss: 1.628, Test accuracy: 83.46
Round  61, Train loss: 1.568, Test loss: 1.628, Test accuracy: 83.45
Round  62, Train loss: 1.571, Test loss: 1.628, Test accuracy: 83.46
Round  63, Train loss: 1.570, Test loss: 1.628, Test accuracy: 83.49
Round  64, Train loss: 1.568, Test loss: 1.628, Test accuracy: 83.47
Round  65, Train loss: 1.568, Test loss: 1.628, Test accuracy: 83.48
Round  66, Train loss: 1.570, Test loss: 1.628, Test accuracy: 83.49
Round  67, Train loss: 1.570, Test loss: 1.627, Test accuracy: 83.48
Round  68, Train loss: 1.570, Test loss: 1.627, Test accuracy: 83.50
Round  69, Train loss: 1.569, Test loss: 1.627, Test accuracy: 83.50
Round  70, Train loss: 1.568, Test loss: 1.627, Test accuracy: 83.50
Round  71, Train loss: 1.573, Test loss: 1.627, Test accuracy: 83.53
Round  72, Train loss: 1.568, Test loss: 1.627, Test accuracy: 83.53
Round  73, Train loss: 1.570, Test loss: 1.627, Test accuracy: 83.52
Round  74, Train loss: 1.567, Test loss: 1.627, Test accuracy: 83.52
Round  75, Train loss: 1.565, Test loss: 1.627, Test accuracy: 83.52
Round  76, Train loss: 1.569, Test loss: 1.627, Test accuracy: 83.52
Round  77, Train loss: 1.570, Test loss: 1.627, Test accuracy: 83.53
Round  78, Train loss: 1.573, Test loss: 1.627, Test accuracy: 83.55
Round  79, Train loss: 1.568, Test loss: 1.627, Test accuracy: 83.53
Round  80, Train loss: 1.563, Test loss: 1.627, Test accuracy: 83.54
Round  81, Train loss: 1.571, Test loss: 1.627, Test accuracy: 83.53
Round  82, Train loss: 1.570, Test loss: 1.627, Test accuracy: 83.54
Round  83, Train loss: 1.569, Test loss: 1.627, Test accuracy: 83.55
Round  84, Train loss: 1.568, Test loss: 1.627, Test accuracy: 83.54
Round  85, Train loss: 1.566, Test loss: 1.627, Test accuracy: 83.57
Round  86, Train loss: 1.572, Test loss: 1.627, Test accuracy: 83.57
Round  87, Train loss: 1.567, Test loss: 1.627, Test accuracy: 83.57
Round  88, Train loss: 1.568, Test loss: 1.627, Test accuracy: 83.58
Round  89, Train loss: 1.570, Test loss: 1.627, Test accuracy: 83.59
Round  90, Train loss: 1.566, Test loss: 1.627, Test accuracy: 83.59
Round  91, Train loss: 1.568, Test loss: 1.627, Test accuracy: 83.59
Round  92, Train loss: 1.566, Test loss: 1.627, Test accuracy: 83.58
Round  93, Train loss: 1.571, Test loss: 1.627, Test accuracy: 83.57
Round  94, Train loss: 1.570, Test loss: 1.627, Test accuracy: 83.56
Round  95, Train loss: 1.569, Test loss: 1.627, Test accuracy: 83.56
Round  96, Train loss: 1.565, Test loss: 1.627, Test accuracy: 83.56
Round  97, Train loss: 1.566, Test loss: 1.627, Test accuracy: 83.55
Round  98, Train loss: 1.572, Test loss: 1.627, Test accuracy: 83.56
Round  99, Train loss: 1.566, Test loss: 1.627, Test accuracy: 83.55/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.568, Test loss: 1.627, Test accuracy: 83.58
Average accuracy final 10 rounds: 83.56775 

4748.433747053146
[3.963975191116333, 7.927950382232666, 11.554420232772827, 15.180890083312988, 18.832578659057617, 22.484267234802246, 26.240899562835693, 29.99753189086914, 33.459625244140625, 36.92171859741211, 40.562440633773804, 44.2031626701355, 48.03207516670227, 51.86098766326904, 55.50684976577759, 59.15271186828613, 62.760509967803955, 66.36830806732178, 70.00986838340759, 73.65142869949341, 77.16449308395386, 80.6775574684143, 84.2035710811615, 87.72958469390869, 91.43926930427551, 95.14895391464233, 98.62867164611816, 102.108389377594, 105.65315866470337, 109.19792795181274, 112.90572786331177, 116.61352777481079, 120.10045337677002, 123.58737897872925, 127.15072870254517, 130.71407842636108, 134.3624505996704, 138.01082277297974, 141.55383157730103, 145.09684038162231, 148.55700707435608, 152.01717376708984, 155.68433213233948, 159.3514904975891, 162.85394024848938, 166.35638999938965, 169.9313304424286, 173.50627088546753, 177.08831810951233, 180.67036533355713, 184.22948694229126, 187.7886085510254, 191.26261711120605, 194.73662567138672, 198.3796889781952, 202.02275228500366, 205.50935816764832, 208.99596405029297, 212.53523635864258, 216.0745086669922, 219.66171860694885, 223.24892854690552, 226.8119602203369, 230.3749918937683, 233.90721940994263, 237.43944692611694, 240.95555019378662, 244.4716534614563, 248.026123046875, 251.5805926322937, 255.00710248947144, 258.43361234664917, 261.8883967399597, 265.34318113327026, 269.30040431022644, 273.2576274871826, 277.20408296585083, 281.15053844451904, 285.09300684928894, 289.03547525405884, 292.9983694553375, 296.9612636566162, 300.4797954559326, 303.998327255249, 307.47800636291504, 310.95768547058105, 314.52156686782837, 318.0854482650757, 321.60721158981323, 325.1289749145508, 328.5874197483063, 332.04586458206177, 335.4838936328888, 338.9219226837158, 342.4809913635254, 346.04006004333496, 349.5473816394806, 353.0547032356262, 356.4816324710846, 359.90856170654297, 363.3809640407562, 366.8533663749695, 370.31432032585144, 373.7752742767334, 377.2649235725403, 380.75457286834717, 384.29366874694824, 387.8327646255493, 391.4058725833893, 394.97898054122925, 398.5890483856201, 402.199116230011, 405.6478328704834, 409.0965495109558, 412.64137268066406, 416.1861958503723, 419.72660183906555, 423.2670078277588, 426.67498683929443, 430.0829658508301, 433.66417598724365, 437.2453861236572, 440.8652980327606, 444.485209941864, 448.04119753837585, 451.5971851348877, 455.0981938838959, 458.59920263290405, 462.19367718696594, 465.78815174102783, 469.3831305503845, 472.9781093597412, 476.47301745414734, 479.96792554855347, 483.4782133102417, 486.98850107192993, 490.8105812072754, 494.63266134262085, 498.2514705657959, 501.87027978897095, 505.40394711494446, 508.93761444091797, 512.6725363731384, 516.4074583053589, 520.018607378006, 523.6297564506531, 527.2276210784912, 530.8254857063293, 534.3164246082306, 537.8073635101318, 541.4313008785248, 545.0552382469177, 548.6074223518372, 552.1596064567566, 555.6966359615326, 559.2336654663086, 562.8017685413361, 566.3698716163635, 569.9756095409393, 573.5813474655151, 577.0047032833099, 580.4280591011047, 583.9424006938934, 587.4567422866821, 591.1167786121368, 594.7768149375916, 598.3274533748627, 601.8780918121338, 605.3547654151917, 608.8314390182495, 612.3440685272217, 615.8566980361938, 619.4559659957886, 623.0552339553833, 626.6659207344055, 630.2766075134277, 633.7370135784149, 637.1974196434021, 640.980991601944, 644.7645635604858, 648.3807172775269, 651.9968709945679, 655.4968917369843, 658.9969124794006, 662.528933763504, 666.0609550476074, 669.6399192810059, 673.2188835144043, 676.7151012420654, 680.2113189697266, 683.7295444011688, 687.2477698326111, 690.8539371490479, 694.4601044654846, 698.0563519001007, 701.6525993347168, 705.2872750759125, 708.9219508171082, 712.3934586048126, 715.8649663925171, 717.6474597454071, 719.4299530982971]
[38.3425, 38.3425, 71.65, 71.65, 73.4175, 73.4175, 74.76, 74.76, 75.435, 75.435, 76.31, 76.31, 81.085, 81.085, 81.4925, 81.4925, 81.805, 81.805, 82.0225, 82.0225, 82.3575, 82.3575, 82.51, 82.51, 82.685, 82.685, 82.7925, 82.7925, 82.8425, 82.8425, 82.9025, 82.9025, 82.9025, 82.9025, 82.985, 82.985, 82.9975, 82.9975, 82.9675, 82.9675, 83.015, 83.015, 83.03, 83.03, 83.1875, 83.1875, 83.24, 83.24, 83.2225, 83.2225, 83.1775, 83.1775, 83.28, 83.28, 83.3325, 83.3325, 83.38, 83.38, 83.355, 83.355, 83.3725, 83.3725, 83.39, 83.39, 83.395, 83.395, 83.3575, 83.3575, 83.38, 83.38, 83.39, 83.39, 83.4, 83.4, 83.38, 83.38, 83.395, 83.395, 83.4, 83.4, 83.4075, 83.4075, 83.3925, 83.3925, 83.4125, 83.4125, 83.415, 83.415, 83.4125, 83.4125, 83.4325, 83.4325, 83.425, 83.425, 83.46, 83.46, 83.455, 83.455, 83.425, 83.425, 83.4475, 83.4475, 83.475, 83.475, 83.43, 83.43, 83.4075, 83.4075, 83.4325, 83.4325, 83.43, 83.43, 83.425, 83.425, 83.4225, 83.4225, 83.425, 83.425, 83.4375, 83.4375, 83.46, 83.46, 83.455, 83.455, 83.46, 83.46, 83.4875, 83.4875, 83.465, 83.465, 83.4825, 83.4825, 83.4925, 83.4925, 83.4775, 83.4775, 83.5025, 83.5025, 83.495, 83.495, 83.4975, 83.4975, 83.525, 83.525, 83.525, 83.525, 83.52, 83.52, 83.5225, 83.5225, 83.52, 83.52, 83.5175, 83.5175, 83.5325, 83.5325, 83.5525, 83.5525, 83.5325, 83.5325, 83.54, 83.54, 83.5325, 83.5325, 83.5375, 83.5375, 83.55, 83.55, 83.5375, 83.5375, 83.5725, 83.5725, 83.57, 83.57, 83.57, 83.57, 83.58, 83.58, 83.5925, 83.5925, 83.59, 83.59, 83.59, 83.59, 83.5825, 83.5825, 83.5675, 83.5675, 83.565, 83.565, 83.565, 83.565, 83.56, 83.56, 83.5525, 83.5525, 83.56, 83.56, 83.545, 83.545, 83.58, 83.58]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.320, Test loss: 2.299, Test accuracy: 22.37
Round   1, Train loss: 2.298, Test loss: 2.294, Test accuracy: 25.82
Round   2, Train loss: 2.292, Test loss: 2.286, Test accuracy: 29.77
Round   3, Train loss: 2.282, Test loss: 2.271, Test accuracy: 30.71
Round   4, Train loss: 2.256, Test loss: 2.223, Test accuracy: 31.06
Round   5, Train loss: 2.191, Test loss: 2.148, Test accuracy: 36.64
Round   6, Train loss: 2.120, Test loss: 2.077, Test accuracy: 47.73
Round   7, Train loss: 2.036, Test loss: 2.004, Test accuracy: 51.58
Round   8, Train loss: 1.972, Test loss: 1.951, Test accuracy: 54.81
Round   9, Train loss: 1.914, Test loss: 1.891, Test accuracy: 67.17
Round  10, Train loss: 1.850, Test loss: 1.815, Test accuracy: 75.15
Round  11, Train loss: 1.770, Test loss: 1.767, Test accuracy: 78.45
Round  12, Train loss: 1.745, Test loss: 1.735, Test accuracy: 80.25
Round  13, Train loss: 1.722, Test loss: 1.714, Test accuracy: 81.04
Round  14, Train loss: 1.714, Test loss: 1.694, Test accuracy: 82.25
Round  15, Train loss: 1.694, Test loss: 1.682, Test accuracy: 83.13
Round  16, Train loss: 1.676, Test loss: 1.677, Test accuracy: 83.42
Round  17, Train loss: 1.681, Test loss: 1.668, Test accuracy: 83.73
Round  18, Train loss: 1.674, Test loss: 1.662, Test accuracy: 83.96
Round  19, Train loss: 1.665, Test loss: 1.653, Test accuracy: 84.33
Round  20, Train loss: 1.664, Test loss: 1.650, Test accuracy: 84.52
Round  21, Train loss: 1.655, Test loss: 1.647, Test accuracy: 84.54
Round  22, Train loss: 1.643, Test loss: 1.645, Test accuracy: 84.72
Round  23, Train loss: 1.652, Test loss: 1.642, Test accuracy: 84.81
Round  24, Train loss: 1.643, Test loss: 1.639, Test accuracy: 84.97
Round  25, Train loss: 1.631, Test loss: 1.636, Test accuracy: 85.21
Round  26, Train loss: 1.628, Test loss: 1.632, Test accuracy: 85.62
Round  27, Train loss: 1.618, Test loss: 1.619, Test accuracy: 86.61
Round  28, Train loss: 1.607, Test loss: 1.609, Test accuracy: 87.72
Round  29, Train loss: 1.589, Test loss: 1.599, Test accuracy: 89.31
Round  30, Train loss: 1.579, Test loss: 1.590, Test accuracy: 90.26
Round  31, Train loss: 1.572, Test loss: 1.584, Test accuracy: 90.68
Round  32, Train loss: 1.582, Test loss: 1.572, Test accuracy: 92.20
Round  33, Train loss: 1.559, Test loss: 1.569, Test accuracy: 92.50
Round  34, Train loss: 1.557, Test loss: 1.565, Test accuracy: 92.69
Round  35, Train loss: 1.561, Test loss: 1.560, Test accuracy: 92.98
Round  36, Train loss: 1.545, Test loss: 1.560, Test accuracy: 93.08
Round  37, Train loss: 1.547, Test loss: 1.557, Test accuracy: 93.38
Round  38, Train loss: 1.548, Test loss: 1.555, Test accuracy: 93.48
Round  39, Train loss: 1.543, Test loss: 1.551, Test accuracy: 93.87
Round  40, Train loss: 1.545, Test loss: 1.549, Test accuracy: 94.06
Round  41, Train loss: 1.541, Test loss: 1.547, Test accuracy: 94.10
Round  42, Train loss: 1.533, Test loss: 1.546, Test accuracy: 94.25
Round  43, Train loss: 1.531, Test loss: 1.544, Test accuracy: 94.26
Round  44, Train loss: 1.528, Test loss: 1.543, Test accuracy: 94.28
Round  45, Train loss: 1.535, Test loss: 1.541, Test accuracy: 94.25
Round  46, Train loss: 1.532, Test loss: 1.540, Test accuracy: 94.38
Round  47, Train loss: 1.526, Test loss: 1.539, Test accuracy: 94.53
Round  48, Train loss: 1.523, Test loss: 1.538, Test accuracy: 94.65
Round  49, Train loss: 1.518, Test loss: 1.538, Test accuracy: 94.65
Round  50, Train loss: 1.523, Test loss: 1.536, Test accuracy: 94.77
Round  51, Train loss: 1.513, Test loss: 1.535, Test accuracy: 94.82
Round  52, Train loss: 1.520, Test loss: 1.534, Test accuracy: 94.85
Round  53, Train loss: 1.523, Test loss: 1.532, Test accuracy: 94.94
Round  54, Train loss: 1.520, Test loss: 1.531, Test accuracy: 95.05
Round  55, Train loss: 1.515, Test loss: 1.530, Test accuracy: 95.04
Round  56, Train loss: 1.513, Test loss: 1.529, Test accuracy: 95.01
Round  57, Train loss: 1.509, Test loss: 1.529, Test accuracy: 95.00
Round  58, Train loss: 1.504, Test loss: 1.529, Test accuracy: 95.11
Round  59, Train loss: 1.505, Test loss: 1.529, Test accuracy: 95.06
Round  60, Train loss: 1.506, Test loss: 1.528, Test accuracy: 95.14
Round  61, Train loss: 1.501, Test loss: 1.527, Test accuracy: 95.15
Round  62, Train loss: 1.507, Test loss: 1.526, Test accuracy: 95.24
Round  63, Train loss: 1.503, Test loss: 1.526, Test accuracy: 95.27
Round  64, Train loss: 1.512, Test loss: 1.524, Test accuracy: 95.42
Round  65, Train loss: 1.497, Test loss: 1.525, Test accuracy: 95.34
Round  66, Train loss: 1.503, Test loss: 1.524, Test accuracy: 95.41
Round  67, Train loss: 1.496, Test loss: 1.523, Test accuracy: 95.41
Round  68, Train loss: 1.504, Test loss: 1.523, Test accuracy: 95.40
Round  69, Train loss: 1.494, Test loss: 1.523, Test accuracy: 95.48
Round  70, Train loss: 1.504, Test loss: 1.522, Test accuracy: 95.58
Round  71, Train loss: 1.496, Test loss: 1.522, Test accuracy: 95.60
Round  72, Train loss: 1.496, Test loss: 1.521, Test accuracy: 95.63
Round  73, Train loss: 1.495, Test loss: 1.521, Test accuracy: 95.63
Round  74, Train loss: 1.497, Test loss: 1.520, Test accuracy: 95.57
Round  75, Train loss: 1.498, Test loss: 1.520, Test accuracy: 95.58
Round  76, Train loss: 1.493, Test loss: 1.520, Test accuracy: 95.69
Round  77, Train loss: 1.490, Test loss: 1.519, Test accuracy: 95.62
Round  78, Train loss: 1.493, Test loss: 1.519, Test accuracy: 95.72
Round  79, Train loss: 1.494, Test loss: 1.518, Test accuracy: 95.76
Round  80, Train loss: 1.489, Test loss: 1.518, Test accuracy: 95.69
Round  81, Train loss: 1.486, Test loss: 1.518, Test accuracy: 95.77
Round  82, Train loss: 1.492, Test loss: 1.518, Test accuracy: 95.74
Round  83, Train loss: 1.489, Test loss: 1.517, Test accuracy: 95.80
Round  84, Train loss: 1.489, Test loss: 1.517, Test accuracy: 95.82
Round  85, Train loss: 1.492, Test loss: 1.516, Test accuracy: 95.85
Round  86, Train loss: 1.485, Test loss: 1.517, Test accuracy: 95.89
Round  87, Train loss: 1.489, Test loss: 1.516, Test accuracy: 95.91
Round  88, Train loss: 1.488, Test loss: 1.516, Test accuracy: 95.90
Round  89, Train loss: 1.490, Test loss: 1.516, Test accuracy: 95.88
Round  90, Train loss: 1.488, Test loss: 1.515, Test accuracy: 95.97
Round  91, Train loss: 1.488, Test loss: 1.515, Test accuracy: 95.98
Round  92, Train loss: 1.487, Test loss: 1.515, Test accuracy: 95.95
Round  93, Train loss: 1.483, Test loss: 1.515, Test accuracy: 95.92
Round  94, Train loss: 1.487, Test loss: 1.514, Test accuracy: 95.96/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.483, Test loss: 1.514, Test accuracy: 95.96
Round  96, Train loss: 1.485, Test loss: 1.514, Test accuracy: 96.04
Round  97, Train loss: 1.483, Test loss: 1.514, Test accuracy: 96.05
Round  98, Train loss: 1.484, Test loss: 1.513, Test accuracy: 96.07
Round  99, Train loss: 1.485, Test loss: 1.513, Test accuracy: 95.99
Final Round, Train loss: 1.478, Test loss: 1.512, Test accuracy: 96.06
Average accuracy final 10 rounds: 95.98916666666666
1268.489092350006
[1.6423332691192627, 3.1503186225891113, 4.621876239776611, 6.08669114112854, 7.566346883773804, 9.08793830871582, 10.5437912940979, 11.976576805114746, 13.325043678283691, 14.733977317810059, 16.18457579612732, 17.605994701385498, 19.032476902008057, 20.4564528465271, 21.84421968460083, 23.201011657714844, 24.679445505142212, 26.154675722122192, 27.584554195404053, 28.964489221572876, 30.38843560218811, 31.83702850341797, 33.242226123809814, 34.62731719017029, 36.048320055007935, 37.51106810569763, 38.878363609313965, 40.39782190322876, 41.86015319824219, 43.30222487449646, 44.699219942092896, 46.09421443939209, 47.53801918029785, 48.95268750190735, 50.30122089385986, 51.72967457771301, 53.11906385421753, 54.51390480995178, 56.044487714767456, 57.48437547683716, 58.93651747703552, 60.3351616859436, 61.8123300075531, 63.33074736595154, 64.76256895065308, 66.22562408447266, 67.63017964363098, 69.07291197776794, 70.59041833877563, 72.03682327270508, 73.37375855445862, 74.7463846206665, 76.15186929702759, 77.54004764556885, 78.95645380020142, 80.41627240180969, 81.83119058609009, 83.16968989372253, 84.55691170692444, 86.00814962387085, 87.44781827926636, 88.79505252838135, 90.2660608291626, 91.7232575416565, 93.22939229011536, 94.67311787605286, 96.13525152206421, 97.57393097877502, 98.93048405647278, 100.32063007354736, 101.84250545501709, 103.33942079544067, 104.73862600326538, 106.17767882347107, 107.6786139011383, 109.15867280960083, 110.61107015609741, 112.09623622894287, 113.54585456848145, 115.07199096679688, 116.50779294967651, 117.97087717056274, 119.42318105697632, 120.89008903503418, 122.31417202949524, 123.71732330322266, 125.13632893562317, 126.61538219451904, 128.1074733734131, 129.4922924041748, 130.92863368988037, 132.3911907672882, 133.84655666351318, 135.28922033309937, 136.76554441452026, 138.23474144935608, 139.68267154693604, 141.0477650165558, 142.5618176460266, 144.0885362625122, 145.91194438934326]
[22.366666666666667, 25.816666666666666, 29.775, 30.708333333333332, 31.058333333333334, 36.641666666666666, 47.733333333333334, 51.583333333333336, 54.80833333333333, 67.16666666666667, 75.15, 78.45, 80.25, 81.04166666666667, 82.25, 83.13333333333334, 83.41666666666667, 83.73333333333333, 83.95833333333333, 84.325, 84.51666666666667, 84.54166666666667, 84.725, 84.80833333333334, 84.975, 85.20833333333333, 85.625, 86.60833333333333, 87.71666666666667, 89.30833333333334, 90.25833333333334, 90.68333333333334, 92.2, 92.5, 92.69166666666666, 92.98333333333333, 93.08333333333333, 93.38333333333334, 93.48333333333333, 93.86666666666666, 94.05833333333334, 94.1, 94.25, 94.25833333333334, 94.28333333333333, 94.25, 94.375, 94.53333333333333, 94.65, 94.65, 94.76666666666667, 94.81666666666666, 94.85, 94.94166666666666, 95.05, 95.04166666666667, 95.00833333333334, 95.0, 95.10833333333333, 95.05833333333334, 95.14166666666667, 95.15, 95.24166666666666, 95.26666666666667, 95.41666666666667, 95.34166666666667, 95.40833333333333, 95.40833333333333, 95.4, 95.48333333333333, 95.58333333333333, 95.6, 95.63333333333334, 95.63333333333334, 95.56666666666666, 95.58333333333333, 95.69166666666666, 95.625, 95.725, 95.75833333333334, 95.69166666666666, 95.76666666666667, 95.74166666666666, 95.8, 95.81666666666666, 95.85, 95.89166666666667, 95.90833333333333, 95.9, 95.88333333333334, 95.975, 95.98333333333333, 95.95, 95.91666666666667, 95.95833333333333, 95.95833333333333, 96.04166666666667, 96.05, 96.06666666666666, 95.99166666666666, 96.05833333333334]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_ditto%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
Round   0, Train loss: 2.301, Test loss: 2.127, Test accuracy: 30.11
Round   1, Train loss: 2.095, Test loss: 1.697, Test accuracy: 80.25
Round   2, Train loss: 1.702, Test loss: 1.637, Test accuracy: 83.40
Round   3, Train loss: 1.660, Test loss: 1.623, Test accuracy: 84.20
Round   4, Train loss: 1.634, Test loss: 1.568, Test accuracy: 90.31
Round   5, Train loss: 1.581, Test loss: 1.553, Test accuracy: 91.45
Round   6, Train loss: 1.564, Test loss: 1.544, Test accuracy: 92.20
Round   7, Train loss: 1.546, Test loss: 1.538, Test accuracy: 92.78
Round   8, Train loss: 1.535, Test loss: 1.534, Test accuracy: 93.16
Round   9, Train loss: 1.532, Test loss: 1.528, Test accuracy: 93.58
Round  10, Train loss: 1.527, Test loss: 1.526, Test accuracy: 93.78
Round  11, Train loss: 1.523, Test loss: 1.523, Test accuracy: 94.20
Round  12, Train loss: 1.516, Test loss: 1.520, Test accuracy: 94.35
Round  13, Train loss: 1.510, Test loss: 1.518, Test accuracy: 94.50
Round  14, Train loss: 1.506, Test loss: 1.517, Test accuracy: 94.68
Round  15, Train loss: 1.508, Test loss: 1.516, Test accuracy: 94.66
Round  16, Train loss: 1.504, Test loss: 1.513, Test accuracy: 95.15
Round  17, Train loss: 1.503, Test loss: 1.511, Test accuracy: 95.21
Round  18, Train loss: 1.504, Test loss: 1.511, Test accuracy: 95.17
Round  19, Train loss: 1.498, Test loss: 1.509, Test accuracy: 95.43
Round  20, Train loss: 1.495, Test loss: 1.508, Test accuracy: 95.53
Round  21, Train loss: 1.493, Test loss: 1.507, Test accuracy: 95.64
Round  22, Train loss: 1.492, Test loss: 1.507, Test accuracy: 95.64
Round  23, Train loss: 1.492, Test loss: 1.506, Test accuracy: 95.72
Round  24, Train loss: 1.488, Test loss: 1.506, Test accuracy: 95.78
Round  25, Train loss: 1.488, Test loss: 1.505, Test accuracy: 95.87
Round  26, Train loss: 1.485, Test loss: 1.504, Test accuracy: 95.86
Round  27, Train loss: 1.486, Test loss: 1.504, Test accuracy: 95.98
Round  28, Train loss: 1.488, Test loss: 1.503, Test accuracy: 96.00
Round  29, Train loss: 1.486, Test loss: 1.502, Test accuracy: 96.03
Round  30, Train loss: 1.485, Test loss: 1.502, Test accuracy: 96.12
Round  31, Train loss: 1.480, Test loss: 1.502, Test accuracy: 96.06
Round  32, Train loss: 1.485, Test loss: 1.501, Test accuracy: 96.07
Round  33, Train loss: 1.478, Test loss: 1.501, Test accuracy: 96.17
Round  34, Train loss: 1.482, Test loss: 1.501, Test accuracy: 96.03
Round  35, Train loss: 1.483, Test loss: 1.501, Test accuracy: 96.25
Round  36, Train loss: 1.481, Test loss: 1.500, Test accuracy: 96.27
Round  37, Train loss: 1.483, Test loss: 1.500, Test accuracy: 96.36
Round  38, Train loss: 1.481, Test loss: 1.499, Test accuracy: 96.36
Round  39, Train loss: 1.480, Test loss: 1.499, Test accuracy: 96.37
Round  40, Train loss: 1.477, Test loss: 1.499, Test accuracy: 96.38
Round  41, Train loss: 1.477, Test loss: 1.499, Test accuracy: 96.45
Round  42, Train loss: 1.480, Test loss: 1.498, Test accuracy: 96.50
Round  43, Train loss: 1.476, Test loss: 1.498, Test accuracy: 96.48
Round  44, Train loss: 1.478, Test loss: 1.498, Test accuracy: 96.41
Round  45, Train loss: 1.477, Test loss: 1.498, Test accuracy: 96.42
Round  46, Train loss: 1.478, Test loss: 1.497, Test accuracy: 96.60
Round  47, Train loss: 1.477, Test loss: 1.497, Test accuracy: 96.62
Round  48, Train loss: 1.474, Test loss: 1.497, Test accuracy: 96.61
Round  49, Train loss: 1.475, Test loss: 1.497, Test accuracy: 96.61
Round  50, Train loss: 1.477, Test loss: 1.496, Test accuracy: 96.67
Round  51, Train loss: 1.473, Test loss: 1.496, Test accuracy: 96.65
Round  52, Train loss: 1.475, Test loss: 1.496, Test accuracy: 96.70
Round  53, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.72
Round  54, Train loss: 1.474, Test loss: 1.496, Test accuracy: 96.72
Round  55, Train loss: 1.472, Test loss: 1.496, Test accuracy: 96.71
Round  56, Train loss: 1.474, Test loss: 1.495, Test accuracy: 96.78
Round  57, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.71
Round  58, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.81
Round  59, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.66
Round  60, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.69
Round  61, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.69
Round  62, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.74
Round  63, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.69
Round  64, Train loss: 1.474, Test loss: 1.495, Test accuracy: 96.69
Round  65, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.66
Round  66, Train loss: 1.471, Test loss: 1.495, Test accuracy: 96.72
Round  67, Train loss: 1.472, Test loss: 1.495, Test accuracy: 96.81
Round  68, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.76
Round  69, Train loss: 1.473, Test loss: 1.495, Test accuracy: 96.76
Round  70, Train loss: 1.472, Test loss: 1.494, Test accuracy: 96.76
Round  71, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.71
Round  72, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.80
Round  73, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.91
Round  74, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.86
Round  75, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.92
Round  76, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.87
Round  77, Train loss: 1.470, Test loss: 1.493, Test accuracy: 96.97
Round  78, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.84
Round  79, Train loss: 1.470, Test loss: 1.494, Test accuracy: 96.85
Round  80, Train loss: 1.471, Test loss: 1.493, Test accuracy: 96.88
Round  81, Train loss: 1.471, Test loss: 1.493, Test accuracy: 96.80
Round  82, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.86
Round  83, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.86
Round  84, Train loss: 1.470, Test loss: 1.493, Test accuracy: 96.92
Round  85, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.97
Round  86, Train loss: 1.471, Test loss: 1.494, Test accuracy: 96.87
Round  87, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.78
Round  88, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.90
Round  89, Train loss: 1.471, Test loss: 1.493, Test accuracy: 96.89
Round  90, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.92
Round  91, Train loss: 1.471, Test loss: 1.493, Test accuracy: 96.96
Round  92, Train loss: 1.471, Test loss: 1.493, Test accuracy: 96.94
Round  93, Train loss: 1.470, Test loss: 1.493, Test accuracy: 96.97
Round  94, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.95
Round  95, Train loss: 1.468, Test loss: 1.493, Test accuracy: 96.92
Round  96, Train loss: 1.467, Test loss: 1.493, Test accuracy: 96.86
Round  97, Train loss: 1.469, Test loss: 1.493, Test accuracy: 97.04
Round  98, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.95
Round  99, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.88
Final Round, Train loss: 1.469, Test loss: 1.493, Test accuracy: 96.88
Average accuracy final 10 rounds: 96.9395
6612.287144422531
[10.056942462921143, 20.126231908798218, 30.159440994262695, 40.155924797058105, 50.112664222717285, 60.170122146606445, 70.42534828186035, 80.60197973251343, 90.59100556373596, 100.58354568481445, 110.6839246749878, 120.33955574035645, 129.8937473297119, 139.50139498710632, 148.93419075012207, 158.5121386051178, 168.10151267051697, 177.68484210968018, 187.12433910369873, 196.02692890167236, 204.88780617713928, 213.96483945846558, 222.8874032497406, 231.7758436203003, 240.67099857330322, 249.70598077774048, 258.6190221309662, 267.6585555076599, 276.63727593421936, 285.57736015319824, 294.6049530506134, 303.6949505805969, 312.6426029205322, 321.5471053123474, 330.61568236351013, 340.4148316383362, 349.36037826538086, 358.1938359737396, 367.1637604236603, 376.87927770614624, 386.10479378700256, 394.8059778213501, 403.73551630973816, 412.6084849834442, 421.4256410598755, 430.3562023639679, 439.16632318496704, 447.9930725097656, 456.7322726249695, 465.7334246635437, 474.52451372146606, 483.5726532936096, 492.3544034957886, 501.1297652721405, 510.0069737434387, 518.8231914043427, 527.7089567184448, 536.7504782676697, 545.6748290061951, 554.6368520259857, 563.4979491233826, 572.3517079353333, 581.4962952136993, 590.2420539855957, 599.1824553012848, 608.2449901103973, 617.0294766426086, 625.867288351059, 634.750869512558, 643.5657334327698, 652.4380834102631, 661.3587398529053, 670.2806522846222, 679.0635826587677, 687.8514242172241, 696.7869806289673, 705.6739461421967, 714.4532639980316, 723.3944735527039, 732.2727184295654, 741.1467974185944, 750.1552391052246, 759.7068109512329, 769.2580971717834, 778.0985863208771, 786.9873504638672, 795.8655803203583, 804.7434587478638, 813.4888124465942, 822.3987305164337, 831.2990920543671, 840.2006883621216, 849.1842079162598, 858.0271770954132, 866.9675583839417, 875.8287720680237, 884.7489449977875, 893.6088123321533, 902.6104602813721, 911.6051194667816, 913.8789911270142]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

[30.105, 80.245, 83.4, 84.205, 90.3075, 91.4475, 92.1975, 92.775, 93.1575, 93.58, 93.775, 94.2025, 94.35, 94.495, 94.6825, 94.6625, 95.1475, 95.21, 95.1675, 95.4275, 95.525, 95.64, 95.645, 95.7175, 95.7775, 95.8725, 95.8625, 95.9775, 95.995, 96.0325, 96.125, 96.055, 96.0675, 96.1675, 96.035, 96.25, 96.27, 96.365, 96.365, 96.37, 96.3825, 96.4525, 96.495, 96.48, 96.4075, 96.42, 96.5975, 96.6225, 96.61, 96.6075, 96.67, 96.6475, 96.7025, 96.7225, 96.72, 96.71, 96.78, 96.7075, 96.81, 96.6575, 96.69, 96.685, 96.7375, 96.6925, 96.6875, 96.6625, 96.7225, 96.81, 96.7575, 96.7575, 96.7625, 96.7075, 96.795, 96.91, 96.8575, 96.9225, 96.8725, 96.965, 96.8375, 96.8525, 96.88, 96.7975, 96.8625, 96.865, 96.915, 96.9675, 96.8675, 96.785, 96.9025, 96.885, 96.915, 96.9625, 96.9375, 96.9725, 96.95, 96.925, 96.8575, 97.0375, 96.955, 96.8825, 96.875]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Fed_apfl%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
Round   0, Train loss: 1.696, Test loss: 2.287, Test accuracy: 32.67
Round   1, Train loss: 1.536, Test loss: 2.208, Test accuracy: 51.08
Round   2, Train loss: 1.394, Test loss: 2.079, Test accuracy: 60.63
Round   3, Train loss: 1.297, Test loss: 1.991, Test accuracy: 67.51
Round   4, Train loss: 1.289, Test loss: 1.954, Test accuracy: 71.42
Round   5, Train loss: 1.275, Test loss: 1.940, Test accuracy: 71.79
Round   6, Train loss: 1.281, Test loss: 1.929, Test accuracy: 72.73
Round   7, Train loss: 1.264, Test loss: 1.921, Test accuracy: 72.98
Round   8, Train loss: 1.264, Test loss: 1.915, Test accuracy: 73.46
Round   9, Train loss: 1.276, Test loss: 1.910, Test accuracy: 74.01
Round  10, Train loss: 1.260, Test loss: 1.907, Test accuracy: 73.99
Round  11, Train loss: 1.259, Test loss: 1.904, Test accuracy: 73.93
Round  12, Train loss: 1.255, Test loss: 1.902, Test accuracy: 73.97
Round  13, Train loss: 1.258, Test loss: 1.901, Test accuracy: 73.81
Round  14, Train loss: 1.260, Test loss: 1.902, Test accuracy: 73.63
Round  15, Train loss: 1.249, Test loss: 1.900, Test accuracy: 73.53
Round  16, Train loss: 1.256, Test loss: 1.899, Test accuracy: 73.53
Round  17, Train loss: 1.257, Test loss: 1.898, Test accuracy: 73.47
Round  18, Train loss: 1.251, Test loss: 1.898, Test accuracy: 73.42
Round  19, Train loss: 1.253, Test loss: 1.902, Test accuracy: 73.71
Round  20, Train loss: 1.214, Test loss: 1.899, Test accuracy: 74.92
Round  21, Train loss: 1.214, Test loss: 1.894, Test accuracy: 75.97
Round  22, Train loss: 1.200, Test loss: 1.887, Test accuracy: 76.88
Round  23, Train loss: 1.197, Test loss: 1.886, Test accuracy: 77.33
Round  24, Train loss: 1.192, Test loss: 1.882, Test accuracy: 77.42
Round  25, Train loss: 1.188, Test loss: 1.879, Test accuracy: 77.33
Round  26, Train loss: 1.190, Test loss: 1.878, Test accuracy: 77.16
Round  27, Train loss: 1.186, Test loss: 1.877, Test accuracy: 77.12
Round  28, Train loss: 1.188, Test loss: 1.875, Test accuracy: 77.17
Round  29, Train loss: 1.182, Test loss: 1.875, Test accuracy: 77.00
Round  30, Train loss: 1.184, Test loss: 1.874, Test accuracy: 77.01
Round  31, Train loss: 1.182, Test loss: 1.874, Test accuracy: 76.53
Round  32, Train loss: 1.183, Test loss: 1.875, Test accuracy: 76.30
Round  33, Train loss: 1.179, Test loss: 1.874, Test accuracy: 76.25
Round  34, Train loss: 1.193, Test loss: 1.874, Test accuracy: 76.16
Round  35, Train loss: 1.177, Test loss: 1.875, Test accuracy: 76.02
Round  36, Train loss: 1.180, Test loss: 1.873, Test accuracy: 75.87
Round  37, Train loss: 1.178, Test loss: 1.875, Test accuracy: 75.58
Round  38, Train loss: 1.179, Test loss: 1.874, Test accuracy: 75.53
Round  39, Train loss: 1.181, Test loss: 1.876, Test accuracy: 75.32
Round  40, Train loss: 1.181, Test loss: 1.877, Test accuracy: 75.11
Round  41, Train loss: 1.180, Test loss: 1.876, Test accuracy: 74.83
Round  42, Train loss: 1.178, Test loss: 1.876, Test accuracy: 74.72
Round  43, Train loss: 1.186, Test loss: 1.876, Test accuracy: 74.65
Round  44, Train loss: 1.181, Test loss: 1.877, Test accuracy: 74.39
Round  45, Train loss: 1.181, Test loss: 1.877, Test accuracy: 74.17
Round  46, Train loss: 1.179, Test loss: 1.877, Test accuracy: 74.03
Round  47, Train loss: 1.179, Test loss: 1.878, Test accuracy: 73.89
Round  48, Train loss: 1.182, Test loss: 1.878, Test accuracy: 73.65
Round  49, Train loss: 1.176, Test loss: 1.878, Test accuracy: 73.52
Round  50, Train loss: 1.178, Test loss: 1.877, Test accuracy: 73.38
Round  51, Train loss: 1.182, Test loss: 1.878, Test accuracy: 73.12
Round  52, Train loss: 1.175, Test loss: 1.878, Test accuracy: 73.18
Round  53, Train loss: 1.184, Test loss: 1.880, Test accuracy: 72.95
Round  54, Train loss: 1.178, Test loss: 1.880, Test accuracy: 72.70
Round  55, Train loss: 1.175, Test loss: 1.880, Test accuracy: 72.58
Round  56, Train loss: 1.175, Test loss: 1.880, Test accuracy: 72.38
Round  57, Train loss: 1.180, Test loss: 1.881, Test accuracy: 72.20
Round  58, Train loss: 1.184, Test loss: 1.881, Test accuracy: 71.97
Round  59, Train loss: 1.180, Test loss: 1.882, Test accuracy: 71.91
Round  60, Train loss: 1.183, Test loss: 1.882, Test accuracy: 71.78
Round  61, Train loss: 1.181, Test loss: 1.881, Test accuracy: 71.67
Round  62, Train loss: 1.177, Test loss: 1.882, Test accuracy: 71.56
Round  63, Train loss: 1.175, Test loss: 1.882, Test accuracy: 71.42
Round  64, Train loss: 1.178, Test loss: 1.883, Test accuracy: 71.23
Round  65, Train loss: 1.178, Test loss: 1.884, Test accuracy: 71.03
Round  66, Train loss: 1.179, Test loss: 1.884, Test accuracy: 70.88
Round  67, Train loss: 1.185, Test loss: 1.885, Test accuracy: 70.86
Round  68, Train loss: 1.178, Test loss: 1.885, Test accuracy: 70.74
Round  69, Train loss: 1.176, Test loss: 1.886, Test accuracy: 70.76
Round  70, Train loss: 1.175, Test loss: 1.886, Test accuracy: 70.70
Round  71, Train loss: 1.174, Test loss: 1.886, Test accuracy: 70.45
Round  72, Train loss: 1.175, Test loss: 1.885, Test accuracy: 70.37
Round  73, Train loss: 1.175, Test loss: 1.887, Test accuracy: 70.11
Round  74, Train loss: 1.178, Test loss: 1.888, Test accuracy: 70.00
Round  75, Train loss: 1.175, Test loss: 1.888, Test accuracy: 69.88
Round  76, Train loss: 1.173, Test loss: 1.889, Test accuracy: 69.72
Round  77, Train loss: 1.179, Test loss: 1.890, Test accuracy: 69.62
Round  78, Train loss: 1.177, Test loss: 1.891, Test accuracy: 69.49
Round  79, Train loss: 1.177, Test loss: 1.890, Test accuracy: 69.53
Round  80, Train loss: 1.173, Test loss: 1.889, Test accuracy: 69.42
Round  81, Train loss: 1.178, Test loss: 1.890, Test accuracy: 69.37
Round  82, Train loss: 1.181, Test loss: 1.891, Test accuracy: 69.17
Round  83, Train loss: 1.174, Test loss: 1.891, Test accuracy: 69.19
Round  84, Train loss: 1.175, Test loss: 1.892, Test accuracy: 69.23
Round  85, Train loss: 1.176, Test loss: 1.892, Test accuracy: 69.14
Round  86, Train loss: 1.178, Test loss: 1.892, Test accuracy: 69.08
Round  87, Train loss: 1.172, Test loss: 1.892, Test accuracy: 68.97
Round  88, Train loss: 1.176, Test loss: 1.893, Test accuracy: 68.83
Round  89, Train loss: 1.175, Test loss: 1.893, Test accuracy: 68.88
Round  90, Train loss: 1.176, Test loss: 1.893, Test accuracy: 68.67
Round  91, Train loss: 1.171, Test loss: 1.893, Test accuracy: 68.58
Round  92, Train loss: 1.175, Test loss: 1.893, Test accuracy: 68.62
Round  93, Train loss: 1.173, Test loss: 1.894, Test accuracy: 68.43
Round  94, Train loss: 1.175, Test loss: 1.894, Test accuracy: 68.32
Round  95, Train loss: 1.176, Test loss: 1.895, Test accuracy: 68.32
Round  96, Train loss: 1.178, Test loss: 1.895, Test accuracy: 68.38
Round  97, Train loss: 1.174, Test loss: 1.896, Test accuracy: 68.20
Round  98, Train loss: 1.177, Test loss: 1.896, Test accuracy: 68.22
Round  99, Train loss: 1.174, Test loss: 1.896, Test accuracy: 68.16
Final Round, Train loss: 1.175, Test loss: 1.897, Test accuracy: 67.92
Average accuracy final 10 rounds: 68.39083333333332
1614.1033306121826
[]
[32.675, 51.075, 60.63333333333333, 67.50833333333334, 71.425, 71.79166666666667, 72.73333333333333, 72.98333333333333, 73.45833333333333, 74.00833333333334, 73.99166666666666, 73.93333333333334, 73.96666666666667, 73.80833333333334, 73.63333333333334, 73.53333333333333, 73.53333333333333, 73.46666666666667, 73.41666666666667, 73.70833333333333, 74.91666666666667, 75.96666666666667, 76.88333333333334, 77.33333333333333, 77.425, 77.325, 77.15833333333333, 77.125, 77.175, 77.0, 77.00833333333334, 76.525, 76.3, 76.25, 76.15833333333333, 76.01666666666667, 75.86666666666666, 75.58333333333333, 75.53333333333333, 75.31666666666666, 75.10833333333333, 74.825, 74.71666666666667, 74.65, 74.39166666666667, 74.16666666666667, 74.025, 73.89166666666667, 73.65, 73.51666666666667, 73.375, 73.125, 73.18333333333334, 72.95, 72.7, 72.575, 72.38333333333334, 72.2, 71.975, 71.90833333333333, 71.775, 71.66666666666667, 71.55833333333334, 71.425, 71.23333333333333, 71.03333333333333, 70.88333333333334, 70.85833333333333, 70.74166666666666, 70.75833333333334, 70.7, 70.45, 70.36666666666666, 70.10833333333333, 70.0, 69.88333333333334, 69.71666666666667, 69.625, 69.49166666666666, 69.53333333333333, 69.425, 69.36666666666666, 69.16666666666667, 69.19166666666666, 69.23333333333333, 69.14166666666667, 69.08333333333333, 68.96666666666667, 68.83333333333333, 68.88333333333334, 68.675, 68.58333333333333, 68.625, 68.43333333333334, 68.31666666666666, 68.31666666666666, 68.375, 68.2, 68.225, 68.15833333333333, 67.925]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Fed_scaffold %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.300, Test loss: 2.301, Test accuracy: 21.55
Round   1, Train loss: 2.297, Test loss: 2.300, Test accuracy: 25.40
Round   2, Train loss: 2.291, Test loss: 2.298, Test accuracy: 26.84
Round   3, Train loss: 2.291, Test loss: 2.297, Test accuracy: 29.32
Round   4, Train loss: 2.284, Test loss: 2.295, Test accuracy: 27.62
Round   5, Train loss: 2.279, Test loss: 2.291, Test accuracy: 28.73
Round   6, Train loss: 2.291, Test loss: 2.290, Test accuracy: 28.84
Round   7, Train loss: 2.294, Test loss: 2.288, Test accuracy: 25.17
Round   8, Train loss: 2.230, Test loss: 2.277, Test accuracy: 26.47
Round   9, Train loss: 2.167, Test loss: 2.261, Test accuracy: 31.13
Round  10, Train loss: 2.173, Test loss: 2.252, Test accuracy: 32.58
Round  11, Train loss: 1.991, Test loss: 2.231, Test accuracy: 33.58
Round  12, Train loss: 2.078, Test loss: 2.248, Test accuracy: 30.32
Round  13, Train loss: 2.278, Test loss: 2.289, Test accuracy: 25.06
Round  14, Train loss: 1.858, Test loss: 2.231, Test accuracy: 30.19
Round  15, Train loss: 2.020, Test loss: 2.211, Test accuracy: 29.37
Round  16, Train loss: 1.974, Test loss: 2.202, Test accuracy: 29.59
Round  17, Train loss: 1.631, Test loss: 2.165, Test accuracy: 33.73
Round  18, Train loss: 1.995, Test loss: 2.135, Test accuracy: 35.86
Round  19, Train loss: 1.264, Test loss: 2.108, Test accuracy: 37.17
Round  20, Train loss: 1.776, Test loss: 2.075, Test accuracy: 38.63
Round  21, Train loss: 0.878, Test loss: 2.059, Test accuracy: 40.23
Round  22, Train loss: 1.333, Test loss: 2.059, Test accuracy: 40.58
Round  23, Train loss: 1.594, Test loss: 2.093, Test accuracy: 37.00
Round  24, Train loss: 2.115, Test loss: 2.170, Test accuracy: 29.60
Round  25, Train loss: 1.390, Test loss: 2.165, Test accuracy: 28.98
Round  26, Train loss: 1.767, Test loss: 2.167, Test accuracy: 28.51
Round  27, Train loss: 0.898, Test loss: 2.126, Test accuracy: 32.72
Round  28, Train loss: 1.397, Test loss: 2.199, Test accuracy: 24.19
Round  29, Train loss: 1.163, Test loss: 2.145, Test accuracy: 31.07
Round  30, Train loss: 1.027, Test loss: 2.110, Test accuracy: 35.19
Round  31, Train loss: 1.253, Test loss: 2.085, Test accuracy: 37.01
Round  32, Train loss: 0.360, Test loss: 2.087, Test accuracy: 36.50
Round  33, Train loss: 0.581, Test loss: 2.054, Test accuracy: 39.78
Round  34, Train loss: 1.183, Test loss: 2.149, Test accuracy: 29.56
Round  35, Train loss: -0.233, Test loss: 2.100, Test accuracy: 34.92
Round  36, Train loss: 0.634, Test loss: 2.105, Test accuracy: 33.61
Round  37, Train loss: 0.497, Test loss: 2.194, Test accuracy: 23.49
Round  38, Train loss: 0.691, Test loss: 2.122, Test accuracy: 31.25
Round  39, Train loss: 0.696, Test loss: 2.118, Test accuracy: 32.29
Round  40, Train loss: -0.456, Test loss: 2.114, Test accuracy: 32.94
Round  41, Train loss: 0.619, Test loss: 2.147, Test accuracy: 29.89
Round  42, Train loss: 0.215, Test loss: 2.098, Test accuracy: 35.27
Round  43, Train loss: -0.260, Test loss: 2.146, Test accuracy: 29.91
Round  44, Train loss: -0.449, Test loss: 2.097, Test accuracy: 35.58
Round  45, Train loss: -0.534, Test loss: 2.133, Test accuracy: 32.99
Round  46, Train loss: 0.227, Test loss: 2.164, Test accuracy: 29.95
Round  47, Train loss: -0.680, Test loss: 2.113, Test accuracy: 34.85
Round  48, Train loss: 0.070, Test loss: 2.008, Test accuracy: 46.27
Round  49, Train loss: 0.082, Test loss: 2.033, Test accuracy: 43.81
Round  50, Train loss: 0.915, Test loss: 2.170, Test accuracy: 31.47
Round  51, Train loss: -0.070, Test loss: 2.068, Test accuracy: 42.39
Round  52, Train loss: -0.921, Test loss: 2.016, Test accuracy: 46.92
Round  53, Train loss: -0.617, Test loss: 2.024, Test accuracy: 46.25
Round  54, Train loss: -0.050, Test loss: 2.074, Test accuracy: 41.63
Round  55, Train loss: -0.019, Test loss: 2.077, Test accuracy: 41.01
Round  56, Train loss: -0.970, Test loss: 1.938, Test accuracy: 53.47
Round  57, Train loss: 0.093, Test loss: 1.955, Test accuracy: 53.12
Round  58, Train loss: -0.584, Test loss: 2.018, Test accuracy: 47.75
Round  59, Train loss: -1.434, Test loss: 1.975, Test accuracy: 52.09
Round  60, Train loss: 1.401, Test loss: 2.047, Test accuracy: 43.57
Round  61, Train loss: -0.316, Test loss: 2.002, Test accuracy: 47.73
Round  62, Train loss: 0.659, Test loss: 2.116, Test accuracy: 38.36
Round  63, Train loss: -1.470, Test loss: 2.031, Test accuracy: 46.58
Round  64, Train loss: 0.391, Test loss: 2.068, Test accuracy: 43.88
Round  65, Train loss: -0.180, Test loss: 2.006, Test accuracy: 51.18
Round  66, Train loss: 0.035, Test loss: 2.058, Test accuracy: 45.25
Round  67, Train loss: -0.126, Test loss: 2.141, Test accuracy: 36.36
Round  68, Train loss: -0.264, Test loss: 2.092, Test accuracy: 40.58
Round  69, Train loss: -1.169, Test loss: 2.041, Test accuracy: 44.49
Round  70, Train loss: -1.795, Test loss: 1.994, Test accuracy: 46.91
Round  71, Train loss: 0.127, Test loss: 2.073, Test accuracy: 40.08
Round  72, Train loss: -0.949, Test loss: 2.081, Test accuracy: 38.58
Round  73, Train loss: -1.036, Test loss: 2.037, Test accuracy: 42.23
Round  74, Train loss: -0.362, Test loss: 2.093, Test accuracy: 36.98
Round  75, Train loss: -0.864, Test loss: 2.048, Test accuracy: 41.10
Round  76, Train loss: -1.343, Test loss: 2.033, Test accuracy: 44.41
Round  77, Train loss: -1.043, Test loss: 1.962, Test accuracy: 52.13
Round  78, Train loss: -1.877, Test loss: 1.960, Test accuracy: 51.13
Round  79, Train loss: -1.367, Test loss: 1.953, Test accuracy: 51.94
Round  80, Train loss: -0.918, Test loss: 1.955, Test accuracy: 52.42
Round  81, Train loss: -2.162, Test loss: 1.974, Test accuracy: 50.90
Round  82, Train loss: -2.072, Test loss: 2.008, Test accuracy: 47.59
Round  83, Train loss: -2.052, Test loss: 1.970, Test accuracy: 50.83
Round  84, Train loss: -1.453, Test loss: 1.924, Test accuracy: 55.08
Round  85, Train loss: -3.341, Test loss: 1.912, Test accuracy: 56.20
Round  86, Train loss: -2.213, Test loss: 1.876, Test accuracy: 59.91
Round  87, Train loss: -1.914, Test loss: 1.888, Test accuracy: 58.30
Round  88, Train loss: -1.342, Test loss: 1.888, Test accuracy: 57.74
Round  89, Train loss: -1.569, Test loss: 1.888, Test accuracy: 58.17
Round  90, Train loss: -2.136, Test loss: 1.899, Test accuracy: 57.29
Round  91, Train loss: -2.924, Test loss: 1.857, Test accuracy: 61.06
Round  92, Train loss: -2.032, Test loss: 1.855, Test accuracy: 61.01
Round  93, Train loss: -2.623, Test loss: 1.835, Test accuracy: 63.00
Round  94, Train loss: -2.152, Test loss: 1.804, Test accuracy: 65.83
Round  95, Train loss: -1.652, Test loss: 1.822, Test accuracy: 64.60
Round  96, Train loss: -2.871, Test loss: 1.792, Test accuracy: 67.26
Round  97, Train loss: -2.446, Test loss: 1.799, Test accuracy: 66.75
Round  98, Train loss: -3.846, Test loss: 1.799, Test accuracy: 66.41
Round  99, Train loss: -3.135, Test loss: 1.767, Test accuracy: 69.75
Final Round, Train loss: 1.860, Test loss: 1.697, Test accuracy: 80.88
Average accuracy final 10 rounds: 64.29583333333332
Average global accuracy final 10 rounds: 64.29583333333332
1236.6242854595184
[]
[21.55, 25.4, 26.841666666666665, 29.316666666666666, 27.625, 28.725, 28.841666666666665, 25.166666666666668, 26.466666666666665, 31.133333333333333, 32.575, 33.575, 30.325, 25.058333333333334, 30.191666666666666, 29.366666666666667, 29.591666666666665, 33.733333333333334, 35.858333333333334, 37.166666666666664, 38.63333333333333, 40.233333333333334, 40.575, 37.0, 29.6, 28.983333333333334, 28.508333333333333, 32.71666666666667, 24.191666666666666, 31.075, 35.19166666666667, 37.00833333333333, 36.5, 39.78333333333333, 29.558333333333334, 34.925, 33.608333333333334, 23.491666666666667, 31.25, 32.291666666666664, 32.94166666666667, 29.891666666666666, 35.266666666666666, 29.908333333333335, 35.583333333333336, 32.99166666666667, 29.95, 34.85, 46.275, 43.80833333333333, 31.466666666666665, 42.391666666666666, 46.925, 46.25, 41.63333333333333, 41.00833333333333, 53.46666666666667, 53.11666666666667, 47.75, 52.09166666666667, 43.56666666666667, 47.733333333333334, 38.358333333333334, 46.583333333333336, 43.875, 51.18333333333333, 45.25, 36.358333333333334, 40.583333333333336, 44.49166666666667, 46.90833333333333, 40.083333333333336, 38.583333333333336, 42.225, 36.983333333333334, 41.1, 44.40833333333333, 52.13333333333333, 51.13333333333333, 51.94166666666667, 52.425, 50.9, 47.59166666666667, 50.825, 55.075, 56.2, 59.90833333333333, 58.3, 57.74166666666667, 58.166666666666664, 57.291666666666664, 61.05833333333333, 61.00833333333333, 63.0, 65.83333333333333, 64.6, 67.25833333333334, 66.75, 66.40833333333333, 69.75, 80.88333333333334]/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  pFedMe   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 10, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.58
Round   0, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.60
Round   1, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.60
Round   1, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.60
Round   2, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.59
Round   2, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.60
Round   3, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.62
Round   3, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.62
Round   4, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.63
Round   4, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.63
Round   5, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.64
Round   5, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.68
Round   6, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.66
Round   6, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.68
Round   7, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.68
Round   7, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.66
Round   8, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.67
Round   8, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.61
Round   9, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.66
Round   9, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.63
Round  10, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.68
Round  10, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.67
Round  11, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.66
Round  11, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.62
Round  12, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.67
Round  12, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.65
Round  13, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.65
Round  13, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.66
Round  14, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.64
Round  14, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.68
Round  15, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.68
Round  15, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.68
Round  16, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.68
Round  16, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.68
Round  17, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.68
Round  17, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.74
Round  18, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.72
Round  18, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.74
Round  19, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.70
Round  19, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.71
Round  20, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.72
Round  20, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.72
Round  21, Train loss: 2.303, Test loss: 2.303, Test accuracy: 8.72
Round  21, Global train loss: 2.303, Global test loss: 2.303, Global test accuracy: 8.76
Round  22, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.71
Round  22, Global train loss: 2.302, Global test loss: 2.303, Global test accuracy: 8.73
Round  23, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.72
Round  23, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.80
Round  24, Train loss: 2.302, Test loss: 2.303, Test accuracy: 8.76
Round  24, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.78
Round  25, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.75
Round  25, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.78
Round  26, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.74
Round  26, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.78
Round  27, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.74
Round  27, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.79
Round  28, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.78
Round  28, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.79
Round  29, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.76
Round  29, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.80
Round  30, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.78
Round  30, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.78
Round  31, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.77
Round  31, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.79
Round  32, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.77
Round  32, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.81
Round  33, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.78
Round  33, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.79
Round  34, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.79
Round  34, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.77
Round  35, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.79
Round  35, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.77
Round  36, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.78
Round  36, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.78
Round  37, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.77
Round  37, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.81
Round  38, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.79
Round  38, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.80
Round  39, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.80
Round  39, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.80
Round  40, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.78
Round  40, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.78
Round  41, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.78
Round  41, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.78
Round  42, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.79
Round  42, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.79
Round  43, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.82
Round  43, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.80
Round  44, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.82
Round  44, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.80
Round  45, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.82
Round  45, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.82
Round  46, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.82
Round  46, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.83
Round  47, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.81
Round  47, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.83
Round  48, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.81
Round  48, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.80
Round  49, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.82
Round  49, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.87
Round  50, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.84
Round  50, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.83
Round  51, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.85
Round  51, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.82
Round  52, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.84
Round  52, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.84
Round  53, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.86
Round  53, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.90
Round  54, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.90
Round  54, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.90
Round  55, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.90
Round  55, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.88
Round  56, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.89
Round  56, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.88
Round  57, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.91
Round  57, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.91
Round  58, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.91
Round  58, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.91
Round  59, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.92
Round  59, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.90
Round  60, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.92
Round  60, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.96
Round  61, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.92
Round  61, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.96
Round  62, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.91
Round  62, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.98
Round  63, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.97
Round  63, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 8.97
Round  64, Train loss: 2.302, Test loss: 2.302, Test accuracy: 8.97
Round  64, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.04
Round  65, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.00
Round  65, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.04
Round  66, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.06
Round  66, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.07
Round  67, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.11
Round  67, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.08
Round  68, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.09
Round  68, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.07
Round  69, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07
Round  69, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.13
Round  70, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.06
Round  70, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.12
Round  71, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07
Round  71, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.14
Round  72, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.09
Round  72, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.15
Round  73, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.07
Round  73, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.12
Round  74, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.11
Round  74, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.16
Round  75, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.13
Round  75, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.15
Round  76, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.15
Round  76, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.15
Round  77, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.19
Round  77, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.19
Round  78, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.19
Round  78, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.19
Round  79, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.18
Round  79, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.18
Round  80, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.22
Round  80, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.23
Round  81, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.25
Round  81, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.24
Round  82, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.24
Round  82, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.32
Round  83, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.29
Round  83, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.37
Round  84, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.31
Round  84, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.37
Round  85, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.36
Round  85, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.38
Round  86, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.37
Round  86, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.38
Round  87, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.39
Round  87, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.41
Round  88, Train loss: 2.301, Test loss: 2.302, Test accuracy: 9.41
Round  88, Global train loss: 2.301, Global test loss: 2.302, Global test accuracy: 9.39
Round  89, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.41
Round  89, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.38
Round  90, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.39
Round  90, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  91, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.43
Round  91, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.45
Round  92, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.46
Round  92, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.50
Round  93, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.48
Round  93, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.51
Round  94, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.49
Round  94, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  95, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.50
Round  95, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.52
Round  96, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.54
Round  96, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Round  97, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  97, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.54/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  98, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.56
Round  98, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.56
Round  99, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.54
Round  99, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Final Round, Train loss: 2.302, Test loss: 2.302, Test accuracy: 9.61
Final Round, Global train loss: 2.302, Global test loss: 2.302, Global test accuracy: 9.53
Average accuracy final 10 rounds: 9.495000000000001 

Average global accuracy final 10 rounds: 9.511666666666667 

1522.157037973404
[1.345780372619629, 2.6438677310943604, 3.897618055343628, 5.149887800216675, 6.393340826034546, 7.657578945159912, 8.903027057647705, 10.173727989196777, 11.458430051803589, 12.669515132904053, 13.899448156356812, 15.189198732376099, 16.481297492980957, 17.737818479537964, 18.966949701309204, 20.26699709892273, 21.476350784301758, 22.717908143997192, 24.001749753952026, 25.178337335586548, 26.34148931503296, 27.54824733734131, 28.787624835968018, 30.01316738128662, 31.240036725997925, 32.49635052680969, 33.735891580581665, 34.97860050201416, 36.2335250377655, 37.46054458618164, 38.68393874168396, 39.940083503723145, 41.16888093948364, 42.419923543930054, 43.66944670677185, 44.904146671295166, 46.126497983932495, 47.351081132888794, 48.579554080963135, 49.83730363845825, 51.065664291381836, 52.28617763519287, 53.53120040893555, 54.76760220527649, 56.024871587753296, 57.28922414779663, 58.49942231178284, 59.72195816040039, 60.94496035575867, 62.20291447639465, 63.48242402076721, 64.73577547073364, 65.99274921417236, 67.21434760093689, 68.44903779029846, 69.684410572052, 70.92881178855896, 72.17439293861389, 73.40532207489014, 74.65847992897034, 75.87550258636475, 77.12732815742493, 78.39271473884583, 79.65710687637329, 80.90868544578552, 82.16736769676208, 83.38770747184753, 84.60808229446411, 85.82097911834717, 87.07660269737244, 88.33470869064331, 89.59155344963074, 90.84845471382141, 92.10500907897949, 93.33581161499023, 94.63117980957031, 95.91133093833923, 97.17639231681824, 98.44738006591797, 99.67497181892395, 100.91582155227661, 102.18282699584961, 103.48695182800293, 104.71833562850952, 106.01604413986206, 107.31885838508606, 108.57763433456421, 109.7736747264862, 110.96989345550537, 112.19551014900208, 113.42441463470459, 114.62148070335388, 115.79871726036072, 117.00493884086609, 118.23540759086609, 119.44957566261292, 120.68578433990479, 121.9721007347107, 123.20292854309082, 124.4455885887146, 126.49242758750916]
[8.583333333333334, 8.6, 8.591666666666667, 8.625, 8.633333333333333, 8.641666666666667, 8.658333333333333, 8.675, 8.666666666666666, 8.658333333333333, 8.675, 8.658333333333333, 8.666666666666666, 8.65, 8.641666666666667, 8.683333333333334, 8.683333333333334, 8.683333333333334, 8.716666666666667, 8.7, 8.716666666666667, 8.725, 8.708333333333334, 8.725, 8.758333333333333, 8.75, 8.741666666666667, 8.741666666666667, 8.775, 8.758333333333333, 8.775, 8.766666666666667, 8.766666666666667, 8.783333333333333, 8.791666666666666, 8.791666666666666, 8.783333333333333, 8.766666666666667, 8.791666666666666, 8.8, 8.783333333333333, 8.783333333333333, 8.791666666666666, 8.825, 8.816666666666666, 8.825, 8.825, 8.808333333333334, 8.808333333333334, 8.825, 8.841666666666667, 8.85, 8.841666666666667, 8.858333333333333, 8.9, 8.9, 8.891666666666667, 8.908333333333333, 8.908333333333333, 8.916666666666666, 8.916666666666666, 8.916666666666666, 8.908333333333333, 8.975, 8.966666666666667, 9.0, 9.058333333333334, 9.108333333333333, 9.091666666666667, 9.075, 9.058333333333334, 9.066666666666666, 9.091666666666667, 9.075, 9.108333333333333, 9.133333333333333, 9.15, 9.191666666666666, 9.191666666666666, 9.175, 9.216666666666667, 9.25, 9.241666666666667, 9.291666666666666, 9.308333333333334, 9.358333333333333, 9.366666666666667, 9.391666666666667, 9.408333333333333, 9.408333333333333, 9.391666666666667, 9.425, 9.458333333333334, 9.483333333333333, 9.491666666666667, 9.5, 9.541666666666666, 9.558333333333334, 9.558333333333334, 9.541666666666666, 9.608333333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:1   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 1, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.145, Test loss: 2.102, Test accuracy: 33.71
Round   0, Global train loss: 2.145, Global test loss: 2.230, Global test accuracy: 15.05
Round   1, Train loss: 1.675, Test loss: 1.930, Test accuracy: 54.35
Round   1, Global train loss: 1.675, Global test loss: 2.098, Global test accuracy: 36.92
Round   2, Train loss: 1.618, Test loss: 1.796, Test accuracy: 72.90
Round   2, Global train loss: 1.618, Global test loss: 2.004, Global test accuracy: 55.83
Round   3, Train loss: 1.580, Test loss: 1.686, Test accuracy: 80.66
Round   3, Global train loss: 1.580, Global test loss: 1.918, Global test accuracy: 61.62
Round   4, Train loss: 1.507, Test loss: 1.674, Test accuracy: 79.57
Round   4, Global train loss: 1.507, Global test loss: 2.173, Global test accuracy: 27.23
Round   5, Train loss: 1.572, Test loss: 1.623, Test accuracy: 84.00
Round   5, Global train loss: 1.572, Global test loss: 2.188, Global test accuracy: 22.51
Round   6, Train loss: 1.578, Test loss: 1.572, Test accuracy: 89.81
Round   6, Global train loss: 1.578, Global test loss: 1.969, Global test accuracy: 53.45
Round   7, Train loss: 1.489, Test loss: 1.565, Test accuracy: 90.34
Round   7, Global train loss: 1.489, Global test loss: 2.262, Global test accuracy: 15.93
Round   8, Train loss: 1.480, Test loss: 1.565, Test accuracy: 90.16
Round   8, Global train loss: 1.480, Global test loss: 2.111, Global test accuracy: 34.88
Round   9, Train loss: 1.532, Test loss: 1.564, Test accuracy: 90.02
Round   9, Global train loss: 1.532, Global test loss: 2.061, Global test accuracy: 41.87
Round  10, Train loss: 1.546, Test loss: 1.534, Test accuracy: 93.37
Round  10, Global train loss: 1.546, Global test loss: 2.046, Global test accuracy: 45.25
Round  11, Train loss: 1.523, Test loss: 1.533, Test accuracy: 93.36
Round  11, Global train loss: 1.523, Global test loss: 2.109, Global test accuracy: 37.73
Round  12, Train loss: 1.476, Test loss: 1.533, Test accuracy: 93.31
Round  12, Global train loss: 1.476, Global test loss: 2.092, Global test accuracy: 36.49
Round  13, Train loss: 1.523, Test loss: 1.532, Test accuracy: 93.30
Round  13, Global train loss: 1.523, Global test loss: 2.023, Global test accuracy: 42.42
Round  14, Train loss: 1.472, Test loss: 1.531, Test accuracy: 93.33
Round  14, Global train loss: 1.472, Global test loss: 2.044, Global test accuracy: 48.58
Round  15, Train loss: 1.468, Test loss: 1.531, Test accuracy: 93.41
Round  15, Global train loss: 1.468, Global test loss: 2.094, Global test accuracy: 35.17
Round  16, Train loss: 1.480, Test loss: 1.517, Test accuracy: 94.88
Round  16, Global train loss: 1.480, Global test loss: 2.018, Global test accuracy: 41.97
Round  17, Train loss: 1.523, Test loss: 1.516, Test accuracy: 94.90
Round  17, Global train loss: 1.523, Global test loss: 2.086, Global test accuracy: 37.88
Round  18, Train loss: 1.467, Test loss: 1.516, Test accuracy: 94.88
Round  18, Global train loss: 1.467, Global test loss: 2.052, Global test accuracy: 41.58
Round  19, Train loss: 1.524, Test loss: 1.516, Test accuracy: 94.86
Round  19, Global train loss: 1.524, Global test loss: 1.977, Global test accuracy: 51.65
Round  20, Train loss: 1.467, Test loss: 1.515, Test accuracy: 94.88
Round  20, Global train loss: 1.467, Global test loss: 2.197, Global test accuracy: 22.70
Round  21, Train loss: 1.469, Test loss: 1.515, Test accuracy: 94.90
Round  21, Global train loss: 1.469, Global test loss: 2.115, Global test accuracy: 31.23
Round  22, Train loss: 1.506, Test loss: 1.501, Test accuracy: 96.42
Round  22, Global train loss: 1.506, Global test loss: 1.993, Global test accuracy: 47.62
Round  23, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.44
Round  23, Global train loss: 1.467, Global test loss: 1.987, Global test accuracy: 48.36
Round  24, Train loss: 1.467, Test loss: 1.501, Test accuracy: 96.44
Round  24, Global train loss: 1.467, Global test loss: 2.042, Global test accuracy: 37.15
Round  25, Train loss: 1.469, Test loss: 1.501, Test accuracy: 96.45
Round  25, Global train loss: 1.469, Global test loss: 2.067, Global test accuracy: 41.83
Round  26, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.47
Round  26, Global train loss: 1.468, Global test loss: 2.139, Global test accuracy: 28.32
Round  27, Train loss: 1.468, Test loss: 1.501, Test accuracy: 96.50
Round  27, Global train loss: 1.468, Global test loss: 2.052, Global test accuracy: 39.08
Round  28, Train loss: 1.469, Test loss: 1.501, Test accuracy: 96.48
Round  28, Global train loss: 1.469, Global test loss: 1.979, Global test accuracy: 49.69
Round  29, Train loss: 1.467, Test loss: 1.500, Test accuracy: 96.48
Round  29, Global train loss: 1.467, Global test loss: 2.021, Global test accuracy: 42.79
Round  30, Train loss: 1.467, Test loss: 1.500, Test accuracy: 96.47
Round  30, Global train loss: 1.467, Global test loss: 2.121, Global test accuracy: 34.23
Round  31, Train loss: 1.468, Test loss: 1.500, Test accuracy: 96.46
Round  31, Global train loss: 1.468, Global test loss: 2.049, Global test accuracy: 41.19
Round  32, Train loss: 1.472, Test loss: 1.499, Test accuracy: 96.56
Round  32, Global train loss: 1.472, Global test loss: 2.006, Global test accuracy: 45.77
Round  33, Train loss: 1.465, Test loss: 1.499, Test accuracy: 96.54
Round  33, Global train loss: 1.465, Global test loss: 2.172, Global test accuracy: 31.12
Round  34, Train loss: 1.467, Test loss: 1.499, Test accuracy: 96.56
Round  34, Global train loss: 1.467, Global test loss: 1.996, Global test accuracy: 50.71
Round  35, Train loss: 1.466, Test loss: 1.499, Test accuracy: 96.54
Round  35, Global train loss: 1.466, Global test loss: 1.958, Global test accuracy: 50.72
Round  36, Train loss: 1.465, Test loss: 1.499, Test accuracy: 96.52
Round  36, Global train loss: 1.465, Global test loss: 2.039, Global test accuracy: 39.73
Round  37, Train loss: 1.466, Test loss: 1.499, Test accuracy: 96.48
Round  37, Global train loss: 1.466, Global test loss: 2.085, Global test accuracy: 41.98
Round  38, Train loss: 1.468, Test loss: 1.499, Test accuracy: 96.52
Round  38, Global train loss: 1.468, Global test loss: 1.997, Global test accuracy: 43.15
Round  39, Train loss: 1.466, Test loss: 1.499, Test accuracy: 96.53
Round  39, Global train loss: 1.466, Global test loss: 2.180, Global test accuracy: 23.93
Round  40, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.55
Round  40, Global train loss: 1.465, Global test loss: 2.035, Global test accuracy: 42.74
Round  41, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.53
Round  41, Global train loss: 1.466, Global test loss: 2.068, Global test accuracy: 37.65
Round  42, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.54
Round  42, Global train loss: 1.467, Global test loss: 2.010, Global test accuracy: 51.03
Round  43, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.53
Round  43, Global train loss: 1.466, Global test loss: 2.144, Global test accuracy: 30.38
Round  44, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.54
Round  44, Global train loss: 1.465, Global test loss: 1.903, Global test accuracy: 59.15
Round  45, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.55
Round  45, Global train loss: 1.466, Global test loss: 2.011, Global test accuracy: 50.52
Round  46, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.54
Round  46, Global train loss: 1.465, Global test loss: 1.978, Global test accuracy: 59.55
Round  47, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.53
Round  47, Global train loss: 1.465, Global test loss: 2.126, Global test accuracy: 29.77
Round  48, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.54
Round  48, Global train loss: 1.466, Global test loss: 2.058, Global test accuracy: 43.05
Round  49, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.53
Round  49, Global train loss: 1.465, Global test loss: 2.098, Global test accuracy: 34.91
Round  50, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.53
Round  50, Global train loss: 1.467, Global test loss: 2.021, Global test accuracy: 44.52
Round  51, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.53
Round  51, Global train loss: 1.465, Global test loss: 2.075, Global test accuracy: 39.09
Round  52, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.53
Round  52, Global train loss: 1.466, Global test loss: 2.017, Global test accuracy: 46.07
Round  53, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.52
Round  53, Global train loss: 1.466, Global test loss: 2.028, Global test accuracy: 41.58
Round  54, Train loss: 1.463, Test loss: 1.498, Test accuracy: 96.52
Round  54, Global train loss: 1.463, Global test loss: 2.075, Global test accuracy: 38.50
Round  55, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.53
Round  55, Global train loss: 1.465, Global test loss: 1.917, Global test accuracy: 61.84
Round  56, Train loss: 1.465, Test loss: 1.498, Test accuracy: 96.52
Round  56, Global train loss: 1.465, Global test loss: 1.979, Global test accuracy: 49.83
Round  57, Train loss: 1.467, Test loss: 1.498, Test accuracy: 96.54
Round  57, Global train loss: 1.467, Global test loss: 2.042, Global test accuracy: 42.87
Round  58, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.54
Round  58, Global train loss: 1.466, Global test loss: 1.983, Global test accuracy: 62.04
Round  59, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.53
Round  59, Global train loss: 1.466, Global test loss: 1.994, Global test accuracy: 46.49
Round  60, Train loss: 1.468, Test loss: 1.498, Test accuracy: 96.53
Round  60, Global train loss: 1.468, Global test loss: 2.021, Global test accuracy: 43.56
Round  61, Train loss: 1.466, Test loss: 1.498, Test accuracy: 96.53
Round  61, Global train loss: 1.466, Global test loss: 2.021, Global test accuracy: 42.54
Round  62, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.57
Round  62, Global train loss: 1.466, Global test loss: 2.125, Global test accuracy: 30.02
Round  63, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.59
Round  63, Global train loss: 1.466, Global test loss: 2.114, Global test accuracy: 33.02
Round  64, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.60
Round  64, Global train loss: 1.466, Global test loss: 2.108, Global test accuracy: 33.15
Round  65, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.59
Round  65, Global train loss: 1.464, Global test loss: 1.971, Global test accuracy: 56.33
Round  66, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.63
Round  66, Global train loss: 1.465, Global test loss: 2.108, Global test accuracy: 33.99
Round  67, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.64
Round  67, Global train loss: 1.467, Global test loss: 2.119, Global test accuracy: 33.38
Round  68, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.63
Round  68, Global train loss: 1.464, Global test loss: 1.964, Global test accuracy: 53.81
Round  69, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.62
Round  69, Global train loss: 1.464, Global test loss: 1.964, Global test accuracy: 53.46
Round  70, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.62
Round  70, Global train loss: 1.464, Global test loss: 1.974, Global test accuracy: 49.83
Round  71, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  71, Global train loss: 1.465, Global test loss: 2.115, Global test accuracy: 34.73
Round  72, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  72, Global train loss: 1.465, Global test loss: 1.989, Global test accuracy: 48.95
Round  73, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.63
Round  73, Global train loss: 1.464, Global test loss: 1.940, Global test accuracy: 52.43
Round  74, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.63
Round  74, Global train loss: 1.464, Global test loss: 2.011, Global test accuracy: 48.62
Round  75, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  75, Global train loss: 1.465, Global test loss: 2.006, Global test accuracy: 48.02
Round  76, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.61
Round  76, Global train loss: 1.464, Global test loss: 2.024, Global test accuracy: 41.95
Round  77, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.61
Round  77, Global train loss: 1.466, Global test loss: 2.001, Global test accuracy: 45.40
Round  78, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.60
Round  78, Global train loss: 1.466, Global test loss: 2.012, Global test accuracy: 45.33
Round  79, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.62
Round  79, Global train loss: 1.467, Global test loss: 2.082, Global test accuracy: 37.38
Round  80, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  80, Global train loss: 1.465, Global test loss: 2.004, Global test accuracy: 44.73
Round  81, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.62
Round  81, Global train loss: 1.466, Global test loss: 1.959, Global test accuracy: 50.83
Round  82, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.63
Round  82, Global train loss: 1.464, Global test loss: 1.952, Global test accuracy: 54.38
Round  83, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.63
Round  83, Global train loss: 1.465, Global test loss: 2.003, Global test accuracy: 46.37
Round  84, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.63
Round  84, Global train loss: 1.465, Global test loss: 1.903, Global test accuracy: 58.39
Round  85, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.63
Round  85, Global train loss: 1.464, Global test loss: 2.075, Global test accuracy: 37.65
Round  86, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.64
Round  86, Global train loss: 1.466, Global test loss: 2.215, Global test accuracy: 21.98
Round  87, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.66
Round  87, Global train loss: 1.466, Global test loss: 2.057, Global test accuracy: 38.76
Round  88, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.66
Round  88, Global train loss: 1.466, Global test loss: 2.073, Global test accuracy: 34.42
Round  89, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.63
Round  89, Global train loss: 1.466, Global test loss: 2.107, Global test accuracy: 35.02
Round  90, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.63
Round  90, Global train loss: 1.465, Global test loss: 2.081, Global test accuracy: 40.80
Round  91, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.62
Round  91, Global train loss: 1.464, Global test loss: 1.960, Global test accuracy: 54.13
Round  92, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  92, Global train loss: 1.465, Global test loss: 1.926, Global test accuracy: 64.61
Round  93, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.62
Round  93, Global train loss: 1.465, Global test loss: 1.987, Global test accuracy: 59.33
Round  94, Train loss: 1.466, Test loss: 1.497, Test accuracy: 96.62
Round  94, Global train loss: 1.466, Global test loss: 1.987, Global test accuracy: 47.31
Round  95, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.63
Round  95, Global train loss: 1.465, Global test loss: 2.064, Global test accuracy: 37.67/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.464, Test loss: 1.497, Test accuracy: 96.63
Round  96, Global train loss: 1.464, Global test loss: 2.017, Global test accuracy: 46.43
Round  97, Train loss: 1.465, Test loss: 1.496, Test accuracy: 96.64
Round  97, Global train loss: 1.465, Global test loss: 1.917, Global test accuracy: 54.19
Round  98, Train loss: 1.465, Test loss: 1.497, Test accuracy: 96.63
Round  98, Global train loss: 1.465, Global test loss: 1.917, Global test accuracy: 53.33
Round  99, Train loss: 1.467, Test loss: 1.497, Test accuracy: 96.63
Round  99, Global train loss: 1.467, Global test loss: 2.049, Global test accuracy: 37.73
Final Round, Train loss: 1.464, Test loss: 1.496, Test accuracy: 96.66
Final Round, Global train loss: 1.464, Global test loss: 2.049, Global test accuracy: 37.73
Average accuracy final 10 rounds: 96.62916666666666 

Average global accuracy final 10 rounds: 49.555 

1770.59437084198
[1.2695696353912354, 2.5391392707824707, 3.742640733718872, 4.946142196655273, 6.141574382781982, 7.337006568908691, 8.54791784286499, 9.758829116821289, 10.976425647735596, 12.194022178649902, 13.386431694030762, 14.578841209411621, 15.796722173690796, 17.01460313796997, 18.222169637680054, 19.429736137390137, 20.58318018913269, 21.736624240875244, 22.894345998764038, 24.052067756652832, 25.2365620136261, 26.421056270599365, 27.61853575706482, 28.816015243530273, 30.03307557106018, 31.250135898590088, 32.380529165267944, 33.5109224319458, 34.73160266876221, 35.95228290557861, 37.140870571136475, 38.329458236694336, 39.53619861602783, 40.74293899536133, 41.945035457611084, 43.14713191986084, 44.394288539886475, 45.64144515991211, 46.86381816864014, 48.086191177368164, 49.2882034778595, 50.49021577835083, 51.673221588134766, 52.8562273979187, 54.062432289123535, 55.26863718032837, 56.47850155830383, 57.6883659362793, 58.90689754486084, 60.12542915344238, 61.34666967391968, 62.56791019439697, 63.754751205444336, 64.9415922164917, 66.1171464920044, 67.29270076751709, 68.4767496585846, 69.6607985496521, 70.8647096157074, 72.0686206817627, 73.2761001586914, 74.48357963562012, 75.67174339294434, 76.85990715026855, 78.072998046875, 79.28608894348145, 80.44841814041138, 81.61074733734131, 82.77267599105835, 83.93460464477539, 85.10278820991516, 86.27097177505493, 87.49661064147949, 88.72224950790405, 89.94824695587158, 91.17424440383911, 92.38775944709778, 93.60127449035645, 94.75914072990417, 95.9170069694519, 97.1247947216034, 98.33258247375488, 99.53003764152527, 100.72749280929565, 101.91712641716003, 103.10676002502441, 104.32203006744385, 105.53730010986328, 106.738942861557, 107.94058561325073, 109.1353816986084, 110.33017778396606, 111.52748012542725, 112.72478246688843, 113.94550490379333, 115.16622734069824, 116.39058566093445, 117.61494398117065, 118.81652331352234, 120.01810264587402, 121.25131583213806, 122.4845290184021, 123.68450164794922, 124.88447427749634, 126.07866263389587, 127.27285099029541, 128.49155044555664, 129.71024990081787, 130.97057127952576, 132.23089265823364, 133.44227409362793, 134.65365552902222, 135.7475757598877, 136.84149599075317, 138.03497767448425, 139.22845935821533, 140.40298676490784, 141.57751417160034, 142.73545503616333, 143.89339590072632, 144.87182331085205, 145.85025072097778, 146.9633150100708, 148.07637929916382, 149.14520835876465, 150.21403741836548, 151.20088696479797, 152.18773651123047, 153.22303199768066, 154.25832748413086, 155.28886604309082, 156.31940460205078, 157.27605414390564, 158.2327036857605, 159.1976888179779, 160.1626739501953, 161.15255880355835, 162.1424436569214, 163.14512085914612, 164.14779806137085, 165.11184573173523, 166.0758934020996, 167.04124999046326, 168.0066065788269, 169.03150129318237, 170.05639600753784, 171.09644269943237, 172.1364893913269, 173.11088299751282, 174.08527660369873, 175.08390045166016, 176.08252429962158, 177.15441989898682, 178.22631549835205, 179.20926094055176, 180.19220638275146, 181.14537239074707, 182.09853839874268, 183.10946083068848, 184.12038326263428, 185.10179829597473, 186.08321332931519, 187.07469487190247, 188.06617641448975, 189.06704020500183, 190.06790399551392, 191.07444548606873, 192.08098697662354, 193.13785123825073, 194.19471549987793, 195.21017909049988, 196.22564268112183, 197.16464757919312, 198.1036524772644, 199.13301372528076, 200.16237497329712, 201.20073580741882, 202.23909664154053, 203.1940610408783, 204.14902544021606, 205.1194155216217, 206.08980560302734, 207.0869562625885, 208.08410692214966, 209.05170464515686, 210.01930236816406, 211.02342128753662, 212.02754020690918, 213.06347846984863, 214.0994167327881, 215.13748908042908, 216.17556142807007, 217.23370099067688, 218.2918405532837, 219.29024624824524, 220.2886519432068, 221.31614470481873, 222.34363746643066, 223.38678216934204, 224.42992687225342, 226.11236691474915, 227.79480695724487]
[33.708333333333336, 33.708333333333336, 54.35, 54.35, 72.9, 72.9, 80.65833333333333, 80.65833333333333, 79.56666666666666, 79.56666666666666, 84.0, 84.0, 89.80833333333334, 89.80833333333334, 90.34166666666667, 90.34166666666667, 90.15833333333333, 90.15833333333333, 90.01666666666667, 90.01666666666667, 93.36666666666666, 93.36666666666666, 93.35833333333333, 93.35833333333333, 93.30833333333334, 93.30833333333334, 93.3, 93.3, 93.33333333333333, 93.33333333333333, 93.40833333333333, 93.40833333333333, 94.88333333333334, 94.88333333333334, 94.9, 94.9, 94.88333333333334, 94.88333333333334, 94.85833333333333, 94.85833333333333, 94.875, 94.875, 94.9, 94.9, 96.425, 96.425, 96.44166666666666, 96.44166666666666, 96.44166666666666, 96.44166666666666, 96.45, 96.45, 96.46666666666667, 96.46666666666667, 96.5, 96.5, 96.48333333333333, 96.48333333333333, 96.48333333333333, 96.48333333333333, 96.475, 96.475, 96.45833333333333, 96.45833333333333, 96.55833333333334, 96.55833333333334, 96.54166666666667, 96.54166666666667, 96.55833333333334, 96.55833333333334, 96.54166666666667, 96.54166666666667, 96.51666666666667, 96.51666666666667, 96.48333333333333, 96.48333333333333, 96.51666666666667, 96.51666666666667, 96.525, 96.525, 96.55, 96.55, 96.53333333333333, 96.53333333333333, 96.54166666666667, 96.54166666666667, 96.53333333333333, 96.53333333333333, 96.54166666666667, 96.54166666666667, 96.55, 96.55, 96.54166666666667, 96.54166666666667, 96.53333333333333, 96.53333333333333, 96.54166666666667, 96.54166666666667, 96.53333333333333, 96.53333333333333, 96.525, 96.525, 96.53333333333333, 96.53333333333333, 96.53333333333333, 96.53333333333333, 96.51666666666667, 96.51666666666667, 96.51666666666667, 96.51666666666667, 96.525, 96.525, 96.51666666666667, 96.51666666666667, 96.54166666666667, 96.54166666666667, 96.54166666666667, 96.54166666666667, 96.53333333333333, 96.53333333333333, 96.525, 96.525, 96.525, 96.525, 96.56666666666666, 96.56666666666666, 96.59166666666667, 96.59166666666667, 96.6, 96.6, 96.59166666666667, 96.59166666666667, 96.63333333333334, 96.63333333333334, 96.64166666666667, 96.64166666666667, 96.63333333333334, 96.63333333333334, 96.625, 96.625, 96.625, 96.625, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.61666666666666, 96.61666666666666, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.60833333333333, 96.6, 96.6, 96.61666666666666, 96.61666666666666, 96.625, 96.625, 96.625, 96.625, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.64166666666667, 96.64166666666667, 96.65833333333333, 96.65833333333333, 96.65833333333333, 96.65833333333333, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.625, 96.625, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.61666666666666, 96.625, 96.625, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.64166666666667, 96.64166666666667, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.63333333333334, 96.65833333333333, 96.65833333333333]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedavg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedavg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedavg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
[]
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.272, Test loss: 2.190, Test accuracy: 39.07
Round   0, Global train loss: 2.272, Global test loss: 2.192, Global test accuracy: 39.39
Round   1, Train loss: 1.945, Test loss: 1.860, Test accuracy: 65.25
Round   1, Global train loss: 1.945, Global test loss: 1.774, Global test accuracy: 72.75
Round   2, Train loss: 1.734, Test loss: 1.776, Test accuracy: 70.74
Round   2, Global train loss: 1.734, Global test loss: 1.716, Global test accuracy: 75.23
Round   3, Train loss: 1.704, Test loss: 1.724, Test accuracy: 74.61
Round   3, Global train loss: 1.704, Global test loss: 1.707, Global test accuracy: 75.70
Round   4, Train loss: 1.695, Test loss: 1.721, Test accuracy: 74.84
Round   4, Global train loss: 1.695, Global test loss: 1.701, Global test accuracy: 76.15
Round   5, Train loss: 1.692, Test loss: 1.721, Test accuracy: 74.81
Round   5, Global train loss: 1.692, Global test loss: 1.699, Global test accuracy: 76.28
Round   6, Train loss: 1.689, Test loss: 1.715, Test accuracy: 75.05
Round   6, Global train loss: 1.689, Global test loss: 1.698, Global test accuracy: 76.33
Round   7, Train loss: 1.685, Test loss: 1.709, Test accuracy: 75.46
Round   7, Global train loss: 1.685, Global test loss: 1.695, Global test accuracy: 76.46
Round   8, Train loss: 1.677, Test loss: 1.708, Test accuracy: 75.55
Round   8, Global train loss: 1.677, Global test loss: 1.694, Global test accuracy: 76.65
Round   9, Train loss: 1.681, Test loss: 1.706, Test accuracy: 75.69
Round   9, Global train loss: 1.681, Global test loss: 1.693, Global test accuracy: 76.69
Round  10, Train loss: 1.674, Test loss: 1.704, Test accuracy: 75.88
Round  10, Global train loss: 1.674, Global test loss: 1.692, Global test accuracy: 76.86
Round  11, Train loss: 1.670, Test loss: 1.702, Test accuracy: 76.07
Round  11, Global train loss: 1.670, Global test loss: 1.691, Global test accuracy: 76.94
Round  12, Train loss: 1.673, Test loss: 1.700, Test accuracy: 76.24
Round  12, Global train loss: 1.673, Global test loss: 1.690, Global test accuracy: 77.02
Round  13, Train loss: 1.670, Test loss: 1.696, Test accuracy: 76.49
Round  13, Global train loss: 1.670, Global test loss: 1.689, Global test accuracy: 77.14
Round  14, Train loss: 1.667, Test loss: 1.695, Test accuracy: 76.60
Round  14, Global train loss: 1.667, Global test loss: 1.688, Global test accuracy: 77.38
Round  15, Train loss: 1.670, Test loss: 1.692, Test accuracy: 76.85
Round  15, Global train loss: 1.670, Global test loss: 1.687, Global test accuracy: 77.45
Round  16, Train loss: 1.667, Test loss: 1.692, Test accuracy: 76.87
Round  16, Global train loss: 1.667, Global test loss: 1.686, Global test accuracy: 77.45
Round  17, Train loss: 1.658, Test loss: 1.679, Test accuracy: 78.34
Round  17, Global train loss: 1.658, Global test loss: 1.644, Global test accuracy: 82.65
Round  18, Train loss: 1.606, Test loss: 1.666, Test accuracy: 79.83
Round  18, Global train loss: 1.606, Global test loss: 1.623, Global test accuracy: 84.35
Round  19, Train loss: 1.595, Test loss: 1.657, Test accuracy: 80.68
Round  19, Global train loss: 1.595, Global test loss: 1.617, Global test accuracy: 84.61
Round  20, Train loss: 1.592, Test loss: 1.641, Test accuracy: 82.38
Round  20, Global train loss: 1.592, Global test loss: 1.612, Global test accuracy: 85.25
Round  21, Train loss: 1.592, Test loss: 1.630, Test accuracy: 83.41
Round  21, Global train loss: 1.592, Global test loss: 1.609, Global test accuracy: 85.52
Round  22, Train loss: 1.587, Test loss: 1.620, Test accuracy: 84.42
Round  22, Global train loss: 1.587, Global test loss: 1.607, Global test accuracy: 85.55
Round  23, Train loss: 1.588, Test loss: 1.614, Test accuracy: 85.03
Round  23, Global train loss: 1.588, Global test loss: 1.605, Global test accuracy: 85.76
Round  24, Train loss: 1.581, Test loss: 1.612, Test accuracy: 85.21
Round  24, Global train loss: 1.581, Global test loss: 1.604, Global test accuracy: 85.83
Round  25, Train loss: 1.578, Test loss: 1.611, Test accuracy: 85.23
Round  25, Global train loss: 1.578, Global test loss: 1.603, Global test accuracy: 85.79
Round  26, Train loss: 1.581, Test loss: 1.609, Test accuracy: 85.42
Round  26, Global train loss: 1.581, Global test loss: 1.602, Global test accuracy: 86.16
Round  27, Train loss: 1.574, Test loss: 1.607, Test accuracy: 85.56
Round  27, Global train loss: 1.574, Global test loss: 1.601, Global test accuracy: 85.94
Round  28, Train loss: 1.575, Test loss: 1.606, Test accuracy: 85.66
Round  28, Global train loss: 1.575, Global test loss: 1.600, Global test accuracy: 86.15
Round  29, Train loss: 1.578, Test loss: 1.605, Test accuracy: 85.78
Round  29, Global train loss: 1.578, Global test loss: 1.599, Global test accuracy: 86.29
Round  30, Train loss: 1.573, Test loss: 1.604, Test accuracy: 85.82
Round  30, Global train loss: 1.573, Global test loss: 1.598, Global test accuracy: 86.29
Round  31, Train loss: 1.571, Test loss: 1.603, Test accuracy: 85.94
Round  31, Global train loss: 1.571, Global test loss: 1.598, Global test accuracy: 86.31
Round  32, Train loss: 1.570, Test loss: 1.602, Test accuracy: 86.00
Round  32, Global train loss: 1.570, Global test loss: 1.597, Global test accuracy: 86.39
Round  33, Train loss: 1.574, Test loss: 1.600, Test accuracy: 86.15
Round  33, Global train loss: 1.574, Global test loss: 1.596, Global test accuracy: 86.56
Round  34, Train loss: 1.571, Test loss: 1.599, Test accuracy: 86.19
Round  34, Global train loss: 1.571, Global test loss: 1.596, Global test accuracy: 86.60
Round  35, Train loss: 1.571, Test loss: 1.599, Test accuracy: 86.22
Round  35, Global train loss: 1.571, Global test loss: 1.595, Global test accuracy: 86.64
Round  36, Train loss: 1.569, Test loss: 1.598, Test accuracy: 86.28
Round  36, Global train loss: 1.569, Global test loss: 1.594, Global test accuracy: 86.69
Round  37, Train loss: 1.570, Test loss: 1.598, Test accuracy: 86.29
Round  37, Global train loss: 1.570, Global test loss: 1.593, Global test accuracy: 86.78
Round  38, Train loss: 1.569, Test loss: 1.597, Test accuracy: 86.38
Round  38, Global train loss: 1.569, Global test loss: 1.593, Global test accuracy: 86.65
Round  39, Train loss: 1.533, Test loss: 1.575, Test accuracy: 88.83
Round  39, Global train loss: 1.533, Global test loss: 1.516, Global test accuracy: 95.12
Round  40, Train loss: 1.486, Test loss: 1.554, Test accuracy: 91.06
Round  40, Global train loss: 1.486, Global test loss: 1.510, Global test accuracy: 95.53
Round  41, Train loss: 1.483, Test loss: 1.541, Test accuracy: 92.44
Round  41, Global train loss: 1.483, Global test loss: 1.508, Global test accuracy: 95.73
Round  42, Train loss: 1.482, Test loss: 1.531, Test accuracy: 93.36
Round  42, Global train loss: 1.482, Global test loss: 1.507, Global test accuracy: 95.84
Round  43, Train loss: 1.479, Test loss: 1.525, Test accuracy: 93.92
Round  43, Global train loss: 1.479, Global test loss: 1.505, Global test accuracy: 95.95
Round  44, Train loss: 1.477, Test loss: 1.524, Test accuracy: 93.98
Round  44, Global train loss: 1.477, Global test loss: 1.504, Global test accuracy: 95.98
Round  45, Train loss: 1.477, Test loss: 1.518, Test accuracy: 94.56
Round  45, Global train loss: 1.477, Global test loss: 1.503, Global test accuracy: 96.22
Round  46, Train loss: 1.476, Test loss: 1.518, Test accuracy: 94.60
Round  46, Global train loss: 1.476, Global test loss: 1.502, Global test accuracy: 96.24
Round  47, Train loss: 1.474, Test loss: 1.512, Test accuracy: 95.16
Round  47, Global train loss: 1.474, Global test loss: 1.501, Global test accuracy: 96.25
Round  48, Train loss: 1.477, Test loss: 1.506, Test accuracy: 95.79
Round  48, Global train loss: 1.477, Global test loss: 1.500, Global test accuracy: 96.40
Round  49, Train loss: 1.476, Test loss: 1.505, Test accuracy: 95.93
Round  49, Global train loss: 1.476, Global test loss: 1.499, Global test accuracy: 96.41
Round  50, Train loss: 1.475, Test loss: 1.504, Test accuracy: 96.04
Round  50, Global train loss: 1.475, Global test loss: 1.499, Global test accuracy: 96.49
Round  51, Train loss: 1.474, Test loss: 1.503, Test accuracy: 96.08
Round  51, Global train loss: 1.474, Global test loss: 1.498, Global test accuracy: 96.47
Round  52, Train loss: 1.473, Test loss: 1.503, Test accuracy: 96.08
Round  52, Global train loss: 1.473, Global test loss: 1.498, Global test accuracy: 96.63
Round  53, Train loss: 1.474, Test loss: 1.502, Test accuracy: 96.10
Round  53, Global train loss: 1.474, Global test loss: 1.497, Global test accuracy: 96.68
Round  54, Train loss: 1.473, Test loss: 1.501, Test accuracy: 96.20
Round  54, Global train loss: 1.473, Global test loss: 1.497, Global test accuracy: 96.64
Round  55, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.22
Round  55, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.60
Round  56, Train loss: 1.472, Test loss: 1.501, Test accuracy: 96.27
Round  56, Global train loss: 1.472, Global test loss: 1.497, Global test accuracy: 96.67
Round  57, Train loss: 1.471, Test loss: 1.501, Test accuracy: 96.26
Round  57, Global train loss: 1.471, Global test loss: 1.497, Global test accuracy: 96.69
Round  58, Train loss: 1.471, Test loss: 1.500, Test accuracy: 96.28
Round  58, Global train loss: 1.471, Global test loss: 1.496, Global test accuracy: 96.66
Round  59, Train loss: 1.472, Test loss: 1.500, Test accuracy: 96.32
Round  59, Global train loss: 1.472, Global test loss: 1.496, Global test accuracy: 96.78
Round  60, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.37
Round  60, Global train loss: 1.471, Global test loss: 1.495, Global test accuracy: 96.78
Round  61, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.40
Round  61, Global train loss: 1.471, Global test loss: 1.496, Global test accuracy: 96.71
Round  62, Train loss: 1.471, Test loss: 1.499, Test accuracy: 96.47
Round  62, Global train loss: 1.471, Global test loss: 1.496, Global test accuracy: 96.64
Round  63, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.53
Round  63, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.87
Round  64, Train loss: 1.470, Test loss: 1.498, Test accuracy: 96.55
Round  64, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.72
Round  65, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  65, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.81
Round  66, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  66, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.73
Round  67, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.56
Round  67, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.76
Round  68, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.55
Round  68, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.75
Round  69, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  69, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.83
Round  70, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.58
Round  70, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.78
Round  71, Train loss: 1.470, Test loss: 1.497, Test accuracy: 96.59
Round  71, Global train loss: 1.470, Global test loss: 1.495, Global test accuracy: 96.82
Round  72, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.59
Round  72, Global train loss: 1.469, Global test loss: 1.495, Global test accuracy: 96.75
Round  73, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.61
Round  73, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.88
Round  74, Train loss: 1.469, Test loss: 1.497, Test accuracy: 96.62
Round  74, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.90
Round  75, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.64
Round  75, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.76
Round  76, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.66
Round  76, Global train loss: 1.468, Global test loss: 1.494, Global test accuracy: 96.93
Round  77, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.67
Round  77, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.97
Round  78, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.69
Round  78, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.83
Round  79, Train loss: 1.468, Test loss: 1.496, Test accuracy: 96.72
Round  79, Global train loss: 1.468, Global test loss: 1.494, Global test accuracy: 97.00
Round  80, Train loss: 1.469, Test loss: 1.496, Test accuracy: 96.75
Round  80, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.91
Round  81, Train loss: 1.468, Test loss: 1.495, Test accuracy: 96.77
Round  81, Global train loss: 1.468, Global test loss: 1.494, Global test accuracy: 96.97
Round  82, Train loss: 1.468, Test loss: 1.495, Test accuracy: 96.80
Round  82, Global train loss: 1.468, Global test loss: 1.494, Global test accuracy: 96.88
Round  83, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.80
Round  83, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 96.93
Round  84, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.83
Round  84, Global train loss: 1.469, Global test loss: 1.494, Global test accuracy: 96.90
Round  85, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.83
Round  85, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 96.92
Round  86, Train loss: 1.468, Test loss: 1.495, Test accuracy: 96.83
Round  86, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.85
Round  87, Train loss: 1.469, Test loss: 1.495, Test accuracy: 96.81
Round  87, Global train loss: 1.469, Global test loss: 1.493, Global test accuracy: 96.90
Round  88, Train loss: 1.468, Test loss: 1.495, Test accuracy: 96.85
Round  88, Global train loss: 1.468, Global test loss: 1.494, Global test accuracy: 96.89
Round  89, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.85
Round  89, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.93
Round  90, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.87
Round  90, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.93
Round  91, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.88
Round  91, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.98
Round  92, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.89
Round  92, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.94
Round  93, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.87
Round  93, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.92
Round  94, Train loss: 1.467, Test loss: 1.494, Test accuracy: 96.87
Round  94, Global train loss: 1.467, Global test loss: 1.493, Global test accuracy: 96.98
Round  95, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.86
Round  95, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.96/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  96, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.86
Round  96, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.97
Round  97, Train loss: 1.467, Test loss: 1.494, Test accuracy: 96.87
Round  97, Global train loss: 1.467, Global test loss: 1.493, Global test accuracy: 96.99
Round  98, Train loss: 1.468, Test loss: 1.494, Test accuracy: 96.87
Round  98, Global train loss: 1.468, Global test loss: 1.493, Global test accuracy: 96.93
Round  99, Train loss: 1.466, Test loss: 1.494, Test accuracy: 96.90
Round  99, Global train loss: 1.466, Global test loss: 1.493, Global test accuracy: 96.92
Final Round, Train loss: 1.467, Test loss: 1.494, Test accuracy: 96.87
Final Round, Global train loss: 1.467, Global test loss: 1.493, Global test accuracy: 96.92
Average accuracy final 10 rounds: 96.87425000000002 

Average global accuracy final 10 rounds: 96.95275000000001 

5812.538912296295
[3.997432231903076, 7.994864463806152, 11.8911612033844, 15.787457942962646, 19.745550632476807, 23.703643321990967, 27.616047859191895, 31.528452396392822, 35.41719722747803, 39.30594205856323, 43.20733165740967, 47.1087212562561, 50.95562243461609, 54.802523612976074, 58.609177589416504, 62.415831565856934, 66.26431345939636, 70.11279535293579, 74.01933169364929, 77.9258680343628, 81.7583065032959, 85.590744972229, 89.39125633239746, 93.19176769256592, 97.13996577262878, 101.08816385269165, 104.9120740890503, 108.73598432540894, 112.5413887500763, 116.34679317474365, 120.15826654434204, 123.96973991394043, 127.7677252292633, 131.56571054458618, 135.31183695793152, 139.05796337127686, 142.86883902549744, 146.67971467971802, 150.50698280334473, 154.33425092697144, 158.22869348526, 162.12313604354858, 165.97899317741394, 169.8348503112793, 173.7904224395752, 177.7459945678711, 181.65723204612732, 185.56846952438354, 189.46096992492676, 193.35347032546997, 197.3920202255249, 201.43057012557983, 205.28929543495178, 209.14802074432373, 212.98032069206238, 216.81262063980103, 220.61584615707397, 224.41907167434692, 228.31033635139465, 232.20160102844238, 236.16481709480286, 240.12803316116333, 244.02914834022522, 247.9302635192871, 251.84854459762573, 255.76682567596436, 259.6417074203491, 263.5165891647339, 267.448947429657, 271.3813056945801, 275.20388889312744, 279.0264720916748, 282.943311214447, 286.86015033721924, 290.83860898017883, 294.8170676231384, 298.6691048145294, 302.5211420059204, 306.346568107605, 310.17199420928955, 314.02214527130127, 317.872296333313, 321.7087264060974, 325.54515647888184, 329.5080349445343, 333.47091341018677, 337.4549870491028, 341.4390606880188, 345.35040307044983, 349.26174545288086, 353.0676779747009, 356.873610496521, 360.709837436676, 364.54606437683105, 368.41644763946533, 372.2868309020996, 376.0879797935486, 379.88912868499756, 383.6526198387146, 387.41611099243164, 391.23057436943054, 395.04503774642944, 398.79713344573975, 402.54922914505005, 406.3784499168396, 410.20767068862915, 414.11254239082336, 418.0174140930176, 421.8524525165558, 425.687490940094, 429.59585189819336, 433.5042128562927, 437.5505714416504, 441.59693002700806, 445.53106212615967, 449.4651942253113, 453.3442187309265, 457.22324323654175, 461.13498187065125, 465.04672050476074, 468.94531178474426, 472.8439030647278, 476.69666266441345, 480.5494222640991, 484.46969294548035, 488.3899636268616, 492.27976632118225, 496.16956901550293, 500.04566049575806, 503.9217519760132, 507.80703234672546, 511.69231271743774, 515.485924243927, 519.2795357704163, 523.1499803066254, 527.0204248428345, 530.8908178806305, 534.7612109184265, 538.650689125061, 542.5401673316956, 546.3775610923767, 550.2149548530579, 554.0523157119751, 557.8896765708923, 561.6908679008484, 565.4920592308044, 569.2823076248169, 573.0725560188293, 576.9301562309265, 580.7877564430237, 584.5735819339752, 588.3594074249268, 592.1878583431244, 596.016309261322, 599.8286743164062, 603.6410393714905, 607.4974026679993, 611.353765964508, 615.212767124176, 619.071768283844, 622.9695081710815, 626.8672480583191, 630.7651484012604, 634.6630487442017, 638.5743911266327, 642.4857335090637, 646.3493072986603, 650.2128810882568, 654.0717918872833, 657.9307026863098, 661.8496856689453, 665.7686686515808, 669.722603559494, 673.6765384674072, 677.6156885623932, 681.5548386573792, 685.4864187240601, 689.417998790741, 693.3694989681244, 697.3209991455078, 701.2955987453461, 705.2701983451843, 709.1209206581116, 712.9716429710388, 716.8329889774323, 720.6943349838257, 724.5689356327057, 728.4435362815857, 732.3162939548492, 736.1890516281128, 740.0309281349182, 743.8728046417236, 747.806875705719, 751.7409467697144, 755.5574588775635, 759.3739709854126, 763.3381822109222, 767.3023934364319, 771.2631571292877, 775.2239208221436, 777.1585392951965, 779.0931577682495]
[39.0725, 39.0725, 65.255, 65.255, 70.7375, 70.7375, 74.615, 74.615, 74.8425, 74.8425, 74.8125, 74.8125, 75.05, 75.05, 75.4625, 75.4625, 75.545, 75.545, 75.6925, 75.6925, 75.8825, 75.8825, 76.0725, 76.0725, 76.24, 76.24, 76.4875, 76.4875, 76.5975, 76.5975, 76.85, 76.85, 76.87, 76.87, 78.345, 78.345, 79.8275, 79.8275, 80.6775, 80.6775, 82.3775, 82.3775, 83.41, 83.41, 84.42, 84.42, 85.0325, 85.0325, 85.21, 85.21, 85.2275, 85.2275, 85.4175, 85.4175, 85.56, 85.56, 85.66, 85.66, 85.7775, 85.7775, 85.8175, 85.8175, 85.935, 85.935, 86.0, 86.0, 86.15, 86.15, 86.1875, 86.1875, 86.215, 86.215, 86.285, 86.285, 86.2925, 86.2925, 86.38, 86.38, 88.825, 88.825, 91.06, 91.06, 92.4425, 92.4425, 93.3575, 93.3575, 93.92, 93.92, 93.98, 93.98, 94.56, 94.56, 94.6025, 94.6025, 95.1625, 95.1625, 95.7925, 95.7925, 95.9325, 95.9325, 96.04, 96.04, 96.0825, 96.0825, 96.08, 96.08, 96.0975, 96.0975, 96.1975, 96.1975, 96.225, 96.225, 96.265, 96.265, 96.2575, 96.2575, 96.2825, 96.2825, 96.3175, 96.3175, 96.37, 96.37, 96.3975, 96.3975, 96.475, 96.475, 96.5275, 96.5275, 96.545, 96.545, 96.575, 96.575, 96.5825, 96.5825, 96.56, 96.56, 96.5475, 96.5475, 96.5775, 96.5775, 96.575, 96.575, 96.595, 96.595, 96.595, 96.595, 96.605, 96.605, 96.62, 96.62, 96.6375, 96.6375, 96.66, 96.66, 96.665, 96.665, 96.685, 96.685, 96.715, 96.715, 96.745, 96.745, 96.7675, 96.7675, 96.8, 96.8, 96.7975, 96.7975, 96.825, 96.825, 96.8325, 96.8325, 96.83, 96.83, 96.8075, 96.8075, 96.85, 96.85, 96.8525, 96.8525, 96.87, 96.87, 96.8775, 96.8775, 96.8925, 96.8925, 96.87, 96.87, 96.8675, 96.8675, 96.86, 96.86, 96.865, 96.865, 96.87, 96.87, 96.8675, 96.8675, 96.9025, 96.9025, 96.8725, 96.8725]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedrep  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.302, Test loss: 2.301, Test accuracy: 10.15
Round   1, Train loss: 2.300, Test loss: 2.298, Test accuracy: 12.88
Round   2, Train loss: 2.297, Test loss: 2.294, Test accuracy: 23.91
Round   3, Train loss: 2.292, Test loss: 2.287, Test accuracy: 38.26
Round   4, Train loss: 2.282, Test loss: 2.269, Test accuracy: 41.52
Round   5, Train loss: 2.252, Test loss: 2.217, Test accuracy: 41.30
Round   6, Train loss: 2.184, Test loss: 2.139, Test accuracy: 53.89
Round   7, Train loss: 2.068, Test loss: 2.001, Test accuracy: 64.40
Round   8, Train loss: 1.912, Test loss: 1.869, Test accuracy: 68.13
Round   9, Train loss: 1.806, Test loss: 1.800, Test accuracy: 72.51
Round  10, Train loss: 1.759, Test loss: 1.750, Test accuracy: 76.36
Round  11, Train loss: 1.725, Test loss: 1.720, Test accuracy: 77.66
Round  12, Train loss: 1.701, Test loss: 1.694, Test accuracy: 79.78
Round  13, Train loss: 1.671, Test loss: 1.676, Test accuracy: 81.17
Round  14, Train loss: 1.654, Test loss: 1.658, Test accuracy: 82.88
Round  15, Train loss: 1.639, Test loss: 1.647, Test accuracy: 83.86
Round  16, Train loss: 1.617, Test loss: 1.634, Test accuracy: 84.85
Round  17, Train loss: 1.605, Test loss: 1.623, Test accuracy: 85.93
Round  18, Train loss: 1.592, Test loss: 1.617, Test accuracy: 86.23
Round  19, Train loss: 1.588, Test loss: 1.609, Test accuracy: 86.83
Round  20, Train loss: 1.576, Test loss: 1.603, Test accuracy: 87.09
Round  21, Train loss: 1.579, Test loss: 1.599, Test accuracy: 87.77
Round  22, Train loss: 1.567, Test loss: 1.595, Test accuracy: 88.07
Round  23, Train loss: 1.564, Test loss: 1.591, Test accuracy: 88.26
Round  24, Train loss: 1.572, Test loss: 1.587, Test accuracy: 88.63
Round  25, Train loss: 1.560, Test loss: 1.584, Test accuracy: 88.88
Round  26, Train loss: 1.554, Test loss: 1.581, Test accuracy: 89.14
Round  27, Train loss: 1.556, Test loss: 1.579, Test accuracy: 89.13
Round  28, Train loss: 1.550, Test loss: 1.577, Test accuracy: 89.45
Round  29, Train loss: 1.551, Test loss: 1.576, Test accuracy: 89.51
Round  30, Train loss: 1.552, Test loss: 1.574, Test accuracy: 89.65
Round  31, Train loss: 1.548, Test loss: 1.571, Test accuracy: 89.97
Round  32, Train loss: 1.546, Test loss: 1.571, Test accuracy: 89.83
Round  33, Train loss: 1.534, Test loss: 1.570, Test accuracy: 89.90
Round  34, Train loss: 1.544, Test loss: 1.569, Test accuracy: 89.87
Round  35, Train loss: 1.539, Test loss: 1.567, Test accuracy: 90.10
Round  36, Train loss: 1.532, Test loss: 1.567, Test accuracy: 89.92
Round  37, Train loss: 1.538, Test loss: 1.567, Test accuracy: 89.99
Round  38, Train loss: 1.535, Test loss: 1.565, Test accuracy: 90.23
Round  39, Train loss: 1.530, Test loss: 1.565, Test accuracy: 90.07
Round  40, Train loss: 1.527, Test loss: 1.565, Test accuracy: 90.28
Round  41, Train loss: 1.526, Test loss: 1.564, Test accuracy: 90.33
Round  42, Train loss: 1.525, Test loss: 1.561, Test accuracy: 90.56
Round  43, Train loss: 1.529, Test loss: 1.561, Test accuracy: 90.49
Round  44, Train loss: 1.526, Test loss: 1.560, Test accuracy: 90.70
Round  45, Train loss: 1.523, Test loss: 1.561, Test accuracy: 90.58
Round  46, Train loss: 1.528, Test loss: 1.560, Test accuracy: 90.62
Round  47, Train loss: 1.522, Test loss: 1.559, Test accuracy: 90.58
Round  48, Train loss: 1.523, Test loss: 1.559, Test accuracy: 90.68
Round  49, Train loss: 1.523, Test loss: 1.558, Test accuracy: 90.75
Round  50, Train loss: 1.515, Test loss: 1.558, Test accuracy: 90.74
Round  51, Train loss: 1.523, Test loss: 1.558, Test accuracy: 90.68
Round  52, Train loss: 1.516, Test loss: 1.557, Test accuracy: 90.75
Round  53, Train loss: 1.512, Test loss: 1.556, Test accuracy: 90.87
Round  54, Train loss: 1.513, Test loss: 1.556, Test accuracy: 90.84
Round  55, Train loss: 1.515, Test loss: 1.555, Test accuracy: 91.03
Round  56, Train loss: 1.512, Test loss: 1.555, Test accuracy: 90.92
Round  57, Train loss: 1.517, Test loss: 1.555, Test accuracy: 90.94
Round  58, Train loss: 1.511, Test loss: 1.553, Test accuracy: 91.32
Round  59, Train loss: 1.512, Test loss: 1.554, Test accuracy: 91.21
Round  60, Train loss: 1.508, Test loss: 1.553, Test accuracy: 91.09
Round  61, Train loss: 1.506, Test loss: 1.553, Test accuracy: 91.17
Round  62, Train loss: 1.510, Test loss: 1.554, Test accuracy: 91.10
Round  63, Train loss: 1.509, Test loss: 1.553, Test accuracy: 91.21
Round  64, Train loss: 1.511, Test loss: 1.552, Test accuracy: 91.33
Round  65, Train loss: 1.503, Test loss: 1.551, Test accuracy: 91.28
Round  66, Train loss: 1.511, Test loss: 1.551, Test accuracy: 91.36
Round  67, Train loss: 1.509, Test loss: 1.552, Test accuracy: 91.19
Round  68, Train loss: 1.506, Test loss: 1.552, Test accuracy: 91.24
Round  69, Train loss: 1.504, Test loss: 1.551, Test accuracy: 91.38
Round  70, Train loss: 1.505, Test loss: 1.552, Test accuracy: 91.28
Round  71, Train loss: 1.508, Test loss: 1.549, Test accuracy: 91.52
Round  72, Train loss: 1.509, Test loss: 1.549, Test accuracy: 91.61
Round  73, Train loss: 1.506, Test loss: 1.549, Test accuracy: 91.51
Round  74, Train loss: 1.506, Test loss: 1.549, Test accuracy: 91.62
Round  75, Train loss: 1.502, Test loss: 1.548, Test accuracy: 91.64
Round  76, Train loss: 1.501, Test loss: 1.549, Test accuracy: 91.51
Round  77, Train loss: 1.504, Test loss: 1.549, Test accuracy: 91.51
Round  78, Train loss: 1.500, Test loss: 1.549, Test accuracy: 91.47
Round  79, Train loss: 1.502, Test loss: 1.548, Test accuracy: 91.58
Round  80, Train loss: 1.500, Test loss: 1.548, Test accuracy: 91.58
Round  81, Train loss: 1.506, Test loss: 1.548, Test accuracy: 91.52
Round  82, Train loss: 1.501, Test loss: 1.549, Test accuracy: 91.45
Round  83, Train loss: 1.505, Test loss: 1.549, Test accuracy: 91.51
Round  84, Train loss: 1.497, Test loss: 1.548, Test accuracy: 91.67
Round  85, Train loss: 1.498, Test loss: 1.548, Test accuracy: 91.63
Round  86, Train loss: 1.500, Test loss: 1.547, Test accuracy: 91.77
Round  87, Train loss: 1.504, Test loss: 1.547, Test accuracy: 91.69
Round  88, Train loss: 1.500, Test loss: 1.547, Test accuracy: 91.66
Round  89, Train loss: 1.498, Test loss: 1.547, Test accuracy: 91.67
Round  90, Train loss: 1.504, Test loss: 1.547, Test accuracy: 91.68
Round  91, Train loss: 1.495, Test loss: 1.548, Test accuracy: 91.61
Round  92, Train loss: 1.500, Test loss: 1.547, Test accuracy: 91.63
Round  93, Train loss: 1.497, Test loss: 1.547, Test accuracy: 91.62
Round  94, Train loss: 1.495, Test loss: 1.547, Test accuracy: 91.60
Round  95, Train loss: 1.497, Test loss: 1.547, Test accuracy: 91.77
Round  96, Train loss: 1.499, Test loss: 1.547, Test accuracy: 91.72
Round  97, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.84
Round  98, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.86/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.494, Test loss: 1.545, Test accuracy: 91.88
Final Round, Train loss: 1.497, Test loss: 1.546, Test accuracy: 91.67
Average accuracy final 10 rounds: 91.72166666666666 

1341.6094379425049
[1.1674270629882812, 2.3348541259765625, 3.3651976585388184, 4.395541191101074, 5.443749189376831, 6.491957187652588, 7.54297137260437, 8.593985557556152, 9.631113529205322, 10.668241500854492, 11.728296041488647, 12.788350582122803, 13.836641311645508, 14.884932041168213, 15.918029308319092, 16.95112657546997, 18.004302978515625, 19.05747938156128, 20.10676121711731, 21.15604305267334, 22.209025382995605, 23.26200771331787, 24.305655002593994, 25.349302291870117, 26.405174016952515, 27.461045742034912, 28.499578714370728, 29.538111686706543, 30.622025728225708, 31.705939769744873, 32.7047164440155, 33.70349311828613, 34.75512933731079, 35.80676555633545, 36.837895154953, 37.86902475357056, 38.90806555747986, 39.94710636138916, 40.995367765426636, 42.04362916946411, 43.12657380104065, 44.20951843261719, 45.239142179489136, 46.268765926361084, 47.31299042701721, 48.35721492767334, 49.464919567108154, 50.57262420654297, 51.61282467842102, 52.65302515029907, 53.720452547073364, 54.787879943847656, 55.82691311836243, 56.8659462928772, 57.9056761264801, 58.94540596008301, 59.946539640426636, 60.947673320770264, 62.02715706825256, 63.10664081573486, 64.10921454429626, 65.11178827285767, 66.18443703651428, 67.2570858001709, 68.30595207214355, 69.35481834411621, 70.41545510292053, 71.47609186172485, 72.52659606933594, 73.57710027694702, 74.65581798553467, 75.73453569412231, 76.77789878845215, 77.82126188278198, 78.83815097808838, 79.85504007339478, 80.900146484375, 81.94525289535522, 82.97558951377869, 84.00592613220215, 85.03421664237976, 86.06250715255737, 87.10261821746826, 88.14272928237915, 89.1543493270874, 90.16596937179565, 91.24033570289612, 92.31470203399658, 93.35681962966919, 94.3989372253418, 95.42806220054626, 96.45718717575073, 97.51508784294128, 98.57298851013184, 99.66839051246643, 100.76379251480103, 101.8084933757782, 102.85319423675537, 103.94420433044434, 105.0352144241333, 106.07491445541382, 107.11461448669434, 108.17317605018616, 109.23173761367798, 110.2622241973877, 111.29271078109741, 112.37518906593323, 113.45766735076904, 114.46054720878601, 115.46342706680298, 116.528235912323, 117.59304475784302, 118.59545874595642, 119.59787273406982, 120.62743520736694, 121.65699768066406, 122.75012850761414, 123.84325933456421, 124.86862015724182, 125.89398097991943, 126.97693729400635, 128.05989360809326, 129.12687230110168, 130.1938509941101, 131.27846360206604, 132.36307621002197, 133.37480998039246, 134.38654375076294, 135.48325204849243, 136.57996034622192, 137.60976481437683, 138.63956928253174, 139.72839426994324, 140.81721925735474, 141.85809206962585, 142.89896488189697, 143.9743001461029, 145.04963541030884, 146.04359245300293, 147.03754949569702, 148.10218620300293, 149.16682291030884, 150.14820957183838, 151.12959623336792, 152.1770749092102, 153.2245535850525, 154.26472568511963, 155.30489778518677, 156.3512101173401, 157.3975224494934, 158.41861724853516, 159.4397120475769, 160.48814153671265, 161.5365710258484, 162.55490922927856, 163.57324743270874, 164.62361979484558, 165.67399215698242, 166.69828724861145, 167.72258234024048, 168.65950536727905, 169.59642839431763, 170.61433291435242, 171.6322374343872, 172.66790342330933, 173.70356941223145, 174.71284651756287, 175.7221236228943, 176.76919436454773, 177.81626510620117, 178.7578103542328, 179.6993556022644, 180.6822624206543, 181.6651692390442, 182.71641945838928, 183.76766967773438, 184.78315424919128, 185.7986388206482, 186.80360794067383, 187.80857706069946, 188.8572793006897, 189.90598154067993, 190.93551564216614, 191.96504974365234, 192.97364449501038, 193.9822392463684, 195.03775215148926, 196.0932650566101, 197.09106302261353, 198.08886098861694, 199.10665249824524, 200.12444400787354, 201.1057586669922, 202.08707332611084, 203.1310169696808, 204.17496061325073, 205.16514921188354, 206.15533781051636, 207.21487069129944, 208.27440357208252, 209.87439680099487, 211.47439002990723]
[10.15, 10.15, 12.875, 12.875, 23.908333333333335, 23.908333333333335, 38.25833333333333, 38.25833333333333, 41.525, 41.525, 41.3, 41.3, 53.891666666666666, 53.891666666666666, 64.4, 64.4, 68.13333333333334, 68.13333333333334, 72.50833333333334, 72.50833333333334, 76.35833333333333, 76.35833333333333, 77.65833333333333, 77.65833333333333, 79.775, 79.775, 81.175, 81.175, 82.875, 82.875, 83.85833333333333, 83.85833333333333, 84.85, 84.85, 85.93333333333334, 85.93333333333334, 86.23333333333333, 86.23333333333333, 86.83333333333333, 86.83333333333333, 87.09166666666667, 87.09166666666667, 87.76666666666667, 87.76666666666667, 88.06666666666666, 88.06666666666666, 88.25833333333334, 88.25833333333334, 88.63333333333334, 88.63333333333334, 88.88333333333334, 88.88333333333334, 89.14166666666667, 89.14166666666667, 89.13333333333334, 89.13333333333334, 89.45, 89.45, 89.50833333333334, 89.50833333333334, 89.65, 89.65, 89.96666666666667, 89.96666666666667, 89.825, 89.825, 89.9, 89.9, 89.86666666666666, 89.86666666666666, 90.1, 90.1, 89.925, 89.925, 89.99166666666666, 89.99166666666666, 90.23333333333333, 90.23333333333333, 90.06666666666666, 90.06666666666666, 90.275, 90.275, 90.33333333333333, 90.33333333333333, 90.55833333333334, 90.55833333333334, 90.49166666666666, 90.49166666666666, 90.7, 90.7, 90.58333333333333, 90.58333333333333, 90.61666666666666, 90.61666666666666, 90.575, 90.575, 90.68333333333334, 90.68333333333334, 90.75, 90.75, 90.74166666666666, 90.74166666666666, 90.68333333333334, 90.68333333333334, 90.75, 90.75, 90.86666666666666, 90.86666666666666, 90.84166666666667, 90.84166666666667, 91.025, 91.025, 90.925, 90.925, 90.94166666666666, 90.94166666666666, 91.31666666666666, 91.31666666666666, 91.20833333333333, 91.20833333333333, 91.09166666666667, 91.09166666666667, 91.16666666666667, 91.16666666666667, 91.1, 91.1, 91.20833333333333, 91.20833333333333, 91.33333333333333, 91.33333333333333, 91.275, 91.275, 91.35833333333333, 91.35833333333333, 91.19166666666666, 91.19166666666666, 91.24166666666666, 91.24166666666666, 91.375, 91.375, 91.28333333333333, 91.28333333333333, 91.51666666666667, 91.51666666666667, 91.60833333333333, 91.60833333333333, 91.50833333333334, 91.50833333333334, 91.625, 91.625, 91.64166666666667, 91.64166666666667, 91.50833333333334, 91.50833333333334, 91.50833333333334, 91.50833333333334, 91.46666666666667, 91.46666666666667, 91.575, 91.575, 91.58333333333333, 91.58333333333333, 91.51666666666667, 91.51666666666667, 91.45, 91.45, 91.50833333333334, 91.50833333333334, 91.675, 91.675, 91.63333333333334, 91.63333333333334, 91.76666666666667, 91.76666666666667, 91.69166666666666, 91.69166666666666, 91.65833333333333, 91.65833333333333, 91.66666666666667, 91.66666666666667, 91.68333333333334, 91.68333333333334, 91.60833333333333, 91.60833333333333, 91.63333333333334, 91.63333333333334, 91.625, 91.625, 91.6, 91.6, 91.76666666666667, 91.76666666666667, 91.725, 91.725, 91.84166666666667, 91.84166666666667, 91.85833333333333, 91.85833333333333, 91.875, 91.875, 91.675, 91.675]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  fedper  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedper , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

fedper
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 25.25
Round   1, Train loss: 2.296, Test loss: 2.291, Test accuracy: 26.23
Round   2, Train loss: 2.270, Test loss: 2.245, Test accuracy: 27.58
Round   3, Train loss: 2.206, Test loss: 2.175, Test accuracy: 36.01
Round   4, Train loss: 2.069, Test loss: 2.014, Test accuracy: 53.23
Round   5, Train loss: 1.881, Test loss: 1.859, Test accuracy: 68.68
Round   6, Train loss: 1.769, Test loss: 1.763, Test accuracy: 74.63
Round   7, Train loss: 1.677, Test loss: 1.697, Test accuracy: 79.83
Round   8, Train loss: 1.643, Test loss: 1.652, Test accuracy: 83.63
Round   9, Train loss: 1.592, Test loss: 1.626, Test accuracy: 85.86
Round  10, Train loss: 1.587, Test loss: 1.607, Test accuracy: 87.38
Round  11, Train loss: 1.577, Test loss: 1.599, Test accuracy: 87.69
Round  12, Train loss: 1.557, Test loss: 1.594, Test accuracy: 88.12
Round  13, Train loss: 1.553, Test loss: 1.580, Test accuracy: 89.48
Round  14, Train loss: 1.550, Test loss: 1.575, Test accuracy: 89.84
Round  15, Train loss: 1.537, Test loss: 1.574, Test accuracy: 89.80
Round  16, Train loss: 1.535, Test loss: 1.571, Test accuracy: 90.08
Round  17, Train loss: 1.532, Test loss: 1.569, Test accuracy: 90.17
Round  18, Train loss: 1.535, Test loss: 1.565, Test accuracy: 90.49
Round  19, Train loss: 1.527, Test loss: 1.564, Test accuracy: 90.67
Round  20, Train loss: 1.526, Test loss: 1.561, Test accuracy: 90.88
Round  21, Train loss: 1.526, Test loss: 1.559, Test accuracy: 90.98
Round  22, Train loss: 1.530, Test loss: 1.555, Test accuracy: 91.11
Round  23, Train loss: 1.520, Test loss: 1.554, Test accuracy: 91.28
Round  24, Train loss: 1.519, Test loss: 1.553, Test accuracy: 91.33
Round  25, Train loss: 1.511, Test loss: 1.552, Test accuracy: 91.43
Round  26, Train loss: 1.514, Test loss: 1.552, Test accuracy: 91.41
Round  27, Train loss: 1.512, Test loss: 1.551, Test accuracy: 91.50
Round  28, Train loss: 1.504, Test loss: 1.551, Test accuracy: 91.51
Round  29, Train loss: 1.512, Test loss: 1.550, Test accuracy: 91.65
Round  30, Train loss: 1.511, Test loss: 1.549, Test accuracy: 91.78
Round  31, Train loss: 1.506, Test loss: 1.548, Test accuracy: 91.76
Round  32, Train loss: 1.509, Test loss: 1.548, Test accuracy: 91.78
Round  33, Train loss: 1.506, Test loss: 1.548, Test accuracy: 91.78
Round  34, Train loss: 1.506, Test loss: 1.547, Test accuracy: 91.87
Round  35, Train loss: 1.502, Test loss: 1.546, Test accuracy: 91.97
Round  36, Train loss: 1.504, Test loss: 1.546, Test accuracy: 91.92
Round  37, Train loss: 1.504, Test loss: 1.545, Test accuracy: 91.98
Round  38, Train loss: 1.500, Test loss: 1.545, Test accuracy: 91.95
Round  39, Train loss: 1.498, Test loss: 1.545, Test accuracy: 91.92
Round  40, Train loss: 1.499, Test loss: 1.545, Test accuracy: 92.06
Round  41, Train loss: 1.500, Test loss: 1.544, Test accuracy: 92.07
Round  42, Train loss: 1.496, Test loss: 1.544, Test accuracy: 92.09
Round  43, Train loss: 1.497, Test loss: 1.544, Test accuracy: 92.12
Round  44, Train loss: 1.496, Test loss: 1.544, Test accuracy: 92.12
Round  45, Train loss: 1.495, Test loss: 1.544, Test accuracy: 92.08
Round  46, Train loss: 1.496, Test loss: 1.543, Test accuracy: 92.14
Round  47, Train loss: 1.499, Test loss: 1.542, Test accuracy: 92.12
Round  48, Train loss: 1.493, Test loss: 1.542, Test accuracy: 92.27
Round  49, Train loss: 1.490, Test loss: 1.541, Test accuracy: 92.25
Round  50, Train loss: 1.495, Test loss: 1.541, Test accuracy: 92.20
Round  51, Train loss: 1.497, Test loss: 1.541, Test accuracy: 92.28
Round  52, Train loss: 1.493, Test loss: 1.540, Test accuracy: 92.38
Round  53, Train loss: 1.494, Test loss: 1.540, Test accuracy: 92.26
Round  54, Train loss: 1.493, Test loss: 1.540, Test accuracy: 92.24
Round  55, Train loss: 1.491, Test loss: 1.541, Test accuracy: 92.34
Round  56, Train loss: 1.492, Test loss: 1.540, Test accuracy: 92.38
Round  57, Train loss: 1.495, Test loss: 1.540, Test accuracy: 92.38
Round  58, Train loss: 1.495, Test loss: 1.540, Test accuracy: 92.39
Round  59, Train loss: 1.493, Test loss: 1.539, Test accuracy: 92.33
Round  60, Train loss: 1.490, Test loss: 1.539, Test accuracy: 92.51
Round  61, Train loss: 1.499, Test loss: 1.539, Test accuracy: 92.45
Round  62, Train loss: 1.493, Test loss: 1.539, Test accuracy: 92.38
Round  63, Train loss: 1.487, Test loss: 1.539, Test accuracy: 92.41
Round  64, Train loss: 1.487, Test loss: 1.539, Test accuracy: 92.41
Round  65, Train loss: 1.491, Test loss: 1.539, Test accuracy: 92.42
Round  66, Train loss: 1.491, Test loss: 1.539, Test accuracy: 92.45
Round  67, Train loss: 1.490, Test loss: 1.539, Test accuracy: 92.35
Round  68, Train loss: 1.496, Test loss: 1.539, Test accuracy: 92.49
Round  69, Train loss: 1.489, Test loss: 1.539, Test accuracy: 92.39
Round  70, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.35
Round  71, Train loss: 1.492, Test loss: 1.539, Test accuracy: 92.42
Round  72, Train loss: 1.495, Test loss: 1.539, Test accuracy: 92.40
Round  73, Train loss: 1.487, Test loss: 1.538, Test accuracy: 92.47
Round  74, Train loss: 1.487, Test loss: 1.538, Test accuracy: 92.53
Round  75, Train loss: 1.488, Test loss: 1.538, Test accuracy: 92.52
Round  76, Train loss: 1.487, Test loss: 1.538, Test accuracy: 92.47
Round  77, Train loss: 1.494, Test loss: 1.538, Test accuracy: 92.47
Round  78, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.46
Round  79, Train loss: 1.491, Test loss: 1.538, Test accuracy: 92.42
Round  80, Train loss: 1.490, Test loss: 1.538, Test accuracy: 92.48
Round  81, Train loss: 1.484, Test loss: 1.537, Test accuracy: 92.51
Round  82, Train loss: 1.489, Test loss: 1.538, Test accuracy: 92.44
Round  83, Train loss: 1.486, Test loss: 1.537, Test accuracy: 92.58
Round  84, Train loss: 1.481, Test loss: 1.537, Test accuracy: 92.67
Round  85, Train loss: 1.487, Test loss: 1.537, Test accuracy: 92.58
Round  86, Train loss: 1.493, Test loss: 1.537, Test accuracy: 92.61
Round  87, Train loss: 1.488, Test loss: 1.537, Test accuracy: 92.65
Round  88, Train loss: 1.487, Test loss: 1.537, Test accuracy: 92.58
Round  89, Train loss: 1.486, Test loss: 1.537, Test accuracy: 92.62
Round  90, Train loss: 1.489, Test loss: 1.537, Test accuracy: 92.64
Round  91, Train loss: 1.493, Test loss: 1.537, Test accuracy: 92.63
Round  92, Train loss: 1.485, Test loss: 1.537, Test accuracy: 92.61
Round  93, Train loss: 1.491, Test loss: 1.537, Test accuracy: 92.70
Round  94, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.58
Round  95, Train loss: 1.487, Test loss: 1.536, Test accuracy: 92.69
Round  96, Train loss: 1.489, Test loss: 1.536, Test accuracy: 92.65
Round  97, Train loss: 1.484, Test loss: 1.536, Test accuracy: 92.63
Round  98, Train loss: 1.488, Test loss: 1.537, Test accuracy: 92.58/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  99, Train loss: 1.490, Test loss: 1.536, Test accuracy: 92.64
Final Round, Train loss: 1.489, Test loss: 1.537, Test accuracy: 92.58
Average accuracy final 10 rounds: 92.63583333333334 

1296.3969767093658
[1.1156461238861084, 2.231292247772217, 3.2666406631469727, 4.3019890785217285, 5.367576837539673, 6.433164596557617, 7.57938551902771, 8.725606441497803, 9.805377721786499, 10.885149002075195, 12.005979061126709, 13.126809120178223, 14.242389678955078, 15.357970237731934, 16.42579674720764, 17.49362325668335, 18.55298686027527, 19.612350463867188, 20.740241765975952, 21.868133068084717, 22.783284425735474, 23.69843578338623, 24.689316272735596, 25.68019676208496, 26.600488424301147, 27.520780086517334, 28.51297092437744, 29.50516176223755, 30.437024116516113, 31.368886470794678, 32.30776810646057, 33.246649742126465, 34.191001415252686, 35.135353088378906, 36.1023964881897, 37.06943988800049, 38.017507791519165, 38.96557569503784, 39.891067028045654, 40.81655836105347, 41.793161392211914, 42.76976442337036, 43.662755250930786, 44.55574607849121, 45.58303093910217, 46.610315799713135, 47.560253858566284, 48.510191917419434, 49.518426179885864, 50.526660442352295, 51.486486196517944, 52.446311950683594, 53.46017527580261, 54.47403860092163, 55.415634870529175, 56.35723114013672, 57.32330846786499, 58.28938579559326, 59.296438455581665, 60.30349111557007, 61.29525685310364, 62.28702259063721, 63.28119444847107, 64.27536630630493, 65.2393445968628, 66.20332288742065, 67.20439457893372, 68.20546627044678, 69.14977836608887, 70.09409046173096, 71.11273074150085, 72.13137102127075, 73.06671619415283, 74.00206136703491, 75.00401902198792, 76.00597667694092, 77.00273180007935, 77.99948692321777, 78.97066617012024, 79.9418454170227, 80.9031548500061, 81.8644642829895, 82.83157873153687, 83.79869318008423, 84.76466083526611, 85.730628490448, 86.76250195503235, 87.7943754196167, 88.73946142196655, 89.6845474243164, 90.61826515197754, 91.55198287963867, 92.53869581222534, 93.52540874481201, 94.4724109172821, 95.4194130897522, 96.35772514343262, 97.29603719711304, 98.25213670730591, 99.20823621749878, 100.17921805381775, 101.15019989013672, 102.1024558544159, 103.05471181869507, 104.05074381828308, 105.0467758178711, 105.98682427406311, 106.92687273025513, 107.88760614395142, 108.8483395576477, 109.80300188064575, 110.7576642036438, 111.67661476135254, 112.59556531906128, 113.53606677055359, 114.4765682220459, 115.43292665481567, 116.38928508758545, 117.361820936203, 118.33435678482056, 119.23487734794617, 120.13539791107178, 121.06542229652405, 121.99544668197632, 122.93801975250244, 123.88059282302856, 124.83459639549255, 125.78859996795654, 126.77797961235046, 127.76735925674438, 128.67311549186707, 129.57887172698975, 130.57735753059387, 131.575843334198, 132.54298996925354, 133.51013660430908, 134.4636869430542, 135.41723728179932, 136.38211512565613, 137.34699296951294, 138.30505180358887, 139.2631106376648, 140.22659587860107, 141.19008111953735, 142.16069674491882, 143.1313123703003, 144.0868763923645, 145.0424404144287, 145.9993190765381, 146.95619773864746, 147.96469712257385, 148.97319650650024, 149.95821261405945, 150.94322872161865, 151.8943133354187, 152.84539794921875, 153.77431535720825, 154.70323276519775, 155.66507720947266, 156.62692165374756, 157.60074853897095, 158.57457542419434, 159.50434732437134, 160.43411922454834, 161.39865016937256, 162.36318111419678, 163.29698657989502, 164.23079204559326, 165.16799020767212, 166.10518836975098, 167.06778287887573, 168.0303773880005, 168.92848777770996, 169.82659816741943, 170.8246750831604, 171.82275199890137, 172.8095464706421, 173.7963409423828, 174.72128129005432, 175.64622163772583, 176.62175226211548, 177.59728288650513, 178.5250108242035, 179.45273876190186, 180.39788460731506, 181.34303045272827, 182.35029220581055, 183.35755395889282, 184.32049441337585, 185.2834348678589, 186.21190977096558, 187.14038467407227, 188.1317217350006, 189.12305879592896, 190.05116510391235, 190.97927141189575, 191.9480426311493, 192.91681385040283, 193.87325763702393, 194.82970142364502, 196.36296582221985, 197.89623022079468]
[25.25, 25.25, 26.225, 26.225, 27.583333333333332, 27.583333333333332, 36.00833333333333, 36.00833333333333, 53.233333333333334, 53.233333333333334, 68.68333333333334, 68.68333333333334, 74.63333333333334, 74.63333333333334, 79.83333333333333, 79.83333333333333, 83.63333333333334, 83.63333333333334, 85.85833333333333, 85.85833333333333, 87.375, 87.375, 87.69166666666666, 87.69166666666666, 88.125, 88.125, 89.48333333333333, 89.48333333333333, 89.84166666666667, 89.84166666666667, 89.8, 89.8, 90.08333333333333, 90.08333333333333, 90.16666666666667, 90.16666666666667, 90.49166666666666, 90.49166666666666, 90.675, 90.675, 90.875, 90.875, 90.98333333333333, 90.98333333333333, 91.10833333333333, 91.10833333333333, 91.275, 91.275, 91.33333333333333, 91.33333333333333, 91.43333333333334, 91.43333333333334, 91.40833333333333, 91.40833333333333, 91.5, 91.5, 91.50833333333334, 91.50833333333334, 91.65, 91.65, 91.775, 91.775, 91.75833333333334, 91.75833333333334, 91.78333333333333, 91.78333333333333, 91.78333333333333, 91.78333333333333, 91.86666666666666, 91.86666666666666, 91.96666666666667, 91.96666666666667, 91.91666666666667, 91.91666666666667, 91.98333333333333, 91.98333333333333, 91.95, 91.95, 91.91666666666667, 91.91666666666667, 92.05833333333334, 92.05833333333334, 92.06666666666666, 92.06666666666666, 92.09166666666667, 92.09166666666667, 92.11666666666666, 92.11666666666666, 92.11666666666666, 92.11666666666666, 92.08333333333333, 92.08333333333333, 92.14166666666667, 92.14166666666667, 92.125, 92.125, 92.26666666666667, 92.26666666666667, 92.25, 92.25, 92.2, 92.2, 92.275, 92.275, 92.375, 92.375, 92.25833333333334, 92.25833333333334, 92.24166666666666, 92.24166666666666, 92.34166666666667, 92.34166666666667, 92.38333333333334, 92.38333333333334, 92.38333333333334, 92.38333333333334, 92.39166666666667, 92.39166666666667, 92.325, 92.325, 92.50833333333334, 92.50833333333334, 92.45, 92.45, 92.375, 92.375, 92.40833333333333, 92.40833333333333, 92.40833333333333, 92.40833333333333, 92.41666666666667, 92.41666666666667, 92.45, 92.45, 92.35, 92.35, 92.49166666666666, 92.49166666666666, 92.39166666666667, 92.39166666666667, 92.35, 92.35, 92.425, 92.425, 92.4, 92.4, 92.46666666666667, 92.46666666666667, 92.53333333333333, 92.53333333333333, 92.51666666666667, 92.51666666666667, 92.475, 92.475, 92.46666666666667, 92.46666666666667, 92.45833333333333, 92.45833333333333, 92.425, 92.425, 92.48333333333333, 92.48333333333333, 92.50833333333334, 92.50833333333334, 92.44166666666666, 92.44166666666666, 92.58333333333333, 92.58333333333333, 92.66666666666667, 92.66666666666667, 92.58333333333333, 92.58333333333333, 92.60833333333333, 92.60833333333333, 92.65, 92.65, 92.575, 92.575, 92.625, 92.625, 92.64166666666667, 92.64166666666667, 92.63333333333334, 92.63333333333334, 92.60833333333333, 92.60833333333333, 92.7, 92.7, 92.58333333333333, 92.58333333333333, 92.69166666666666, 92.69166666666666, 92.65, 92.65, 92.63333333333334, 92.63333333333334, 92.575, 92.575, 92.64166666666667, 92.64166666666667, 92.575, 92.575]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  lg  local_only:0   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: lg , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist, level_n_system: 0.0 , level_n_lowerb:0.0  

lg
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 17098 (global); Percentage 3.11 (17098/550346 
)
learning rate, batch size: 0.01, 10 

Round   0, Train loss: 2.301, Test loss: 2.299, Test accuracy: 24.82
Round   1, Train loss: 2.296, Test loss: 2.293, Test accuracy: 36.88
Round   2, Train loss: 2.283, Test loss: 2.277, Test accuracy: 40.03
Round   3, Train loss: 2.199, Test loss: 2.186, Test accuracy: 39.17
Round   4, Train loss: 2.070, Test loss: 2.074, Test accuracy: 50.23
Round   5, Train loss: 1.954, Test loss: 1.974, Test accuracy: 60.64
Round   6, Train loss: 1.849, Test loss: 1.891, Test accuracy: 64.00
Round   7, Train loss: 1.790, Test loss: 1.813, Test accuracy: 72.33
Round   8, Train loss: 1.686, Test loss: 1.783, Test accuracy: 73.78
Round   9, Train loss: 1.682, Test loss: 1.748, Test accuracy: 76.28
Round  10, Train loss: 1.652, Test loss: 1.726, Test accuracy: 77.45
Round  11, Train loss: 1.617, Test loss: 1.719, Test accuracy: 77.71
Round  12, Train loss: 1.636, Test loss: 1.689, Test accuracy: 78.71
Round  13, Train loss: 1.624, Test loss: 1.679, Test accuracy: 79.43
Round  14, Train loss: 1.609, Test loss: 1.673, Test accuracy: 79.86
Round  15, Train loss: 1.594, Test loss: 1.671, Test accuracy: 79.83
Round  16, Train loss: 1.597, Test loss: 1.670, Test accuracy: 79.90
Round  17, Train loss: 1.593, Test loss: 1.668, Test accuracy: 80.01
Round  18, Train loss: 1.589, Test loss: 1.667, Test accuracy: 80.13
Round  19, Train loss: 1.587, Test loss: 1.667, Test accuracy: 79.96
Round  20, Train loss: 1.583, Test loss: 1.666, Test accuracy: 80.06
Round  21, Train loss: 1.593, Test loss: 1.664, Test accuracy: 80.33
Round  22, Train loss: 1.578, Test loss: 1.663, Test accuracy: 80.28
Round  23, Train loss: 1.577, Test loss: 1.663, Test accuracy: 80.35
Round  24, Train loss: 1.579, Test loss: 1.662, Test accuracy: 80.24
Round  25, Train loss: 1.572, Test loss: 1.662, Test accuracy: 80.33
Round  26, Train loss: 1.578, Test loss: 1.662, Test accuracy: 80.36
Round  27, Train loss: 1.580, Test loss: 1.661, Test accuracy: 80.32
Round  28, Train loss: 1.570, Test loss: 1.661, Test accuracy: 80.28
Round  29, Train loss: 1.567, Test loss: 1.661, Test accuracy: 80.28
Round  30, Train loss: 1.581, Test loss: 1.661, Test accuracy: 80.32
Round  31, Train loss: 1.571, Test loss: 1.661, Test accuracy: 80.30
Round  32, Train loss: 1.573, Test loss: 1.661, Test accuracy: 80.34
Round  33, Train loss: 1.566, Test loss: 1.661, Test accuracy: 80.41
Round  34, Train loss: 1.567, Test loss: 1.660, Test accuracy: 80.47
Round  35, Train loss: 1.561, Test loss: 1.660, Test accuracy: 80.45
Round  36, Train loss: 1.574, Test loss: 1.660, Test accuracy: 80.49
Round  37, Train loss: 1.563, Test loss: 1.660, Test accuracy: 80.47
Round  38, Train loss: 1.564, Test loss: 1.660, Test accuracy: 80.48
Round  39, Train loss: 1.577, Test loss: 1.659, Test accuracy: 80.51
Round  40, Train loss: 1.566, Test loss: 1.659, Test accuracy: 80.48
Round  41, Train loss: 1.573, Test loss: 1.659, Test accuracy: 80.46
Round  42, Train loss: 1.571, Test loss: 1.659, Test accuracy: 80.47
Round  43, Train loss: 1.573, Test loss: 1.659, Test accuracy: 80.52
Round  44, Train loss: 1.567, Test loss: 1.659, Test accuracy: 80.51
Round  45, Train loss: 1.567, Test loss: 1.659, Test accuracy: 80.45
Round  46, Train loss: 1.575, Test loss: 1.659, Test accuracy: 80.51
Round  47, Train loss: 1.570, Test loss: 1.659, Test accuracy: 80.48
Round  48, Train loss: 1.572, Test loss: 1.659, Test accuracy: 80.53
Round  49, Train loss: 1.574, Test loss: 1.658, Test accuracy: 80.54
Round  50, Train loss: 1.571, Test loss: 1.658, Test accuracy: 80.55
Round  51, Train loss: 1.568, Test loss: 1.658, Test accuracy: 80.61
Round  52, Train loss: 1.571, Test loss: 1.658, Test accuracy: 80.62
Round  53, Train loss: 1.567, Test loss: 1.658, Test accuracy: 80.61
Round  54, Train loss: 1.567, Test loss: 1.658, Test accuracy: 80.61
Round  55, Train loss: 1.572, Test loss: 1.658, Test accuracy: 80.62
Round  56, Train loss: 1.568, Test loss: 1.658, Test accuracy: 80.62
Round  57, Train loss: 1.565, Test loss: 1.655, Test accuracy: 80.71
Round  58, Train loss: 1.530, Test loss: 1.643, Test accuracy: 82.66
Round  59, Train loss: 1.513, Test loss: 1.637, Test accuracy: 83.32
Round  60, Train loss: 1.508, Test loss: 1.631, Test accuracy: 83.76
Round  61, Train loss: 1.502, Test loss: 1.629, Test accuracy: 84.14
Round  62, Train loss: 1.502, Test loss: 1.624, Test accuracy: 84.39
Round  63, Train loss: 1.496, Test loss: 1.623, Test accuracy: 84.38
Round  64, Train loss: 1.493, Test loss: 1.622, Test accuracy: 84.44
Round  65, Train loss: 1.493, Test loss: 1.619, Test accuracy: 84.69
Round  66, Train loss: 1.492, Test loss: 1.618, Test accuracy: 84.90
Round  67, Train loss: 1.488, Test loss: 1.616, Test accuracy: 85.09
Round  68, Train loss: 1.486, Test loss: 1.616, Test accuracy: 84.96
Round  69, Train loss: 1.490, Test loss: 1.615, Test accuracy: 85.07
Round  70, Train loss: 1.488, Test loss: 1.615, Test accuracy: 85.15
Round  71, Train loss: 1.484, Test loss: 1.614, Test accuracy: 85.15
Round  72, Train loss: 1.485, Test loss: 1.614, Test accuracy: 85.12
Round  73, Train loss: 1.487, Test loss: 1.613, Test accuracy: 85.20
Round  74, Train loss: 1.485, Test loss: 1.613, Test accuracy: 85.27
Round  75, Train loss: 1.483, Test loss: 1.613, Test accuracy: 85.23
Round  76, Train loss: 1.486, Test loss: 1.612, Test accuracy: 85.28
Round  77, Train loss: 1.485, Test loss: 1.612, Test accuracy: 85.36
Round  78, Train loss: 1.483, Test loss: 1.612, Test accuracy: 85.23
Round  79, Train loss: 1.485, Test loss: 1.612, Test accuracy: 85.36
Round  80, Train loss: 1.484, Test loss: 1.612, Test accuracy: 85.32
Round  81, Train loss: 1.483, Test loss: 1.612, Test accuracy: 85.32
Round  82, Train loss: 1.480, Test loss: 1.612, Test accuracy: 85.29
Round  83, Train loss: 1.487, Test loss: 1.612, Test accuracy: 85.27
Round  84, Train loss: 1.480, Test loss: 1.611, Test accuracy: 85.32
Round  85, Train loss: 1.488, Test loss: 1.611, Test accuracy: 85.33
Round  86, Train loss: 1.482, Test loss: 1.611, Test accuracy: 85.34
Round  87, Train loss: 1.485, Test loss: 1.611, Test accuracy: 85.33
Round  88, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.33
Round  89, Train loss: 1.483, Test loss: 1.611, Test accuracy: 85.35
Round  90, Train loss: 1.481, Test loss: 1.611, Test accuracy: 85.34
Round  91, Train loss: 1.480, Test loss: 1.611, Test accuracy: 85.27
Round  92, Train loss: 1.485, Test loss: 1.611, Test accuracy: 85.31
Round  93, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.34
Round  94, Train loss: 1.484, Test loss: 1.611, Test accuracy: 85.32
Round  95, Train loss: 1.480, Test loss: 1.611, Test accuracy: 85.33
Round  96, Train loss: 1.480, Test loss: 1.611, Test accuracy: 85.33
Round  97, Train loss: 1.487, Test loss: 1.611, Test accuracy: 85.28
Round  98, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.29
Round  99, Train loss: 1.484, Test loss: 1.610, Test accuracy: 85.33/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Final Round, Train loss: 1.483, Test loss: 1.610, Test accuracy: 85.45
Average accuracy final 10 rounds: 85.31416666666667 

1455.1031889915466
[1.148658037185669, 2.297316074371338, 3.4235434532165527, 4.549770832061768, 5.728681325912476, 6.907591819763184, 8.098511934280396, 9.289432048797607, 10.46448802947998, 11.639544010162354, 12.778467655181885, 13.917391300201416, 15.094293355941772, 16.27119541168213, 17.47000026702881, 18.66880512237549, 19.780238389968872, 20.891671657562256, 22.024731636047363, 23.15779161453247, 24.366709232330322, 25.575626850128174, 26.722997903823853, 27.87036895751953, 29.02412724494934, 30.17788553237915, 31.403275728225708, 32.628665924072266, 33.851738691329956, 35.07481145858765, 36.234295129776, 37.393778800964355, 38.55108642578125, 39.708394050598145, 40.93096470832825, 42.15353536605835, 43.32083320617676, 44.488131046295166, 45.620497703552246, 46.752864360809326, 47.95318651199341, 49.15350866317749, 50.37284874916077, 51.59218883514404, 52.79559850692749, 53.99900817871094, 55.19762945175171, 56.39625072479248, 57.62688112258911, 58.85751152038574, 60.10313606262207, 61.3487606048584, 62.551149129867554, 63.75353765487671, 64.94641661643982, 66.13929557800293, 67.3897442817688, 68.64019298553467, 69.87251901626587, 71.10484504699707, 72.30968642234802, 73.51452779769897, 74.70183324813843, 75.88913869857788, 77.07483339309692, 78.26052808761597, 79.47011852264404, 80.67970895767212, 81.86363935470581, 83.0475697517395, 84.25274872779846, 85.45792770385742, 86.65539598464966, 87.8528642654419, 89.07347512245178, 90.29408597946167, 91.51532340049744, 92.7365608215332, 93.91579937934875, 95.0950379371643, 96.28297924995422, 97.47092056274414, 98.68707537651062, 99.9032301902771, 101.11799311637878, 102.33275604248047, 103.51659274101257, 104.70042943954468, 105.92799663543701, 107.15556383132935, 108.39652633666992, 109.6374888420105, 110.81788372993469, 111.99827861785889, 113.18815636634827, 114.37803411483765, 115.5785460472107, 116.77905797958374, 118.01376223564148, 119.24846649169922, 120.49187088012695, 121.73527526855469, 122.93737196922302, 124.13946866989136, 125.37485361099243, 126.6102385520935, 127.84298539161682, 129.07573223114014, 130.3011326789856, 131.52653312683105, 132.73124623298645, 133.93595933914185, 135.1913080215454, 136.44665670394897, 137.6857032775879, 138.9247498512268, 140.16397428512573, 141.40319871902466, 142.60596776008606, 143.80873680114746, 145.12418484687805, 146.43963289260864, 147.78233313560486, 149.12503337860107, 150.47521138191223, 151.8253893852234, 153.14107060432434, 154.4567518234253, 155.75179934501648, 157.04684686660767, 158.3567931652069, 159.66673946380615, 160.98251461982727, 162.2982897758484, 163.61906123161316, 164.93983268737793, 166.23852944374084, 167.53722620010376, 168.8371090888977, 170.13699197769165, 171.4242217540741, 172.71145153045654, 174.03995180130005, 175.36845207214355, 176.59757804870605, 177.82670402526855, 179.05680751800537, 180.2869110107422, 181.48042488098145, 182.6739387512207, 183.86888790130615, 185.0638370513916, 186.25135231018066, 187.43886756896973, 188.60912704467773, 189.77938652038574, 190.900874376297, 192.02236223220825, 193.29061603546143, 194.5588698387146, 195.74192810058594, 196.92498636245728, 198.0370650291443, 199.1491436958313, 200.28631925582886, 201.42349481582642, 202.58648538589478, 203.74947595596313, 204.86675000190735, 205.98402404785156, 207.11009860038757, 208.23617315292358, 209.41117477416992, 210.58617639541626, 211.74379062652588, 212.9014048576355, 214.01017689704895, 215.1189489364624, 216.27955532073975, 217.4401617050171, 218.6073853969574, 219.7746090888977, 220.93369436264038, 222.09277963638306, 223.23987817764282, 224.3869767189026, 225.54960131645203, 226.71222591400146, 227.83600759506226, 228.95978927612305, 230.11012816429138, 231.26046705245972, 232.34971976280212, 233.43897247314453, 234.4168562889099, 235.3947401046753, 236.35976910591125, 237.32479810714722, 238.28676390647888, 239.24872970581055, 240.80773663520813, 242.3667435646057]
[24.816666666666666, 24.816666666666666, 36.875, 36.875, 40.03333333333333, 40.03333333333333, 39.175, 39.175, 50.225, 50.225, 60.641666666666666, 60.641666666666666, 64.0, 64.0, 72.325, 72.325, 73.78333333333333, 73.78333333333333, 76.275, 76.275, 77.45, 77.45, 77.70833333333333, 77.70833333333333, 78.70833333333333, 78.70833333333333, 79.43333333333334, 79.43333333333334, 79.85833333333333, 79.85833333333333, 79.825, 79.825, 79.9, 79.9, 80.00833333333334, 80.00833333333334, 80.13333333333334, 80.13333333333334, 79.95833333333333, 79.95833333333333, 80.05833333333334, 80.05833333333334, 80.325, 80.325, 80.275, 80.275, 80.35, 80.35, 80.24166666666666, 80.24166666666666, 80.33333333333333, 80.33333333333333, 80.35833333333333, 80.35833333333333, 80.31666666666666, 80.31666666666666, 80.275, 80.275, 80.28333333333333, 80.28333333333333, 80.31666666666666, 80.31666666666666, 80.3, 80.3, 80.34166666666667, 80.34166666666667, 80.40833333333333, 80.40833333333333, 80.46666666666667, 80.46666666666667, 80.45, 80.45, 80.49166666666666, 80.49166666666666, 80.46666666666667, 80.46666666666667, 80.48333333333333, 80.48333333333333, 80.50833333333334, 80.50833333333334, 80.48333333333333, 80.48333333333333, 80.45833333333333, 80.45833333333333, 80.475, 80.475, 80.51666666666667, 80.51666666666667, 80.50833333333334, 80.50833333333334, 80.45, 80.45, 80.50833333333334, 80.50833333333334, 80.48333333333333, 80.48333333333333, 80.525, 80.525, 80.54166666666667, 80.54166666666667, 80.55, 80.55, 80.60833333333333, 80.60833333333333, 80.61666666666666, 80.61666666666666, 80.60833333333333, 80.60833333333333, 80.60833333333333, 80.60833333333333, 80.625, 80.625, 80.61666666666666, 80.61666666666666, 80.70833333333333, 80.70833333333333, 82.65833333333333, 82.65833333333333, 83.31666666666666, 83.31666666666666, 83.75833333333334, 83.75833333333334, 84.14166666666667, 84.14166666666667, 84.39166666666667, 84.39166666666667, 84.38333333333334, 84.38333333333334, 84.44166666666666, 84.44166666666666, 84.69166666666666, 84.69166666666666, 84.9, 84.9, 85.09166666666667, 85.09166666666667, 84.95833333333333, 84.95833333333333, 85.06666666666666, 85.06666666666666, 85.15, 85.15, 85.15, 85.15, 85.125, 85.125, 85.2, 85.2, 85.26666666666667, 85.26666666666667, 85.23333333333333, 85.23333333333333, 85.275, 85.275, 85.35833333333333, 85.35833333333333, 85.23333333333333, 85.23333333333333, 85.35833333333333, 85.35833333333333, 85.31666666666666, 85.31666666666666, 85.31666666666666, 85.31666666666666, 85.29166666666667, 85.29166666666667, 85.26666666666667, 85.26666666666667, 85.31666666666666, 85.31666666666666, 85.33333333333333, 85.33333333333333, 85.34166666666667, 85.34166666666667, 85.325, 85.325, 85.33333333333333, 85.33333333333333, 85.35, 85.35, 85.34166666666667, 85.34166666666667, 85.26666666666667, 85.26666666666667, 85.30833333333334, 85.30833333333334, 85.34166666666667, 85.34166666666667, 85.31666666666666, 85.31666666666666, 85.33333333333333, 85.33333333333333, 85.325, 85.325, 85.28333333333333, 85.28333333333333, 85.29166666666667, 85.29166666666667, 85.33333333333333, 85.33333333333333, 85.45, 85.45]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FedPAC%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

# alg: fedrep , epochs: 100, shard_per_user: 3, limit_local_output: 0, local_rep_ep: 3 , local_only: 0, is_concept_shift: 0, dataset: mnist  

fedrep
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
odict_keys(['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias'])
8
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias']
['layer_input.weight', 'layer_input.bias', 'layer_hidden1.weight', 'layer_hidden1.bias', 'layer_hidden2.weight', 'layer_hidden2.bias', 'layer_out.weight', 'layer_out.bias']
# Params: 550346 (local), 549696 (global); Percentage 99.88 (549696/550346)
learning rate, batch size: 0.01, 10
MLP(
  (layer_input): Linear(in_features=784, out_features=512, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0, inplace=False)
  (layer_hidden1): Linear(in_features=512, out_features=256, bias=True)
  (layer_hidden2): Linear(in_features=256, out_features=64, bias=True)
  (layer_out): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax(dim=1)
)
Round   0, Train loss: 2.309, Test loss: 2.269, Test accuracy: 23.22
Round   1, Train loss: 2.198, Test loss: 2.037, Test accuracy: 56.24
Round   2, Train loss: 1.930, Test loss: 1.832, Test accuracy: 77.84
Round   3, Train loss: 1.758, Test loss: 1.726, Test accuracy: 82.32
Round   4, Train loss: 1.702, Test loss: 1.682, Test accuracy: 83.66
Round   5, Train loss: 1.662, Test loss: 1.666, Test accuracy: 84.52
Round   6, Train loss: 1.649, Test loss: 1.636, Test accuracy: 86.90
Round   7, Train loss: 1.606, Test loss: 1.607, Test accuracy: 89.64
Round   8, Train loss: 1.585, Test loss: 1.581, Test accuracy: 91.57
Round   9, Train loss: 1.570, Test loss: 1.565, Test accuracy: 92.65
Round  10, Train loss: 1.557, Test loss: 1.555, Test accuracy: 93.56
Round  11, Train loss: 1.550, Test loss: 1.548, Test accuracy: 93.91
Round  12, Train loss: 1.547, Test loss: 1.540, Test accuracy: 94.41
Round  13, Train loss: 1.539, Test loss: 1.533, Test accuracy: 94.92
Round  14, Train loss: 1.534, Test loss: 1.527, Test accuracy: 95.50
Round  15, Train loss: 1.526, Test loss: 1.525, Test accuracy: 95.62
Round  16, Train loss: 1.525, Test loss: 1.521, Test accuracy: 95.85
Round  17, Train loss: 1.522, Test loss: 1.519, Test accuracy: 96.01
Round  18, Train loss: 1.514, Test loss: 1.517, Test accuracy: 96.14
Round  19, Train loss: 1.510, Test loss: 1.515, Test accuracy: 96.33
Round  20, Train loss: 1.514, Test loss: 1.513, Test accuracy: 96.42
Round  21, Train loss: 1.507, Test loss: 1.511, Test accuracy: 96.47
Round  22, Train loss: 1.506, Test loss: 1.509, Test accuracy: 96.64
Round  23, Train loss: 1.505, Test loss: 1.508, Test accuracy: 96.65
Round  24, Train loss: 1.500, Test loss: 1.507, Test accuracy: 96.74
Round  25, Train loss: 1.498, Test loss: 1.506, Test accuracy: 96.83
Round  26, Train loss: 1.496, Test loss: 1.506, Test accuracy: 96.82
Round  27, Train loss: 1.496, Test loss: 1.504, Test accuracy: 96.94
Round  28, Train loss: 1.496, Test loss: 1.503, Test accuracy: 96.97
Round  29, Train loss: 1.498, Test loss: 1.502, Test accuracy: 97.04
Round  30, Train loss: 1.493, Test loss: 1.501, Test accuracy: 97.07
Round  31, Train loss: 1.493, Test loss: 1.501, Test accuracy: 97.14
Round  32, Train loss: 1.491, Test loss: 1.500, Test accuracy: 97.13
Round  33, Train loss: 1.488, Test loss: 1.500, Test accuracy: 97.14
Round  34, Train loss: 1.488, Test loss: 1.499, Test accuracy: 97.22
Round  35, Train loss: 1.486, Test loss: 1.498, Test accuracy: 97.28
Round  36, Train loss: 1.487, Test loss: 1.498, Test accuracy: 97.31
Round  37, Train loss: 1.486, Test loss: 1.498, Test accuracy: 97.32
Round  38, Train loss: 1.486, Test loss: 1.497, Test accuracy: 97.34
Round  39, Train loss: 1.487, Test loss: 1.497, Test accuracy: 97.33
Round  40, Train loss: 1.483, Test loss: 1.497, Test accuracy: 97.33
Round  41, Train loss: 1.484, Test loss: 1.496, Test accuracy: 97.36
Round  42, Train loss: 1.483, Test loss: 1.496, Test accuracy: 97.36
Round  43, Train loss: 1.483, Test loss: 1.496, Test accuracy: 97.38
Round  44, Train loss: 1.483, Test loss: 1.495, Test accuracy: 97.41
Round  45, Train loss: 1.481, Test loss: 1.495, Test accuracy: 97.39
Round  46, Train loss: 1.482, Test loss: 1.495, Test accuracy: 97.45
Round  47, Train loss: 1.482, Test loss: 1.494, Test accuracy: 97.46
Round  48, Train loss: 1.478, Test loss: 1.494, Test accuracy: 97.46
Round  49, Train loss: 1.480, Test loss: 1.494, Test accuracy: 97.56
Round  50, Train loss: 1.479, Test loss: 1.494, Test accuracy: 97.48
Round  51, Train loss: 1.478, Test loss: 1.494, Test accuracy: 97.50
Round  52, Train loss: 1.478, Test loss: 1.493, Test accuracy: 97.55
Round  53, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.57
Round  54, Train loss: 1.478, Test loss: 1.493, Test accuracy: 97.57
Round  55, Train loss: 1.477, Test loss: 1.493, Test accuracy: 97.56
Round  56, Train loss: 1.478, Test loss: 1.492, Test accuracy: 97.57
Round  57, Train loss: 1.477, Test loss: 1.492, Test accuracy: 97.55
Round  58, Train loss: 1.477, Test loss: 1.492, Test accuracy: 97.64
Round  59, Train loss: 1.476, Test loss: 1.492, Test accuracy: 97.68
Round  60, Train loss: 1.476, Test loss: 1.492, Test accuracy: 97.67
Round  61, Train loss: 1.477, Test loss: 1.491, Test accuracy: 97.66
Round  62, Train loss: 1.475, Test loss: 1.491, Test accuracy: 97.66
Round  63, Train loss: 1.474, Test loss: 1.491, Test accuracy: 97.75
Round  64, Train loss: 1.474, Test loss: 1.491, Test accuracy: 97.78
Round  65, Train loss: 1.474, Test loss: 1.491, Test accuracy: 97.75
Round  66, Train loss: 1.473, Test loss: 1.491, Test accuracy: 97.79
Round  67, Train loss: 1.473, Test loss: 1.491, Test accuracy: 97.80
Round  68, Train loss: 1.474, Test loss: 1.490, Test accuracy: 97.76
Round  69, Train loss: 1.473, Test loss: 1.490, Test accuracy: 97.78
Round  70, Train loss: 1.473, Test loss: 1.490, Test accuracy: 97.82
Round  71, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.81
Round  72, Train loss: 1.474, Test loss: 1.490, Test accuracy: 97.83
Round  73, Train loss: 1.472, Test loss: 1.490, Test accuracy: 97.84
Round  74, Train loss: 1.473, Test loss: 1.489, Test accuracy: 97.86
Round  75, Train loss: 1.471, Test loss: 1.490, Test accuracy: 97.83
Round  76, Train loss: 1.473, Test loss: 1.489, Test accuracy: 97.88
Round  77, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.86
Round  78, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.89
Round  79, Train loss: 1.472, Test loss: 1.489, Test accuracy: 97.88
Round  80, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.88
Round  81, Train loss: 1.473, Test loss: 1.489, Test accuracy: 97.89
Round  82, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.88
Round  83, Train loss: 1.470, Test loss: 1.489, Test accuracy: 97.88
Round  84, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.89
Round  85, Train loss: 1.471, Test loss: 1.489, Test accuracy: 97.94
Round  86, Train loss: 1.469, Test loss: 1.488, Test accuracy: 97.93
Round  87, Train loss: 1.469, Test loss: 1.488, Test accuracy: 97.97
Round  88, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.94
Round  89, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.97
Round  90, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.94
Round  91, Train loss: 1.469, Test loss: 1.488, Test accuracy: 97.98
Round  92, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.99
Round  93, Train loss: 1.470, Test loss: 1.488, Test accuracy: 97.98
Round  94, Train loss: 1.470, Test loss: 1.488, Test accuracy: 98.00/home/ChenSM/code/FL_HLS/utils/sampling.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(dataset.targets[i]).item()

Round  95, Train loss: 1.468, Test loss: 1.488, Test accuracy: 98.01
Round  96, Train loss: 1.470, Test loss: 1.487, Test accuracy: 98.04
Round  97, Train loss: 1.468, Test loss: 1.488, Test accuracy: 98.04
Round  98, Train loss: 1.469, Test loss: 1.487, Test accuracy: 98.03
Round  99, Train loss: 1.469, Test loss: 1.487, Test accuracy: 98.05
Final Round, Train loss: 1.467, Test loss: 1.487, Test accuracy: 98.02
Average accuracy final 10 rounds: 98.006
3522.2111954689026
[4.6772966384887695, 9.169788122177124, 13.6083984375, 18.115689754486084, 22.614426612854004, 27.155760765075684, 31.67457938194275, 36.1779146194458, 40.69432234764099, 44.82439684867859, 48.92185020446777, 52.961761713027954, 57.009196281433105, 61.05418872833252, 65.22852921485901, 69.43591356277466, 73.3585467338562, 77.47500491142273, 81.54006290435791, 85.5850338935852, 89.59129166603088, 93.5286774635315, 97.58910775184631, 101.70829391479492, 105.79706883430481, 109.76884508132935, 114.00753235816956, 118.06011128425598, 122.16379594802856, 126.2124855518341, 130.3318452835083, 134.38882875442505, 138.46145868301392, 142.7674744129181, 146.86300539970398, 150.99062418937683, 155.18209385871887, 159.29687023162842, 163.43836402893066, 167.35473132133484, 171.5176305770874, 175.5577962398529, 179.56477451324463, 183.71388220787048, 187.78987216949463, 191.92318725585938, 196.00862979888916, 200.08796882629395, 204.0442247390747, 208.2152020931244, 212.47323322296143, 216.39925146102905, 220.6771321296692, 224.85243439674377, 228.8513686656952, 232.91793155670166, 237.00141763687134, 241.08088898658752, 245.11490941047668, 249.20864582061768, 253.2317397594452, 257.2712950706482, 261.3890492916107, 265.4494616985321, 269.4285442829132, 273.4762804508209, 277.57254910469055, 281.65556502342224, 285.6742537021637, 289.79200172424316, 293.8436667919159, 297.8333489894867, 301.83777379989624, 305.94549322128296, 310.0732705593109, 314.0079855918884, 318.0961723327637, 322.029682636261, 326.0899736881256, 330.0804944038391, 334.20689702033997, 338.2272720336914, 342.25922083854675, 346.34844303131104, 350.38136982917786, 354.4167721271515, 358.5422749519348, 362.68914556503296, 366.6699731349945, 370.7962782382965, 375.0537140369415, 379.079843044281, 383.25506019592285, 387.56238293647766, 391.5675699710846, 395.73115158081055, 399.8620672225952, 404.0107276439667, 408.0504512786865, 412.1681225299835, 413.785902261734]
[23.22, 56.2375, 77.8425, 82.32, 83.655, 84.515, 86.9025, 89.6425, 91.5725, 92.65, 93.5625, 93.91, 94.405, 94.9225, 95.495, 95.6175, 95.8475, 96.0125, 96.1425, 96.335, 96.415, 96.4675, 96.6425, 96.65, 96.7375, 96.8325, 96.8175, 96.935, 96.965, 97.04, 97.0675, 97.145, 97.1275, 97.14, 97.225, 97.2775, 97.305, 97.3225, 97.3375, 97.3325, 97.335, 97.3575, 97.3625, 97.3775, 97.4125, 97.39, 97.4475, 97.4575, 97.4625, 97.56, 97.485, 97.5025, 97.5475, 97.5725, 97.57, 97.5575, 97.5725, 97.545, 97.64, 97.68, 97.675, 97.66, 97.6575, 97.745, 97.775, 97.7525, 97.7875, 97.8025, 97.76, 97.78, 97.8225, 97.8125, 97.835, 97.8375, 97.865, 97.825, 97.88, 97.8625, 97.885, 97.88, 97.8825, 97.895, 97.88, 97.88, 97.885, 97.935, 97.9325, 97.97, 97.9375, 97.9725, 97.94, 97.9775, 97.99, 97.9825, 98.0, 98.0125, 98.0375, 98.04, 98.0325, 98.0475, 98.0225]
